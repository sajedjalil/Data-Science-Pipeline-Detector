{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d7bc35bfdb4aa0dbb83ad76872ffb50446c5295"},"cell_type":"code","source":"directory = '../input'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import models\nimport torchvision\n\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UNet11(nn.Module):\n    def __init__(self, num_filters=32):\n        \"\"\"\n        :param num_classes:\n        :param num_filters:\n        \"\"\"\n        super().__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Convolutions are from VGG11\n        self.encoder = models.vgg11().features\n        \n        # \"relu\" layer is taken from VGG probably for generality, but it's not clear \n        self.relu = self.encoder[1]\n        \n        self.conv1 = self.encoder[0]\n        self.conv2 = self.encoder[3]\n        self.conv3s = self.encoder[6]\n        self.conv3 = self.encoder[8]\n        self.conv4s = self.encoder[11]\n        self.conv4 = self.encoder[13]\n        self.conv5s = self.encoder[16]\n        self.conv5 = self.encoder[18]\n\n        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n        \n        self.final = nn.Conv2d(num_filters, 1, kernel_size=1, )\n\n    def forward(self, x):\n        conv1 = self.relu(self.conv1(x))\n        conv2 = self.relu(self.conv2(self.pool(conv1)))\n        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n        conv3 = self.relu(self.conv3(conv3s))\n        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n        conv4 = self.relu(self.conv4(conv4s))\n        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n        conv5 = self.relu(self.conv5(conv5s))\n\n        center = self.center(self.pool(conv5))\n\n        # Deconvolutions with copies of VGG11 layers of corresponding size \n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n        return F.sigmoid(self.final(dec1))\n\n\ndef unet11(**kwargs):\n    model = UNet11(**kwargs)\n\n    return model\n\ndef get_model():\n    model = unet11()\n    model.train()\n    return model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import cv2\nfrom pathlib import Path\nfrom torch.nn import functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7114b9f3da03d4688ecfdecd7c7008a0be0c8004"},"cell_type":"code","source":"def load_image(path, mask = False):\n    \"\"\"\n    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n    \n    if pad = True:\n        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n    else:\n        returns image as numpy.array\n    \"\"\"\n    img = cv2.imread(str(path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    height, width, _ = img.shape\n\n    # Padding in needed for UNet models because they need image size to be divisible by 32 \n    if height % 32 == 0:\n        y_min_pad = 0\n        y_max_pad = 0\n    else:\n        y_pad = 32 - height % 32\n        y_min_pad = int(y_pad / 2)\n        y_max_pad = y_pad - y_min_pad\n        \n    if width % 32 == 0:\n        x_min_pad = 0\n        x_max_pad = 0\n    else:\n        x_pad = 32 - width % 32\n        x_min_pad = int(x_pad / 2)\n        x_max_pad = x_pad - x_min_pad\n    \n    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n    if mask:\n        # Convert mask to 0 and 1 format\n        img = img[:, :, 0:1] // 255\n        return torch.from_numpy(img).float().permute([2, 0, 1])\n    else:\n        img = img / 255.0\n        return torch.from_numpy(img).float().permute([2, 0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e0c6c34c6916e43b8f4e8e1f6eb708f8049b3d","collapsed":true},"cell_type":"code","source":"# Adapted from vizualization kernel\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\nfrom torch.utils import data\n\nclass TGSSaltDataset(data.Dataset):\n    def __init__(self, root_path, file_list, is_test = False):\n        self.is_test = is_test\n        self.root_path = root_path\n        self.file_list = file_list\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        file_id = self.file_list[index]\n        \n        image_folder = os.path.join(self.root_path, \"images\")\n        image_path = os.path.join(image_folder, file_id + \".png\")\n        \n        mask_folder = os.path.join(self.root_path, \"masks\")\n        mask_path = os.path.join(mask_folder, file_id + \".png\")\n        \n        image = load_image(image_path)\n        \n        if self.is_test:\n            return (image,)\n        else:\n            mask = load_image(mask_path, mask = True)\n            return image, mask\n\ndepths_df = pd.read_csv(os.path.join(directory, 'train.csv'))\n\ntrain_path = os.path.join(directory, 'train')\nfile_list = list(depths_df['id'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"89289aeceba7a47c7478e9a7fb1232cedeed70b2"},"cell_type":"code","source":"device = \"cpu\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"767dae95adfbe7fefcb4cc3dd397ea8ba0e3f8d1","collapsed":true},"cell_type":"code","source":"import tqdm\n\nfile_list_val = file_list[::10]\nfile_list_train = [f for f in file_list if f not in file_list_val]\ndataset = TGSSaltDataset(train_path, file_list_train)\ndataset_val = TGSSaltDataset(train_path, file_list_val)\n\nmodel = get_model()\n#\n\nlearning_rate = 1e-4\nloss_fn = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nfor e in range(1):\n    train_loss = []\n    for image, mask in tqdm.tqdm(data.DataLoader(dataset, batch_size = 30, shuffle = True)):\n        image = image.type(torch.float).to(device)\n        y_pred = model(image)\n        loss = loss_fn(y_pred, mask.to(device))\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n        train_loss.append(loss.item())\n        \n    val_loss = []\n    for image, mask in data.DataLoader(dataset_val, batch_size = 50, shuffle = False):\n        image = image.to(device)\n        y_pred = model(image)\n\n        loss = loss_fn(y_pred, mask.to(device))\n        val_loss.append(loss.item())\n\n    print(\"Epoch: %d, Train: %.3f, Val: %.3f\" % (e, np.mean(train_loss), np.mean(val_loss)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60392c6252c7f2628db1e4fef8ae69e29f7e753e"},"cell_type":"code","source":"import glob\n\ntest_path = os.path.join(directory, 'test')\ntest_file_list = glob.glob(os.path.join(test_path, 'images', '*.png'))\ntest_file_list = [f.split('/')[-1].split('.')[0] for f in test_file_list]\ntest_file_list[:3], test_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"971e75a32512f23aee3cbc629df64c8079940e91"},"cell_type":"code","source":"print(len(test_file_list))\ntest_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n\nall_predictions = []\nfor image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 30)):\n    image = image[0].type(torch.float).to(device)\n    y_pred = model(image).cpu().detach().numpy()\n    all_predictions.append(y_pred)\nall_predictions_stacked = np.vstack(all_predictions)[:, 0, :, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"95e82b2a7155377310f1d743dd8b077f99cba657"},"cell_type":"code","source":"height, width = 101, 101\n\nif height % 32 == 0:\n    y_min_pad = 0\n    y_max_pad = 0\nelse:\n    y_pad = 32 - height % 32\n    y_min_pad = int(y_pad / 2)\n    y_max_pad = y_pad - y_min_pad\n\nif width % 32 == 0:\n    x_min_pad = 0\n    x_max_pad = 0\nelse:\n    x_pad = 32 - width % 32\n    x_min_pad = int(x_pad / 2)\n    x_max_pad = x_pad - x_min_pad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a0c02220ba7768ac904e2e27f449393e5182cac5"},"cell_type":"code","source":"all_predictions_stacked = all_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc57c37629a96ac4c16dfbc18546278591716613"},"cell_type":"code","source":"all_predictions_stacked.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"65d1b4f79719da90faa68bb73359a5189ea70bfd"},"cell_type":"code","source":"test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n\nval_predictions = []\nval_masks = []\nfor image, mask in tqdm.tqdm(data.DataLoader(dataset_val, batch_size = 30)):\n    image = image.type(torch.float).to(device)\n    y_pred = model(image).cpu().detach().numpy()\n    val_predictions.append(y_pred)\n    val_masks.append(mask)\n    \nval_predictions_stacked = np.vstack(val_predictions)[:, 0, :, :]\n\nval_masks_stacked = np.vstack(val_masks)[:, 0, :, :]\nval_predictions_stacked = val_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n\nval_masks_stacked = val_masks_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\nval_masks_stacked.shape, val_predictions_stacked.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dbd7cbac108805a401a445947b4cfd95721e10ee"},"cell_type":"code","source":"from sklearn.metrics import jaccard_similarity_score\n\nmetric_by_threshold = []\nfor threshold in np.linspace(0, 1, 11):\n    val_binary_prediction = (val_predictions_stacked > threshold).astype(int)\n    \n    iou_values = []\n    for y_mask, p_mask in zip(val_masks_stacked, val_binary_prediction):\n        iou = jaccard_similarity_score(y_mask.flatten(), p_mask.flatten())\n        iou_values.append(iou)\n    iou_values = np.array(iou_values)\n    \n    accuracies = [\n        np.mean(iou_values > iou_threshold)\n        for iou_threshold in np.linspace(0.5, 0.95, 10)\n    ]\n    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))\n    metric_by_threshold.append((np.mean(accuracies), threshold))\n    \nbest_metric, best_threshold = max(metric_by_threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"69b3e6549ac9dac536eacd7d116d1942a61b0b50"},"cell_type":"code","source":"threshold = best_threshold\nbinary_prediction = (all_predictions_stacked > threshold).astype(int)\n\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\nall_masks = []\nfor p_mask in list(binary_prediction):\n    p_mask = rle_encoding(p_mask)\n    all_masks.append(' '.join(map(str, p_mask)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d2d05d0ed3c54619523a55683d6e1afc22dace1f"},"cell_type":"code","source":"submit = pd.DataFrame([test_file_list, all_masks]).T\nsubmit.columns = ['id', 'rle_mask']\nsubmit.to_csv('submit_baseline2.csv.gz', compression = 'gzip', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"811efa49c3fc86adbf40e13003664a8212f77fed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}