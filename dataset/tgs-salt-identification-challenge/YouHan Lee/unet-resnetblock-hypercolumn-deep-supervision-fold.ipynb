{"cells":[{"metadata":{"_uuid":"fb46101352ef46c98fb59d9c42d6f0c5c457908b"},"cell_type":"markdown","source":"# Background\n- Based on dicussions and kernels, I made function to make a Unet+ResNetBlock+Hypercolumn+Deep supervision.\n- Thanks for sharing to all of authors!\n- https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/68435\n- https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/68190\n- For scSE, I could't find the good position where scSE is added. You can add scSE in more good position!"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:47.664316Z","start_time":"2018-10-19T16:27:46.400238Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\nfrom keras.backend import tf as ktf\nimport cv2\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm import tqdm #, tnrange\n#from itertools import chain\nfrom skimage.io import imread, imshow #, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model, save_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add, AveragePooling2D, GlobalAveragePooling2D, concatenate, Activation, Flatten, UpSampling2D, Dense\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras import optimizers\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img#,save_img\nfrom keras.preprocessing.image import ImageDataGenerator\nimport time\nt_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:47.668415Z","start_time":"2018-10-19T16:27:47.665713Z"},"_uuid":"2c36d16775556c3e358edefab5710dc1541ccfc4","trusted":true},"cell_type":"code","source":"cv_total = 2 # small for explaination\n#cv_index = 1 -5\n\n\nversion = 1\nbasic_name_ori = 'Unet+resnetblock+hyper+multipleloss'\nsave_model_name = basic_name_ori + '.model'\nsubmission_file = basic_name_ori + '.csv'\n\nprint(save_model_name)\nprint(submission_file)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:47.770877Z","start_time":"2018-10-19T16:27:47.669632Z"},"_uuid":"63c469280793719bf311d51e6ba2cdaea157d175","trusted":true},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:47.942862Z","start_time":"2018-10-19T16:27:47.775548Z"},"_uuid":"1a64babef03b9a0dbc94387a1dad54971c3e028d","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:49.68315Z","start_time":"2018-10-19T16:27:47.944307Z"},"_uuid":"80c3768717007fb5f087d3e01619f1a9f9a3beac","trusted":true},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in (train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:50.428823Z","start_time":"2018-10-19T16:27:49.684358Z"},"_uuid":"9f55103f7daad6f03ec874c643077fe686c31bee","trusted":true},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in (train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c916289a52d0576816502db92470c5d47926c9e6"},"cell_type":"markdown","source":"#### calculate mask type for stratify, the difficuly of training different mask type is different. \n* Reference  from Heng's discussion, search \"error analysis\" in the following link\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984#382657****"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:50.655869Z","start_time":"2018-10-19T16:27:50.430011Z"},"_uuid":"030d17f898e7d9b7eccfe8726dd1fb1407ec98cd","trusted":true},"cell_type":"code","source":"#### Reference  from Heng's discussion\n# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984#382657\ndef get_mask_type(mask):\n    border = 10\n    outer = np.zeros((101-2*border, 101-2*border), np.float32)\n    outer = cv2.copyMakeBorder(outer, border, border, border, border, borderType = cv2.BORDER_CONSTANT, value = 1)\n\n    cover = (mask>0.5).sum()\n    if cover < 8:\n        return 0 # empty\n    if cover == ((mask*outer) > 0.5).sum():\n        return 1 #border\n    if np.all(mask==mask[0]):\n        return 2 #vertical\n\n    percentage = cover/(101*101)\n    if percentage < 0.15:\n        return 3\n    elif percentage < 0.25:\n        return 4\n    elif percentage < 0.50:\n        return 5\n    elif percentage < 0.75:\n        return 6\n    else:\n        return 7\n\ndef histcoverage(coverage):\n    histall = np.zeros((1,8))\n    for c in coverage:\n        histall[0,c] += 1\n    return histall\n\ntrain_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_target, 2)\n\ntrain_df[\"coverage_class\"] = train_df.masks.map(get_mask_type)\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:50.662791Z","start_time":"2018-10-19T16:27:50.657161Z"},"_uuid":"0bbed9f883dd9c6ce22767c6a86bce4fe12b8a45","trusted":true},"cell_type":"code","source":"train_all = []\nevaluate_all = []\nskf = StratifiedKFold(n_splits=cv_total, random_state=1234, shuffle=True)\nfor train_index, evaluate_index in skf.split(train_df.index.values, train_df.coverage_class):\n    train_all.append(train_index)\n    evaluate_all.append(evaluate_index)\n    print(train_index.shape,evaluate_index.shape) # the shape is slightly different in different cv, it's OK","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19e3d9e187703c0cad21251155d13302b7215cf8"},"cell_type":"markdown","source":"# depth data"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:50.776861Z","start_time":"2018-10-19T16:27:50.663911Z"},"trusted":true,"_uuid":"b8d9fac01c262c93d610f43d3471b578d47a39a3"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:50.86514Z","start_time":"2018-10-19T16:27:50.78064Z"},"trusted":true,"_uuid":"73d4c1c3f7e23c64ced16c2dd1aaff16d2384955"},"cell_type":"code","source":"scaler = StandardScaler()\ntrain_depth = scaler.fit_transform(depths_df['z'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:27:51.031441Z","start_time":"2018-10-19T16:27:50.868653Z"},"trusted":true,"_uuid":"fce8fb2dc5ac95824565c0c50dab8fc95cb43579"},"cell_type":"code","source":"test_depth = scaler.transform(test_df['z'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a194642b926cf4c932bfa51a5d6a1c427f714b9"},"cell_type":"markdown","source":"# Coverage"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:00.111871Z","start_time":"2018-10-19T16:28:00.027634Z"},"trusted":true,"_uuid":"9fc579548c25efbffeba02f447cc0e6d04dc4799"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\ntrain_coverage = encoder.fit_transform(train_df['coverage_class'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:00.199435Z","start_time":"2018-10-19T16:28:00.116413Z"},"_uuid":"738cb1bf628e12bb471a5a5d36cd68814004f5a5","trusted":true},"cell_type":"code","source":"def get_cv_data(cv_index):\n    train_index = train_all[cv_index-1]\n    evaluate_index = evaluate_all[cv_index-1]\n    \n    x_train = np.array(train_df.images[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_train = np.array(train_df.masks[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    \n    y_train_depth = train_depth[train_index]\n    y_train_cover = train_df.coverage[train_index]\n    y_train_cover_class = train_df.coverage_class[train_index]\n    \n    x_valid = np.array(train_df.images[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_valid = np.array(train_df.masks[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    \n    y_valid_depth = train_depth[evaluate_index]\n    y_valid_cover = train_df.coverage[evaluate_index]\n    y_valid_cover_class = train_df.coverage_class[evaluate_index]\n    \n    return x_train,y_train, y_train_depth, y_train_cover, y_train_cover_class,  x_valid,y_valid, y_valid_depth, y_valid_cover, y_valid_cover_class","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2956f050f3aa78830ca41df34ff948deea9c6a82"},"cell_type":"markdown","source":"#### Show  some examples of different mask"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:00.909098Z","start_time":"2018-10-19T16:28:00.203242Z"},"_uuid":"ecf001b66bdc494b3869c25ce26760705f25a9a0","trusted":true},"cell_type":"code","source":"cv_index = 1\ntrain_index = train_all[cv_index-1]\nevaluate_index = evaluate_all[cv_index-1]\n\nprint(train_index.shape,evaluate_index.shape)\nhistall = histcoverage(train_df.coverage_class[train_index].values)\n# print(f'train cv{cv_index}, number of each mask class = \\n \\t{histall}')\nhistall_test = histcoverage(train_df.coverage_class[evaluate_index].values)\n# print(f'evaluate cv{cv_index}, number of each mask class = \\n \\t {histall_test}')\n\nfig, axes = plt.subplots(nrows=2, ncols=8, figsize=(24, 6), sharex=True, sharey=True)\n\n# show mask class example\nfor c in range(8):\n    j= 0\n    for i in train_index:\n        if train_df.coverage_class[i] == c:\n            axes[j,c].imshow(np.array(train_df.masks[i])  )\n            axes[j,c].set_axis_off()\n            axes[j,c].set_title('class {}'.format(c))\n            j += 1\n            if(j>=2):\n                break","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:00.91704Z","start_time":"2018-10-19T16:28:00.910624Z"},"trusted":true,"_uuid":"3d19c27ff7e8b07bc328a5b1d37402f3a3758700"},"cell_type":"code","source":"from keras.losses import binary_crossentropy\nfrom keras import backend as K\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred / (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) / K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 / w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:01.016157Z","start_time":"2018-10-19T16:28:00.918275Z"},"_uuid":"7fb577cdf27f365d4a912728c2a7654d0e60fac8","trusted":true},"cell_type":"code","source":"def BatchActivate(x):\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    if activation == True:\n        x = BatchActivate(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16, batch_activate = False):\n    x = BatchActivate(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    if batch_activate:\n        x = BatchActivate(x)\n    return x\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:01.14014Z","start_time":"2018-10-19T16:28:01.017987Z"},"_uuid":"02967d71ee7f936254ab54acf2aa7c2e038a2b21","trusted":true},"cell_type":"code","source":"# Build model\ndef build_model(input_layer, lr, start_neurons, DropoutRatio = 0.5):\n    # 101 -> 50\n    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(input_layer)\n    conv1 = residual_block(conv1,start_neurons * 1)\n    conv1 = residual_block(conv1,start_neurons * 1, True)\n    pool1 = MaxPooling2D((2, 2))(conv1)\n    pool1 = Dropout(DropoutRatio/2)(pool1)\n\n    # 50 -> 25\n    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(pool1)\n    conv2 = residual_block(conv2,start_neurons * 2)\n    conv2 = residual_block(conv2,start_neurons * 2, True)\n    pool2 = MaxPooling2D((2, 2))(conv2)\n    pool2 = Dropout(DropoutRatio)(pool2)\n\n    # 25 -> 12\n    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(pool2)\n    conv3 = residual_block(conv3,start_neurons * 4)\n    conv3 = residual_block(conv3,start_neurons * 4, True)\n    pool3 = MaxPooling2D((2, 2))(conv3)\n    pool3 = Dropout(DropoutRatio)(pool3)\n\n    # 12 -> 6\n    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(pool3)\n    conv4 = residual_block(conv4,start_neurons * 8)\n    conv4 = residual_block(conv4,start_neurons * 8, True)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(DropoutRatio)(pool4)\n\n    # Middle\n    convm = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 16)\n    convm = residual_block(convm,start_neurons * 16, True)\n    img_pool = AveragePooling2D(pool_size=8)(convm)\n    image_pool = Conv2D(64, 1)(img_pool)\n    \n    classification_cover_class = Flatten()(image_pool)\n    classification_cover_class = Dense(8, activation='sigmoid', name='cover_class_output')(classification_cover_class)\n    \n    classification_cover = Flatten()(image_pool)\n    classification_cover = Dense(1, name='cover_output')(classification_cover)\n    \n    classification_depth = Flatten()(image_pool)\n    classification_depth = Dense(1, name='depth_output')(classification_depth)\n    \n\n    \n    # 6 -> 12\n    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(DropoutRatio)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 8)\n    uconv4 = residual_block(uconv4,start_neurons * 8, True)\n    \n    # 12 -> 25\n    #deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(DropoutRatio)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 4)\n    uconv3 = residual_block(uconv3,start_neurons * 4, True)\n\n    # 25 -> 50\n    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(DropoutRatio)(uconv2)\n    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 2)\n    uconv2 = residual_block(uconv2,start_neurons * 2, True)\n    \n    # 50 -> 101\n    #deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(DropoutRatio)(uconv1)\n    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 1)\n    uconv1 = residual_block(uconv1,start_neurons * 1, True)\n    \n    \n    \n#     from keras.backend import tf as ktf\n    \n    hypercolumn = concatenate(\n        [\n            uconv1,\n            Lambda(lambda image: ktf.image.resize_images(image, (img_size_target, img_size_target)))(uconv2),\n            Lambda(lambda image: ktf.image.resize_images(image, (img_size_target, img_size_target)))(uconv3),\n            Lambda(lambda image: ktf.image.resize_images(image, (img_size_target, img_size_target)))(uconv4)\n        ]\n    )\n    hypercolumn = Dropout(0.5)(hypercolumn)\n    hypercolumn = Conv2D(start_neurons * 1, (3, 3), padding=\"same\", activation='relu')(hypercolumn)\n\n    \n    up_image_pool = UpSampling2D(128)(image_pool)\n    \n    fusion = concatenate([hypercolumn, up_image_pool])\n    fusion = Conv2D(1, (3, 3), padding='same')(fusion)\n    fusion = Activation('sigmoid', name='fusion_output')(fusion)\n    \n    #uconv1 = Dropout(DropoutRatio/2)(uconv1)\n    #output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n    output_layer_noActi = Conv2D(1, (1,1), padding=\"same\", activation=None)(hypercolumn)\n    output_layer =  Activation('sigmoid', name='seg_output')(output_layer_noActi)\n    losses = {\n        \"cover_class_output\": \"categorical_crossentropy\",\n        \"cover_output\": \"mean_squared_error\",\n        'depth_output': 'mean_squared_error',\n        'seg_output': bce_dice_loss,\n        'fusion_output': bce_dice_loss\n    }\n#     lossWeights = {\n#         \"cover_class_output\": 0.5, \n#         \"depth_output\": 0.5,\n#         'depth_output': 0.5,\n#         'seg_output': 1.0,\n#         'fusion_output': 1.0\n#     }\n    \n    \n    \n    model = Model(inputs=input_layer, outputs=[classification_cover_class, classification_cover, classification_depth, output_layer, fusion])\n    c = optimizers.adam(lr=lr)\n    \n    model.compile(loss=losses, optimizer=c, metrics=[my_iou_metric])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:01.251201Z","start_time":"2018-10-19T16:28:01.144724Z"},"trusted":true,"_uuid":"5a27cb5cce577b9f69b457231ecb2863bbf58a36"},"cell_type":"code","source":"def build_complie_model(lr = 0.01):\n    input_layer = Input((img_size_target, img_size_target, 1))\n    model = build_model(input_layer, lr, 16, 0.5)\n\n#     model1 = Model(input_layer, output_layer)\n\n#     c = optimizers.adam(lr = lr)\n#     model1.compile(loss=\"binary_crossentropy\", optimizer=c, metrics=[my_iou_metric])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:01.380921Z","start_time":"2018-10-19T16:28:01.255157Z"},"_uuid":"2bd5e479d3aa211bcb5fe32ce9c4d71cd012eefa","trusted":true},"cell_type":"code","source":"def get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        t, p = A[batch]>0, B[batch]>0\n#         if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n#             metric.append(0)\n#             continue\n#         if np.count_nonzero(t) >= 1 and np.count_nonzero(p) == 0:\n#             metric.append(0)\n#             continue\n#         if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n#             metric.append(1)\n#             continue\n        \n        intersection = np.logical_and(t, p)\n        union = np.logical_or(t, p)\n        iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n        thresholds = np.arange(0.5, 1, 0.05)\n        s = []\n        for thresh in thresholds:\n            s.append(iou > thresh)\n        metric.append(np.mean(s))\n\n    return np.mean(metric)\n\ndef my_iou_metric(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred>0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)\n\n# code download from: https://github.com/bermanmaxim/LovaszSoftmax\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\ndef lovasz_loss(y_true, y_pred):\n    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n    #logits = K.log(y_pred / (1. - y_pred))\n    logits = y_pred #Jiaxin\n    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-19T16:28:01.512049Z","start_time":"2018-10-19T16:28:01.385881Z"},"_uuid":"01d976e148adc235da239a8d679f0e31a99ddfc4","trusted":true},"cell_type":"code","source":"def plot_history(history,metric_name):\n    fig, (ax_loss, ax_score) = plt.subplots(1, 2, figsize=(15,5))\n    ax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax_loss.legend()\n    ax_score.plot(history.epoch, history.history[metric_name], label=\"Train score\")\n    ax_score.plot(history.epoch, history.history[\"val_\" + metric_name], label=\"Validation score\")\n    ax_score.legend()\n\ndef predict_result(model,x_test,img_size_target): # predict both orginal and reflect x\n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test = model.predict(x_test).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict(x_test_reflect).reshape(-1, img_size_target, img_size_target)\n    preds_test += np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    return preds_test/2","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:30:38.125Z"},"_uuid":"30622932f68888e895a9b8cac91810a1bb3c5e75","scrolled":false,"trusted":true},"cell_type":"code","source":"# training\nious = [0] * cv_total\nfor cv_index in range(cv_total):\n    basic_name = 'Unet_resnet_v{}_cv{}'.format(version, cv_index+1)\n    print('############################################\\n', basic_name)\n    save_model_name = basic_name + '.model'\n    \n    train_index = train_all[cv_index-1]\n    evaluate_index = evaluate_all[cv_index-1]\n\n    x_train = np.array(train_df.images[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_train = np.array(train_df.masks[train_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n\n    y_train_depth = train_depth[train_index]\n    y_train_cover = train_df.coverage[train_index]\n    y_train_cover_class = train_coverage[train_index]\n\n    x_valid = np.array(train_df.images[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n    y_valid = np.array(train_df.masks[evaluate_index].map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\n\n    y_valid_depth = train_depth[evaluate_index]\n    y_valid_cover = train_df.coverage[evaluate_index]\n    y_valid_cover_class = train_coverage[evaluate_index]\n    \n    #Data augmentation\n    x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n    y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n    \n    y_train_cover_class = np.concatenate([y_train_cover_class, y_train_cover_class])\n    y_train_cover = pd.concat([y_train_cover, y_train_cover])\n    y_train_depth = np.concatenate([y_train_depth, y_train_depth])\n\n    model = build_complie_model(lr = 0.005)\n    ######################## first learning multi loss\n    early_stopping = EarlyStopping(monitor='val_seg_output_my_iou_metric', mode = 'max',patience=20, verbose=1)\n    model_checkpoint = ModelCheckpoint(save_model_name,monitor='val_seg_output_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(monitor='val_seg_output_my_iou_metric', mode = 'max',\n                                  factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n\n    epochs = 2 #small number for demonstration \n    batch_size = 32\n\n    dict_train = {'cover_class_output': y_train_cover_class,\n                 'cover_output': y_train_cover,\n                 'depth_output': y_train_depth,\n                 'seg_output': y_train,\n                 'fusion_output': y_train}\n    dict_valid = {'cover_class_output': y_valid_cover_class,\n                 'cover_output': y_valid_cover,\n                 'depth_output': y_valid_depth,\n                 'seg_output': y_valid,\n                 'fusion_output': y_valid}\n    \n    history = model.fit(x_train, dict_train,\n                        validation_data=[x_valid, dict_valid], \n                        epochs=epochs,\n                        batch_size=batch_size,\n                        callbacks=[early_stopping, model_checkpoint,reduce_lr], \n                        verbose=1)\n#     plot_history(history,'my_iou_metric')\n    \n    ############################################## 2nd learning lovasz    \n    model.load_weights(save_model_name)\n    # remove model activation layer and use losvasz loss\n    \n    input_x = model.layers[0].input\n\n    output_layer = model.layers[-1].input\n    model1 = Model(input_x, output_layer)\n    c = optimizers.adam(lr = 0.001)\n\n    # lovasz_loss need input range (-∞，+∞), so cancel the last \"sigmoid\" activation  \n    # Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\n    model1.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])\n    \n    \n    early_stopping = EarlyStopping(monitor='my_iou_metric_2', mode = 'max',patience=20, verbose=1)\n    model_checkpoint = ModelCheckpoint(save_model_name,monitor='my_iou_metric_2', \n                                       mode = 'max', save_best_only=True, verbose=1)\n    reduce_lr = ReduceLROnPlateau(monitor='my_iou_metric_2', mode = 'max',\n                                  factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n    \n    \n    epochs = 1 #small number for demonstration \n    batch_size = 32\n    \n    \n    history = model1.fit(x_train, y_train,\n                        validation_data=[x_valid, y_valid], \n                        epochs=epochs,\n                        batch_size=batch_size,\n                        callbacks=[early_stopping, model_checkpoint,reduce_lr], \n                        verbose=1)\n    \n    model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n                                                       'lovasz_loss': lovasz_loss, 'ktf':ktf, 'img_size_target': img_size_target})\n    \n    \n    preds_valid = predict_result(model,x_valid,img_size_target)\n    ious[cv_index] = get_iou_vector(y_valid, (preds_valid > 0.5))\n    del model\n    \n#model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:53.89Z"},"_uuid":"9de0c7646478423ddede8f16c631ca9c842d040e","trusted":true},"cell_type":"code","source":"for cv_index in range(cv_total):\n    print(\"cv {} ious = {}\".format(cv_index, ious[cv_index]))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:54.193Z"},"_uuid":"1ad6a747a0e32945572cafe870b48d34ab05dc46","trusted":true},"cell_type":"code","source":"\"\"\"\nused for converting the decoded image to rle mask\nFast compared to previous one\n\"\"\"\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:54.472Z"},"_uuid":"452d89840bc86c630f13c3d63d0700ab4ad27b1b","scrolled":true,"trusted":true},"cell_type":"code","source":"x_test = [(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale = True))) / 255 for idx in (test_df.index)]\nx_test = [resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True) for img in (x_test)]\nx_test = np.array(x_test).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:54.736Z"},"trusted":true,"_uuid":"7a8fd99de4ad2b5b0467df429822c1eb6e7b6800"},"cell_type":"code","source":"# # np.save('../input/test/x_test.npy', x_test)\n# x_test = np.load('../input/test/x_test.npy')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:54.992Z"},"trusted":true,"_uuid":"cf0b8e0e5f286cf41ff2623c00183b9a7bb3baa1"},"cell_type":"code","source":"model = load_model(save_model_name,custom_objects={'my_iou_metric_2': my_iou_metric_2,\n                                                   'lovasz_loss': lovasz_loss, 'ktf':ktf, 'img_size_target': img_size_target})","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:55.361Z"},"_uuid":"f68651e1ce6ad9a461c8f1a25a1250fd489adfed","trusted":true},"cell_type":"code","source":"# average the predictions from different folds\nt1 = time.time()\npreds_test = np.zeros(np.squeeze(x_test).shape)\nfor cv_index in range(cv_total):\n    basic_name = 'Unet_resnet_v{}_cv{}'.format(version, cv_index+1)\n    model.load_weights(basic_name + '.model')\n    preds_test += predict_result(model,x_test,img_size_target) /cv_total\n    \nt2 = time.time()\nprint(\"Usedtime = {} s\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:55.848Z"},"_uuid":"7ccac81a492b9caaff4a25401165e145cf2c6f8e","scrolled":true,"trusted":true},"cell_type":"code","source":"\nt1 = time.time()\nthreshold  = 0.5 # some value in range 0.4- 0.5 may be better \npred_dict = {idx: rle_encode(np.round(cv2.resize(preds_test[i], dsize=(101,101), interpolation = cv2.INTER_CUBIC)) > threshold) for i, idx in enumerate(tqdm(test_df.index.values))}\nt2 = time.time()\n\nprint(\"Usedtime = {} s\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:56.177Z"},"trusted":true,"_uuid":"ceb38b9173dbe1bff9dbc0abece214d2cadac218"},"cell_type":"code","source":"submission_file = '../submissions/HopeThisHelpsYou.csv'","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:56.553Z"},"_uuid":"770d7d596656f4f1ad17a6063ad662ac80e11b24","trusted":true},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv(submission_file)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-19T16:31:57.601Z"},"_uuid":"c89c406884ee54bee2c57aff51b116c157553ae9","trusted":true},"cell_type":"code","source":"t_finish = time.time()\nprint(\"Kernel run time = {} hours\".format((t_finish-t_start)/3600))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}