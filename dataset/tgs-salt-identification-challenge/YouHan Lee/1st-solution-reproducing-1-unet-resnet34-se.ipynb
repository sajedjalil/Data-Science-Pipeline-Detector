{"cells":[{"metadata":{"_uuid":"30f1af4e341d75878ebad7fb7b476344ac85a889"},"cell_type":"markdown","source":"# Background\n- This competition was very good for me to learn the object segmentation.\n- But, I didn't start early, so, I couldn't spend much time on this competition.\n- After finishing, I've decided to learn more here.\n- Many authors who earned prizes or get high positions shared their experiences, codes and tips.\n- I think the reproducing of them is very worthy to me.\n- At first, I will reprocude the first prize solution.\n- Actually, It would not be totally with the code. But, reaching the similar score will be good purpose.\n- First, Unet+Resnet34 + SE is selected.\n- During this challenge, the amazing qubvel library, https://github.com/qubvel/segmentation_models, is very helpful. Most of my scripts are from his repository. \n- I really appreciate him!\n- If you see my code and find some error, please talk to me. Your comment will help me and improve my ML skills!"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:42:34.318332Z","start_time":"2018-10-25T03:42:20.795918Z"},"trusted":true,"_uuid":"4c0be244051f8bed563d73cf95b3359781509d13"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport six\n\nfrom random import randint\n\n# import matplotlib.pyplot as plt\n# plt.style.use('seaborn-white')\n# import seaborn as sns\n# sns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nfrom keras.preprocessing.image import load_img\nfrom keras.layers.core import Lambda, RepeatVector, Reshape\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.regularizers import l2\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate,add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:42:34.78195Z","start_time":"2018-10-25T03:42:34.323946Z"},"trusted":true,"_uuid":"b8b76ff3ff7431797b4e2789854042824a2ae19a"},"cell_type":"code","source":"import tensorflow as tf\ntf.set_random_seed(1989)\nnp.random.seed(1989)\n# # Read dataset\nimport time\nt_start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"354d170d6d740db5fd97292c623a2f53860488ee"},"cell_type":"markdown","source":"# Check dataset"},{"metadata":{"_uuid":"92b5e94183c42590454440833105a642f1240984"},"cell_type":"markdown","source":"1.1 Params and Helpers"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:42:34.788856Z","start_time":"2018-10-25T03:42:34.785455Z"},"trusted":true,"_uuid":"d63f342ac9095eb092f938f3b478f141d8b8576a"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128 # you can change 128 to 256. In this kernel, I set 128 because of the memory.\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16afd3f7389e5147d2ab9753d9208f26903b94db"},"cell_type":"markdown","source":"## Loading of traning/testing ids and depths"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:42:35.283283Z","start_time":"2018-10-25T03:42:34.789935Z"},"trusted":true,"_uuid":"8edebfb71a94d29f5932d226771144ceeb207ff3"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8106b8e89bb55bd8cce854b55a7bbc604f131e7"},"cell_type":"markdown","source":"## Read images and masks"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:42:57.929897Z","start_time":"2018-10-25T03:42:35.284925Z"},"trusted":true,"_uuid":"fed33ea3c4a95498007c84b1fc9958f9a70d2f24"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in (train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:04.975703Z","start_time":"2018-10-25T03:42:57.934997Z"},"trusted":true,"_uuid":"2490a8da3332a91a2d7f6c19c9c428d3f4457dcb"},"cell_type":"code","source":"train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in (train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ba1e0eaaca4411ca39c50b5625bed27a7f683e2"},"cell_type":"markdown","source":"## Calculating the salt coverage and salt coverage classes"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:05.084631Z","start_time":"2018-10-25T03:43:04.978127Z"},"trusted":true,"_uuid":"98a493e0583eb8e26169f83428158c84712bd6b4"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0965256d6a6f01d09c9e28a55ddd5efdb07cfa9d"},"cell_type":"markdown","source":"# Dataset preparation"},{"metadata":{"_uuid":"1fb4e79c4f8ce032de12e67152cac99bd091573e"},"cell_type":"markdown","source":"## Create train/validation split stratified by salt coverage"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:24.208667Z","start_time":"2018-10-25T03:43:05.086486Z"},"trusted":true,"_uuid":"2f36c25bc7e5326ed411a66faa9a19ec66a652b5"},"cell_type":"code","source":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_valid, X_feat_train, X_feat_valid = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1989)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:46.732987Z","start_time":"2018-10-25T03:43:24.209953Z"},"trusted":true,"_uuid":"f74aea00e2f4d32628905924d8573b83af5cbfe8"},"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n\nx_train = np.repeat(x_train,3,axis=3)\nx_valid = np.repeat(x_valid,3,axis=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a96c69cbf7f71e0551e1ecd7f3ef7577180c5d59"},"cell_type":"markdown","source":"# Model development"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-24T22:39:03.037277Z","start_time":"2018-10-24T22:39:03.035437Z"},"_uuid":"ea3f13dbe3f6227d48ac617b798271c913f8118d"},"cell_type":"markdown","source":"## Build U-Net Model"},{"metadata":{"_uuid":"4e9540462b03c7bea1d757bde6ed47fdfa2a9e6b"},"cell_type":"markdown","source":"## Functions"},{"metadata":{"_uuid":"64d5b968c5a27065060d277faedfe250e8797c57"},"cell_type":"markdown","source":"### Block"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:46.92553Z","start_time":"2018-10-25T03:43:46.787983Z"},"trusted":true,"_uuid":"c3721a3c55c578bede7c38ec2a7b5c8e00ad36e0"},"cell_type":"code","source":"from keras.layers import Conv2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Add\nfrom keras.layers import ZeroPadding2D\n\ndef get_conv_params(**params):\n    default_conv_params = {\n        'kernel_initializer': 'glorot_uniform',\n        'use_bias': False,\n        'padding': 'valid',\n    }\n    default_conv_params.update(params)\n    return default_conv_params\n\n\ndef get_bn_params(**params):\n    default_bn_params = {\n        'axis': 3,\n        'momentum': 0.99,\n        'epsilon': 2e-5,\n        'center': True,\n        'scale': True,\n    }\n    default_bn_params.update(params)\n    return default_bn_params\n\ndef handle_block_names(stage, block):\n    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n    conv_name = name_base + 'conv'\n    bn_name = name_base + 'bn'\n    relu_name = name_base + 'relu'\n    sc_name = name_base + 'sc'\n    return conv_name, bn_name, relu_name, sc_name\n\n\ndef basic_identity_block(filters, stage, block):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer\n\n\ndef basic_conv_block(filters, stage, block, strides=(2, 2)):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        shortcut = x\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef conv_block(filters, stage, block, strides=(2, 2)):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        shortcut = x\n        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '3')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n\n        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n        x = Add()([x, shortcut])\n        return x\n\n    return layer\n\n\ndef identity_block(filters, stage, block):\n    \"\"\"The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n\n    def layer(input_tensor):\n        conv_params = get_conv_params()\n        bn_params = get_bn_params()\n        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n\n        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n        x = Activation('relu', name=relu_name + '1')(x)\n        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '2')(x)\n        x = ZeroPadding2D(padding=(1, 1))(x)\n        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n\n        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n        x = Activation('relu', name=relu_name + '3')(x)\n        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n\n        x = Add()([x, input_tensor])\n        return x\n\n    return layer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c416136252724f8e21452376ad056f10b7e3bb1"},"cell_type":"markdown","source":"### Builders"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.124293Z","start_time":"2018-10-25T03:43:46.932516Z"},"trusted":true,"_uuid":"1beef6ee50160494e5d5911894c4322c527f8760"},"cell_type":"code","source":"import keras.backend as K\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.engine import get_source_inputs\n\nimport keras\nfrom distutils.version import StrictVersion\n\nif StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n    from keras.applications.imagenet_utils import _obtain_input_shape\nelse:\n    from keras_applications.imagenet_utils import _obtain_input_shape\n\ndef build_resnet(\n     repetitions=(2, 2, 2, 2),\n     include_top=True,\n     input_tensor=None,\n     input_shape=None,\n     classes=1000,\n     block_type='usual'):\n    \n    \"\"\"\n    TODO\n    \"\"\"\n    \n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=197,\n                                      data_format='channels_last',\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape, name='data')\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    \n    # get parameters for model layers\n    no_scale_bn_params = get_bn_params(scale=False)\n    bn_params = get_bn_params()\n    conv_params = get_conv_params()\n    init_filters = 64\n\n    if block_type == 'basic':\n        conv_block = basic_conv_block\n        identity_block = basic_identity_block\n    else:\n        conv_block = usual_conv_block\n        identity_block = usual_identity_block\n    \n    # resnet bottom\n    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n    x = ZeroPadding2D(padding=(3, 3))(x)\n    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n    x = BatchNormalization(name='bn0', **bn_params)(x)\n    x = Activation('relu', name='relu0')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n    \n    # resnet body\n    for stage, rep in enumerate(repetitions):\n        for block in range(rep):\n            \n            filters = init_filters * (2**stage)\n            \n            # first block of first stage without strides because we have maxpooling before\n            if block == 0 and stage == 0:\n                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n                \n            elif block == 0:\n                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n                \n            else:\n                x = identity_block(filters, stage, block)(x)\n                \n    x = BatchNormalization(name='bn1', **bn_params)(x)\n    x = Activation('relu', name='relu1')(x)\n\n    # resnet top\n    if include_top:\n        x = GlobalAveragePooling2D(name='pool1')(x)\n        x = Dense(classes, name='fc1')(x)\n        x = Activation('softmax', name='softmax')(x)\n\n    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n        \n    # Create model.\n    model = Model(inputs, x)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"113f16ef7b127ec7c3e330c1007504f024472bea"},"cell_type":"markdown","source":"### Squeeze and exication"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.249042Z","start_time":"2018-10-25T03:43:47.130587Z"},"trusted":true,"_uuid":"c89ae7967bcd84f973e657cb125dbdede8501527"},"cell_type":"code","source":"def squeeze_excite_block(input, ratio=16):\n    init = input\n    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n    filters = init._keras_shape[channel_axis]\n    se_shape = (1, 1, filters)\n\n    se = GlobalAveragePooling2D()(init)\n    se = Reshape(se_shape)(se)\n    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n\n    if K.image_data_format() == 'channels_first':\n        se = Permute((3, 1, 2))(se)\n\n    x = multiply([init, se])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9ca56820fcba51e1665b56646e5fa8b2cec54c8"},"cell_type":"markdown","source":"### models"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.4576Z","start_time":"2018-10-25T03:43:47.254726Z"},"trusted":true,"_uuid":"0ba6e32995d631303a9f3dec2015def8be28d327"},"cell_type":"code","source":"weights_collection = [\n\n    # ResNet18\n    {\n        'model': 'resnet18',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000.h5',\n        'name': 'resnet18_imagenet_1000.h5',\n        'md5': '64da73012bb70e16c901316c201d9803',\n    },\n\n    {\n        'model': 'resnet18',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet18_imagenet_1000_no_top.h5',\n        'name': 'resnet18_imagenet_1000.h5',\n        'md5': '318e3ac0cd98d51e917526c9f62f0b50',\n    },\n\n    # ResNet34\n    {\n        'model': 'resnet34',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n        'name': 'resnet34_imagenet_1000.h5',\n        'md5': '2ac8277412f65e5d047f255bcbd10383',\n    },\n\n    {\n        'model': 'resnet34',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n        'name': 'resnet34_imagenet_1000_no_top.h5',\n        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n    },\n\n    # ResNet50\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000.h5',\n        'name': 'resnet50_imagenet_1000.h5',\n        'md5': 'd0feba4fc650e68ac8c19166ee1ba87f',\n    },\n\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_1000_no_top.h5',\n        'name': 'resnet50_imagenet_1000_no_top.h5',\n        'md5': 'db3b217156506944570ac220086f09b6',\n    },\n\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet11k-places365ch',\n        'classes': 11586,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_places365_11586.h5',\n        'name': 'resnet50_places365_11586.h5',\n        'md5': 'bb8963db145bc9906452b3d9c9917275',\n    },\n\n    {\n        'model': 'resnet50',\n        'dataset': 'imagenet11k-places365ch',\n        'classes': 11586,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet50_imagenet_11586_no_top.h5',\n        'name': 'resnet50_imagenet_11586_no_top.h5',\n        'md5': 'd8bf4e7ea082d9d43e37644da217324a',\n    },\n\n    # ResNet101\n    {\n        'model': 'resnet101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet101_imagenet_1000.h5',\n        'name': 'resnet101_imagenet_1000.h5',\n        'md5': '9489ed2d5d0037538134c880167622ad',\n    },\n\n    {\n        'model': 'resnet101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet101_imagenet_1000_no_top.h5',\n        'name': 'resnet101_imagenet_1000_no_top.h5',\n        'md5': '1016e7663980d5597a4e224d915c342d',\n    },\n\n\n    # ResNet152\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet_1000.h5',\n        'name': 'resnet152_imagenet_1000.h5',\n        'md5': '1efffbcc0708fb0d46a9d096ae14f905',\n    },\n\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet_1000_no_top.h5',\n        'name': 'resnet152_imagenet_1000_no_top.h5',\n        'md5': '5867b94098df4640918941115db93734',\n    },\n\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet11k',\n        'classes': 11221,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet11k_11221.h5',\n        'name': 'resnet152_imagenet11k_11221.h5',\n        'md5': '24791790f6ef32f274430ce4a2ffee5d',\n    },\n\n    {\n        'model': 'resnet152',\n        'dataset': 'imagenet11k',\n        'classes': 11221,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet152_imagenet11k_11221_no_top.h5',\n        'name': 'resnet152_imagenet11k_11221_no_top.h5',\n        'md5': '25ab66dec217cb774a27d0f3659cafb3',\n    },\n\n\n    # ResNeXt50\n    {\n        'model': 'resnext50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext50_imagenet_1000.h5',\n        'name': 'resnext50_imagenet_1000.h5',\n        'md5': '7c5c40381efb044a8dea5287ab2c83db',\n    },\n\n    {\n        'model': 'resnext50',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext50_imagenet_1000_no_top.h5',\n        'name': 'resnext50_imagenet_1000_no_top.h5',\n        'md5': '7ade5c8aac9194af79b1724229bdaa50',\n    },\n\n\n    # ResNeXt101\n    {\n        'model': 'resnext101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': True,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext101_imagenet_1000.h5',\n        'name': 'resnext101_imagenet_1000.h5',\n        'md5': '432536e85ee811568a0851c328182735',\n    },\n\n    {\n        'model': 'resnext101',\n        'dataset': 'imagenet',\n        'classes': 1000,\n        'include_top': False,\n        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnext101_imagenet_1000_no_top.h5',\n        'name': 'resnext101_imagenet_1000_no_top.h5',\n        'md5': '91fe0126320e49f6ee607a0719828c7e',\n    },\n\n]\n\nfrom keras.utils import get_file\n\n\ndef find_weights(weights_collection, model_name, dataset, include_top):\n    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n    w = list(filter(lambda x: x['dataset'] == dataset, w))\n    w = list(filter(lambda x: x['include_top'] == include_top, w))\n    return w\n\ndef load_model_weights(weights_collection, model, dataset, classes, include_top):\n    weights = find_weights(weights_collection, model.name, dataset, include_top)\n\n    if weights:\n        weights = weights[0]\n\n        if include_top and weights['classes'] != classes:\n            raise ValueError('If using `weights` and `include_top`'\n                             ' as true, `classes` should be {}'.format(weights['classes']))\n\n        weights_path = get_file(weights['name'],\n                                weights['url'],\n                                cache_subdir='models',\n                                md5_hash=weights['md5'])\n\n        model.load_weights(weights_path)\n\n    else:\n        raise ValueError('There is no weights for such configuration: ' +\n                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n                         'classes = {}, include_top = {}.'.format(classes, include_top))\n\ndef ResNet34(input_shape, input_tensor=None, weights=None, classes=1000, include_top=True):\n    model = build_resnet(input_tensor=input_tensor,\n                         input_shape=input_shape,\n                         repetitions=(3, 4, 6, 3),\n                         classes=classes,\n                         include_top=include_top,\n                         block_type='basic')\n    model.name = 'resnet34'\n\n    if weights:\n        load_model_weights(weights_collection, model, weights, classes, include_top)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62f78f754ecf88b4f02cb2125392797249ee650a"},"cell_type":"markdown","source":"### Unet"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.591281Z","start_time":"2018-10-25T03:43:47.459267Z"},"trusted":true,"_uuid":"f707fcafcc6079c6c70916db75c8f9c50f4e78f7"},"cell_type":"code","source":"def sse_block(prevlayer, prefix):\n    conv = Conv2D(1, (1, 1), padding=\"same\", kernel_initializer=\"he_normal\", activation='sigmoid', strides=(1, 1),\n                  name=prefix + \"_conv\")(prevlayer)\n    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n    return conv\n\n\nDEFAULT_SKIP_CONNECTIONS = {\n    'vgg16':            ('block5_conv3', 'block4_conv3', 'block3_conv3', 'block2_conv2', 'block1_conv2'),\n    'vgg19':            ('block5_conv4', 'block4_conv4', 'block3_conv4', 'block2_conv2', 'block1_conv2'),\n    'resnet18':         ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'), # check 'bn_data'\n    'resnet34':         ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n    'resnet50':         ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n    'resnet101':        ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n    'resnet152':        ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n    'resnext50':        ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n    'resnext101':       ('stage4_unit1_relu1', 'stage3_unit1_relu1', 'stage2_unit1_relu1', 'relu0'),\n    'inceptionv3':          (228, 86, 16, 9),\n    'inceptionresnetv2':    (594, 260, 16, 9),\n    'densenet121':          (311, 139, 51, 4),\n    'densenet169':          (367, 139, 51, 4),\n    'densenet201':          (479, 139, 51, 4),\n}\n\n\n# from .classification_models.classification_models import ResNet18, ResNet34, ResNet50, ResNet101, ResNet152\n# from .classification_models.classification_models import ResNeXt50, ResNeXt101\n\n# from .inception_resnet_v2 import InceptionResNetV2\n# from .inception_v3 import InceptionV3\n\n# from keras.applications import DenseNet121, DenseNet169, DenseNet201\n# from keras.applications import VGG16\n# from keras.applications import VGG19\n\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Conv2D, Multiply\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\n\nbackbones = {\n#     \"vgg16\": VGG16,\n#     \"vgg19\": VGG19,\n#     \"resnet18\": ResNet18,\n    \"resnet34\": ResNet34,\n#     \"resnet50\": ResNet50,\n#     \"resnet101\": ResNet101,\n#     \"resnet152\": ResNet152,\n#     \"resnext50\": ResNeXt50,\n#     \"resnext101\": ResNeXt101,\n#     \"inceptionresnetv2\": InceptionResNetV2,\n#     \"inceptionv3\": InceptionV3,\n#     \"densenet121\": DenseNet121,\n#     \"densenet169\": DenseNet169,\n#     \"densenet201\": DenseNet201,\n\n}\n\ndef get_backbone(name, *args, **kwargs):\n    return backbones[name](*args, **kwargs)\n\n\n\n\ndef get_layer_number(model, layer_name):\n    \"\"\"\n    Help find layer in Keras model by name\n    Args:\n        model: Keras `Model`\n        layer_name: str, name of layer\n    Returns:\n        index of layer\n    Raises:\n        ValueError: if model does not contains layer with such name\n    \"\"\"\n    for i, l in enumerate(model.layers):\n        if l.name == layer_name:\n            return i\n    raise ValueError('No layer with name {} in  model {}.'.format(layer_name, model.name))\n\n\ndef extract_outputs(model, layers, include_top=False):\n    \"\"\"\n    Help extract intermediate layer outputs from model\n    Args:\n        model: Keras `Model`\n        layer: list of integers/str, list of layers indexes or names to extract output\n        include_top: bool, include final model layer output\n    Returns:\n        list of tensors (outputs)\n    \"\"\"\n    layers_indexes = ([get_layer_number(model, l) if isinstance(l, str) else l\n                      for l in layers])\n    outputs = [model.layers[i].output for i in layers_indexes]\n\n    if include_top:\n        outputs.insert(0, model.output)\n\n    return outputs\n\n\ndef reverse(l):\n    \"\"\"Reverse list\"\"\"\n    return list(reversed(l))\n\n\n# decorator for models aliases, to add doc string\ndef add_docstring(doc_string=None):\n    def decorator(fn):\n        if fn.__doc__:\n            fn.__doc__ += doc_string\n        else:\n            fn.__doc__ = doc_string\n\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            return fn(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef recompile(model):\n    model.compile(model.optimizer, model.loss, model.metrics)    \n\n    \ndef freeze_model(model):\n    for layer in model.layers:\n        layer.trainable = False\n    return\n\n\ndef set_trainable(model):\n    for layer in model.layers:\n        layer.trainable = True\n    recompile(model)\n\n\ndef to_tuple(x):\n    if isinstance(x, tuple):\n        if len(x) == 2:\n            return x\n    elif np.isscalar(x):\n        return (x, x)\n\n    raise ValueError('Value should be tuple of length 2 or int value, got \"{}\"'.format(x))\n\n\n# def handle_block_names(stage):\n#     conv_name = 'decoder_stage{}_conv'.format(stage)\n#     bn_name = 'decoder_stage{}_bn'.format(stage)\n#     relu_name = 'decoder_stage{}_relu'.format(stage)\n#     up_name = 'decoder_stage{}_upsample'.format(stage)\n#     return conv_name, bn_name, relu_name, up_name\n\n\ndef ConvRelu(filters, kernel_size, use_batchnorm=False, conv_name='conv', bn_name='bn', relu_name='relu'):\n    def layer(x):\n        x = Conv2D(filters, kernel_size, padding=\"same\", name=conv_name, use_bias=not(use_batchnorm))(x)\n        if use_batchnorm:\n            x = BatchNormalization(name=bn_name)(x)\n        x = Activation('relu', name=relu_name)(x)\n        return x\n    return layer\n\n\ndef Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                     use_batchnorm=False, skip=None):\n    def handle_block_names(stage):\n        conv_name = 'decoder_stage{}_conv'.format(stage)\n        bn_name = 'decoder_stage{}_bn'.format(stage)\n        relu_name = 'decoder_stage{}_relu'.format(stage)\n        up_name = 'decoder_stage{}_upsample'.format(stage)\n        return conv_name, bn_name, relu_name, up_name\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n\n        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + '1', bn_name=bn_name + '1', relu_name=relu_name + '1')(x)\n\n        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + '2', bn_name=bn_name + '2', relu_name=relu_name + '2')(x)\n\n        return x\n    return layer\n\n\ndef Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                      transpose_kernel_size=(4,4), use_batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n\n        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n                            padding='same', name=up_name, use_bias=not(use_batchnorm))(input_tensor)\n        if use_batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n                     conv_name=conv_name + '2', bn_name=bn_name + '2', relu_name=relu_name + '2')(x)\n\n        return x\n    return layer\n\n\n\n\ndef Unet(backbone_name='vgg16',\n         input_shape=(None, None, 3),\n         input_tensor=None,\n         encoder_weights='imagenet',\n         freeze_encoder=False,\n         skip_connections='default',\n         decoder_block_type='upsampling',\n         decoder_filters=(256,128,64,32,16),\n         decoder_use_batchnorm=True,\n         n_upsample_blocks=5,\n         upsample_rates=(2,2,2,2,2),\n         classes=1,\n         activation='sigmoid'):\n    \"\"\"\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n    Returns:\n        keras.models.Model instance\n    \"\"\"\n\n\n\n    backbone = get_backbone(backbone_name,\n                            input_shape=input_shape,\n                            input_tensor=input_tensor,\n                            weights=encoder_weights,\n                            include_top=False)\n\n    if skip_connections == 'default':\n        skip_connections = DEFAULT_SKIP_CONNECTIONS[backbone_name]\n\n    model = build_unet(backbone,\n                       classes,\n                       skip_connections,\n                       decoder_filters=decoder_filters,\n                       block_type=decoder_block_type,\n                       activation=activation,\n                       n_upsample_blocks=n_upsample_blocks,\n                       upsample_rates=upsample_rates,\n                       use_batchnorm=decoder_use_batchnorm)\n\n    # lock encoder weights for fine-tuning\n    if freeze_encoder:\n        freeze_model(backbone)\n\n    model.name = 'u-{}'.format(backbone_name)\n\n    return model\n\n\n\ndef build_unet(backbone, classes, skip_connection_layers,\n               decoder_filters=(256,128,64,32,16),\n               upsample_rates=(2,2,2,2,2),\n               n_upsample_blocks=5,\n               block_type='upsampling',\n               activation='sigmoid',\n               use_batchnorm=True):\n\n    input = backbone.input\n    x = backbone.output\n\n    if block_type == 'transpose':\n        up_block = Transpose2D_block\n    else:\n        up_block = Upsample2D_block\n\n    # convert layer names to indices\n    skip_connection_idx = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n                               for l in skip_connection_layers])\n\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            skip_connection = backbone.layers[skip_connection_idx[i]].output\n            skip_connection = sse_block(skip_connection, 'sse_encoder_{}'.format(i))\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = up_block(decoder_filters[i], i, upsample_rate=upsample_rate,\n                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n        x = sse_block(x, 'sse_decoder_{}'.format(i))\n        \n    \n\n    x = Dropout(0.25)(x)\n    x = Conv2D(classes, (1,1), padding='same', name='final_conv')(x)\n    x = Activation(activation, name=activation)(x)\n\n    model = Model(input, x)\n\n    return model\n\nbgr_transpose = lambda x: x[..., ::-1]\n\nmodels_preprocessing = {\n    'resnet18': bgr_transpose,\n    'resnet34': bgr_transpose,\n}\n\n\ndef get_preprocessing(backbone):\n    return models_preprocessing[backbone]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cea2eb21a23a39b897802f7a33f3fae9a7da7f46"},"cell_type":"markdown","source":"## Define loss functions"},{"metadata":{"_uuid":"3cd6818b4c79c85ba8b5145781bfc1ce9b98b85a"},"cell_type":"markdown","source":"### Dice loss"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.709815Z","start_time":"2018-10-25T03:43:47.596465Z"},"trusted":true,"_uuid":"33fe5d4a7fbb8ca8bf55c32953f219073ae3027e"},"cell_type":"code","source":"from keras.losses import binary_crossentropy\nfrom keras import backend as K\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ddc41fc7f5a62bafb816eeeefdf518c876032de"},"cell_type":"markdown","source":"### Lovasz loss"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.848632Z","start_time":"2018-10-25T03:43:47.715858Z"},"trusted":true,"_uuid":"5de3a75cb68c19a054dc2f84784ee1c3d4be2d81"},"cell_type":"code","source":"# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\ndef get_iou_vector(A, B):\n    # Numpy version    \n    batch_size = A.shape[0]\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n        \n        # deal with empty mask first\n        if true == 0:\n            metric += (pred == 0)\n            continue\n        \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection / union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n        \n        metric += iou\n        \n    # teake the average over all images in batch\n    metric /= batch_size\n    return metric\n\n\ndef my_iou_metric(label, pred):\n    # Tensorflow version\n    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)\n\ndef my_iou_metric_2(label, pred):\n    return tf.py_func(get_iou_vector, [label, pred >0], tf.float64)\n\n# code download from: https://github.com/bermanmaxim/LovaszSoftmax\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n# --------------------------- BINARY LOSSES ---------------------------\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.elu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\ndef lovasz_loss(y_true, y_pred):\n    y_true, y_pred = K.cast(K.squeeze(y_true, -1), 'int32'), K.cast(K.squeeze(y_pred, -1), 'float32')\n    #logits = K.log(y_pred / (1. - y_pred))\n    logits = y_pred #Jiaxin\n    loss = lovasz_hinge(logits, y_true, per_image = True, ignore = None)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c913234ce0ef8495ec624db48349eb1204debe9c"},"cell_type":"markdown","source":"# Train model"},{"metadata":{"_uuid":"115be8fb6039830e961358775f207a7504161286"},"cell_type":"markdown","source":"## Initilize parameters"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:47.966467Z","start_time":"2018-10-25T03:43:47.854722Z"},"trusted":true,"_uuid":"716853c076b479c5cdde863476866a8de67c6a61"},"cell_type":"code","source":"version = 1\nbasic_name = '../working/Unet_resnet_se'\nsave_model_name = basic_name + '.model'\nsubmission_file = basic_name + '.csv'","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:48.066871Z","start_time":"2018-10-25T03:43:47.97145Z"},"trusted":true,"_uuid":"85046a96fa4298b8fec074634e86ef3762dbe3b1"},"cell_type":"code","source":"preprocessing_fn = get_preprocessing('resnet34')\nx = preprocessing_fn(x_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8ca54346741d4bfea1ee939ec7d16fe441fb9dc"},"cell_type":"markdown","source":"## Build model and compile - 1st Dice loss"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:48.185439Z","start_time":"2018-10-25T03:43:48.072753Z"},"trusted":true,"_uuid":"cf7c552b91c280ea92d7d9f9c849cf4659d2e8b1"},"cell_type":"code","source":"from keras.optimizers import RMSprop","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:43:48.961996Z","start_time":"2018-10-25T03:43:48.191912Z"},"trusted":true,"_uuid":"0453ad4ff6262d7e87536781850d5294e2142cc5"},"cell_type":"code","source":"c = RMSprop(lr=0.0001)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:44:16.990996Z","start_time":"2018-10-25T03:43:48.965657Z"},"scrolled":false,"trusted":true,"_uuid":"ded639164b90689fc1c72fed0951db3ff0f34c7a"},"cell_type":"code","source":"model = Unet(backbone_name='resnet34', encoder_weights='imagenet')\nmodel.compile(loss=bce_dice_loss, optimizer=c, metrics=[my_iou_metric])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:44:17.023779Z","start_time":"2018-10-25T03:44:17.002902Z"},"trusted":true,"_uuid":"c1b5b254e98de28d520764d608facc69373b09ec"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T02:45:28.255354Z","start_time":"2018-10-25T02:35:43.142421Z"},"trusted":true,"_uuid":"4079d7804432ac3e16472962ecc5d44a14b79a71"},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=10, monitor='val_my_iou_metric',verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name, monitor='val_my_iou_metric', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=4,monitor='val_my_iou_metric', min_lr=0.00001, verbose=1)\n\nepochs = 1\nbatch_size = 24\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n                    verbose=1,\n                    shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"befe981ace589487f20eafbb18d1ba16b83a6828"},"cell_type":"markdown","source":"## 2nd learning lovasz"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:44:22.816886Z","start_time":"2018-10-25T03:44:20.953251Z"},"trusted":true,"_uuid":"c1035ab8ec2e5edb35003768d7e2b8d305531c9e"},"cell_type":"code","source":"input_x = model.layers[0].input\n\noutput_layer = model.layers[-1].input\nmodel = Model(input_x, output_layer)\nc = RMSprop(lr = 0.0005)\n\n# lovasz_loss need input range (-+), so cancel the last \"sigmoid\" activation  \n# Then the default threshod for pixel prediction is 0 instead of 0.5, as in my_iou_metric_2.\nmodel.compile(loss=lovasz_loss, optimizer=c, metrics=[my_iou_metric_2])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:44:22.853381Z","start_time":"2018-10-25T03:44:22.818163Z"},"trusted":true,"_uuid":"5bde3d55afdb09249b52b3160588c04c5a230246"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:44:22.974662Z","start_time":"2018-10-25T03:44:22.872092Z"},"trusted":true,"_uuid":"c81eb282c87498d0c2e45491a408df8364e95b0f"},"cell_type":"code","source":"save_model_name","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T03:44:23.08737Z","start_time":"2018-10-25T03:44:22.980091Z"},"trusted":true,"_uuid":"42bdfa17f5dc1f58e076741f598bb148a979346e"},"cell_type":"code","source":"save_model_name_lovasz  = '../working/Unet_resnet_se_lovasz.model'","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"start_time":"2018-10-25T05:23:36.025Z"},"trusted":true,"_uuid":"70b0b868f791795e1bd877469d688726df6436e3"},"cell_type":"code","source":"early_stopping = EarlyStopping(patience=30, monitor='val_my_iou_metric_2',verbose=1)\nmodel_checkpoint = ModelCheckpoint(save_model_name_lovasz, monitor='val_my_iou_metric_2', save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=4,monitor='val_my_iou_metric_2', min_lr=0.00001, verbose=1)\n\nepochs = 1\nbatch_size = 24\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n                    verbose=1,\n                    shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9652ce5f4ae1bc3dda988a00b00b3a3b03321c20"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check"},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:12:07.549195Z","start_time":"2018-10-25T05:11:57.857646Z"},"trusted":true,"_uuid":"cfb3c74f68a5b6d3cc56319765a38ecb208b4238"},"cell_type":"code","source":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:12:09.674446Z","start_time":"2018-10-25T05:12:09.647181Z"},"trusted":true,"_uuid":"5cd1274a721883c15589e880a9b41e2e1a4897fa"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:12:49.980175Z","start_time":"2018-10-25T05:12:10.779325Z"},"trusted":true,"_uuid":"1dd0d4eaf5a33e32d2faa74f8ff0e21ec417728f"},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:12:52.428512Z","start_time":"2018-10-25T05:12:52.410517Z"},"trusted":true,"_uuid":"1908994924cca48dcd393c430b5550c11dea55c3"},"cell_type":"code","source":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:12:55.551592Z","start_time":"2018-10-25T05:12:54.002602Z"},"trusted":true,"_uuid":"a22e761fb46feae9e3667891ea75a5fb0da4ed18"},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:12:56.057952Z","start_time":"2018-10-25T05:12:55.557343Z"},"trusted":true,"_uuid":"1757cdbb6b85f03548804660532045e9c51b7992"},"cell_type":"code","source":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T02:23:39.971045Z","start_time":"2018-10-25T02:23:39.968838Z"},"trusted":true,"_uuid":"41335a79408ebbcf87d1e550cce5164d76bf4e98"},"cell_type":"code","source":"# x_test = [(np.array(load_img(\"../input/test/{}.png\".format(idx), grayscale = True))) / 255 for idx in (test_df.index)]\n# x_test = [resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True) for img in (x_test)]\n# x_test = np.array(x_test).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T02:23:40.193475Z","start_time":"2018-10-25T02:23:39.972615Z"},"trusted":true,"_uuid":"da125511bc8a1d0855f796392cfd3a56834c8106"},"cell_type":"code","source":"# del x_train, x_valid","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T02:23:40.277743Z","start_time":"2018-10-25T02:23:40.199695Z"},"trusted":true,"_uuid":"57a3a1c87949e2d18711f44c0776c2f8ae9c3238"},"cell_type":"code","source":"# np.save('../input/x_test.npy', x_test)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:13:05.56252Z","start_time":"2018-10-25T05:13:05.557349Z"},"trusted":true,"_uuid":"fa2f9dfe585fab9d7a2f140bc0052f27f655c842"},"cell_type":"code","source":"def predict_result(model,x_test,img_size_target,batch_size): # predict both orginal and reflect x\n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test1 = model.predict([x_test],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict([x_test_reflect],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    preds_avg = (preds_test1 +preds_test2)/2\n    return preds_avg","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:20:02.598686Z","start_time":"2018-10-25T05:13:06.89696Z"},"trusted":true,"_uuid":"252446afa8eaf4043f37d08d3e4233e237d5c7ac"},"cell_type":"code","source":"batch_size = 500\npreds_test = []\ni = 0\nwhile i < test_df.shape[0]:\n    index_val = test_df.index[i:i+batch_size]\n#     depth_val = test_df.z[i:i+batch_size]\n    x_test = np.array([upsample(np.array(load_img(\"../input/test/{}.png\".format(idx), grayscale=True))) / 255 for idx in (index_val)]).reshape(-1, img_size_target, img_size_target, 1)\n    x_test = np.repeat(x_test,3,axis=3)\n    preds_test_temp = predict_result(model,x_test,img_size_target,32)\n    if i==0:\n        preds_test = preds_test_temp\n    else:\n        preds_test = np.concatenate([preds_test,preds_test_temp],axis=0)\n    if i%2000==0:\n        print('Images Processed:',i)\n    i += batch_size    \nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T02:30:53.328832Z","start_time":"2018-10-25T02:30:47.054535Z"},"trusted":true,"_uuid":"e1bd2e06accb80d280bdd8a8fe8d8067cb09841b"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(index_val[:max_images]):\n    img = x_test[i]\n    pred = preds_test[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:20:10.130508Z","start_time":"2018-10-25T05:20:03.761501Z"},"trusted":true,"_uuid":"5543c75657b427d1e97031000d99afd23a22184a"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(index_val[:max_images]):\n    img = x_test[i]\n    pred = preds_test[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:21:24.862937Z","start_time":"2018-10-25T05:21:24.857175Z"},"trusted":true,"_uuid":"4dd60abfa8c5a9e7b676053c155942783c0527dd"},"cell_type":"code","source":"\"\"\"\nused for converting the decoded image to rle mask\nFast compared to previous one\n\"\"\"\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:21:42.994454Z","start_time":"2018-10-25T05:21:25.314837Z"},"trusted":true,"_uuid":"a2ffadb446cd40d50cf2af07bff056b6dc31cf04"},"cell_type":"code","source":"import time\nt1 = time.time()\npred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm(test_df.index.values))}\nt2 = time.time()\n\nprint(\"Usedtime = {} s\".format(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-10-25T05:21:43.337037Z","start_time":"2018-10-25T05:21:42.995668Z"},"trusted":true,"_uuid":"a52f7cdd8a4340150027090475b26171caf58230"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission_{}.csv'.format(basic_name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"493cb94b2cc4f40b93d94119c438fab7c990489d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b4ac1c15478fabf1be4aa90e0a0d9acd43eeed8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5898e4d4eba6b05c85e5c9498e1fb5f644541fe0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23084dcb5bbe80f91aa66d2f7bce937dc2115a78"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c85da09a52a8f6cf1503a7c386e34e6e97516750"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"155cf114eb20ca171e5886e8767f13b3936551d4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}