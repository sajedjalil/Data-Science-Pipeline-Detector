{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport math\nimport cv2\nimport PIL\nimport re\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n#from sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\n%matplotlib inline\n#from keras.preprocessing import image\nimport glob\nimport tensorflow.keras.applications.densenet as dense\nfrom kaggle_datasets import KaggleDatasets\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:20.75007Z","iopub.execute_input":"2022-01-18T04:13:20.750457Z","iopub.status.idle":"2022-01-18T04:13:27.242195Z","shell.execute_reply.started":"2022-01-18T04:13:20.75034Z","shell.execute_reply":"2022-01-18T04:13:27.241345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.243861Z","iopub.execute_input":"2022-01-18T04:13:27.244149Z","iopub.status.idle":"2022-01-18T04:13:27.252999Z","shell.execute_reply.started":"2022-01-18T04:13:27.244112Z","shell.execute_reply":"2022-01-18T04:13:27.251993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/train.csv')\ntest = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv')\n\nprint('Train: ', train.shape)\nprint(\"Test:\", test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.254376Z","iopub.execute_input":"2022-01-18T04:13:27.254909Z","iopub.status.idle":"2022-01-18T04:13:27.385969Z","shell.execute_reply.started":"2022-01-18T04:13:27.25486Z","shell.execute_reply":"2022-01-18T04:13:27.38501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.387906Z","iopub.execute_input":"2022-01-18T04:13:27.388463Z","iopub.status.idle":"2022-01-18T04:13:27.409088Z","shell.execute_reply.started":"2022-01-18T04:13:27.388416Z","shell.execute_reply":"2022-01-18T04:13:27.408198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.410168Z","iopub.execute_input":"2022-01-18T04:13:27.410398Z","iopub.status.idle":"2022-01-18T04:13:27.456387Z","shell.execute_reply.started":"2022-01-18T04:13:27.410373Z","shell.execute_reply":"2022-01-18T04:13:27.455438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.457687Z","iopub.execute_input":"2022-01-18T04:13:27.459459Z","iopub.status.idle":"2022-01-18T04:13:27.473559Z","shell.execute_reply.started":"2022-01-18T04:13:27.459413Z","shell.execute_reply":"2022-01-18T04:13:27.47248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.475462Z","iopub.execute_input":"2022-01-18T04:13:27.475767Z","iopub.status.idle":"2022-01-18T04:13:27.497811Z","shell.execute_reply.started":"2022-01-18T04:13:27.475729Z","shell.execute_reply":"2022-01-18T04:13:27.496768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data EDA...\n\n","metadata":{}},{"cell_type":"code","source":"train['benign_malignant'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.49953Z","iopub.execute_input":"2022-01-18T04:13:27.500102Z","iopub.status.idle":"2022-01-18T04:13:27.516282Z","shell.execute_reply.started":"2022-01-18T04:13:27.50006Z","shell.execute_reply":"2022-01-18T04:13:27.515503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train['benign_malignant'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.517577Z","iopub.execute_input":"2022-01-18T04:13:27.517819Z","iopub.status.idle":"2022-01-18T04:13:27.780891Z","shell.execute_reply.started":"2022-01-18T04:13:27.517793Z","shell.execute_reply":"2022-01-18T04:13:27.779986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sex'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.784507Z","iopub.execute_input":"2022-01-18T04:13:27.784794Z","iopub.status.idle":"2022-01-18T04:13:27.796856Z","shell.execute_reply.started":"2022-01-18T04:13:27.784759Z","shell.execute_reply":"2022-01-18T04:13:27.79606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].groupby(train['sex']).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.797789Z","iopub.execute_input":"2022-01-18T04:13:27.797995Z","iopub.status.idle":"2022-01-18T04:13:27.818571Z","shell.execute_reply.started":"2022-01-18T04:13:27.797971Z","shell.execute_reply":"2022-01-18T04:13:27.817745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train['sex'], hue=train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:27.820068Z","iopub.execute_input":"2022-01-18T04:13:27.820329Z","iopub.status.idle":"2022-01-18T04:13:28.096781Z","shell.execute_reply.started":"2022-01-18T04:13:27.820296Z","shell.execute_reply":"2022-01-18T04:13:28.096184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].groupby(train['age_approx']).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:28.097713Z","iopub.execute_input":"2022-01-18T04:13:28.098288Z","iopub.status.idle":"2022-01-18T04:13:28.109711Z","shell.execute_reply.started":"2022-01-18T04:13:28.098239Z","shell.execute_reply":"2022-01-18T04:13:28.108804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(train['age_approx'], hue=train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:28.110919Z","iopub.execute_input":"2022-01-18T04:13:28.111153Z","iopub.status.idle":"2022-01-18T04:13:28.534075Z","shell.execute_reply.started":"2022-01-18T04:13:28.111125Z","shell.execute_reply":"2022-01-18T04:13:28.533217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['anatom_site_general_challenge'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:28.537109Z","iopub.execute_input":"2022-01-18T04:13:28.537362Z","iopub.status.idle":"2022-01-18T04:13:28.549864Z","shell.execute_reply.started":"2022-01-18T04:13:28.537332Z","shell.execute_reply":"2022-01-18T04:13:28.548785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].groupby(train['anatom_site_general_challenge']).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:28.551376Z","iopub.execute_input":"2022-01-18T04:13:28.551769Z","iopub.status.idle":"2022-01-18T04:13:28.567377Z","shell.execute_reply.started":"2022-01-18T04:13:28.551725Z","shell.execute_reply":"2022-01-18T04:13:28.566556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(train['anatom_site_general_challenge'], hue=train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:28.568355Z","iopub.execute_input":"2022-01-18T04:13:28.568925Z","iopub.status.idle":"2022-01-18T04:13:29.093672Z","shell.execute_reply.started":"2022-01-18T04:13:28.568893Z","shell.execute_reply":"2022-01-18T04:13:29.092825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['diagnosis'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:29.095016Z","iopub.execute_input":"2022-01-18T04:13:29.095298Z","iopub.status.idle":"2022-01-18T04:13:29.109194Z","shell.execute_reply.started":"2022-01-18T04:13:29.095247Z","shell.execute_reply":"2022-01-18T04:13:29.108277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].groupby(train['diagnosis']).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:29.11026Z","iopub.execute_input":"2022-01-18T04:13:29.110506Z","iopub.status.idle":"2022-01-18T04:13:29.126528Z","shell.execute_reply.started":"2022-01-18T04:13:29.11048Z","shell.execute_reply":"2022-01-18T04:13:29.125691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train['diagnosis'], hue=train['target'])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:29.127796Z","iopub.execute_input":"2022-01-18T04:13:29.128215Z","iopub.status.idle":"2022-01-18T04:13:29.517036Z","shell.execute_reply.started":"2022-01-18T04:13:29.128182Z","shell.execute_reply":"2022-01-18T04:13:29.516184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train[['sex','age_approx','anatom_site_general_challenge','diagnosis','target']]\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:29.518323Z","iopub.execute_input":"2022-01-18T04:13:29.518639Z","iopub.status.idle":"2022-01-18T04:13:29.534814Z","shell.execute_reply.started":"2022-01-18T04:13:29.518606Z","shell.execute_reply":"2022-01-18T04:13:29.533913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df = train_df.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:29.536381Z","iopub.execute_input":"2022-01-18T04:13:29.536876Z","iopub.status.idle":"2022-01-18T04:13:29.800889Z","shell.execute_reply.started":"2022-01-18T04:13:29.536835Z","shell.execute_reply":"2022-01-18T04:13:29.800027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.pairplot(train_df, hue=\"diagnosis\")","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:13:29.802332Z","iopub.execute_input":"2022-01-18T04:13:29.802857Z","iopub.status.idle":"2022-01-18T04:14:03.658237Z","shell.execute_reply.started":"2022-01-18T04:13:29.802811Z","shell.execute_reply":"2022-01-18T04:14:03.657249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train_df.corr(),annot=True,linewidths=0.2) \nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:03.659751Z","iopub.execute_input":"2022-01-18T04:14:03.660008Z","iopub.status.idle":"2022-01-18T04:14:04.079588Z","shell.execute_reply.started":"2022-01-18T04:14:03.659977Z","shell.execute_reply":"2022-01-18T04:14:04.078922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_training_curves(training, validation, title, subplot):\n  if subplot%10==1: # set up the subplots on the first call\n    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n    plt.tight_layout()\n  ax = plt.subplot(subplot)\n  ax.set_facecolor('#F8F8F8')\n  ax.plot(training)\n  ax.plot(validation)\n  ax.set_title('model '+ title)\n  ax.set_ylabel(title)\n  ax.set_xlabel('epoch')\n  ax.legend(['train', 'valid.'])\n\ncols, rows = 4, 3\ndef grid_display(list_of_images, no_of_columns=2, figsize=(15,15), title = False):\n    fig = plt.figure(figsize=figsize)\n    column = 0\n    z = 0\n    for i in range(len(list_of_images)):\n        column += 1\n        #  check for end of column and create a new figure\n        if column == no_of_columns+1:\n            fig = plt.figure(figsize=figsize)\n            column = 1\n        fig.add_subplot(1, no_of_columns, column)\n        if title:\n            if i >= no_of_columns:\n                plt.title(titles[z])\n                z +=1\n            else:\n                plt.title(titles[i])\n        plt.imshow(list_of_images[i])\n        plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:04.080848Z","iopub.execute_input":"2022-01-18T04:14:04.081489Z","iopub.status.idle":"2022-01-18T04:14:04.092032Z","shell.execute_reply.started":"2022-01-18T04:14:04.081454Z","shell.execute_reply":"2022-01-18T04:14:04.091414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the Chunk of Non-Melanoma Images...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['target'] == 0].sample(8)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\n#show_images(image_all, cols=1)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:04.093381Z","iopub.execute_input":"2022-01-18T04:14:04.093625Z","iopub.status.idle":"2022-01-18T04:14:13.039167Z","shell.execute_reply.started":"2022-01-18T04:14:04.0936Z","shell.execute_reply":"2022-01-18T04:14:13.038329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the Chunk of Melanoma Images...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['target'] == 1].sample(8)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:13.040386Z","iopub.execute_input":"2022-01-18T04:14:13.040604Z","iopub.status.idle":"2022-01-18T04:14:17.195226Z","shell.execute_reply.started":"2022-01-18T04:14:13.040579Z","shell.execute_reply":"2022-01-18T04:14:17.194345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the skin cancer at torso...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['anatom_site_general_challenge'] == 'torso'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:17.199832Z","iopub.execute_input":"2022-01-18T04:14:17.200079Z","iopub.status.idle":"2022-01-18T04:14:25.706698Z","shell.execute_reply.started":"2022-01-18T04:14:17.200052Z","shell.execute_reply":"2022-01-18T04:14:25.705855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the skin cancer at lower extremity...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['anatom_site_general_challenge'] == 'lower extremity'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:25.707903Z","iopub.execute_input":"2022-01-18T04:14:25.708797Z","iopub.status.idle":"2022-01-18T04:14:31.715828Z","shell.execute_reply.started":"2022-01-18T04:14:25.708764Z","shell.execute_reply":"2022-01-18T04:14:31.71496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the skin cancer at Upper extremity...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['anatom_site_general_challenge'] == 'upper extremity'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:31.717185Z","iopub.execute_input":"2022-01-18T04:14:31.717654Z","iopub.status.idle":"2022-01-18T04:14:37.919091Z","shell.execute_reply.started":"2022-01-18T04:14:31.717622Z","shell.execute_reply":"2022-01-18T04:14:37.917997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer at head/neck...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['anatom_site_general_challenge'] == 'head/neck'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:37.920824Z","iopub.execute_input":"2022-01-18T04:14:37.921283Z","iopub.status.idle":"2022-01-18T04:14:45.235874Z","shell.execute_reply.started":"2022-01-18T04:14:37.921235Z","shell.execute_reply":"2022-01-18T04:14:45.235191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer at Palms/soles...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['anatom_site_general_challenge'] == 'palms/soles'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:45.237129Z","iopub.execute_input":"2022-01-18T04:14:45.237558Z","iopub.status.idle":"2022-01-18T04:14:46.064516Z","shell.execute_reply.started":"2022-01-18T04:14:45.237527Z","shell.execute_reply":"2022-01-18T04:14:46.063893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer at Oral/genital...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['anatom_site_general_challenge'] == 'oral/genital'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:46.065747Z","iopub.execute_input":"2022-01-18T04:14:46.066138Z","iopub.status.idle":"2022-01-18T04:14:47.138898Z","shell.execute_reply.started":"2022-01-18T04:14:46.066093Z","shell.execute_reply":"2022-01-18T04:14:47.138349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer nevus....","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'nevus'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:47.140055Z","iopub.execute_input":"2022-01-18T04:14:47.140432Z","iopub.status.idle":"2022-01-18T04:14:49.174527Z","shell.execute_reply.started":"2022-01-18T04:14:47.140399Z","shell.execute_reply":"2022-01-18T04:14:49.173882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer Melanoma...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'melanoma'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:49.17577Z","iopub.execute_input":"2022-01-18T04:14:49.176167Z","iopub.status.idle":"2022-01-18T04:14:51.153067Z","shell.execute_reply.started":"2022-01-18T04:14:49.176137Z","shell.execute_reply":"2022-01-18T04:14:51.152164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer seborrheic keratosis...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'seborrheic keratosis'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:51.154457Z","iopub.execute_input":"2022-01-18T04:14:51.154769Z","iopub.status.idle":"2022-01-18T04:14:58.639061Z","shell.execute_reply.started":"2022-01-18T04:14:51.15473Z","shell.execute_reply":"2022-01-18T04:14:58.638154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer lentigo NOS...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'lentigo NOS'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:14:58.640498Z","iopub.execute_input":"2022-01-18T04:14:58.64082Z","iopub.status.idle":"2022-01-18T04:15:04.49973Z","shell.execute_reply.started":"2022-01-18T04:14:58.640779Z","shell.execute_reply":"2022-01-18T04:15:04.498854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer lichenoid keratosis...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'lichenoid keratosis'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:04.500924Z","iopub.execute_input":"2022-01-18T04:15:04.501267Z","iopub.status.idle":"2022-01-18T04:15:09.822577Z","shell.execute_reply.started":"2022-01-18T04:15:04.501212Z","shell.execute_reply":"2022-01-18T04:15:09.821702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer solar lentigo...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'solar lentigo'].sample(4)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:09.824033Z","iopub.execute_input":"2022-01-18T04:15:09.824919Z","iopub.status.idle":"2022-01-18T04:15:16.270021Z","shell.execute_reply.started":"2022-01-18T04:15:09.824875Z","shell.execute_reply":"2022-01-18T04:15:16.269112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer atypical melanocytic proliferation...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'atypical melanocytic proliferation'].sample(1)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:16.271456Z","iopub.execute_input":"2022-01-18T04:15:16.27184Z","iopub.status.idle":"2022-01-18T04:15:18.376023Z","shell.execute_reply.started":"2022-01-18T04:15:16.271811Z","shell.execute_reply":"2022-01-18T04:15:18.375197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer cafe-au-lait macule...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'cafe-au-lait macule'].sample(1)['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:18.377154Z","iopub.execute_input":"2022-01-18T04:15:18.377392Z","iopub.status.idle":"2022-01-18T04:15:20.483355Z","shell.execute_reply.started":"2022-01-18T04:15:18.377364Z","shell.execute_reply":"2022-01-18T04:15:20.482446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualiza the skin cancer unknown...","metadata":{}},{"cell_type":"code","source":"image_list = train[train['diagnosis'] == 'unknown'].sample()['image_name']\nimage_all=[]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n    img = np.array(Image.open(image_file))\n    image_all.append(img)\ngrid_display(image_all, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:20.484667Z","iopub.execute_input":"2022-01-18T04:15:20.48489Z","iopub.status.idle":"2022-01-18T04:15:23.177948Z","shell.execute_reply.started":"2022-01-18T04:15:20.484865Z","shell.execute_reply":"2022-01-18T04:15:23.177362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Skin cancer At different Age Group...","metadata":{}},{"cell_type":"code","source":"arr = [15.0,20.0,25.0,30.0,35.0,40.0,45.0,50.0,55.0,60.0,65.0,70.0,75.0,80.0,85.0,90.0]\nimage_all=[]\ntitles = ['At Age 15.0','At Age 20.0','At Age 25.0','At Age 30.0','At Age 35.0','At Age 40.0'\n          ,'At Age 45.0','At Age 50.0','At Age 55.0','At Age 60.0','At Age 65.0','At Age 70.0'\n          ,'At Age 75.0','At Age 80.0','At Age 85.0','At Age 90.0']\nfor i in arr:\n    image_list = train[train['age_approx'] == i].sample()['image_name']\n    for image_id in image_list:\n        image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg' \n        img = np.array(Image.open(image_file))\n        image_all.append(img)\ngrid_display(image_all, 4, (15,15), title = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:23.178904Z","iopub.execute_input":"2022-01-18T04:15:23.179666Z","iopub.status.idle":"2022-01-18T04:15:47.433418Z","shell.execute_reply.started":"2022-01-18T04:15:23.179618Z","shell.execute_reply":"2022-01-18T04:15:47.432481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply some Image Processing...\n\n","metadata":{}},{"cell_type":"code","source":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Reduced Noise', \"Gaussian Blur\", 'Adjusted Contrast']\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    #Reducing Noise\n    result = cv2.fastNlMeansDenoisingColored(img,None,20,10,7,21)\n    image_all.append(result)\n    #Gaussian Blur\n    blur_image = cv2.GaussianBlur(img, (7,7), 0)\n    image_all.append(blur_image)\n    #Adjusted contrast\n    contrast_img = cv2.addWeighted(img, 2.5, np.zeros(img.shape, img.dtype), 0, 0)\n    image_all.append(contrast_img)\ngrid_display(image_all, 4, (15,15), title = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:15:47.434883Z","iopub.execute_input":"2022-01-18T04:15:47.435293Z","iopub.status.idle":"2022-01-18T04:16:07.845291Z","shell.execute_reply.started":"2022-01-18T04:15:47.435236Z","shell.execute_reply":"2022-01-18T04:16:07.844382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Adaptive thresholding', \"Binary thresholding\"]\nfor image_id in image_list:\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    #Adaptive Thresholding..\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    thresh1 = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n    image_all.append(thresh1)\n    #Binary Thresholding...\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) \n    res, thresh = cv2.threshold(hsv[:, :, 0], 0, 255, cv2.THRESH_BINARY_INV)\n    image_all.append(thresh)\ngrid_display(image_all, 3, (15,15), title = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:07.846783Z","iopub.execute_input":"2022-01-18T04:16:07.847095Z","iopub.status.idle":"2022-01-18T04:16:11.094786Z","shell.execute_reply.started":"2022-01-18T04:16:07.847055Z","shell.execute_reply":"2022-01-18T04:16:11.0939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Otsuâ€™s Binarization....\n","metadata":{}},{"cell_type":"code","source":"img = cv2.imread('/kaggle/input/siim-isic-melanoma-classification/jpeg/train/ISIC_0052212.jpg', 0)\n# global thresholding\nret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n\n# Otsu's thresholding\nret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n# Otsu's thresholding after Gaussian filtering\nblur = cv2.GaussianBlur(img,(5,5),0)\nret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\n# plot all the images and their histograms\nimages = [img, 0, th1,\n          img, 0, th2,\n          blur, 0, th3]\ntitles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\nplt.figure(figsize=(15,10))\nfor i in range(3):\n    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:11.096065Z","iopub.execute_input":"2022-01-18T04:16:11.096416Z","iopub.status.idle":"2022-01-18T04:16:14.519383Z","shell.execute_reply.started":"2022-01-18T04:16:11.096383Z","shell.execute_reply":"2022-01-18T04:16:14.518458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating Area and Parameter of cancerous part of cell...","metadata":{}},{"cell_type":"code","source":"image = []\ntitles = ['Original', 'Thresold Image', 'Contour Image']\nimg = cv2.imread('/kaggle/input/siim-isic-melanoma-classification/jpeg/train/ISIC_0052212.jpg', 0)\nimg = img[200:900, 500:1500]\nimage.append(img)\n#Apply thresholding\nblur = cv2.GaussianBlur(img,(5,5),0)\nret3,th3 = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\nimage.append(th3)\ncontours, hierarcy = cv2.findContours(th3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\nimg2 = img.copy()\nindex = -1\nthickness = 4\ncolor = (255, 0, 255)\n\nobjects = np.zeros([img.shape[0], img.shape[1], 3], 'uint8')\nfor c in contours:\n    cv2.drawContours(objects, [c], -1, color, -1)\n    \n    area = cv2.contourArea(c)\n    perimeter = cv2.arcLength(c, True)\n    \n    M = cv2.moments(c)\n    if M[\"m00\"] != 0:\n        cx = int(M[\"m10\"] / M[\"m00\"])\n        cy = int(M[\"m01\"] / M[\"m00\"])\n    else:\n    # set values as what you need in the situation\n        cx, cy = 0, 0\n    cv2.circle(objects, (cx, cy), 4, (0, 0, 255), -1)\n    \n    print(\"AREA:{}, perimeter:{}\".format(area, perimeter))\n\nimage.append(objects)\ngrid_display(image, 3, (15,15), title = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:14.520686Z","iopub.execute_input":"2022-01-18T04:16:14.521252Z","iopub.status.idle":"2022-01-18T04:16:15.143324Z","shell.execute_reply.started":"2022-01-18T04:16:14.521219Z","shell.execute_reply":"2022-01-18T04:16:15.142597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scale Up & Scale Down...\n\n","metadata":{}},{"cell_type":"code","source":"image_list = train[train['target'] == 1].sample(2)['image_name']\nimage_all=[]\ntitles = ['original', 'Scale Down', \"Scale Up\"]\nfor image_id in image_list:\n    scaleX = 0.6\n    scaleY = 0.6\n    image_file = f'/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'+image_id+'.jpg'\n    img = cv2.imread(image_file,1)\n    image_all.append(img)\n    # Scaling Down the image 0.6 times\n    scaleDown = cv2.resize(img, None, fx= scaleX, fy= scaleY, interpolation= cv2.INTER_LINEAR)\n    image_all.append(scaleDown)\n    # Scaling up the image 1.8 times\n    scaleUp = cv2.resize(img, None, fx= scaleX*3, fy= scaleY*3, interpolation= cv2.INTER_LINEAR)\n    image_all.append(scaleUp)\ngrid_display(image_all, 3, (15,15), title = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:15.144206Z","iopub.execute_input":"2022-01-18T04:16:15.144453Z","iopub.status.idle":"2022-01-18T04:16:22.79345Z","shell.execute_reply.started":"2022-01-18T04:16:15.144426Z","shell.execute_reply":"2022-01-18T04:16:22.792819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ORB (Oriented FAST and Rotated BRIEF)...\n\n","metadata":{}},{"cell_type":"code","source":"image_all=[]\ntitles = ['original', 'ORB Detected', \"Zoom Image\"]\nimg = cv2.imread('/kaggle/input/siim-isic-melanoma-classification/jpeg/train/ISIC_0052212.jpg', 1)\nimage_all.append(img)\n# Initiate ORB detector\norb = cv2.ORB_create()\n# find the keypoints with ORB\nkp = orb.detect(img,None)\n# compute the descriptors with ORB\nkp, des = orb.compute(img, kp)\n# draw only keypoints location,not size and orientation\nimg2 = cv2.drawKeypoints(img, kp, None, color=(0,255,0), flags=0)\nimage_all.append(img2)\nimg3 = img2[350:800,600:1250]\nimage_all.append(img3)\ngrid_display(image_all, 3, (35,35), title = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:22.794558Z","iopub.execute_input":"2022-01-18T04:16:22.794884Z","iopub.status.idle":"2022-01-18T04:16:24.211359Z","shell.execute_reply.started":"2022-01-18T04:16:22.794857Z","shell.execute_reply":"2022-01-18T04:16:24.21048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choosing tensorflow Record for model...\n","metadata":{}},{"cell_type":"markdown","source":"****TPU","metadata":{}},{"cell_type":"code","source":"# Detect hardware\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n  tpu = None\n#If TPU not found try with GPUs\n  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy for hardware\nif tpu:\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n  print('Running on TPU ', tpu.master())  \nelif len(gpus) > 0:\n  strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n  print('Running on ', len(gpus), ' GPU(s) ')\nelse:\n  strategy = tf.distribute.get_strategy()\n  print('Running on CPU')\n\n# How many accelerators do we have ?\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:24.212578Z","iopub.execute_input":"2022-01-18T04:16:24.21282Z","iopub.status.idle":"2022-01-18T04:16:30.141881Z","shell.execute_reply.started":"2022-01-18T04:16:24.212793Z","shell.execute_reply":"2022-01-18T04:16:30.141046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('siim-isic-melanoma-classification')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:30.142973Z","iopub.execute_input":"2022-01-18T04:16:30.143218Z","iopub.status.idle":"2022-01-18T04:16:30.671252Z","shell.execute_reply.started":"2022-01-18T04:16:30.143188Z","shell.execute_reply":"2022-01-18T04:16:30.67039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords/train*')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords/test*')\nBATCH_SIZE = 10 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [1024, 1024]\nAUTO = tf.data.experimental.AUTOTUNE\nimSize = 1024\nEPOCHS = 10\n\nVALIDATION_SPLIT = 0.18\nsplit = int(len(TRAINING_FILENAMES) * VALIDATION_SPLIT)\ntraining_filenames = TRAINING_FILENAMES[split:]\nvalidation_filenames = TRAINING_FILENAMES[:split]\nprint(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\"\n      .format(len(TRAINING_FILENAMES), len(training_filenames), len(validation_filenames)))\nTRAINING_FILENAMES = training_filenames","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:30.672544Z","iopub.execute_input":"2022-01-18T04:16:30.672896Z","iopub.status.idle":"2022-01-18T04:16:31.068486Z","shell.execute_reply.started":"2022-01-18T04:16:30.672863Z","shell.execute_reply":"2022-01-18T04:16:31.067608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Intialize the Value...","metadata":{}},{"cell_type":"markdown","source":"# Reading Tensorflow Record...","metadata":{}},{"cell_type":"code","source":"def read_labeled_tfrecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, [imSize,imSize])\n    label = tf.cast(example['target'], tf.int32)\n    return image, label \n\ndef read_unlabeled_tfrecord(example):\n    u_features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, u_features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, [imSize,imSize])\n    idnum = example['image_name']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:31.069736Z","iopub.execute_input":"2022-01-18T04:16:31.069973Z","iopub.status.idle":"2022-01-18T04:16:31.082758Z","shell.execute_reply.started":"2022-01-18T04:16:31.069946Z","shell.execute_reply":"2022-01-18T04:16:31.082126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    #image = tf.image.random_brightness(x, 0.2)\n    #image = cutmix(image, label)\n    return image, label   \n\ndef get_training_dataset(dataset, do_aug=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(filenames, train=False):\n    dataset = load_dataset(filenames, labeled=True)\n    dataset = dataset.cache() # This dataset fits in RAM\n    if train:\n    # Best practices for Keras:\n    # Training dataset: repeat then batch\n    # Evaluation dataset: do not repeat\n        dataset = dataset.repeat()\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n        dataset = dataset.shuffle(2000)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n    \ndef get_test_dataset(dataset, ordered=False):\n    dataset = load_dataset(dataset, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nvalidation_dataset = get_validation_dataset(validation_filenames, train=False)\ntraining_dataset = get_training_dataset(TRAINING_FILENAMES)\ntest_dataset = get_test_dataset(TEST_FILENAMES)\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nNUM_VALID_IMAGES = count_data_items(validation_filenames)\nvalidation_steps = NUM_VALID_IMAGES // BATCH_SIZE\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\nprint('Dataset: {} training images, {} unlabeled test images, {} validition images'\n      .format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES, NUM_VALID_IMAGES))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:31.083887Z","iopub.execute_input":"2022-01-18T04:16:31.084657Z","iopub.status.idle":"2022-01-18T04:16:31.461924Z","shell.execute_reply.started":"2022-01-18T04:16:31.084623Z","shell.execute_reply":"2022-01-18T04:16:31.461061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply Some Image Augmentation Technique...\n","metadata":{"trusted":true}},{"cell_type":"markdown","source":"## Cutout data augmentation...\n","metadata":{}},{"cell_type":"code","source":"def get_random_eraser(input_img,p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n   # def eraser(input_img):\n    img_h, img_w, img_c = input_img.shape\n\n    while True:\n        s = np.random.uniform(s_l, s_h) * img_h * img_w\n        r = np.random.uniform(r_1, r_2)\n        w = int(np.sqrt(s / r))\n        h = int(np.sqrt(s * r))\n        left = np.random.randint(0, img_w)\n        top = np.random.randint(0, img_h)\n\n        if left + w <= img_w and top + h <= img_h:\n            break\n\n    if pixel_level:\n        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n    else:\n        c = np.random.uniform(v_l, v_h)\n\n    input_img[top:top + h, left:left + w, :] = c\n\n    return input_img","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:31.463339Z","iopub.execute_input":"2022-01-18T04:16:31.463638Z","iopub.status.idle":"2022-01-18T04:16:31.472872Z","shell.execute_reply.started":"2022-01-18T04:16:31.463601Z","shell.execute_reply":"2022-01-18T04:16:31.472294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN = '/kaggle/input/siim-isic-melanoma-classification/jpeg/train/'\nIMAGE_SIZE = 1024\nn_imgs = 12\nimg_filenames = os.listdir(TRAIN)[:n_imgs]\nimg_filenames[:3]\nimage=[]\nfor file_name in img_filenames:\n    img = cv2.imread(TRAIN +file_name)\n    img = get_random_eraser(img)\n    image.append(img)\ngrid_display(image, 4, (15,15))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:31.47393Z","iopub.execute_input":"2022-01-18T04:16:31.474839Z","iopub.status.idle":"2022-01-18T04:16:48.277143Z","shell.execute_reply.started":"2022-01-18T04:16:31.474797Z","shell.execute_reply":"2022-01-18T04:16:48.276294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CutMix data augmentation\n","metadata":{}},{"cell_type":"code","source":"# if you have label in images\ndef onehot(image,label):\n    CLASSES = 2 # Define number of classes our model have\n    return image,tf.one_hot(label,CLASSES)\n\ndef cutmix(image, label): #, PROBABILITY = 1.0\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = 1024 #IMAGE_SIZE[0]\n    CLASSES = 2\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32)\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:48.27839Z","iopub.execute_input":"2022-01-18T04:16:48.278636Z","iopub.status.idle":"2022-01-18T04:16:48.59394Z","shell.execute_reply.started":"2022-01-18T04:16:48.278607Z","shell.execute_reply":"2022-01-18T04:16:48.593048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUG_BATCH = 48\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH//col)\nall_elements = training_dataset.unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(cutmix)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:16:48.59551Z","iopub.execute_input":"2022-01-18T04:16:48.595929Z","iopub.status.idle":"2022-01-18T04:17:12.382144Z","shell.execute_reply.started":"2022-01-18T04:16:48.595885Z","shell.execute_reply":"2022-01-18T04:17:12.381454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GridMask data augmentation\n","metadata":{}},{"cell_type":"code","source":"def transform(image, inv_mat, image_shape):\n    h, w, c = image_shape\n    cx, cy = w//2, h//2\n    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n    new_zs = tf.ones([h*w], dtype=tf.int32)\n    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\n    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n    rotated_image_channel = list()\n    for i in range(c):\n        vals = rotated_image_values[:,i]\n        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])\n\ndef random_rotate(image, angle, image_shape):\n    def get_rotation_mat_inv(angle):\n        # transform to radian\n        angle = math.pi * angle / 180\n        cos_val = tf.math.cos(angle)\n        sin_val = tf.math.sin(angle)\n        one = tf.constant([1], tf.float32)\n        zero = tf.constant([0], tf.float32)\n        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\n        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n        return rot_mat_inv\n    angle = float(angle) * tf.random.normal([1],dtype='float32')\n    rot_mat_inv = get_rotation_mat_inv(angle)\n    return transform(image, rot_mat_inv, image_shape)\n\n\ndef GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\n    h, w = image_height, image_width\n    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n    hh = hh+1 if hh%2==1 else hh\n    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n\n    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n\n    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n\n    for i in range(0, hh//d+1):\n        s1 = i * d + st_h\n        s2 = i * d + st_w\n        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n\n    x_clip_mask = tf.logical_or(x_ranges < 0 , x_ranges > hh-1)\n    y_clip_mask = tf.logical_or(y_ranges < 0 , y_ranges > hh-1)\n    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n\n    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n\n    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n    x_ranges = tf.repeat(x_ranges, hh)\n    y_ranges = tf.repeat(y_ranges, hh)\n\n    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n\n    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n\n    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n\n    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n\n    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n    mask = tf.image.crop_to_bounding_box(mask, (hh-h)//2, (hh-w)//2, image_height, image_width)\n\n    return mask\n\ndef apply_grid_mask(image, image_shape):\n    AugParams = {\n        'd1' : 100,\n        'd2': 160,\n        'rotate' : 45,\n        'ratio' : 0.3\n    }\n    mask = GridMask(image_shape[0], image_shape[1], AugParams['d1'], AugParams['d2'], AugParams['rotate'], AugParams['ratio'])\n    if image_shape[-1] == 3:\n        mask = tf.concat([mask, mask, mask], axis=-1)\n    return image * tf.cast(mask,tf.float32)\n\ndef gridmask(img_batch, label_batch):\n    return apply_grid_mask(img_batch, (1024,1024, 3)), label_batch","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:17:12.383202Z","iopub.execute_input":"2022-01-18T04:17:12.383918Z","iopub.status.idle":"2022-01-18T04:17:12.420372Z","shell.execute_reply.started":"2022-01-18T04:17:12.383883Z","shell.execute_reply":"2022-01-18T04:17:12.41937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUG_BATCH = 48\nrow = 6; col = 4;\nrow = min(row,AUG_BATCH//col)\nall_elements = training_dataset.unbatch()\naugmented_element = all_elements.repeat().batch(AUG_BATCH).map(gridmask)\n\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(15,int(15*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:17:12.42144Z","iopub.execute_input":"2022-01-18T04:17:12.42166Z","iopub.status.idle":"2022-01-18T04:17:25.307875Z","shell.execute_reply.started":"2022-01-18T04:17:12.421635Z","shell.execute_reply":"2022-01-18T04:17:25.305428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data shape...","metadata":{}},{"cell_type":"code","source":"# data dump\nprint(\"Training data shapes:\")\nfor image, label in training_dataset.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Training data label examples:\", label.numpy())\n\nprint(\"Validation data shapes:\")\nfor image, label in validation_dataset.take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Validation data label examples:\", label.numpy())\n\nprint(\"Test data shapes:\")\nfor image, idnum in test_dataset.take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", idnum.numpy().astype('U'))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:17:25.309303Z","iopub.execute_input":"2022-01-18T04:17:25.309599Z","iopub.status.idle":"2022-01-18T04:17:58.157445Z","shell.execute_reply.started":"2022-01-18T04:17:25.309565Z","shell.execute_reply":"2022-01-18T04:17:58.156481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# with strategy.scope():\n#     model = tf.keras.Sequential([\n#         dense.DenseNet121(\n#             input_shape=(imSize, imSize, 3),\n#             weights='imagenet',\n#             include_top=False\n#         ),\n#         layers.GlobalAveragePooling2D(),\n#         layers.Dense(1, activation='sigmoid')\n#     ])\n        \n#     model.compile(\n#         optimizer='adam',\n#         loss = 'binary_crossentropy',\n#         metrics=['accuracy']\n#     )\n#     model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:17:58.158827Z","iopub.execute_input":"2022-01-18T04:17:58.159069Z","iopub.status.idle":"2022-01-18T04:17:58.163585Z","shell.execute_reply.started":"2022-01-18T04:17:58.159041Z","shell.execute_reply":"2022-01-18T04:17:58.16272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.applications.xception as xcep\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        xcep.Xception(\n            input_shape=(imSize, imSize, 3),\n            weights='imagenet',\n            include_top=False\n        ),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(1024, activation= 'relu'), \n        layers.Dropout(0.2),\n        layers.Dense(512, activation= 'relu'), \n        layers.Dropout(0.2), \n        layers.Dense(256, activation='relu'), \n        layers.Dropout(0.2), \n        layers.Dense(128, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(64, activation='relu'), \n        layers.Dropout(0.1),\n        layers.Dense(1, activation='sigmoid')\n    ])\n        \n    model.compile(\n        optimizer='adam',\n        loss = 'binary_crossentropy',\n        metrics=['accuracy']\n    )\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:17:58.1653Z","iopub.execute_input":"2022-01-18T04:17:58.165631Z","iopub.status.idle":"2022-01-18T04:18:11.845483Z","shell.execute_reply.started":"2022-01-18T04:17:58.165591Z","shell.execute_reply":"2022-01-18T04:18:11.843605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -q efficientnet","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:18:11.846674Z","iopub.execute_input":"2022-01-18T04:18:11.846957Z","iopub.status.idle":"2022-01-18T04:18:11.851571Z","shell.execute_reply.started":"2022-01-18T04:18:11.846928Z","shell.execute_reply":"2022-01-18T04:18:11.85066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import efficientnet.tfkeras as efn\n# with strategy.scope():\n#     model = tf.keras.Sequential([\n#         efn.EfficientNetB5(\n#             input_shape=(imSize, imSize, 3),\n#             weights='imagenet',\n#             include_top=False\n#         ),\n#         layers.GlobalAveragePooling2D(),\n#         layers.Dense(512, activation= 'relu'), \n#         layers.Dropout(0.2), \n#         layers.Dense(256, activation='relu'), \n#         layers.Dropout(0.2), \n#         layers.Dense(128, activation='relu'), \n#         layers.Dropout(0.1),\n#         layers.Dense(64, activation='relu'), \n#         layers.Dropout(0.1),\n#         layers.Dense(1, activation='sigmoid')\n#     ])\n        \n#     model.compile(\n#         optimizer='adam',\n#         loss = 'binary_crossentropy',\n#         metrics=['accuracy']\n#     )\n#     model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:18:11.852663Z","iopub.execute_input":"2022-01-18T04:18:11.852905Z","iopub.status.idle":"2022-01-18T04:18:11.864246Z","shell.execute_reply.started":"2022-01-18T04:18:11.85288Z","shell.execute_reply":"2022-01-18T04:18:11.863337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def lrfn(epoch):\n    LR_START = 0.00001\n    LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n    LR_MIN = 0.00001\n    LR_RAMPUP_EPOCHS = 5\n    LR_SUSTAIN_EPOCHS = 0\n    LR_EXP_DECAY = .8\n    \n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:18:11.865683Z","iopub.execute_input":"2022-01-18T04:18:11.865916Z","iopub.status.idle":"2022-01-18T04:18:11.879342Z","shell.execute_reply.started":"2022-01-18T04:18:11.865891Z","shell.execute_reply":"2022-01-18T04:18:11.878322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\nhistory = model.fit(training_dataset, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n                    validation_data=validation_dataset,callbacks=[lr_schedule])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:18:11.881225Z","iopub.execute_input":"2022-01-18T04:18:11.881622Z","iopub.status.idle":"2022-01-18T04:59:10.49345Z","shell.execute_reply.started":"2022-01-18T04:18:11.881585Z","shell.execute_reply":"2022-01-18T04:59:10.492452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing training Curve...","metadata":{}},{"cell_type":"code","source":"display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:10.495212Z","iopub.execute_input":"2022-01-18T04:59:10.495934Z","iopub.status.idle":"2022-01-18T04:59:10.983832Z","shell.execute_reply.started":"2022-01-18T04:59:10.495889Z","shell.execute_reply":"2022-01-18T04:59:10.982871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Prediction...","metadata":{}},{"cell_type":"code","source":"test_ds = test_dataset#get_test_dataset(ordered=True)\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds).flatten()\nprint(probabilities)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, probabilities]), fmt=['%s', '%f'],\n           delimiter=',', header='image_name,target', comments='')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:10.985164Z","iopub.execute_input":"2022-01-18T04:59:10.986066Z","iopub.status.idle":"2022-01-18T05:00:13.187127Z","shell.execute_reply.started":"2022-01-18T04:59:10.986027Z","shell.execute_reply":"2022-01-18T05:00:13.186134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Submission...","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"submission.csv\")\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:00:13.188385Z","iopub.execute_input":"2022-01-18T05:00:13.188632Z","iopub.status.idle":"2022-01-18T05:00:13.212369Z","shell.execute_reply.started":"2022-01-18T05:00:13.188604Z","shell.execute_reply":"2022-01-18T05:00:13.211377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'].hist()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T05:00:13.213526Z","iopub.execute_input":"2022-01-18T05:00:13.213788Z","iopub.status.idle":"2022-01-18T05:00:13.455148Z","shell.execute_reply.started":"2022-01-18T05:00:13.213761Z","shell.execute_reply":"2022-01-18T05:00:13.454087Z"},"trusted":true},"execution_count":null,"outputs":[]}]}