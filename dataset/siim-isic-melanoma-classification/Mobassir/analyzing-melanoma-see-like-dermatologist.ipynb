{"cells":[{"metadata":{},"cell_type":"markdown","source":"**In this kernel we will like to analyze images and  if possible (measure size of spots) and also some  data augmentation ideas that looks promising**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://nci-media.cancer.gov/pdq/media/images/578083-750.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"if you are a researcher, then probably you will like to spend some times analyzing melanoma mole sizes as i have shown LEVELS OF MELANOMA in my past kernel [In-Depth Melanoma with modeling](https://www.kaggle.com/mobassir/in-depth-melanoma-with-modeling) so we know that.... <br> <br> **The Clark Scale has 5 levels of melanoma:**\n\n* Cells are in the out layer of the skin (epidermis)\n\n* Cells are in the layer directly under the epidermis (pupillary dermis)\n\n* The cells are touching the next layer known as the deep dermis\n\n* Cells have spread to the reticular dermis\n\n* Cells have grown in the fat layer\n\n![](https://media.giphy.com/media/lSJElktZ5BKUvYSztq/giphy.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# References \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* [TensorFlow + Transfer Learning: Melanoma](https://www.kaggle.com/amyjang/tensorflow-transfer-learning-melanoma)\n* [Measuring size of objects in an image with OpenCV](https://www.pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv/)\n* [object-size](https://github.com/snsharma1311/object-size)\n* [Ensemble of Convolutional Neural Networks for Disease Classification of Skin Lesions](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification)\n* [In-Depth Melanoma with modeling](https://www.kaggle.com/mobassir/in-depth-melanoma-with-modeling?scriptVersionId=39094350)\n* [GENERAL INFORMATION ABOUT MELANOMA](https://www.uhhospitals.org/services/cancer-services/skin-cancer/melanoma/about-melanoma)\n\n* [[Training CV] Melanoma Starter](https://www.kaggle.com/shonenkov/training-cv-melanoma-starter)\n\n* [Measuring Size of Objects with OpenCV](https://github.com/Practical-CV/Measuring-Size-of-Objects-with-OpenCV)\n* [Color Constancy](https://github.com/MinaSGorgi/Color-Constancy)\n* [Edge-Based Color Constancy](https://ieeexplore.ieee.org/document/4287009)\n* [Python | Thresholding techniques using OpenCV | Set-1 (Simple Thresholding)](https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-1-simple-thresholding/)\n* [lesion-GAN](https://github.com/alxiang/lesion-GAN)\n* [Data-Augmentation-and-Segmentation-with-GANs-for-Medical-Images](https://github.com/apolanco3225/Data-Augmentation-and-Segmentation-with-GANs-for-Medical-Images)\n* [Towards Interpretable Skin Lesion Classification with Deep Learning Models](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7153112/pdf/3203149.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# imports","execution_count":null},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install imutils","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom pathlib import Path\nimport pandas as pd\nfrom torch.utils.data import Dataset,DataLoader\n\nfrom scipy.spatial import distance as dist\nfrom imutils import perspective\nfrom imutils import contours\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport imutils\nimport cv2\n\nimport skimage.measure\nimport imageio\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom torchvision import transforms as T\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.model_selection import GroupKFold\nfrom kaggle_datasets import KaggleDatasets\n\nfrom scipy.spatial.distance import euclidean\nfrom imutils import perspective\nfrom imutils import contours\nimport numpy as np\nimport imutils\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport cv2\nfrom skimage import io\nimport albumentations as A\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.nn import functional as F\nfrom glob import glob\nimport sklearn\nfrom torch import nn\n\n\nimport keras\nimport numpy as np\nimport tensorflow as tf\nfrom keras.models import model_from_json, load_model\nimport json\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom functools import partial\n\nimport glob\nimport numpy as np\nimport cv2\nfrom skimage import filters as skifilters\nfrom scipy import ndimage\nfrom skimage import filters\nimport matplotlib.pyplot as plt\nimport tqdm\nfrom sklearn.utils import shuffle\nimport pandas as pd\n\nimport os\nimport h5py\nimport time\nimport json\nimport warnings\nfrom PIL import Image\n\nfrom fastprogress.fastprogress import master_bar, progress_bar\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom torchvision import models\nimport pdb\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensor\nimport matplotlib.pyplot as plt\n\nimport pickle \nimport os\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def list_files(path:Path):\n    return [o for o in path.iterdir()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/jpeg-melanoma-768x768/')\ndf_path = Path('../input/jpeg-melanoma-768x768/')\nim_sz = 256\nbs = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_fnames = list_files(path/'train')\ndf = pd.read_csv(df_path/'train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndf.target.value_counts(),df.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nGCS_PATH = KaggleDatasets().get_gcs_path('melanoma-768x768')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.int64)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    if labeled:\n        label = tf.cast(example['target'], tf.int32)\n        return image, label\n    idnum = example['image_name']\n    return image, idnum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nBATCH_SIZE = 8\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nIMAGE_SIZE = [768, 768]\nTRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '/train*.tfrec'),\n    test_size=0.2, random_state=5\n)\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\nprint('Train TFRecord Files:', len(TRAINING_FILENAMES))\nprint('Validation TFRecord Files:', len(VALID_FILENAMES))\nprint('Test TFRecord Files:', len(TEST_FILENAMES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    #dataset = dataset.map(augmentation_pipeline, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_training_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(15,15))\n    for n in range(8):\n        ax = plt.subplot(8,8,n+1)\n        plt.imshow(image_batch[n])\n        if label_batch[n]:\n            plt.title(\"MALIGNANT(1)\")\n        else:\n            plt.title(\"BENIGN(0)\")\n        plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor i in range(0,10):\n    image_batch, label_batch = next(iter(train_dataset))\n    for j in range(0,8):\n        var = label_batch[j].numpy()\n        if(var!=0):\n            show_batch(image_batch.numpy(), label_batch.numpy())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Samples with Melanoma\")\nimgs = df[df.target==1]['image_name'].values\n_, axs = plt.subplots(2, 3, figsize=(20, 8))\naxs = axs.flatten()\nfor f_name,ax in zip(imgs[10:20],axs):\n    img = Image.open(path/f'train/{f_name}.jpg')\n    ax.imshow(img)\n    ax.axis('off')    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The thickness of the tumor. The thickness of the tumor is measured from the surface of the skin to the deepest part of the tumor.\n\n![](https://nci-media.cancer.gov/pdq/media/images/799465.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Whether the tumor is ulcerated (has broken through the skin).**\n![](https://nci-media.cancer.gov/pdq/media/images/799466-750.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"from above 2 images we can see that depth of melanoma increases  as the mole size grows","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Algorithm","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Image pre-processing\n - Read an image and convert it it no grayscale\n - Blur the image using Gaussian Kernel to remove un-necessary edges\n - Edge detection using Canny edge detector\n - Perform morphological closing operation to remove noisy contours\n2. Object Segmentation\n - Find contours\n - Remove small contours by calculating its area (threshold used here is 100)\n - Sort contours from left to right to find the reference objects\n3. Reference object\n - Calculate how many pixels are there per metric (centi meter is used here)\n4. Compute results\n - Draw bounding boxes around each object and calculate its height and width","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Usage: This script will measure different objects in the frame using a reference object \n\n\n\n# Function to show array of images (intermediate results)\n\ndef show_images(images):\n    for i, img in enumerate(images):\n        plt.figure(figsize=(20,20))\n        plt.imshow(img)\n        plt.show()\n       \n\n        \nimgs = df[df.target==1]['image_name'].values\nprint(\"Samples with Melanoma\")\nfor f_name,ax in zip(imgs[:100],axs):\n \n    \n    im1 = Image.open(path/f'train/{f_name}.jpg')\n    print(path/f'train/{f_name}.jpg')\n    im1.save('./a.png')\n    img_path = '../working/a.png'\n\n\n\n    '''load our image from disk, convert it to grayscale, and then smooth it using a Gaussian filter.\n    We then perform edge detection along with a dilation + erosion to close any gaps \n    in between edges in the edge map\n    '''\n\n    # Read image and preprocess\n    image = cv2.imread(img_path)\n \n    #image = img\n\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blur = cv2.GaussianBlur(gray, (7, 7), 0)\n\n    edged = cv2.Canny(blur, 50, 100)\n    edged = cv2.dilate(edged, None, iterations=1)\n    edged = cv2.erode(edged, None, iterations=1)\n\n    #show_images([blur, edged])\n\n    '''find contours (i.e., the outlines) that correspond to the objects in our edge map.'''\n    # Find contours\n    cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n\n    # Sort contours from left to right as leftmost contour is reference object\n    try:\n        '''These contours are then sorted from left-to-right (allowing us to extract our reference object)'''\n        (cnts, _) = contours.sort_contours(cnts)\n         # Remove contours which are not large enough\n        for k in range(0,20):\n            try:\n                cnts = [x for x in cnts if cv2.contourArea(x) > k]\n                # Reference object dimensions\n                # Here for reference I have used a 2cm x 2cm square\n                mid = len(cnts)//2\n                ref_object = cnts[mid]\n            except:\n                pass\n    except:\n        #print(\"An exception occurred\") \n        continue\n\n    #print(len(cnts))\n    #print(cnts)\n    #cv2.drawContours(image, cnts, -1, (0,255,0), 3)\n\n    #show_images([image, edged])\n    #print(len(cnts))\n\n    # compute the rotated bounding box of the contour\n    orig = image.copy()\n    box = cv2.minAreaRect(ref_object)\n    box = cv2.boxPoints(box)\n    box = np.array(box, dtype=\"int\")\n    \n    # order the points in the contour such that they appear\n    # in top-left, top-right, bottom-right, and bottom-left\n    # order, then draw the outline of the rotated bounding\n    # box\n    \n    box = perspective.order_points(box)\n    \n    cv2.drawContours(orig, [box.astype(\"int\")], -1, (0, 255, 0), 2)\n    # loop over the original points and draw them\n    for (x, y) in box:\n        cv2.circle(orig, (int(x), int(y)), 5, (0, 0, 255), -1)\n        \n    (tl, tr, br, bl) = box\n    dist_in_pixel = euclidean(tl, tr)\n    dist_in_cm = 2\n    pixel_per_cm = dist_in_pixel/dist_in_cm\n    largestht = []\n    largestwid = []\n    # Draw remaining contours\n    for cnt in cnts:\n        box = cv2.minAreaRect(cnt)\n        box = cv2.boxPoints(box)\n        box = np.array(box, dtype=\"int\")\n        box = perspective.order_points(box)\n        (tl, tr, br, bl) = box\n        cv2.drawContours(image, [box.astype(\"int\")], -1, (0, 0, 255), 2)\n        mid_pt_horizontal = (tl[0] + int(abs(tr[0] - tl[0])/2), tl[1] + int(abs(tr[1] - tl[1])/2))\n        mid_pt_verticle = (tr[0] + int(abs(tr[0] - br[0])/2), tr[1] + int(abs(tr[1] - br[1])/2))\n        wid = euclidean(tl, tr)/pixel_per_cm\n        ht = euclidean(tr, br)/pixel_per_cm\n        largestht.append(ht)\n        largestwid.append(wid)\n       \n        #cv2.putText(image, \"{:.1f}cm\".format(wid), (int(mid_pt_horizontal[0] - 15), int(mid_pt_horizontal[1] - 10)), \n        #cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n        #cv2.putText(image, \"{:.1f}cm\".format(ht), (int(mid_pt_verticle[0] + 10), int(mid_pt_verticle[1])), \n        #cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n    show_images([image])   \n    if(len(largestht)>0):\n        a = largestht.index(max(largestht))\n        b = largestwid.index(max(largestwid))\n        largestht1 = largestht[b]\n        largestwid1 = largestwid[b]\n        largestht = largestht[a]\n        largestwid = largestwid[a]\n        \n\n        print(\"Rectangle 1  has : HEIGHT = \",largestht,\"and WIDTH = \",largestwid)\n        print(\"Rectangle 2  has : HEIGHT = \",largestht1,\"and WIDTH = \",largestwid1)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the outputs we can see that some images has scale measurements and our  object detector detected those and we can measure that scale too,but my  questions to researchers out there? - \"can we use those for any further analysis? and isn't it possible that learning algorithms focusing on such noises instead of melanoma moles?\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/C8GkNQn/findings.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"from the picture attached  we can see that mole colour is another very important parameter for analyzing and detecting MELANOMA,hence good data augmentation techniques are very very important \n\n![](https://i.pinimg.com/474x/4c/a8/df/4ca8dfc60d3640b55d91ea5bddde7e68.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Note: The following approach won 1st place in the 2019 Computer-Aided Diagnosis: Deep Learning in Dermascopy Challenge at Universitat de Girona scoring 92.2% accuracy (kappa: 0.819) at test-time, during the 2018-20 Joint Master of Science in Medical Imaging and Applications (MaIA) program.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def shades_gray(image, njet=0, mink_norm=1, sigma=1):\n    \"\"\"\n    Estimates the light source of an input_image as proposed in:\n    J. van de Weijer, Th. Gevers, A. Gijsenij\n    \"Edge-Based Color Constancy\"\n    IEEE Trans. Image Processing, accepted 2007.\n    Depending on the parameters the estimation is equal to Grey-World, Max-RGB, general Grey-World,\n    Shades-of-Grey or Grey-Edge algorithm.\n    :param image: rgb input image (NxMx3)\n    :param njet: the order of differentiation (range from 0-2)\n    :param mink_norm: minkowski norm used (if mink_norm==-1 then the max\n           operation is applied which is equal to minkowski_norm=infinity).\n    :param sigma: sigma used for gaussian pre-processing of input image\n    :return: illuminant color estimation\n    :raise: ValueError\n    \n    Ref: https://github.com/MinaSGorgy/Color-Constancy\n    \"\"\"\n    gauss_image = filters.gaussian(image, sigma=sigma, multichannel=True)\n    if njet == 0:\n        deriv_image = [gauss_image[:, :, channel] for channel in range(3)]\n    else:   \n        if njet == 1:\n            deriv_filter = filters.sobel\n        elif njet == 2:\n            deriv_filter = filters.laplace\n        else:\n            raise ValueError(\"njet should be in range[0-2]! Given value is: \" + str(njet))     \n        deriv_image = [np.abs(deriv_filter(gauss_image[:, :, channel])) for channel in range(3)]\n    for channel in range(3):\n        deriv_image[channel][image[:, :, channel] >= 255] = 0.\n    if mink_norm == -1:  \n        estimating_func = np.max \n    else:\n        estimating_func = lambda x: np.power(np.sum(np.power(x, mink_norm)), 1 / mink_norm)\n    illum = [estimating_func(channel) for channel in deriv_image]\n    som   = np.sqrt(np.sum(np.power(illum, 2)))\n    illum = np.divide(illum, som)\n    return illum\n\n\ndef correct_image(image, illum):\n    \"\"\"\n    Corrects image colors by performing diagonal transformation according to \n    given estimated illumination of the image.\n    :param image: rgb input image (NxMx3)\n    :param illum: estimated illumination of the image\n    :return: corrected image\n    \n    Ref: https://github.com/MinaSGorgy/Color-Constancy\n    \"\"\"\n    correcting_illum = illum * np.sqrt(3)\n    corrected_image = image / 255.\n    for channel in range(3):\n        corrected_image[:, :, channel] /= correcting_illum[channel]\n    return np.clip(corrected_image, 0., 1.)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ABSTRACT** from the Paper [**Edge-Based Color Constancy**](https://ieeexplore.ieee.org/document/4287009)\n\nColor constancy is the ability to measure colors of objects independent of the color of the light source. A well-known color constancy method is based on the gray-world assumption which assumes that the average reflectance of surfaces in the world is achromatic. In this paper, we propose a new hypothesis for color constancy namely the gray-edge hypothesis, which assumes that the average edge difference in a scene is achromatic. Based on this hypothesis, we propose an algorithm for color constancy. Contrary to existing color constancy algorithms, which are computed from the zero-order structure of images, our method is based on the derivative structure of images. Furthermore, we propose a framework which unifies a variety of known (gray-world, max-RGB, Minkowski norm) and the newly proposed gray-edge and higher order gray-edge algorithms. The quality of the various instantiations of the framework is tested and compared to the state-of-the-art color constancy methods on two large data sets of images recording objects under a large number of different light sources. The experiments show that the proposed color constancy algorithms obtain comparable results as the state-of-the-art color constancy methods with the merit of being computationally more efficient.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Color Transformations\nmx    = correct_image(image, shades_gray(image, njet=0, mink_norm=-1, sigma=0))  # MaxRGB Constancy\ngw    = correct_image(image, shades_gray(image, njet=0, mink_norm=+1, sigma=0))  # Gray World Constancy \nhsv   = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)                                   # HSV Color Space\nlab   = cv2.cvtColor(image, cv2.COLOR_RGB2Lab)                                   # CIELab Color Space\n\n# Concatenate to Output Image\nop    = np.concatenate((gw/255,np.expand_dims(hsv[:,:,0]/179,axis=2),hsv[:,:,1:]/255,\n                        np.expand_dims(lab[:,:,0]/255,axis=2),lab[:,:,1:]/128),axis=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(op[:,:,:3]*255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_circular_mask(h, w, center=None, radius=None):\n    if center is None: # use the middle of the image\n        center = [int(w/2), int(h/2)]\n    if radius is None: # use the smallest distance between the center and image walls\n        radius = min(center[0], center[1], w-center[0], h-center[1])\n    Y, X = np.ogrid[:h, :w]\n    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2)\n    mask = dist_from_center <= radius\n    return mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"circa_mask            = create_circular_mask(op.shape[0], op.shape[1], radius=200).astype(bool)\nop                    = np.multiply(op, np.dstack((circa_mask,circa_mask,circa_mask,circa_mask,circa_mask,\n                                                             circa_mask,circa_mask,circa_mask,circa_mask)))        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img1 = op[:,:,:3]*255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimageio.imwrite('filename1.png', img1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = cv2.imread('../working/filename1.png')\nplt.imshow(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imo = Image.fromarray((gw*255).astype(np.uint8))\nimo","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#os.listdir('../input/jpeg-melanoma-768x768/train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://i.ibb.co/PtgTy2Z/key.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef midpoint(ptA, ptB):\n    return ((ptA[0] + ptB[0]) * 0.5, (ptA[1] + ptB[1]) * 0.5)\n\n\ndef show_image(images):\n    plt.figure(figsize=(20,20))\n    plt.imshow(images)\n    plt.show()\n       \n\n\npath = '../input/jpeg-melanoma-768x768/train/ISIC_4789377.jpg' #\"../working/filename1.png\"\nim1 = Image.open(path)\nim1.save('./c.png')\npath = '../working/c.png'\n\n    \nwidth = 0.99\n\n\nimage = cv2.imread(path)\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ngray = cv2.GaussianBlur(gray, (7, 7), 0)\n\nedged = cv2.Canny(gray, 50, 100)\nshow_image(edged)\nedged = cv2.dilate(edged, None, iterations=1)\nedged = cv2.erode(edged, None, iterations=1)\nshow_image( edged)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncnts = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\nprint(\"Total number of contours are: \", len(cnts))\nif(len(cnts)>0):\n    (cnts, _) = contours.sort_contours(cnts)\npixelPerMetric = None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 0\ntotals = []\ntotal = len(cnts)\nfor c in cnts:\n    if cv2.contourArea(c) < 500:\n        continue\n    count += 1\n\n    orig = image.copy()\n    box = cv2.minAreaRect(c)\n    box = cv2.cv.BoxPoints(box) if imutils.is_cv2() else cv2.boxPoints(box)\n    box = np.array(box, dtype=\"int\")\n\n    box = perspective.order_points(box)\n    cv2.drawContours(orig, [box.astype(\"int\")], -1, (0, 255, 0), 2)\n\n    for (x, y) in box:\n        cv2.circle(orig, (int(x), int(y)), 5, (0, 0, 255), -1)\n\n\n    (tl, tr, br, bl) = box\n    (tltrX, tltrY) = midpoint(tl, tr)\n    (blbrX, blbrY) = midpoint(bl, br)\n    (tlblX, tlblY) = midpoint(tl, bl)\n    (trbrX, trbrY) = midpoint(tr, br)\n\n    cv2.circle(orig, (int(tltrX), int(tltrY)), 5, (255, 0, 0), -1)\n    cv2.circle(orig, (int(blbrX), int(blbrY)), 5, (255, 0, 0), -1)\n    cv2.circle(orig, (int(tlblX), int(tlblY)), 5, (255, 0, 0), -1)\n    cv2.circle(orig, (int(trbrX), int(trbrY)), 5, (255, 0, 0), -1)\n\n    cv2.line(orig, (int(tltrX), int(tltrY)), (int(blbrX), int(blbrY)), (255, 0, 255), 2)\n    cv2.line(orig, (int(tlblX), int(tlblY)), (int(trbrX), int(trbrY)), (255, 0, 255), 2)\n\n    dA = dist.euclidean((tltrX, tltrY), (blbrX, blbrY))\n    dB = dist.euclidean((tlblX, tlblY), (trbrX, trbrY))\n\n    if pixelPerMetric is None:\n        pixelPerMetric = dB / width\n\n    dimA = dA / pixelPerMetric\n    dimB = dB / pixelPerMetric\n\n    cv2.putText(orig, \"{:.1f}in\".format(dimA), (int(tltrX - 15), int(tltrY - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 255, 255), 2)\n    cv2.putText(orig, \"{:.1f}in\".format(dimB), (int(trbrX + 10), int(trbrY)), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (255, 255, 255), 2)\n    totals.append(orig)\n    plt.imshow(orig)\nprint(\"Total contours processed: \", count)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_row = 2\nn_col = 2\n_, axs = plt.subplots(n_row, n_col, figsize=(12, 12))\naxs = axs.flatten()\n\nfor i in range(len(totals)):\n    axs[i].imshow(totals[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"failing........... :(","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Some other Augmentation Ideas to share...\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification/blob/master/reports/images/data_augmentation.png?raw=true)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**All 5 different types of data augmentation [vertical (b)/horizontal (c) flips, brightness shift (d), saturation (e)/contrast (f) boost) used at train-time to broaden the data representation beyond limited pre-existing samples, and test-time to ensure a full prediction from the classifier that is unaffected by the orientation or lighting conditions of the scan. Predictions from all 6 variations [including the original (a)] are averaged to obtain the final prediction per sample.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"in bengali.ai competition we tried a lot of different augmentations and centercropping,cutout  these augmentations worked really well in that competition,here i also expect them to  do well...\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification/blob/master/reports/images/multi-scale_io.png?raw=true)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Original RGB image (left), center cropped 448 x 448 x 3 image used to train 3 CNN member models and the further center cropped 224 x 224 x 3 image used to train 2 more CNN member models. Each model learns to classify at a different scale, with the hypothesis that the collective ensemble benefits from a multi-scale input.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Maps\n\nthanks to this amazing solution : [**Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification**](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification/raw/master/reports/images/imgnet_efn.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Features maps derived from the output of the second block of expanded convolutional layers in a pre-trained EfficientNet-B6 with ImageNet weights, after passing an input skin lesion image through the network.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://github.com/anindox8/Ensemble-of-Multi-Scale-CNN-for-Dermatoscopy-Classification/blob/master/reports/images/imgnetplus_efn.png?raw=true)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Features maps derived from the output of the second block of expanded convolutional layers in a finetuned EfficientNet-B6 initialized with ImageNet weights, after passing an input skin lesion image through the network.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Thresholding is a very popular segmentation technique, used for separating an object considered as a foreground from its background. A threshold is a value which has two regions on its either side i.e. below the threshold or above the threshold.\nIn Computer Vision, this technique of thresholding is done on grayscale images. So initially, the image has to be converted in grayscale color space.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n  \n# cv2.cvtColor is applied over the \n# image input with applied parameters \n# to convert the image in grayscale  \nimage = cv2.imread('../input/jpeg-melanoma-768x768/train/ISIC_0232101.jpg')\nimg = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \n  \n# applying different thresholding  \n# techniques on the input image \n# all pixels value above 120 will  \n# be set to 255 \n\nret, thresh = cv2.threshold(img, 120, 255, cv2.THRESH_TOZERO) \n\n  \n# the window showing output images \n# with the corresponding thresholding  \n# techniques applied to the input images \nplt.imshow(thresh) \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's use dataloader from this awesome kernel [[Training CV] Melanoma Starter](https://www.kaggle.com/shonenkov/training-cv-melanoma-starter) of @shonenkov and vizualize some augmentations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_meta = pd.read_csv('../input/melanoma-merged-external-data-512x512-jpeg/folds_13062020.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_meta.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_train_transforms():\n    return A.Compose([\n            A.RandomSizedCrop(min_max_height=(400, 400), height=512, width=512, p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.Resize(height=512, width=512, p=1),\n            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n            ToTensorV2(p=1.0),                  \n        ], p=1.0)\n\ndef get_train_transforms1():\n    return A.Compose([\n\n            A.Resize(height=512, width=512, p=1),\n            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n            ToTensorV2(p=1.0),                  \n        ], p=1.0)\n\ndef get_train_transforms2():\n    return A.Compose([\n\n            A.Resize(height=512, width=512, p=1),\n            A.CenterCrop(256, 256),\n            ToTensorV2(p=1.0),                  \n        ], p=1.0)\n\ndef get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\nDATA_PATH = '../input/melanoma-merged-external-data-512x512-jpeg'\nTRAIN_ROOT_PATH = f'{DATA_PATH}/512x512-dataset-melanoma/512x512-dataset-melanoma'\n\ndef onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, image_ids, labels, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.labels = labels\n        self.transforms = transforms\n\n    def __getitem__(self, idx: int):\n        image_id = self.image_ids[idx]\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        #plt.imshow(image)\n        #image = image.astype(np.float32) / 255.0\n\n        label = self.labels[idx]\n\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        target = onehot(2, label)\n        return image, target\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def get_labels(self):\n        return list(self.labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_folds = pd.read_csv(f'{DATA_PATH}/folds.csv', index_col='image_id')\ntrain_dataset = DatasetRetriever(\n        image_ids=df_folds[df_folds['fold'] != 1].index.values,\n        labels=df_folds[df_folds['fold'] != 1].target.values,\n        transforms=get_train_transforms(),\n    )\n\ntrain_dataset1 = DatasetRetriever(\n        image_ids=df_folds[df_folds['fold'] != 1].index.values,\n        labels=df_folds[df_folds['fold'] != 1].target.values,\n        transforms=get_train_transforms1(),\n    )\ntrain_dataset2 = DatasetRetriever(\n        image_ids=df_folds[df_folds['fold'] != 1].index.values,\n        labels=df_folds[df_folds['fold'] != 1].target.values,\n        transforms=get_train_transforms2(),\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentations @shonenkov using in his kernel [[Training CV] Melanoma Starter](https://www.kaggle.com/shonenkov/training-cv-melanoma-starter)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image, label = train_dataset[0]\nplt.imshow(image.reshape(512,512,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# only cutout with 8 holes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image, label = train_dataset1[0]\nplt.imshow(image.reshape(512,512,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CenterCropping -> height = width = 256","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image, label = train_dataset2[0]\nplt.imshow(image.reshape(256,256,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation with Generative Networks for Medical Imaging","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**this is my first time with GAN, i can make mistakes very easily,correct  me if i am wrong :)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"i uploaded the weights from this awesome repo [lesion-GAN](https://github.com/alxiang/lesion-GAN)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dataset Link [lesion-GAN](https://www.kaggle.com/mobassir/ganweight)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"i am using those weights here for  testing and learning purposes  only :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"the weight file we  are gonna use,implements this awesome paper [**Towards Interpretable Skin Lesion Classification with Deep Learning Models**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7153112/pdf/3203149.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"ABSTRACT from that paper : \n\nSkin disease is a prevalent condition all over the world. Computer vision-based technology for automatic skin lesion\nclassification holds great promise as an effective screening tool for early diagnosis. In this paper, we propose an\naccurate and interpretable deep learning pipeline to achieve such a goal. Comparing with existing research, we would\nlike to highlight the following aspects of our model. 1) Rather than a single model, our approach ensembles a set of\ndeep learning architectures to achieve better classification accuracy; 2) Generative adversarial network (GAN) is\ninvolved in the model training to promote data scale and diversity; 3) Local interpretable model-agnostic explanation\n(LIME) strategy is applied to extract evidence from the skin images to support the classification results. Our\nexperimental results on real-world skin image corpus demonstrate the effectiveness and robustness of our method.\nThe explainability of our model further enhances its applicability in real clinical practice.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir('../input/ganweight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# so what is GAN and  why should we care?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Generative Adversarial Networks (GAN) were first introduced by Ian Goodfellow et al, in 2014 (https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf).\n\nIt was shown that random handwritten digits could be generated from the generator network of a GAN, after training on the MNIST dataset (http://yann.lecun.com/exdb/mnist/).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Lets begin remembering how GANs work:\n\nThe idea behind GANs is that you have two networks, a generator $G$ and a discriminator $D$, competing against each other. The generator makes \"fake\" data to pass to the discriminator. The discriminator also sees real training data and predicts if the data it's received is real or fake.\n\nThe generator is trained to fool the discriminator, it wants to output data that looks as close as possible to real, training data.\nThe discriminator is a classifier that is trained to figure out which data is real and which is fake.\nWhat ends up happening is that the generator learns to make data that is indistinguishable from real data to the discriminator.\n\n![](https://i.ibb.co/jkZPNyz/ref.png)\n\nThe general structure of the GAN that we are using here is shown in the diagram above. The latent sample is a random vector that the generator uses to construct its fake images. This is often called a latent vector and that vector space is called latent space. As the generator trains, it figures out how to map latent vectors to recognizable images that can fool the discriminator.\n\nOnce the generator has trained, we can throw out the discriminator after we are done with training. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv2.imread('../input/melanoma-merged-external-data-512x512-jpeg/512x512-dataset-melanoma/512x512-dataset-melanoma/ISIC_4568001.jpg')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\njson_file = open('../input/ganweight/generator.json', 'r')\ngenerator_json = json_file.read()\njson_file.close()\ngenerator = model_from_json(generator_json)\ngenerator.load_weights('../input/ganweight/generator_weights.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img[:,:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"originals = img\n\n#data = np.load(\".../HAMNOAUG_256.npz\")\n\n#labels = 1\n\n#temp = np.empty((0, 128, 128, 3))\nfor i in range(originals.shape[0]):\n    temp_r = skimage.measure.block_reduce(originals[:,:,0], (4,4), np.mean) # (4,4) = factor of reduction\n    temp_g = skimage.measure.block_reduce(originals[:,:,1], (4,4), np.mean)\n    temp_b = skimage.measure.block_reduce(originals[:,:,2], (4,4), np.mean)\n    temp_rgb = np.stack([temp_r, temp_g, temp_b], axis=-1)\n\n    #temp[i] = temp_rgb    \noriginals = temp_rgb\noriginals /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_sampled_labels = np.concatenate([np.zeros(3), np.ones(3), np.ones(3)+1])\nnn_noise = np.random.normal(0, 1, (9, 128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_imgs = 0.5*generator.predict([nn_noise, nn_sampled_labels]) + 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"originals.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Original","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fake Images :)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_row = 3\nn_col = 3\n_, axs = plt.subplots(n_row, n_col, figsize=(15, 15))\naxs = axs.flatten()\n\nfor i in range(9):\n    axs[i].imshow(gen_imgs[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"you can play with the models implementation from here [ACGAN](https://github.com/alxiang/lesion-GAN/blob/master/ACGAN.ipynb) and probably want to convert it into pytorch. because **THERE IS NOTHING LIKE PYTORCH.** üòä\n\n**PYTORCH IS** ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Thank You For Reading :)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"‡¶à‡¶¶ ‡¶Æ‡ßã‡¶¨‡¶æ‡¶∞‡¶ïüòä","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}