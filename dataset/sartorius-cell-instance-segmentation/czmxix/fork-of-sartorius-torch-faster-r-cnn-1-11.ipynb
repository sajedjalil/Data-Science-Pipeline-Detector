{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ¦  Sartorius - Torch Mask R-CNN\n### A self-contained, Torch Mask R-CNN implementation\n\nAdapted from https://www.kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-202\n\nMain differences to Julian's notebook: \n - use 3 classes for model training\n - use different thresholds for each class\n - use IOUmAP score to select best model\n\n### Changelog\n\n\n| Version | Comments | Validation | LB |\n| --- | --- | --- | --- |\n|51| use CV2 for image processing, set random state in train_test_split | 0.275 | 0.278 |\n|48| fix combine_masks mistake | 0.267 | 0.291 |\n|46| revert cutoffs to V43 | 0.247 | 0.288 |\n|45| update cutoffs | 0.242 | 0.281 |\n|43| update cutoffs | 0.249 | 0.29 |\n|42| BOX_DETECTIONS_PER_IMG = 540 (from Julians notebook) | 0.245 | 0.281 |\n|40| BOX_DETECTIONS_PER_IMG = 450 | 0.245 | 0.28 |\n|39| use different thresholds for each class | 0.242 | 0.279|\n|37| use cell_type as class labels, use best validation epoch using IOU score | 0.241 | 0.274 |\n|28| use cell_type as class labels, use best validation epoch | | 0.265 |\n|26| same as V 16, select correct best model (best_epoch+1) | | 0.274 |\n|16| with `MIN_SCORE=0.5`, use best validation epoch (19) | | 0.263 |\n|11| 30 epochs, use best validation (17) | | 0.203 |\n|5| 10 epochs, Adam optimizer | | 0.135 | \n|1| 8 epochs. With Scheduler. | | 0.197 | \n\n[Julian's](https://www.kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-202) log:\n\n|| Version | Comments | LB |\n|---|  --- | --- | --- |\n||30| Version 18 with `MIN_SCORE=0.5`. Remove validation. | `0.273` |\n||28| V27 but pick best epoch using mask-only validation loss. 18 epochs. | `0.205` |\n||27| V18 + 7.5% validation (`PCT_IMAGES_VALIDATION`) w/best epoch for pred. Added `BOX_DETECTIONS_PER_IMG` and `MIN_SCORE` but not used yet. | `0.178` |\n||24| 8 epochs. With Scheduler. | `0.195` |\n||23| 8 epochs. Mask loss only. | `0.036` |\n||22| 8 epochs. Normalize. (7 epochs = `0.189`) | `0.202`|\n||19| 3 epochs size 25%. 3 epochs size 50%. 6 epochs full sized| `0.178` |\n||18| 8 epochs. Full sized. Tidied-up code.|  `0.202` |\n||15| 12 -> 15 epochs. Setup classification head with classes. Bugfix in `analyze_train_sample`|  `0.172` |\n|| *14* | *12 epochs. Full sized* |`0.173` |\n|| 8 | 12 epochs. Resize to (256, 256) |`0.057` |\n\n","metadata":{"id":"AJ_abxrh0zRR","papermill":{"duration":0.024534,"end_time":"2021-10-30T15:50:04.957023","exception":false,"start_time":"2021-10-30T15:50:04.932489","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"1NZ-_x8E0zRW","papermill":{"duration":0.023251,"end_time":"2021-10-30T15:50:05.006187","exception":false,"start_time":"2021-10-30T15:50:04.982936","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#æ— å¤–éƒ¨ä¾èµ–å…³ç³»ï¼ˆä»…æ¨¡åž‹æƒé‡ï¼‰\n#æˆ‘ä»¬åªä¾èµ–Pytorch\n#å¯¼å…¥å†…éƒ¨å’Œå¤–éƒ¨æ‰€ä¾èµ–çš„åº“ï¼ŒimportåŽå¯¹åº”çš„æ–¹æ³•ä¼šåœ¨ä»£ç ä¸­è¿›è¡Œä½¿ç”¨\nimport os\nimport random\nimport time\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"oQYSI0Y00zRX","papermill":{"duration":2.167855,"end_time":"2021-10-30T15:50:07.197808","exception":false,"start_time":"2021-10-30T15:50:05.029953","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:20.528587Z","iopub.execute_input":"2022-05-19T06:17:20.529182Z","iopub.status.idle":"2022-05-19T06:17:20.536047Z","shell.execute_reply.started":"2022-05-19T06:17:20.529138Z","shell.execute_reply":"2022-05-19T06:17:20.535243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:17:20.553703Z","iopub.execute_input":"2022-05-19T06:17:20.553912Z","iopub.status.idle":"2022-05-19T06:17:20.558355Z","shell.execute_reply.started":"2022-05-19T06:17:20.553887Z","shell.execute_reply":"2022-05-19T06:17:20.55759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fix randomness\n #torchä¸­éšæœºåŒ–çš„è¿‡ç¨‹\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)            # æ¡†æž¶è¿è¡Œæ—¶ä¼šç”Ÿæˆéšæœºç²’å­\n    os.environ['PYTHONHASHSEED'] = str(seed)       #å®šä¹‰seedçš„è·¯å¾„\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n    \nfix_all_seeds(2021)","metadata":{"id":"Y7fwE02H0zRY","papermill":{"duration":0.032827,"end_time":"2021-10-30T15:50:07.253793","exception":false,"start_time":"2021-10-30T15:50:07.220966","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:20.579081Z","iopub.execute_input":"2022-05-19T06:17:20.579451Z","iopub.status.idle":"2022-05-19T06:17:20.586905Z","shell.execute_reply.started":"2022-05-19T06:17:20.579421Z","shell.execute_reply":"2022-05-19T06:17:20.586167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{"id":"NqZ4eNVK0zRZ","papermill":{"duration":0.022674,"end_time":"2021-10-30T15:50:07.29946","exception":false,"start_time":"2021-10-30T15:50:07.276786","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Reduced the train dataset to 5000 rows\nTEST = False\n\nif os.path.exists(\"../input/sartorius-cell-instance-segmentation\"):     # é€šè¿‡osæ–¹æ³•ä¸­çš„è·¯å¾„åˆ¤æ–­å‡½æ•°åˆ¤æ–­æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨\n    # running on kaggle\n    data_directory = '../input/sartorius-cell-instance-segmentation'     #å®šä¹‰äº†ä¸€ä¸ªæ•°æ®æ–‡ä»¶å¤¹ï¼Œæ•°æ®å­˜æ”¾åœ¨è¯¥è·¯å¾„ä¹‹ä¸‹\n    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  #å®šä¹‰äº†ä»£ç æ˜¯ä½¿ç”¨cudaç‰ˆæœ¬è¿˜æ˜¯cpuè¿›è¡ŒæŽ¨ç†\n    BATCH_SIZE = 2   #é€šè¿‡è®­ç»ƒä¸¤å¼ å›¾ç‰‡ï¼Œå¯¹ä¸¤å¼ å›¾ç‰‡çš„æŸå¤±æ±‚å‡å€¼ï¼Œåå‘ä¼ æ’­ï¼Œå¯ä»¥è®©è®­ç»ƒçš„æŸå¤±æ›´åŠ å¹³ç¼“ï¼Œè®­ç»ƒçš„æ¨¡åž‹æ•ˆæžœæ›´å¥½\n    NUM_EPOCHS = 2   #è®¾ç½®è®­ç»ƒè½®æ¬¡=1\n\nelif 'google.colab' in str(get_ipython()):\n    # running on CoLab\n    from google.colab import drive\n    drive.mount('/content/drive')\n    data_directory = '/content/drive/MyDrive/input'\n    DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    BATCH_SIZE = 1\n    NUM_EPOCHS = 5\n    \nelse:\n    data_directory = 'input'\n    DEVICE = torch.device('cpu')\n    BATCH_SIZE = 2\n    NUM_EPOCHS = 1\n    TEST = True\n#if elifå’Œelseæ˜¯ä¸€ä¸ªä¸‰é€‰æ‹©åˆ¤æ–­ï¼Œä¸‰ä¸ªæ¡ä»¶æ»¡è¶³ä¹‹ä¸€ï¼Œè¿è¡Œä¹‹ä¸€ï¼Œä¸‰è€…éƒ½æ˜¯é€šè¿‡æ‰¾data_directoryçš„è·¯å¾„ï¼Œå®šä¹‰æ¡†æž¶æŽ¨ç†æ¨¡å¼ï¼Œè®¾ç½®BATCH_SIZEå’Œè®­ç»ƒè½®æ¬¡\n\nTRAIN_CSV = f\"{data_directory}/train.csv\"   #æ‰¾åˆ°data_directoryæ–‡ä»¶å¤¹åŽï¼Œåœ¨è¯¥è·¯å¾„ä¸‹ä¼šæœ‰train.csvæ–‡ä»¶ï¼Œé€šè¿‡åŽè¾¹çš„è·¯å¾„å¯ä»¥æ‰¾åˆ°è®­ç»ƒæ–‡ä»¶æ‰€åœ¨è·¯å¾„\nTRAIN_PATH = f\"{data_directory}/train\"    #åŽŸç†åŒä¸Šï¼Œæ‰¾åˆ°è®­ç»ƒæ–‡ä»¶æ‰€åœ¨è·¯å¾„\nTEST_PATH = f\"{data_directory}/test\"      #åŽŸç†åŒä¸Šï¼Œæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶æ‰€åœ¨è·¯å¾„\n\n#è®¾ç½®å›¾ç‰‡çš„å®½åº¦å’Œé«˜åº¦ï¼Œå¯¹ä¸åŒå¤§å°çš„å›¾ç‰‡åœ¨è¿›å…¥ç½‘ç»œä¹‹å‰è¿›è¡Œç»Ÿä¸€å¤„ç†\nWIDTH = 704\nHEIGHT = 520\n\n#å¯¹æ•°æ®é›†è¿›è¡Œå›¾åƒå¢žå¼ºçš„å¤„ç†\nresize_factor = 0.5 # 0.5  å¦‚æžœæ˜¯tureçš„è¯ï¼ŒåŽè¾¹æ ‡çš„0.5ç›¸å½“äºŽå¯¹çŽ°æœ‰çš„æ•°æ®é›†è¿›è¡Œé•¿å®½ç¼©å°ä¸€åŠçš„æ“ä½œï¼Œæ··åˆåˆ°åŽŸæœ‰æ•°æ®é›†å…±åŒå‚åŠ è®­ç»ƒï¼Œä¸°å¯Œæ•°æ®æ ·æœ¬çš„è¿‡ç¨‹\n\n# Normalize to resnet mean and std if True.\nNORMALIZE = False    #æ ‡å‡†åŒ–çš„ä¸€ä¸ªè¿‡ç¨‹ï¼Œæ˜¯ä¸€ä¸ªå‚æ•°è®¾ç½®ä¸ºfalse\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)     #æ ‡å‡†å·®è¿›è¡Œåˆå§‹åŒ–\n\n# No changes tried with the optimizer yet.\nMOMENTUM = 0.9   #ä½¿ç”¨stdéšæœºåˆå§‹åŒ–çš„ä¼˜åŒ–å™¨æ–¹å¼å†åŠ ä¸€ä¸ªåŠ¨é‡è¿›åŽ»ï¼ŒåŠ¨é‡æ˜¯0.9\n#éšæœºåˆå§‹åŒ–åŠ åŠ¨é‡çš„åŽŸå› æ˜¯ï¼šåœ¨ä¸‹é™è¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒçš„è¿‡ç¨‹ä¸­ä¼šé™·å…¥åˆ°å±€éƒ¨æœ€ä¼˜ï¼Œå¹¶ä¸èƒ½å¾—åˆ°å…¨å±€æœ€ä¼˜ï¼ŒåŠ å…¥åŠ¨é‡æ˜¯é‡‡ç”¨ç‰©ç†å­¦çš„æ–¹æ³•ï¼Œåœ¨ä¸‹é™çš„è¿‡ç¨‹å¸¦æœ‰ä¸€å®šçš„æƒ¯æ€§ï¼Œå°½é‡å®žçŽ°å…¨å±€æœ€ä¼˜\nLEARNING_RATE = 0.001  #å­¦ä¹ çŽ‡ï¼Œè®­ç»ƒä¸€æ¬¡ï¼Œè¿›è¡Œæ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ï¼Œä½¿ç”¨å›žå½’çš„æŸå¤±å‡½æ•°*å­¦ä¹ çŽ‡*WEIGHT_DECAYå‚æ•°ï¼Œè°ƒæ•´å­¦ä¹ çŽ‡çš„å¤§å°ï¼Œé€šè¿‡æ–œçŽ‡è§‚å¯Ÿæ¢¯åº¦çš„æ–¹å‘\nWEIGHT_DECAY = 0.01\n\n# Changes the confidence required for a pixel to be kept for a mask. \n# Only used 0.5 till now.\n# MASK_THRESHOLD = 0.5\n# MIN_SCORE = 0.5\n# cell type specific thresholds\ncell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}     #ç»†èƒžåç§°\nmask_threshold_dict = {1: 0.6, 2: 0.8, 3:  0.7}\nmin_score_dict = {1: 0.6, 2: 0.8, 3: 0.7}     #å¯¹æ¯ä¸€ç±»é˜ˆå€¼è¿›è¡Œé™åˆ¶\n\n# Use a StepLR scheduler if True. \nUSE_SCHEDULER = False\n\nPCT_IMAGES_VALIDATION = 0.075\n\nBOX_DETECTIONS_PER_IMG = 540","metadata":{"id":"VSPe6quz0zRZ","lines_to_next_cell":1,"outputId":"e2cca0e2-0ada-471b-e688-33ee16049407","papermill":{"duration":0.071974,"end_time":"2021-10-30T15:50:07.394165","exception":false,"start_time":"2021-10-30T15:50:07.322191","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:20.605232Z","iopub.execute_input":"2022-05-19T06:17:20.605663Z","iopub.status.idle":"2022-05-19T06:17:20.617376Z","shell.execute_reply.started":"2022-05-19T06:17:20.605633Z","shell.execute_reply":"2022-05-19T06:17:20.616487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities","metadata":{"id":"iIpvad7y0zRb","papermill":{"duration":0.022537,"end_time":"2021-10-30T15:50:07.439624","exception":false,"start_time":"2021-10-30T15:50:07.417087","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height, width, channels) of array to return\n    color: color for the mask\n    Returns numpy array (mask)\n\n    '''\n    s = mask_rle.split()\n\n    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n    lengths = list(map(int, s[1::2]))\n    ends = [x + y for x, y in zip(starts, lengths)]\n    if len(shape)==3:\n        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    else:\n        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for start, end in zip(starts, ends):\n        img[start : end] = color\n\n    return img.reshape(shape)   #å¯¹mask_rle, shape, colorè¿›è¡Œå¤„ç†ï¼Œè¿”å›žimg.reshapeï¼Œå¯¹å›¾ç‰‡è¿›è¡Œæ ¼å¼è°ƒæ•´\n\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask\n\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((HEIGHT, WIDTH))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n        maskimg[mask>mask_threshold] = m\n    return maskimg\n\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    for i, mask in enumerate(pred[\"masks\"]):                                   \n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n\n    return use_masks\n","metadata":{"papermill":{"duration":0.034074,"end_time":"2021-10-27T04:04:40.693264","exception":false,"start_time":"2021-10-27T04:04:40.65919","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:20.780221Z","iopub.execute_input":"2022-05-19T06:17:20.780777Z","iopub.status.idle":"2022-05-19T06:17:20.79778Z","shell.execute_reply.started":"2022-05-19T06:17:20.780738Z","shell.execute_reply":"2022-05-19T06:17:20.796931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metric: mean of the precision values at each IoU threshold\n\nRef: https://www.kaggle.com/theoviel/competition-metric-map-iou","metadata":{"papermill":{"duration":0.022763,"end_time":"2021-10-30T15:50:07.545798","exception":false,"start_time":"2021-10-30T15:50:07.523035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh):\n    \"\"\"\n    Applies non max supression and eliminates low score bounding boxes.\n\n      Args:\n        orig_prediction: the model output. A dictionary containing element scores and boxes.\n        iou_thresh: Intersection over Union threshold. Every bbox prediction with an IoU greater than this value\n                      gets deleted in NMS.\n\n      Returns:\n        final_prediction: Resulting prediction\n    \"\"\"\n\n    # torchvision returns the indices of the bboxes to keep\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n\n    # Keep indices from nms\n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n\n    return final_prediction\n\ndef IOU(box1, box2):\n\n   #è®¡ç®—äº¤ç‚¹åæ ‡\n    xmin_inter = max(box1[0], box2[0])\n    ymin_inter = max(box1[1], box2[1])\n    xmax_inter = min(box1[2], box2[2])\n    ymax_inter = min(box1[3], box2[3])\n\n    #è®¡ç®—ç›¸äº¤çŸ©å½¢çš„é¢ç§¯\n    inter_area = max(0, xmax_inter - xmin_inter + 1) * max(0, ymax_inter - ymin_inter + 1) # FIXME why plus one?\n \n    #è®¡ç®—boxesé¢ç§¯\n    area1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n    area2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n \n    #è®¡ç®—IoU\n    iou = inter_area / float(area1 + area2 - inter_area)\n    assert iou >= 0\n    return iou\n\ndef compute_AP(ground_truth, predictions, iou_thresh=0.5, n_classes=3,score_thred=0.5):\n    \n    \"\"\"\n    #è®¡ç®—æ‰€æœ‰ç±»çš„å¹³å‡ç²¾åº¦ã€‚\n\n    Args:\n        ground_truth: éœ€è¦å…·æœ‰ä»¥ä¸‹æ ¼å¼ï¼š[åºåˆ—ã€å¸§ã€objã€[xminã€yminã€xmaxã€ymax]ï¼Œæ ‡ç­¾ã€åˆ†æ•°]\n        predictions: åˆ—å‡ºé¢„æµ‹å¯¹è±¡ã€‚éœ€è¦å…·æœ‰ä»¥ä¸‹æ ¼å¼ï¼š[åºåˆ—ã€å¸§ã€objã€[xminã€yminã€xmaxã€ymax]ï¼Œæ ‡ç­¾ã€åˆ†æ•°]\n        iou_thresh: å°†é¢„æµ‹ä¸Žåœ°é¢çœŸç›¸ç›¸æ¯”è¾ƒè¢«è®¤ä¸ºæ˜¯æ­£ç¡®çš„iouã€‚\n        n_classes: çŽ°æœ‰ç±»çš„æ•°é‡\n    Returns:\n        Average precision for the specified threshold.\n    \"\"\"\n   #åˆå§‹åŒ–åˆ—è¡¨\n    APs = []\n    class_gt = []\n    class_predictions = []\n\n    # æ¯ä¸€ç±»è®¡ç®—çš„AP\n    for c in range(n_classes):\n        # Find gt and predictions of the class\n        for gt in ground_truth:\n            if gt[4] == c:\n                class_gt.append(gt)\n        for predict in predictions:\n            if predict[4] == c:\n                class_predictions.append(predict)\n\n        # åœ¨æ¯å¼ å›¾åƒä¸­ä¸ºbbåˆ›å»ºå¸¦æœ‰é›¶æ•°ç»„çš„dict\n        gt_amount_bb = Counter([gt[1] for gt in class_gt])\n        for key, val in gt_amount_bb.items():\n            gt_amount_bb[key] = np.zeros(val)\n\n        # æŒ‰åˆ†æ•°å¯¹ç±»é¢„æµ‹è¿›è¡ŒæŽ’åº\n        class_predictions = sorted(class_predictions, key=lambda x: x[5], reverse=True)\n\n        # ä¸º Positives åˆ›å»ºæ•°ç»„(True and False)\n        TP = np.zeros(len(class_predictions))\n        FP = np.zeros(len(class_predictions))\n        # Number of true boxes\n        truth = len(class_gt)\n\n        # åˆå§‹åŒ–ä¸ºå˜é‡\n        epsilon = 1e-6\n\n        # åœ¨æ¯å¼ å›¾ç‰‡ä¸­é‡å¤é¢„æµ‹ï¼Œå¹¶ä¸Žground truthsè¿›è¡Œæ¯”è¾ƒ\n        for predict_idx, prediction in enumerate(class_predictions):\n            # Filter prediction image ground truths\n            image_gt = [obj for obj in class_gt if obj[1] == prediction[1]]\n\n            # åˆå§‹åŒ–ä¸ºå˜é‡\n            best_iou = -1\n            best_gt_iou_idx = -1\n\n            # åå¤åˆ†æžå›¾åƒä¸­çš„ground truthså¹¶è®¡ç®—å‡ºIoUs\n            for gt_idx, gt in enumerate(image_gt):\n                iou = IOU(prediction[3], gt[3])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_iou_idx = gt_idx\n\n            # å¦‚æžœæœ€ä½³IoUå¤§äºŽé˜ˆå€¼ï¼Œåˆ™å·²æ‰¾åˆ°TPé¢„æµ‹\n            if best_iou > iou_thresh and best_gt_iou_idx > -1:\n                # Check if gt box was already covered\n                if  gt_amount_bb[prediction[1]][best_gt_iou_idx] == 0:\n                    gt_amount_bb[prediction[1]][best_gt_iou_idx] = 1  # set as covered\n                    TP[predict_idx] = 1  # Count as true positive\n                else:\n                    FP[predict_idx] = 1\n            else:\n                FP[predict_idx] = 1\n\n        # è®¡ç®—å¬å›žçŽ‡å’Œå‡†ç¡®çŽ‡\n        TP_cumsum = np.cumsum(TP)\n        FP_cumsum = np.cumsum(FP)\n        recall = np.append([0], TP_cumsum / (truth + epsilon))\n        precision = np.append([1], np.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon)))\n\n        # è®¡ç®—åŒºåŸŸç²¾åº¦/å¬å›žçŽ‡å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­\n        APs.append(np.trapz(precision, recall))\n\n    return sum(APs)/len(APs) # average of class precisions\n\n\ndef compute_mAP(ground_truth, predictions, n_classes):\n    \"\"\"\n    Calls AP computation for different levels of IoUs, [0.5:.05:0.95].\n\n    Args:\n        ground_truth: list with ground-truth objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n        predictions: list with predictions objects. Needs to have the following format: [sequence, frame, obj, [xmin, ymin, xmax, ymax], label, score]\n        n_classes: number of existent classes.\n\n    Returns:\n        mAp and list with APs for each IoU threshold.\n    \"\"\"\n    iou_thresh=0.4\n    score_thresh=0.8\n    APs=[]\n    \n    # return mAP\n    for iou_thresh in np.arange(0.5,0.95,0.05):\n        APs.append(compute_AP(ground_truth, predictions, iou_thresh, n_classes,score_thresh))\n    #APs=[compute_AP(ground_truth, predictions, iou_thresh, n_classes,score_thresh) for iou_thresh in np.arange(0.5,1.0,0.0.5)]\n    \n    #APs = compute_AP(ground_truth, predictions, iou_thresh, n_classes)\n    return np.mean(APs), APs\n","metadata":{"papermill":{"duration":0.042481,"end_time":"2021-10-30T15:50:07.612219","exception":false,"start_time":"2021-10-30T15:50:07.569738","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:20.813138Z","iopub.execute_input":"2022-05-19T06:17:20.813647Z","iopub.status.idle":"2022-05-19T06:17:20.839551Z","shell.execute_reply.started":"2022-05-19T06:17:20.813606Z","shell.execute_reply":"2022-05-19T06:17:20.838819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nAuthor: Roman Solovyev, IPPM RAS\nURL: https://github.com/ZFTurbo\n\nCode based on: https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/eval.py\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\n# try:\n#     import pyximport\n#     pyximport.install(setup_args={\"include_dirs\": np.get_include()}, reload_support=False)\n#     from .compute_overlap import compute_overlap\n# except:\n#     print(\"Couldn't import fast version of function compute_overlap, will use slow one. Check cython intallation\")\n#     from .compute_overlap_slow import compute_overlap\ndef compute_overlap(boxes, query_boxes):\n    \"\"\"\n    Args\n        a: (N, 4) ndarray of float\n        b: (K, 4) ndarray of float\n\n    Returns\n        overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    \"\"\"\n    N = boxes.shape[0]\n    K = query_boxes.shape[0]\n    overlaps = np.zeros((N, K), dtype=np.float64)\n    for k in range(K):\n        box_area = (\n            (query_boxes[k, 2] - query_boxes[k, 0]) *\n            (query_boxes[k, 3] - query_boxes[k, 1])\n        )\n        for n in range(N):\n            iw = (\n                min(boxes[n, 2], query_boxes[k, 2]) -\n                max(boxes[n, 0], query_boxes[k, 0])\n            )\n            if iw > 0:\n                ih = (\n                    min(boxes[n, 3], query_boxes[k, 3]) -\n                    max(boxes[n, 1], query_boxes[k, 1])\n                )\n                if ih > 0:\n                    ua = np.float64(\n                        (boxes[n, 2] - boxes[n, 0]) *\n                        (boxes[n, 3] - boxes[n, 1]) +\n                        box_area - iw * ih\n                    )\n                    overlaps[n, k] = iw * ih / ua\n    return overlaps\n\ndef get_real_annotations(table):\n    res = dict()\n    ids = table['ImageID'].values.astype(np.str)\n    labels = table['LabelName'].values.astype(np.str)\n    xmin = table['XMin'].values.astype(np.float32)\n    xmax = table['XMax'].values.astype(np.float32)\n    ymin = table['YMin'].values.astype(np.float32)\n    ymax = table['YMax'].values.astype(np.float32)\n\n    for i in range(len(ids)):\n        id = ids[i]\n        label = labels[i]\n        if id not in res:\n            res[id] = dict()\n        if label not in res[id]:\n            res[id][label] = []\n        box = [xmin[i], ymin[i], xmax[i], ymax[i]]\n        res[id][label].append(box)\n\n    return res\n\n\ndef get_detections(table):\n    res = dict()\n    ids = table['ImageID'].values.astype(np.str)\n    labels = table['LabelName'].values.astype(np.str)\n    scores = table['Conf'].values.astype(np.float32)\n    xmin = table['XMin'].values.astype(np.float32)\n    xmax = table['XMax'].values.astype(np.float32)\n    ymin = table['YMin'].values.astype(np.float32)\n    ymax = table['YMax'].values.astype(np.float32)\n\n    for i in range(len(ids)):\n        id = ids[i]\n        label = labels[i]\n        if id not in res:\n            res[id] = dict()\n        if label not in res[id]:\n            res[id][label] = []\n        box = [xmin[i], ymin[i], xmax[i], ymax[i], scores[i]]\n        res[id][label].append(box)\n\n    return res\n\n\ndef _compute_ap(recall, precision):\n    \"\"\" Compute the average precision, given the recall and precision curves.\n\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    \"\"\"\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], recall, [1.]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef mean_average_precision_for_boxes(ann, pred, iou_threshold=0.5, exclude_not_in_annotations=False, verbose=True):\n    \"\"\"\n\n    :param ann: path to CSV-file with annotations or numpy array of shape (N, 6)\n    :param pred: path to CSV-file with predictions (detections) or numpy array of shape (N, 7)\n    :param iou_threshold: IoU between boxes which count as 'match'. Default: 0.5\n    :param exclude_not_in_annotations: exclude image IDs which are not exist in annotations. Default: False\n    :param verbose: print detailed run info. Default: True\n    :return: tuple, where first value is mAP and second values is dict with AP for each class.\n    \"\"\"\n    \n    if isinstance(ann, str):\n        valid = pd.read_csv(ann)\n    else:\n        valid = pd.DataFrame(ann, columns=['ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax'])\n\n    if isinstance(pred, str):\n        preds = pd.read_csv(pred)\n    else:\n        preds = pd.DataFrame(pred, columns=['ImageID', 'LabelName', 'Conf', 'XMin', 'XMax', 'YMin', 'YMax'])\n\n    ann_unique = valid['ImageID'].unique()\n    preds_unique = preds['ImageID'].unique()\n\n    if verbose:\n        print('Number of files in annotations: {}'.format(len(ann_unique)))\n        print('Number of files in predictions: {}'.format(len(preds_unique)))\n\n    # Exclude files not in annotations!\n    if exclude_not_in_annotations:\n        preds = preds[preds['ImageID'].isin(ann_unique)]\n        preds_unique = preds['ImageID'].unique()\n        if verbose:\n            print('Number of files in detection after reduction: {}'.format(len(preds_unique)))\n\n    unique_classes = valid['LabelName'].unique().astype(np.str)\n    if verbose:\n        print('Unique classes: {}'.format(len(unique_classes)))\n\n    all_detections = get_detections(preds)\n    all_annotations = get_real_annotations(valid)\n    if verbose:\n        print('Detections length: {}'.format(len(all_detections)))\n        print('Annotations length: {}'.format(len(all_annotations)))\n\n    average_precisions = {}\n    for zz, label in enumerate(sorted(unique_classes)):\n\n        # Negative class\n        if str(label) == 'nan':\n            continue\n\n        false_positives = []\n        true_positives = []\n        scores = []\n        num_annotations = 0.0\n\n        for i in range(len(ann_unique)):\n#             print('iter_{} '.format(i))\n            detections = []\n            annotations = []\n            id = str(ann_unique[i])\n            if id in all_detections:\n                if label in all_detections[id]:\n                    detections = all_detections[id][label]\n            if id in all_annotations:\n                if label in all_annotations[id]:\n                    annotations = all_annotations[id][label]\n\n            if len(detections) == 0 and len(annotations) == 0:\n                continue\n\n            num_annotations += len(annotations)\n            detected_annotations = []\n\n            annotations = np.array(annotations, dtype=np.float64)\n            for d in detections:\n                scores.append(d[4])\n\n                if len(annotations) == 0:\n                    false_positives.append(1)\n                    true_positives.append(0)\n                    continue\n\n                overlaps = compute_overlap(np.expand_dims(np.array(d, dtype=np.float64), axis=0), annotations)\n                assigned_annotation = np.argmax(overlaps, axis=1)\n                max_overlap = overlaps[0, assigned_annotation]\n\n                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n                    false_positives.append(0)\n                    true_positives.append(1)\n                    detected_annotations.append(assigned_annotation)\n                else:\n                    false_positives.append(1)\n                    true_positives.append(0)\n\n        if num_annotations == 0:\n            average_precisions[label] = 0, 0\n            continue\n\n        false_positives = np.array(false_positives)\n        true_positives = np.array(true_positives)\n        scores = np.array(scores)\n#         print('ddd1 '*10)\n        # sort by score\n        indices = np.argsort(-scores)\n        false_positives = false_positives[indices]\n        true_positives = true_positives[indices]\n#         print('ddd2 '*10)\n        # compute false positives and true positives\n        false_positives = np.cumsum(false_positives)\n        true_positives = np.cumsum(true_positives)\n#         print('ddd3 '*10)\n        # compute recall and precision\n        recall = true_positives / num_annotations\n        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n#         print('ddd4 '*10)\n        # compute average precision\n        average_precision = _compute_ap(recall, precision)\n        average_precisions[label] = average_precision, num_annotations\n        if verbose:\n            s1 = \"{:30s} | {:.6f} | {:7d}\".format(label, average_precision, int(num_annotations))\n            print(s1)\n\n    present_classes = 0\n    precision = 0\n    for label, (average_precision, num_annotations) in average_precisions.items():\n        if num_annotations > 0:\n            present_classes += 1\n            precision += average_precision\n    mean_ap = precision / present_classes\n    if verbose:\n        print('mAP: {:.6f}'.format(mean_ap))\n    return mean_ap, average_precisions\n","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:17:20.857581Z","iopub.execute_input":"2022-05-19T06:17:20.858177Z","iopub.status.idle":"2022-05-19T06:17:20.90132Z","shell.execute_reply.started":"2022-05-19T06:17:20.858134Z","shell.execute_reply":"2022-05-19T06:17:20.900439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, data_loader, device, sequences=1):\n   \n    # Set evaluation mode flag\n    model.eval()\n    # Create list with all object detection -> [set, frame, obj, [xmin,ymin,xmax,ymax], label, score]\n    ground_truth = []\n    predictions = []\n\n    # Gather all targets and outputs on test set\n    for image, targets in data_loader:\n        image = [img.to(device) for img in image]\n        outputs = model(image)\n        for idx in range(len(outputs)):\n            outputs[idx] = apply_nms(outputs[idx], iou_thresh=0.3)\n\n        # create list for targets and outputs to pass to compute_mAP()\n        # lists have the following structure:  [sequence, frame, obj_idx, [xmin, ymin, xmax, ymax], label, score]\n        for s in range(sequences):\n            obj_gt = 0\n            obj_target = 0\n            for out, target in zip(outputs, targets):\n\n                for i in range(len(target['boxes'])):\n                    ground_truth.append([s, target['image_id'].detach().cpu().numpy()[0], obj_target,\n                                         target['boxes'].detach().cpu().numpy()[i],\n                                         target['labels'].detach().cpu().numpy()[i], 1])\n                    obj_target += 1\n\n                for j in range(len(out['boxes'])):\n                    predictions.append([s, target['image_id'].detach().cpu().numpy()[0], obj_gt,\n                                        out['boxes'].detach().cpu().numpy()[j],\n                                        out['labels'].detach().cpu().numpy()[j],\n                                        out['scores'].detach().cpu().numpy()[j]])\n                    obj_gt += 1\n#     print(ground_truth[:5])\n#     print(predictions[:5])\n    ann=[]\n    pre=[]\n    for gt_info in ground_truth:\n        se,imageid,_,box,label_indx,confi=gt_info\n        box_XMin,box_YMin,box_XMax,box_YMax=box.tolist()\n        ann.append([imageid,str(label_indx),box_XMin,box_XMax,box_YMin,box_YMax])\n    for predict_info in predictions:\n        se,imageid,_,box,label_indx,confi=predict_info\n        box_XMin,box_YMin,box_XMax,box_YMax=box.tolist()\n        pre.append([imageid,str(label_indx),confi,box_XMin,box_XMax,box_YMin,box_YMax])\n#     print('$$'*10)\n#     print(ann)\n#     print('$$'*10)\n#     print(pre)\n    mAP, AP = mean_average_precision_for_boxes(ann, pre,iou_threshold=0.3)\n#     print('*'*10)\n    print(\"mAP:{:.3f}\".format(mAP))\n    print(\"AP:{}\".format(AP))#AP is dict with AP for each class\n   # for ap_metric, iou in zip(AP, np.arange(0.0, 1, 0.1)):\n     #    print(\"\\tAP at IoU level [{:.2f}]: {:.3f}\".format(iou, ap_metric))\n\n    return mAP, AP","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:17:20.905416Z","iopub.execute_input":"2022-05-19T06:17:20.905955Z","iopub.status.idle":"2022-05-19T06:17:20.923121Z","shell.execute_reply.started":"2022-05-19T06:17:20.905913Z","shell.execute_reply":"2022-05-19T06:17:20.922293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformations\nJust Horizontal and Vertical Flip for now.\n\nNormalization to Resnet's mean and std can be performed using the parameter `NORMALIZE` in the top cell.\n\nThe first 3 transformations come from [this](https://www.kaggle.com/abhishek/maskrcnn-utils) utils package by Abishek, `VerticalFlip` is my adaption of HorizontalFlip, and `Normalize` is of my own.","metadata":{"papermill":{"duration":0.023883,"end_time":"2021-10-30T15:50:07.659293","exception":false,"start_time":"2021-10-30T15:50:07.63541","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# These are slight redefinitions of torch.transformation classes\n# The difference is that they handle the target and the mask\n# Copied from Abishek, added new ones\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:               #åž‚ç›´ç¿»è½¬\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n           # target[\"masks\"] = target[\"masks\"].flip(-2)   #<-- æ•°æ®å¢žå¼ºæ–¹æ³•è°ƒç”¨datasetï¼Œdatasetä¸­æˆ‘ä»¬å°†masksæ³¨é”€æŽ‰äº†ï¼Œè¿™éƒ¨åˆ†åœ¨ä¸‹ä¸€ä¸ªcellä¸­ï¼ŒæŽ¥æ”¶çš„targetæ²¡æœ‰masksè¿™ä¸€æ ·ï¼Œè¿™é‡Œä¹Ÿéœ€è¦æ³¨é”€æŽ‰\n        return image, target\n\nclass HorizontalFlip:         #æ°´å¹³ç¿»è½¬\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n           # target[\"masks\"] = target[\"masks\"].flip(-1)   #<-- æ•°æ®å¢žå¼ºæ–¹æ³•è°ƒç”¨datasetï¼Œdatasetä¸­æˆ‘ä»¬å°†masksæ³¨é”€æŽ‰äº†ï¼Œè¿™éƒ¨åˆ†åœ¨ä¸‹ä¸€ä¸ªcellä¸­ï¼ŒæŽ¥æ”¶çš„targetæ²¡æœ‰masksè¿™ä¸€æ ·ï¼Œè¿™é‡Œä¹Ÿéœ€è¦æ³¨é”€æŽ‰\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n    \n\ndef get_transform(train):\n    transforms = [ToTensor()]\n    if NORMALIZE:\n        transforms.append(Normalize())\n    \n    # è®­ç»ƒæ•°æ®æ‰©å……\n    if train: \n        transforms.append(HorizontalFlip(0.5))\n        transforms.append(VerticalFlip(0.5))\n\n    return Compose(transforms)","metadata":{"papermill":{"duration":0.038125,"end_time":"2021-10-30T15:50:07.72041","exception":false,"start_time":"2021-10-30T15:50:07.682285","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:20.924856Z","iopub.execute_input":"2022-05-19T06:17:20.925456Z","iopub.status.idle":"2022-05-19T06:17:20.939945Z","shell.execute_reply.started":"2022-05-19T06:17:20.925414Z","shell.execute_reply":"2022-05-19T06:17:20.939147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Dataset and DataLoader","metadata":{"id":"hHT_aovU0zRd","papermill":{"duration":0.022607,"end_time":"2021-10-30T15:50:07.76565","exception":false,"start_time":"2021-10-30T15:50:07.743043","status":"completed"},"tags":[]}},{"cell_type":"code","source":"cell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n\nclass CellDataset(Dataset):\n    def __init__(self, image_dir, df, transforms=None, resize=False):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        \n        self.should_resize = resize is not False\n        if self.should_resize:\n            self.height = int(HEIGHT * resize)\n            self.width = int(WIDTH * resize)\n            print(\"image size used:\", self.height, self.width)\n        else:\n            self.height = HEIGHT\n            self.width = WIDTH\n        \n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby([\"id\", \"cell_type\"])['annotation'].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            self.image_info[index] = {\n                    'image_id': row['id'],\n                    'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n                    'annotations': list(row[\"annotation\"]),\n                    'cell_type': cell_type_dict[row[\"cell_type\"]]\n                    }\n            \n    def get_box(self, a_mask):\n        ''' Get the bounding box of a given mask '''\n        pos = np.where(a_mask)\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n        return [xmin, ymin, xmax, ymax]\n\n    def __getitem__(self, idx):\n        ''' Get the image and the target'''\n        \n        img_path = self.image_info[idx][\"image_path\"]\n        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        \n        if self.should_resize:\n            img = cv2.resize(img, (self.width, self.height))\n\n        info = self.image_info[idx]\n\n        n_objects = len(info['annotations'])\n        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n        labels = []\n        for i, annotation in enumerate(info['annotations']):\n            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n            \n            if self.should_resize:\n                a_mask = cv2.resize(a_mask, (self.width, self.height))\n            \n            a_mask = np.array(a_mask) > 0\n            masks[i, :, :] = a_mask\n            \n            boxes.append(self.get_box(a_mask))\n\n        # labels\n        labels = [int(info[\"cell_type\"]) for _ in range(n_objects)]\n        #labels = [1 for _ in range(n_objects)]\n        \n        \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)  #<-- å°† masks æ³¨é”€åªè¿›è¡Œboxé¢„æµ‹ \n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n\n        # This is the required target for the Mask R-CNN\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n          #  'masks': masks,   #<-- å°† masks ä»Žè¾“å‡ºç»“æž„ä½“ä¸­åˆ é™¤ï¼Œè¿™é‡Œå°†masksè¿™ä¸€KeyåŽ»æŽ‰äº†ï¼Œä¸Šé¢çš„cellä¸­æ•°æ®å¢žå¼ºä¸èƒ½ä½¿ç”¨mask\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_info)","metadata":{"id":"C9Y03YgA0zRd","papermill":{"duration":0.044667,"end_time":"2021-10-30T15:50:07.833528","exception":false,"start_time":"2021-10-30T15:50:07.788861","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.033552Z","iopub.execute_input":"2022-05-19T06:17:21.034056Z","iopub.status.idle":"2022-05-19T06:17:21.056709Z","shell.execute_reply.started":"2022-05-19T06:17:21.033989Z","shell.execute_reply":"2022-05-19T06:17:21.055645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_base = pd.read_csv(TRAIN_CSV, nrows=5000 if TEST else None)","metadata":{"id":"tmCw3DTL0zRe","papermill":{"duration":0.594812,"end_time":"2021-10-30T15:50:08.451409","exception":false,"start_time":"2021-10-30T15:50:07.856597","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.060861Z","iopub.execute_input":"2022-05-19T06:17:21.06118Z","iopub.status.idle":"2022-05-19T06:17:21.380309Z","shell.execute_reply.started":"2022-05-19T06:17:21.061141Z","shell.execute_reply":"2022-05-19T06:17:21.378782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_images = df_base.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n\nfor ct in cell_type_dict:\n    ctdf = df_images[df_images[\"cell_type\"]==ct].copy()\n    if len(ctdf)>0:\n        ctdf['quantiles'] = pd.qcut(ctdf['annotation'], 5)\n        display(ctdf.head())","metadata":{"id":"pQXGwdnL0zRe","outputId":"08e892c7-aa0c-4b96-de86-7eb8a4234f16","papermill":{"duration":0.151173,"end_time":"2021-10-30T15:50:08.629709","exception":false,"start_time":"2021-10-30T15:50:08.478536","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.382015Z","iopub.execute_input":"2022-05-19T06:17:21.382527Z","iopub.status.idle":"2022-05-19T06:17:21.488412Z","shell.execute_reply.started":"2022-05-19T06:17:21.382487Z","shell.execute_reply":"2022-05-19T06:17:21.487647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_images.groupby(\"cell_type\").annotation.describe().astype(int)","metadata":{"id":"DQeF1Zi00zRf","outputId":"af89f7a7-245f-4583-85e3-4f4872c62171","papermill":{"duration":0.053443,"end_time":"2021-10-30T15:50:08.713717","exception":false,"start_time":"2021-10-30T15:50:08.660274","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.492575Z","iopub.execute_input":"2022-05-19T06:17:21.494631Z","iopub.status.idle":"2022-05-19T06:17:21.521897Z","shell.execute_reply.started":"2022-05-19T06:17:21.494587Z","shell.execute_reply":"2022-05-19T06:17:21.520882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We used this as a reference to fill BOX_DETECTIONS_PER_IMG=140\ndf_images[['annotation']].describe().astype(int)","metadata":{"id":"oCRxcK2f0zRf","outputId":"ea5a6673-5758-4964-cdde-03235c5c8215","papermill":{"duration":0.041853,"end_time":"2021-10-30T15:50:08.783532","exception":false,"start_time":"2021-10-30T15:50:08.741679","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.524515Z","iopub.execute_input":"2022-05-19T06:17:21.526174Z","iopub.status.idle":"2022-05-19T06:17:21.545874Z","shell.execute_reply.started":"2022-05-19T06:17:21.526125Z","shell.execute_reply":"2022-05-19T06:17:21.545058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the quantiles of amoount of annotations to stratify\ndf_images_train, df_images_val = train_test_split(df_images, stratify=df_images['cell_type'], \n                                                  test_size=PCT_IMAGES_VALIDATION,\n                                                  random_state=1234)\ndf_train = df_base[df_base['id'].isin(df_images_train['id'])]\ndf_val = df_base[df_base['id'].isin(df_images_val['id'])]\nprint(f\"Images in train set:           {len(df_images_train)}\")\nprint(f\"Annotations in train set:      {len(df_train)}\")\nprint(f\"Images in validation set:      {len(df_images_val)}\")\nprint(f\"Annotations in validation set: {len(df_val)}\")","metadata":{"id":"2v7VvtTp0zRf","outputId":"82690b02-dd4b-4c1d-ed5d-78e30bf084a2","papermill":{"duration":0.057122,"end_time":"2021-10-30T15:50:08.872848","exception":false,"start_time":"2021-10-30T15:50:08.815726","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.547084Z","iopub.execute_input":"2022-05-19T06:17:21.547533Z","iopub.status.idle":"2022-05-19T06:17:21.593567Z","shell.execute_reply.started":"2022-05-19T06:17:21.547492Z","shell.execute_reply":"2022-05-19T06:17:21.592721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train = CellDataset(TRAIN_PATH, df_train, resize=resize_factor, transforms=get_transform(train=True))\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n\nds_val = CellDataset(TRAIN_PATH, df_val, resize=resize_factor, transforms=get_transform(train=False))\ndl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n                    num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"id":"kUcpAbdO0zRg","papermill":{"duration":0.113642,"end_time":"2021-10-30T15:50:09.011921","exception":false,"start_time":"2021-10-30T15:50:08.898279","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.594842Z","iopub.execute_input":"2022-05-19T06:17:21.595406Z","iopub.status.idle":"2022-05-19T06:17:21.749484Z","shell.execute_reply.started":"2022-05-19T06:17:21.595366Z","shell.execute_reply":"2022-05-19T06:17:21.747967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train model","metadata":{"id":"y8JNMn770zRg","papermill":{"duration":0.026132,"end_time":"2021-10-30T15:50:09.063742","exception":false,"start_time":"2021-10-30T15:50:09.03761","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## setup model","metadata":{"papermill":{"duration":0.026483,"end_time":"2021-10-30T15:50:09.116837","exception":false,"start_time":"2021-10-30T15:50:09.090354","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Override pythorch checkpoint with an \"offline\" version of the file\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/cocopre/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","metadata":{"id":"VMaqdcNa0zRg","outputId":"c5ca31a5-6d8d-4639-e547-f44e5772c725","papermill":{"duration":4.941361,"end_time":"2021-10-30T15:50:14.083567","exception":false,"start_time":"2021-10-30T15:50:09.142206","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:21.750887Z","iopub.execute_input":"2022-05-19T06:17:21.751168Z","iopub.status.idle":"2022-05-19T06:17:23.614771Z","shell.execute_reply.started":"2022-05-19T06:17:21.75113Z","shell.execute_reply":"2022-05-19T06:17:23.613352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(num_classes, model_chkpt=None):\n    # This is just a dummy value for the classification head\n    \n    if NORMALIZE:\n#         model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n#             pretrained=True, box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n#             image_mean=RESNET_MEAN,image_std=RESNET_STD\n#         )\n        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(  # <-- è¿™é‡Œæˆ‘å°† mask-rcnn ç½‘ç»œä¿®æ”¹æˆfaster-rcnn \n            pretrained=True, box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n            image_mean=RESNET_MEAN,image_std=RESNET_STD\n        )                                                          \n    else:\n#         model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n#             pretrained=True, box_detections_per_img=BOX_DETECTIONS_PER_IMG\n#         )\n        model = torchvision.models.detection.fasterrcnn_resnet50_fpn(  # <-- è¿™é‡Œæˆ‘å°† mask-rcnn ç½‘ç»œä¿®æ”¹æˆfaster-rcnn \n            pretrained=True, box_detections_per_img=BOX_DETECTIONS_PER_IMG\n        )                        \n    \n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n\n    \n#     # now get the number of input features for the mask classifier    \n#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n#     hidden_layer = 256\n#     # and replace the mask predictor with a new one\n#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)  #<-- ç”¨faster-rcnnï¼Œæ‰€ä»¥ä¸éœ€è¦MASKåˆ†æ”¯\n    \n#     if model_chkpt:\n#         model.load_state_dict(torch.load(model_chkpt, map_location=DEVICE))     #<-- ç½‘ç»œé¢„è®­ç»ƒå‚æ•°åŠ è½½ï¼Œå¯ä»¥è‡ªåŠ¨åŠ è½½\n    return model\n\n\n# ============================================================================================================\n# åˆ©ç”¨ä¸Šé¢ get_model å‡½æ•°å»ºç«‹ Faster-RCNN æ¨¡åž‹ï¼ˆåŽŸæ¥æ˜¯å»ºç«‹Mask-RCNNï¼‰\n# The model does classification, bounding boxesfor individuals, all at the same time\n# We only care about MASKS\nmodel = get_model(num_classes=len(cell_type_dict))#è°ƒç”¨get_modelå‡½æ•°ï¼Œä¼ å…¥çš„å‚æ•°num_classesé‡‡ç”¨lençš„æ–¹æ³•è®¡ç®—cell_type_dictä¸­æœ‰å‡ ç±»ç›®æ ‡ï¼Œnum_classes=3\nmodel.to(DEVICE)#æŠŠæ¨¡åž‹åŠ è½½åˆ°cpuæˆ–gpuçš„æ¡†æž¶é‡Œè¿›è¡Œè¿ç®—\n\n# TODO: try removing this for\nfor param in model.parameters():#ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼ŒéåŽ†äº†ä»¥åŽï¼Œå°†param.modelæ‰€æœ‰å‚æ•°éƒ½è®¾ç½®æˆäº†å¯ä»¥æ¢¯åº¦ä¸‹é™çš„æ¨¡å¼\n    param.requires_grad = True\n    \nmodel.train(); #åŠ è½½æ¨¡åž‹ï¼Œè®¾ç½®å®Œæˆï¼Œå®šä¹‰è®­ç»ƒæ¨¡å¼","metadata":{"id":"3Ds5dHex0zRh","papermill":{"duration":3.679063,"end_time":"2021-10-30T15:50:17.789168","exception":false,"start_time":"2021-10-30T15:50:14.110105","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:23.616939Z","iopub.execute_input":"2022-05-19T06:17:23.617244Z","iopub.status.idle":"2022-05-19T06:17:24.383835Z","shell.execute_reply.started":"2022-05-19T06:17:23.617208Z","shell.execute_reply":"2022-05-19T06:17:24.383071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:17:24.387085Z","iopub.execute_input":"2022-05-19T06:17:24.387308Z","iopub.status.idle":"2022-05-19T06:17:24.393604Z","shell.execute_reply.started":"2022-05-19T06:17:24.387282Z","shell.execute_reply":"2022-05-19T06:17:24.392744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop!","metadata":{"id":"RvawgUM30zRh","papermill":{"duration":0.028196,"end_time":"2021-10-30T15:50:17.847237","exception":false,"start_time":"2021-10-30T15:50:17.819041","status":"completed"},"tags":[]}},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nn_batches, n_batches_val = len(dl_train), len(dl_val)\n\nfor epoch in range(1, NUM_EPOCHS + 1): #ä¹‹å‰å®šä¹‰äº†è®­ç»ƒè½®æ¬¡ç­‰äºŽ30ï¼Œé€šè¿‡å¾ªçŽ¯è¿›è¡Œ30è½®è®­ç»ƒ\n    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\") #è¾“å‡ºå½“å‰è¿›è¡Œçš„è½®æ¬¡\n    model.train()\n    time_start = time.time() #è®­ç»ƒå¼€å§‹è®°å½•æ—¶é—´\n    loss_accum = 0.0#è®¡ç®—æŸå¤±çš„å‚æ•°åˆå§‹åŒ–ä¸º0.0\n    loss_classifier_accum = 0.0#åˆ†ç±»å‚æ•°çš„åˆå§‹åŒ–ä¸º0.0\n    \n    for batch_idx, (images, targets) in enumerate(dl_train, 1): \n    \n        # Predict\n        images = list(image.to(DEVICE) for image in images)#å°†å›¾ç‰‡è½¬åŒ–æˆtorchå¼ é‡çš„å½¢å¼ï¼Œæ–¹ä¾¿åŽç»­è¾“å…¥\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]#å°†æ ‡ç­¾è½¬åŒ–æˆgpuæˆ–cpuçš„å¼ é‡æ ¼å¼ï¼Œå†è¾“å…¥æ¨¡åž‹ä¸­è¿›è¡ŒæŸå¤±è®¡ç®—ï¼Œè¿”å›žloss dict\n\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n        #loss_dict.valuesæ–¹æ³•ä¼šèŽ·å–loss_dictå­—å…¸é‡Œçš„æ‰€æœ‰å€¼ï¼ŒæŠŠå€¼æå–å‡ºæ¥æ±‚å’Œ\n        #å°†æ¯ä¸€ä¸ªbatchçš„è®¡ç®—çš„æŸå¤±æ±‚å’Œï¼Œå†å¯¹æŸå¤±æ±‚å¹³å‡ï¼Œå¾—åˆ°å¹³å‡æŸå¤±\n        \n        #Backpropâ€”â€”åå‘ä¼ æ’­ï¼Œå¾—åˆ°æŸå¤±åŽï¼Œé’ˆå¯¹æŸå¤±æƒ…å†µï¼ˆé¢„æµ‹ç»“æžœå’ŒçœŸå®žå€¼çš„è¯¯å·®ï¼‰å¯¹æ¨¡åž‹è¿›è¡Œè°ƒæ•´ï¼Œç„¶åŽå†è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ\n        optimizer.zero_grad()#å¯¹ä¼˜åŒ–å™¨è¿›è¡Œ0åˆå§‹åŒ–\n        loss.backward()  #æŸå¤±çš„åå‘ä¼ æ’­\n        optimizer.step() #åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è¿›è¡Œä¼˜åŒ–å™¨çš„æ“ä½œ\n        \n        # Logging\n        loss_accum += loss.item()\n        loss_classifier_accum += loss_dict['loss_classifier'].item()#å°†åˆ†ç±»æŸå¤±è¿›è¡Œæ±‚å’Œ\n        \n    #    if batch_idx % 500 == 0: #åœ¨æ•°æ®å¾ˆå¤šæ—¶ï¼Œå½“batchæ‰¹æ¬¡å¯ä»¥è¢«500æ•´é™¤æ—¶ï¼Œè¾“å‡ºè®­ç»ƒå„é¡¹æŸå¤±çš„æƒ…å†µ\n     #       print(f\"[Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}.\")\n                        \n    if USE_SCHEDULER:\n        lr_scheduler.step()\n\n    # Train losses\n    train_loss = loss_accum / n_batches#å°†æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„æŸå¤±æ±‚å’ŒåŽé™¤æ€»æ‰¹æ¬¡ï¼Œå¾—è¯¥è½®è®­ç»ƒæŸå¤±\n    train_loss_classifier = loss_classifier_accum / n_batches #åŒä¸Šï¼Œå¾—åˆ°åˆ†ç±»æŸå¤±\n\n    # Validationâ€”â€”éªŒè¯è¿‡ç¨‹ï¼Œå¯¹è®­ç»ƒå¥½çš„æ¨¡åž‹è¿›è¡Œç²¾åº¦æµ‹è¯•\n    val_loss_accum = 0 #åˆå§‹åŒ–äº†éªŒè¯æŸå¤±\n    val_loss_classifier_accum = 0#åˆå§‹åŒ–éªŒè¯åˆ†ç±»æŸå¤±\n    \n    with torch.no_grad(): #ä½¿ç”¨torch.no_gradçš„æ–¹æ³•ï¼Œå›ºåŒ–å‚æ•°ï¼Œä¸å…è®¸è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå¯¹éªŒè¯é›†è¿›è¡ŒéåŽ†ï¼Œæœ€åŽå¯¹éªŒè¯æŽ¨ç†è¿›è¡Œæ±‚å’Œå¹³å‡çš„è®¡ç®—\n #æ–¹æ³•åŒè®­ç»ƒä¸€è‡´\n        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n            images = list(image.to(DEVICE) for image in images)\n            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n            val_loss_dict = model(images, targets)\n            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n            val_loss_accum += val_batch_loss.item()\n            val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n\n    # Validation losses\n    val_loss = val_loss_accum / n_batches_val\n    val_loss_classifier = val_loss_classifier_accum / n_batches_val\n    elapsed = time.time() - time_start#è®¡ç®—è®­ç»ƒä¸€è½®åˆ°éªŒè¯å®Œçš„æ—¶é—´æ¶ˆè€—\n    \n    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\") #å¯¹è®­ç»ƒçš„æ¨¡åž‹è¿›è¡Œä¿å­˜\n    prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n\n    print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n    \n    evaluate(model,dl_val, device=DEVICE)\n    \n#å®šä¹‰å­—ç¬¦ä¸²ï¼Œå°†è®­ç»ƒè½®æ¬¡å’Œè®­ç»ƒæ€»è½®æ¬¡é€šè¿‡printæ–¹å¼å°†prefixè¿›è¡Œè¾“å‡º","metadata":{"id":"52B16JCW0zRh","outputId":"9b9c5ad9-58c1-4d50-dd7c-79b18b1e57b7","papermill":{"duration":7670.472684,"end_time":"2021-10-30T17:58:08.347309","exception":false,"start_time":"2021-10-30T15:50:17.874625","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:17:24.395054Z","iopub.execute_input":"2022-05-19T06:17:24.395639Z","iopub.status.idle":"2022-05-19T06:22:37.699814Z","shell.execute_reply.started":"2022-05-19T06:17:24.395601Z","shell.execute_reply":"2022-05-19T06:22:37.699025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze prediction results for train set","metadata":{"id":"MspyyJlP0zRh","papermill":{"duration":0.054203,"end_time":"2021-10-30T17:58:08.456349","exception":false,"start_time":"2021-10-30T17:58:08.402146","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Plots: the image, The image + the ground truth mask, The image + the predicted mask\n\ndef analyze_train_sample(model, ds_train, sample_index): #å®šä¹‰analyze_train_sampleå‡½æ•°ï¼Œ #è¿”å›žå›¾ç‰‡å’Œæ ‡ç­¾\n     \n    img, targets = ds_train[sample_index]\n    #print(img.shape)  #è¾“å‡ºtagretsï¼Œå¯¹åº”ä¸‹è¾¹çš„æ•°å­—çŸ©é˜µ     \n    l = np.unique(targets[\"labels\"])\n    ig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,60), facecolor=\"#fefefe\")\n    ax[0].imshow(img.numpy().transpose((1,2,0)))\n    ax[0].set_title(f\"cell type {l}\")   #ç»†èƒžç±»åž‹\n    ax[0].axis(\"off\")\n    #å¯¹æ ‡ç­¾è¿›è¡Œå¯è§†åŒ–ï¼Œç”»å›¾çš„æ„æ€ï¼Œæ˜¾ç¤ºåšæŽ¨ç†çš„å›¾ç‰‡â€”â€”åŽŸå›¾\n    \n    \n\n    #img = img_tensor.cpu().data\n    ax[1].imshow(img.numpy().transpose((1,2,0)))\n    for i, box in enumerate(targets[\"boxes\"]):\n        xmin, ymin, xmax, ymax = box\n        rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='green',\n                                     facecolor='none')\n        ax[1].add_patch(rect)\n    #plt.show()\n    \n   # masks = combine_masks(targets['boxes'], 0.5)\n    #plt.imshow(img.numpy().transpose((1,2,0)))\n #   ax[1].imshow(masks)\n   # ax[1].set_title(f\"Ground truth, {len(targets['area'])} cells\")\n #   ax[1].axis(\"off\")\n    #è°ƒç”¨maskæ–¹æ³•ï¼Œå†æ¬¡ç”»äº†ä¸€å¼ å›¾ ï¼Œç”»çš„æ˜¯å¯¹åŽŸå›¾è¿›è¡Œæ ‡æ³¨çš„å›¾å½¢ \n    model.eval()\n    with torch.no_grad():\n        preds = model([img.to(DEVICE)])[0]\n    \n#     print('---------------------------')\n#     print(targets)\n#     print(preds)\n#     print('--------------------------')\n    \n    \n    l = pd.Series(preds['labels'].cpu().numpy()).value_counts()              \n    lstr = \"\"\n    for i in l.index:\n        lstr += f\"{l[i]}x{i} \"\n    #print(l, l.sort_values().index[-1])\n    \n    ax[2].imshow(img.numpy().transpose((1,2,0)))                      #ç”»å‡ºé¢„æµ‹çš„æ¡†\n    s_boxes=[]                                                       #ç”¨æ¥ä¿å­˜åˆ†æ•°å¤§äºŽé˜ˆå€¼çš„æ¡†\n    s_scores=[]                                                      #ç”¨æ¥ä¿å­˜å¤§äºŽé˜ˆå€¼çš„åˆ†æ•°\n    s_classes=[]                                                     #ç”¨æ¥ä¿å­˜å¤§äºŽé˜ˆå€¼çš„ç±»åˆ«\n    for i, box in enumerate(preds[\"boxes\"]):\n        scr = preds[\"scores\"][i].cpu().item()\n        label = preds[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            #mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            #binary_mask = mask > mask_threshold_dict[label]\n            s_boxes.append(box.cpu().numpy())\n            s_scores.append(scr)\n            s_classes.append(label)\n#             xmin, ymin, xmax, ymax = box\n#             rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='red',\n#                                          facecolor='none')\n        \n        #ax[2].add_patch(rect)\n    s_boxes=torch.tensor(s_boxes)\n    s_scores=torch.tensor(s_scores)\n    s_classes=torch.tensor(s_classes)\n#     print(s_boxes)\n#     print(s_scores)\n#     print(s_classes)\n    thre=0.6                    #è®¾ç½®nmsçš„é˜ˆå€¼\n    output_boxes=torchvision.ops.boxes.batched_nms(s_boxes,s_scores,s_classes,thre)               #è°ƒç”¨å¤šç±»åˆ«NMS\n    for index in output_boxes:\n        xmin, ymin, xmax, ymax = s_boxes[index]\n        rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='red',\n                                         facecolor='none')\n        ax[2].add_patch(rect)\n    print('---------------')\n    print(output_boxes)\n    print('---------------')\n    \n    plt.show()\n    \n#     plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n#     mask_threshold = mask_threshold_dict[l.sort_values().index[-1]]\n#     print(mask_threshold)\n#     pred_masks = combine_masks(get_filtered_masks(preds), mask_threshold)\n#     ax[2].imshow(pred_masks)\n#     ax[2].set_title(f\"Predictions, labels: {lstr}\")\n#     ax[2].axis(\"off\")\n#     plt.show() \n#     #ç”»å›¾ï¼Œç”»çš„æ˜¯æ¨¡åž‹æŽ¨ç†çš„å›¾åƒï¼Œæ˜¾ç¤ºå¯¹æ¨¡åž‹é¢„æµ‹çš„ç»“æžœ\n#     print(masks.shape, pred_masks.shape)\n#     score = iou_map([masks],[pred_masks])\n#     print(\"Score:\", score)    \n    \n    \n# NOTE: It puts the model in eval mode!! Revert for re-training\nanalyze_train_sample(model, ds_train, 20)\n#è°ƒç”¨analyze_train_sampleæ–¹æ³•ï¼Œ20ä»£è¡¨ä»Žds_trainä¸­é€‰å‡ºçš„ç¬¬20å¼ å›¾ç‰‡ï¼Œå¯ä»¥ä»»æ„ä¿®æ”¹ï¼Œå‡ºæ¥çš„å›¾ç‰‡åº”è¯¥éƒ½ä¸ä¸€æ ·æ‰å¯¹  \n#é—®é¢˜åœ¨äºŽå°‘ä¸€äº›ä¸œè¥¿ï¼Œmaskæ¨¡åž‹æŽ¨ç†çš„ç»“æžœæ˜¯å­—å…¸è¾“å‡ºçš„å’Œfaster rcnnçš„é”®åæ˜¯ä¸ä¸€è‡´çš„ï¼Œå¯¹ä¸ä¸Šï¼Œåˆä¸çŸ¥é“åº”è¯¥è¾“å…¥çš„æ•°æ®æ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥ä¸çŸ¥é“æ€Žä¹ˆæ”¹","metadata":{"execution":{"iopub.status.busy":"2022-05-19T06:22:37.702805Z","iopub.execute_input":"2022-05-19T06:22:37.703073Z","iopub.status.idle":"2022-05-19T06:22:38.539504Z","shell.execute_reply.started":"2022-05-19T06:22:37.703041Z","shell.execute_reply":"2022-05-19T06:22:38.536728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 102)","metadata":{"id":"ZRx9K5n60zRi","outputId":"3ff2b8d0-8e4d-46a0-b3cb-7f2fad862808","papermill":{"duration":1.189143,"end_time":"2021-10-30T17:58:12.06041","exception":false,"start_time":"2021-10-30T17:58:10.871267","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:22:38.540846Z","iopub.execute_input":"2022-05-19T06:22:38.541603Z","iopub.status.idle":"2022-05-19T06:22:39.150546Z","shell.execute_reply.started":"2022-05-19T06:22:38.541563Z","shell.execute_reply":"2022-05-19T06:22:39.149861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 7)","metadata":{"id":"Rn6YeGVZ0zRi","outputId":"3b0b0c2b-1f0b-40b0-9529-275d24c3afa3","papermill":{"duration":5.359614,"end_time":"2021-10-30T17:58:17.470546","exception":false,"start_time":"2021-10-30T17:58:12.110932","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:22:39.153315Z","iopub.execute_input":"2022-05-19T06:22:39.153691Z","iopub.status.idle":"2022-05-19T06:22:40.333778Z","shell.execute_reply.started":"2022-05-19T06:22:39.153655Z","shell.execute_reply":"2022-05-19T06:22:40.333053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get the model from the best epoch","metadata":{"id":"Jon4MSmk0zRj","papermill":{"duration":0.059699,"end_time":"2021-10-30T17:58:17.590822","exception":false,"start_time":"2021-10-30T17:58:17.531123","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Epochs with their losses and IOU scores val_scores = pd.DataFrame()\nval_scores = pd.DataFrame()\nfor e, val_loss in enumerate(validation_mask_losses):\n    model_chk = f\"pytorch_model-e{e+1}.bin\"\n    print(\"Loading:\", model_chk)\n    model = get_model(len(cell_type_dict), model_chk)\n    model.load_state_dict(torch.load(model_chk))\n    model = model.to(DEVICE)\n    val_scores.loc[e,\"mask_loss\"] = val_loss\n    val_scores.loc[e,\"score\"] = get_score(ds_val, model)\n    \n    \n#display(val_scores.sort_values(\"score\", ascending=False))\n\n# best_epoch = np.argmax(val_scores[\"score\"])\n# print(best_epoch+1)","metadata":{"id":"O0ejRcer0zRj","outputId":"0806ad69-abcf-440a-c9ad-d5a2a454abe4","papermill":{"duration":2288.212204,"end_time":"2021-10-30T18:36:25.86261","exception":false,"start_time":"2021-10-30T17:58:17.650406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:22:40.335311Z","iopub.execute_input":"2022-05-19T06:22:40.335711Z","iopub.status.idle":"2022-05-19T06:22:40.367709Z","shell.execute_reply.started":"2022-05-19T06:22:40.335674Z","shell.execute_reply":"2022-05-19T06:22:40.365384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{"id":"bTNGfMuQ0zRi","papermill":{"duration":0.080258,"end_time":"2021-10-30T18:36:26.026589","exception":false,"start_time":"2021-10-30T18:36:25.946331","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Test Dataset and DataLoader","metadata":{"id":"tRSo-FPt0zRi","papermill":{"duration":0.081736,"end_time":"2021-10-30T18:36:26.192122","exception":false,"start_time":"2021-10-30T18:36:26.110386","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CellTestDataset(Dataset):\n    def __init__(self, image_dir, transforms=None, resize=False):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.image_ids = [f[:-4]for f in os.listdir(self.image_dir)]\n        self.should_resize = resize is not False\n        if self.should_resize:\n            self.height = int(HEIGHT * resize)\n            self.width = int(WIDTH * resize)\n            print(\"image size used:\", self.height, self.width)\n            \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.image_dir, image_id + '.png')\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        if self.should_resize:\n            image = cv2.resize(image, (self.width, self.height))\n\n        if self.transforms is not None:\n            image, _ = self.transforms(image=image, target=None)\n        return {'image': image, 'image_id': image_id}\n\n    def __len__(self):\n        return len(self.image_ids)","metadata":{"id":"ijZzdcHB0zRj","papermill":{"duration":0.092733,"end_time":"2021-10-30T18:36:26.365881","exception":false,"start_time":"2021-10-30T18:36:26.273148","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:22:40.371309Z","iopub.status.idle":"2022-05-19T06:22:40.375365Z","shell.execute_reply.started":"2022-05-19T06:22:40.374867Z","shell.execute_reply":"2022-05-19T06:22:40.374902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = CellTestDataset(TEST_PATH, transforms=get_transform(train=False))","metadata":{"id":"WbciaVrJ0zRj","outputId":"3ec69aa1-4136-41a1-b853-67d38654ef9a","papermill":{"duration":0.090469,"end_time":"2021-10-30T18:36:26.536396","exception":false,"start_time":"2021-10-30T18:36:26.445927","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:22:40.380703Z","iopub.status.idle":"2022-05-19T06:22:40.381445Z","shell.execute_reply.started":"2022-05-19T06:22:40.381177Z","shell.execute_reply":"2022-05-19T06:22:40.381206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_chk = f\"pytorch_model-e{best_epoch+1}.bin\"\nprint(\"Loading:\", model_chk)\nmodel = get_model(len(cell_type_dict))\nmodel.load_state_dict(torch.load(model_chk))\nmodel = model.to(DEVICE)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nmodel.eval();\n\nsubmission = []\nfor sample in ds_test:\n    img = sample['image']\n    image_id = sample['image_id']\n    with torch.no_grad():\n        result = model([img.to(DEVICE)])[0]\n    \n    previous_masks = []\n    for i, mask in enumerate(result[\"masks\"]):\n\n        # Filter-out low-scoring results.\n        score = result[\"scores\"][i].cpu().item()\n        label = result[\"labels\"][i].cpu().item()\n        if score > min_score_dict[label]:\n            mask = mask.cpu().numpy()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n            previous_masks.append(binary_mask)\n            rle = rle_encoding(binary_mask)\n            submission.append((image_id, rle))\n\n    # Add empty prediction if no RLE was generated for this image\n    all_images_ids = [image_id for image_id, rle in submission]\n    if image_id not in all_images_ids:\n        submission.append((image_id, \"\"))\n\ndf_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","metadata":{"id":"dUrfObRF0zRk","outputId":"5f0432a3-8b87-4540-f49d-9d033b61bdbc","papermill":{"duration":5.323233,"end_time":"2021-10-30T18:36:31.939163","exception":false,"start_time":"2021-10-30T18:36:26.61593","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-19T06:22:40.383809Z","iopub.status.idle":"2022-05-19T06:22:40.385287Z","shell.execute_reply.started":"2022-05-19T06:22:40.385028Z","shell.execute_reply":"2022-05-19T06:22:40.385056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"w73Jvhk20zRk","papermill":{"duration":0.080047,"end_time":"2021-10-30T18:36:32.10092","exception":false,"start_time":"2021-10-30T18:36:32.020873","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}