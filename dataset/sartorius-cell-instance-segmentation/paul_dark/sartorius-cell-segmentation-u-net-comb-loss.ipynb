{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>Sartorius: Cell Instance Segmentation</center></h1>\n                                                      \n<center><img src = \"https://neurosciencenews.com/files/2019/09/motor-neurons-als-neurosciencenews-public.jpg\" width = \"750\" height = \"500\"/></center>    ","metadata":{}},{"cell_type":"markdown","source":"<h1><center>Load Modules</center></h1>","metadata":{}},{"cell_type":"code","source":"# !pip install git+https://github.com/qubvel/segmentation_models.pytorch > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:09.017972Z","iopub.execute_input":"2021-12-24T03:08:09.018413Z","iopub.status.idle":"2021-12-24T03:08:09.026183Z","shell.execute_reply.started":"2021-12-24T03:08:09.018294Z","shell.execute_reply":"2021-12-24T03:08:09.025467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/losses/Losses.py\", dst = \"../working/Losses.py\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:09.02846Z","iopub.execute_input":"2021-12-24T03:08:09.029025Z","iopub.status.idle":"2021-12-24T03:08:09.058507Z","shell.execute_reply.started":"2021-12-24T03:08:09.02899Z","shell.execute_reply":"2021-12-24T03:08:09.057715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport copy\nimport warnings\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nimport torchvision\nfrom torchvision.transforms import functional as F\n\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2\n\nimport albumentations as A\nimport albumentations.pytorch\nfrom albumentations import (HorizontalFlip, VerticalFlip, \n                            ShiftScaleRotate, Normalize, Resize, \n                            Compose, GaussNoise)\nfrom albumentations.pytorch import ToTensorV2\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport scipy.ndimage as ndi\nfrom tqdm import tqdm, tqdm_notebook\n\nimport pickle\nfrom pathlib import Path\nimport PIL\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n#import seaborn as sns\nfrom skimage.filters import threshold_otsu\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport skimage\n\nimport time\nimport collections\nimport itertools\n#import segmentation_models_pytorch as smp\n\nfrom Losses import ComboLoss","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:09.059958Z","iopub.execute_input":"2021-12-24T03:08:09.060576Z","iopub.status.idle":"2021-12-24T03:08:10.779892Z","shell.execute_reply.started":"2021-12-24T03:08:09.060538Z","shell.execute_reply":"2021-12-24T03:08:10.77916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\n%matplotlib inline\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.enabled = True","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:10.78117Z","iopub.execute_input":"2021-12-24T03:08:10.781415Z","iopub.status.idle":"2021-12-24T03:08:10.787076Z","shell.execute_reply.started":"2021-12-24T03:08:10.781382Z","shell.execute_reply":"2021-12-24T03:08:10.786423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random(n):\n    random.seed(n)\n    np.random.seed(n)\n    torch.manual_seed(n)\n    torch.cuda.manual_seed(n)\n    torch.cuda.manual_seed_all(n)\n    torch.backends.cudnn.deterministic = True\n    os.environ['PYTHONHASHSEED'] = str(n)\n    \nset_random(42)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:10.789639Z","iopub.execute_input":"2021-12-24T03:08:10.790423Z","iopub.status.idle":"2021-12-24T03:08:10.802357Z","shell.execute_reply.started":"2021-12-24T03:08:10.790385Z","shell.execute_reply":"2021-12-24T03:08:10.801604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:10.805123Z","iopub.execute_input":"2021-12-24T03:08:10.80533Z","iopub.status.idle":"2021-12-24T03:08:11.486492Z","shell.execute_reply.started":"2021-12-24T03:08:10.805306Z","shell.execute_reply":"2021-12-24T03:08:11.485706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center>Configurations</center></h1>","metadata":{}},{"cell_type":"code","source":"data_dir = '../input/sartorius-cell-instance-segmentation'\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 4\nNUM_EPOCHS = 30\nTRAIN_CSV = f\"{data_dir}/train.csv\"\nTRAIN_PATH = f\"{data_dir}/train\"\nTEST_PATH = f\"{data_dir}/test\"\nSAMPLE_SUBMISSION = f\"{data_dir}/sample_submission.csv\"\n\nWIDTH = 704\nHEIGHT = 520\n# Threshold for mask length\nTH = 40\nLR = 1e-3\nWEIGHT_DECAY = 0.0005\n\n\n# Normalize to resnet mean and std if True.\nRESNET_MEAN = [0.485, 0.456, 0.406]\nRESNET_STD = [0.229, 0.224, 0.225]\nIMAGE_RESIZE = (224, 224)\n\n\nSAVE_MODEL = True\nLOAD_MODEL = True\n\nWORKERS = 2\n\ncmap = mpl.colors.ListedColormap(['black', 'gray', 'orange', 'green'])","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.489078Z","iopub.execute_input":"2021-12-24T03:08:11.489296Z","iopub.status.idle":"2021-12-24T03:08:11.521008Z","shell.execute_reply.started":"2021-12-24T03:08:11.48927Z","shell.execute_reply":"2021-12-24T03:08:11.520311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load the data</center></h2>","metadata":{}},{"cell_type":"code","source":"train_files = sorted(list(Path(TRAIN_PATH).rglob('*png')))\ntest_files = sorted(list(Path(TEST_PATH).rglob('*.png')))\nprint(f'Number of pictures in train dir: {len(train_files)} pcs')\nprint()\nprint(f'Number of pictures in test dir: {len(test_files)} pcs')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.522189Z","iopub.execute_input":"2021-12-24T03:08:11.522576Z","iopub.status.idle":"2021-12-24T03:08:11.551946Z","shell.execute_reply.started":"2021-12-24T03:08:11.522538Z","shell.execute_reply":"2021-12-24T03:08:11.551153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cell_df = pd.read_csv(TRAIN_CSV)\ncell_df.info()\ncell_df.sample(4)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.553328Z","iopub.execute_input":"2021-12-24T03:08:11.553575Z","iopub.status.idle":"2021-12-24T03:08:11.906698Z","shell.execute_reply.started":"2021-12-24T03:08:11.553542Z","shell.execute_reply":"2021-12-24T03:08:11.905943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Utilits</center></h2>","metadata":{}},{"cell_type":"markdown","source":"***Run Length Encoding: Annotation Data to Mask ImageÂ¶***\n\nNext funtion is refferred to INVERSION's notebook'Run Length Decoding - Quick Start'.\n\nhttps://www.kaggle.com/inversion/run-length-decoding-quick-start","metadata":{}},{"cell_type":"markdown","source":"***What is Run-length encoding (RLE)?***\n\nRun-length encoding (RLE) is a form of lossless data compression in which runs of data (sequences in which the same data value occurs   in many consecutive data elements) are stored as a single data value and count, rather than as the original run. This is most efficient on data that contains many such runs, for example, simple graphic images such as icons, line drawings, Conway's Game of Life, and animations. For files that do not have many runs, RLE could increase the file size. (from Wikipedia)\n\n#### [WIKI](https://en.wikipedia.org/wiki/Run-length_encoding)","metadata":{}},{"cell_type":"code","source":"def rle_decode(mask_rle,shape,color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    if colors = 1, then:\n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    \n    s = mask_rle.split()\n    # get starting pixel and cells's length\n    start, length = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    start -=1\n    # Ending pixels\n    ends = start + length\n    img = np.zeros((shape[0] * shape[1], shape [2]), dtype=np.float32)\n    for lo, hi in zip (start, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.908039Z","iopub.execute_input":"2021-12-24T03:08:11.908306Z","iopub.status.idle":"2021-12-24T03:08:11.918151Z","shell.execute_reply.started":"2021-12-24T03:08:11.908275Z","shell.execute_reply":"2021-12-24T03:08:11.917402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Masks have an abnormally long continuous lines.\n\nThe mask is not representative enough.\n\nWe can take only correct masks to train our model.\n\n**Thanks** [Slavko Prytula](https://www.kaggle.com/slavkoprytula/mask-correction-mask-filtering-updated#Utils)","metadata":{}},{"cell_type":"code","source":"def clean_mask(mask):\n    '''\n    Function is called to identify whether the mask is broken\n    returns True or False state and also returns a mask\n    '''\n    mask = mask > threshold_otsu(np.array(mask).astype(np.uint8))\n    mask = ndi.binary_fill_holes(mask).astype(np.uint8)\n    \n    # New code for mask acceptance\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    c = contours[0][:, 0]\n    diff = c - np.roll(c, 1, 0)\n    # find horizontal lines longer than threshold\n    targets = (diff[:, 1] == 0) & (np.abs(diff[:, 0]) >= TH)  \n    \n    return mask, (True in targets)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.919551Z","iopub.execute_input":"2021-12-24T03:08:11.919831Z","iopub.status.idle":"2021-12-24T03:08:11.928365Z","shell.execute_reply.started":"2021-12-24T03:08:11.919797Z","shell.execute_reply":"2021-12-24T03:08:11.927689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_mask(labels, input_shape, colors=True):\n    height, width = input_shape\n    masks = np.zeros((height, width, 1))\n    #masks = np.zeros((height,width), dtype=np.uint8)\n    for i, label in enumerate(labels):\n        a_mask = rle_decode(label, shape=(height,width, 1))\n        a_mask = np.array(a_mask) > 0 \n        a_mask, broken_mask = clean_mask(a_mask)\n        if broken_mask:\n            continue\n        masks += a_mask\n    masks = masks.clip(0,1)\n    return masks","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.93045Z","iopub.execute_input":"2021-12-24T03:08:11.931398Z","iopub.status.idle":"2021-12-24T03:08:11.94089Z","shell.execute_reply.started":"2021-12-24T03:08:11.931361Z","shell.execute_reply":"2021-12-24T03:08:11.940074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Visualize the Data</center></h2>","metadata":{}},{"cell_type":"code","source":"temp = cell_df.groupby(['id','cell_type'])['annotation'].agg(lambda x: list(x)).reset_index()\nfor i in range(4):\n    #rand_idx = int(np.random.uniform(0, 607))\n    #im = PIL.Image.open(train_files[rand_idx])\n    im = PIL.Image.open(train_files[i])\n    \n    start = time.time()\n    label = (train_files[i]).stem\n    \n    cell_type = temp[temp['id'] == label]\n    cell_type = str(cell_type['cell_type'].values)\n    print(\"--- %s seconds in label ---\" % (time.time() - start))\n    \n    start = time.time()\n    sample_im_df = cell_df[cell_df['id'] == label]\n    print(\"--- %s seconds in sorting df ---\" % (time.time() - start))\n    \n    sample_rles = sample_im_df['annotation'].values\n    start = time.time()\n    sample_masks1 = build_mask(sample_rles,input_shape=(HEIGHT,WIDTH), colors=False)\n    print(\"--- %s seconds in mask1 ---\" % (time.time() - start))\n    \n    start = time.time()\n    sample_masks2 = build_mask(sample_rles,input_shape=(HEIGHT,WIDTH), colors=True)\n    print(\"--- %s seconds in mask2 ---\" % (time.time() - start))\n    \n    \n\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 20),)\n    ax1.imshow(im,cmap='gray')\n    ax2.imshow(sample_masks1, cmap='gray')\n    ax3.imshow(im,cmap='gray')\n    ax3.imshow(sample_masks2,alpha = .2)\n    \n    ax1.set_title('Cell id: ' + str(label) + 'Type:' + str(cell_type))\n    ax2.set_title('Mask for id: ' + str(label) + 'Type:' + str(cell_type))\n    ax3.set_title('Mask & pic: ' + str(label) + 'Type:' + str(cell_type))\n    plt.show();\ndel temp, im, sample_masks1, sample_masks2","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:11.943252Z","iopub.execute_input":"2021-12-24T03:08:11.94376Z","iopub.status.idle":"2021-12-24T03:08:42.283921Z","shell.execute_reply.started":"2021-12-24T03:08:11.943715Z","shell.execute_reply":"2021-12-24T03:08:42.283252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center>Preparing DataSets And DataLoaders</center></h1>","metadata":{}},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Define Augmenting</center></h2>","metadata":{}},{"cell_type":"code","source":"def get_augmentation(pic_size,mode):\n    if mode !='train':\n        augmentation_test = albumentations.Compose([A.Resize(pic_size,pic_size),\n                                                    A.Normalize(\n                                                        mean=RESNET_MEAN,\n                                                        std=RESNET_STD),\n                                                    A.pytorch.transforms.ToTensorV2()])\n        return augmentation_test\n    else:\n        augmentation_train = A.Compose([\n            A.Resize(pic_size,pic_size),\n            A.RandomCrop(pic_size, pic_size),\n            A.OneOf([\n                A.HorizontalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                #A.VerticalFlip(p=0.5)\n            ], p=0.5),\n            A.OneOf([\n                A.GaussNoise(),], p=0.8),\n\n            A.OneOf([\n                A.CLAHE(clip_limit=2),\n                A.RandomBrightnessContrast(),], p=0.2),\n#             A.HueSaturationValue(p=0.2),\n            A.Normalize(\n                mean=RESNET_MEAN,\n                std=RESNET_STD),\n            A.pytorch.transforms.ToTensorV2()], p=1)\n        return augmentation_train","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.287385Z","iopub.execute_input":"2021-12-24T03:08:42.288807Z","iopub.status.idle":"2021-12-24T03:08:42.298633Z","shell.execute_reply.started":"2021-12-24T03:08:42.288768Z","shell.execute_reply":"2021-12-24T03:08:42.297808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NeuroDataSet(Dataset):\n    def __init__(self, files, df: pd.core.frame.DataFrame,train:bool, transform=None):   \n        self.files = files\n        self.df = df\n        self.height = HEIGHT\n        self.width = WIDTH\n        self.gb = self.df.groupby('id')\n        self.transform = transform\n        self.len_ = len(self.files)\n        self.train = train\n        \n    \n    def load_sample(self, file):\n        img = cv2.imread(str(file), cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_id = file.stem\n        return img, img_id\n    \n    def __getitem__(self,index):\n        # load image\n        img, img_id = self.load_sample(self.files[index])\n        df_temp = self.gb.get_group(img_id)\n        \n        annotations = df_temp['annotation'].values\n        mask = build_mask(annotations, (self.height,self.width), colors=False)\n        mask = mask.astype('float32')\n        if self.transform is not None:\n            augmented = self.transform(image = img, mask = mask)\n            img = augmented['image']\n            mask = augmented['mask']\n            return img, mask.reshape((1, IMAGE_RESIZE[0], IMAGE_RESIZE[1]))\n        else:\n            return img, mask.reshape((1, IMAGE_RESIZE[0], IMAGE_RESIZE[1]))\n     \n    def __len__(self):\n        return self.len_","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.300461Z","iopub.execute_input":"2021-12-24T03:08:42.300964Z","iopub.status.idle":"2021-12-24T03:08:42.317079Z","shell.execute_reply.started":"2021-12-24T03:08:42.300926Z","shell.execute_reply":"2021-12-24T03:08:42.316357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Split train-val set</center></h2>","metadata":{}},{"cell_type":"code","source":"# Split dataset to train and val sets\ntrain_pics, val_pics = train_test_split(train_files, test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.318516Z","iopub.execute_input":"2021-12-24T03:08:42.318981Z","iopub.status.idle":"2021-12-24T03:08:42.329428Z","shell.execute_reply.started":"2021-12-24T03:08:42.318944Z","shell.execute_reply":"2021-12-24T03:08:42.328712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract pic Id from Path\ntrain_pic_id = [path.stem for path in train_pics]\nval_pic_id = [path.stem for path in val_pics]\n# Print number of files in tran and val sets\nprint(f'Number of pictures in train set: {len(train_pics)}')\nprint()\nprint(f'Number of pictures in val set: {len(val_pics)}')","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.33087Z","iopub.execute_input":"2021-12-24T03:08:42.331342Z","iopub.status.idle":"2021-12-24T03:08:42.362891Z","shell.execute_reply.started":"2021-12-24T03:08:42.331307Z","shell.execute_reply":"2021-12-24T03:08:42.35964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split pandas dataframe with annotations for train df and val df\n# in consistance with given file Ids\ndf_train = cell_df[(cell_df['id'].isin(train_pic_id))]\ndf_val = cell_df[(cell_df['id'].isin(val_pic_id))]\n\nprint(f\"Number of picture's id in train set {df_train.id.nunique()}\")\nprint()\nprint(f\"Number of picture's id in val set {df_val.id.nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.363911Z","iopub.execute_input":"2021-12-24T03:08:42.364167Z","iopub.status.idle":"2021-12-24T03:08:42.424825Z","shell.execute_reply.started":"2021-12-24T03:08:42.364132Z","shell.execute_reply":"2021-12-24T03:08:42.42064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = NeuroDataSet(train_pics,\n                             df_train,\n                             train = True,\n                             transform=get_augmentation(224, 'train')\n                            )","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.427029Z","iopub.execute_input":"2021-12-24T03:08:42.427454Z","iopub.status.idle":"2021-12-24T03:08:42.437503Z","shell.execute_reply.started":"2021-12-24T03:08:42.427384Z","shell.execute_reply":"2021-12-24T03:08:42.434684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=False,\n                          pin_memory=True,\n                          num_workers=WORKERS,)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.439351Z","iopub.execute_input":"2021-12-24T03:08:42.439912Z","iopub.status.idle":"2021-12-24T03:08:42.455889Z","shell.execute_reply.started":"2021-12-24T03:08:42.439877Z","shell.execute_reply":"2021-12-24T03:08:42.454547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dataset = NeuroDataSet(val_pics,\n                           df_val,train=False,\n                          transform=get_augmentation(224, 'val')\n                          )","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.457054Z","iopub.execute_input":"2021-12-24T03:08:42.457602Z","iopub.status.idle":"2021-12-24T03:08:42.464048Z","shell.execute_reply.started":"2021-12-24T03:08:42.457563Z","shell.execute_reply":"2021-12-24T03:08:42.463074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_loader = DataLoader(val_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=False,\n                          pin_memory=True,\n                          num_workers=WORKERS,)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.46547Z","iopub.execute_input":"2021-12-24T03:08:42.466278Z","iopub.status.idle":"2021-12-24T03:08:42.474026Z","shell.execute_reply.started":"2021-12-24T03:08:42.466207Z","shell.execute_reply":"2021-12-24T03:08:42.473082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_loader)) \nimages, masks = batch\nprint(f\"image shape: {images.shape},\\nmask shape:{masks.shape},\\nbatch len: {len(batch)}\")\ndel batch, images, masks","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:08:42.476902Z","iopub.execute_input":"2021-12-24T03:08:42.477788Z","iopub.status.idle":"2021-12-24T03:09:06.058822Z","shell.execute_reply.started":"2021-12-24T03:08:42.477742Z","shell.execute_reply":"2021-12-24T03:09:06.057978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Visualize Augmented DataSet</center></h2>","metadata":{}},{"cell_type":"code","source":"def visualize_augmentations(dataset, idx=1, samples=9, cols=3):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, A.pytorch.ToTensorV2))])\n    rows = samples // cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 12))\n    for i in range(samples):\n        image, mask = dataset[idx]\n        ax.ravel()[i].imshow(image)\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()\n    del dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:06.060182Z","iopub.execute_input":"2021-12-24T03:09:06.060807Z","iopub.status.idle":"2021-12-24T03:09:06.067697Z","shell.execute_reply.started":"2021-12-24T03:09:06.060764Z","shell.execute_reply":"2021-12-24T03:09:06.066966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_augmentations(train_dataset,idx=3)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:06.068916Z","iopub.execute_input":"2021-12-24T03:09:06.069326Z","iopub.status.idle":"2021-12-24T03:09:10.351089Z","shell.execute_reply.started":"2021-12-24T03:09:06.069289Z","shell.execute_reply":"2021-12-24T03:09:10.350478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center>UNET from SCRATCH</center></h1>","metadata":{}},{"cell_type":"code","source":"# Create convolution block class\nclass Conv_Block(nn.Module):\n    '''convolution ==> BN ==> ReLU'''\n    \n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        x = self.conv(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.352396Z","iopub.execute_input":"2021-12-24T03:09:10.352812Z","iopub.status.idle":"2021-12-24T03:09:10.360016Z","shell.execute_reply.started":"2021-12-24T03:09:10.352778Z","shell.execute_reply":"2021-12-24T03:09:10.359434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            Conv_Block(in_channels, out_channels),\n            nn.MaxPool2d(2)\n        )\n\n    def forward(self, x):\n        x = x.cuda()\n        return self.maxpool_conv(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.361081Z","iopub.execute_input":"2021-12-24T03:09:10.361764Z","iopub.status.idle":"2021-12-24T03:09:10.372892Z","shell.execute_reply.started":"2021-12-24T03:09:10.361722Z","shell.execute_reply":"2021-12-24T03:09:10.372251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n            \n        self.conv = Conv_Block(in_channels, out_channels)\n        \n    def forward(self, x1, x2):\n        x1 = x1.cuda()\n        x2 = x2.cuda()\n        x1 = self.up(x1)\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.374178Z","iopub.execute_input":"2021-12-24T03:09:10.374612Z","iopub.status.idle":"2021-12-24T03:09:10.384553Z","shell.execute_reply.started":"2021-12-24T03:09:10.374579Z","shell.execute_reply":"2021-12-24T03:09:10.383802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.385849Z","iopub.execute_input":"2021-12-24T03:09:10.386292Z","iopub.status.idle":"2021-12-24T03:09:10.392829Z","shell.execute_reply.started":"2021-12-24T03:09:10.386259Z","shell.execute_reply":"2021-12-24T03:09:10.39206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n        \n        self.inc = Conv_Block(n_channels, 64)\n        self.enc1 = Encoder(64, 128)\n        self.enc2 = Encoder(128, 256)\n        self.enc3 = Encoder(256, 512)\n        self.enc4 = Encoder(512, 512)\n        self.dec1 = Decoder(1024, 256, bilinear)\n        self.dec2 = Decoder(512, 128, bilinear)\n        self.dec3 = Decoder(256, 64, bilinear)\n        self.dec4 = Decoder(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n        \n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.enc1(x1)\n        x3 = self.enc2(x2)\n        x4 = self.enc3(x3)\n        x5 = self.enc4(x4)\n        x = self.dec1(x5, x4)\n        x = self.dec2(x, x3)\n        x = self.dec3(x, x2)\n        x = self.dec4(x, x1)\n        logits = self.outc(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.394107Z","iopub.execute_input":"2021-12-24T03:09:10.394565Z","iopub.status.idle":"2021-12-24T03:09:10.407739Z","shell.execute_reply.started":"2021-12-24T03:09:10.394533Z","shell.execute_reply":"2021-12-24T03:09:10.407092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Define loss functions</center></h2>","metadata":{}},{"cell_type":"code","source":"from Losses import ComboLoss\n# JaccardLoss = smp.losses.JaccardLoss(mode='binary') \n# Dice        = smp.losses.DiceLoss(mode='binary',)\n# BCELoss     = smp.losses.SoftBCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.409121Z","iopub.execute_input":"2021-12-24T03:09:10.409588Z","iopub.status.idle":"2021-12-24T03:09:10.422343Z","shell.execute_reply.started":"2021-12-24T03:09:10.409551Z","shell.execute_reply":"2021-12-24T03:09:10.42141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Define Save Weights function</center></h2>","metadata":{}},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filename)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.424002Z","iopub.execute_input":"2021-12-24T03:09:10.424501Z","iopub.status.idle":"2021-12-24T03:09:10.430997Z","shell.execute_reply.started":"2021-12-24T03:09:10.424464Z","shell.execute_reply":"2021-12-24T03:09:10.43022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.432441Z","iopub.execute_input":"2021-12-24T03:09:10.433073Z","iopub.status.idle":"2021-12-24T03:09:10.439778Z","shell.execute_reply.started":"2021-12-24T03:09:10.433001Z","shell.execute_reply":"2021-12-24T03:09:10.439083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Train</center></h2>","metadata":{}},{"cell_type":"code","source":"def train_loop(model, optimizer, criterion, train_loader, device=DEVICE):\n    running_loss = 0\n    model.train()\n    pbar = tqdm(train_loader, desc='Iterating over train data')\n    for imgs, masks in pbar:\n        # pass to device\n        imgs = imgs.to(device)\n        masks = masks.to(device)\n        # forward\n        out = model(imgs)\n        loss = criterion(out, masks)\n        running_loss += loss.item()*imgs.shape[0]  # += loss * current batch size\n        # optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    running_loss /= len(train_loader.sampler)\n    del imgs, masks\n    torch.cuda.empty_cache()\n    return running_loss","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.44127Z","iopub.execute_input":"2021-12-24T03:09:10.44186Z","iopub.status.idle":"2021-12-24T03:09:10.450392Z","shell.execute_reply.started":"2021-12-24T03:09:10.441821Z","shell.execute_reply":"2021-12-24T03:09:10.449581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_loop(model, criterion, eval_loader, device=DEVICE):\n    running_loss = 0\n    model.eval()\n    with torch.no_grad():\n        accuracy, f1_scores = [], []\n        pbar = tqdm(eval_loader, desc='Iterating over evaluation data')\n        for imgs, masks in pbar:\n            # pass to device\n            imgs = imgs.to(device)\n            masks = masks.to(device)\n            # forward\n            out = model(imgs)\n            loss = criterion(out, masks)\n            running_loss += loss.item()*imgs.shape[0]\n            # calculate predictions using output\n            predicted = (out > 0.5).float()\n            predicted = predicted.view(-1).cpu().numpy()\n            labels = masks.view(-1).cpu().numpy()\n            accuracy.append(accuracy_score(labels, predicted))\n            f1_scores.append(f1_score(labels, predicted))\n    acc = sum(accuracy)/len(accuracy)\n    f1 = sum(f1_scores)/len(f1_scores)\n    running_loss /= len(eval_loader.sampler)\n    return {\n        'accuracy':acc,\n        'f1_macro':f1, \n        'loss':running_loss}","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.451747Z","iopub.execute_input":"2021-12-24T03:09:10.452053Z","iopub.status.idle":"2021-12-24T03:09:10.463108Z","shell.execute_reply.started":"2021-12-24T03:09:10.452018Z","shell.execute_reply":"2021-12-24T03:09:10.462406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef visualize_train(model,val_loader, device=DEVICE):\n    model.eval()\n    X_val, Y_val = next(iter(val_loader))\n    Y_hat = model(X_val.to(device))\n    \n    Y_hat = torch.tanh(Y_hat)\n    Y_hat = Y_hat.detach().cpu().numpy()\n    X_val = X_val.detach().cpu().numpy()\n    \n    #clear_output(wait=True)\n    for k in range(2):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5),)\n        ax1.imshow(np.rollaxis(Y_val[k].numpy(), 0, 3),cmap='gray')\n        ax2.imshow(((Y_hat[k, 0] * 255)), cmap='gray')\n        ax1.set_axis_off()\n        ax2.set_axis_off()\n        ax1.set_title('Real')\n        ax2.set_title('Output')\n    plt.show()\n    del X_val, Y_hat","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.465564Z","iopub.execute_input":"2021-12-24T03:09:10.466506Z","iopub.status.idle":"2021-12-24T03:09:10.474811Z","shell.execute_reply.started":"2021-12-24T03:09:10.466469Z","shell.execute_reply":"2021-12-24T03:09:10.473966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, criterion, train_loader, valid_loader,\n          device=DEVICE, \n          num_epochs=NUM_EPOCHS, \n          valid_loss_min=np.inf,\n          vis=False,\n          model_name=None):\n    #step = 0\n    history = []\n       \n    for e in range(num_epochs):\n        # train for epoch\n        train_loss = train_loop(\n            model, optimizer, criterion, train_loader, device=device)\n        torch.cuda.empty_cache() #!!!!!!!!!!!!!!!!!\n        # evaluate on validation set\n        metrics = eval_loop(\n            model, criterion, valid_loader, device=device\n        )\n        history.append((train_loss,metrics[\"loss\"],metrics[\"accuracy\"],metrics[\"f1_macro\"]))\n        \n        #writer = SummaryWriter(f\"runs/Sartorius LR_{LR}_BS_{BATCH_SIZE}\")\n        # visualize model in tensorboard\n        images, masks = next(iter(train_loader))\n        #writer.add_graph(model, images.to(device))\n        #writer.close()\n        \n        if vis:\n            visualize_train(model,val_loader,device=DEVICE)\n        \n        # show progress\n        print_string = f'Epoch: {e+1} '\n        print_string+= f'TrainLoss: {train_loss:.5f} '\n        print_string+= f'ValidLoss: {metrics[\"loss\"]:.5f} '\n        print_string+= f'ACC: {metrics[\"accuracy\"]:.5f} '\n        print_string+= f'F1: {metrics[\"f1_macro\"]:.3f}'\n        print(print_string)\n\n\n        # save the model \n        if SAVE_MODEL and metrics[\"loss\"] <= valid_loss_min:\n            save_checkpoint(model, optimizer, filename=model_name)\n            valid_loss_min = metrics[\"loss\"]\n\n    return history","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.476032Z","iopub.execute_input":"2021-12-24T03:09:10.47651Z","iopub.status.idle":"2021-12-24T03:09:10.487502Z","shell.execute_reply.started":"2021-12-24T03:09:10.476473Z","shell.execute_reply":"2021-12-24T03:09:10.486688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet(3, 1).to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY )\ncriterion        = ComboLoss(**{'weights':{'dice':3, 'focal':2, 'jaccard':3}})","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.488557Z","iopub.execute_input":"2021-12-24T03:09:10.488988Z","iopub.status.idle":"2021-12-24T03:09:10.627368Z","shell.execute_reply.started":"2021-12-24T03:09:10.488952Z","shell.execute_reply":"2021-12-24T03:09:10.626696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"../input/d/pauldark/u-net-loss/Unet_combo.pth\", dst = \"../working/Unet_combo.pth\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.628702Z","iopub.execute_input":"2021-12-24T03:09:10.628949Z","iopub.status.idle":"2021-12-24T03:09:10.930299Z","shell.execute_reply.started":"2021-12-24T03:09:10.628917Z","shell.execute_reply":"2021-12-24T03:09:10.92966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CHECKPOINT = 'Unet_combo.pth'","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.931888Z","iopub.execute_input":"2021-12-24T03:09:10.932623Z","iopub.status.idle":"2021-12-24T03:09:10.936481Z","shell.execute_reply.started":"2021-12-24T03:09:10.932586Z","shell.execute_reply":"2021-12-24T03:09:10.935712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LOAD_MODEL:\n        load_checkpoint(\n            CHECKPOINT, model, optimizer, LR,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:10.944106Z","iopub.execute_input":"2021-12-24T03:09:10.944295Z","iopub.status.idle":"2021-12-24T03:09:11.067304Z","shell.execute_reply.started":"2021-12-24T03:09:10.944272Z","shell.execute_reply":"2021-12-24T03:09:11.066522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = train(model, optimizer, criterion, train_loader, val_loader, model_name=CHECKPOINT)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.069477Z","iopub.execute_input":"2021-12-24T03:09:11.070107Z","iopub.status.idle":"2021-12-24T03:09:11.073957Z","shell.execute_reply.started":"2021-12-24T03:09:11.070069Z","shell.execute_reply":"2021-12-24T03:09:11.073096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let' download weights of trained model.","metadata":{}},{"cell_type":"code","source":"# Load the latest model data_dir\n# if LOAD_MODEL:\n#         load_checkpoint(\n#             CHECKPOINT, model, optimizer, LR,\n#         )\n        \n# metrics = eval_loop(model, criterion, val_loader)\n# print('accuracy:', metrics['accuracy'])\n# print('f1 macro:', metrics['f1_macro'])\n# print('test loss:', metrics['loss'])","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.075207Z","iopub.execute_input":"2021-12-24T03:09:11.075994Z","iopub.status.idle":"2021-12-24T03:09:11.082949Z","shell.execute_reply.started":"2021-12-24T03:09:11.075958Z","shell.execute_reply":"2021-12-24T03:09:11.082217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #del metrics\n# torch.cuda.empty_cache()\n# del model","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.084301Z","iopub.execute_input":"2021-12-24T03:09:11.084559Z","iopub.status.idle":"2021-12-24T03:09:11.090685Z","shell.execute_reply.started":"2021-12-24T03:09:11.084525Z","shell.execute_reply":"2021-12-24T03:09:11.089928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Visualize Training Results</center></h2>","metadata":{}},{"cell_type":"markdown","source":"Let's check loss and accuracies during the training of the model","metadata":{}},{"cell_type":"code","source":"def plot_loss_acc(history):\n    \"\"\"Print Loss in train and val sets\"\"\"\n    train_loss, val_loss, val_acc, _ = zip(*history)\n\n    fig, (ax1, ax2) = plt.subplots(2, figsize=(10,10))\n    fig.suptitle('Loss and Accuracy')\n    ax1.plot(train_loss, label=\"train_loss\")\n    ax1.plot(val_loss, label=\"val_loss\")\n    ax1.legend(loc='best')\n    plt.ylabel(\"loss\")\n\n    ax2.plot(val_acc, label=\"val_accuracy\")\n    ax2.legend(loc='best')\n    plt.xlabel(\"epochs\")\n    plt.ylabel(\"accuracy\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.091945Z","iopub.execute_input":"2021-12-24T03:09:11.0922Z","iopub.status.idle":"2021-12-24T03:09:11.099234Z","shell.execute_reply.started":"2021-12-24T03:09:11.092164Z","shell.execute_reply":"2021-12-24T03:09:11.09852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_loss_acc(history)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.100631Z","iopub.execute_input":"2021-12-24T03:09:11.101118Z","iopub.status.idle":"2021-12-24T03:09:11.10752Z","shell.execute_reply.started":"2021-12-24T03:09:11.101081Z","shell.execute_reply":"2021-12-24T03:09:11.106702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspired by:\n\n[Ant ðŸœ ](https://www.kaggle.com/superant/simple-nn-segmentation/notebook)","metadata":{}},{"cell_type":"code","source":"def get_threshold(Y, pred):\n    '''Function is called for finding threshols and IoU\n    '''\n    scores = list(pred.ravel())\n    mask = list(Y.ravel())\n    \n    #idx from the end to the begining\n    idxs=np.argsort(scores)[::-1]\n    mask_sorted=np.array(mask)[idxs]\n    sum_mask_one=np.cumsum(mask_sorted)\n    IoU=sum_mask_one/(np.arange(1,len(mask_sorted)+1)+np.sum(mask_sorted)-sum_mask_one)\n    best_IoU_idx=IoU.argmax()\n    best_threshold=scores[idxs[best_IoU_idx]]\n    best_IoU=IoU[best_IoU_idx]\n\n    return best_threshold, best_IoU","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.108939Z","iopub.execute_input":"2021-12-24T03:09:11.109315Z","iopub.status.idle":"2021-12-24T03:09:11.11772Z","shell.execute_reply.started":"2021-12-24T03:09:11.109279Z","shell.execute_reply":"2021-12-24T03:09:11.116965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Re-assign val_loader with higher num of batches (for further plotting)\n","metadata":{}},{"cell_type":"code","source":"val_loader_plot = DataLoader(val_dataset,\n                          batch_size=16,\n                          shuffle=False,\n                          pin_memory=True,\n                          num_workers=WORKERS,)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.119071Z","iopub.execute_input":"2021-12-24T03:09:11.119408Z","iopub.status.idle":"2021-12-24T03:09:11.126219Z","shell.execute_reply.started":"2021-12-24T03:09:11.119328Z","shell.execute_reply":"2021-12-24T03:09:11.125417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define function to get the best value of threshold for the IoU metric","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef get_best_threshold(model,loader,device=DEVICE,N=3):\n    img_thresholds = []\n    img_IoUs = []\n    model.eval()\n    \n    for img, mask in tqdm(itertools.islice(loader, N), total=N):\n        img = img.to(device)\n        mask = mask.detach().numpy()\n        \n        Y_hat = torch.sigmoid(model(img)).cpu().detach().numpy()\n        \n        for i in range(mask.shape[0]):\n            best_img_threshold, best_img_IoU = get_threshold(mask[i], Y_hat[i])\n            img_thresholds.append(best_img_threshold)\n            img_IoUs.append(best_img_IoU)\n    del model #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼        \n    return img_thresholds,img_IoUs","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.127594Z","iopub.execute_input":"2021-12-24T03:09:11.127883Z","iopub.status.idle":"2021-12-24T03:09:11.135611Z","shell.execute_reply.started":"2021-12-24T03:09:11.127847Z","shell.execute_reply":"2021-12-24T03:09:11.13471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_thresholds,img_IoUs = get_best_threshold(model,val_loader_plot)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:09:11.137146Z","iopub.execute_input":"2021-12-24T03:09:11.137564Z","iopub.status.idle":"2021-12-24T03:12:14.026543Z","shell.execute_reply.started":"2021-12-24T03:09:11.137529Z","shell.execute_reply":"2021-12-24T03:12:14.025227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_threshold = np.mean(img_thresholds)\nthreshold_std = np.std(img_thresholds)\navg_IoU = np.mean(img_IoUs)\n\nprint(f\"Best threshold: {best_threshold:.3g} (STD: +-{threshold_std:.3g}), Avg. Val IoU: {avg_IoU:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:12:14.028135Z","iopub.execute_input":"2021-12-24T03:12:14.028676Z","iopub.status.idle":"2021-12-24T03:12:14.035771Z","shell.execute_reply.started":"2021-12-24T03:12:14.028613Z","shell.execute_reply":"2021-12-24T03:12:14.0349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = best_threshold\nmodel.eval()\n\nX, Y = next(iter(val_loader_plot))\nX = X.to(DEVICE)\nY = Y.detach().numpy()\n\nwith torch.no_grad():\n    pred=torch.sigmoid(model(X)).cpu().detach().numpy()\n    \npred_Y = (pred >= threshold)\ndel X","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:12:14.037021Z","iopub.execute_input":"2021-12-24T03:12:14.037427Z","iopub.status.idle":"2021-12-24T03:14:14.221993Z","shell.execute_reply.started":"2021-12-24T03:12:14.03739Z","shell.execute_reply":"2021-12-24T03:14:14.22107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a function to plot Real mask, Predicted mask and Overlapping mask with following colors:\n\n***green: correct prediction***\n\n***gray: false positive (too much)***\n\n***orange: false negative (missed)***","metadata":{}},{"cell_type":"code","source":"def plot(Y_val, Y_hat):\n    output = np.zeros_like(Y_val)\n    output = np.where((Y_val == 0) & (Y_hat == 1), 1, output)\n    output = np.where((Y_val == 1) & (Y_hat == 0), 2, output)\n    output = np.where((Y_val == 1) & (Y_hat == 1), 3, output)\n\n    \n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 15),)\n    ax1.imshow(Y_val,cmap='gray')\n    ax2.imshow(((Y_hat * 255)), cmap='gray')\n    ax3.imshow(output, cmap=cmap)\n    ax1.set_axis_off()\n    ax2.set_axis_off()\n    ax3.set_axis_off()\n    ax1.set_title('Real')\n    ax2.set_title('Output')\n    ax3.set_title('Real+Output')\n    plt.xticks([])\n    plt.yticks([]);","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:14.223738Z","iopub.execute_input":"2021-12-24T03:14:14.224013Z","iopub.status.idle":"2021-12-24T03:14:14.232117Z","shell.execute_reply.started":"2021-12-24T03:14:14.223973Z","shell.execute_reply":"2021-12-24T03:14:14.231106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 5\nfor i in range(N):\n    img_Y = Y[i, 0]\n    img_pred = pred_Y[i, 0]\n    \n    plot(img_Y, img_pred)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:14.233437Z","iopub.execute_input":"2021-12-24T03:14:14.233945Z","iopub.status.idle":"2021-12-24T03:14:15.490466Z","shell.execute_reply.started":"2021-12-24T03:14:14.233909Z","shell.execute_reply":"2021-12-24T03:14:15.489792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Save Results to a Pandas DataFrame </center></h2>","metadata":{}},{"cell_type":"code","source":"def get_scores(report_df,\n               name,loss_name, img_IoUs, \n               img_thresholds,metrics = None,\n               history = None,\n               load_weight = False):\n    '''Create and add metrics into a pandas DF after experiment'''\n    \n    report = pd.DataFrame(columns={'Criterion'}, data=[0])\n    \n    if load_weight:\n        train_loss, val_loss, val_acc, f1_macro = zip(*history)\n        report['Validation_loss'] = val_loss[-1]\n        report['Val_Acc_at_last_epoch'] = val_acc[-1]\n        report['Mean_acc_in_val_set'] = np.mean(val_acc)\n        report['F1_macro'] = f1_macro[-1]\n        report['mean_F1_along_epochs'] = np.mean(f1_macro)\n\n    \n    report['Criterion'] = loss_name\n    report['Average_Val_IoU'] = np.mean(img_IoUs)\n    report['Best_threshold'] = np.mean(img_thresholds)\n\n    report.index = [name]\n    report_df = report_df.append(report)\n    return report_df","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.491768Z","iopub.execute_input":"2021-12-24T03:14:15.492141Z","iopub.status.idle":"2021-12-24T03:14:15.502486Z","shell.execute_reply.started":"2021-12-24T03:14:15.4921Z","shell.execute_reply":"2021-12-24T03:14:15.501711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_report = pd.DataFrame(data=None)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.503871Z","iopub.execute_input":"2021-12-24T03:14:15.504142Z","iopub.status.idle":"2021-12-24T03:14:15.516372Z","shell.execute_reply.started":"2021-12-24T03:14:15.504103Z","shell.execute_reply":"2021-12-24T03:14:15.51557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_report = get_scores(df_report, 'Unet_BaseLine', 'ComboLoss',\n                      img_IoUs,img_thresholds)\ndf_report","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.517965Z","iopub.execute_input":"2021-12-24T03:14:15.518266Z","iopub.status.idle":"2021-12-24T03:14:15.534736Z","shell.execute_reply.started":"2021-12-24T03:14:15.51823Z","shell.execute_reply":"2021-12-24T03:14:15.533951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center>SUBMISSION</center></h1>","metadata":{}},{"cell_type":"markdown","source":"Define function for encoding masks","metadata":{}},{"cell_type":"code","source":"class NeuroDataSet(Dataset):\n    def __init__(self, files, transform=None):   \n        self.files = files\n        self.height = HEIGHT\n        self.width = WIDTH\n        self.transform = transform\n        self.len_ = len(self.files)\n        \n    \n    def load_sample(self, file):\n        img = cv2.imread(str(file), cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_id = file.stem\n        return img, img_id\n    \n    def __getitem__(self,index):\n        # load image\n        img, img_id = self.load_sample(self.files[index])\n        \n        if self.transform is not None:\n            augmented = self.transform(image = img)\n            img = augmented['image']\n            return img,img_id\n        else:\n            return img,img_id\n     \n    def __len__(self):\n        return self.len_","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.53592Z","iopub.execute_input":"2021-12-24T03:14:15.536567Z","iopub.status.idle":"2021-12-24T03:14:15.547371Z","shell.execute_reply.started":"2021-12-24T03:14:15.536527Z","shell.execute_reply":"2021-12-24T03:14:15.546642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = NeuroDataSet(test_files,\n                             transform=get_augmentation(224, 'test')\n                            )","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.548747Z","iopub.execute_input":"2021-12-24T03:14:15.549032Z","iopub.status.idle":"2021-12-24T03:14:15.556736Z","shell.execute_reply.started":"2021-12-24T03:14:15.548989Z","shell.execute_reply":"2021-12-24T03:14:15.555993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(test_dataset,\n                          batch_size=3,\n                          shuffle=False,\n                          pin_memory=True,\n                          num_workers=WORKERS,)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.558249Z","iopub.execute_input":"2021-12-24T03:14:15.558507Z","iopub.status.idle":"2021-12-24T03:14:15.568342Z","shell.execute_reply.started":"2021-12-24T03:14:15.558469Z","shell.execute_reply":"2021-12-24T03:14:15.567613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef predict_mask(image,model,device=DEVICE):\n    model.eval()\n#     image = image.view(-1,3,224,224)\n    image = image.to(device)\n    y_pred = torch.sigmoid(model(image)).cpu().detach().numpy()\n#     y_pred = np.squeeze(y_pred,axis=0)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.569556Z","iopub.execute_input":"2021-12-24T03:14:15.570298Z","iopub.status.idle":"2021-12-24T03:14:15.578412Z","shell.execute_reply.started":"2021-12-24T03:14:15.57026Z","shell.execute_reply":"2021-12-24T03:14:15.577714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, im_id = next(iter(test_loader))\npreds_test = predict_mask(img,model)\n#preds_test = preds_test >= threshold","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:15.57959Z","iopub.execute_input":"2021-12-24T03:14:15.580443Z","iopub.status.idle":"2021-12-24T03:14:16.744003Z","shell.execute_reply.started":"2021-12-24T03:14:15.580403Z","shell.execute_reply":"2021-12-24T03:14:16.743123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mask_convert(mask):\n    #mask = mask.clone().cpu().detach().numpy()\n    mask = mask.transpose((1,2,0))\n    mask = np.squeeze(mask)\n    return mask","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:16.747705Z","iopub.execute_input":"2021-12-24T03:14:16.747946Z","iopub.status.idle":"2021-12-24T03:14:16.75336Z","shell.execute_reply.started":"2021-12-24T03:14:16.747916Z","shell.execute_reply":"2021-12-24T03:14:16.751371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:16.754882Z","iopub.execute_input":"2021-12-24T03:14:16.755149Z","iopub.status.idle":"2021-12-24T03:14:16.763244Z","shell.execute_reply.started":"2021-12-24T03:14:16.755114Z","shell.execute_reply":"2021-12-24T03:14:16.762443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for pred in preds_test:\n    pred = mask_convert(pred)\n    plt.imshow(pred)\n    plt.title('pred before resize')\n    plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:16.764635Z","iopub.execute_input":"2021-12-24T03:14:16.764931Z","iopub.status.idle":"2021-12-24T03:14:17.080797Z","shell.execute_reply.started":"2021-12-24T03:14:16.764896Z","shell.execute_reply":"2021-12-24T03:14:17.080034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_mask: after reshape before fix_overlapping\ntest_masks = []\nfor pred in preds_test:\n    pred = mask_convert(pred)\n    test_mask=cv2.resize(pred,dsize=(704,520),interpolation=cv2.INTER_CUBIC).reshape(520,704,1)\n    print(test_mask.shape)\n    test_mask = test_mask >= threshold\n    test_masks.append(test_mask)\n        \n    plt.imshow(test_mask)\n    plt.title('pred after resize')\n    plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:17.081998Z","iopub.execute_input":"2021-12-24T03:14:17.082762Z","iopub.status.idle":"2021-12-24T03:14:17.736439Z","shell.execute_reply.started":"2021-12-24T03:14:17.082708Z","shell.execute_reply":"2021-12-24T03:14:17.735547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_masks[0].shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:17.737601Z","iopub.execute_input":"2021-12-24T03:14:17.738072Z","iopub.status.idle":"2021-12-24T03:14:17.744032Z","shell.execute_reply.started":"2021-12-24T03:14:17.738006Z","shell.execute_reply":"2021-12-24T03:14:17.743176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_overlap(msk):\n    msk = msk.astype(np.bool).astype(np.uint8)\n    return np.any(np.sum(msk, axis=-1)>1)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:17.746012Z","iopub.execute_input":"2021-12-24T03:14:17.746275Z","iopub.status.idle":"2021-12-24T03:14:17.754916Z","shell.execute_reply.started":"2021-12-24T03:14:17.746242Z","shell.execute_reply":"2021-12-24T03:14:17.753987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:17.756066Z","iopub.execute_input":"2021-12-24T03:14:17.756477Z","iopub.status.idle":"2021-12-24T03:14:19.035112Z","shell.execute_reply.started":"2021-12-24T03:14:17.756439Z","shell.execute_reply":"2021-12-24T03:14:19.034293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_overlap(msk):\n    \"\"\"\n    Args:\n        mask: multi-channel mask, each channel is an instance of cell, shape:(520,704,None)\n    Returns:\n        multi-channel mask with non-overlapping values, shape:(520,704,None)\n    \"\"\"\n    msk = np.array(msk)\n    msk = np.pad(msk, [[0,0],[0,0],[1,0]])\n    ins_len = msk.shape[-1]\n    msk = np.argmax(msk,axis=-1)\n    msk = tf.keras.utils.to_categorical(msk, num_classes=ins_len)\n    msk = msk[...,1:]\n    msk = msk[...,np.any(msk, axis=(0,1))]\n    return msk","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:19.036291Z","iopub.execute_input":"2021-12-24T03:14:19.038209Z","iopub.status.idle":"2021-12-24T03:14:19.047037Z","shell.execute_reply.started":"2021-12-24T03:14:19.038178Z","shell.execute_reply":"2021-12-24T03:14:19.046287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for test_mask in test_masks:\n    overlap_test_masks=check_overlap(test_mask)\n    print(overlap_test_masks)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:19.048448Z","iopub.execute_input":"2021-12-24T03:14:19.04876Z","iopub.status.idle":"2021-12-24T03:14:19.063108Z","shell.execute_reply.started":"2021-12-24T03:14:19.048712Z","shell.execute_reply":"2021-12-24T03:14:19.062312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_mask2: after reshape after fix_overlapping\ntest_masks2=[]\nfor test_mask in test_masks:\n    test_mask2 = fix_overlap(test_mask).reshape(520,704,1)\n    print(test_mask2.shape)\n    test_masks2+=[test_mask2]","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:19.064503Z","iopub.execute_input":"2021-12-24T03:14:19.064784Z","iopub.status.idle":"2021-12-24T03:14:19.239413Z","shell.execute_reply.started":"2021-12-24T03:14:19.064748Z","shell.execute_reply":"2021-12-24T03:14:19.238702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for test_mask2 in test_masks2:\n    overlap_test_masks2=check_overlap(test_mask2)\n    print(overlap_test_masks2)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:19.240695Z","iopub.execute_input":"2021-12-24T03:14:19.241113Z","iopub.status.idle":"2021-12-24T03:14:19.248745Z","shell.execute_reply.started":"2021-12-24T03:14:19.241076Z","shell.execute_reply":"2021-12-24T03:14:19.248039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted2 = [rle_encoding(test_mask2) for test_mask2 in test_masks2]","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:19.250149Z","iopub.execute_input":"2021-12-24T03:14:19.250619Z","iopub.status.idle":"2021-12-24T03:14:19.345789Z","shell.execute_reply.started":"2021-12-24T03:14:19.25058Z","shell.execute_reply":"2021-12-24T03:14:19.345088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img,img_name in test_loader:\n    masks = predict_mask(img,model)\n    print(masks.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:19.347127Z","iopub.execute_input":"2021-12-24T03:14:19.347392Z","iopub.status.idle":"2021-12-24T03:14:20.000625Z","shell.execute_reply.started":"2021-12-24T03:14:19.347356Z","shell.execute_reply":"2021-12-24T03:14:19.999017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission=pd.read_csv(SAMPLE_SUBMISSION)\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.002386Z","iopub.execute_input":"2021-12-24T03:14:20.002982Z","iopub.status.idle":"2021-12-24T03:14:20.01936Z","shell.execute_reply.started":"2021-12-24T03:14:20.002939Z","shell.execute_reply":"2021-12-24T03:14:20.018567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = sample_submission.copy()\nsubmit['predicted'] = predicted2\n#submit.to_csv('submission.csv', index=False)\nsubmit","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.020443Z","iopub.execute_input":"2021-12-24T03:14:20.020715Z","iopub.status.idle":"2021-12-24T03:14:20.030652Z","shell.execute_reply.started":"2021-12-24T03:14:20.020678Z","shell.execute_reply":"2021-12-24T03:14:20.029728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids=sample_submission['id'].tolist()\nprint(ids)\nsubmit2=pd.DataFrame(columns=sample_submission.columns)  \n\nfor idi, test_mask2 in zip(ids,test_masks2):\n    annos=rle_encoding(test_mask2)\n    annos2=annos.split(' ')\n    annos4=[]\n    for i in range(len(annos2)//100):\n        annos3=''\n        for j in range(100):\n            annos3+=annos2[i*100+j]+' '\n        annos4+=[annos3]\n        \n    submit=pd.DataFrame(columns=sample_submission.columns)    \n    submit['predicted']=annos4\n    submit['id']=idi\n    submit2=pd.concat([submit2,submit],axis=0)\nsubmit2=submit2.reset_index(drop=True)\n    \nsubmit2.to_csv('submission.csv', index=False)\nsubmit2","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.032383Z","iopub.execute_input":"2021-12-24T03:14:20.032848Z","iopub.status.idle":"2021-12-24T03:14:20.158685Z","shell.execute_reply.started":"2021-12-24T03:14:20.032812Z","shell.execute_reply":"2021-12-24T03:14:20.157886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_masks[0])\nplt.title('before fix_overlapping')\nplt.axis(\"off\")\nplt.show()\n\nplt.imshow(test_masks2[0])\nplt.title('after fix_overlapping')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.159905Z","iopub.execute_input":"2021-12-24T03:14:20.16021Z","iopub.status.idle":"2021-12-24T03:14:20.436944Z","shell.execute_reply.started":"2021-12-24T03:14:20.16017Z","shell.execute_reply":"2021-12-24T03:14:20.436251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NERABOTAYUSHAYA HERNYA","metadata":{}},{"cell_type":"code","source":"# def rle_encoding(x):\n#     dots = np.where(x.flatten() == 1)[0]\n#     run_lengths = []\n#     prev = -2\n#     for b in dots:\n#         if (b>prev+1): run_lengths.extend((b + 1, 0))\n#         run_lengths[-1] += 1\n#         prev = b\n#     return ' '.join(map(str, run_lengths))\n\n\n# def remove_overlapping_pixels(mask, other_masks):\n#     for other_mask in other_masks:\n#         if np.sum(np.logical_and(mask, other_mask)) > 0:\n#             mask[np.logical_and(mask, other_mask)] = 0\n#     return mask","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.438274Z","iopub.execute_input":"2021-12-24T03:14:20.439021Z","iopub.status.idle":"2021-12-24T03:14:20.443347Z","shell.execute_reply.started":"2021-12-24T03:14:20.438986Z","shell.execute_reply":"2021-12-24T03:14:20.442384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # converting tensor to image\n# def image_convert(image):\n#     image = image.clone().cpu().numpy()\n#     image = image.transpose((1,2,0))\n#     mean = np.array([0.485, 0.456, 0.406])\n#     std = np.array([0.229, 0.224, 0.225])\n#     image  = std * image + mean\n#     image = image.clip(0,1)\n#     image = (image * 255).astype(np.uint8)\n#     return image\n\n# def mask_convert(mask):\n#     #mask = mask.clone().cpu().detach().numpy()\n#     mask = mask.transpose((1,2,0))\n#     mask = np.squeeze(mask)\n#     return mask","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.44462Z","iopub.execute_input":"2021-12-24T03:14:20.445322Z","iopub.status.idle":"2021-12-24T03:14:20.452276Z","shell.execute_reply.started":"2021-12-24T03:14:20.445285Z","shell.execute_reply":"2021-12-24T03:14:20.451325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def plot(img, Y_hat):\n    \n#     img = image_convert(img)\n#     Y_hat = mask_convert(Y_hat)\n#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 15),)\n#     ax1.imshow(img,)\n#     ax2.imshow((Y_hat), cmap='seismic')\n# #     ax2.imshow(((Y_hat * 255)), cmap='gray')\n#     ax1.set_axis_off()\n#     ax2.set_axis_off()\n#     ax1.set_title('Real')\n#     ax2.set_title('Output')\n#     plt.xticks([])\n#     plt.yticks([]);","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.453616Z","iopub.execute_input":"2021-12-24T03:14:20.453988Z","iopub.status.idle":"2021-12-24T03:14:20.4631Z","shell.execute_reply.started":"2021-12-24T03:14:20.453955Z","shell.execute_reply":"2021-12-24T03:14:20.462382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @torch.no_grad()\n# def predict_mask(image,model,device=DEVICE):\n#     model.eval()\n#     image = image.view(-1,3,224,224)\n#     image = image.to(device)\n#     y_pred = torch.sigmoid(model(image)).cpu().detach().numpy()\n#     y_pred = np.squeeze(y_pred,axis=0)\n#     return y_pred","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.466035Z","iopub.execute_input":"2021-12-24T03:14:20.466241Z","iopub.status.idle":"2021-12-24T03:14:20.471394Z","shell.execute_reply.started":"2021-12-24T03:14:20.466217Z","shell.execute_reply":"2021-12-24T03:14:20.470678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# threshold = best_threshold\n# submission = []\n# img, img_id = next(iter(test_loader))\n# for i, im_id in enumerate(img_id):\n#     previous_masks = []\n#     result = predict_mask(img[i],model) \n#     binary_mask = (result >= threshold) \n#     #binary_mask = result > 0.5\n#     binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n#     previous_masks.append(binary_mask)\n#     rle = rle_encoding(binary_mask)\n#     submission.append((im_id, rle))\n#     plot(img[i],binary_mask)\n    \n#     all_images_ids = [im_id for im_id, rle in submission]\n#     if im_id not in all_images_ids:\n#         submission.append((im_id, \"\"))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.474448Z","iopub.execute_input":"2021-12-24T03:14:20.474676Z","iopub.status.idle":"2021-12-24T03:14:20.480905Z","shell.execute_reply.started":"2021-12-24T03:14:20.474627Z","shell.execute_reply":"2021-12-24T03:14:20.480136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\n# df_sub.to_csv(\"submission.csv\", index=False)\n# df_sub","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.482103Z","iopub.execute_input":"2021-12-24T03:14:20.482494Z","iopub.status.idle":"2021-12-24T03:14:20.49292Z","shell.execute_reply.started":"2021-12-24T03:14:20.482388Z","shell.execute_reply":"2021-12-24T03:14:20.492185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HEROVAYA HERNYA","metadata":{}},{"cell_type":"code","source":"# test_dataset = NeuroDataSet(test_files,\n#                              transform=get_augmentation(224, 'test')\n#                             )\n\n# test_loader = DataLoader(test_dataset,\n#                           batch_size=3,\n#                           shuffle=False,\n#                           pin_memory=True,\n#                           num_workers=WORKERS,)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.494272Z","iopub.execute_input":"2021-12-24T03:14:20.494668Z","iopub.status.idle":"2021-12-24T03:14:20.50099Z","shell.execute_reply.started":"2021-12-24T03:14:20.494592Z","shell.execute_reply":"2021-12-24T03:14:20.500357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for image,img_id in test_loader:\n#     print(img_id)\n#     print(image.shape)\n#     break","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.502405Z","iopub.execute_input":"2021-12-24T03:14:20.502753Z","iopub.status.idle":"2021-12-24T03:14:20.509065Z","shell.execute_reply.started":"2021-12-24T03:14:20.502631Z","shell.execute_reply":"2021-12-24T03:14:20.508267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# @torch.no_grad()\n# def predict_mask(image,model,device=DEVICE):\n#     model.eval()\n#     image = image.to(device)\n#     y_pred = torch.sigmoid(model(image)).cpu().detach().numpy()\n#     return y_pred\n\n# def mask_convert(mask):\n#     #mask = mask.clone().cpu().detach().numpy()\n#     mask = mask.transpose((1,2,0))\n#     mask = np.squeeze(mask)\n#     return mask\n\n# # converting tensor to image\n# def image_convert(image):\n#     image = image.clone().cpu().numpy()\n#     image = image.transpose((1,2,0))\n#     mean = np.array([0.485, 0.456, 0.406])\n#     std = np.array([0.229, 0.224, 0.225])\n#     image  = std * image + mean\n#     image = image.clip(0,1)\n#     image = (image * 255).astype(np.uint8)\n#     return image","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.510467Z","iopub.execute_input":"2021-12-24T03:14:20.51077Z","iopub.status.idle":"2021-12-24T03:14:20.51763Z","shell.execute_reply.started":"2021-12-24T03:14:20.510722Z","shell.execute_reply":"2021-12-24T03:14:20.516844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for img,img_names in test_loader:\n#     masks = predict_mask(img,model)\n#     for i,mask in enumerate(masks):\n#         mask = mask_convert(mask)\n#         mask = np.where(mask > 0.5, 1, 0)\n#         image = img[i]\n#         image = image_convert(image)\n#         fig,axes = plt.subplots(1,2,figsize=(16,7))\n#         axes[0].imshow(image)\n#         axes[0].set_title('Image')\n#         axes[1].imshow(mask,cmap=\"gray\")\n#         axes[1].set_title('Predicted Mask')\n#         plt.suptitle(img_names[i])\n#         plt.tight_layout()\n#         plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.519367Z","iopub.execute_input":"2021-12-24T03:14:20.519747Z","iopub.status.idle":"2021-12-24T03:14:20.526124Z","shell.execute_reply.started":"2021-12-24T03:14:20.519704Z","shell.execute_reply":"2021-12-24T03:14:20.525468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def ins2rle(ins):\n#     '''\n#     img: numpy array, 1 - mask, 0 - background\n#     Returns run length as string formated\n#     '''\n#     ins    = np.array(ins)\n#     pixels = ins.flatten()\n#     pad    = np.array([0])\n#     pixels = np.concatenate([pad, pixels, pad])\n#     runs   = np.where(pixels[1:] != pixels[:-1])[0] + 1\n#     runs[1::2] -= runs[::2]\n#     rles = ' '.join(str(x) for x in runs)\n#     return rles\n\n# def mask2rle(mask, cutoff=0.5, min_object_size=1.0):\n#     \"\"\" Return run length encoding of mask. \n#         ref: https://www.kaggle.com/raoulma/nuclei-dsb-2018-tensorflow-u-net-score-0-352\n#     \"\"\"\n#     # segment image and label different objects\n#     lab_mask = skimage.morphology.label(mask > cutoff)\n    \n     \n#     # Keep only objects that are large enough.\n#     (mask_labels, mask_sizes) = np.unique(lab_mask, return_counts=True)\n    \n\n#     if (mask_sizes < min_object_size).any():\n# #         print(mask_sizes)\n#         mask_labels = mask_labels[mask_sizes < min_object_size]\n        \n#         for n in mask_labels:\n#             lab_mask[lab_mask == n] = 0\n#         lab_mask = skimage.morphology.label(lab_mask > cutoff) \n        \n#     rles = []\n#     # Loop over each object excluding the background labeled by 0.\n#     for i in range(1, lab_mask.max() + 1):\n#         rle = ins2rle(lab_mask == i)\n#         rles.append(rle)\n        \n#     return rles\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.527555Z","iopub.execute_input":"2021-12-24T03:14:20.527857Z","iopub.status.idle":"2021-12-24T03:14:20.537182Z","shell.execute_reply.started":"2021-12-24T03:14:20.527825Z","shell.execute_reply":"2021-12-24T03:14:20.536464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred_rles , pred_paths = [],[]\n# for img,img_name in test_loader:\n#     masks = predict_mask(img,model)\n#     for i,mask in enumerate(masks):\n#         mask = mask_convert(mask)\n#         msk = cv2.resize(mask,(704,520),interpolation=cv2.INTER_NEAREST)\n#         rle_list = mask2rle(msk)\n#         img_id = img_name[i].split('.')[0]\n#         idx_list = [img_id for i in range(len(rle_list))] \n#         pred_rles.extend(rle_list)\n#         pred_paths.extend(idx_list)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.538411Z","iopub.execute_input":"2021-12-24T03:14:20.538806Z","iopub.status.idle":"2021-12-24T03:14:20.54614Z","shell.execute_reply.started":"2021-12-24T03:14:20.53877Z","shell.execute_reply":"2021-12-24T03:14:20.545421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = pd.DataFrame({'id':pred_paths, 'predicted':pred_rles})\n# df.to_csv('submission.csv', index=False)\n# df = pd.read_csv('submission.csv')\n# df","metadata":{"execution":{"iopub.status.busy":"2021-12-24T03:14:20.547257Z","iopub.execute_input":"2021-12-24T03:14:20.548027Z","iopub.status.idle":"2021-12-24T03:14:20.555669Z","shell.execute_reply.started":"2021-12-24T03:14:20.547988Z","shell.execute_reply":"2021-12-24T03:14:20.555082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}