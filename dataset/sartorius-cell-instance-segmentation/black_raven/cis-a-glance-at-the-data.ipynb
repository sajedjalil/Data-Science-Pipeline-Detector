{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"header\"></a>\n<h1 style=\"background:Orange;color:white;padding-top:20px\">\n    <center>\n        Sartorius - Cell Instance Segmentation\n    </center>\n</h1>\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/30201/logos/header.png\">","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div style=\"background: rgb(49,114,163);padding-bottom:10px\">\n<h1 style=\"text-align:center;color:white;\">Table of Contents</h1>\n</div>\n<div style=\"padding:10px;text-align:center;background:rgba(0,0,0,0.08)\">\n<a href=\"#first\" target=\"_self\">Problem Statement<span style=\"color:white;float:right;background:rgba(0,0,0,0.5);padding:5px;border-radius:10px\">1</span></a>\n</div>\n\n<div style=\"padding:10px;text-align:center;\">\n<a href=\"#second\" target=\"_self\">About Available Data<span style=\"color:white;float:right;background:rgba(0,0,0,0.5);padding:5px;border-radius:10px\">2</span></a>\n</div>\n\n<div style=\"padding:10px;text-align:center;background:rgba(0,0,0,0.08)\">\n<a href=\"#third\" target=\"_self\" >[updated] Data Exploration<span style=\"color:white;float:right;background:rgba(0,0,0,0.5);padding:5px;border-radius:10px\">3</span></a>\n</div>\n\n<div style=\"padding:10px;text-align:center\">\n<a href=\"#fourth\" target=\"_self\" >[updated] Vizualization of Images<span style=\"color:white;float:right;background:rgba(0,0,0,0.5);padding:5px;border-radius:10px\">4</span></a>\n</div>","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom termcolor import colored","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-17T05:09:38.994492Z","iopub.execute_input":"2021-10-17T05:09:38.995324Z","iopub.status.idle":"2021-10-17T05:09:40.205159Z","shell.execute_reply.started":"2021-10-17T05:09:38.995224Z","shell.execute_reply":"2021-10-17T05:09:40.204235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"first\"></a>\n<h1 style=\"background:purple;color:white;padding-top:20px\">\n    <center>\n        Problem Statement<span style=\"float:right\"><a href=\"#top\"><img src=\"https://www.clipartmax.com/png/middle/163-1630443_go-to-top-white-arrow-in-circle.png\" height=\"40px\" width=\"40px\"></a></span>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color:red;font-weight:bold\">This description has been copied from the competition page</p>\n<p style=\"font-family:callibri;font-size:1.2em\">Neurological disorders, including neurodegenerative diseases such as Alzheimer's and brain tumors, are a leading cause of death and disability across the globe. However, it is hard to quantify how well these deadly disorders respond to treatment. One accepted method is to review neuronal cells via light microscopy, which is both accessible and non-invasive. Unfortunately, segmenting individual neuronal cells in microscopic images can be challenging and time-intensive. Accurate instance segmentation of these cells, with the help of computer vision, could lead to new and effective drug discoveries to treat the millions of people with these disorders. Current solutions have limited accuracy for neuronal cells in particular. In internal studies to develop cell instance segmentation models, the neuroblastoma cell line SH-SY5Y consistently exhibits the lowest precision scores out of eight different cancer cell types tested. This could be because neuronal cells have a very unique, irregular and concave morphology associated with them, making them challenging to segment with commonly used mask heads.</p>\n<p style=\"font-family:callibri;font-size:1.2em\">In this competition, you‚Äôll detect and delineate distinct objects of interest in biological images depicting neuronal cell types commonly used in the study of neurological disorders. More specifically, you'll use phase contrast microscopy images to train and test your model for instance segmentation of neuronal cells. Successful models will do this with a high level of accuracy.</p>\n<h2 style=\"color:blue\"> Evaluation Metric: </h2>\n<p style=\"font-family:callibri;font-size:1.2em\">\nThis competition is evaluated on the mean average precision at different intersection over union  thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n<pre style=\"font-size:1.4em;text-align:center\"> ùêºùëúùëà(ùê¥,ùêµ) = ùê¥‚à©ùêµ/ùê¥‚à™ùêµ</pre>\n\n<p style=\"font-family:callibri;font-size:1.2em\">The metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\n <p style=\"font-family:callibri;font-size:1.2em\">\nAt each threshold value , a precision value is calculated based on the number of true positives , false negatives , and false positives  resulting from comparing the predicted object to all ground truth objects:</p>\n<pre style=\"font-size:1.4em;text-align:center\">ùëáùëÉ(ùë°) / (ùëáùëÉ(ùë°)+ùêπùëÉ(ùë°)+ùêπùëÅ(ùë°))</pre>\n<p style=\"font-family:callibri;font-size:1.2em\">A true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:\n\n<p style=\"font-family:callibri;font-size:1.2em\">\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"second\"></a>\n<h1 style=\"background:purple;color:white;padding-top:20px\">\n    <center>\n        About Available Data<span style=\"float:right\"><a href=\"#top\"><img src=\"https://www.clipartmax.com/png/middle/163-1630443_go-to-top-white-arrow-in-circle.png\" height=\"40px\" width=\"40px\"></a></span>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:callibri;font-size:1.2em\">This competition requires us to perform segmentation of neruonal cell in the images. The train and test images are provided in the respective folders. The annotations for each image is given in the train.csv file along with other info about the sample. The <b>train_semi_supervised</b> data has been provided incase the participants want to try a semi supervised approach to the problem. The <b>LIVECell_dataset_2021</b> data can be used as additional data for transfer learning.</p>\n<h2>Files</h2>\n<b>train.csv</b> - IDs and masks for all training objects. None of this metadata is provided for the test set.\n\n<ul>\n    <li>id - unique identifier for object</li>\n\n<li>annotation - run length encoded pixels for the identified neuronal cell</li>\n\n<li>width - source image width</li>\n\n<li>height - source image height</li>\n\n<li>cell_type - the cell line</li>\n\n<li>plate_time - time plate was created</li>\n\n<li>sample_date - date sample was created</li>\n\n<li>sample_id - sample identifier</li>\n\n<li>elapsed_timedelta - time since first image taken of sample</li>\n</ul>\n\n<b>sample_submission.csv</b> - a sample submission file in the correct format<br>\n\n<b>train</b> - train images in PNG format<br>\n\n<b>test</b> - test images in PNG format. Only a few test set images are available for download; the remainder can only be accessed by your notebooks when you submit.<br>\n\n<b>train_semi_supervised</b> - unlabeled images offered in case you want to use additional data for a semi-supervised approach.<br>\n\n<b>LIVECell_dataset_2021</b> - A mirror of the data from the LIVECell dataset. LIVECell is the predecessor dataset to this competition. You will find extra data for the SH-SHY5Y cell line, plus several other cell lines not covered in the competition dataset that may be of interest for transfer learning.<br>\n    <p style=\"font-family:callibri;font-size:1.2em\"> Now let's explore the given data and try to generate our own insights and some understanding.</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"third\"></a>\n<h1 style=\"background:purple;color:white;padding-top:20px\">\n    <center>\n       [updated] Data Exploration<span style=\"float:right\"><a href=\"#top\"><img src=\"https://www.clipartmax.com/png/middle/163-1630443_go-to-top-white-arrow-in-circle.png\" height=\"40px\" width=\"40px\"></a></span>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/sartorius-cell-instance-segmentation/train.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T05:09:43.86958Z","iopub.execute_input":"2021-10-17T05:09:43.86991Z","iopub.status.idle":"2021-10-17T05:09:44.489518Z","shell.execute_reply.started":"2021-10-17T05:09:43.869878Z","shell.execute_reply":"2021-10-17T05:09:44.488666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_imgs = len(os.listdir(\"../input/sartorius-cell-instance-segmentation/train\"))\nte_imgs = len(os.listdir(\"../input/sartorius-cell-instance-segmentation/test\"))\nprint(f\"Size of the train data: {colored(train.shape[0], 'red')}\")\nprint(f\"Number of images in train set: {colored(tr_imgs, 'red')}\")\nprint(f\"Number of images in test set: {colored(te_imgs, 'red')}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-17T05:10:16.325249Z","iopub.execute_input":"2021-10-17T05:10:16.325892Z","iopub.status.idle":"2021-10-17T05:10:16.453498Z","shell.execute_reply.started":"2021-10-17T05:10:16.325843Z","shell.execute_reply":"2021-10-17T05:10:16.452836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:callibri;font-size:1.2em\"> As you can see, although the number of train images is only 606, the size of the train set is sufficiently large. This is because a single image has multiple annotated objects. Since this is a code competition, the test set is hidden and one has to make notebook submissions. According to the host, the hidden test set has roughly 240 images. Let's see the number of annotations availble for each id.</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,6));\nvl = train[\"id\"].value_counts().to_frame().reset_index().rename(columns={'index':'id',\"id\":\"count\"})\nle = LabelEncoder()\nvl[\"id\"] = le.fit_transform(vl[\"id\"])\nsns.barplot(x=\"id\",y = \"count\", data=vl);\nplt.title(\"Number of samples for each Id\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-17T05:10:19.260125Z","iopub.execute_input":"2021-10-17T05:10:19.261892Z","iopub.status.idle":"2021-10-17T05:10:28.843006Z","shell.execute_reply.started":"2021-10-17T05:10:19.261843Z","shell.execute_reply":"2021-10-17T05:10:28.842378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-family:callibri;font-size:1.2em\"> While particular id may have upto roughly 800 annotations, majority of the ids have less than 100.</p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-family:callibri;font-size:1.2em\"> Now let's have a look at different cell_types. As mentioned in the description, cell_types indicates the different types of cell line. In this data, there are three differnt cell types. Majority of the samples have cell type <b>shsy5y</b></p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6));\nsns.countplot(y=\"cell_type\",data = train);\nplt.title(\"Counts of Cell Types\");","metadata":{"execution":{"iopub.status.busy":"2021-10-17T05:11:48.549923Z","iopub.execute_input":"2021-10-17T05:11:48.550661Z","iopub.status.idle":"2021-10-17T05:11:48.785324Z","shell.execute_reply.started":"2021-10-17T05:11:48.550614Z","shell.execute_reply":"2021-10-17T05:11:48.784431Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"fourth\"></a>\n<h1 style=\"background:purple;color:white;padding-top:20px\">\n    <center>\n        [updated] Vizualization of Images<span style=\"float:right\"><a href=\"#top\"><img src=\"https://www.clipartmax.com/png/middle/163-1630443_go-to-top-white-arrow-in-circle.png\" height=\"40px\" width=\"40px\"></a></span>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color:red;font-weight:bold\">This code has been copied from the <a href=\"https://www.kaggle.com/ihelon/cell-segmentation-run-length-decoding\">notebook</a> by <a href=\"https://www.kaggle.com/ihelon\">@ihelon</a></p>","metadata":{}},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height, width, channels) of array to return \n    color: color for the mask\n    Returns numpy array (mask)\n\n    '''\n    s = mask_rle.split()\n    \n    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n    lengths = list(map(int, s[1::2]))\n    ends = [x + y for x, y in zip(starts, lengths)]\n    \n    img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n            \n    for start, end in zip(starts, ends):\n        img[start : end] = color\n    \n    return img.reshape(shape)\n\ndef plot_masks(image_id, colors=True):\n    labels = train[train[\"id\"] == image_id][\"annotation\"].tolist()\n    cell_type = train[train[\"id\"] == image_id][\"cell_type\"].tolist()\n    cmap = {\"shsy5y\":(0,0,255),\"astro\":(0,255,0),\"cort\":(255,0,0)}\n\n    if colors:\n        mask = np.zeros((520, 704, 3))\n        for label,cell_type in zip(labels,cell_type):\n            c = cmap[cell_type]\n            mask += rle_decode(label, shape=(520, 704, 3), color=c)\n    else:\n        mask = np.zeros((520, 704, 1))\n        for label in labels:\n            mask += rle_decode(label, shape=(520, 704, 1))\n    mask = mask.clip(0, 1)\n\n    image = cv2.imread(f\"../input/sartorius-cell-instance-segmentation/train/{image_id}.png\")\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    plt.figure(figsize=(16, 32))\n    plt.subplot(3, 1, 1)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    plt.subplot(3, 1, 2)\n    plt.imshow(image)\n    plt.imshow(mask, alpha=0.5)\n    plt.axis(\"off\")\n    plt.subplot(3, 1, 3)\n    plt.imshow(mask)\n    plt.axis(\"off\")\n    \n    plt.show();","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-17T05:24:37.266939Z","iopub.execute_input":"2021-10-17T05:24:37.267636Z","iopub.status.idle":"2021-10-17T05:24:37.282899Z","shell.execute_reply.started":"2021-10-17T05:24:37.267588Z","shell.execute_reply":"2021-10-17T05:24:37.282083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_masks(\"ffdb3cc02eef\", colors=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T05:25:13.170491Z","iopub.execute_input":"2021-10-17T05:25:13.171399Z","iopub.status.idle":"2021-10-17T05:25:14.164238Z","shell.execute_reply.started":"2021-10-17T05:25:13.171353Z","shell.execute_reply":"2021-10-17T05:25:14.163232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_masks(\"0030fd0e6378\", colors=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T05:25:24.290609Z","iopub.execute_input":"2021-10-17T05:25:24.290891Z","iopub.status.idle":"2021-10-17T05:25:25.413556Z","shell.execute_reply.started":"2021-10-17T05:25:24.290865Z","shell.execute_reply":"2021-10-17T05:25:25.41263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_masks_all(image_ids, colors=True):\n    fig, ax = plt.subplots(len(image_ids),3,figsize=(16, 21))\n    for idx,image_id in enumerate(image_ids):\n        labels = train[train[\"id\"] == image_id][\"annotation\"].tolist()\n        cell_type = train[train[\"id\"] == image_id][\"cell_type\"].tolist()\n        cmap = {\"shsy5y\":(0,0,255),\"astro\":(0,255,0),\"cort\":(255,0,0)}\n\n        if colors:\n            mask = np.zeros((520, 704, 3))\n            for label,cell_t in zip(labels,cell_type):\n                c = cmap[cell_t]\n                mask += rle_decode(label, shape=(520, 704, 3), color=c)\n        else:\n            mask = np.zeros((520, 704, 1))\n            for label in labels:\n                mask += rle_decode(label, shape=(520, 704, 1))\n        mask = mask.clip(0, 1)\n\n        image = cv2.imread(f\"../input/sartorius-cell-instance-segmentation/train/{image_id}.png\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        ax[idx,0].imshow(image)\n        ax[idx,0].set_title(f\"Image {image_id}: cell_type {cell_t} Original\")\n        plt.axis(\"off\")\n        \n        ax[idx,1].imshow(image)\n        ax[idx,1].imshow(mask, alpha=0.5)\n        ax[idx,1].set_title(f\"Image with mask\")\n        plt.axis(\"off\")\n        \n        ax[idx,2].imshow(mask)\n        ax[idx,2].set_title(f\"Only Mask\")\n        plt.axis(\"off\")\n    plt.tight_layout()\n    fig.suptitle(\"Annotations Colored by Cell_Type: {blue: shsy5y, green: astro, red: cort}\",fontsize=16)\n    plt.show();\n\nplot_masks_all(['042dc0e561a4', '04928f0866b0', '049f02e0f764', '085eb8fec206'])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T05:45:08.969374Z","iopub.execute_input":"2021-10-17T05:45:08.96968Z","iopub.status.idle":"2021-10-17T05:45:12.688281Z","shell.execute_reply.started":"2021-10-17T05:45:08.96964Z","shell.execute_reply":"2021-10-17T05:45:12.685785Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Work in Progress!!! üößüößüöß","metadata":{}}]}