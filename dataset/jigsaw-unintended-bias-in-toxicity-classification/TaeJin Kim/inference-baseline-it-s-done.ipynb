{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Install & Load Library"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ignore warnings"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* load library"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport gc\nimport re\nimport sys\nimport pickle\nimport random\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm, tqdm_notebook\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nfrom nltk.tokenize.treebank import TreebankWordTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Util functions"},{"metadata":{},"cell_type":"markdown","source":"* set seed functions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set seed\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"* preprocessing of public kernel"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# preprocessing of public kernel\nclass Preprocess_pb_kernel:\n    \n    def __init__(self):\n        self.tokenizer_preprocess = TreebankWordTokenizer()\n   \n        symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\n        symbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\n\n        self.isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n        self.remove_dict = {ord(c):f'' for c in symbols_to_delete}\n    \n    def _handle_punctuation(self, x):\n        x = x.translate(self.remove_dict)\n        x = x.translate(self.isolate_dict)\n        return x\n\n    def _handle_contractions(self, x):\n        x = self.tokenizer_preprocess.tokenize(x)\n        return x\n\n    def _fix_quote(self, x):\n        x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n        x = ' '.join(x)\n        return x\n    \n    def preprocess(self, text):\n        text = self._handle_punctuation(text)\n        text = self._handle_contractions(text)\n        text = self._fix_quote(text)\n\n        return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocessing of sogna kernel"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# preprocessing of public kernel\nclass Preprocess_sogna_kernel:\n    \n    def __init__(self):\n        contraction_mapping = {\n            \"'cause\": 'because',\n            ',cause': 'because',\n            ';cause': 'because',\n            'Â´cause': 'because',\n            'â€™cause': 'because',\n\n            \"ain't\": 'am not',\n            'ain,t': 'am not', \n            'ain;t': 'am not',\n            'ainÂ´t': 'am not',\n            'ainâ€™t': 'am not',\n\n            \"aren't\": 'are not', \"arn't\": \"are not\",\n            'aren,t': 'are not', \"arn,t\": \"are not\",\n            'aren;t': 'are not', \"arn;t\": \"are not\",\n            'arenÂ´t': 'are not', \"arnÂ´t\": \"are not\",\n            'arenâ€™t': 'are not', \"arnâ€™t\": \"are not\",\n\n            \"isn't\": 'is not', \"is'nt\": \"is not\",\n            'isn,t': 'is not', \"is,nt\": \"is not\",\n            'isn;t': 'is not', \"is;nt\": \"is not\",\n            'isnÂ´t': 'is not', \"isÂ´nt\": \"is not\",\n            'isnâ€™t': 'is not', \"isâ€™nt\": \"is not\",\n\n            \"wasn't\": 'was not', \"was'nt\": \"was not\",\n            'wasn,t': 'was not', \"was,nt\": \"was not\",\n            'wasn;t': 'was not', \"was;nt\": \"was not\",\n            'wasnÂ´t': 'was not', \"wasÂ´nt\": \"was not\",\n            'wasnâ€™t': 'was not', \"wasâ€™nt\": \"was not\",\n\n            \"weren't\": 'were not',\n            'weren,t': 'were not',\n            'weren;t': 'were not',\n            'werenÂ´t': 'were not',\n            'werenâ€™t': 'were not',\n\n            \"didn't\": 'did not', \"d'int\": \"did not\", \"did'nt\": \"did not\", \"din't\": \"did not\",\n            'didn,t': 'did not', \"d,int\": \"did not\", \"did,nt\": \"did not\", \"din,t\": \"did not\",\n            'didn;t': 'did not', \"d;int\": \"did not\", \"did;nt\": \"did not\", \"din;t\": \"did not\",\n            'didnÂ´t': 'did not', \"dÂ´int\": \"did not\", \"didÂ´nt\": \"did not\", \"dinÂ´t\": \"did not\",\n            'didnâ€™t': 'did not', \"dâ€™int\": \"did not\", \"didâ€™nt\": \"did not\", \"dinâ€™t\": \"did not\",\n\n            \"doesn't\": 'does not', \"doens't\": \"does not\", \"dosen't\": \"does not\", \"dosn't\": \"does not\",\n            'doesn,t': 'does not', \"doens,t\": \"does not\", \"dosen,t\": \"does not\", \"dosn,t\": \"does not\",\n            'doesn;t': 'does not', \"doens;t\": \"does not\", \"dosen;t\": \"does not\", \"dosn;t\": \"does not\",\n            'doesnÂ´t': 'does not', \"doensÂ´t\": \"does not\", \"dosenÂ´t\": \"does not\", \"dosnÂ´t\": \"does not\",\n            'doesnâ€™t': 'does not', \"doensâ€™t\": \"does not\", \"dosenâ€™t\": \"does not\", \"dosnâ€™t\": \"does not\",\n\n            \"don't\": 'do not', \"dont't\": \"do not\",\n            'don,t': 'do not', \"dont,t\": \"do not\",\n            'don;t': 'do not', \"dont;t\": \"do not\",\n            'donÂ´t': 'do not', \"dontÂ´t\": \"do not\",\n            'donâ€™t': 'do not', \"dontâ€™t\": \"do not\",\n\n            'don\"\"t': \"do not\",\n\n            \"hadn't\": 'had not', \"hadn't've\": 'had not have', \"hasn't\": 'has not', \"haven't\": 'have not', \"havn't\": 'have not',\n            'hadn,t': 'had not', 'hadn,t,ve': 'had not have', 'hasn,t': 'has not', 'haven,t': 'have not', \"havn,t\": 'have not',\n            'hadn;t': 'had not', 'hadn;t;ve': 'had not have', 'hasn;t': 'has not', 'haven;t': 'have not', \"havn;t\": 'have not',\n            'hadnÂ´t': 'had not', 'hadnÂ´tÂ´ve': 'had not have', 'hasnÂ´t': 'has not', 'havenÂ´t': 'have not', \"havnÂ´t\": 'have not',\n            'hadnâ€™t': 'had not', 'hadnâ€™tâ€™ve': 'had not have', 'hasnâ€™t': 'has not', 'havenâ€™t': 'have not', \"havnâ€™t\": 'have not',\n\n            \"won't\": 'will not', \"will've\": \"will have\", \"won't've\": \"will not have\",\n            'won,t': 'will not', \"will,ve\": \"will have\", \"won,t,ve\": \"will not have\",\n            'won;t': 'will not', \"will;ve\": \"will have\", \"won;t;ve\": \"will not have\",\n            'wonÂ´t': 'will not', \"willÂ´ve\": \"will have\", \"wonÂ´tÂ´ve\": \"will not have\",\n            'wonâ€™t': 'will not', \"willâ€™ve\": \"will have\", \"wonâ€™tâ€™ve\": \"will not have\",\n\n            \"wouldn't\": 'would not', \"would've\": \"would have\", \"wouldn't've\": \"would not have\",\n            'wouldn,t': 'would not', \"would,ve\": \"would have\", \"wouldn,t,ve\": \"would not have\",\n            'wouldn;t': 'would not', \"would;ve\": \"would have\", \"wouldn;t;ve\": \"would not have\",\n            'wouldnÂ´t': 'would not', \"wouldÂ´ve\": \"would have\", \"wouldnÂ´tÂ´ve\": \"would not have\",\n            'wouldnâ€™t': 'would not', \"wouldâ€™ve\": \"would have\", \"wouldnâ€™tâ€™ve\": \"would not have\",\n\n            \"can't\": 'cannot',\"can't've\": 'cannot have',\n            'can,t': 'cannot','can,t,ve': 'cannot have',\n            'can;t': 'cannot','can;t;ve': 'cannot have',\n            'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have',\n            'canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',\n\n            \"could've\": 'could have', \"couldn't\": 'could not', \"couldn't've\": 'could not have', \"could'nt\": 'could not',\n            'could,ve': 'could have', 'couldn,t': 'could not', 'couldn,t,ve': 'could not have', \"could,nt\": 'could not',\n            'could;ve': 'could have', 'couldn;t': 'could not', 'couldn;t;ve': 'could not have', \"could;nt\": 'could not',\n            'couldÂ´ve': 'could have', 'couldnÂ´t': 'could not', 'couldnÂ´tÂ´ve': 'could not have', \"couldÂ´nt\": 'could not',\n            'couldâ€™ve': 'could have', 'couldnâ€™t': 'could not', 'couldnâ€™tâ€™ve': 'could not have', \"couldâ€™nt\": 'could not',\n\n            \"sha'n't\": 'shall not', \"shan't\": 'shall not', \"shan't've\": \"shall not have\", \n            'sha,n,t': 'shall not', 'shan,t': 'shall not', \"shan,t,ve\": \"shall not have\", \n            'sha;n;t': 'shall not', 'shan;t': 'shall not', \"shan;t;ve\": \"shall not have\", \n\n            \"should've\": 'should have', \"shouldn't\": 'should not', \"shouldn't've\": \"should not have\", \"shoudn't\": \"should not\",\n            'should,ve': 'should have', 'shouldn,t': 'should not', \"shouldn,t,ve\": \"should not have\", \"shoudn,t\": \"should not\",\n            'should;ve': 'should have', 'shouldn;t': 'should not', \"shouldn;t;ve\": \"should not have\", \"shoudn;t\": \"should not\",\n\n            \"mayn't\": 'may not',\n            'mayn,t': 'may not',\n            'mayn;t': 'may not',\n            'maynÂ´t': 'may not',\n            'maynâ€™t': 'may not',\n\n            \"might've\": 'might have', \"mightn't\": 'might not', \"mightn't've\": \"might not have\", \n            'might,ve': 'might have', 'mightn,t': 'might not', \"mightn,t,ve\": \"might not have\", \n            'might;ve': 'might have', 'mightn;t': 'might not', \"mightn;t;ve\": \"might not have\", \n\n            \"must've\": 'must have', \"mustn't\": 'must not', \"mustn't've\": \"must not have\",  \n            'must,ve': 'must have', 'mustn,t': 'must not', \"mustn,t,ve\": \"must not have\", \n            'must;ve': 'must have', 'mustn;t': 'must not', \"mustn;t;ve\": \"must not have\", \n\n            \"needn't\": 'need not', \"needn't've\": \"need not have\",\n            'needn,t': 'need not', \"needn,t,ve\": \"need not have\",\n            'needn;t': 'need not', \"needn;t;ve\": \"need not have\",\n\n            \"oughtn't\": 'ought not', \"oughtn't've\": \"ought not have\",\n            'oughtn,t': 'ought not', \"oughtn,t,ve\": \"ought not have\",\n            'oughtn;t': 'ought not', \"oughtn;t;ve\": \"ought not have\",\n\n            \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he's\": 'he is', \"he'll've\": \"he will have\",\n            'he,d': 'he would', 'he,d,ve': 'he would have', 'he,ll': 'he will', 'he,s': 'he is', \"he,ll,ve\": \"he will have\",\n            'he;d': 'he would', 'he;d;ve': 'he would have', 'he;ll': 'he will', 'he;s': 'he is', \"he;ll;ve\": \"he will have\",\n            'heÂ´d': 'he would', 'heÂ´dÂ´ve': 'he would have', 'heÂ´ll': 'he will', 'heÂ´s': 'he is', \"heÂ´llÂ´ve\": \"he will have\",\n            'heâ€™d': 'he would', 'heâ€™dâ€™ve': 'he would have', 'heâ€™ll': 'he will', 'heâ€™s': 'he is', \"heâ€™llâ€™ve\": \"he will have\",\n\n            \"she'd\": 'she would', \"she'll\": 'she will' ,\"she's\": 'she is', \"she'd've\": \"she would have\", \"she'll've\": \"she will have\",\n            'she,d': 'she would', 'she,ll': 'she will', 'she,s': 'she is', \"she,d,ve\": \"she would have\", \"she,ll,ve\": \"she will have\",\n            'she;d': 'she would', 'she;ll': 'she will', 'she;s': 'she is', \"she;d;ve\": \"she would have\", \"she;ll;ve\": \"she will have\",\n            'sheÂ´d': 'she would', 'sheÂ´ll': 'she will', 'sheÂ´s': 'she is', \"sheÂ´dÂ´ve\": \"she would have\", \"sheÂ´llÂ´ve\": \"she will have\",\n            'sheâ€™d': 'she would', 'sheâ€™ll': 'she will', 'sheâ€™s': 'she is', \"sheâ€™dâ€™ve\": \"she would have\", \"sheâ€™llâ€™ve\": \"she will have\",\n\n            \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"i've\": 'i have', \"i'd've\": \"i would have\", \"i'll've\": \"i will have\", \"i'ma\": \"i am\", \"i'am\": 'i am', \"i'l\": \"i will\", \"i'v\": 'i have',\n            'i,d': 'i would', 'i,ll': 'i will', 'i,m': 'i am', 'i,ve': 'i have', \"i,d,ve\": \"i would have\", \"i,ll,ve\": \"i will have\", \"i,ma\": \"i am\", \"i,am\": 'i am', \"i,l\": \"i will\", \"i,v\": 'i have',\n            'i;d': 'i would', 'i;ll': 'i will', 'i;m': 'i am', 'i;ve': 'i have', \"i;d;ve\": \"i would have\", \"i;ll;ve\": \"i will have\", \"i;ma\": \"i am\", \"i;am\": 'i am', \"i;l\": \"i will\", \"i;v\": 'i have',\n            'iÂ´d': 'i would', 'iÂ´ll': 'i will', 'iÂ´m': 'i am', 'iÂ´ve': 'i have', \"iÂ´dÂ´ve\": \"i would have\", \"iÂ´llÂ´ve\": \"i will have\", \"iÂ´ma\": \"i am\", \"iÂ´am\": 'i am', \"iÂ´l\": \"i will\", \"iÂ´v\": 'i have',\n            'iâ€™d': 'i would', 'iâ€™ll': 'i will', 'iâ€™m': 'i am', 'iâ€™ve': 'i have', \"iâ€™dâ€™ve\": \"i would have\", \"iâ€™llâ€™ve\": \"i will have\", \"iâ€™ma\": \"i am\", \"iâ€™am\": 'i am', \"iâ€™l\": \"i will\", \"iâ€™v\": 'i have',\n\n            'i\"\"m': 'i am',\n\n            \"we'd\": 'we would', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"we'd've\": \"we would have\", \"we'll've\": \"we will have\",\n            'we,d': 'we would', 'we,ll': 'we will', 'we,re': 'we are', 'we,ve': 'we have', \"we,d,ve\": \"we would have\", \"we,ll,ve\": \"we will have\",\n            'we;d': 'we would', 'we;ll': 'we will', 'we;re': 'we are', 'we;ve': 'we have', \"we;d;ve\": \"we would have\", \"we;ll;ve\": \"we will have\",\n            'weÂ´d': 'we would', 'weÂ´ll': 'we will', 'weÂ´re': 'we are', 'weÂ´ve': 'we have', \"weÂ´dÂ´ve\": \"we would have\", \"weÂ´llÂ´ve\": \"we will have\",\n            'weâ€™d': 'we would', 'weâ€™ll': 'we will', 'weâ€™re': 'we are', 'weâ€™ve': 'we have', \"weâ€™dâ€™ve\": \"we would have\", \"weâ€™llâ€™ve\": \"we will have\",\n\n            \"you'd\": 'you would', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": \"you have\", \"your'e\": \"you are\", \"u're\": \"you are\", \"ya'll\": \"you all\", \"you'r\": \"you are\",\n            'you,d': 'you would', 'you,ll': 'you will', 'you,re': 'you are', \"you,ve\": \"you have\", \"your,e\": \"you are\", \"u,re\": \"you are\", \"ya,ll\": \"you all\", \"you,r\": \"you are\", \n            'you;d': 'you would', 'you;ll': 'you will', 'you;re': 'you are', \"you;ve\": \"you have\", \"your;e\": \"you are\", \"u;re\": \"you are\", \"ya;ll\": \"you all\", \"you;r\": \"you are\",\n            'youÂ´d': 'you would', 'youÂ´ll': 'you will', 'youÂ´re': 'you are', \"youÂ´ve\": \"you have\", \"yourÂ´e\": \"you are\", \"uÂ´re\": \"you are\", \"yaÂ´ll\": \"you all\", \"youÂ´r\": \"you are\",\n            'youâ€™d': 'you would', 'youâ€™ll': 'you will', 'youâ€™re': 'you are', \"youâ€™ve\": \"you have\", \"yourâ€™e\": \"you are\", \"uâ€™re\": \"you are\", \"yaâ€™ll\": \"you all\", \"youâ€™r\": \"you are\",\n\n            \"y'all\": \"you all\", \"y'know\": \"you know\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n            \"y,all\": \"you all\", \"y,know\": \"you know\", \"y,all,d\": \"you all would\", \"y,all,d,ve\": \"you all would have\",\"y,all,re\": \"you all are\",\"y,all,ve\": \"you all have\", \n            \"y;all\": \"you all\", \"y;know\": \"you know\", \"y;all;d\": \"you all would\", \"y;all;d;ve\": \"you all would have\",\"y;all;re\": \"you all are\",\"y;all;ve\": \"you all have\", \n            \"yÂ´all\": \"you all\", \"yÂ´know\": \"you know\", \"yÂ´allÂ´d\": \"you all would\", \"yÂ´allÂ´dÂ´ve\": \"you all would have\",\"yÂ´allÂ´re\": \"you all are\",\"yÂ´allÂ´ve\": \"you all have\", \n            \"yâ€™all\": \"you all\", \"yâ€™know\": \"you know\", \"yâ€™allâ€™d\": \"you all would\", \"yâ€™allâ€™dâ€™ve\": \"you all would have\",\"yâ€™allâ€™re\": \"you all are\",\"yâ€™allâ€™ve\": \"you all have\", \n\n            \"you.i\": \"you i\",\n\n            \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"they'd've\": \"they would have\", \"they'll've\": \"they will have\",\n            'they,d': 'they would', 'they,ll': 'they will', 'they,re': 'they are', 'they,ve': 'they have', \"they,d,ve\": \"they would have\", \"they,ll,ve\": \"they will have\",\n            'they;d': 'they would', 'they;ll': 'they will', 'they;re': 'they are', 'they;ve': 'they have', \"they;d;ve\": \"they would have\", \"they;ll;ve\": \"they will have\",\n            'theyÂ´d': 'they would', 'theyÂ´ll': 'they will', 'theyÂ´re': 'they are', 'theyÂ´ve': 'they have', \"theyÂ´dÂ´ve\": \"they would have\", \"theyÂ´llÂ´ve\": \"they will have\",\n            'theyâ€™d': 'they would', 'theyâ€™ll': 'they will', 'theyâ€™re': 'they are', 'theyâ€™ve': 'they have', \"theyâ€™dâ€™ve\": \"they would have\", \"theyâ€™llâ€™ve\": \"they will have\",\n\n            \"it'd\": 'it would', \"it'll\": 'it will', \"it's\": 'it is', \"it'd've\": \"it would have\", \"it'll've\": \"it will have\",\n            'it,d': 'it would', 'it,ll': 'it will', 'it,s': 'it is', \"it,d,ve\": \"it would have\", \"it,ll,ve\": \"it will have\", \n            'it;d': 'it would', 'it;ll': 'it will', 'it;s': 'it is', \"it;d;ve\": \"it would have\", \"it;ll;ve\": \"it will have\",\n            'itÂ´d': 'it would', 'itÂ´ll': 'it will', 'itÂ´s': 'it is', \"itÂ´dÂ´ve\": \"it would have\", \"itÂ´llÂ´ve\": \"it will have\",\n            'itâ€™d': 'it would', 'itâ€™ll': 'it will', 'itâ€™s': 'it is', \"itâ€™dâ€™ve\": \"it would have\", \"itâ€™llâ€™ve\": \"it will have\",\n\n            \"this'll\": \"this all\", \"this's\": \"this is\",\n            \"this,ll\": \"this all\", \"this,s\": \"this is\",\n            \"this;ll\": \"this all\", \"this;s\": \"this is\",\n\n            \"that'd\": 'that would', \"that's\": 'that is', \"that'll\": \"that will\", \"that'd've\": \"that would have\",\n            'that,d': 'that would', 'that,s': 'that is', \"that,ll\": \"that will\", \"that,d,ve\": \"that would have\",\n            'that;d': 'that would', 'that;s': 'that is', \"that;ll\": \"that will\", \"that;d;ve\": \"that would have\",\n            'thatÂ´d': 'that would', 'thatÂ´s': 'that is', \"thatÂ´ll\": \"that will\", \"thatÂ´dÂ´ve\": \"that would have\",\n            'thatâ€™d': 'that would', 'thatâ€™s': 'that is', \"thatâ€™ll\": \"that will\", \"thatâ€™dâ€™ve\": \"that would have\",\n\n            \"there'd\": 'there had', \"there's\": 'there is', \"there'll\": \"there will\",\"there're\": \"there are\", \"there'd've\": \"there would have\",\n            'there,d': 'there had', 'there,s': 'there is', \"there,ll\": \"there will\",\"there,re\": \"there are\", \"there,d,ve\": \"there would have\",\n            'there;d': 'there had', 'there;s': 'there is', \"there;ll\": \"there will\",\"there;re\": \"there are\", \"there;d;ve\": \"there would have\",\n            'thereÂ´d': 'there had', 'thereÂ´s': 'there is', \"thereÂ´ll\": \"there will\",\"thereÂ´re\": \"there are\", \"thereÂ´dÂ´ve\": \"there would have\",\n            'thereâ€™d': 'there had', 'thereâ€™s': 'there is', \"thereâ€™ll\": \"there will\",\"thereâ€™re\": \"there are\", \"thereâ€™dâ€™ve\": \"there would have\",\n\n            \"here's\": \"here is\", \"here're\": \"here are\",\n            \"here,s\": \"here is\", \"here,re\": \"here are\",\n            \"here;s\": \"here is\", \"here;re\": \"here are\",\n\n            \"when's\": \"when is\", \"when've\": \"when have\", \"when're\": \"when are\",\n            \"when's\": \"when is\", \"when've\": \"when have\", \"when're\": \"when are\",\n            \"when's\": \"when is\", \"when've\": \"when have\", \"when're\": \"when are\",\n\n            \"where'd\": 'where did', \"where's\": 'where is', \"where've\": \"where have\", \"where're\": \"where are\",\n            'where,d': 'where did', 'where,s': 'where is', \"where,ve\": \"where have\", \"where,re\": \"where are\",\n            'where;d': 'where did', 'where;s': 'where is', \"where;ve\": \"where have\", \"where;re\": \"where are\",\n\n            \"who'll\": 'who will', \"who's\": 'who is', \"who'd\": \"who would\", \"who're\": \"who are\",\"who've\": \"who have\", \"who'll've\": \"who will have\",\n            'who,ll': 'who will', 'who,s': 'who is', \"who,d\": \"who would\", \"who,re\": \"who are\",\"who,ve\": \"who have\", \"who,ll,ve\": \"who will have\",\n            'who;ll': 'who will', 'who;s': 'who is', \"who;d\": \"who would\", \"who;re\": \"who are\",\"who;ve\": \"who have\", \"who;ll;ve\": \"who will have\",\n\n            \"how'd\": 'how did', \"how'll\": 'how will', \"how's\": 'how is', \"how'd'y\": \"how do you\",\n            'how,d': 'how did', 'how,ll': 'how will', 'how,s': 'how is', \"how,d,y\": \"how do you\",\n            'how;d': 'how did', 'how;ll': 'how will', 'how;s': 'how is', \"how;d;y\": \"how do you\",\n\n            \"what'll\": 'what will', \"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', \"what'll've\": \"what will have\",\n            'what,ll': 'what will', 'what,re': 'what are', 'what,s': 'what is', 'what,ve': 'what have', \"what,ll,ve\": \"what will have\",\n            'what;ll': 'what will', 'what;re': 'what are', 'what;s': 'what is', 'what;ve': 'what have', \"what;ll;ve\": \"what will have\",\n\n            \"why'd\": \"why would\", \"why'll\": \"why will\", \"why're\": \"why are\", \"why's\": \"why is\", \"why've\": \"why have\",\n            \"why,d\": \"why would\", \"why,ll\": \"why will\", \"why,re\": \"why are\", \"why,s\": \"why is\", \"why,ve\": \"why have\",\n            \"why;d\": \"why would\", \"why;ll\": \"why will\", \"why;re\": \"why are\", \"why;s\": \"why is\", \"why;ve\": \"why have\",\n\n            \"let's\": 'let us', 'let,s': 'let us', 'let;s': 'let us',\n\n            \"ma'am\": 'madam', 'ma,am': 'madam', 'ma;am': 'madam',\n\n            \"wan't\": 'want', \"wan,t\": 'want', \"wan;t\": 'want',\n\n            \"agains't\": \"against\", \"agains,t\": \"against\", \"agains;t\": \"against\",\n\n            \"c'mon\": \"common\", \"c,mon\": \"common\", \"c;mon\": \"common\",\n\n            \"gov't\": \"government\", \"gov,t\": \"government\", \"gov;t\": \"government\",\n\n            'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at', 'á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\n            'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first','á´‡É´á´…':'end',\n            'á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra','GÊ€á´‡á´€á´›':'great',\n            'sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\n            'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\n            'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', \n            'É´á´‡á´‡á´…':' need ','á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start',\n        }\n        \n        self.contraction_mapping = {**contraction_mapping, **self._get_upper_contraction(contraction_mapping)}\n    \n    \n        self.spaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\n\n        self.special_punc_mappings = {\n            \"â€”\": \"-\", \"â€“\": \"-\", \"_\": \"-\", 'âˆ’': '-', 'â€¢': '-', \"â€”\": \"-\", \"â€“\": \"-\", \"_\": \"-\", 'âˆ’': '-', \n\n            'â€': '\"', 'â€³': '\"', 'â€œ': '\"', 'â€œ': '\"', 'â€': '\"', 'â€œ': '\"', \n            \"â€™\": \"'\", \"â€˜\": \"'\", \"Â´\": \"'\", \"`\": \"'\", \"â€˜\": \"'\", \"Â´\": \"'\", \"â€™\": \"'\", \"`\": \"'\",\n\n            \"â‚¹\": \"e\", \"â‚¬\": \"e\", \"â„¢\": \"tm\",  \"Ã—\": \"x\", \"Â²\": \"2\", \"Â£\": \"e\", 'Ã ': 'a', 'Â³': '3', \n            \"âˆš\": \" sqrt \", 'Î±': 'alpha', 'Î²': 'beta', 'Î¸': 'theta', 'Ï€': 'pi', 'âˆž': 'infinity', 'Ã·': '/', 'âˆ…': '',\n\n            'ØŒ':'', 'â€ž':'', \"Â°\": \"\", 'à¤•à¤°à¤¨à¤¾': '', 'à¤¹à¥ˆ': '',\n            'â€¦': ' ... '\n        }\n\n        self.punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"â€œâ€â€™' + 'âˆžÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'\n\n    def _get_upper_contraction(self, mapping):\n        result_dict = {}\n        for key, value in mapping.items():\n            if key[0].isalpha():\n                key = key[0].upper() + key[1:]\n                value = value[0].upper() + value[1:]\n                result_dict[key] = value\n        return result_dict\n    \n\n    def _clean_contractions(self, text, mapping):\n        specials = [\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n        for s in specials:\n            text = text.replace(s, \"'\")\n        text = ' '.join([mapping[t] if t in mapping else t for t in text.split()])\n        return text\n    \n    \n    # remove space\n    def _remove_space(self, text):\n        for space in self.spaces:\n            text = text.replace(space, ' ')\n\n        text = text.strip()\n        text = re.sub('\\s+', ' ', text)\n\n        return text\n    \n    # remove special punctuations\n    def _clean_special_punctuations(self, text):\n        for punc in self.special_punc_mappings:\n            text = text.replace(punc, self.special_punc_mappings[punc])\n\n        return text\n    \n    def _spacing_punctuations(self, text):\n        for p in self.punct:\n            text = text.replace(p, f' {p} ')\n\n        return text\n    \n    def _preprocess_punctuations(self, text):\n        text = self._remove_space(text)\n        text = self._clean_special_punctuations(text)\n        text = self._spacing_punctuations(text)\n        text = self._remove_space(text)\n\n        return text\n    \n    def preprocess(self, text):\n        text = self._clean_contractions(text, self.contraction_mapping)\n        text = self._preprocess_punctuations(text)\n        \n        return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"### LSTM"},{"metadata":{},"cell_type":"markdown","source":"* spatial dropout"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2) # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1) # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x) # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1) # (N, T, 1, K)\n        x = x.squeeze(2) # (N, T, K)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* embedding"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Embedding_LSTM(nn.Module):\n    def __init__(self, embedding_matrix):\n        super(Embedding_LSTM, self).__init__()\n        \n        self.embedding = nn.Embedding(embedding_matrix.shape[0], EMBED_SIZE * 2)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.embedding_dropout(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* encoder"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Encoder_LSTM(nn.Module):\n    def __init__(self, num_aux_targets):\n        super(Encoder_LSTM, self).__init__()\n    \n        self.cell1 = nn.LSTM(EMBED_SIZE * 2, HIDDEN_SIZE, bidirectional=True, batch_first=True)\n        self.cell2 = nn.LSTM(HIDDEN_SIZE * 2, HIDDEN_SIZE, bidirectional=True, batch_first=True)\n        \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n                \n    def forward(self, x):\n        cell1, _ = self.cell1(x)\n        cell2, _ = self.cell2(cell1)\n        \n        avg_pool = torch.mean(cell2, 1)\n        max_pool, _ = torch.max(cell2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* lstm model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class Model_LSTM(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(Model_LSTM, self).__init__()\n        \n        self.embedding = Embedding_LSTM(embedding_matrix)\n        self.encoder = Encoder_LSTM(num_aux_targets)\n\n    def forward(self, x):\n        \n        x = self.embedding(x)\n        out = self.encoder(x) \n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceTemplate:\n    def __init__(self, preprocess_funcs=None, debug=False):\n        self.preprocess_funcs = preprocess_funcs\n        self.debug = debug\n    \n    # Data Related Functions\n    def _load_text(self, category=\"test\"):\n        \n        # load raw text\n        print(f\"Load {category} Text ...\")\n        path = f'../input/jigsaw-unintended-bias-in-toxicity-classification/{category}.csv'\n        text = pd.read_csv(path).comment_text.astype(str)\n        \n        # debug mode\n        if self.debug:\n            print(\"Debug Mode ...\")\n            text = text[:10000]\n        \n        # preprocessing\n        if self.preprocess_funcs:\n            print(\"Preprocessing ...\")\n            \n            for i, preprocess_func in enumerate(self.preprocess_funcs):\n                print(f\"---- Preprocessing {i}'th\")\n                text = text.apply(lambda v: preprocess_func.preprocess(v))\n        else:\n            print(\"No preprocessing ...\")\n            \n        print('\\n')\n        return text\n    \n    # Inference Related Functions\n    def _sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* lstm inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceLstm(InferenceTemplate):\n\n    def make_data(self, preprocess_type=None):\n        \n        \n        # load text\n        if preprocess_type == 'sogna':\n            print(\"Train Text Preprocessing ... [sogna]\")\n            train_text = pd.read_csv('../input/toxic-preprocessed-train-text/train_text_preprocessed_sogna.csv')['comment_text_sogna'].astype(str)\n        elif preprocess_type == 'pb':\n            print(\"Train Text Preprocessing ... [pb]\")\n            train_text = pd.read_csv('../input/toxic-preprocessed-train-text/train_text_preprocessed_pb.csv')['comment_text_pb'].astype(str)\n        elif preprocess_type == 'sogna_pb':\n            print(\"Train Text Preprocessing ... [sogna + pb]\")\n            train_text = pd.read_csv('../input/toxic-preprocessed-train-text/train_text_preprocessed_sogna_pb.csv')['comment_text_sogna_pb'].astype(str)\n        else:\n            train_text = self._load_text('train')\n            \n        if self.debug:\n            train_text = train_text[:10000]\n        \n        # train_text = self._load_text('train')\n        test_text = self._load_text('test')\n        \n        # tokenizer\n        tokenizer = Tokenizer(lower=False, filters=\"\")\n        tokenizer.fit_on_texts(list(train_text) + list(test_text))\n        \n        # get token (sequence)\n        test_text = tokenizer.texts_to_sequences(list(test_text))\n        \n        return test_text, tokenizer\n    \n    \n    # Embedding Related Functions\n    def _load_embeddings(self, path):\n        with open(path,'rb') as f:\n            emb_arr = pickle.load(f)\n        return emb_arr\n    \n    def _build_matrix(self, word_index, path):\n        embedding_index = self._load_embeddings(path)\n        embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n        unknown_words = []\n\n        for word, i in word_index.items():\n            try: embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try: embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try: embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError: unknown_words.append(word)\n\n        return embedding_matrix, unknown_words\n    \n    def make_embedding(self, tokenizer):\n        \n        # crawl\n        crawl_matrix, unkown_words = self._build_matrix(tokenizer.word_index,\n                                                        CRAWL_EMBEDDING_PATH)\n        \n        print('n unknown words (crawl): ', len(unkown_words))\n        \n        # glove\n        glove_matrix, unkown_words = self._build_matrix(tokenizer.word_index, \n                                                        GLOVE_EMBEDDING_PATH)\n        \n        print('n unknown words (glove): ', len(unkown_words))\n        \n        # embedding_matrix\n        embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\n        print(\"Embedding Matrix Shape: \", embedding_matrix.shape)\n        \n        del crawl_matrix, glove_matrix, unkown_words\n        gc.collect()\n        \n        return embedding_matrix\n    \n    # Model Related Functions\n    def make_model(self, embedding_matrix, path):\n        \n        # template\n        model = Model_LSTM(embedding_matrix, 7)\n        \n        # load weight\n        model.encoder.load_state_dict(torch.load(path)['model_state_dict'])\n        return model\n    \n    # Inference Related Functions\n    def _get_padding_size(self, lengths):\n        padding_size = 0\n        _max = np.max(lengths)\n        _threshold = np.ceil(np.quantile(lengths, 0.95))\n\n        if _max >= MAX_LEN:\n            padding_size = MAX_LEN\n\n        else:\n            padding_size = _max\n\n        return int(padding_size)\n    \n    def inference(self, test, model):\n        \n        model.cuda()\n        model.eval()\n        \n        total_batch = int(len(test) / BATCH_SIZE)\n        test_preds = np.zeros((len(test), 8))\n        \n        print(\"total batch: {}, test size: {}, batch size: {}\".format(total_batch, len(test), BATCH_SIZE))\n        \n        for i in range(total_batch + 1):\n\n            # get batch\n            batch_X = test[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n\n            # when data size divides by batch size\n            if len(batch_X) == 0: break\n\n            # adaptive padding size\n            padding_size = self._get_padding_size([len(v) for v in batch_X])\n            batch_X = pad_sequences(batch_X, maxlen=padding_size, padding='post')\n\n            # torch tensor\n            batch_X = torch.tensor(batch_X, dtype=torch.long).cuda()\n\n            # prediction\n            pred_y = model(batch_X).detach().squeeze(dim=-1)\n\n            # predict test target\n            test_preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE, :] = self._sigmoid(pred_y.cpu().numpy())\n            \n            del batch_X, pred_y\n            torch.cuda.empty_cache()\n            \n        return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* bert inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceBert(InferenceTemplate):\n    \n    # Data Related Functions\n    def _convert_lines(self, example, max_seq_length, tokenizer):\n        max_seq_length -=2\n        all_tokens = []\n        longer = 0\n        for text in tqdm_notebook(example):\n            tokens_a = tokenizer.tokenize(text)\n            if len(tokens_a) > max_seq_length:\n                tokens_a = tokens_a[:max_seq_length]\n                longer += 1\n            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (max_seq_length - len(tokens_a))\n            all_tokens.append(one_token)\n\n        print(\"Num Of Lines Over Max Sequences: {}/{}\".format(longer, len(all_tokens)))\n        return np.array(all_tokens)\n    \n    def make_data(self):\n        \n        # load text\n        test_text = self._load_text('test')\n            \n        # tokenize\n        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\n        \n        # get token (sequence)\n        test_text = self._convert_lines(test_text.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n        \n        return test_text\n    \n    # Model Related Functions\n    def make_model(self, path):\n        ckpt = torch.load(f'{path}', map_location='cuda:0')\n        \n        # bert config\n        bert_config = ckpt['bert_config']\n        \n        # model\n        model = BertForSequenceClassification(bert_config, num_labels=8)\n        model.load_state_dict(ckpt['model_state_dict'])\n        \n        return model\n    \n    # Inference Related Functions\n    def inference(self, test, model):\n        \n        model = model.to(device)\n\n        test_preds = np.zeros((len(test)))\n        test_dataset = data.TensorDataset(torch.tensor(test, dtype=torch.long))\n        test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        for param in model.parameters():\n            param.requires_grad=False\n        \n        model.eval()\n        \n        tq = tqdm_notebook(test_loader)\n        for i, (x_batch,)  in enumerate(tq):\n            pred = model(x_batch.to(device),\n                         attention_mask = (x_batch > 0).to(device), \n                         labels=None)\n            \n            # print(pred[:, 0].detach().cpu().squeeze().numpy())\n\n            test_preds[i*BATCH_SIZE:(i+1)*BATCH_SIZE]= self._sigmoid(pred[:,0].detach().cpu().squeeze().numpy())\n\n            del x_batch, pred\n            torch.cuda.empty_cache()\n\n        return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* gpt2 inference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class InferenceGPT2(InferenceTemplate):\n    \n    # Data Related Functions\n    def _convert_lines(self, example, max_seq_length, tokenizer):\n        all_tokens = []\n        longer = 0\n        for text in tqdm_notebook(example):\n            tokens_a = tokenizer.tokenize(text)\n            if len(tokens_a) > max_seq_length:\n                tokens_a = tokens_a[:max_seq_length]\n                longer += 1\n            one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0] * (max_seq_length - len(tokens_a))\n            all_tokens.append(one_token)\n\n        print(\"Num Of Lines Over Max Sequences: {}/{}\".format(longer, len(all_tokens)))\n        return np.array(all_tokens)\n    \n    def make_data(self):\n        \n        # load text\n        test_text = self._load_text('test')\n            \n        # tokenize\n        tokenizer = GPT2Tokenizer.from_pretrained(GPT2_MODEL_PATH, cache_dir=None)\n        \n        # get token (sequence)\n        test_text = self._convert_lines(test_text.fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)\n        \n        return test_text\n    \n    # Model Related Functions\n    def make_model(self, path):\n        ckpt = torch.load(f'{path}')\n        \n        # bert config\n        bert_config = ckpt['bert_config']\n        \n        # model\n        model = GPT2ClassificationHeadModel(bert_config)\n        model.load_state_dict(ckpt['model_state_dict'])\n        \n        return model\n    \n    # Inference Related Functions\n    def inference(self, test, model):\n        \n        test_preds = np.zeros((len(test)))\n        test_dataset = data.TensorDataset(torch.tensor(test, dtype=torch.long))\n        test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        for param in model.parameters():\n            param.requires_grad=False\n        \n        model.cuda()\n        model.eval()\n        \n        tq = tqdm_notebook(test_loader)\n        for i, (x_batch,)  in enumerate(tq):\n            pred = model(x_batch.cuda())\n            \n            # print(self._sigmoid(pred[:, 0].detach().cpu().squeeze().numpy()))\n            \n            test_preds[i*BATCH_SIZE:(i+1)*BATCH_SIZE]= self._sigmoid(pred[:,0].detach().cpu().squeeze().numpy())\n\n            del x_batch, pred\n            torch.cuda.empty_cache()\n\n        return test_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"debug = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 42\nseed_everything(random_state)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocess pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pb = Preprocess_pb_kernel()\npreprocess_sogna = Preprocess_sogna_kernel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pipe = [preprocess_sogna]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"# constants\nCRAWL_EMBEDDING_PATH = '../input/pickled-crawl300d2m/pickled-crawl300d2m/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '../input/pickled-glove840b300d/pickled-glove840b300d/glove.840B.300d.pkl'\nEMBED_SIZE = 300\nHIDDEN_SIZE = 128\nDENSE_HIDDEN_UNITS = 4 * HIDDEN_SIZE\nBATCH_SIZE = 512 * 8\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* inference class"},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_lstm = InferenceLstm(\n    preprocess_funcs=preprocess_pipe, \n    debug=debug\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* make data and tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text, tokenizer = inference_lstm.make_data(preprocess_type='sogna')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* make embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = inference_lstm.make_embedding(tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nall_test_preds = []\n\nfor model_idx in range(2):\n    test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(4)]\n\n    for epoch in range(4):  \n        path = f'../input/sub-toxic-lstm-weight/lstm_wh_model_idx_{model_idx}_epcoh_{epoch}.pt'\n        model = inference_lstm.make_model(embedding_matrix, path=path)\n        test_preds.append(inference_lstm.inference(test_text, model))\n\n    all_test_preds.append(np.average(test_preds, weights=checkpoint_weights, axis=0))\n\ntest_preds_lstm = np.mean(all_test_preds, axis=0)[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embedding_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT2"},{"metadata":{},"cell_type":"markdown","source":"* load library"},{"metadata":{"trusted":true},"cell_type":"code","source":"PYTORCH_BERT_DIR = \"../input/gpt2source/gpt2-pytorch/pytorch-pretrained-BERT-master/\"\nsys.path.insert(0, PYTORCH_BERT_DIR)\n\nfrom pytorch_pretrained_bert import GPT2Tokenizer, GPT2ClassificationHeadModel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocess pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pipe = [preprocess_sogna, preprocess_pb]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"# constants\nGPT2_MODEL_PATH = '../input/gpt2-pretrained-models/gpt2-models/'\nMAX_SEQUENCE_LENGTH = 260\nBATCH_SIZE = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_gpt2 = InferenceGPT2(\n    preprocess_funcs=preprocess_pipe, \n    debug=debug\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = inference_gpt2.make_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_gpt2.make_model('../input/sub-toxic-gpt2-weight/gpt2_dis_260len_2epoch_32batch_4accum_8e-05lr_0.005warmup_0.2dropout_all_full.pt')\ntest_preds_gpt2 = inference_gpt2.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* delete library"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"lib_modules_list = [s for s in list(sys.modules.keys()) if \"pytorch_pretrained_bert\" in s]\n\ndel GPT2Tokenizer, GPT2ClassificationHeadModel\nfor m in lib_modules_list :\n    del sys.modules[m]\n\nsys.path.remove(PYTORCH_BERT_DIR)\nsys.path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT"},{"metadata":{},"cell_type":"markdown","source":"* load library"},{"metadata":{"trusted":true},"cell_type":"code","source":"PYTORCH_BERT_DIR = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT/\"\nsys.path.insert(0, PYTORCH_BERT_DIR)\n\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* preprocess pipe"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_pipe = [preprocess_sogna, preprocess_pb]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* constants"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda')\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nMAX_SEQUENCE_LENGTH = 300\nBATCH_SIZE = 256","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* inference class"},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_bert = InferenceBert(\n    preprocess_funcs=preprocess_pipe, \n    debug=debug\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* make data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = inference_bert.make_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_bert.make_model('../input/sub-toxic-bert-weight/model_1_1_300len_2epoch_64batch_2accum_2e-05lr_all_full_0.01warm_1.3weight.pt')\ntest_preds_bert_94560 = inference_bert.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_bert.make_model('../input/sub-toxic-bert-weight/model_1_1_pretrained_300len_2epoch_64batch_2accum_2e-05lr_all_full_0.01warm_94501.pt')\ntest_preds_bert_94501 = inference_bert.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = inference_bert.make_model('../input/sub-toxic-bert-weight/model_1_1_300len_2epoch_32batch_1accum_2e-05lr_full.pt')\ntest_preds_bert_94427 = inference_bert.inference(test_text, model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* delete library"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"lib_modules_list = [s for s in list(sys.modules.keys()) if \"pytorch_pretrained_bert\" in s]\n\ndel BertTokenizer, BertForSequenceClassification\nfor m in lib_modules_list :\n    del sys.modules[m]\n\nsys.path.remove(PYTORCH_BERT_DIR)\nsys.path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT - KERAS"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pprint\nimport time\nimport json\nimport os\nimport sys\nimport collections\nimport csv\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom tqdm import tqdm_notebook\n\nos.environ['TF_KERAS'] = '1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('pip install --no-index --find-links=\"../input/kerasbert/keras-bert-lib/\" keras-bert')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n\n# import python modules defined by BERT\nimport run_classifier\nimport modeling\nimport optimization\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/'\nBERT_PRETRAINED_PATH = '../input/bert-seq-360/'\nBERT_MODEL_PATH = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_list = os.listdir(BERT_PRETRAINED_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_NAME = 'keras-bert-toxic-vocab-360-02.h5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# parameter\nMAX_SEQUENCE_LENGTH = 360\nLR = 2e-5\nloss_weight = 3.2092275837114372","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_FILE = os.path.join(BERT_MODEL_PATH, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_MODEL_PATH, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_MODEL_PATH, 'bert_model.ckpt')\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\nsymbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer_preprocess = TreebankWordTokenizer()\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer_preprocess.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\n# Converting the lines to BERT format\n# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_test['comment_text'] = df_test['comment_text'].astype(str) \ndf_test['comment_text'] = df_test['comment_text'].apply(lambda x:preprocess(x))\nX_test = convert_lines(df_test['comment_text'].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n\nX_seg_input = np.zeros((X_test.shape[0], MAX_SEQUENCE_LENGTH))\nX_mask_input = np.ones((X_test.shape[0], MAX_SEQUENCE_LENGTH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import load_trained_model_from_checkpoint\n\nbase_model = load_trained_model_from_checkpoint(CONFIG_FILE, checkpoint_file, training=True, seq_len=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras import backend as K\n\ndef custom_target_loss(y_true, y_pred):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    target_loss = binary_crossentropy(K.reshape(y_true[:, 0], shape=(-1, 1)), y_pred) * y_true[:, 1]\n    return (target_loss * loss_weight)\n\ndef custom_aux_loss(y_true, y_pred):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    aux_loss = binary_crossentropy(y_true, y_pred)\n\n    return aux_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python import keras\nfrom keras_bert import calc_train_steps\n\nextract = base_model.get_layer('Extract').output\n\nfinal_layer = keras.layers.Dense(256, activation='relu')(extract)\nfinal_layer = keras.layers.Dropout(0.25)(final_layer)\n\ntarget_layer = keras.layers.Dense(1, activation='sigmoid', name='target_layer')(final_layer)\naux_layer = keras.layers.Dense(6, activation='sigmoid', name='aux_layer')(final_layer)\n\nmodel = keras.models.Model(inputs=base_model.input, outputs=[target_layer, aux_layer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(os.path.join(BERT_PRETRAINED_PATH, MODEL_NAME))\nmodel.compile(loss=[custom_target_loss, custom_aux_loss], optimizer=tf.train.AdamOptimizer(learning_rate=LR))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_939 = model.predict([X_test, X_seg_input, X_mask_input], batch_size=128, verbose=1, use_multiprocessing=True)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_gpt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_94560","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_94501","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_94427","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert_939","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = pd.DataFrame(index=range(len(test_text)))\nmeta['lstm_939'] = test_preds_lstm\nmeta['gpt2_941'] = test_preds_gpt2\nmeta['bert_94560'] = test_preds_bert_94560\nmeta['bert_94501'] = test_preds_bert_94501\nmeta['bert_94427'] = test_preds_bert_94427\nmeta['bert_939'] = test_preds_bert_939\n\nmeta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\nfinal_preds = (meta.bert_94560 ** 2) * 0.4 + (meta.bert_94501 ** 2) * 0.17 + (meta.bert_94427 ** 2) * 0.12 + (meta.gpt2_941 ** 2) * 0.11 + (meta.lstm_939 ** 2) * 0.1 + (meta.bert_939 ** 2) * 0.1\nsubmission.prediction = final_preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}