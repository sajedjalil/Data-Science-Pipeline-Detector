{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport re\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = '../input/bertpretrainedmodels/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = BertConfig('../input/bertjson/bert_config_new.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification(bert_config, num_labels=16)\nmodel.load_state_dict(torch.load(\"../input/bert2epaux01wwd1e4decay95weachrebasev1/bert_pytorch.bin\"))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_bert = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRAWL_EMBEDDING_PATH = '../input/fasttextcrawl300d2m/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove840b300dtxt/glove.840B.300d.txt'\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1/(1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(learn,test,output_dim,lr=0.00083,\n                batch_size=512, n_epochs=4,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n        \ndef preprocess(data):\n    '''\n    Credit goes to \n    https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_train = preprocess(train['comment_text'])\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = preprocess(test['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = np.ones((len(x_train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\nmax_features = None\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = max_features or len(tokenizer.word_index) + 1\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_torch = torch.tensor(x_train, dtype=torch.long)\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n\nx_test_torch = torch.tensor(x_test, dtype=torch.long)\n\nbatch_size = 512\n\ntrain_dataset = data.TensorDataset(x_train_torch, y_train_torch)\nvalid_dataset = data.TensorDataset(x_train_torch[:batch_size], y_train_torch[:batch_size])\ntest_dataset = data.TensorDataset(x_test_torch)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n\ndatabunch = DataBunch(train_dl=train_loader,valid_dl=valid_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n    \nall_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1234 + model_idx)\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    learn = Learner(databunch,model,loss_func=custom_loss)\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \n    all_test_preds.append(test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': 0.88*test_pred + 0.12*(test_prednp.mean(all_test_preds, axis=0)[:, 0])})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': 0.78*test_pred + 0.22*(test_prednp.mean(all_test_preds, axis=0)[:, 0])})\nsubmission.to_csv('submission_v.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': 0.68*test_pred + 0.32*(test_prednp.mean(all_test_preds, axis=0)[:, 0])})\nsubmission.to_csv('submission_v1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}