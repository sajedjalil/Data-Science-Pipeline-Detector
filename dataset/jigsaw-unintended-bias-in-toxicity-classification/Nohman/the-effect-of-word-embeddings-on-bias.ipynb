{"cells":[{"metadata":{},"cell_type":"markdown","source":"The Quora Insincere Questions Classification spawned several great kernels for more background on word embeddings: \n\n- [How to: Preprocessing when using embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)\n- [A look at different embeddings.!](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n\nIn this kernel I'm going to explore the influence of different word embeddings on unintended bias. First, I'm going to benchmark popular word embeddings with the [Simple LSTM](https://www.kaggle.com/thousandvoices/simple-lstm) courtesy of thousandvoices. If you want to skip ahead to the results, click the link for [word embeddings comparison](#compare). After that I'll introduce operations we can perform on word embeddings and I'll construct a bunch of combinations to benchmark. I'll cover concatenating embeddings and constructing meta embeddings from several different vector spaces. Skip to [complete embeddings comparison](#complete) for the final results. If this helps your model or if you have any ideas for other combinations leave a comment and upvote!\n\nI have since added a proper section on [BERT embeddings](#bert) since my initial test scored much lower than expected. \n\n<br>\n![](https://qph.fs.quoracdn.net/main-qimg-3e812fd164a08f5e4f195000fecf988f)\n<br>\n# Contents\n---\n- [Without Pretrained Embeddings](#no-pretrain)\n- [Fasttext Embeddings](#fasttext)\n- [GloVe Embeddings](#glove)\n- [Concept Numberbatch Embeddings](#numberbatch)\n- [BERT Embeddings](#bert)\n- [Word Embeddings Comparison](#compare)\n- [Concatenating Fasttext+GloVe Embeddings](#glove+fasttext)\n- [Weighted Predictions by Bias Score](#weighted)\n- [Constructing Meta-Embeddings](#meta)\n- [GloVe+Fasttext Meta-Embeddings](#meta-glove+fasttext)\n- [Weighted Meta-Embeddings](#meta-weighted)\n- [Complete Embeddings Comparison](#complete)"},{"metadata":{"scrolled":true,"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.model_selection import train_test_split\n\nimport plotly\nimport colorlover as cl\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nplotly.tools.set_credentials_file(username='nholloway', api_key='Ef8vuHMUdvaIpvtC2lux')\npy.init_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FASTTEXT_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\nNUMBERBATCH_PATH = '../input/conceptnet-numberbatch-vectors/numberbatch-en-17.06.txt/numberbatch-en-17.06.txt'\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4\nMAX_LEN = 220","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we define our procedure for loading the embeddings and vocabulary into an embedding matrix."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"We will preprocess our text by mapping punctuation and contractions to strings to make it easier to find embeddings. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def preprocess(text):\n    s_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    p_mapping = {\"_\":\" \", \"`\":\" \"}    \n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    \n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([s_mapping[t] if t in s_mapping else t for t in text.split(\" \")])\n    for p in p_mapping:\n        text = text.replace(p, p_mapping[p])    \n    for p in punct:\n        text = text.replace(p, f' {p} ')     \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we split our train and test set make sure that the test set retains the identity labels to benchmark subgroup bias. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_columns = ['target', 'black', 'white', 'male', 'female', 'homosexual_gay_or_lesbian',\n                'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness', \n                'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat'] \n\nx_train, x_val, y_train, y_val = train_test_split(train['comment_text'], train[test_columns], test_size=.2, random_state=42)\n\nx_train = x_train.apply(lambda x: preprocess(x.lower()))\ny_train['target'] = np.where(y_train['target'] >= 0.5, 1, 0)\nx_val = x_val.apply(lambda x: preprocess(x.lower()))\ny_val['target'] = np.where(y_val['target'] >= 0.5, 1, 0)\ny_aux_train = y_train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].copy()\ny_train = y_train['target']\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_val))\n \nx_train = tokenizer.texts_to_sequences(x_train)\nx_val = tokenizer.texts_to_sequences(x_val)\n\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_val = sequence.pad_sequences(x_val, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def train_model(embedding_matrix, model_name):\n    checkpoint_predictions = []\n    weights = []\n    \n    for model_idx in range(NUM_MODELS):\n        model = build_model(embedding_matrix, y_aux_train.shape[-1])\n        for global_epoch in range(EPOCHS):\n            model.fit(\n                x_train,\n                [y_train, y_aux_train],\n                batch_size=BATCH_SIZE,\n                epochs=1,\n                verbose=2,\n                callbacks=[\n                    LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n                ])\n            checkpoint_predictions.append(model.predict(x_val, batch_size=2048)[0].flatten())\n            weights.append(2 ** global_epoch)\n            model.save(f'data/toxicity/{model_name}_model.h5')\n\n    predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bias Benchmarks\n---\n- Subgroup AUC: The AUC score for the entire subgroup- a low score hear means the model **fails to distinguish between toxic and non-toxic comments** that mention this identity. \n- BPSN AUC: Background positive, subgroup negative. A low value here means the model confuses **non-toxic examples that mention the identity with toxic examples that do not**.\n- BNSP AUC: Background negative, subgroup positive. A low value here means that the model confuses **toxic examples that mention the identity with non-toxic examples that do not**. \n\nThe final score used in this competition is a combination of these bias metrics, which we will also compute. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"IDENTITY_COLUMNS = ['black', 'white', 'male', 'female', 'homosexual_gay_or_lesbian',\n                   'christian', 'jewish', 'muslim', 'psychiatric_or_mental_illness'] \n    \ndef compute_bpsn_auc(df, subgroup, model, label):\n    subgroup_positive_examples = df.loc[(df[subgroup] == 1) & (df[label] == 1)]\n    non_subgroup_negative_examples = df.loc[df[subgroup] != 1 & (df[label] == 0)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return auc(examples[label], examples[model])  \n    \ndef compute_bnsp_auc(df, subgroup, model, label):\n    subgroup_negative_examples = df.loc[(df[subgroup] == 1) & (df[label] == 0)]\n    non_subgroup_positive_examples = df.loc[(df[subgroup] != 1) & (df[label] == 1)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return auc(examples[label], examples[model])\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total/len(series), 1/p)\n\ndef compute_final_bias(bias_df, overall_auc, power=-5, overall_model_weight=0.25):\n    bias_score = np.average([\n        power_mean(bias_df['subgroup_auc'], power),\n        power_mean(bias_df['bpsn_auc'], power),\n        power_mean(bias_df['bnsp_auc'], power)\n    ])\n    return (overall_model_weight * overall_auc) + ((1 - overall_model_weight)* bias_score)\n    \ndef compute_subgroup_bias_metrics(df, subgroups, model, label):\n    records = []\n    for subgroup in subgroups:\n        subgroup_df = df.loc[df[subgroup] == 1]\n        record = {\n            'subgroup': subgroup, \n            'subgroup_size': len(subgroup_df)\n        }\n        record['subgroup_auc'] = auc(subgroup_df['target'], subgroup_df[model])\n        record['bpsn_auc'] = compute_bpsn_auc(df, subgroup, model, label)\n        record['bnsp_auc'] = compute_bnsp_auc(df, subgroup, model, label)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', \n                                                 ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first model to train is a control- where we construct our own embeddings rather than use pretrained ones."},{"metadata":{},"cell_type":"markdown","source":" <a id='no-pretrain'></a>\n## Without Pretrained Embeddings \n---"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"def build_control_model(embedding_size, max_features, num_aux_targets):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(max_features, embedding_size)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model\n\ndef train_control_model(embedding_size, max_features, model_name):\n    checkpoint_predictions = []\n    weights = []\n    \n    for model_idx in range(NUM_MODELS):\n        model = build_control_model(embedding_size, max_features, y_aux_train.shape[-1])\n        for global_epoch in range(EPOCHS):\n            model.fit(\n                x_train,\n                [y_train, y_aux_train],\n                batch_size=BATCH_SIZE,\n                epochs=1,\n                verbose=2,\n                callbacks=[\n                    LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n                ])\n            checkpoint_predictions.append(model.predict(x_val, batch_size=2048)[0].flatten())\n            weights.append(2 ** global_epoch)\n            model.save(f'data/toxicity/{model_name}_model.h5')\n\n    predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = len(tokenizer.word_index.keys())\nemb_size = 300\n# This model took 3hr 47min to train compared to \n# ~3hr 3min for most the pretrained embedding models\n# %time control_preds = train_control_model(emb_size, max_features, 'no pretrained embeddings')\nresults_df = pd.read_pickle('../input/embedding-bias-benchmark/final_results.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(control_preds, columns=['w/o pretrained']).reset_index(drop=True), results_df], axis=1)\nctrl_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'w/o pretrained', 'target')\noverall_auc = auc(results_df['target'], results_df['w/o pretrained'])\nctrl_final_bias = compute_final_bias(ctrl_bias_metrics, overall_auc)\ndisplay(ctrl_bias_metrics)\nprint(f'Final Metric: {ctrl_final_bias}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='fasttext'></a>\n## FastText \n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"fasttext_matrix = build_matrix(tokenizer.word_index, FASTTEXT_PATH)\n# %time fast_preds = train_model(fasttext_matrix, 'fasttext')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(fast_preds, columns=['fasttext']).reset_index(drop=True), y_val.reset_index(drop=True)], axis=1).fillna(0)\nft_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'fasttext', 'target')\noverall_auc = auc(results_df['target'], results_df['fasttext'])\nft_final_bias = compute_final_bias(ft_bias_metrics, overall_auc)\ndisplay(ft_bias_metrics)\nprint(f'Final Metric: {ft_final_bias}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='glove'></a>\n## GloVE \n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"glove_matrix = build_matrix(tokenizer.word_index, GLOVE_PATH)\n# %time glove_preds = train_model(glove_matrix, 'glove')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(glove_preds, columns=['glove']).reset_index(drop=True), results_df], axis=1)\ngl_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'glove', 'target')\noverall_auc = auc(results_df['target'], results_df['glove'])\ngl_final_bias = compute_final_bias(gl_bias_metrics, overall_auc)\ndisplay(gl_bias_metrics)\nprint(f'Final Metric: {gl_final_bias}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Other popular models seem to use GloVe and Fasttext embeddings- but I have yet to see the use of [ConceptNet Numberbatch](https://github.com/commonsense/conceptnet-numberbatch) embeddings- which according to the README were specifically created for dealing with bias in text.  "},{"metadata":{},"cell_type":"markdown","source":"<a id='numberbatch'></a>\n## Conceptnet Numberbatch\n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"numb_matrix = build_matrix(tokenizer.word_index, NUMBERBATCH_PATH)\n# %time numb_preds = train_model(numb_matrix, 'numberbatch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(numb_preds, columns=['numberbatch']).reset_index(drop=True), results_df], axis=1)\nnb_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'numberbatch', 'target')\noverall_auc = auc(results_df['target'], results_df['numberbatch'])\nnb_final_bias = compute_final_bias(nb_bias_metrics, overall_auc)\ndisplay(nb_bias_metrics)\nprint(f'Final Metric: {nb_final_bias}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='bert'></a>\n## BERT Embeddings\n---"},{"metadata":{},"cell_type":"markdown","source":"In this section I'll show how to create custom BERT embeddings from a pretrained BERT model. Unfortunately, the BERT embeddings have 768 dimensions when trained on the small pre-trained model (and 1024 for the larger one) and I wasn't able train a model to benchmark because there isn't a lot of text pre-processing in this kernel and the vocabulary is really large. If you run into memory issues with the larger BERT embeddings consider decreasing the vocabulary."},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"! pip install bert_embedding\nfrom bert_embedding import BertEmbedding\n\ndef get_bert_embed_matrix():\n    %%time\n    # Total CPU time (my machine): 1d 4h 7min\n    vocab = list(tokenizer.word_index.keys())\n    embedding_results = bert_embedding(vocab)\n    bert_embeddings = {}\n    for emb in embedding_results:\n        try: \n            bert_embeddings[emb[0][0]] = emb[1][0]\n        except:\n            pass\n    with open('../input/bert.768.pkl', 'wb') as f:\n        pickle.dump(bert_embeddings, f)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method will extract word embeddings and save them as a dict in a pickle file, so to load them into a model you'll have to use a slightly different `build_matrix` and `load_embedding` methods like those seen in [How To: Preprocessing for GloVe Part2: Usage](https://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage). Saving and loading as a pickle will also be considerably faster. If I find a way to do a commensurate benchmark with my BERT embeddings I will, but for now I'll just leave the method to create your own.\n<a id='compare'></a>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def scale(x):\n    mini = 272\n    maxi = 6047\n    scale_rng = [10, 55]\n    return (scale_rng[1] - scale_rng[0])*((x-mini)/(maxi-mini))+scale_rng[0]\n    \ntrace0 = go.Scatter(\n{\n        'x': ctrl_bias_metrics['subgroup_auc'], \n        'y': ctrl_bias_metrics['subgroup_size'],\n        'legendgroup': 'w/o pretrained',\n        'name': 'w/o pretrained', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][0],\n            'size': [scale(x) for x in ctrl_bias_metrics['subgroup_size']]\n        },\n        'text': ctrl_bias_metrics['subgroup']\n    })\n\ntrace1 = go.Scatter(\n{\n        'x': ft_bias_metrics['subgroup_auc'], \n        'y': ft_bias_metrics['subgroup_size'],\n        'legendgroup': 'fasttext',\n        'name': 'fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][2],\n            'size': [scale(x) for x in ft_bias_metrics['subgroup_size']]\n        },\n        'text': ft_bias_metrics['subgroup']\n    })\n\ntrace2 = go.Scatter(\n{\n        'x': gl_bias_metrics['subgroup_auc'], \n        'y': gl_bias_metrics['subgroup_size'],\n        'legendgroup': 'glove',\n        'name': 'glove', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][4],\n            'size': [scale(x) for x in gl_bias_metrics['subgroup_size']]\n        },\n        'text': gl_bias_metrics['subgroup']\n    })\n\ntrace3 = go.Scatter(\n{\n        'x': nb_bias_metrics['subgroup_auc'], \n        'y': nb_bias_metrics['subgroup_size'],\n        'legendgroup': 'numberbatch',\n        'name': 'numberbatch', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][6],\n            'size': [scale(x) for x in nb_bias_metrics['subgroup_size']]\n        },\n        'text': nb_bias_metrics['subgroup']\n    })\n\nlayout = go.Layout(\n    title= 'Subgroup Size vs Subgroup AUC',\n    hovermode = 'closest',\n    xaxis = dict(\n        title='Subgroup AUC'\n    ),\n    yaxis = dict(\n        title='Subgroup Size'\n    ),\n    showlegend = True\n)\n\nfig = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def scale(x):\n    mini = 272\n    maxi = 6047\n    scale_rng = [10, 55]\n    return (scale_rng[1] - scale_rng[0])*((x-mini)/(maxi-mini))+scale_rng[0]\n    \ntrace0 = go.Scatter(\n{\n        'x': ctrl_bias_metrics['bnsp_auc'], \n        'y': ctrl_bias_metrics['bpsn_auc'],\n        'legendgroup': 'w/o pretrained',\n        'name': 'w/o pretrained', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][0],\n            'size': [scale(x) for x in ctrl_bias_metrics['subgroup_size']]\n        },\n        'text': ctrl_bias_metrics['subgroup']\n    })\n\ntrace1 = go.Scatter(\n{\n        'x': ft_bias_metrics['bnsp_auc'], \n        'y': ft_bias_metrics['bpsn_auc'],\n        'legendgroup': 'fasttext',\n        'name': 'fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][2],\n            'size': [scale(x) for x in ft_bias_metrics['subgroup_size']]\n        },\n        'text': ft_bias_metrics['subgroup']\n    })\n\ntrace2 = go.Scatter(\n{\n        'x': gl_bias_metrics['bnsp_auc'], \n        'y': gl_bias_metrics['bpsn_auc'],\n        'legendgroup': 'glove',\n        'name': 'glove', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][4],\n            'size': [scale(x) for x in gl_bias_metrics['subgroup_size']]\n        },\n        'text': gl_bias_metrics['subgroup']\n    })\n\ntrace3 = go.Scatter(\n{\n        'x': nb_bias_metrics['bnsp_auc'], \n        'y': nb_bias_metrics['bpsn_auc'],\n        'legendgroup': 'numberbatch',\n        'name': 'numberbatch', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][6],\n            'size': [scale(x) for x in nb_bias_metrics['subgroup_size']]\n        },\n        'text': nb_bias_metrics['subgroup']\n    })\n\n\nlayout = go.Layout(\n    title= 'Word Embeddings Comparison',\n    hovermode = 'closest',\n    xaxis = dict(\n        title='BNSP-AUC'\n    ),\n    yaxis = dict(\n        title='BPSN-AUC'\n    ),\n    showlegend = True\n)\n\nfig = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How to Combine Embeddings \n---\nThere are several ways to combine embeddings: \n1. Train seperate models on each embedding and blend the results. [SDK](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings) has a kernel on this approach. One benefit in this competition would be weighting the results of each model according to their normalized final biased score, which we will show. \n2. Concatenate embeddings. In the [simple LSTM](https://www.kaggle.com/thousandvoices/simple-lstm) that our model is based on the author concatenates the Fasttext and GloVe embeddings. One of the solutions from the previous [toxic comment classifier](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/52630) includes concatenating multiple embeddings and additional subword information.\n3. Construct meta-embeddings. [Shujian Liu](https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding) has a kernel on how to compute new embeddings from our mixture of embeddings. There are many proposed ways to compute meta-embeddings, but for our next model we will simply average our embeddings- which will give us a 300 dimensional embedding that is more robust than our original. "},{"metadata":{},"cell_type":"markdown","source":"<a id='glove+fasttext'></a>\n## Concatenate GloVe and Fasttext\n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"vec_files = [GLOVE_PATH, FASTTEXT_PATH]\nglft_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in vec_files], axis=-1)\n\n# The 600 dimension embedding takes 3h 36min to run compared to \n# ~3h 3min for the other 300 dimension embedding models\n# %time glft_preds = train_model(glft_matrix, 'glove+fasttext')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(glft_preds, columns=['glove+fast']).reset_index(drop=True), results_df], axis=1)\nglft_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'glove+fast', 'target')\noverall_auc = auc(results_df['target'], results_df['glove+fast'])\nglft_final_bias = compute_final_bias(glft_bias_metrics, overall_auc)\ndisplay(glft_bias_metrics)\nprint(f'Final Metric: {glft_final_bias}')\ndel(glft_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='weighted'></a>\n## Weighing Results by Bias Scores\n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"bias_sum = nb_final_bias + gl_final_bias + ft_final_bias\nnb_weight = nb_final_bias/bias_sum\ngl_weight = gl_final_bias/bias_sum\nft_weight = ft_final_bias/bias_sum\n\n# results_df['weighted'] = results_df.apply(lambda x: (ft_weight*x['fasttext']) + (gl_weight*x['glove']) + (nb_weight*x['numberbatch']), axis=1)\nw_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'weighted', 'target')\noverall_auc = auc(results_df['target'], results_df['weighted'])\nw_final_bias = compute_final_bias(w_bias_metrics, overall_auc)\ndisplay(w_bias_metrics)\nprint(f'Final Metric: {w_final_bias}')\nprint(f'Glove Weight: {gl_weight}')\nprint(f'Fasttext Weight: {ft_weight}')\nprint(f'Numberbatch Weight: {nb_weight}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite the low score for the Conceptnet Numberbatch embeddings, the weighted average still produces a final bias better than each of the constituent embedding predictions. Later we will train a model with an embedding matrix that is the weighted average of the other embedding matrices, using the same weights we used to combine predictions here."},{"metadata":{},"cell_type":"markdown","source":"<a id='meta'></a>\n## Constructing Meta Embeddings\n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"meta_matrix = np.divide(fasttext_matrix + glove_matrix + numb_matrix, 3)\n# %time meta_preds = train_model(meta_matrix, 'meta')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(meta_preds, columns=['meta']).reset_index(drop=True), results_df], axis=1)\nm_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'meta', 'target')\noverall_auc = auc(results_df['target'], results_df['meta'])\nm_final_bias = compute_final_bias(m_bias_metrics, overall_auc)\ndisplay(m_bias_metrics)\nprint(f'Final Metric: {m_final_bias}')\ndel (meta_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='meta-glove+fasttext'></a>\n## GloVe and Fasttext Meta Embedding\n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"meta2_matrix = np.divide(fasttext_matrix + glove_matrix, 2)\n# %time meta2_preds = train_model(meta2_matrix, 'meta-glove+fasttext')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(meta2_preds, columns=['meta-glove+fasttext']).reset_index(drop=True), results_df], axis=1)\nm2_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'meta-glove+fasttext', 'target')\noverall_auc = auc(results_df['target'], results_df['meta-glove+fasttext'])\nm2_final_bias = compute_final_bias(m2_bias_metrics, overall_auc)\ndisplay(m2_bias_metrics)\nprint(f'Final Metric: {m2_final_bias}')\ndel (meta2_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='meta-weighted'></a>\n## Weighted Meta Embedding\n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"nb_weighted = np.divide(numb_matrix, nb_weight)\ngl_weighted = np.divide(glove_matrix, gl_weight)\nft_weighted = np.divide(fasttext_matrix, ft_weight)\nmeta_weighted_matrix = nb_weighted+gl_weighted+ft_weighted\n# %time meta_weighted_preds = train_model(meta_weighted_matrix, 'meta-weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# results_df = pd.concat([pd.DataFrame(meta_weighted_preds, columns=['meta-weighted']).reset_index(drop=True), results_df], axis=1)\nmw_bias_metrics = compute_subgroup_bias_metrics(results_df, IDENTITY_COLUMNS, 'meta-weighted', 'target')\noverall_auc = auc(results_df['target'], results_df['meta-weighted'])\nmw_final_bias = compute_final_bias(mw_bias_metrics, overall_auc)\ndisplay(mw_bias_metrics)\nprint(f'Final Metric: {mw_final_bias}')\ndel (meta_weighted_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='complete'></a>\n# Complete Embeddings Comparison \n---"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"final_bias = {'w/o pretrained': ctrl_final_bias, 'fasttext': ft_final_bias, 'glove': gl_final_bias, 'numberbatch': nb_final_bias, 'weighted': w_final_bias,'glove+fasttext': glft_final_bias,'meta': m_final_bias, 'meta-glove+fasttext': m2_final_bias, 'meta-weighted': mw_final_bias}\nfinal_bias = pd.DataFrame(data = final_bias, index=['final bias score'])\ndisplay(final_bias)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trace5 = go.Scatter(\n{\n        'x': glft_bias_metrics['bnsp_auc'], \n        'y': glft_bias_metrics['bpsn_auc'],\n        'legendgroup': 'glove+fasttext',\n        'name': 'glove+fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][1],\n            'size': [scale(x) for x in glft_bias_metrics['subgroup_size']]\n        },\n        'text': glft_bias_metrics['subgroup']\n    })\n\ntrace6 = go.Scatter(\n{\n        'x': m_bias_metrics['bnsp_auc'], \n        'y': m_bias_metrics['bpsn_auc'],\n        'legendgroup': 'meta-embedding',\n        'name': 'meta-embedding', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][3],\n            'size': [scale(x) for x in m_bias_metrics['subgroup_size']]\n        },\n        'text': m_bias_metrics['subgroup']\n    })\n\ntrace7 = go.Scatter(\n{\n        'x': m2_bias_metrics['bnsp_auc'], \n        'y': m2_bias_metrics['bpsn_auc'],\n        'legendgroup': 'meta-glove+fasttext',\n        'name': 'meta-glove+fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][5],\n            'size': [scale(x) for x in m2_bias_metrics['subgroup_size']]\n        },\n        'text': m2_bias_metrics['subgroup']\n    })\n\ntrace8 = go.Scatter(\n{\n        'x': mw_bias_metrics['bnsp_auc'], \n        'y': mw_bias_metrics['bpsn_auc'],\n        'legendgroup': 'meta-weighted',\n        'name': 'meta-weighted', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][7],\n            'size': [scale(x) for x in mw_bias_metrics['subgroup_size']]\n        },\n        'text': mw_bias_metrics['subgroup']\n    })\n\nlayout = go.Layout(\n    title= 'Concatenated and Meta Word Embeddings Comparison',\n    hovermode = 'closest',\n    xaxis = dict(\n        title='BNSP-AUC'\n    ),\n    yaxis = dict(\n        title='BPSN-AUC'\n    ),\n    showlegend = True\n)\n\nfig = go.Figure(data=[trace5, trace6, trace7, trace8], layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"layout = go.Layout(\n    title= 'Complete Embeddings Comparison',\n    hovermode = 'closest',\n    xaxis = dict(\n        title='BNSP-AUC'\n    ),\n    yaxis = dict(\n        title='BPSN-AUC'\n    ),\n    showlegend = True\n)\n\nfig = go.Figure(data=[trace0, trace1, trace2, trace3, trace5, trace6, trace7, trace8], layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trace0 = go.Scatter(\n{\n        'x': final_bias['w/o pretrained'], \n        'y': [0],\n        'name': 'w/o pretrained', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][0],\n            'size': 25\n        }\n    })\ntrace1 = go.Scatter(\n{\n        'x': final_bias['fasttext'], \n        'y': [0],\n        'name': 'fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][2],\n            'size': 25\n        }\n    })\ntrace2 = go.Scatter(\n{\n        'x': final_bias['glove'], \n        'y': [0],\n        'name': 'glove', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][4],\n            'size': 25\n        }\n    })\ntrace3 = go.Scatter(\n{\n        'x': final_bias['numberbatch'], \n        'y': [0],\n        'name': 'numberbatch', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][6],\n            'size': 25\n        }\n    })\ntrace5 = go.Scatter(\n{\n        'x': final_bias['glove+fasttext'], \n        'y': [0],\n        'name': 'glove+fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][1],\n            'size': 25\n        }\n    })\ntrace6 = go.Scatter(\n{\n        'x': final_bias['meta'], \n        'y': [0],\n        'name': 'meta', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][3],\n            'size': 25\n        }\n    })\ntrace7 = go.Scatter(\n{\n        'x': final_bias['meta-glove+fasttext'], \n        'y': [0],\n        'name': 'meta-glove+fasttext', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][5],\n            'size': 25\n        }\n    })\ntrace8 = go.Scatter(\n{\n        'x': final_bias['meta-weighted'], \n        'y': [0],\n        'name': 'meta-weighted', \n        'mode': 'markers', \n        'marker': {\n            'color': cl.scales['9']['div']['Spectral'][7],\n            'size': 25\n        }\n    })\n\nlayout = go.Layout(\n    title= 'Final Bias Score for All Embeddings',\n    hovermode = 'closest',\n    xaxis = dict(\n        title='Final Bias Score'\n    ),\n    showlegend = True\n)\n\nfig = go.Figure(data=[trace0, trace1, trace2, trace3, trace5, trace6, trace7, trace8], layout=layout)\n\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hopefully, this kernel was helpful! There are a lot of ways you could extend this- particularly by combining embeddings with subword information that may acentuate toxic and non-toxic comment differences- if you fork it and add any interesting benchmarks make sure to add a link in the comments! \n\nOtherwise if you liked the kernel or if it helped you understand how we can improve our NLP models please upvote or leave a comment!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}