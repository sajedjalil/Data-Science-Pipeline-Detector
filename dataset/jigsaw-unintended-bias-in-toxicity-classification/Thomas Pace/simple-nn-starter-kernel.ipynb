{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Jigsaw Unintended Bias in Toxicity Classification\n\n1. [Introduction](#Introduction) <br>\n2. [Functions](#Functions) <br>\n3. [Data](#Data) <br>\n4. [Embedding](#Embedding) <br>\n5. [Model](#Model) <br>\n6. [Submission](#Submission) <br>\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Introduction\n\nThis is a simple Neural Network (NN) Kernel using Keras preprocessing features and TensorFlow for backend processing.  This is a simple baseline kernel that does not perform as well as the more sophisticated kernels but does include several concise loading functions.  This kernel is intended to be used as a starting point for more elaborate kernels.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \n\nimport numpy as np\n\nimport pandas as pd\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom keras.preprocessing import text, sequence\n\nfrom keras.preprocessing.text import Tokenizer\n\nraw_train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n\nraw_test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nvocab_size = 100000\n\nmax_length = 220\n\ntext_column = 'comment_text'\n\ntarget_column = 'target'\n\nchar_filter = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n\nembedding_loc = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"#### Functions\n\nThe prepare_data function takes the pd.DataFrame training and test data objects and returns tokenized and padded x data, y training data and the fitted keras.Tokenizer object.  The build_embedding function creates the embedding index, embedding matrix and embedding layer from a word2vec source and a word index dictionary.  The build_embedding function was motivated by Dieter's excellent <a href='https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold'> baseline lstm + attention 5-fold kernel </a>.    "},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_data(train_data, test_data, text_column, target_column, char_filter, vocab_size, max_length):\n    \n    raw_x_test = test_data[text_column].astype(str)\n    \n    raw_x_train = train_data[text_column].astype(str)\n    \n    y_train = train_data[target_column].values\n    \n    tokenizer = Tokenizer(num_words=vocab_size, filters=char_filter)\n\n    tokenizer.fit_on_texts(list(raw_x_train) + list(raw_x_test))\n    \n    x_test = tokenizer.texts_to_sequences(raw_x_test)\n    \n    x_train = tokenizer.texts_to_sequences(raw_x_train)\n    \n    x_test = sequence.pad_sequences(x_test, maxlen = max_length)\n    \n    x_train = sequence.pad_sequences(x_train, maxlen = max_length)\n    \n    return x_train, y_train, x_test, tokenizer\n\ndef vec_parser(word, *coeffs):\n    \n    return word, np.asarray(coeffs, dtype='float32')\n\ndef build_embedding(embedding_loc, word_index, max_length, dimensionality):\n    \n    embedding_index = dict(vec_parser(*line.strip().split(\" \")) for line in open(embedding_loc, encoding='utf-8'))\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, dimensionality))\n    \n    for word, i in word_index.items():\n        \n        try:\n            \n            embedding_matrix[i] = embedding_index[word]\n        \n        except:\n            \n            embedding_matrix[i] = embedding_index[\"unknown\"]\n            \n    embedding_layer = keras.layers.Embedding(len(word_index)+1, \n                                         dimensionality, \n                                         weights=[embedding_matrix], \n                                         input_length=max_length, \n                                         trainable=False)\n    \n    return embedding_index, embedding_matrix, embedding_layer\n\ndef joint_shuffle(x_data, y_data):\n    \n    if len(x_data) == len(y_data):\n    \n        p = np.random.permutation(len(x_data))\n    \n    return x_data[p], y_data[p]","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data\n\nThe data is loaded using the prepare_data function and the embedding is created using the build_embedding function.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y_train, x_test, tokenizer = prepare_data(raw_train, \n                                        raw_test, \n                                        text_column, \n                                        target_column, \n                                        char_filter, \n                                        vocab_size, \n                                        max_length)\n\nembedding_index, embedding_matrix, embedding_layer = build_embedding(embedding_loc, \n                                                                     tokenizer.word_index, \n                                                                     max_length, \n                                                                     300)\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model\n\nThe model used is a simple feedforward neural network that is very is essentially the same as the one proposed in the <a href= 'https://www.tensorflow.org/tutorials/keras/basic_text_classification'> Tensor Flow tutorial </a>.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential()\n\nmodel.add(embedding_layer)\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\nmodel.summary()\n\nx_train, y_train = joint_shuffle(x_train, y_train)\n\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nsimple_callback = keras.callbacks.EarlyStopping(monitor='val_acc', \n                                                min_delta=0.005,\n                                                patience=5,\n                                                mode='max')\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=1,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1,\n                    callbacks=[simple_callback])","execution_count":4,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 220, 300)          98517000  \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 300)               0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                4816      \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 98,521,833\nTrainable params: 4,833\nNon-trainable params: 98,517,000\n_________________________________________________________________\nTrain on 1794874 samples, validate on 10000 samples\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n1794874/1794874 [==============================] - 224s 125us/sample - loss: 0.3172 - acc: 0.7003 - val_loss: 0.2802 - val_acc: 0.6998\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Submission\nThe predicted classification is placed into a pd.DataFrame object for submission and saved locally.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_results = model.predict(x_test)\n\nresults = np.average(raw_results, axis=1)\n\nsubmission = pd.DataFrame.from_dict({\n    'id': raw_test.id,\n    'prediction': results})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":5,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}