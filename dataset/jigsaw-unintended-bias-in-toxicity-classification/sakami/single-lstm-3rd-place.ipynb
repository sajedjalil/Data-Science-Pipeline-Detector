{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom contextlib import contextmanager\nimport copy\nfrom functools import partial\nfrom itertools import chain\nfrom multiprocessing import Pool\nimport os\nimport random\nimport re\nimport string\nimport time\nimport warnings\n\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.utils import shuffle\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, Sampler, DataLoader\nfrom torch.optim.optimizer import Optimizer","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"EMBEDDING_FASTTEXT = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nTRAIN_DATA = '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\nTEST_DATA = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'\nSAMPLE_SUBMISSION = '../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv'\n\nembed_size = 300\nmax_features = 100000\nmax_len = 220\n\nbatch_size = 512\ntrain_epochs = 6\nn_splits = 5\n\nmu = 0.9\nupdates_per_epoch = 10\n\nseed = 1029\ndevice = torch.device('cuda:0')\n\nps = PorterStemmer()\nlc = LancasterStemmer()\nsb = SnowballStemmer('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(msg):\n    t0 = time.time()\n    print(f'[{msg}] start.')\n    yield\n    elapsed_time = time.time() - t0\n    print(f'[{msg}] done in {elapsed_time / 60:.2f} min.')\n\n\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n\n\ndef _get_misspell(misspell_dict):\n    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n    return misspell_dict, misspell_re\n\n\ndef replace_typical_misspell(text):\n    misspellings, misspellings_re = _get_misspell(misspell_dict)\n\n    def replace(match):\n        return misspellings[match.group(0)]\n\n    return misspellings_re.sub(replace, text)\n    \n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts + list(string.punctuation):\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    return re.sub(r'\\d+', ' ', x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embedding(embedding_path, word_index):\n\n    def get_coefs(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    embeddings_index = dict(get_coefs(*o.strip().split(' ')) for o in open(embedding_path))\n    \n    # word_index = tokenizer.word_index\n    nb_words = min(max_features + 2, len(word_index))\n    embedding_matrix = np.zeros((nb_words, embed_size))\n\n    for key, i in word_index.items():\n        word = key\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_and_prec():\n    train = pd.read_csv(TRAIN_DATA, index_col='id')\n    test = pd.read_csv(TEST_DATA, index_col='id')\n    \n    # lower\n    train['comment_text'] = train['comment_text'].str.lower()\n    test['comment_text'] = test['comment_text'].str.lower()\n\n    # clean misspellings\n    train['comment_text'] = train['comment_text'].apply(replace_typical_misspell)\n    test['comment_text'] = test['comment_text'].apply(replace_typical_misspell)\n\n    # clean the text\n    train['comment_text'] = train['comment_text'].apply(clean_text)\n    test['comment_text'] = test['comment_text'].apply(clean_text)\n\n    # clean numbers\n    train['comment_text'] = train['comment_text'].apply(clean_numbers)\n    test['comment_text'] = test['comment_text'].apply(clean_numbers)\n    \n    # strip\n    train['comment_text'] = train['comment_text'].str.strip()\n    test['comment_text'] = test['comment_text'].str.strip()\n    \n    # replace blank with nan\n    train['comment_text'].replace('', np.nan, inplace=True)\n    test['comment_text'].replace('', np.nan, inplace=True)\n\n    # nan prediction\n    nan_pred = train['target'][train['comment_text'].isna()].mean()\n    \n    # fill up the missing values\n    train_x = train['comment_text'].fillna('_##_').values\n    test_x = test['comment_text'].fillna('_##_').values\n    \n    # get the target values\n    identity_columns = [\n        'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n        'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n    weights = np.ones((len(train),))\n    weights += train[identity_columns].fillna(0).values.sum(axis=1) * 3\n    weights += train['target'].values * 8\n    weights /= weights.max()\n    train_y = np.vstack([train['target'].values, weights]).T\n    \n    train_y_identity = train[identity_columns].values\n\n    # shuffling the data\n    np.random.seed(seed)\n    train_idx = np.random.permutation(len(train_x))\n\n    train_x = train_x[train_idx]\n    train_y = train_y[train_idx]\n    train_y_identity = train_y_identity[train_idx]\n\n    return train_x, train_y, train_y_identity, test_x, nan_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(texts, max_features):\n    counter = Counter()\n    for text in texts:\n        counter.update(text.split())\n\n    vocab = {\n        'token2id': {'<PAD>': 0, '<UNK>': max_features + 1},\n        'id2token': {}\n    }\n    vocab['token2id'].update(\n        {token: _id + 1 for _id, (token, count) in\n         enumerate(counter.most_common(max_features))})\n    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n    return vocab\n\n\ndef tokenize(texts, vocab):\n    \n    def text2ids(text, token2id):\n        return [\n            token2id.get(token, len(token2id) - 1)\n            for token in text.split()[:max_len]]\n    \n    return [\n        text2ids(text, vocab['token2id'])\n        for text in texts]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"reference: [5th place solution](https://www.kaggle.com/jiangm/5th-place-solution)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(Dataset):\n\n    def __init__(self, seqs, targets=None, maxlen=200):\n        if targets is not None:\n            self.targets = targets\n        else:\n            self.targets = np.random.randint(2, size=(len(seqs),))\n        \n        self.seqs = seqs\n        self.maxlen = maxlen\n        \n    def __len__(self):\n        return len(self.seqs)\n        \n    def get_keys(self):\n        lens = np.fromiter(\n            ((min(self.maxlen, len(seq))) for seq in self.seqs),\n            dtype=np.int32)\n        return lens\n        \n    def __getitem__(self, index):\n        return index, self.seqs[index], self.targets[index]\n\n\ndef collate_fn(data):\n\n    def _pad_sequences(seqs):\n        lens = [len(seq) for seq in seqs]\n        max_len = max(lens)\n\n        padded_seqs = torch.zeros(len(seqs), max_len).long()\n        for i, seq in enumerate(seqs):\n            start = max_len - lens[i]\n            padded_seqs[i, start:] = torch.LongTensor(seq)\n        return padded_seqs\n\n    index, seqs, targets = zip(*data)\n    seqs = _pad_sequences(seqs)\n    return index, seqs, torch.FloatTensor(targets)\n\n\nclass BucketSampler(Sampler):\n\n    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1048, shuffle_data=True):\n        super().__init__(data_source)\n        self.shuffle = shuffle_data\n        self.batch_size = batch_size\n        self.sort_keys = sort_keys\n        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n        self.weights = None\n\n        if not shuffle_data:\n            self.index = self.prepare_buckets()\n        else:\n            self.index = None\n\n    def set_weights(self, weights):\n        assert weights >= 0\n        total = np.sum(weights)\n        if total != 1:\n            weights = weights / total\n        self.weights = weights\n\n    def __iter__(self):\n        indices = None\n        if self.weights is not None:\n            total = len(self.sort_keys)\n            indices = np.random.choice(total, (total,), p=self.weights)\n        if self.shuffle:\n            self.index = self.prepare_buckets(indices)\n        return iter(self.index)\n\n    def get_reverse_indexes(self):\n        indexes = np.zeros((len(self.index),), dtype=np.int32)\n        for i, j in enumerate(self.index):\n            indexes[j] = i\n        return indexes\n\n    def __len__(self):\n        return len(self.sort_keys)\n        \n    def prepare_buckets(self, indices=None):\n        lens = - self.sort_keys\n        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lens)\n\n        if indices is None:\n            if self.shuffle:\n                indices = shuffle(np.arange(len(lens), dtype=np.int32))\n                lens = lens[indices]\n            else:\n                indices = np.arange(len(lens), dtype=np.int32)\n\n        #  bucket iterator\n        def divide_chunks(l, n):\n            if n == len(l):\n                yield np.arange(len(l), dtype=np.int32), l\n            else:\n                # looping till length l\n                for i in range(0, len(l), n):\n                    data = l[i:i + n]\n                    yield np.arange(i, i + len(data), dtype=np.int32), data\n    \n        new_indices = []\n        extra_batch = None\n        for chunk_index, chunk in divide_chunks(lens, self.bucket_size):\n            # sort indices in bucket by descending order of length\n            indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n            batches = []\n            for _, batch in divide_chunks(indices_sorted, self.batch_size):\n                if len(batch) == self.batch_size:\n                    batches.append(batch.tolist())\n                else:\n                    assert extra_batch is None\n                    assert batch is not None\n                    extra_batch = batch\n    \n            # shuffling batches within buckets\n            if self.shuffle:\n                batches = shuffle(batches)\n            for batch in batches:\n                new_indices.extend(batch)\n    \n        if extra_batch is not None:\n            new_indices.extend(extra_batch)\n        return indices[new_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n\n    def __init__(self, embedding_matrix):\n        super(NeuralNet, self).__init__()\n\n        lstm_hidden_size = 120\n        gru_hidden_size = 60\n        self.gru_hidden_size = gru_hidden_size\n\n        self.embedding = nn.Embedding(*embedding_matrix.shape)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = nn.Dropout2d(0.2)\n\n        self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n\n        self.linear = nn.Linear(gru_hidden_size * 6, 20)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(20, 1)\n        \n    def apply_spatial_dropout(self, h_embedding):\n        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n        return h_embedding\n\n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.apply_spatial_dropout(h_embedding)\n\n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, hh_gru = self.gru(h_lstm)\n\n        hh_gru = hh_gru.view(-1, self.gru_hidden_size * 2)\n\n        avg_pool = torch.mean(h_gru, 1)\n        max_pool, _ = torch.max(h_gru, 1)\n\n        conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"reference: [PME_EMA 6 x 8 pochs](https://www.kaggle.com/tks0123456789/pme-ema-6-x-8-pochs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EMA:\n\n    def __init__(self, model, mu, level='batch', n=1):\n        # self.ema_model = copy.deepcopy(model)\n        self.mu = mu\n        self.level = level\n        self.n = n\n        self.cnt = self.n\n        self.shadow = {}\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data\n\n    def _update(self, model):\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                new_average = (1 - self.mu) * param.data + self.mu * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    def set_weights(self, ema_model):\n        for name, param in ema_model.named_parameters():\n            if param.requires_grad:\n                param.data = self.shadow[name]\n\n    def on_batch_end(self, model):\n        if self.level is 'batch':\n            self.cnt -= 1\n            if self.cnt == 0:\n                self._update(model)\n                self.cnt = self.n\n                \n    def on_epoch_end(self, model):\n        if self.level is 'epoch':\n            self._update(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ParamScheduler:\n    \n    def __init__(self, optimizer, scale_fn, step_size):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        \n        self.optimizer = optimizer\n        self.scale_fn = scale_fn\n        self.step_size = step_size\n        self.last_batch_iteration = 0\n        \n    def batch_step(self):\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = self.scale_fn(self.last_batch_iteration / self.step_size)\n        \n        self.last_batch_iteration += 1\n\n\ndef combine_scale_functions(scale_fns, phases=None):\n    if phases is None:\n        phases = [1. / len(scale_fns)] * len(scale_fns)\n    phases = [phase / sum(phases) for phase in phases]\n    phases = torch.tensor([0] + phases)\n    phases = torch.cumsum(phases, 0)\n    \n    def _inner(x):\n        idx = (x >= phases).nonzero().max()\n        actual_x = (x - phases[idx]) / (phases[idx + 1] - phases[idx])\n        return scale_fns[idx](actual_x)\n        \n    return _inner\n\n\ndef scale_cos(start, end, x):\n    return start + (1 + np.cos(np.pi * (1 - x))) * (end - start) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class JigsawEvaluator:\n    \n    def __init__(self, y_binary, y_identity_binary, power=-5, overall_model_weight=0.25):\n        self.y = y_binary\n        self.y_i = y_identity_binary\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n        \n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n        \n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n        \n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n        \n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n        \n    def compute_bias_metrics_for_model(self, y_pred):\n        records = np.zeros((3, self.n_subgroups))\n        for i in range(self.n_subgroups):\n            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n        return records\n        \n    def _calculate_overall_auc(self, y_pred):\n        return roc_auc_score(self.y, y_pred)\n        \n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total / len(array), 1 / self.power)\n        \n    def get_final_metric(self, y_pred):\n        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average([\n            self._power_mean(bias_metrics[0]),\n            self._power_mean(bias_metrics[1]),\n            self._power_mean(bias_metrics[2])\n        ])\n        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n        bias_score = (1 - self.overall_model_weight) * bias_score\n        return overall_score + bias_score\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef eval_model(model, data_loader):\n    model.eval()\n    preds_fold = np.zeros(len(data_loader.dataset))\n\n    with torch.no_grad():\n        for index, x_batch, y_batch in data_loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n            y_pred = model(x_batch).detach()\n            preds_fold[list(index)] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n    return preds_fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\nseed_torch(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('load data'):\n    train_x, train_y, train_y_identity, test_x, nan_pred = load_and_prec()\n    train_nan_mask = train_x == '_##_'\n    test_nan_mask = test_x == '_##_'\n    y_binary = (train_y[:, 0] >= 0.5).astype(int)\n    y_identity_binary = (train_y_identity >= 0.5).astype(int)\n    vocab = build_vocab(chain(train_x, test_x), max_features)\n    embedding_matrix = load_embedding(EMBEDDING_FASTTEXT, vocab['token2id'])\n\n    train_x = np.array(tokenize(train_x, vocab))\n    test_x = np.array(tokenize(test_x, vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('pseudo label'):\n    train_preds = np.zeros((len(train_x)))\n    test_preds = np.zeros((len(test_x)))\n\n    ema_train_preds = np.zeros((len(train_x)))\n    ema_test_preds = np.zeros((len(test_x)))\n\n    train_dataset = TextDataset(train_x, targets=train_y, maxlen=max_len)\n    test_dataset = TextDataset(test_x, maxlen=max_len)\n\n    train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n                                  bucket_size=batch_size * 20, batch_size=batch_size)\n    test_sampler = BucketSampler(test_dataset, test_dataset.get_keys(),\n                                 batch_size=batch_size, shuffle_data=False)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n                              sampler=train_sampler, num_workers=0, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler,\n                             shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n    models = {}\n    model = NeuralNet(embedding_matrix).to(device)\n\n    ema_model = copy.deepcopy(model)\n    ema_model.eval()\n\n    ema_n = int(len(train_loader.dataset) / (updates_per_epoch * batch_size))\n    ema = EMA(model, mu, n=ema_n)\n\n    scale_fn = combine_scale_functions(\n        [partial(scale_cos, 1e-4, 5e-3), partial(scale_cos, 5e-3, 1e-3)], [0.2, 0.8])\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    scheduler = ParamScheduler(optimizer, scale_fn, train_epochs * len(train_loader))\n\n    all_test_preds = []\n\n    for epoch in range(train_epochs):\n        start_time = time.time()\n        model.train()\n\n        for _, x_batch, y_batch in train_loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            scheduler.batch_step()\n            y_pred = model(x_batch)\n\n            loss = nn.BCEWithLogitsLoss(weight=y_batch[:, 1])(y_pred[:, 0], y_batch[:, 0])\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            ema.on_batch_end(model)\n\n        elapsed_time = time.time() - start_time\n        print('Epoch {}/{} \\t time={:.2f}s'.format(\n            epoch + 1, train_epochs, elapsed_time))\n\n        test_preds = eval_model(model, test_loader)\n        all_test_preds.append(test_preds)\n\n        ema.on_epoch_end(model)\n\n    ema.set_weights(ema_model)\n    ema_model.lstm.flatten_parameters()\n    ema_model.gru.flatten_parameters()\n\n    checkpoint_weights = np.array([2 ** epoch for epoch in range(train_epochs)])\n    checkpoint_weights = checkpoint_weights / checkpoint_weights.sum()\n\n    ema_test_y = eval_model(ema_model, test_loader)\n    test_y = np.average(all_test_preds, weights=checkpoint_weights, axis=0)\n    test_y = np.mean([test_y, ema_test_y], axis=0)\n    test_y[test_nan_mask] = nan_pred\n    weight = np.ones((len(test_y)))\n    test_y = np.vstack((test_y, weight)).T\n\n    models['model'] = model.state_dict()\n    models['ema_model'] = ema_model.state_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('train'):\n    splits = list(\n        StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_x, y_binary))\n    splits_test = list(KFold(n_splits=n_splits, shuffle=True, random_state=seed).split(test_x))\n\n    for fold, ((train_idx, valid_idx), (train_idx_test, _)) in enumerate(zip(splits, splits_test)):\n        print(f'Fold {fold + 1}')\n\n        x_train_fold = np.concatenate((train_x[train_idx], test_x[train_idx_test]), axis=0)\n        y_train_fold = np.concatenate((train_y[train_idx], test_y[train_idx_test]), axis=0)\n\n        x_valid_fold = train_x[valid_idx]\n        y_valid_fold = train_y[valid_idx]\n\n        valid_nan_mask = train_nan_mask[valid_idx]\n\n        y_valid_fold_binary = y_binary[valid_idx]\n        y_valid_fold_identity_binary = y_identity_binary[valid_idx]\n        evaluator = JigsawEvaluator(y_valid_fold_binary, y_valid_fold_identity_binary)\n\n        train_dataset = TextDataset(x_train_fold, targets=y_train_fold, maxlen=max_len)\n        valid_dataset = TextDataset(x_valid_fold, targets=y_valid_fold, maxlen=max_len)\n\n        train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n                                      bucket_size=batch_size * 20, batch_size=batch_size)\n        valid_sampler = BucketSampler(valid_dataset, valid_dataset.get_keys(),\n                                      batch_size=batch_size, shuffle_data=False)\n\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n                                  sampler=train_sampler, num_workers=0, collate_fn=collate_fn)\n        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n                                  sampler=valid_sampler, collate_fn=collate_fn)\n\n        model = NeuralNet(embedding_matrix).to(device)\n\n        ema_model = copy.deepcopy(model)\n        ema_model.eval()\n\n        ema_n = int(len(train_loader.dataset) / (updates_per_epoch * batch_size))\n        ema = EMA(model, mu, n=ema_n)\n\n        scale_fn = combine_scale_functions(\n            [partial(scale_cos, 1e-4, 5e-3), partial(scale_cos, 5e-3, 1e-3)], [0.2, 0.8])\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n        scheduler = ParamScheduler(optimizer, scale_fn, train_epochs * len(train_loader))\n\n        all_valid_preds = []\n        all_test_preds = []\n\n        for epoch in range(train_epochs):\n            start_time = time.time()\n            model.train()\n\n            for _, x_batch, y_batch in train_loader:\n                x_batch = x_batch.to(device)\n                y_batch = y_batch.to(device)\n\n                scheduler.batch_step()\n                y_pred = model(x_batch)\n\n                loss = nn.BCEWithLogitsLoss(weight=y_batch[:, 1])(y_pred[:, 0], y_batch[:, 0])\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                ema.on_batch_end(model)\n\n            valid_preds = eval_model(model, valid_loader)\n            valid_preds[valid_nan_mask] = nan_pred\n            all_valid_preds.append(valid_preds)\n\n            auc_score = evaluator.get_final_metric(valid_preds)\n            elapsed_time = time.time() - start_time\n            print('Epoch {}/{} \\t auc={:.5f} \\t time={:.2f}s'.format(\n                epoch + 1, train_epochs, auc_score, elapsed_time))\n\n            test_preds = eval_model(model, test_loader)\n            all_test_preds.append(test_preds)\n\n            models[f'model_{fold}{epoch}'] = model.state_dict()\n\n            ema.on_epoch_end(model)\n\n        ema.set_weights(ema_model)\n        ema_model.lstm.flatten_parameters()\n        ema_model.gru.flatten_parameters()\n\n        models[f'ema_model_{fold}'] = ema_model.state_dict()\n\n        checkpoint_weights = np.array([2 ** epoch for epoch in range(train_epochs)])\n        checkpoint_weights = checkpoint_weights / checkpoint_weights.sum()\n\n        valid_preds_fold = np.average(all_valid_preds, weights=checkpoint_weights, axis=0)\n        valid_preds_fold[valid_nan_mask] = nan_pred\n        auc_score = evaluator.get_final_metric(valid_preds)\n        print(f'cv model \\t auc={auc_score:.5f}')\n\n        ema_valid_preds_fold = eval_model(ema_model, valid_loader)\n        ema_valid_preds_fold[valid_nan_mask] = nan_pred\n        auc_score = evaluator.get_final_metric(ema_valid_preds_fold)\n        print(f'EMA model \\t auc={auc_score:.5f}')\n\n        train_preds[valid_idx] = valid_preds_fold\n        ema_train_preds[valid_idx] = ema_valid_preds_fold\n\n        test_preds_fold = np.average(all_test_preds, weights=checkpoint_weights, axis=0)\n        ema_test_preds_fold = eval_model(ema_model, test_loader)\n\n        test_preds += test_preds_fold / n_splits\n        ema_test_preds += ema_test_preds_fold / n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(models, 'model.pt')\ntest_preds[test_nan_mask] = nan_pred\nema_test_preds[test_nan_mask] = nan_pred\nevaluator = JigsawEvaluator(y_binary, y_identity_binary)\nauc_score = evaluator.get_final_metric(train_preds)\nema_auc_score = evaluator.get_final_metric(ema_train_preds)\nprint(f'cv score: {auc_score:<8.5f}')\nprint(f'EMA cv score: {ema_auc_score:<8.5f}')\n\ntrain_preds = np.mean([train_preds, ema_train_preds], axis=0)\ntest_preds = np.mean([test_preds, ema_test_preds], axis=0)\nauc_score = evaluator.get_final_metric(train_preds)\nprint(f'final prediction score: {auc_score:<8.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(SAMPLE_SUBMISSION, index_col='id')\nsubmission['prediction'] = test_preds * 0.9 + test_y[:, 0] * 0.1\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}