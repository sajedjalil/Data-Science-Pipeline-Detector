{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\nWelcome. In this kernel we are going to implement a temporal convolutional network (abreviated as TCN - see https://arxiv.org/abs/1608.08242 ) There is a nice keras repository we will use as a starting point ( https://github.com/philipperemy/keras-tcn ) We will furthermore use one of the public kernel for all code not related to the TCN. Let me illustrate the main idea of a TCN:\n\n"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://github.com/philipperemy/keras-tcn/blob/master/misc/Dilated_Conv.png?raw=true\" width=\"1000px\"/>"},{"metadata":{},"cell_type":"markdown","source":"As you can see it has a directional structure, which captures dependencies between the input (in our case words) and aggregates into a numer of units. Similar to what LSTM and GRU does, however with less loops. So get into action."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom tqdm import tqdm_notebook as tqdm\nimport pickle\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"EMBEDDING_FILES = ['../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl','../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl']\nBATCH_SIZE = 512\nTCN_UNITS = 128\nDENSE_HIDDEN_UNITS = 2*TCN_UNITS\nEPOCHS = 4\nMAX_LEN = 220\nNUM_MODELS = 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://github.com/philipperemy/keras-tcn\n\n\nimport keras.backend as K\nimport keras.layers\nfrom keras import optimizers\nfrom keras.engine.topology import Layer\nfrom keras.layers import Activation, Lambda\nfrom keras.layers import Conv1D, SpatialDropout1D\nfrom keras.layers import Convolution1D, Dense\nfrom keras.models import Input, Model\nfrom typing import List, Tuple\n\n\ndef channel_normalization(x):\n    # type: (Layer) -> Layer\n    \"\"\" Normalize a layer to the maximum activation\n    This keeps a layers values between zero and one.\n    It helps with relu's unbounded activation\n    Args:\n        x: The layer to normalize\n    Returns:\n        A maximal normalized layer\n    \"\"\"\n    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n    out = x / max_values\n    return out\n\n\ndef wave_net_activation(x):\n    # type: (Layer) -> Layer\n    \"\"\"This method defines the activation used for WaveNet\n    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n    Args:\n        x: The layer we want to apply the activation to\n    Returns:\n        A new layer with the wavenet activation applied\n    \"\"\"\n    tanh_out = Activation('tanh')(x)\n    sigm_out = Activation('sigmoid')(x)\n    return keras.layers.multiply([tanh_out, sigm_out])\n\n\ndef residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n    # type: (Layer, int, int, str, int, int, float, str) -> Tuple[Layer, Layer]\n    \"\"\"Defines the residual block for the WaveNet TCN\n    Args:\n        x: The previous layer in the model\n        s: The stack index i.e. which stack in the overall TCN\n        i: The dilation power of 2 we are using for this residual block\n        activation: The name of the type of activation to use\n        nb_filters: The number of convolutional filters to use in this block\n        kernel_size: The size of the convolutional kernel\n        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n        name: Name of the model. Useful when having multiple TCN.\n    Returns:\n        A tuple where the first element is the residual model layer, and the second\n        is the skip connection.\n    \"\"\"\n\n    original_x = x\n    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n                  dilation_rate=i, padding=padding,\n                  name=name + '_dilated_conv_%d_tanh_s%d' % (i, s))(x)\n    if activation == 'norm_relu':\n        x = Activation('relu')(conv)\n        x = Lambda(channel_normalization)(x)\n    elif activation == 'wavenet':\n        x = wave_net_activation(conv)\n    else:\n        x = Activation(activation)(conv)\n\n    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n\n    # 1x1 conv.\n    x = Convolution1D(nb_filters, 1, padding='same')(x)\n    res_x = keras.layers.add([original_x, x])\n    return res_x, x\n\n\ndef process_dilations(dilations):\n    def is_power_of_two(num):\n        return num != 0 and ((num & (num - 1)) == 0)\n\n    if all([is_power_of_two(i) for i in dilations]):\n        return dilations\n\n    else:\n        new_dilations = [2 ** i for i in dilations]\n        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n        return new_dilations\n\n\nclass TCN(Layer):\n    \"\"\"Creates a TCN layer.\n        Args:\n            input_layer: A tensor of shape (batch_size, timesteps, input_dim).\n            nb_filters: The number of filters to use in the convolutional layers.\n            kernel_size: The size of the kernel to use in each convolutional layer.\n            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n            nb_stacks : The number of stacks of residual blocks to use.\n            activation: The activations to use (norm_relu, wavenet, relu...).\n            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n            name: Name of the model. Useful when having multiple TCN.\n        Returns:\n            A TCN layer.\n        \"\"\"\n\n    def __init__(self,\n                 nb_filters=64,\n                 kernel_size=2,\n                 nb_stacks=1,\n                 dilations=None,\n                 activation='norm_relu',\n                 padding='causal',\n                 use_skip_connections=True,\n                 dropout_rate=0.0,\n                 return_sequences=True,\n                 name='tcn'):\n        super().__init__()\n        self.name = name\n        self.return_sequences = return_sequences\n        self.dropout_rate = dropout_rate\n        self.use_skip_connections = use_skip_connections\n        self.activation = activation\n        self.dilations = dilations\n        self.nb_stacks = nb_stacks\n        self.kernel_size = kernel_size\n        self.nb_filters = nb_filters\n        self.padding = padding\n\n        # backwards incompatibility warning.\n        # o = tcn.TCN(i, return_sequences=False) =>\n        # o = tcn.TCN(return_sequences=False)(i)\n\n        if padding != 'causal' and padding != 'same':\n            raise ValueError(\"Only 'causal' or 'same' paddings are compatible for this layer.\")\n\n        if not isinstance(nb_filters, int):\n            print('An interface change occurred after the version 2.1.2.')\n            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n            raise Exception()\n\n    def __call__(self, inputs):\n        if self.dilations is None:\n            self.dilations = [1, 2, 4, 8, 16, 32]\n        x = inputs\n        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n        skip_connections = []\n        for s in range(self.nb_stacks):\n            for i in self.dilations:\n                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n                skip_connections.append(skip_out)\n        if self.use_skip_connections:\n            x = keras.layers.add(skip_connections)\n        x = Activation('relu')(x)\n\n        if not self.return_sequences:\n            output_slice_index = -1\n            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def get_coefs(word, *arr):\n#    return word, np.asarray(arr, dtype='float32')\n\n\n#def load_embeddings(path):\n#    with open(path) as f:\n#        return dict(get_coefs(*line.strip().split(' ')) for line in tqdm(f))\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        return pickle.load(f)\n    \ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix\n    \n\ndef custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]\n\n\ndef build_model(embedding_matrix, num_aux_targets, loss_weight):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16], name = 'tnc1_forward')(x) #, activation = 'wavenet'\n    x2 = Lambda(lambda z: K.reverse(z,axes=-1))(x)\n    x2 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16],name = 'tnc1_backward')(x2) #,dilations = [1, 2, 4]\n    x = add([x1,x2])\n    x1 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16], name = 'tnc2_forward')(x)\n    x2 = Lambda(lambda z: K.reverse(z,axes=-1))(x)\n    x2 = TCN(TCN_UNITS, return_sequences=True, dilations = [1, 2, 4, 8, 16],name = 'tnc2_backward')(x2)\n    x = add([x1,x2])\n    #x = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x)])\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], optimizer='adam')\n\n    return model\n    \n\ndef preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_train = preprocess(train['comment_text'])\nx_test = preprocess(test['comment_text'])\n\nidentity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', 'white', 'psychiatric_or_mental_illness']\ny_identities = (train[identity_columns] >= 0.5).astype(int).values\n# Overall\nweights = np.ones((len(x_train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n\nembedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n\n\n\nwith open('temporary.pickle', mode='wb') as f:\n    pickle.dump(x_test, f) # use temporary file to reduce memory\n\ndel identity_columns, weights, tokenizer, train, test, x_test\ngc.collect()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntr_ind, val_ind = train_test_split(list(range(len(x_train))) ,test_size = 0.05, random_state = 23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\ncheckpoint_predictions = []\ncheckpoint_val_preds = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1],loss_weight)\n    for global_epoch in range(EPOCHS):\n        es = EarlyStopping(patience = 1, verbose = True)\n        model.fit(x_train[tr_ind],[y_train[tr_ind], y_aux_train[tr_ind]],validation_data = (x_train[val_ind],[y_train[val_ind], y_aux_train[val_ind]]),\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 2e-3 * (0.6 ** global_epoch)),es\n            ]\n        )\n        with open('temporary.pickle', mode='rb') as f:\n            x_test = pickle.load(f) # use temporary file to reduce memory\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        checkpoint_val_preds.append(model.predict(x_train[val_ind], batch_size=2048)[0].flatten())\n        del x_test\n        gc.collect()\n        weights.append(2 ** global_epoch)\n    del model\n    gc.collect()\n\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = np.average(checkpoint_val_preds, weights=weights, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\ndef power_mean(x, p=-5):\n    return np.power(np.mean(np.power(x, p)),1/p)\n\ndef get_s_auc(y_true,y_pred,y_identity):\n    mask = y_identity==1\n    s_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    return s_auc\n\ndef get_bpsn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==0) | (y_identity==0) & (y_true==1)\n    bpsn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    return bpsn_auc\n\ndef get_bspn_auc(y_true,y_pred,y_identity):\n    mask = (y_identity==1) & (y_true==1) | (y_identity==0) & (y_true==0)\n    bspn_auc = roc_auc_score(y_true[mask],y_pred[mask])\n    return bspn_auc\n\ndef get_total_auc(y_true,y_pred,y_identities):\n\n    N = y_identities.shape[1]\n    saucs = np.array([get_s_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bpsns = np.array([get_bpsn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n    bspns = np.array([get_bspn_auc(y_true,y_pred,y_identities[:,i]) for i in range(N)])\n\n    M_s_auc = power_mean(saucs)\n    M_bpsns_auc = power_mean(bpsns)\n    M_bspns_auc = power_mean(bspns)\n    rauc = roc_auc_score(y_true,y_pred)\n\n\n    total_auc = M_s_auc + M_bpsns_auc + M_bspns_auc + rauc\n    total_auc/= 4\n\n    return total_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_total_auc(y_train[val_ind][:,0],val_preds,y_identities[val_ind])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\ndf_submit.prediction = predictions\ndf_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}