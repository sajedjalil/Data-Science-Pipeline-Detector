{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"debug = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()\nimport sys\npackage_dir = \"../input/pytorchpretrainedberthaqishen/pytorch-pretrained-bert/pytorch-pretrained-BERT/\"\nsys.path = [package_dir] + sys.path","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pickle\nimport argparse\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, BertModel, BertPreTrainedModel\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling_gpt2 import GPT2Config, GPT2Model, GPT2PreTrainedModel\nfrom pytorch_pretrained_bert.tokenization_gpt2 import GPT2Tokenizer\nfrom pytorch_pretrained_bert.modeling_xlnet import XLNetConfig, XLNetModel, XLNetPreTrainedModel\nfrom pytorch_pretrained_bert.tokenization_xlnet import XLNetTokenizer\n\nfrom scipy.stats import rankdata\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nTtokenizer = TreebankWordTokenizer()\n\nsymbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥âž¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûžâ€ Î¼âœ’âž¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼Žâ¬…â„…Â»Ð’Ð°Ð²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—žâ–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹âž¡Â«Ï†â…“â€žâœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑÐ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜ŽâœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆŽâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼žÊ•ÉÌ£Î”â‚€âœžâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜žï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\nsymbols_to_delete = '\\nðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶ï¸\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ðŸ˜ðŸ’–ðŸ’µÐ•ðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ðŸ˜‹ðŸ‘×©×œ×•××‘×™ðŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜××¢×›×—ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œðŸ˜–á´ ðŸš²â€ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ðŸ™„ï¼¨ðŸ˜ \\ufeff\\u2028ðŸ˜‰ðŸ˜¤â›ºðŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ðŸ‘®ðŸ’™ÙØ²Ø·ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08â€‘ðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šåœ°ç„è°·ÑƒÐ»ÐºÐ½ÐŸÐ¾ÐÐðŸ¾ðŸ•ðŸ˜†×”ðŸ”—ðŸš½æ­Œèˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸Ð¼Ï…Ñ‚Ñ•â¤µðŸ†ðŸŽƒðŸ˜©\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽÑÐ¿Ñ€Ð´\\x95ðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ðŸš¬ðŸ¤“\\ue602ðŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª×ž×“×£× ×¨×š×¦×˜ðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ÐµÑ…ðŸ˜²á¼¸á¾¶á½ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613å°åœŸè±†ðŸ¡â”â‰\\u202fðŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡è‹±æ–‡ðŸŒžðŸŽ²ãƒ¬ã‚¯ã‚µã‚¹ðŸ˜›å¤–å›½äººå…³ç³»Ð¡Ð±ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽÑŒÑ‹Ð³Ñä¸æ˜¯\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°á¸·Ð—Ð·â–±Ñ†ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®ä¼šä¸‹é™ä½ å¤±åŽ»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨Žéª—å­ðŸãƒ„ðŸŽ…\\x85ðŸºØ¢Ø¥Ø´Ø¡ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´Ê²ÑˆÑ‡Ð˜ÐžÐ Ð¤Ð”Ð¯ÐœÑŽÐ¶ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ðŸ’¨åœ†æ˜Žå›­×§â„ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦\\u200dð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–Ñ™ÐšÑ›ðŸ€ðŸ˜«ðŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºÏ–\\u2000Ò¯Õ½á´¦áŽ¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹ð€ð¥ðªðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸Ø¬íŒ¨í‹°ï¼·ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£Ñ–ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›â€’ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸å…¬å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ç¾Žð’‘ð’šð’ð‘´ðŸ¤™ðŸ’æ¬¢è¿Žæ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ð™«ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»ðŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—ðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·ð—¢ðŸ³ðŸ±ðŸ¬â¦ãƒžãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™Î¶ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šå¤šä¼¦ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ðŸ”«ðŸ‘å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ç‰ˆðŸ­ð‘¥ð‘¦ð‘§ï¼®ï¼§ðŸ‘£\\uf020ã£ðŸ‰Ñ„ðŸ’­ðŸŽ¥ÎžðŸ´ðŸ‘¨ðŸ¤³ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ú†ÛŒð‘®ð—•ð—´ðŸ’êœ¥â²£â²ðŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ðŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ðŸ‡³ð’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«Ú©Î’ÏŽðŸ’¢ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´ðŸ’’âŠ˜È»ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ï½‡ï½’ï½Žï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸ\\uf10aáƒšÚ¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œã•ã‚ˆã†ãªã‚‰ðŸ”¼'\nCONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\nfrom tqdm import tqdm\nimport warnings\nimport traceback\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\n\nprint('Import done! Time past %.2f secs' % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pandas multiprocessing\ndef _apply_df(args):\n    df, func, kwargs = args\n    return df.apply(func, **kwargs)\n\ndef apply_by_multiprocessing(df, func, **kwargs):\n    workers = kwargs.pop('workers')\n    pool = multiprocessing.Pool(processes=workers)\n    result = pool.map(_apply_df, [(d, func, kwargs)\n            for d in np.array_split(df, workers)])\n    pool.close()\n    return pd.concat(list(result))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertForJigsaw(BertPreTrainedModel):\n\n    def __init__(self, config, out_dim=7):\n        super(BertForJigsaw, self).__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.linear = nn.Linear(config.hidden_size, out_dim)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.linear(pooled_output)\n        return logits\n    \nclass GPT2ClassificationHeadModel(GPT2PreTrainedModel):\n\n    def __init__(self, config, clf_dropout=0.4, out_dim=8):\n        super(GPT2ClassificationHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.linear = nn.Linear(config.n_embd * 2, out_dim)\n\n        nn.init.normal_(self.linear.weight, std = 0.02)\n        nn.init.normal_(self.linear.bias, 0)\n        self.apply(self.init_weights)\n\n    def set_num_special_tokens(self, num_special_tokens):\n        pass\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None, **kwargs):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        if isinstance(hidden_states, list):\n            hidden_states = hidden_states[-1]\n        avg_pool = torch.mean(hidden_states, 1)\n        max_pool, _ = torch.max(hidden_states, 1)\n        h_conc = torch.cat((avg_pool, max_pool), 1)\n        return self.linear(h_conc)\n    \n    \nclass XLNetForJigSaw(XLNetPreTrainedModel):\n    def __init__(self, config, out_dim):\n        \n        super(XLNetForJigSaw, self).__init__(config)\n        self.attn_type = config.attn_type\n        self.same_length = config.same_length\n        self.summary_type = \"last\"\n\n        self.transformer = XLNetModel(config, output_attentions=False, keep_multihead_output=False)\n        self.dense = nn.Linear(config.d_model, config.d_model)\n        self.activation = nn.Tanh()\n        self.linear = nn.Linear(config.d_model, out_dim, bias=True)\n        self.apply(self.init_xlnet_weights)\n\n    def forward(self, input_ids, seg_id=None, input_mask=None,\n                mems=None, perm_mask=None, target_mapping=None, inp_q=None,\n                target=None, output_all_encoded_layers=True, head_mask=None, **kargs):\n\n        output, hidden_states, new_mems = self.transformer(input_ids, seg_id, input_mask,\n                                            mems, perm_mask, target_mapping, inp_q,\n                                            output_all_encoded_layers, head_mask)\n        first_token_tensor = output[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n\n        return self.linear(pooled_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_line(row, max_seq_length, tokenizer, model_name='bert'):\n    guid = row['id']\n    text_a = row['comment_text']\n\n    if 'label' in row.keys():\n        label = row['label']\n    else:\n        label = None\n\n    tokens_a = tokenizer.tokenize(text_a)\n\n    if 'bert' in model_name:\n        if len(tokens_a) > max_seq_length - 2:\n            tokens_a = tokens_a[:(max_seq_length - 2)]\n        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    else:\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n        tokens = tokens_a\n        input_ids = tokenizer.convert_tokens_to_ids(tokens_a)\n\n    segment_ids = [0] * len(tokens)\n    input_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += padding\n    input_mask += padding\n    segment_ids += padding\n\n    return input_ids, input_mask, segment_ids, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(x):\n    for k, v in CONTRACTION_MAPPING.items():\n        x = x.replace(' %s ' % k, ' %s ' % v)\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    x = Ttokenizer.tokenize(x)\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_input_data(test_data):\n    all_input_ids, all_input_mask,  all_segment_ids, all_label_ids = [], [], [], []\n    for i, (input_ids, input_mask, segment_ids, label) in test_data.items():\n        all_input_ids.append(input_ids)\n        all_input_mask.append(input_mask)\n        all_segment_ids.append(segment_ids)\n        all_label_ids.append(label)\n    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n    all_input_mask = torch.tensor(all_input_mask, dtype=torch.long)\n    all_segment_ids = torch.tensor(all_segment_ids, dtype=torch.long)\n    try:\n        all_label_ids = torch.tensor(all_label_ids, dtype=torch.float32)\n    except:\n        pass\n    \n    return all_input_ids, all_input_mask,  all_segment_ids, all_label_ids\n\nprint('Def functions done! Time past %.2f secs' % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading data...')\ndf = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nif debug:\n    df = df.loc[:25]\ntest_ids = df['id'].tolist()\ndf['comment_text'] = df['comment_text'].astype(str)\nprint('Preprocessing...')\ndf['comment_text'] = apply_by_multiprocessing(df['comment_text'], preprocess, workers=4)\nprint('Done! Time past %.2f secs' % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bert Small V2 29bin 300seq NAUX","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    model_dir = '../input/jigsawmodels/bert_small_v2_29bin_naus_300seq/bert_small_v2_29bin_naus_300seq/'\n    max_seq_length = 300\n    short_length = 100\n    tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=True)\n    print('Converting data to sequences...')\n    test_data = apply_by_multiprocessing(df, convert_line, axis=1, max_seq_length=max_seq_length, tokenizer=tokenizer, model_name='bert', workers=4)  # takes 2 mins\n    all_input_ids, all_input_mask,  all_segment_ids, all_label_ids = get_input_data(test_data)\n    long_idx = (all_input_ids[:, short_length-max_seq_length:].sum(1) > 0).nonzero().squeeze().numpy()\n    short_idx = (all_input_ids[:, short_length-max_seq_length:].sum(1) == 0).nonzero().squeeze().numpy()\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    print('Loading model from %s ...' % model_dir)\n    bert_config = BertConfig(os.path.join(model_dir, 'config.json'))\n    model = BertForJigsaw(bert_config, out_dim=29+6)  # with NAUX\n    model.load_state_dict(torch.load('%s/pytorch_model.bin' % model_dir))\n    model.to(device)\n    model.eval()\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    print('Predicting model Bert Small...')\n    predictions_bert_small = np.zeros(df.shape[0])\n    with torch.no_grad():\n        for i, idx in enumerate([short_idx, long_idx]):\n            test_data = TensorDataset(all_input_ids[idx], all_input_mask[idx], all_segment_ids[idx]) if i == 1 else \\\n                        TensorDataset(all_input_ids[idx, :short_length], all_input_mask[idx, :short_length], all_segment_ids[idx, :short_length])\n            test_sampler = SequentialSampler(test_data)\n            test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=128)\n\n            pred = []\n            for input_ids, input_mask, segment_ids in tqdm(test_dataloader):\n                input_ids = input_ids.to(device)\n                input_mask = input_mask.to(device)\n                segment_ids = segment_ids.to(device)\n\n                logits = model(input_ids, segment_ids, input_mask, labels=None)\n                logits = torch.sigmoid(logits).detach().cpu().numpy()\n                pred.append(logits[:, :-6])  # remove NAUX\n            predictions_bert_small[idx] = np.vstack(pred).mean(1)\n\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\nexcept:\n    print('Something wrong with Bert Small.')\n    traceback.print_exc()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predictions_bert_small)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_bert_small[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load and show train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.sample()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.hist(bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.severe_toxicity.hist(bins=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What problem do we address? \n# Synonyms proposal.\n# What cases do we have? \n# 1. User looks for synonism for offensive words. In this case we shouldn't provide them. Basically, it already works.\n# 2. Variety/Deduplication/Synonyms proposal â€“ proposed words in the given context shoudn't be neither offensive nor sensitive.\n# 2.1 Some of such words could be neutral by itself but offensive in some context. This couldn't be checked via dictionary only and we should address exact this problem.\n# 3. Let's find some examples that falls under this denition. Let's focus on short examples. \n# 4. Then check if they could be converted back to normal by replacing some words certain words. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show statistic on sentence length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['comment_text_len'] = train_df.comment_text.apply(lambda t: len(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['comment_text_len'].hist(bins=100, figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_text_len = 160","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"short_text_train_df = train_df[train_df.comment_text_len <= max_text_len]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(short_text_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show sample of both severe and not ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show prediction on portion of this data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For now, let's focus on certain category â€“ black. black >= 0.5, target >= 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"black_df = short_text_train_df[(short_text_train_df.target >= .5) & (short_text_train_df.black >= .5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(black_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for t in black_df[:100].comment_text: print(t); print(\"\\n *** \\n\"); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bert Large V2 99bin 250seq","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    model_dir = '../input/jigsawmodels/bert_large_v2_99bin_250seq/bert_large_v2_99bin_250seq/'\n    max_seq_length = 250\n    short_length = 100\n    tokenizer = BertTokenizer.from_pretrained(model_dir, do_lower_case=True)\n    print('Converting data to sequences...')\n    test_data = apply_by_multiprocessing(df, convert_line, axis=1, max_seq_length=max_seq_length, tokenizer=tokenizer, model_name='bert', workers=4)  # takes 2 mins\n    all_input_ids, all_input_mask,  all_segment_ids, all_label_ids = get_input_data(test_data)\n    long_idx = (all_input_ids[:, short_length-max_seq_length:].sum(1) > 0).nonzero().squeeze().numpy()\n    short_idx = (all_input_ids[:, short_length-max_seq_length:].sum(1) == 0).nonzero().squeeze().numpy()\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    print('Loading model from %s ...' % model_dir)\n    bert_config = BertConfig(os.path.join(model_dir, 'config.json'))\n    model = BertForJigsaw(bert_config, out_dim=99)\n    model.load_state_dict(torch.load('%s/pytorch_model.bin' % model_dir))\n    model.to(device)\n    model.eval()\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    print('Predicting model Bert Large')\n    predictions_bert_large = np.zeros(df.shape[0])\n    with torch.no_grad():\n        for i, idx in enumerate([short_idx, long_idx]):\n            test_data = TensorDataset(all_input_ids[idx], all_input_mask[idx], all_segment_ids[idx]) if i == 1 else \\\n                        TensorDataset(all_input_ids[idx, :short_length], all_input_mask[idx, :short_length], all_segment_ids[idx, :short_length])\n            test_sampler = SequentialSampler(test_data)\n            test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=64)\n\n            pred = []\n            for input_ids, input_mask, segment_ids in tqdm(test_dataloader):\n                input_ids = input_ids.to(device)\n                input_mask = input_mask.to(device)\n                segment_ids = segment_ids.to(device)\n\n                logits = model(input_ids, segment_ids, input_mask, labels=None)\n                logits = torch.sigmoid(logits).detach().cpu().numpy()\n                pred.append(logits)\n            predictions_bert_large[idx] = np.vstack(pred).mean(1)\n\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\nexcept:\n    print('Something wrong with Bert Large.')\n    traceback.print_exc()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XLNet 9bin 220seq","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    model_dir = '../input/jigsawmodels/xlnet_large_9bin_220seq/xlnet_large_9bin_220seq/'\n    max_seq_length = 220\n    tokenizer = XLNetTokenizer.from_pretrained(model_dir)\n    print('Converting to sequences...')\n    test_data = []\n    for i, row in tqdm(df.iterrows()):\n        test_data.append(convert_line(\n            row,\n            max_seq_length=max_seq_length,\n            tokenizer=tokenizer,\n            model_name='xlnet'\n        ))\n    test_data = pd.Series(test_data)\n    all_input_ids, all_input_mask,  all_segment_ids, all_label_ids = get_input_data(test_data)\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n    test_sampler = SequentialSampler(test_data)\n    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    print('Loading model from %s ...' % model_dir)\n    xlnet_config = XLNetConfig(os.path.join(model_dir, 'config.json'))\n    model = XLNetForJigSaw(xlnet_config, out_dim=9)\n    model.load_state_dict(torch.load('%s/pytorch_model.bin' % model_dir))\n    model.to(device)\n    model.eval()\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    print('Predicting model xlnet_large_v2_9bin_220seq')\n    predictions_xlnet = []\n    with torch.no_grad():\n\n        for input_ids, input_mask, segment_ids in tqdm(test_dataloader):\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n\n            logits = model(input_ids)\n            logits = torch.sigmoid(logits).detach().cpu().numpy()\n            predictions_xlnet.append(logits)\n\n    predictions_xlnet = np.vstack(predictions_xlnet).mean(1)\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\nexcept:\n    print('Something wrong with xlnet_large_v2_9bin_220seq.')\n    traceback.print_exc()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT2 V2 29bin 350seq NAUX","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    model_dir = '../input/jigsawmodels/gpt2_29bin_350seq_aus/gpt2_29bin_350seq_aus/'\n    max_seq_length = 350\n    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n    print('Converting data to sequences...')\n    test_data = apply_by_multiprocessing(df, convert_line, axis=1, max_seq_length=max_seq_length, tokenizer=tokenizer, model_name='gpt2', workers=4)  # takes 2 mins\n    all_input_ids, all_input_mask,  all_segment_ids, all_label_ids = get_input_data(test_data)\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n    test_sampler = SequentialSampler(test_data)\n    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=64)\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    print('Loading model from %s ...' % model_dir)\n    gpt2_config = GPT2Config(os.path.join(model_dir, 'config.json'))\n    model = GPT2ClassificationHeadModel(gpt2_config, out_dim=29+6)  # with NAUX\n    model.load_state_dict(torch.load('%s/pytorch_model.bin' % model_dir))\n    model.to(device)\n    model.eval()\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\n\n    print('Predicting model GPT2...')\n    predictions_gpt2 = []\n    with torch.no_grad():\n\n        for input_ids, input_mask, segment_ids in tqdm(test_dataloader):\n            \n            if (time.time() - start_time) > 7140:\n                print('STOP GPT2 FOR THE TIME SACK.')\n                break\n\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n\n            logits = model(\n                input_ids=input_ids,\n                token_type_ids=segment_ids,\n                attention_mask=input_mask,\n            )\n            logits = torch.sigmoid(logits).detach().cpu().numpy()\n            predictions_gpt2.append(logits[:, :-6])  # remove NAUX\n\n    predictions_gpt2 = np.vstack(predictions_gpt2).mean(1)\n    print('Done! Time past %.2f secs' % (time.time() - start_time))\nexcept:\n    print('Something wrong with GPT2:')\n    traceback.print_exc()\n    print('But the program is still running')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Output","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.zeros(df.shape[0])\n\ntry:\n    predictions += predictions_bert_small * 0.9\nexcept: pass\n\ntry:\n    predictions += predictions_bert_large * 1.0 \nexcept: pass\n\ntry:\n    predictions += predictions_gpt2 * 0.7\nexcept: pass\n\ntry:\n    predictions += predictions_xlnet * 0.8\nexcept: pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_prediction = pd.DataFrame({\n    'id': test_ids,\n    'prediction': predictions,\n})\ndf_prediction.to_csv('./submission.csv', index=False)\nprint('Output done! Time past %.2f secs' % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_prediction.head(25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sandbox","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}