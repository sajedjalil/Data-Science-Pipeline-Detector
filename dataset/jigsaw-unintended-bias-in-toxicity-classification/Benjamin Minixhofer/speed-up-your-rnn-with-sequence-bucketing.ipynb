{"cells":[{"metadata":{},"cell_type":"markdown","source":"__Update: Following the suggestion by @adityaecdrid and @jackwei (thanks!) I have added a comparison between different implementations of sequence bucketing including the one used in [@authmans great kernel](https://www.kaggle.com/authman/simple-lstm-pytorch-with-batch-loading) and changed the sequence bucketing method used as a result of this.__"},{"metadata":{},"cell_type":"markdown","source":"# Preface"},{"metadata":{},"cell_type":"markdown","source":"In this kernel, I am going to give you an overview of sequence bucketing. I was not able to find a good, simple tutorial for this so I decided to make one myself. \n\nIn RNNs, the input sequences are often all padded to the same length by doing something along the lines of this:\n\n`x_train = pad_sequences(x_train, maxlen=MAX_LEN)`\n\nThis is suboptimal because when iterating over the dataset in batches, there will be some batches where the length of all samples is smaller than `MAX_LEN`. So there will be tokens which are zero everywhere in the batch but are still processed by the RNN. Using sequence bucketing, we can speed this up by dynamically padding every batch to the maximum sequence length which occurs in that batch. Or to e. g. the 95th percentile of lengths in that batch.\n\nTo avoid confusion, you should refer to this practice as *sequence bucketing*, not just bucketing. \"Bucketing\" is often used synonimously to quantization, binning or discretization. [When googling \"bucketing machine learning\" you find results for a mix of both meanings which can be very confusing](http://lmgtfy.com/?q=bucketing+machine+learning).\n\nThe preprocessing and neural network architecture is taken from the best scoring public kernel at the time of writing: [Simple LSTM with Identity Parameters - Fast AI](https://www.kaggle.com/kunwar31/simple-lstm-with-identity-parameters-fastai). Up to the tokenization using the keras tokenizer, the code is exactly the same, so I have hidden it."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Put these at the top of every notebook, to get automatic reloading and inline plotting\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionPatch\nimport seaborn as sns\n\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# disable progress bars when submitting\ndef is_interactive():\n   return 'SHLVL' not in os.environ\n\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\n\nCRAWL_EMBEDDING_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nGLOVE_EMBEDDING_PATH = '../input/glove840b300dtxt/glove.840B.300d.txt'\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        emb_arr = []\n        for line in tqdm(f):\n            try:\n                emb_arr.append(get_coefs(*line.strip().split(' ')))\n            except Exception as e:\n                print(e)\n                               \n        return dict(emb_arr)\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef preprocess(data):\n    '''\n    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n    '''\n    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data\n\ndef train_model(learn,test,output_dim,lr=0.001,\n                batch_size=512, n_epochs=4,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds\n\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n\nx_train = preprocess(train['comment_text'])\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = preprocess(test['comment_text'])\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) / 4\n# Subgroup\nweights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative\nweights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive\nweights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n   (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\ny_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n\nmax_features = None\n\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()\n\n# x_train_torch = torch.tensor(x_train, dtype=torch.long)\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sequence Bucketing"},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So now we have `x_train` and `x_test`, both Python lists consisting of a list with tokens for each sample."},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I am going to take a look at the expected length of each batch with different batch sizes when using sequence bucketing and see how it compares to the fixed length of 220 to which the sequences were previously padded in the kernel."},{"metadata":{"trusted":false},"cell_type":"code","source":"lengths = np.array([len(x) for x in x_train])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function `plot_expected_lengths` samples a number of batches (per default 10000) of a specific batch size from the given lengths, and checks which length this batch would be padded to when using some function `choose_length`. `choose_length` has to return the length which will be padded to given an array of lengths in a batch. In the simplest case, this would be `lambda lengths: lengths.max()`. It could also be `lambda lengths: np.percentile(lengths, q=95)` to pad to the 95th percentile of lengths in a batch.\n\nThe lengths each of those 10000 batches are padded to are then plotted as a histogram, and the mean of all lengths with this batch size is plotted as a green line. This length can then be compared to the fixed length (shown in red) which the dataset was previously padded to."},{"metadata":{"trusted":false},"cell_type":"code","source":"def plot_expected_lengths(lengths, batch_sizes, choose_length, markers=[], n_batches=10000):\n    fig, axarr = plt.subplots(len(batch_sizes), 1, figsize=(14, 20), sharex=True)\n    expected_lengths = {}\n    \n    for i, batch_size in enumerate(batch_sizes):\n        maxs = []\n\n        for _ in tqdm(range(n_batches), disable=False):\n            val = choose_length(np.random.choice(lengths, batch_size))\n            maxs.append(math.ceil(val))\n\n        pd.Series(maxs).plot.hist(bins=50, ax=axarr[i], density=True, color='black', edgecolor='white', alpha=0.1)\n        expected = np.mean(maxs)\n        expected_lengths[batch_size] = expected\n        \n        max_y = axarr[i].get_ylim()[1]\n        \n        axarr[i].vlines([expected], 0, 1e3, 'limegreen', lw=4)\n        axarr[i].set_ylim([0, max_y])\n        axarr[i].set_xlim([0, max(lengths)])\n        axarr[i].set_ylabel(f'batch_size={batch_size}', rotation=0)\n        axarr[i].yaxis.set_label_coords(-0.1, 0.45)\n        axarr[i].set_yticks([])\n\n    for marker in markers:\n        con = ConnectionPatch(xyA=(marker, axarr[0].get_ylim()[1]), xyB=(marker, 0), coordsA='data', \n                              coordsB='data', axesA=axarr[0], axesB=axarr[-1], color='red', lw=4)\n        axarr[0].add_artist(con)\n    \n    axarr[0].set_zorder(1)\n    axarr[0].set_title(f'Expected sequence lengths with various batch sizes (n per batch = {n_batches})')\n    plt.subplots_adjust(hspace=0)\n    \n    return expected_lengths","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"batch_sizes = [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]\n\nexpected_lengths = plot_expected_lengths(lengths, batch_sizes, lambda lengths: lengths.max(), markers=[MAX_LEN])\nplt.xlabel('Maximum sequence length in batch')\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The expected batch length increases with the batch size. It even surpasses the maximum length of 220 at batch_size=32768, and it is significantly smaller than the fixed padding at a reasonable batch size of e. g. 512. When looking at the histogram, you can also see very well that the number of outliers increases when increasing the batch size. Because we are padding to the maximum length, the expected batch size is strongly influenced by outliers.\n\nNote that the difference between the green line and the red line for each batch size does not directly relate to the speedup; there is some small overhead to dynamically padding the sequences."},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"expected_lengths = plot_expected_lengths(lengths, batch_sizes, lambda lengths: np.percentile(lengths, q=95), markers=[MAX_LEN])\nplt.xlabel('95th percentile of sequence length in batch')\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When padding to the 95th percentile of batch lengths instead, we can see another interesting pattern. The expected sequence length does not change that much when increasing batch size because it is more robust to outliers. In fact, it very quickly approaches the 95th percentile of lengths in the whole dataset!"},{"metadata":{"trusted":false},"cell_type":"code","source":"np.percentile(lengths, q=95)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So it is questionable whether it makes sense to use sequence bucketing when padding to some percentile of the lengths for this dataset. We could just as well statically pad to the length at that percentile. \n\nBut it definitely does make sense if we want to get the last bit of performance out of our model and don't want to loose any information by truncating sequences. And it is still faster than the static padding of 220 with reasonably small batch sizes in that case."},{"metadata":{},"cell_type":"markdown","source":"Next, I'll compare different methods of sequence bucketing and check which one is fastest. If you just want to see the results using the best method you can skip this section."},{"metadata":{},"cell_type":"markdown","source":"# Which method of sequence bucketing should I use?"},{"metadata":{},"cell_type":"markdown","source":"I'm going to compare 3 methods:\n1. The one used in [@authmans great kernel](https://www.kaggle.com/authman/simple-lstm-pytorch-with-batch-loading). Uses a custom dataset that stores the text as regular list with variable lengths and implements sequence bucketing in a `collate_fn` in the data loader.\n2. The version I originally used in this kernel (version 1). Does sequence bucketing and splitting the data in batches on a dataset level. The data loader just always has a batch size of 1.\n3. A new version which uses a `collate_fn` in the data loader to change already padded sequences to variable-length batches. The dataset used is just a regular `TensorDataset`. Can be used as a drop-in replacement to static padding (and it is, as you'll see, the fastest).\n\nThe time it takes to iterate over the whole train dataset will be recorded for each of these methods when padding to the maximum length in a batch."},{"metadata":{},"cell_type":"markdown","source":"## Method 1: TextDataset with collate_fn"},{"metadata":{},"cell_type":"markdown","source":"Copied from [@authmans kernel](https://www.kaggle.com/authman/simple-lstm-pytorch-with-batch-loading). The only difference is that the data is not moved to the GPU to be able to compare it with the other methods. Remember to upvote that kernel :)\n\nA custom `collate_fn` is used in the DataLoader. The collate function takes a list of all samples in the batch as input and combines these to form one batch. This way, we can easily implement sequence bucketing in the data loader leaving the dataset untouched."},{"metadata":{"trusted":false},"cell_type":"code","source":"batch_size = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class TextDataset(data.Dataset):\n    def __init__(self, text, lens, y=None):\n        self.text = text\n        self.lens = lens\n        self.y = y\n\n    def __len__(self):\n        return len(self.lens)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.text[idx], self.lens[idx]\n        return self.text[idx], self.lens[idx], self.y[idx]\n    \nclass Collator(object):\n    def __init__(self, test=False, percentile=100):\n        self.test = test\n        self.percentile = percentile\n        \n    def __call__(self, batch):\n        global MAX_LEN\n        \n        if self.test:\n            texts, lens = zip(*batch)\n        else:\n            texts, lens, target = zip(*batch)\n\n        lens = np.array(lens)\n        max_len = min(int(np.percentile(lens, self.percentile)), MAX_LEN)\n        texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_len), dtype=torch.long)\n        \n        if self.test:\n            return texts\n        \n        return texts, torch.tensor(target, dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_collate = Collator(percentile=100)\ntrain_dataset = TextDataset(x_train, lengths, y_train_torch.numpy())\ntrain_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_collate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_repeats = 10\n\nstart_time = time.time()\nfor _ in range(n_repeats):\n    for batch in tqdm(train_loader):\n        pass\nmethod1_time = (time.time() - start_time) / n_repeats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 2: SequenceDataset with default loader"},{"metadata":{},"cell_type":"markdown","source":"The method used in version 1 of this kernel. Uses a custom dataset to do all the work (sequence bucketing, splitting in batches, shuffling). The data loader then just has a batch size of 1."},{"metadata":{"trusted":false},"cell_type":"code","source":"class SequenceDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset using sequence bucketing to pad each batch individually.\n    \n    Arguments:\n        sequences (list): A list of variable length tokens (e. g. from keras tokenizer.texts_to_sequences)\n        choose_length (function): A function which receives a numpy array of sequence lengths of one batch as input\n                                  and returns the length this batch should be padded to.\n        other_features (list, optional): A list of tensors with other features that should be fed to the NN alongside the sequences.\n        labels (Tensor, optional): A tensor with labels for the samples.\n        indices (np.array, optional): A numpy array consisting of indices to iterate over. \n        shuffle (bool): Whether to shuffle the dataset or not.  Default false.\n        batch_size (int): Batch size of the samples. Default 512.\n    \"\"\"\n    def __init__(self, sequences, choose_length, other_features=None, labels=None, \n                 indices=None, shuffle=False, batch_size=512):\n        super(SequenceDataset, self).__init__()\n        \n        self.sequences = np.array(sequences)\n        self.lengths = np.array([len(x) for x in sequences])\n        self.n_samples = len(sequences)\n        self.choose_length = choose_length\n        self.other_features = other_features\n        self.labels = labels\n        \n        if indices is not None:\n            self.indices = indices\n        else:\n            self.indices = np.arange(len(sequences))\n        \n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        if self.shuffle:\n            self._shuffle()\n        \n    def __len__(self):\n        return math.ceil(len(self.indices) / self.batch_size)\n        \n    def _shuffle(self):\n        self.indices = np.random.permutation(self.indices)\n    \n    def __getitem__(self, i):\n        idx = self.indices[(self.batch_size * i):(self.batch_size * (i + 1))]\n        \n        if self.shuffle and i == len(self) - 1:\n            self._shuffle()\n        \n        pad_length = math.ceil(self.choose_length(self.lengths[idx]))\n        padded_sequences = sequence.pad_sequences(self.sequences[idx], maxlen=pad_length)\n        \n        x_batch = [torch.tensor(padded_sequences, dtype=torch.long)]\n\n        if self.other_features is not None:\n            x_batch += [x[idx] for x in self.other_features]\n            \n        if self.labels is not None:\n            out = x_batch, self.labels[idx]\n        else:\n            out = x_batch\n    \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_dataset = SequenceDataset(x_train, lambda lengths: lengths.max(), other_features=[lengths], shuffle=False, batch_size=batch_size)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_repeats = 10\n\nstart_time = time.time()\nfor _ in range(n_repeats):\n    for batch in tqdm(train_loader):\n        pass\nmethod2_time = (time.time() - start_time) / n_repeats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 3: Default TensorDataset with custom collate_fn"},{"metadata":{},"cell_type":"markdown","source":"A new method using the DataLoader `collate_fn` for sequence bucketing that works with a regular `TensorDataset`."},{"metadata":{"trusted":false},"cell_type":"code","source":"lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n\nmaxlen = lengths.max() \nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\nx_train_padded.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"class SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\ntrain_loader = data.DataLoader(dataset, batch_size=batch_size, collate_fn=SequenceBucketCollator(lambda x: x.max(), 0, 1, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_repeats = 10\n\nstart_time = time.time()\nfor _ in range(n_repeats):\n    for batch in tqdm(train_loader):\n        pass\nmethod3_time = (time.time() - start_time) / n_repeats","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.set(font_scale=1.2)\nbarplot = sns.barplot(x=[\n    'TextDataset with collate_fn', \n    'SequenceDataset with default loader', \n    'Default TensorDataset with custom collate_fn'], y=[\n    method1_time,\n    method2_time,\n    method3_time\n])\n\nplt.title('Speed comparison of sequence bucketing methods')\nplt.ylabel('Time in seconds to iterate over whole train dataset')\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Method 1 is quite a bit solower than the rest, but method 2 and 3 are pretty close to each other (keep in mind that the majority of the time it takes to train the NN is spent in the actual computation anyway, not while loading). I am going to use method 3 because it is much more elegant and can be used as a drop-in replacement to static padding."},{"metadata":{},"cell_type":"markdown","source":"# Implementation & comparing static padding with sequence bucketing"},{"metadata":{},"cell_type":"markdown","source":"The `train_model` function is exactly the same. The NN itself is also only slightly different. It also accepts an optional `lengths` parameter because lengths are part of the dataset now."},{"metadata":{"trusted":false},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's time for some benchmarks! I am going to compare the time it takes to predict the whole train dataset using:\n- static padding (max_len=220)\n- sequence bucketing (maximum length)\n- static padding (to 95th percentile of lengths)\n- sequence bucketing (to 95th percentile of lengths)\n\nAll with a batch size of 512. You can see the result below."},{"metadata":{"trusted":false,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"batch_size = 512\n\nmodel = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\nmodel.cuda()\n\n# sequence bucketing - max length\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\ncollator = SequenceBucketCollator(lambda lengths: lengths.max(), 0, 1, 2)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                                           shuffle=False, collate_fn=collator)\n\nstart_time = time.time()\nfor x_batch in tqdm(train_loader):\n    model(x_batch[0][0].cuda())\nmax_length_seq_bucketing_time = time.time() - start_time\n\n# sequence bucketing - 95th percentile of lenghts\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\ncollator = SequenceBucketCollator(lambda lengths: np.percentile(lengths.numpy(), q=95), 0, 1, 2)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                                           shuffle=False, collate_fn=collator)\nstart_time = time.time()\nfor x_batch in tqdm(train_loader):\n    model(x_batch[0][0].cuda())\npercentile_seq_bucketing_time = time.time() - start_time\n\n# static padding - maxlen of 220\nx_train_padded = torch.tensor(sequence.pad_sequences(x_train, maxlen=MAX_LEN))\ntrain_dataset = data.TensorDataset(x_train_padded)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nstart_time = time.time()\nfor x_batch in tqdm(train_loader):\n    model(x_batch[0].cuda())\nmaxlen_static_time = time.time() - start_time\n\n# static padding - 95th percentile\npercentile_length = math.ceil(np.percentile(lengths, q=95))\nx_train_padded = torch.tensor(sequence.pad_sequences(x_train, maxlen=percentile_length))\ntrain_dataset = data.TensorDataset(x_train_padded)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n\nstart_time = time.time()\nfor x_batch in tqdm(train_loader):\n    model(x_batch[0].cuda())\npercentile_static_time = time.time() - start_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.set(font_scale=1.2)\nbarplot = sns.barplot(x=[\n    'static padding (max_len=220)', \n    'sequence bucketing (maximum length)', \n    'static padding (max_len=95th percentile)', \n    'sequence bucketing (95th percentile)'], y=[\n    maxlen_static_time,\n    max_length_seq_bucketing_time,\n    percentile_static_time,\n    percentile_seq_bucketing_time    \n])\n\nplt.title('Speed comparison of sequence bucketing and static padding')\nplt.ylabel('Inference time in seconds for whole train dataset')\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{},"cell_type":"markdown","source":"For training in this kernel, I will use sequence bucketing with maximum length."},{"metadata":{},"cell_type":"markdown","source":"Now we can instantiate a test, train and valid dataset and train the network. The validation dataset is only added so that the fast.ai DataBunch works as expected and it consists of only 2 samples."},{"metadata":{"trusted":false},"cell_type":"code","source":"lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\ntest_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\nmaxlen = lengths.max() \n\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"batch_size = 512\n\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def custom_loss(data, targets):\n    ''' Define custom loss function for weighted BCE on 'target' column '''\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, train the model and see that it is faster than before!\n\nOn my local machine, one epoch with statically padded sequences takes 7:25 to train (445 seconds). With sequence bucketing, one batch takes 6:26 (386 seconds). So the version with sequence bucketing is 1.15x faster."},{"metadata":{"trusted":false},"cell_type":"code","source":"all_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1234 + model_idx)\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \n    all_test_preds.append(test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Where to go from here"},{"metadata":{},"cell_type":"markdown","source":"In this kernel, I have shown the simplest possible method of sequence bucketing. There are also other methods, notably splitting the dataset itself into buckets consisting of samples with similar length and then taking all samples in a batch from one of those buckets instead of randomly choosing samples for a batch. This was introduced in [this paper](https://arxiv.org/abs/1708.05604).\n\nImplementing this method could potentially speed the training up even more.\n\nSequence bucketing could also be used to speed up the training of BERT. I don't have that much experience with BERT, but from what I have seen, samples are paddded statically in the popular [PyTorch implementation of BERT](https://github.com/huggingface/pytorch-pretrained-BERT). You might be able to train BERT significantly faster using sequence bucketing.\n\nThanks for reading!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}