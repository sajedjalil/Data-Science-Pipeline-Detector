{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport fastText\nfrom fastText import load_model\nimport gc\nimport re\ntqdm.pandas()\nfrom gensim.models import KeyedVectors\nfrom fastprogress import master_bar, progress_bar\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nVECS_PATH = Path('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec')\nTRAIN_DATA = '../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'\nTEST_DATA = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_misspell(misspell_dict):\n    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n    return misspell_dict, misspell_re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_typical_misspell(text):\n    misspellings, misspellings_re = _get_misspell(misspell_dict)\n\n    def replace(match):\n        return misspellings[match.group(0)]\n\n    return misspellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n          '>', '%', '=', '#', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', \"*\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(x):\n    x = str(x)\n    for punct in puncts + list(string.punctuation):\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_numbers(x):\n    return re.sub('\\d+', ' ', x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_fasttext(word_index):\n    embeddings_index = dict(get_coefs(*o.strip().split(' ')) for o in open(VECS_PATH))\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 240\nmax_features = 100000\nembed_size = 300\nbatch_size = 1024\ntrain_epochs = 5\nn_splits = 5\nseed = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_and_prec():\n    train = pd.read_csv(TRAIN_DATA, index_col='id')\n    test = pd.read_csv(TEST_DATA, index_col='id')\n    train['comment_text'] = train['comment_text'].str.lower()\n    test['comment_text'] = test['comment_text'].str.lower()\n    train['comment_text'] = train['comment_text'].apply(replace_typical_misspell)\n    test['comment_text'] = test['comment_text'].apply(replace_typical_misspell)\n    train['comment_text'] = train['comment_text'].apply(clean_text)\n    test['comment_text'] = test['comment_text'].apply(clean_text)\n    train['comment_text'] = train['comment_text'].apply(clean_numbers)\n    test['comment_text'] = test['comment_text'].apply(clean_numbers)\n    train_x = train['comment_text'].fillna('_##_').values\n    test_x = test['comment_text'].fillna('_##_').values\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_x))\n    train_x = tokenizer.texts_to_sequences(train_x)\n    test_x = tokenizer.texts_to_sequences(test_x)\n    train_x = pad_sequences(train_x, maxlen=maxlen)\n    test_x = pad_sequences(test_x, maxlen=maxlen)\n    train_y = (train['target'].values > 0.5).astype(int)\n    np.random.seed(seed)\n    train_idx = np.random.permutation(len(train_x))\n    train_x = train_x[train_idx]\n    train_y = train_y[train_idx]\n    return train_x, train_y, test_x, tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    \n    def __init__(self, embedding_matrix):\n        super(Net, self).__init__()\n        lstm_hidden_size = 120\n        gru_hidden_size = 60\n        self.gru_hidden_size = gru_hidden_size\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = nn.Dropout2d(0.1)\n        self.lstm = nn.LSTM(embed_size, lstm_hidden_size, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(lstm_hidden_size*2, gru_hidden_size, bidirectional=True, batch_first=True)\n        self.linear = nn.Linear(gru_hidden_size*6, 16)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.1)\n        self.out = nn.Linear(16, 1)\n        \n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = torch.unsqueeze(h_embedding.transpose(1, 2), 2)\n        h_embedding = torch.squeeze(self.embedding_dropout(h_embedding)).transpose(1, 2)\n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, hh_gru = self.gru(h_lstm)\n        hh_gru = hh_gru.view(-1, self.gru_hidden_size*2)\n        avg_pool = torch.mean(h_gru, 1)\n        max_pool, _ = torch.max(h_gru, 1)\n        conc = torch.cat((hh_gru, avg_pool, max_pool), 1)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from contextlib import contextmanager\nimport time\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@contextmanager\ndef timer(msg):\n    t0 = time.time()\n    print(f'[{msg}] start.')\n    yield\n    elapsed_time = time.time() - t0\n    print(f'[{msg}] done in {elapsed_time / 60:.2f} min.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with timer('load data'):\n    train_x, train_y, test_x, word_index = load_and_prec()\n    embedding_matrix = load_fasttext(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_torch(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n_camel_re2 = re.compile('([a-z0-9])([A-Z])')\ndef camel2snake(name):\n    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Callback():\n    _order = 0\n    def set_runner(self, run): self.run = run\n    def __getattr__(self, k): return getattr(self.run, k)\n    @property\n    def name(self):\n        name = re.sub(r'Callback$', '', self.__class__.__name__)\n        return camel2snake(name or 'callback')\n    def __call__(self, cb_name):\n        f = getattr(self, cb_name, None)\n        if f and f(): return True\n        return False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainEvalCallback(Callback):\n    def begin_fit(self):\n        self.run.n_epochs = 0\n        self.run.n_iter = 0\n    def after_batch(self):\n        if not self.in_train: return\n        self.run.n_epochs+=1./self.iters\n        self.run.n_iter+=1\n    def begin_epoch(self):\n        self.run.n_epochs = self.epoch\n        self.model.train()\n        self.run.in_train = True\n    def begin_validate(self):\n        self.model.eval()\n        self.run.in_train = False\n\nclass CancelTrainException(Exception): pass\nclass CancelEpochException(Exception): pass\nclass CancelBatchException(Exception): pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Iterable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def listify(o):\n    if o is None: return [] \n    if isinstance(o, list): return o\n    if isinstance(o, str): return [o]\n    if isinstance(o, Iterable): return list(o)\n    return [o]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Runner():\n    def __init__(self, cbs=None, cb_funcs=None):\n        cbs = listify(cbs)\n        for cbf in listify(cb_funcs):\n            cb = cbf()\n            setattr(self, cb.name, cb)\n            cbs.append(cb)\n        self.stop, self.cbs = False, [TrainEvalCallback()] + cbs\n    @property\n    def opt(self): return self.learn.opt\n    @property\n    def model(self): return self.learn.model\n    @property\n    def loss_func(self): return self.learn.loss_func\n    @property\n    def data(self): return self.learn.data\n    \n    def one_batch(self, xb, yb):\n        try: \n            self.xb, self.yb = xb, yb\n            self('begin_batch')\n            self.pred = self.model(self.xb)\n            self('after_pred')\n            self.loss = self.loss_func(self.pred, self.yb)\n            self('after_loss')\n            if not self.in_train: return\n            self.loss.backward()\n            self('after_backward')\n            self.opt.step()\n            self('after_step')\n            self.opt.zero_grad()\n        except CancelBatchException: self('after_cancel_batch')\n        finally: self('after_batch')\n    \n    def all_batches(self, dl):\n        self.iters = len(dl)\n        try:\n            for xb, yb in progress_bar(dl, leave=False): self.one_batch(xb, yb)\n        except CancelEpochException: self('after_cancel_epoch')\n    def fit(self, epochs, learn):\n        self.epochs, self.learn, self.loss = epochs, learn, torch.tensor(0.)\n        try: \n            for cb in self.cbs: cb.set_runner(self)\n            self('begin_fit')\n            for epoch in range(epochs):\n                self.epoch = epoch\n                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n                with torch.no_grad():\n                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n                self('after_epoch')\n        except CancelTrainException: self('after_cancel_train')\n        finally:\n            self('after_fit')\n            self.learn = None\n    def __call__(self, cb_name):\n        res = False\n        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n        return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Learner():\n    def __init__(self, model, opt, loss_func, data):\n        self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model(data, lr=0.001):\n    model = Net(embedding_matrix).to(device)\n    return model, torch.optim.Adam(model.parameters(), lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataBunch():\n    def __init__(self, train_dl, valid_dl):\n        self.train_dl, self.valid_dl = train_dl, valid_dl\n    @property\n    def train_ds(self): return self.train_dl.dataset\n    \n    @property\n    def valid_ds(self): return self.valid_dl.dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nsplits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed).split(train_x, train_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_cuda = torch.tensor(test_x, dtype=torch.long).to(device)\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_dl = DataLoader(test, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(train_idx, valid_idx):\n    x_train_ds = torch.tensor(train_x[train_idx], dtype=torch.long).to(device)\n    y_train_ds = torch.tensor(train_y[train_idx, np.newaxis], dtype=torch.float32).to(device)\n    x_val_ds = torch.tensor(train_x[valid_idx], dtype=torch.long).to(device)\n    y_val_ds = torch.tensor(train_y[valid_idx, np.newaxis], dtype=torch.float32).to(device)\n    train_ds = torch.utils.data.TensorDataset(x_train_ds, y_train_ds)\n    valid_ds = torch.utils.data.TensorDataset(x_val_ds, y_val_ds)\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n    data = DataBunch(train_dl, valid_dl)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=4, logits=False, reduce=True):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduce = reduce\n\n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n\n\nloss_fn = FocalLoss(logits=True)#nn.BCEWithLogitsLoss(reduction='sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_learner(train_idx, valid_idx):\n    data = get_data(train_idx, valid_idx)\n    learn = Learner(*get_model(data), loss_fn, data=data)\n    return learn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AvgStats():\n    def __init__(self, metrics, in_train): self.metrics, self.in_train = listify(metrics), in_train\n    def reset(self):\n        self.tot_loss, self.count = 0., 0\n        self.tot_mets = [0.]*len(self.metrics)\n    @property\n    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n    @property\n    def avg_stats(self): return [o/self.count for o in self.all_stats]\n    \n    def __repr__(self):\n        if not self.count: return ''\n        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n    def accumulate(self, run):\n        bn = run.xb.shape[0]\n        self.tot_loss+=run.loss*bn\n        self.count+=bn\n        for i, m in enumerate(self.metrics):\n            self.tot_mets[i]+=m(run.pred, run.yb)*bn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AvgStatsCallBack(Callback):\n    def __init__(self, metrics):\n        self.train_stats, self.valid_stats = AvgStats(metrics, True), AvgStats(metrics, False)\n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n    def after_loss(self):\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad(): stats.accumulate(self.run)\n    def after_epoch(self):\n        print(self.train_stats)\n        print(self.valid_stats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def roc(out, y):\n    score = roc_auc_score(y.cpu().detach().numpy(), out.cpu().detach().numpy())\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Recorder(Callback):\n    def begin_fit(self):\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n    def after_batch(self):\n        if not self.in_train: return\n        for pg, lr in zip(self.opt.param_groups, self.lrs): lr.append(pg['lr'])\n        self.losses.append(self.loss.detach().cpu())\n    \n    def plot_lr(self, pgid=-1): plt.plot(self.lrs[pgid])\n    def plot_loss(self, skip_last=0): plt.plot(self.losse[:len(self.losses)-skip_last])\n    def plot(self, skip_last=0, pgid=-1):\n        losses = [o.item() for o in self.losses]\n        lrs = self.lrs[pgid]\n        n = len(losses)-skip_last\n        plt.xscale('log')\n        plt.plot(lrs[:n], losses[:n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ParamScheduler(Callback):\n    _order = 1\n    def __init__(self, pname, sched_funcs): self.pname, self.sched_funcs = pname, sched_funcs\n    def begin_fit(self):\n        if not isinstance(self.sched_funcs, (list, tuple)):\n            self.sched_funcs = [self.sched_funcs]*len(self.opt.param_groups)\n    def set_param(self):\n        assert len(self.opt.param_groups)==len(self.sched_funcs)\n        for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs/self.epochs)\n    def begin_batch(self):\n        if self.in_train: self.set_param()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LR_Find(Callback):\n    _order = 1\n    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr\n        self.best_loss = 1e9\n    def begin_batch(self):\n        if not self.in_train: return\n        pos = self.n_iter/self.max_iter\n        lr = self.min_lr*(self.max_lr/self.min_lr)**pos\n        for pg in self.opt.param_groups: pg['lr'] = lr\n    def after_step(self):\n        if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:\n            raise CancelTrainException()\n        if self.loss<self.best_loss: self.best_loss = self.loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run = Runner(cb_funcs=[LR_Find, Recorder])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def annealer(f):\n    def _inner(start, end): return partial(f, start, end)\n    return _inner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@annealer\ndef sched_lin(start, end, pos): return start + pos*(end-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@annealer\ndef sched_cos(start, end, pos): return start + (1+math.cos(math.pi*(1-pos)))*(end-start)/2\n\n@annealer\ndef sched_no(start, end, pos): return start\n\n@annealer\ndef sched_exp(start, end, pos): return start*(end/start)**pos\n\ntorch.Tensor.ndim = property(lambda x: len(x.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_scheds(pcts, scheds):\n    assert sum(pcts)==1.\n    pcts = torch.tensor([0] + listify(pcts))\n    assert torch.all(pcts>=0)\n    pcts = torch.cumsum(pcts, 0)\n    def _inner(pos):\n        idx = (pos>=pcts).nonzero().max()\n        actual_pos = (pos-pcts[idx])/(pcts[idx+1]-pcts[idx])\n        return scheds[idx](actual_pos)\n    return _inner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pg_dicts(pgs): return [{'params': o} for o in pgs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cos_1cycle_anneal(start, high, end):\n    return [sched_cos(start, high), sched_cos(high, end)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phases = [0.2, 0.8]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scheds = combine_scheds(phases, [sched_cos(1e-4, 5e-3), sched_cos(5e-3, 1e-3)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb.long()).float().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbfs = [Recorder, partial(AvgStatsCallBack, roc), partial(ParamScheduler, 'lr', scheds)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run = Runner(cb_funcs=cbfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_eval():\n    test_preds = np.zeros(len(test_x))\n    for fold, (train_idx, valid_idx) in enumerate(splits):\n        print('Fold:', fold)\n        torch.cuda.empty_cache()\n        learn = get_learner(train_idx, valid_idx)\n        gc.collect()\n        run = Runner(cb_funcs=cbfs)\n        learn.model.train()\n        run.fit(4, learn)\n        learn.model.eval()\n        test_preds_fold = np.zeros(len(test_dl.dataset))\n        for i, (x_batch,) in enumerate(test_dl):\n            with torch.no_grad():\n                y_pred = learn.model(x_batch).detach()\n            test_preds_fold[i*batch_size:(i+1)*batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        test_preds+=test_preds_fold/len(splits)\n        del(learn)\n        gc.collect()\n        print(f'Test {fold} added')\n    print('Training Completed')\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x): return 1/(1+np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = train_and_eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsub['prediction'] = preds\nsub.reset_index(drop=False, inplace=True)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}