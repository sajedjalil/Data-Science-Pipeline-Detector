{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.data import Field\nfrom torchtext.data import Iterator, BucketIterator\nimport torch.optim as optim\nimport os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 64\nMAX_VOCAB_SIZE = 25_000","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample = pd.read_csv('../input/sample_submission.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"small = train[:-10000]\nvalid = train[-10000:]\nsmall.to_csv('small.csv', index=False)\nvalid.to_csv('valid.csv', index=False)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntokenize = lambda x: x.split()\nTEXT = Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True)\nLABEL = Field(sequential=False, use_vocab=False, dtype=torch.float)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data import TabularDataset\n \ntrain_datafields = [(col, TEXT) if col == 'comment_text' else \n                              (col, LABEL) if col == 'target' else \n                              (col, None) \n                              for col in train.columns]\ntrain_data, valid_data = TabularDataset.splits(\n            path='',\n            train='small.csv',\n            validation='valid.csv',\n            format='csv',\n            skip_header=True,\n            fields=train_datafields)\n\ntest_datafields = [('id', None), ('comment_text', TEXT)]\ntest_data = TabularDataset(\n            path=\"../input/test.csv\",\n            format='csv',\n            skip_header=True,\n            fields=test_datafields)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of test examples: {len(test_data)}')","execution_count":28,"outputs":[{"output_type":"stream","text":"Number of training examples: 1794874\nNumber of validation examples: 10000\nNumber of test examples: 97320\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\nLABEL.build_vocab(train_data)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iter, val_iter = BucketIterator.splits(\n    (train_data, valid_data),\n    batch_sizes=(BATCH_SIZE, BATCH_SIZE),\n    device=device,\n    sort_key=lambda x: len(x.comment_text),\n    sort_within_batch=True,\n    repeat=False\n)\n\ntest_iter = Iterator(test_data, batch_size=1, device=device, sort=False, sort_within_batch=False, repeat=False)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kudos to http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/\n\nclass BatchWrapper:\n    def __init__(self, iterator, x_var, y_vars):\n        self.iterator, self.x_var, self.y_vars = iterator, x_var, y_vars\n  \n    def __iter__(self):\n        for batch in self.iterator:\n            x = getattr(batch, self.x_var)\n            if self.y_vars is not None:\n                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n            else:\n                y = torch.zeros((1))\n            yield (x, y)\n    def __len__(self):\n        return len(self.iterator)\n\ntrain_loader = BatchWrapper(train_iter, \"comment_text\", [\"target\"])\nvalid_loader = BatchWrapper(val_iter, \"comment_text\", [\"target\"])\ntest_loader = BatchWrapper(test_iter, \"comment_text\", None)","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n        self.dropout = nn.Dropout(dropout)\n \n\n    def forward(self, text, text_lengths):\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        return self.fc(hidden.squeeze(0))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RNN(INPUT_DIM, \n            EMBEDDING_DIM, \n            HIDDEN_DIM, \n            OUTPUT_DIM, \n            N_LAYERS, \n            BIDIRECTIONAL, \n            DROPOUT, \n            PAD_IDX)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters())\nloss_func = nn.BCEWithLogitsLoss()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":15,"outputs":[{"output_type":"stream","text":"The model has 4,810,857 trainable parameters\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(device)\nloss_func = loss_func.to(device)","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, data_loader, optimizer, loss_func):\n    epoch_loss = 0\n    \n    model.train()\n    \n    for x, y in data_loader:\n        optimizer.zero_grad()\n        text, text_lengths = x\n        preds = model(text, text_lengths)\n        loss = loss_func(preds, y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(data_loader)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate_model(model, data_loader, loss_func):\n    val_loss = 0.0\n    model.eval()\n    for x, y in data_loader:\n        text, text_lengths = x\n        preds = model(text, text_lengths)\n        loss = loss_func(preds, y)\n        val_loss += loss.item()\n    return val_loss / len(data_loader)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 1\n\nbest_valid_loss = float('inf')\nbest_epoch = 0\n        \nfor epoch in range(1, epochs + 1):\n    epoch_loss = train_model(model, train_loader, optimizer, loss_func)\n    val_loss = validate_model(model, valid_loader, loss_func)\n    if val_loss < best_valid_loss:\n        best_valid_loss = val_loss\n        best_epoch = epoch\n        print(f'Best validation loss!! {best_valid_loss}')\n        torch.save(model.state_dict(), 'toxic_model.pt')\n    print(f'Epoch: {epoch}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Best validation loss at epoch = {best_epoch}')\nmodel.load_state_dict(torch.load('toxic_model.pt'))\ntest_preds = []\nfor i, tup in enumerate(test_loader):\n    if i % 1000 == 0:\n        print(f'Progress = {i/len(test_loader):.2%}')\n    x, y = tup\n    text, text_lengths = x\n    preds = model(text, text_lengths)\n    preds = preds.view(x[0].shape[1])\n    preds = preds.data.cpu().numpy()\n    preds = 1 / (1 + np.exp(-preds))\n    test_preds.append(preds)\ntest_preds = np.hstack(test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/test.csv')\nsubmission.loc[:, 'prediction'] = test_preds\nsubmission.drop('comment_text', axis=1).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}