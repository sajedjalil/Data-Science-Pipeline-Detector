{"cells":[{"metadata":{},"cell_type":"markdown","source":" - This has been created by using Past Quora comps + Artgor's kernel ! \n Thanks a lot :)\n - PS  The goal isn't really to beat Artgor's baseline, he can well beat me anytime but to help everyone in picking up Regex mainly!\n \n > Few Kernels used\n>> - https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2\n>> - https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n>> - https://www.kaggle.com/taindow/simple-cudnngru-python-keras\n>> - https://www.kaggle.com/artgor/basic-cnn-in-keras\n>> - https://www.kaggle.com/takuok/bidirectional-lstm-and-attention-lb-0-043/"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport random\ndef set_seed(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nset_seed(2411)\nSEED = 42\nimport psutil\nfrom multiprocessing import Pool\nimport multiprocessing\n\nnum_partitions = 10  # number of partitions to split dataframe\nnum_cores = psutil.cpu_count()  # number of cores on your machine\n\nprint('number of cores:', num_cores)\n\ndef df_parallelize_run(df, func):\n    \n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <center> A Tutorial On Understanding ([Rr]ege)(x|xp|xes|xps|xen)</center>\n\n<center> A common workflow with regular expressions is that you write a pattern for the thing you are looking for... <center>"},{"metadata":{},"cell_type":"markdown","source":"# Let's see our very first expression\n\n- **<\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}\\b>**  It's a complex pattern as it includes lots of things like \n    - Character Class\n    - Alphabets\n    - Percentage\n    - Numbers\n    - Underscores\n    - $\\{\\}$, word Boundaries etc...\n    \n> In short  - **this pattern describes an email address; With the above regex pattern, we can search through a text file to find email addresses, or verify if a given string looks like an email address..**\n\nThe most basic regex pattern in a token like just an $<b>$ i.e a single literal character. In the string **\" Zebra is an animal.\"**, this will match the very first $b$ in the **Ze$b$** Note that it doesn't matter whether it's present in the middle of the word as of now..\n\nNow let me introduce few very basics things used in $<regex>$ to define itself (remeber the e-mail address pattern above, now we will break into piece by piece..)\n\nIn the regex discussed in this tutorial, there are **11** characters with special meanings:\nthe opening square bracket $<[>$, the backslash, the caret <^>, the dollar sign <$>, the period or dot <.>, the\nvertical bar or pipe symbol <|>, the question mark <?>, the asterisk or star <*>, the plus sign <+>, the opening\nround bracket <(> and the closing round bracket <)>. These special characters are often called **‚Äúmetacharacters‚Äù**.\n\n|Meta character|Description|\n|:----:|----|\n|**.**|<b>Period matches any single character except a line break.<b>|\n|**[ ]**|<b>Character class. Matches any character contained between the square brackets.<b>|\n|**[^ ]**|<b>Negated character class. Matches any character that is not contained between the square brackets <b>.|\n|*****|<b>Matches 0 or more repetitions of the preceding symbol.<b>|\n|**+**|<b>Matches 1 or more repetitions of the preceding symbol.<b>|\n|**?**|<b>Makes the preceding symbol optional.<b>|\n|**{n,m}**|<b>Braces. Matches at least \"n\" but not more than \"m\" repetitions of the preceding symbol.<b>|\n|**(xyz)**|<b>Character group. Matches the characters xyz in that exact order.<b>|\n|**&#124;**|<b>Alternation. Matches either the characters before or the characters after the symbol.<b>|\n|**&#92;**|<b>Escapes the next character. This allows you to match reserved characters `[ ] ( ) { } . * + ? ^ $ \\`.<b>| \n|**^**|<b>Matches the beginning of the input.<b>|\n|**$**|<b>Matches the end of the input.<b>|\n\n\nRead More [here](http://www.rexegg.com/regex-quickstart.html#chars) and [here](http://www.greenend.org.uk/rjk/tech/regexp.html). Both are Very Very Good...\n\n- Example  - If you want to use any of these characters as a literal in a regex, you need to escape them with a backslash. If\nyou want to match **<1+1=2>**, the correct regex is $1\\+1=2$. Otherwise, the plus sign will have a special meaning. **Note** that **<1+1=2>**, with the *backslash omitted*, is a **valid** regex. So you will **not** get an error message. But it\nwill not match **<1+1=2>**. "},{"metadata":{},"cell_type":"markdown","source":"# The Regex-Directed Engine Always Returns the Left-most Match\nThis is a very important point to understand: a regex-directed engine will always return the leftmost match,\neven if a **better** match could be found later. When applying a regex to a string, the engine will start at the\nfirst character of the string. It will try all possible permutations of the regular expression at the first character.\nOnly if all possibilities have been tried and found to fail, will the engine continue with the second character in\nthe text. Again, it will try all possible permutations of the regex, in exactly the same order. The result is that\nthe regex-directed engine will return the leftmost match.\n- When applying **<cat\\>** to **He captured a catfish for his cat.**, the engine will try to match the first\ntoken in the regex **<c\\>** to the first character in the match **H**. This fails. There are no other possible\npermutations of this regex, because it merely consists of a sequence of literal characters. So the regex engine\ntries to match the **<c\\>** with the **e**. This fails too, as does matching the **c** with the space. Arriving at the 4th\ncharacter in the match, **<c\\>** matches **c**. The engine will then try to match the second token **<a\\>** to the 5th\ncharacter, **a**. This succeeds too. But then, **<t\\>** fails to match **p**. At that point, the engine knows the regex\ncannot be matched starting at the 4th character in the match. So it will continue with the 5th: **a**. Again, **<c\\>**\nfails to match here and the engine carries on. At the 15th character in the match, **<c\\>** again matches **c**. The\nengine then proceeds to attempt to match the remainder of the regex at character 15 and finds that **<a\\>**\nmatches **a** and **<t\\>** matches **t**.\n\n- The entire regular expression could be matched starting at character 15. The engine is **\"eager\"** to report a\nmatch. **It will therefore report the first three letters of catfish as a valid match**. The engine **never** proceeds\nbeyond this point to see if there are any **better** matches. The *first match* is considered good enough. "},{"metadata":{},"cell_type":"markdown","source":"# Regex's Fundamentals"},{"metadata":{},"cell_type":"markdown","source":"### Character Sets/Classes\nCharacter sets are also called character class. **Square brackets** are used to specify character sets. Use a **hyphen** inside a character set to specify the characters' range. The order of the character range inside square brackets doesn't matter. For example, the regular expression `[Tt]he` means: `an uppercase T or lowercase t, followed by the letter h, followed by the letter e.`\n\n- **<[Tt]he>** => <font color=red>The</font> car parked in <font color=red>the</font> garage.\n\nA period inside a character set, however, means a literal period. The regular expression **<ar[.]>** means: a lowercase character a, followed by letter r, followed by a period **.** character.\n\n- **<ar[.]>** => A garage is a good place to park a c<font color=red>ar.</font>\n\n- **<[0-9]>** => Matches a **single digit between 0 and 9**. You can use more than one range.\n- **<[0-9a-fA-F]>** => Matches a **single hexadecimal digit**, case insensitively. \n- You can combine ranges and single characters. **<[0-9a-fxA-FX]>** matches a hexadecimal digit or the letter X.* Again, the order of the characters and the ranges does not matter.*\n- Find a word, even if it is misspelled, such as **<sep[ae]r[ae]te>** or **<li[cs]en[cs]e>**. \n\n### Negated Character Sets/Classes\n\nTyping a **caret(^)** after the opening square bracket will negate the character class. **The result is that the character\nclass will match any character that is <font color = red>not </font> in the character class.**\n- It is important to remember that a negated character class **still must match a character**. **<q[^u]>** does not\nmean: **<font color= red> a q not followed by a u </font>**. It means: **<font color= red a q followed by a character that is not a u </font>**. It will **not** match the\n$q$ in the string $Iraq$. It will match the $q$ and $the space$ after the $q$ in **Iraq is a country**.\n\n###  Shorthand Character Sets\n\nRegular expression provides **shorthands** for the commonly used character sets,\nwhich offer **convenient shorthands** for commonly used regular expressions. The\nshorthand character sets are as follows:\n\n|Shorthand|Description|\n|:----:|----|\n|<b>.<b>|<b>Any character except new line. It's the most commonly misused metacharacter.<b>|\n|<b>\\w<b>|<b>Matches alphanumeric characters: `[a-zA-Z0-9_]`<b>|\n|<b>\\W<b>|<b>Matches non-alphanumeric characters: `[^\\w]`<b>|\n|<b>\\d<b>|<b>Matches digit: `[0-9]`<b>|\n|<b>\\D<b>|<b>Matches non-digit: `[^\\d]`<b>|\n|<b>\\s<b>|<b>Matches whitespace character: `[\\t\\n\\f\\r\\p{Z}]`<b>|\n|<b>\\S<b>|<b>Matches non-whitespace character: `[^\\s]`<b>|\n    \n## Repetitions\n\nFollowing meta characters `+`, `*` or `?` are used to specify how many times a\nsubpattern can occur. These meta characters act differently in different\nsituations.\n\n### The Star *\n\nThe symbol `*` matches zero or more repetitions of the preceding matcher. The\nregular expression `a*` means: zero or more repetitions of preceding lowercase\ncharacter `a`. But if it appears after a character set or class then it finds\nthe repetitions of the whole character set. \nFor example, the regular expression\n- `[a-z]*` means: any number of lowercase letters in a row.\n\nThe `*` symbol can be used with the meta character `.` to match any string of\ncharacters `.*`. The `*` symbol can be used with the whitespace character `\\s`\nto match a string of whitespace characters. For example, the expression\n`\\s*cat\\s*` means: zero or more spaces, followed by lowercase character `c`,\nfollowed by lowercase character `a`, followed by lowercase character `t`,\nfollowed by zero or more spaces.\n\n### The Plus +\n\nThe symbol `+` matches one or more repetitions of the preceding character. For\nexample, the regular expression `c.+t` means: lowercase letter `c`, followed by\nat least one character, followed by the lowercase character `t`. It needs to be\nclarified that `t` is the last `t` in the sentence.\n\n- **<c.+t>** => The fat <font color='red'> cat sat on the mat</font>.\n\n### The Question Mark ?\nIn regular expression the meta character `?` makes the preceding character optional. This symbol matches zero or one instance of the preceding character. For example, the regular expression `[T]?he` means: `Optional the uppercase letter T, followed by the lowercase character h, followed by the lowercase character e.`\n\n- **<[Tt]he>** => <font color=red>The</font> car parked in <font color=red>the</font> garage.\n\n### The Lazy Star *? \n\nRepeats the previous item zero or more times. Lazy, so the engine first attempts to skip the\nprevious item, before trying permutations with ever increasing matches of the preceding\nitem. \n\n|Regex|Means|\n|:----:|----|\n|abc+|        matches a string that has ab followed by one or more c|\n|abc?|       matches a string that has ab followed by zero or one c|\n|abc{2}|      matches a string that has ab followed by 2 c|\n|abc{2,}|    matches a string that has ab followed by 2 or more c|\n|abc{2,5}|    matches a string that has ab followed by 2 up to 5 c|\n|a(bc)\\*|     matches a string that has a followed by zero or more copies of the sequence bc|\n|a(bc){2,5}|  matches a string that has a followed by 2 up to 5 copies of the sequence bc|\n|**<.+>**| matches `<div>simple div</div>`|\n\n## Full stop or Period or dot **.**\n\nIn regular expressions, the dot or period is one of the most commonly used metacharacters. Unfortunately, it\nis also the most commonly misused metacharacter. The dot is short for the negated character class **<[^\\n]>** (UNIX regex flavors) or\n**<[^\\r\\n]>** (Windows regex flavors).\n\n<font color = 'Red'> <b> Use The Dot Sparingly </b> </font>\n- The dot is a **very powerful** regex metacharacter. It allows you to be **lazy**. `Put in a dot, and everything will\nmatch just fine when you test the regex on valid data. The problem is that the regex will also match in cases\nwhere it should not match..`\n    \nExample - Let‚Äôs say we want to match a date in `mm/dd/yy` format, but we\nwant to leave the user the choice of date separators. The quick solution is **<\\d\\d.\\d\\d.\\d\\d>**. Seems fine at\nfirst sight.. It will match a date like `02/12/03` just what we intended, So fine... \n- <font color='red'> <b> Trouble is: 02512703<b></font> is also considered a **valid date** by this regular expression. In this match, the first dot matched $5$, and the second matched $7$. Obviously $not$ what we intended. \n    \n## Start of String and End of String Anchors ( $ and ^)\n\nAnchors are a different breed. They do not match any character at all. Instead, they match a position before,\nafter or between characters. They can be used to `anchor` the regex match at a certain position. \n- The caret **<^>**\nmatches the position before the first character in the string. Applying **<^a>** to `abc` matches `a`. **<^b>** will\nnot match `abc` at all, because the **<b\\>** cannot be matched right after the start of the string, matched by **<^>**.\n- Similarly, **<\\$>** matches right after the last character in the string. **<c\\$>** matches `c` in `abc`, while **<a\\$>** `does not` match `abc` at all...."},{"metadata":{},"cell_type":"markdown","source":"# Examples\n\n- Regex are now written in Quotes (\\`)\n- The String to be matched is in (\"bold\")\n\nSuppose you want to use a regex to match a list of function names in a programming language: \"**Get, GetValue, Set or SetValue.**\" \n- The obvious solution is `Get|GetValue|Set|SetValue`\n\n*Now take a look closer carefully at the regex and the string, both.\nHere are some other ways to do the same task*\n-  `Get(Value)?|Set(Value)?`\n-  `\\b(Get|GetValue|Set|SetValue)\\b`\n-  `\\b(Get(Value)?|Set(Value)?)\\b`\n- Even this one is correct `\\b(Get|Set)(Value)?\\b`\n\n**Regex**:\t`<[^>]+>`\n- **What it does**:\tThis finds any HTML, such as `<\\a>, <\\b>, <\\img />, <\\br />, etc`. You can use this to find segments that have HTML tags you need to deal with, or to remove all HTML tags from a text.\n    \n**Regex**:\t`https?:\\/\\/[\\w\\.\\/\\-?=&%,]+`\n- What it does:\tThis will find a URL. It will capture most URLs that begin with http:// or https://.\n\n**Regex**:\t`'\\w+?'`\n- **What it does**:\tThis finds single words that are surrounded by apostrophes.\n\n**Regex**:\t`([-A-Za-z0-9_]*?([-A-Za-z_][0-9]|[0-9][-A-Za-z_])[-A-Za-z0-9_]*)`\n- **What it does**:\tAlphanumeric part numbers and references like: 1111_A, AA1AAA or 1-1-1-A, 21A1 and 10UC10P-BACW, abcd-1234, 1234-pqtJK, sft-0021 or 21-1_AB and 55A or AK7_GY.\nThis can be very useful if you are translating documents that have a lot of alphanumeric codes or references in them, and you need to be able to find them easily.\n\n**Regex**:\t`\\b(the|The)\\b.*?\\b(?=\\W?\\b(is|are|was|can|shall| must|that|which|about|by|at|if|when|should|among|above|under|$)\\b)`\n- **What it does**:\tThis finds text that begins with the or The and ends with stop words such as is, are, was, can, shall, must, that, which, about, by, at, if, when, should, among, above or under, or the end of the segment.\nThis is particularly useful when you need to extract terminology. Suppose you have segments like these:\n`\nThe Web based look up is our new feature. A project manager should not proofread... Our Product Name is...`\n    - The Regex shown above would find anything between The and is, or should. With most texts, there is a good chance that anything this Regex finds is a good term that you can add to your Termbase.\n\n**Regex**:\t`\\b(a|an|A|An)\\b.*?\\b(?=\\W?\\b(is|are|was|can|shall|must |that|which|about|by|at|if|when|among|above|under|$)\\b)`\n- **What it does**:\tThis works much like the Regex shown above, except that it finds text that begins with a or an, rather than the. This can also be very helpful when you need to extract terminology from a project.\n\n**Regex**:  `\\b(this|these|This|These)\\b.*?\\b(?=\\W?\\b(is|are|was|can|shall|must|that|which|about|by|at|if|when|among|above|under|$)\\b)`\n    - **What it does**:\tThis works much like the Regex shown above, except that it finds text that begins with this or these. This can also be very helpful when you need to extract terminology from a project.\n\n**Regex** :`(.*?)`\n- **What it does** : Accept blah-blah-blah...\n\n## Python [re module](https://docs.python.org/3/library/re.html)\n\n- `re.sub(regex, replacement, subject)` performs a search-and-replace across subject, replacing all\nmatches of regex in subject with replacement. The result is returned by the sub() function. **The subject\nstring you pass is not modified**. The re.sub() function applies the same backslash logic to the replacement text as is applied to the regular expression. Therefore, you should use raw strings for the replacement text..."},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ntweet = '#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android +#apps +#beautiful \\\n         #cute #health #igers #iphoneonly #iphonesia #iphone \\\n             <3 ;D :( :-('\n\n#Let's take care of emojis and the #(hash-tags)...\n\nprint(f'Original Tweet ---- \\n {tweet}')\n\n## Replacing #hashtag with only hashtag\ntweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n#this gets a bit technical as here we are using Backreferencing and Character Sets Shorthands and replacing the captured Group.\n#\\S = [^\\s] Matches any charachter that isn't white space\nprint(f'\\n Tweet after replacing hashtags ----\\n  {tweet}')\n\n## Love -- <3, :*\ntweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\nprint(f'\\n Tweet after replacing Emojis for Love with EMP_POS ----\\n  {tweet}')\n\n#The parentheses are for Grouping, so we search (remeber the raw string (`r`))\n#either for <3 or(|) :\\* (as * is a meta character, so preceeded by the backslash)\n\n## Wink -- ;-), ;), ;-D, ;D, (;,  (-;\ntweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\nprint(f'\\n Tweet after replacing Emojis for Wink with EMP_POS ----\\n  {tweet}')\n\n#The parentheses are for Grouping as usual, then we first focus on `;-), ;),`, so we can see that 1st we need to have a ;\n#and then we can either have a `-` or nothing, so we can do this via using our `?` clubbed with `;` and hence we have the very\n#starting with `(;-?\\)` and simarly for others...\n\n## Sad -- :-(, : (, :(, ):, )-:\ntweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\nprint(f'\\n Tweet after replacing Emojis for Sad with EMP_NEG ----\\n  {tweet}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##See the Output Carefully, there are Spaces inbetween un-necessary...\n## Replace multiple spaces with a single space\ntweet = re.sub(r'\\s+', ' ', tweet)\nprint(f'\\n Tweet after replacing xtra spaces ----\\n  {tweet}')\n      \n##Replace the Puctuations (+,;) \ntweet = re.sub(r'[^\\w\\s]','',tweet)\nprint(f'\\n Tweet after replacing Punctuation + with PUNC ----\\n  {tweet}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bags of positive/negative smiles (You can extend the above example to take care of these few too...))) A good Excercise...\n\npositive_emojis = set([\n\":‚Äë)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‚ÄëD\",\":D\",\"8‚ÄëD\",\"8D\",\n\"x‚ÄëD\",\"xD\",\"X‚ÄëD\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‚Äë)\",\";)\",\"*-)\",\"*)\",\";‚Äë]\",\";]\",\";^)\",\":‚Äë,\",\";D\",\":‚ÄëP\",\":P\",\"X‚ÄëP\",\"XP\",\n\"x‚Äëp\",\"xp\",\":‚Äëp\",\":p\",\":‚Äë√û\",\":√û\",\":‚Äë√æ\",\":√æ\",\":‚Äëb\",\":b\",\"d:\",\"=p\",\">:P\", \":'‚Äë)\", \":')\",  \":-*\", \":*\", \":√ó\"\n])\nnegative_emojis = set([\n\":‚Äë(\",\":(\",\":‚Äëc\",\":c\",\":‚Äë<\",\":<\",\":‚Äë[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‚Äë':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‚Äë/\",\n\":/\",\":‚Äë.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‚Äë|\",\":|\",\"|‚ÄëO\",\"<:‚Äë|\"\n])\n\ndel positive_emojis, negative_emojis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Valid Dates..\npattern = r'(19|20)\\d\\d[- /.](0[1-9]|1[012])[- /.](0[1-9]|[12][0-9]|3[01])' ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- matches a date in yyyy-mm-dd format from between 1900-01-01 and 2099-12-31, with a choice of four separators(space included :))\n\n- The year is matched by `(19|20)\\d\\d`\n- The month is matched by `(0[1-9]|1[012])` (rounding brackets are necessary so that to include both the options)\n    - By using character classes, \n        - the first option matches a number between `01 and 09`, and \n        - the second matches `10, 11 or 12`\n- The last part of the regex consists of three options. The first matches the numbers `01\nthrough 09`, the second `10 through 29`, and the third matches `30 or 31`... "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Pattern to match any IP Addresses \npattern = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the above pattern will also match `999.999.999.999` but that isn't a valid IP at all\nNow this depends on the data at hand as to how far you want the regex to be accurate...\nTo restrict all `4` numbers in the IP address to `0..255`, you can use this\ncomplex beast: \n- `\\b(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-\n9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.(25[0-5]|2[0-\n4][0-9]|[01]?[0-9][0-9]?)\\b`"},{"metadata":{},"cell_type":"markdown","source":"# References (A lot)\n\n- http://www.rexegg.com/ <<<< THE ULTIMATE WEBSITE>>>>\n- https://github.com/aloisdg/awesome-regex\n- http://linuxreviews.org/beginner/tao_of_regular_expressions/tao_of_regular_expressions.en.print.pdf\n- https://developers.google.com/edu/python/regular-expressions\n- https://www.youtube.com/watch?v=EkluES9Rvak\n\nPS I wrote (above things) for the [Amazing Course](https://mlcourse.ai/) which is maintained by [@kashnitsky](https://www.kaggle.com/kashnitsky),[@artgor](https://www.kaggle.com/artgor) , [@datamove](https://www.kaggle.com/metadist) etc. and many many many other amazing peoples from ODS who framed the amazing course;\n\nYou can find the source nbs [here](https://github.com/Yorko/mlcourse.ai/tree/master/jupyter_english/tutorials) and a lot of very cool stuffs there as well !!!"},{"metadata":{},"cell_type":"markdown","source":"<center> ** <font color ='red'> So Now we are good to go!! Armed with regex, let's see what they can do.. </font> ** </center>"},{"metadata":{},"cell_type":"markdown","source":" #### StarDate Captain's Logs xD\n - latest - I tried updating the arch.\n - v14 - I tried some text cleaning via spelling correction and addded a new re pattern to fix texts. Looking fo an automated way to clean text."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv',usecols=['comment_text', 'target'])\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# From Quora kaggle Comp's (latest one)\nimport re\n# remove space\nspaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\ndef remove_space(text):\n    \"\"\"\n    remove extra spaces and ending space if any\n    \"\"\"\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text\n\n# replace strange punctuations and raplace diacritics\nfrom unicodedata import category, name, normalize\n\ndef remove_diacritics(s):\n    return ''.join(c for c in normalize('NFKD', s.replace('√∏', 'o').replace('√ò', 'O').replace('‚Åª', '-').replace('‚Çã', '-'))\n                  if category(c) != 'Mn')\n\nspecial_punc_mappings = {\"‚Äî\": \"-\", \"‚Äì\": \"-\", \"_\": \"-\", '‚Äù': '\"', \"‚Ä≥\": '\"', '‚Äú': '\"', '‚Ä¢': '.', '‚àí': '-',\n                         \"‚Äô\": \"'\", \"‚Äò\": \"'\", \"¬¥\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ÿå':'','‚Äû':'',\n                         '‚Ä¶': ' ... ', '\\ufeff': ''}\ndef clean_special_punctuations(text):\n    for punc in special_punc_mappings:\n        if punc in text:\n            text = text.replace(punc, special_punc_mappings[punc])\n    text = remove_diacritics(text)\n    return text\n\n# clean numbers\ndef clean_number(text):\n    if bool(re.search(r'\\d', text)):\n        text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text) # digits followed by a single alphabet...\n        text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text) #1st, 2nd, 3rd, 4th...\n        text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    return text\n\nimport string\nregular_punct = list(string.punctuation)\nextra_punct = [\n    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '‚Ä¢',  '~', '@', '¬£',\n    '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',\n    '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù',\n    '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ',\n    '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº',\n    '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤',\n    '√®', '¬∏', '¬æ', '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª',\n    'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò',\n    '¬π', '‚â§', '‚Ä°', '‚àö', '¬´', '¬ª', '¬¥', '¬∫', '¬æ', '¬°', '¬ß', '¬£', '‚Ç§',\n    ':)', ': )', ':-)', '(:', '( :', '(-:', ':\\')',\n    ':D', ': D', ':-D', 'xD', 'x-D', 'XD', 'X-D',\n    '<3', ':*',\n    ';-)', ';)', ';-D', ';D', '(;',  '(-;',\n    ':-(', ': (', ':(', '\\'):', ')-:',\n    '-- :','(', ':\\'(', ':\"(\\'',]\n\ndef handle_emojis(text): #Speed can be improved via a simple if check :)\n    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n    text = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', text)\n    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n    text = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', text)\n    # Love -- <3, :*\n    text = re.sub(r'(<3|:\\*)', ' EMO_POS ', text)\n    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n    text = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', text)\n    # Sad -- :-(, : (, :(, ):, )-:\n    text = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', text)\n    # Cry -- :,(, :'(, :\"(\n    text = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', text)\n    return text\n\ndef stop(text):\n    \n    from nltk.corpus import stopwords\n    \n    text = \" \".join([w.lower() for w in text.split()])\n    stop_words = stopwords.words('english')\n    \n    words = [w for w in text.split() if not w in stop_words]\n    return \" \".join(words)\n\nall_punct = list(set(regular_punct + extra_punct))\n# do not spacing - and .\nall_punct.remove('-')\nall_punct.remove('.')\n\n# clean repeated letters\ndef clean_repeat_words(text):\n    \n    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n    text = re.sub(r\"(L|l)(L|l)(L|l)+y\", \"lly\", text)\n    text = re.sub(r\"(A|a)(A|a)(A|a)+\", \"a\", text)\n    text = re.sub(r\"(C|c)(C|c)(C|c)+\", \"cc\", text)\n    text = re.sub(r\"(D|d)(D|d)(D|d)+\", \"dd\", text)\n    text = re.sub(r\"(E|e)(E|e)(E|e)+\", \"ee\", text)\n    text = re.sub(r\"(F|f)(F|f)(F|f)+\", \"ff\", text)\n    text = re.sub(r\"(G|g)(G|g)(G|g)+\", \"gg\", text)\n    text = re.sub(r\"(I|i)(I|i)(I|i)+\", \"i\", text)\n    text = re.sub(r\"(K|k)(K|k)(K|k)+\", \"k\", text)\n    text = re.sub(r\"(L|l)(L|l)(L|l)+\", \"ll\", text)\n    text = re.sub(r\"(M|m)(M|m)(M|m)+\", \"mm\", text)\n    text = re.sub(r\"(N|n)(N|n)(N|n)+\", \"nn\", text)\n    text = re.sub(r\"(O|o)(O|o)(O|o)+\", \"oo\", text)\n    text = re.sub(r\"(P|p)(P|p)(P|p)+\", \"pp\", text)\n    text = re.sub(r\"(Q|q)(Q|q)+\", \"q\", text)\n    text = re.sub(r\"(R|r)(R|r)(R|r)+\", \"rr\", text)\n    text = re.sub(r\"(S|s)(S|s)(S|s)+\", \"ss\", text)\n    text = re.sub(r\"(T|t)(T|t)(T|t)+\", \"tt\", text)\n    text = re.sub(r\"(V|v)(V|v)+\", \"v\", text)\n    text = re.sub(r\"(Y|y)(Y|y)(Y|y)+\", \"y\", text)\n    text = re.sub(r\"plzz+\", \"please\", text)\n    text = re.sub(r\"(Z|z)(Z|z)(Z|z)+\", \"zz\", text)\n    text = re.sub(r\"(-+|\\.+)\", \" \", text) #new haha #this adds a space token so we need to remove xtra spaces\n    return text\n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text\n\ndef preprocess(text):\n    \"\"\"\n    preprocess text main steps\n    \"\"\"\n    text = remove_space(text)\n    text = clean_special_punctuations(text)\n    text = handle_emojis(text)\n    text = clean_number(text)\n    text = spacing_punctuation(text)\n    text = clean_repeat_words(text)\n    text = remove_space(text)\n    #text = stop(text)# if changing this, then chnage the dims \n    #(not to be done yet as its effecting the embeddings..,we might be\n    #loosing words)...\n    return text\n\nmispell_dict = {'üòâ':'wink','üòÇ':'joy','üòÄ':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndef correct_contraction(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndef text_clean_wrapper(df):\n    \n    df[\"comment_text\"] = df[\"comment_text\"].astype('str').transform(preprocess)\n    df['comment_text'] = df['comment_text'].transform(lambda x: correct_spelling(x, mispell_dict))\n    df['comment_text'] = df['comment_text'].transform(lambda x: correct_contraction(x, contraction_mapping))\n    \n    return df\n\n#fast!\ntrain = df_parallelize_run(train, text_clean_wrapper)\ntest  = df_parallelize_run(test, text_clean_wrapper)\n\nimport gc\ngc.enable()\ndel mispell_dict, all_punct, special_punc_mappings, regular_punct, extra_punct\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n\nimport os\nimport time\nimport gensim\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nlc = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\nsb = SnowballStemmer(\"english\")\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D, BatchNormalization\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import EarlyStopping\n\nimport sys\nfrom os.path import dirname\nfrom keras.engine import InputSpec, Layer\n\nimport spacy\n\n\n# https://www.kaggle.com/takuok/bidirectional-lstm-and-attention-lb-0-043/\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()\nprint(\"Loading data ...\")\n\nfull_text = pd.concat([train['comment_text'].astype(str), test['comment_text'].astype(str)])\ny = train['target']\nnum_train_data = y.shape[0]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\nfrom tqdm import tqdm_notebook as tqdm\nstart_time = time.time()\nprint(\"Spacy NLP ...\")\nnlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\nnlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)\nword_dict = {}\nword_index = 1\nlemma_dict = {}\ndocs = nlp.pipe(full_text, n_threads = 4)\nword_sequences = []\nfor doc in tqdm(docs):\n    word_seq = []\n    for token in doc:\n        if (token.text not in word_dict) and (token.pos_ is not \"PUNCT\"):\n            word_dict[token.text] = word_index\n            word_index += 1\n            lemma_dict[token.text] = token.lemma_\n        if token.pos_ is not \"PUNCT\":\n            word_seq.append(word_dict[token.text])\n    word_sequences.append(word_seq)\ndel docs\nimport gc\ngc.collect()\ntrain_word_sequences = word_sequences[:num_train_data]\ntest_word_sequences = word_sequences[num_train_data:]\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 128\nX_train = pad_sequences(train_word_sequences, maxlen = max_len)\nX_test  = pad_sequences(test_word_sequences, maxlen = max_len)\ndel train_word_sequences, test_word_sequences\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 300\n#max_features = 377645 #NB this will change if you change any pre-processing (working to auto-mating this, kinda NEW to NLP:))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#quora comp\ndef load_glove(word_dict, lemma_dict):\n    from gensim.models import KeyedVectors\n    EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_dict.keys())\n    #embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    embed_size = 300\n    nb_words = len(word_dict)+1\n    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\n    print(unknown_vector[:5])\n    for key in tqdm(word_dict):\n        word = key\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.lower()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.upper()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = key.capitalize()\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = ps.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lc.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = sb.stem(key)\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        word = lemma_dict[key]\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[word_dict[key]] = embedding_vector\n            continue\n        embedding_matrix[word_dict[key]] = unknown_vector                    \n    return embedding_matrix, nb_words\n\nprint(\"Loading embedding matrix ...\")\nembedding_matrix, nb_words =  load_glove(word_dict, lemma_dict)\nmax_features = nb_words\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n#..........","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.where(train['target'] >= 0.5, True, False) * 1 #As per comp's DESC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del nb_words, lemma_dict, word_dict, word_index, train, test\ngc.collect()\nembedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\n\ndef build_model(lr=0.0, lr_d=0.0, spatial_dr=0.0,  dense_units=128, dr=0.1):\n    \n    from keras.layers import LSTM, Bidirectional, Dropout\n    \n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape=(max_len,))\n\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(spatial_dr)(x)\n    \n    x1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n    \n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    \n    conc = Concatenate()([max_pool1, max_pool2])\n    predictions = Dense(1, activation='sigmoid')(conc)\n    \n    model = Model(inputs=inp, outputs=predictions)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n\n    history = model.fit(X_train, y, batch_size = 512, epochs = 15, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    \n    #model = load_model(file_path)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(lr = 1e-3, lr_d = 0.001, spatial_dr = 0.23, dr=0.2)\ndel X_train, embedding_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test, batch_size = 512, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(pred);\nplt.title('Distribution of predictions');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['prediction'] = pred\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(X_test, batch_size = 512, verbose = 1)\n\nfrom keras.utils import plot_model\nplot_model(model, to_file='model.png')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}