{"cells":[{"metadata":{},"cell_type":"markdown","source":"Many thanks to the following kagglers and their great kernels:\n\n@Andrew Lukyanenko, https://www.kaggle.com/artgor/toxicity-eda-model-interpretation-and-more\n\n@Eike Dehling: https://www.kaggle.com/eikedehling/feature-engineering\n\n@Jagan: https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n\n@Theo Viel: https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n\n@Aditya Soni: https://www.kaggle.com/adityaecdrid/public-version-text-cleaning-vocab-65\n\n@Guillaume Martin: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\n@Shujian Liu: https://www.kaggle.com/shujian/test-the-difficulty-of-this-classification-tasks\n\nThanks @kotakota1110 for his suggestion in Time Series part.\n"},{"metadata":{},"cell_type":"markdown","source":"**Content**\n\n* Text Features heatmap\n\n* Weighted toxic comments & different identities\n\n* Identities & Comment Labels.\n\n* Time Series Toxicity with Race, Religion, Sexual orientation, Gender and Disability (updated April 18, weighted the data again)\n\n* What happened in Jan 2017? (updated April 14)\n\n* Which Time are People More Toxic? (updated April 16)\n\n* Words Frequented and Toxic_Mask\n\n* Text Processing (updated April 21)\n\n* Memory Reducing (updated April 22)\n\n* Test the Difficulty of the Task (updated April 24)\n\n-----To be added"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train.isnull().sum(), test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"FE: Some features might have relations with Toxicity, like capitals letters in the text, punctuations in the texts. Add the new features into the training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['total_length'] = train['comment_text'].apply(len)\ntrain['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\ntrain['caps_vs_length'] = train.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)\ntrain['num_exclamation_marks'] = train['comment_text'].apply(lambda comment: comment.count('!'))\ntrain['num_question_marks'] = train['comment_text'].apply(lambda comment: comment.count('?'))\ntrain['num_punctuation'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\ntrain['num_symbols'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\ntrain['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))\ntrain['num_unique_words'] = train['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\ntrain['words_vs_unique'] = train['num_unique_words'] / train['num_words']\ntrain['num_smilies'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ('total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks','num_question_marks', 'num_punctuation', 'num_words', 'num_unique_words','words_vs_unique', 'num_smilies', 'num_symbols')\ncolumns = ('target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit','identity_annotator_count', 'toxicity_annotator_count')\nrows = [{c:train[f].corr(train[c]) for c in columns} for f in features]\ntrain_correlations = pd.DataFrame(rows, index=features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the correlations between new features and targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_correlations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlations between new features and targets in heatmap:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.set(font_scale=1)\nax = sns.heatmap(train_correlations, vmin=-0.1, vmax=0.1, center=0.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Percent of toxic comments related to different identities, using target and popolation amount of each identity as weights:"},{"metadata":{"trusted":true},"cell_type":"code","source":"demographics = train.loc[:, ['target']+list(train)[slice(8,32)]].dropna()\nweighted_toxic = demographics.iloc[:, 1:].multiply(demographics.iloc[:, 0], axis=\"index\").sum()/demographics.iloc[:, 1:][demographics.iloc[:, 1:]>0].count()\nweighted_toxic = weighted_toxic.sort_values(ascending=False)\nplt.figure(figsize=(30,20))\nsns.set(font_scale=3)\nax = sns.barplot(x = weighted_toxic.values, y = weighted_toxic.index, alpha=0.8)\nplt.ylabel('Demographics')\nplt.xlabel('Weighted Toxic')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Meanwhile, we can check the correlations between identities and the comment labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"identities = tuple(train.iloc[:, 8:32])\nrows = [{c:train[f].corr(train[c]) for c in columns} for f in identities]\npoptoxicity_correlations = pd.DataFrame(rows, index=identities)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"poptoxicity_correlations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.set(font_scale=1)\nax = sns.heatmap(poptoxicity_correlations, vmin=-0.1, vmax=0.1, center=0.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also check the Time Series for Toxicity with different identities:\n\n(Thanks again for @kotakota1110's suggestion. Now we are using \"target\" and \"identity data amount\" to weight the data twice, which make more sense.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"withdate = train.loc[:, ['created_date', 'target']+list(train)[slice(8,32)]].dropna()\nraceweighted = withdate.iloc[:, 2:]/withdate.iloc[:, 2:].sum()\nrace_target_weighted = raceweighted.multiply(withdate.iloc[:, 1], axis=\"index\")\nrace_target_weighted['created_date'] = pd.to_datetime(withdate['created_date']).values.astype('datetime64[M]')\nweighted_demo = race_target_weighted.groupby(['created_date']).sum().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly\nimport plotly.plotly as py\nimport cufflinks as cf\nimport plotly.graph_objs as go\nplotly.tools.set_credentials_file(username='13217', api_key='FG6itEaCMouvPJVR7DlI')\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_demo[['white', 'asian', 'black', 'jewish', 'latino', 'other_race_or_ethnicity']].iplot(title = 'Time Series Toxicity & Race', filename='Time Series Toxicity & Race' )\n\n# Click on the legend to change display. Double click for single identity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_demo[['atheist', 'buddhist', 'christian', 'hindu', 'muslim', 'other_religion']].iplot(title = 'Time Series Toxicity & Religion', filename='Time Series Toxicity & Religion')\n\n# Click on the legend to change display. Double click for single identity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_demo[['heterosexual', 'homosexual_gay_or_lesbian', 'bisexual', 'other_sexual_orientation']].iplot(title = 'Time Series Toxicity & Sexual Orientation', filename='Time Series Toxicity & Sexual Orientation')\n\n# Click on the legend to change display. Double click for single identity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_demo[['male', 'female', 'transgender', 'other_gender']].iplot(title = 'Time Series Toxicity & Gender', filename='Time Series Toxicity & Gender')\n\n# Click on the legend to change display. Double click for single identity.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weighted_demo[['physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']].iplot(title = 'Time Series Toxicity & Disability', filename='Time Series Toxicity & Disability')\n\n# Click on the legend to change display. Double click for single identity.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When plotting these charts, I found that most data have a peak around Jan 2017. A bit curious. Let's check what's different between Jan 2017 and other time."},{"metadata":{"trusted":true},"cell_type":"code","source":"alldate_toxicity = train[train['target'] >= 0.5].loc[:, ['created_date', 'target', 'comment_text']].dropna()\nalldate_toxicity['created_date'] = pd.to_datetime(alldate_toxicity['created_date']).values.astype('datetime64[M]')\njan_2017_toxicity = alldate_toxicity[alldate_toxicity['created_date'] == '2017-01-01']\n\nfrom nltk.corpus import stopwords\ndef check_frequency(data = alldate_toxicity['comment_text'], n = 20):\n    stop = stopwords.words('english')\n    data  = data.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    data = data.str.replace('[^\\w\\s]','')\n    data = data.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    freq = pd.Series(' '.join(data).split()).value_counts()[:n]\n    return freq\n\ntop_10_toxicity_othertime = check_frequency(data = alldate_toxicity[alldate_toxicity['created_date'] != '2017-01-01']['comment_text'], n = 10)\ntop_10_toxicity_jan_2017 = check_frequency(data = jan_2017_toxicity['comment_text'], n = 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which toxicity related word appears Top 10 in jan_2017, but not in other time Top 10?"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_toxicity_jan_2017.index.difference(top_10_toxicity_othertime.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of them... All the same... Then let's theck their frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_toxicity_othertime = top_10_toxicity_othertime/alldate_toxicity[alldate_toxicity['created_date'] != '2017-01-01']['comment_text'].str.split().str.len().sum()\npercent_toxicity_jan_2017 = top_10_toxicity_jan_2017/jan_2017_toxicity['comment_text'].str.split().str.len().sum()\ntop_toxicity = pd.concat([percent_toxicity_jan_2017, percent_toxicity_othertime], axis=1, sort=False)\ntop_toxicity.columns = ['Jan_2017', 'Other_Time']\ntop_toxicity['Difference'] = top_toxicity['Jan_2017'] - top_toxicity['Other_Time']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_toxicity.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objs as go\ntrace1 = go.Bar(\n    x=top_toxicity.index,\n    y=top_toxicity['Jan_2017'],\n    name='Jan_2017'\n)\ntrace2 = go.Bar(\n    x=top_toxicity.index,\n    y=top_toxicity['Other_Time'],\n    name='Other_Time'\n)\n\ndata = [trace2, trace1]\nlayout = go.Layout(\n    barmode='group'\n)\nlayout = go.Layout(yaxis=dict(tickformat=\".2%\"))\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, title = 'Top Toxicity Comarision', filename='top_toxicity_comarision')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After checking the whole time series, I'm also curious about, Which Time are People More Toxic?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['datetime64'] = pd.to_datetime(train['created_date']).values.astype('datetime64[h]')\ntrain['hour'] = train['datetime64'].dt.hour\nall_comments_by_hour = train['target'].groupby(train['hour']).sum().sort_index()/train['target'].groupby(train['hour']).sum().sum()\ntoxic_comments_by_hour = train[train['target'] >= 0.5]['target'].groupby(train['hour']).sum().sort_index()/train[train['target'] >= 0.5]['target'].groupby(train['hour']).sum().sum()\ncomments_hour_check = pd.concat([all_comments_by_hour, toxic_comments_by_hour], axis=1, sort=False)\ncomments_hour_check.columns = ['all_comments', 'toxic_comments']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['Midnight', 'Morning', 'Noon', 'Evening', 'Midnight']\ntickvals = ['0', '6', '12', '18', comments_hour_check.index.max()]\n\ntrace1 = go.Scatter(\n    x=comments_hour_check.index,\n    y=comments_hour_check['all_comments'],\n    name = 'comment percent per H',\n    line = dict(\n        color = ('rgb(22, 96, 167)'),\n        width = 1)\n)\ntrace2 = go.Scatter(\n    x=comments_hour_check.index,\n    y=comments_hour_check['toxic_comments'],\n    name = 'toxic comment percent per H',\n    line = dict(\n        color = ('rgb(205, 12, 24)'),\n        width = 1,)\n)\n\ntrace3 = go.Bar(\n    x=comments_hour_check.index,\n    y=comments_hour_check['toxic_comments']-comments_hour_check['all_comments'],\n    name = 'More Toxic Comment Ratio'\n)\n\ndata = [trace1, trace2, trace3]\n\nlayout = go.Layout(yaxis=dict(tickformat=\".2%\"),\n                   title = 'Which Time are People More Toxic',\n                   xaxis=go.layout.XAxis(\n                       ticktext=labels, \n                       tickvals=tickvals\n                   ),\n                  )\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='Which Time are People More Toxic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moreover, we can do something fun, digging into the text with WordCloud. Let's check the Words frequented in Toxic Comments."},{"metadata":{"trusted":true},"cell_type":"code","source":"def toxicwordcloud(subset=train[train.target>0.7], title = \"Words Frequented\", picture = \"../input/imagesforkernal/anger.png\"):\n    stopword=set(STOPWORDS)\n    toxic_mask=np.array(Image.open(picture))\n    toxic_mask=toxic_mask[:,:,1]\n    text=subset.comment_text.values\n    wc= WordCloud(background_color=\"black\",max_words=4000,mask=toxic_mask,stopwords=stopword)\n    wc.generate(\" \".join(text))\n    plt.figure(figsize=(8,8))\n    plt.xticks([])\n    plt.yticks([])\n    plt.axis('off')\n    plt.title(title, fontsize=20)\n    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxicwordcloud(picture = \"../input/imagesforkernal/toxic-sign.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxicwordcloud(subset = train[(train['female'] >0)&(train['target']>0.8)],title = \"Words Frequented - Female Related\", picture = \"../input/imagesforkernal/anger.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toxicwordcloud(subset = train[(train['insult'] >0.8)&(train['target']>0.8)],title = \"Words Frequented - Insult Related\", picture = \"../input/imagesforkernal/biohazard-symbol.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some simple clasic text precessing and generating the new dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator \nimport re\nimport gensim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Due to the memory limit, here we only are using glove, while if you have a better machine, you can also load crawl and other embeddings\n\ndf = pd.concat([train.iloc[:, [0,2]] ,test.iloc[:, :2]])\nglove = '../input/glove840b300dtxt/glove.840B.300d.txt'\n# crawl =  '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n    \ndef load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    if file == '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec':\n        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(crawl)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n    return embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Extracting GloVe embedding\")\nembed_glove = load_embed(glove)\n# print(\"Extracting Crawl embedding\")\n# embed_crawl = load_embed(crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\nvocab = build_vocab(df['comment_text'])\n\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_crawl = check_coverage(vocab, embed_crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['lowered_comment'] = df['comment_text'].apply(lambda x: x.lower())\nvocab_low = build_vocab(df['lowered_comment'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab_low, embed_glove)\n# print(\"Crawl : \")\n# oov_crawl = check_coverage(vocab_low, embed_crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")\n    \nprint(\"Glove : \")\nadd_lower(embed_glove, vocab)\n# oov_glove = check_coverage(vocab_low, embed_glove)\n# print(\"Crawl : \")\n# add_lower(embed_crawl, vocab)\n# oov_crawl = check_coverage(vocab_low, embed_crawl)\n\n# Check Result\noov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following contraction_mapping is borrowed from @Aditya Soni. Credit goes to https://www.kaggle.com/adityaecdrid/public-version-text-cleaning-vocab-65"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\n    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n    'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',\"aren't\": 'are not',\n    'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n    'can;t': 'cannot','can;t;ve': 'cannot have',\n    'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',\n    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n    'couldn;t;ve': 'could not have','couldnÂ´t': 'could not',\n    'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not','couldnâ€™tâ€™ve': 'could not have','couldÂ´ve': 'could have',\n    'couldâ€™ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',\n    'didnâ€™t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',\n    'doesnâ€™t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',\n    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n    'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not','hadnâ€™tâ€™ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not','hasnâ€™t': 'has not',\n    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not','havenâ€™t': 'have not',\"he'd\": 'he would',\n    \"he'd've\": 'he would have',\"he'll\": 'he will',\n    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have','heÂ´ll': 'he will',\n    'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n    'how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will','howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will',\n    'howâ€™s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n    'isn,t': 'is not','isn;t': 'is not','isnÂ´t': 'is not','isnâ€™t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is',\n    'itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',\n    'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',\n    'iâ€™ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us',\n    'letâ€™s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n    'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightnÂ´t': 'might not',\n    'mightnâ€™t': 'might not','mightÂ´ve': 'might have','mightâ€™ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not','mustÂ´ve': 'must have',\n    'mustâ€™ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','neednÂ´t': 'need not','neednâ€™t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n    'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n    'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not','shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',\n    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will',\n    'sheÂ´s': 'she is','sheâ€™d': 'she would','sheâ€™ll': 'she will','sheâ€™s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have',\n    'shouldâ€™ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n    'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',\"there'd\": 'there had',\n    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n    'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',\n    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n    'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are','theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will',\n    'theyâ€™re': 'they are','theyâ€™ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not',\n    'wasnâ€™t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not','weÂ´d': 'we would','weÂ´ll': 'we will',\n    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would','weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n    'what;s': 'what is','what;ve': 'what have','whatÂ´ll': 'what will',\n    'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will','whatâ€™re': 'what are','whatâ€™s': 'what is',\n    'whatâ€™ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n    'where;s': 'where is','whereÂ´d': 'where did','whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is',\n    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n    'whoÂ´ll': 'who will','whoÂ´s': 'who is','whoâ€™ll': 'who will','whoâ€™s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n    'wonÂ´t': 'will not','wonâ€™t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldnÂ´t': 'would not',\n    'wouldnâ€™t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n    'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would','youâ€™ll': 'you will','youâ€™re': 'you are',\n    'Â´cause': 'because','â€™cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n    \"havn't\": 'have not',\"hereâ€™s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"youâ€™ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"â€˜I\":'I',\n    'á´€É´á´…':'and','á´›Êœá´‡':'the','Êœá´á´á´‡':'home','á´œá´˜':'up','Ê™Ê':'by','á´€á´›':'at','â€¦and':'and','civilbeat':'civil beat',\\\n    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','á´„Êœá´‡á´„á´‹':'check','Ò“á´Ê€':'for','á´›ÊœÉªs':'this','á´„á´á´á´˜á´œá´›á´‡Ê€':'computer',\\\n    'á´á´É´á´›Êœ':'month','á´¡á´Ê€á´‹ÉªÉ´É¢':'working','á´Šá´Ê™':'job','Ò“Ê€á´á´':'from','Sá´›á´€Ê€á´›':'start','gubmit':'submit','COâ‚‚':'carbon dioxide','Ò“ÉªÊ€sá´›':'first',\\\n    'á´‡É´á´…':'end','á´„á´€É´':'can','Êœá´€á´ á´‡':'have','á´›á´':'to','ÊŸÉªÉ´á´‹':'link','á´Ò“':'of','Êœá´á´œÊ€ÊŸÊ':'hourly','á´¡á´‡á´‡á´‹':'week','á´‡É´á´…':'end','á´‡xá´›Ê€á´€':'extra',\\\n    'GÊ€á´‡á´€á´›':'great','sá´›á´œá´…á´‡É´á´›s':'student','sá´›á´€Ê':'stay','á´á´á´s':'mother','á´Ê€':'or','á´€É´Êá´É´á´‡':'anyone','É´á´‡á´‡á´…ÉªÉ´É¢':'needing','á´€É´':'an','ÉªÉ´á´„á´á´á´‡':'income',\\\n    'Ê€á´‡ÊŸÉªá´€Ê™ÊŸá´‡':'reliable','Ò“ÉªÊ€sá´›':'first','Êá´á´œÊ€':'your','sÉªÉ¢É´ÉªÉ´É¢':'signing','Ê™á´á´›á´›á´á´':'bottom','Ò“á´ÊŸÊŸá´á´¡ÉªÉ´É¢':'following','Má´€á´‹á´‡':'make',\\\n    'á´„á´É´É´á´‡á´„á´›Éªá´É´':'connection','ÉªÉ´á´›á´‡Ê€É´á´‡á´›':'internet','financialpost':'financial post', 'Êœaá´ á´‡':' have ', 'á´„aÉ´':' can ', 'Maá´‹á´‡':' make ', 'Ê€á´‡ÊŸÉªaÊ™ÊŸá´‡':' reliable ', 'É´á´‡á´‡á´…':' need ',\n    'á´É´ÊŸÊ':' only ', 'á´‡xá´›Ê€a':' extra ', 'aÉ´':' an ', 'aÉ´Êá´É´á´‡':' anyone ', 'sá´›aÊ':' stay ', 'Sá´›aÊ€á´›':' start', 'SHOPO':'shop',\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known\n\nprint(\"- Known Contractions -\")\nprint(\"   Glove :\")\nprint(known_contractions(embed_glove))\n# print(\"   Crawl :\")\n# print(known_contractions(embed_crawl))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndf['treated_comment'] = df['lowered_comment'].apply(lambda x: clean_contractions(x, contraction_mapping))\n\nvocab = build_vocab(df['treated_comment'])\n\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_paragram = check_coverage(vocab, embed_crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"â€œâ€â€™' + 'âˆÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'\n\ndef unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown\n\nprint(\"Glove :\")\nprint(unknown_punct(embed_glove, punct))\n# print(\"Crawl :\")\n# print(unknown_punct(embed_crawl, punct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = {\"â€˜\": \"'\", \"â‚¹\": \"e\", \"Â´\": \"'\", \"Â°\": \"\", \"â‚¬\": \"e\", \"â„¢\": \"tm\", \"âˆš\": \" sqrt \", \"Ã—\": \"x\", \"Â²\": \"2\", \"â€”\": \"-\", \"â€“\": \"-\", \"â€™\": \"'\", \"_\": \"-\", \"`\": \"'\", 'â€œ': '\"', 'â€': '\"', 'â€œ': '\"', \"Â£\": \"e\", 'âˆ': 'infinity', 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha', 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta', 'âˆ…': '', 'Â³': '3', 'Ï€': 'pi', }\n\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    specials = {'\\u200b': ' ', 'â€¦': ' ... ', '\\ufeff': '', 'à¤•à¤°à¤¨à¤¾': '', 'à¤¹à¥ˆ': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    return text\n\ndf['treated_comment'] = df['treated_comment'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\nvocab = build_vocab(df['treated_comment'])\n\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_paragram = check_coverage(vocab, embed_crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_glove[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ğŸ˜‰':'wink','ğŸ˜‚':'joy','ğŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndf['treated_comment'] = df['treated_comment'].apply(lambda x: correct_spelling(x, mispell_dict))\n\nvocab = build_vocab(df['treated_comment'])\n\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\n# print(\"Crawl : \")\n# oov_paragram = check_coverage(vocab, embed_crawl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = df['treated_comment'][:1804874]\ntest['comment_text'] = df['treated_comment'][1804874:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-' * 80)\nprint('train')\ntrain = reduce_mem_usage(train)\n\nprint('-' * 80)\nprint('test')\ntest = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test the Difficulty of this Classification Tasks.\n(Borrowed from\n\nKernel: https://www.kaggle.com/shujian/test-the-difficulty-of-this-classification-tasks\n\nPaper: https://arxiv.org/abs/1811.01910\n\nCode: https://github.com/Wluper/edm)"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install edm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.sample(frac=0.003)\nsents = df[\"comment_text\"].values\nlabels = df[\"target\"].values\nfrom edm import report\nprint(report.get_difficulty_report(sents, labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_pickle(\"train.pkl\")\ntest.to_pickle(\"test.pkl\")\ntrain.to_csv('train_cleaned.csv', index=None)\ntest.to_csv('test_cleaned.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To be continued...**"},{"metadata":{},"cell_type":"markdown","source":"![](https://habrastorage.org/webt/mh/4h/nr/mh4hnrif7tzbmycmjpiduozssa4.png)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}