{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel response to competitiion [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification). It is my first hand on **Text Classification**. I will use Tensorflow and Keras for this project."},{"metadata":{},"cell_type":"markdown","source":"I have built few great kernel for beginner with deep learning that you can check it out: \n* https://www.kaggle.com/uysimty/keras-cnn-dog-or-cat-classification\n* https://www.kaggle.com/uysimty/learn-titanic-survival\n* https://www.kaggle.com/uysimty/keras-predict-google-stock-using-lstm"},{"metadata":{},"cell_type":"markdown","source":"Because of our analyst part taking too long. So it is disable me from submit to compotition. So I have other kernel that take only processing part which return 92.3 acurracy score.\n\n* https://www.kaggle.com/uysimty/simple-toxicity-classification-submission"},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nfrom gensim.models import KeyedVectors\nimport operator\nfrom tqdm import tqdm\ntqdm.pandas()\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=25\nbatch_size=128\nmax_words=100000\nmax_seq_size=256","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\")) \nprint(os.listdir(\"../input/jigsaw-unintended-bias-in-toxicity-classification\"))\nprint(os.listdir(\"../input/quoratextemb\"))\nprint(os.listdir(\"../input/quoratextemb/embeddings\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df  = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub_df   = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Take care of dataframe memory "},{"metadata":{"trusted":true},"cell_type":"code","source":"mem_usg = train_df.memory_usage().sum() / 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll select only the columns that we need to reduce some memory usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[[\"target\", \"comment_text\"]]\nmem_usg = train_df.memory_usage().sum() / 1024**2 \nprint(\"Memory usage is: \", mem_usg, \" MB\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See? we have more free memory"},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\n"},{"metadata":{},"cell_type":"markdown","source":"### Load Embedding"},{"metadata":{},"cell_type":"markdown","source":"To increase our covarage we try to combine few embedding together in order for us to more vocab covrage. \nIn term of memory optimize, we will convert our vector to ```float16``` to reduce some memory usage. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use for combine the vector files that have given \ndef combine_embedding(vec_files):\n    \n    # convert victor to float16 to make it use less memory\n    def get_coefs(word, *arr): \n        return word, np.asarray(arr, dtype='float16')\n\n    # make our embed smaller by get_coefs\n    def optimize_embedding(embedding): \n        optimized_embedding = {}\n        for word in embedding.vocab:\n            optimized_embedding[word] = np.asarray(embedding[word], dtype='float16')\n        return optimized_embedding\n\n    \n    # load embed vector from file\n    def load_embed(file):\n        print(\"Loading {}\".format(file))\n\n        if file == '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n            return dict(get_coefs(*o.strip().split(\" \")) for o in open(file) if len(o) > 100)\n        \n        elif file == '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec':\n            return optimize_embedding(KeyedVectors.load_word2vec_format(file))\n\n        else:\n            return dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n        \n    combined_embedding = {}\n    for file in vec_files:\n        combined_embedding.update(load_embed(file))\n    return combined_embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vec_files = [\n    \"../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\",\n    \"../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt\",\n    \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_index = combine_embedding(vec_files)\ncovered_vocabs = set(list(embedding_index.keys()))\nembedding_index.clear()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count occurance of words "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use for count how many time word accure in our data\ndef count_words_from(series):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    sentences =  series.str.split()\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Coverage"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use for check coverage of our vocab\ndef check_coverage_for(vocab):\n    a = 0\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        if word in covered_vocabs:\n            a += 1\n            k += vocab[word]\n        else:\n            oov[word] = vocab[word]\n            i += vocab[word]\n\n    print('Found embeddings for {:.2%} of vocab'.format(a / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    \n    return sorted_x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this methods help to clean up some memory while improve the coverage. As it will release the varaible the send of method\ndef check_current_coverage(num=50):\n    vocab = count_words_from( train_df[\"comment_text\"] )\n    coverage = check_coverage_for(vocab)\n    return coverage[:num]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's check the first coverage "},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see only 18% of our vocab has been covered. But 92% of our text has already cover. From the top first uncovered we see we have some problem with contractions. Let's get rid of it. "},{"metadata":{},"cell_type":"markdown","source":"### Clean contractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\n    \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n    \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n    \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \n    \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n    \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n    \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n    \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \n    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n    \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \n    \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n    \"Trump's\": \"trump is\", \"Obama's\": \"obama is\", \"Canada's\": \"canada is\", \"today's\": \"today is\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"known_contractions = []\nfor contract in contraction_mapping:\n    if contract in covered_vocabs:\n        known_contractions.append(contract)\nprint(known_contractions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our embedding have known some contractions. So we will remove that known contractions from our dictionary and let's our embedding handle it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for cont in known_contractions:\n    contraction_mapping.pop(cont)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text):\n    specials = [\"â€™\", \"â€˜\", \"Â´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    words = [contraction_mapping[word] if word in contraction_mapping else word for word in text.split(\" \")]\n    return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_contractions(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let check coverage again"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our covarage have increase a little. But at least we are good for next step. From the top uncovered we see that we have problem with some specials charator. "},{"metadata":{},"cell_type":"markdown","source":"#### Clean special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"â€œâ€â€™' + 'âˆžÎ¸Ã·Î±â€¢Ã âˆ’Î²âˆ…Â³Ï€â€˜â‚¹Â´Â°Â£â‚¬\\Ã—â„¢âˆšÂ²â€”â€“&'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specail_signs = { \"â€¦\": \"...\", \"â‚‚\": \"2\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unknown_puncts = []\nfor p in punct:\n    if p not in covered_vocabs:\n        unknown_puncts.append(p)\nprint(' '.join(unknown_puncts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All contraction are known"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_chars(text):\n    for s in specail_signs: \n        text = text.replace(s, specail_signs[s])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_special_chars(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check coverage again"},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the excited part that we have our coverage increase much. Our vocab coverage has increase to 65% and Most of our text (99.6%) has been covered"},{"metadata":{},"cell_type":"markdown","source":"# Clean Special Caps"},{"metadata":{},"cell_type":"markdown","source":"We see some like like ```Êœá´á´á´‡```, ```á´œá´˜```, ```á´„Êœá´‡á´„á´‹``` etc ... We need to convert it to up, home, check ...."},{"metadata":{"trusted":true},"cell_type":"code","source":"special_caps_mapping = { \n    \"á´€\": \"a\", \"Ê™\": \"b\", \"á´„\": \"c\", \"á´…\": \"d\", \"á´‡\": \"e\", \"Ò“\": \"f\", \"É¢\": \"g\", \"Êœ\": \"h\", \"Éª\": \"i\", \"á´Š\": \"j\", \"á´‹\": \"k\", \"ÊŸ\": \"l\", \"á´\": \"m\",\n    \"É´\": \"n\", \"á´\": \"o\", \"á´˜\": \"p\", \"Ç«\": \"q\", \"Ê€\": \"r\", \"s\": \"s\", \"á´›\": \"t\", \"á´œ\": \"u\", \"á´ \": \"v\", \"á´¡\": \"w\", \"x\": \"x\", \"Ê\": \"y\", \"á´¢\": \"z\",\n    \"ð˜Š\": \"C\", \"ð˜¦\": \"e\", \"ð˜³\": \"r\", \"ð˜¢\": \"a\", \"ð˜µ\": \"t\", \"ð˜°\": \"o\", \"ð˜¤\": \"c\", \"ð˜º\": \"y\", \"ð˜´\": \"s\", \"ð˜ª\": \"i\", \"ð˜§\": \"f\", \"ð˜®\": \"m\", \"ð˜£\": \"b\",\n    \"Ð¼\": \"m\", \"Ï…\": \"u\", \"Ñ‚\": \"t\", \"Ñ•\": \"s\", \"ð™€\": \"E\", \"ð’›\": \"z\", \"ð‘²\": \"K\", \"ð‘³\": \"L\", \"ð‘¾\": \"W\", \"ð’‹\": \"j\", \"ðŸ’\": \"4\",\n    \"ð™’\": \"W\", \"ð˜¾\": \"C\", \"ð˜½\": \"B\", \"ð‘±\": \"J\", \"ð‘¹\": \"R\", \"ð‘«\": \"D\", \"ð‘µ\": \"N\", \"ð‘ª\": \"C\", \"ð‘¯\": \"H\", \"ð’’\": \"q\", \"ð‘®\": \"G\", \"ð—•\": \"B\", \"ð—´\": \"g\", \n    \"ðŸ\": \"2\", \"ð—¸\": \"k\", \"ð—Ÿ\": \"L\", \"ð— \": \"M\", \"ð—·\": \"j\", \"ðŽ\": \"O\", \"ð\": \"N\", \"ðŠ\": \"K\", \"ð‘­\": \"F\", \"Ð•\": \"E\"\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_small_caps(text):\n    for char in special_caps_mapping:\n        text = text.replace(char, special_caps_mapping[char])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: clean_small_caps(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Emoji "},{"metadata":{"trusted":true},"cell_type":"code","source":"emojis = \"ðŸ•ðŸµðŸ˜‘ðŸ˜¢ðŸ¶ï¸ðŸ˜œðŸ˜ŽðŸ‘ŠðŸ˜ðŸ˜ðŸ’–ðŸ’µðŸ‘ŽðŸ˜€ðŸ˜‚ðŸ”¥ðŸ˜„ðŸ»ðŸ’¥ðŸ˜‹ðŸ‘ðŸ˜±ðŸšŒá´µÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•ðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜ðŸ’©ðŸ’¯â›½ðŸš„ðŸ˜–ðŸ¼ðŸš²ðŸ˜ŸðŸ˜ˆðŸ’ªðŸ™ðŸŽ¯ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡ðŸ‘ŒðŸ™„ðŸ˜ ðŸ˜‰ðŸ˜¤â›ºðŸ™‚ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜žðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ªðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨ðŸ™ƒðŸ’•ðŸ’—ðŸ’šðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸â¤µðŸ†ðŸŽƒðŸ˜©ðŸ‘®ðŸ’™ðŸ¾ðŸ•ðŸ˜†ðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’ŽðŸ–ðŸ™…â›²ðŸ°ðŸ¤ðŸ‘†ðŸ™ŒðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰ðŸš¬ðŸ¤“ðŸ˜µðŸ˜’ÍðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“ðŸ˜£âºðŸ˜ŒðŸ¤‘ðŸŒðŸ˜¯ðŸ˜²ðŸ’žðŸš“ðŸ””ðŸ“šðŸ€ðŸ‘ðŸ’¤ðŸ‡ðŸ¡â”â‰ðŸ‘ ã€‹ðŸ‡¹ðŸ‡¼ðŸŒ¸ðŸŒžðŸŽ²ðŸ˜›ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽðŸ—‘ðŸ’ƒðŸ“£ðŸ‘¿à¼¼ã¤à¼½ðŸ˜°ðŸ¤£ðŸðŸŽ…ðŸºðŸŽµðŸŒŽÍŸðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§ðŸš€ðŸ¤´ðŸ˜ðŸ’¨ðŸˆðŸ˜ºðŸŒâá»‡ðŸ”ðŸ®ðŸðŸ†ðŸ‘ðŸŒ®ðŸŒ¯ðŸ¤¦ðŸ€ðŸ˜«ðŸ¤¤ðŸŽ¼ðŸ•ºðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜ðŸ¤ ðŸ‘©ðŸ–’ðŸšªðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ˜·ðŸ‡¨ðŸ‡¦ðŸŒðŸ“ºðŸ‹ðŸ’˜ðŸ’“ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ðŸ‘ºðŸ·ðŸš¶ðŸ¤˜Í¦ðŸ’¸ðŸ‘‚ðŸ‘ƒðŸŽ«ðŸš¢ðŸš‚ðŸƒðŸ‘½ðŸ˜™ðŸŽ¾ðŸ‘¹âŽŒðŸ’â›¸ðŸ„ðŸ€ðŸš‘ðŸ¤·ðŸ¤™ðŸ’ðŸˆï·»ðŸ¦„ðŸš—ðŸ³ðŸ‘‡â›·ðŸ‘‹ðŸ¦ŠðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹ðŸ·ðŸ¦†â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽðŸðŸ¦ðŸ™‹ðŸ˜¶ðŸ”«ðŸ‘ðŸ’²ðŸ—¯ðŸ‘‘ðŸš¿ðŸ’¡ðŸ˜¦ðŸðŸ‡°ðŸ‡µðŸ‘¾ðŸ„ðŸŽˆðŸ”¨ðŸŽðŸ¤žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ðŸ­ðŸ‘£ðŸ‰ðŸ’­ðŸŽ¥ðŸ´ðŸ‘¨ðŸ¤³ðŸ¦ðŸ©ðŸ˜—ðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²ðŸ’ðŸ‘â°ðŸ’ŠðŸŒ¤ðŸŠðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ðŸ’¢ðŸ’’ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŸ¤–ðŸŽŽðŸ˜¼ðŸ•·ðŸ‘¼ðŸ“‰ðŸŸðŸ¦ðŸŒˆðŸ”­ã€ŠðŸŠðŸðŸ¦ðŸ¡ðŸ’³á¼±ðŸ™‡ðŸ¥œðŸ”¼\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emojis(text):\n    for emoji in emojis:\n        text = text.replace(emoji, '')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"comment_text\"] = train_df[\"comment_text\"].progress_apply(lambda text: remove_emojis(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_current_coverage()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean Unprocessable Symbols"},{"metadata":{},"cell_type":"markdown","source":"Do some clean up for memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"del covered_vocabs\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's clean up of testing set"},{"metadata":{},"cell_type":"markdown","source":"* Convert to lower case\n* Clean contractions\n* Clean special charactor\n* Convert small caps"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_up_text_with_all_process(text):\n    text = text.lower()\n    text = clean_contractions(text)\n    text = clean_special_chars(text)\n    text = clean_small_caps(text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"comment_text\"] = test_df[\"comment_text\"].progress_apply(lambda text: clean_up_text_with_all_process(text))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transform Text "},{"metadata":{"trusted":true},"cell_type":"code","source":"tranformer = Tokenizer(lower = True, filters='', num_words=max_words)\ntranformer.fit_on_texts( list(train_df[\"comment_text\"].values) + list(test_df[\"comment_text\"].values) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"transformed_x = tranformer.texts_to_sequences(train_df[\"comment_text\"].values)\ntransformed_x = pad_sequences(transformed_x, maxlen = max_seq_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform predicting set"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_predict = tranformer.texts_to_sequences(test_df[\"comment_text\"])\nx_predict = pad_sequences(x_predict, maxlen = max_seq_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Martix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_matrix(word_index, total_vocab, embedding_size):\n    embedding_index = combine_embedding(vec_files)\n    matrix = np.zeros((total_vocab, embedding_size))\n    for word, index in tqdm(word_index.items()):\n        try:\n            matrix[index] = embedding_index[word]\n        except KeyError:\n            pass\n    return matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tranformer.word_index\ntotal_vocab = len(word_index) + 1\nembedding_size = 300\nembedding_matrix = build_embedding_matrix(tranformer.word_index, total_vocab, embedding_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean up some memory"},{"metadata":{},"cell_type":"markdown","source":"Let free up some memory before to other hard job. I'll clean ```vocab``` and ```coverage``` up in order for us to have enough memory to continue"},{"metadata":{"trusted":true},"cell_type":"code","source":"del tranformer\ndel word_index\ndel embedding_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Select features and Target"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = (train_df['target'].values > 0.5).astype(int)\nx_train, x_test, y_train, y_test = train_test_split(transformed_x, y, random_state=10, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let clean memory again,  I'll clean ```word_index``` and ```embedding_index``` up in order for us to have enough memory for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df\ndel y\ndel test_df\ndel transformed_x\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.nn import relu, sigmoid\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.keras.layers import CuDNNGRU, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_input = Input(shape=(max_seq_size,), dtype='int32')\nembedding_layer = Embedding(total_vocab,\n                            embedding_size,\n                            weights=[embedding_matrix],\n                            input_length=max_seq_size,\n                            trainable=False)\n\nx_layer = embedding_layer(sequence_input)\nx_layer = SpatialDropout1D(0.2)(x_layer)\nx_layer = Bidirectional(CuDNNGRU(64, return_sequences=True))(x_layer)   \nx_layer = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_layer)\n\navg_pool1 = GlobalAveragePooling1D()(x_layer)\nmax_pool1 = GlobalMaxPooling1D()(x_layer)     \n\nx_layer = concatenate([avg_pool1, max_pool1])\n\npreds = Dense(1, activation=sigmoid)(x_layer)\n\nmodel = Model(sequence_input, preds)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compile Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [\n    EarlyStopping(patience=10, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Virtualize Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\nax1.plot(history.history['loss'], color='b', label=\"Training loss\")\nax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\nax1.set_xticks(np.arange(1, epochs, 1))\nplt.legend(loc='best', shadow=True)\n\nax2.plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax2.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nax2.set_xticks(np.arange(1, epochs, 1))\nplt.legend(loc='best', shadow=True)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluate Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test, y_test, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(x_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df[\"prediction\"] = y_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Credit**\n* https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2\n* https://www.kaggle.com/thousandvoices/simple-lstm"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}