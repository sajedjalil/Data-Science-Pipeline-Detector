{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import GeneralScheduler, TrainingPhase\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\n\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nimport torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors\n\nfrom sklearn.metrics import roc_auc_score\n\nimport copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\ndef build_matrix(word_index, path, dim=300):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, dim))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef train_model(learn,output_dim,lr=0.001,\n                batch_size=512, n_epochs=5):\n    \n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    models_array = []\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        learn.save('model_{}'.format(epoch))\n        models_array.append(copy.deepcopy(learn.model))\n        \n    return models_array\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n    \nclass SoftmaxPooling(nn.Module):\n    def __init__(self, dim=1):\n        super(self.__class__, self).__init__()\n        self.dim = dim\n        \n    def forward(self, x):\n        return (x * x.softmax(dim=self.dim)).sum(dim=self.dim)\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.linear_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        )\n        \n        self.linear_aux_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        )\n        \n        self.softmaxpool = SoftmaxPooling()\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        # softmax pooling\n        soft_pool = self.softmaxpool(h_lstm2)\n        \n        h_conc = torch.cat((max_pool, avg_pool, soft_pool), 1)\n        \n        hidden = h_conc\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef ensemble_predictions(predictions, weights, type_=\"linear\"):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=0)\n    elif type_ == \"harmonic\":\n        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n        return 1 / res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=0\n        )\n        res = np.exp(numerator / sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n        return res / (len(res) + 1)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nSEED = 1234\nBATCH_SIZE = 512\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim'\nPARAGRAM_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/paragram_300_sl999.gensim'\nNUM_MODELS = 1\nLSTM_UNITS = 256\nDENSE_HIDDEN_UNITS = 1536\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = pd.read_csv('../input/jigsawbiaspreprocessed/x_train.csv', header=None)[0].astype('str')\ny_aux_train = np.load('../input/jigsawbiaspreprocessed/y_aux_train.npy')\ny_train = np.load('../input/jigsawbiaspreprocessed/y_train.npy')\n\nloss_weight = 3.209226860170181\n\nmax_features = 400000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\nannot_idx = train[train['identity_annotator_count'] > 0].sample(n=48660, random_state=13).index\nnot_annot_idx = train[train['identity_annotator_count'] == 0].sample(n=48660, random_state=13).index\nx_val_idx = list(set(annot_idx).union(set(not_annot_idx)))\nx_train_idx = list(set(x_train.index) - set(x_val_idx))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = x_train.loc[x_train_idx]\nY_train = y_train[x_train_idx]\nY_train[:, 0] = Y_train[:, 0] * 0.9 + 0.05\nY_aux_train = y_aux_train[x_train_idx] * 0.9 + 0.05\nX_val = x_train.loc[x_val_idx]\nY_val = y_train[x_val_idx]\nY_aux_val = y_aux_train[x_val_idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(list(X_train))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nparagram_matrix, unknown_words_paragram = build_matrix(tokenizer.word_index, PARAGRAM_EMBEDDING_PATH)\nprint('n unknown words (paragram): ', len(unknown_words_paragram))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, paragram_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\ndel crawl_matrix\ndel paragram_matrix\ngc.collect()\n\ny_train_torch = torch.tensor(np.hstack([Y_train, Y_aux_train]), dtype=torch.float32)\nX_train = tokenizer.texts_to_sequences(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/45735070/keras-text-preprocessing-saving-tokenizer-object-to-file-for-scoring\nimport pickle\n\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lengths = torch.from_numpy(np.array([len(x) for x in X_train]))\n \nmaxlen = 300\nX_train_padded = torch.from_numpy(sequence.pad_sequences(X_train, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\ntrain_dataset = data.TensorDataset(X_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\n\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1 + model_idx)\n    model = NeuralNet(embedding_matrix, Y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    models_array = train_model(learn,output_dim=7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val = tokenizer.texts_to_sequences(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen = 300\nval_lengths = torch.from_numpy(np.array([len(x) for x in X_val]))\n\nX_val_padded = torch.from_numpy(sequence.pad_sequences(X_val, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\ntest_dataset = data.TensorDataset(X_val_padded, val_lengths)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=test_loader, valid_dl=test_loader, collate_fn=test_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model, output_dim=7):\n    test_preds = np.zeros((len(X_val), output_dim)) \n    for i, x_batch in enumerate(test_loader):\n        X = x_batch[0].cuda()\n        y_pred = sigmoid(model(X).detach().cpu().numpy())\n        test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df = train.loc[x_val_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in tqdm(range(5)):\n    model = models_array[j]\n    y_pred = test_model(model)\n    val_df['model_0_{}'.format(j)] = y_pred[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\nval_df = convert_dataframe_to_bool(val_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df['target']\n    predicted_labels = df[model_name]\n    return roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cols = [col for col in val_df.columns if col.startswith('model_')]\nfor m_col in model_cols:\n    bias_metrics_df = compute_bias_metrics_for_model(val_df, identity_columns, m_col, 'target')\n    print(m_col)\n    print(get_final_metric(bias_metrics_df, calculate_overall_auc(val_df, m_col)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}