{"cells":[{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/coolcoder22/simple-logisticregression"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom scipy import sparse\nimport re\nimport string\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annot_idx = train_df[train_df['identity_annotator_count'] > 0].sample(n=48660, random_state=13).index\nnot_annot_idx = train_df[train_df['identity_annotator_count'] == 0].sample(n=48660, random_state=13).index\nx_val_idx = list(set(annot_idx).union(set(not_annot_idx)))\n\nX_val = train_df.loc[x_val_idx]\nX_train = train_df.loc[list(set(train_df.index) - set(x_val_idx))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s):\n    return text.sub(r' \\1 ', s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = train_df.shape[0]\nword_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n               min_df=5, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectorizer.fit(X_train['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tfidf = word_vectorizer.transform(X_train['comment_text'])\nval_tfidf = word_vectorizer.transform(X_val['comment_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_tfidf.shape)\nprint(val_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nwith open('word_vectorizer.pickle', 'wb') as handle:\n    pickle.dump(word_vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        y = y\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n        \n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.where(X_train['target'] >= 0.5, 1, 0)\ny_val = np.where(X_val['target'] >= 0.5, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NbSvm = NbSvmClassifier(C=1.5, dual=True, n_jobs=-1)\nNbSvm.fit(train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='lbfgs', random_state=13)\nlr.fit(train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('lr_model.pickle', 'wb') as handle:\n    pickle.dump(lr, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(train_tfidf, y_train)\nlgb_eval = lgb.Dataset(val_tfidf, y_val, reference=lgb_train)\n\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective':'binary',\n    'metric': {'auc'},\n    'nthread': -1,\n    'feature_fraction': 0.4,\n    'num_leaves': 50,\n    'verbose': 1,\n    'num_iterations': 500\n}\n\nprint('Start training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=200,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('gbm_model.pickle', 'wb') as handle:\n    pickle.dump(gbm, handle, protocol=pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VALIDATION PART**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val['model_nbsvm'] = NbSvm.predict_proba(val_tfidf)[:, 1]\nX_val['model_lr'] = lr.predict_proba(val_tfidf)[:, 1]\nX_val['model_gbm'] = gbm.predict(val_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\nval_df = convert_dataframe_to_bool(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df['target']\n    predicted_labels = df[model_name]\n    return roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cols = [col for col in val_df.columns if (col.startswith('model_'))]\nfor m_col in model_cols:\n    bias_metrics_df = compute_bias_metrics_for_model(val_df, identity_columns, m_col, 'target')\n    print(m_col)\n    print(get_final_metric(bias_metrics_df, calculate_overall_auc(val_df, m_col)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}