{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks for @christofhenkel @abhishek @iezepov for their great work:\n\nhttps://www.kaggle.com/christofhenkel/how-to-preprocessing-for-glove-part2-usage\nhttps://www.kaggle.com/abhishek/pytorch-bert-inference\nhttps://www.kaggle.com/iezepov/starter-gensim-word-embeddings"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n# Pytorchã®BERTï¼ˆgithubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚³ãƒ¼ãƒ‰ï¼‰ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# åˆ¥ã®å ´æ‰€ã‹ã‚‰importã—ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPytorchã®BERTå®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’è‡ªå‹•èª­ã¿è¾¼ã¿\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import *\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ç”¨ã«åˆ†å‰²\ndef convert_lines(example, max_seq_length,tokenizer):\n    # ä¸€å›ã®å­¦ç¿’ã§ä½¿ç”¨ã™ã‚‹æ–‡ç« ã®å˜èªæ•°\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        # BERTã‚»ãƒ«ã«å˜èªã‚’å…¥åŠ›ï¼Ÿ\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒé•·ã™ããŸã‚‰ã€é€”ä¸­ã§åˆ‡ã‚‹ï¼Ÿ\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n# embeddingãƒ¬ã‚¤ãƒ¤ã‚’ä½œæˆ\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\n# å˜èªã®åˆ†æ•£è¡¨ç¾ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’å–å¾—\ndef build_matrix(word_index, path):\n    # embeddingãƒ¬ã‚¤ãƒ¤ã‚’ä½œæˆï¼ˆãƒãƒƒãƒˆã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ç‰©ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼‰\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                # embeddingãƒ¬ã‚¤ãƒ¤ã§ãƒ‡ãƒ¼ã‚¿å†…ã®å˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    # embeddingãƒ¬ã‚¤ãƒ¤ã§ãƒ‡ãƒ¼ã‚¿å†…ã®å˜èªï¼ˆå°æ–‡å­—ï¼‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        # embeddingãƒ¬ã‚¤ãƒ¤ã§ãƒ‡ãƒ¼ã‚¿å†…ã®å˜èªï¼ˆå…ˆé ­ã®æ–‡å­—ãŒå¤§æ–‡å­—ã€äºŒç•ªç›®ä»¥é™ã®æ–‡å­—ãŒå°æ–‡å­—ï¼‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        # embeddingãƒ¬ã‚¤ãƒ¤ã«è¨˜éŒ²ã•ã‚Œã¦ã„ãªã‹ã£ãŸã‚‰unknown_wordã¨ã—ã¦å‡¦ç†\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef train_model(learn,test,output_dim,lr=0.001,\n                batch_size=512, n_epochs=3,\n                enable_checkpoint_ensemble=True):\n    \n    all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        test_preds = np.zeros((len(test), output_dim))    \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n\n        all_test_preds.append(test_preds)\n\n\n    if enable_checkpoint_ensemble:\n        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n    else:\n        test_preds = all_test_preds[-1]\n        \n    return test_preds\n\n# ç‰¹æ®Šæ–‡å­—ã®å–å¾—ã¨è¦ã‚‰ãªã„æ–‡å­—ã®å‰Šé™¤\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\n# Treebankã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒãƒˆ\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\n# ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\n# ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\n# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’minibatchæ¯ã«é…ã‚‹ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n# æ­£ç›´ã€è‰¯ãåˆ†ã‹ã‚‰ã‚“â€¦\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n# ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’ä½œæˆ\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        \n        h_conc = torch.cat((max_pool, avg_pool), 1)\n        h_conc_linear1  = F.relu(self.linear1(h_conc))\n        h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\n# ãƒ‡ãƒ¼ã‚¿å®¹é‡ã‚’æœ€å°åŒ–\ndef reduce_mem_usage(df):\n    # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿å®¹é‡ã‚’è¨˜éŒ²\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    # ãƒ‡ãƒ¼ã‚¿å®¹é‡ã‚’ã‚«ãƒ©ãƒ æ¯ã«åœ§ç¸®\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # ã‚«ãƒ©ãƒ ãŒobjectï¼ˆæ–‡å­—åˆ—å‹ï¼‰ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚’åœ§ç¸®\n        if col_type != object:\n            # ã‚«ãƒ©ãƒ ï¼ˆæ•°å€¤å‹ï¼‰ã®æœ€å°å€¤ã€æœ€å¤§å€¤ã‚’è¨˜éŒ²\n            c_min = df[col].min()\n            c_max = df[col].max()\n            # ã‚«ãƒ©ãƒ ã®å‹ãŒintå‹ã®ç‰©ã‚’åœ§ç¸®\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ndef ensemble_predictions(predictions, weights, type_=\"linear\"):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=0)\n    elif type_ == \"harmonic\":\n        res = np.average([1 / p for p in predictions], weights=weights, axis=0)\n        return 1 / res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=0\n        )\n        res = np.exp(numerator / sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n        return res / (len(res) + 1)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# è¨­å®šã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ã®å†…ã€æœ€åˆã«æ¤œå‡ºã•ã‚ŒãŸç‰©ã ã‘ã‚’è¡¨ç¤ºã€‚\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nMAX_SEQUENCE_LENGTH = 300\nSEED = 1234\nBATCH_SIZE = 512\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n# äºˆæ¸¬æ–¹æ³•ã‚’æ±ºå®šç†è«–ï¼ˆå­¦ç¿’ã•ã‚ŒãŸäº‹å¾Œç¢ºç‡P(A|B)ã‹ã‚‰äºˆæ¸¬ã‚’ç®—å‡ºï¼‰ã«æŒ‡å®š\ntorch.backends.cudnn.deterministic = True\n# Bertãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©\nbert_config = BertConfig('../input/bert-inference/bert/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n\n# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¨­å®š\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/crawl-300d-2M.gensim'\nGLOVE_EMBEDDING_PATH = '../input/gensim-embeddings-dataset/glove.840B.300d.gensim'\nNUM_MODELS = 2\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nMAX_LEN = 220\n# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¡¨ç¤ºè¨­å®š\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**BERT Part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\ntest_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\n# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’strå‹ã«å¤‰æ›\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \n# æ¬ æå€¤ã«å€¤ã‚’æŒ¿å…¥ï¼ˆDUMMY_VARUEãŒä½•ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‹åˆ†ã‹ã‚‰ãªã„ï¼‰\n# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¸€å›ã®å­¦ç¿’æ¯ã®ã‚µã‚¤ã‚ºï¼ˆæ–‡ç« ï¼‰ã«åˆ†å‰²ï¼Ÿ\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bertãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\nmodel = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(\"../input/bert-inference/bert/bert_pytorch.bin\"))\n# cudaãƒ¡ãƒ¢ãƒªã«ä½œæˆã—ãŸBertãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\nmodel.to(device)\n# ãã‚ç´°ã‹ã„æ•°å€¤æ•´å½¢ï¼ˆrequires_gradï¼‰ã‚’è¡Œã‚ãªã„ï¼Ÿ\n# æŒã£ã¦ããŸBertãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã—ãªã„ï¼ˆãã®ã¾ã¾ä½¿ã†ï¼‰\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’å…¥ã‚Œã‚‹å¤‰æ•°ã‚’ä½œæˆ\ntest_preds = np.zeros((len(X_test)))\n# Tensorå‹ã®ä¸€æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä»˜ä¸\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\n# ãƒŸãƒ‹ãƒãƒƒãƒæ¯ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å†é…å¸ƒ\ntest_loader = torch.utils.data.DataLoader(test, batch_size=512, shuffle=False)\n# å­¦ç¿’ä¸­ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®è¡¨ç¤º\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    # cudaãƒ¡ãƒ¢ãƒªä¸Šã®Bertãƒ¢ãƒ‡ãƒ«ã§ãƒŸãƒ‹ãƒãƒƒãƒæ¯ã«ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    # å­¦ç¿’çµæœã‚’ä¿å­˜\n    test_preds[i * 512:(i + 1) * 512] = pred[:, 0].detach().cpu().squeeze().numpy()\n\n# å­¦ç¿’çµæœã‚’æ´»æ€§åŒ–é–¢æ•°(ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰)ã«æ›ã‘ã‚‹\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_bert = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LSTM Part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å®¹é‡ã‚’æœ€å°åŒ–\ntrain_df = reduce_mem_usage(pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ï¼¼ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â‰ â„¢ËˆÊŠÉ’âˆÂ§{}Â·Ï„Î±â¤â˜ºÉ¡|Â¢â†’Ì¶`â¥â”â”£â”«â”—ï¼¯â–ºâ˜…Â©â€•Éªâœ”Â®\\x96\\x92â—Â£â™¥â¤Â´Â¹â˜•â‰ˆÃ·â™¡â—â•‘â–¬â€²É”Ëâ‚¬Û©Ûâ€ Î¼âœ’â¥â•â˜†ËŒâ—„Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½Êƒâœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼©ï¼´â˜»Â±â™ÂµÂºÂ¾âœ“â—¾ØŸï¼â¬…â„…Â»Ğ’Ğ°Ğ²â£â‹…Â¿Â¬â™«ï¼£ï¼­Î²â–ˆâ–“â–’â–‘â‡’â­â€ºÂ¡â‚‚â‚ƒâ§â–°â–”â—â–€â–‚â–ƒâ–„â–…â–†â–‡â†™Î³Ì„â€³â˜¹â¡Â«Ï†â…“â€âœ‹ï¼šÂ¥Ì²Ì…Ìâˆ™â€›â—‡âœâ–·â“â—Â¶ËšË™ï¼‰ÑĞ¸Ê¿âœ¨ã€‚É‘\\x80â—•ï¼ï¼…Â¯âˆ’ï¬‚ï¬â‚Â²ÊŒÂ¼â´â„â‚„âŒ â™­âœ˜â•ªâ–¶â˜­âœ­â™ªâ˜”â˜ â™‚â˜ƒâ˜âœˆâœŒâœ°â†â˜™â—‹â€£âš“å¹´âˆâ„’â–ªâ–™â˜â…›ï½ƒï½ï½“Ç€â„®Â¸ï½—â€šâˆ¼â€–â„³â„â†â˜¼â‹†Ê’âŠ‚ã€â…”Â¨Í¡à¹âš¾âš½Î¦Ã—Î¸ï¿¦ï¼Ÿï¼ˆâ„ƒâ©â˜®âš æœˆâœŠâŒâ­•â–¸â– â‡Œâ˜â˜‘âš¡â˜„Ç«â•­âˆ©â•®ï¼Œä¾‹ï¼Ê•ÉÌ£Î”â‚€âœâ”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ‰¥â˜’â†‘â˜É¹âœ…â˜›â™©â˜ï¼¡ï¼ªï¼¢â—”â—¡â†“â™€â¬†Ì±â„\\x91â €Ë¤â•šâ†ºâ‡¤âˆâœ¾â—¦â™¬Â³ã®ï½œï¼âˆµâˆ´âˆšÎ©Â¤â˜œâ–²â†³â–«â€¿â¬‡âœ§ï½ï½–ï½ï¼ï¼’ï¼ï¼˜ï¼‡â€°â‰¤âˆ•Ë†âšœâ˜'\nsymbols_to_delete = '\\nğŸ•\\rğŸµğŸ˜‘\\xa0\\ue014\\t\\uf818\\uf04a\\xadğŸ˜¢ğŸ¶ï¸\\uf0e0ğŸ˜œğŸ˜ğŸ‘Š\\u200b\\u200eğŸ˜Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ø£Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±ğŸ˜ğŸ’–ğŸ’µĞ•ğŸ‘ğŸ˜€ğŸ˜‚\\u202a\\u202cğŸ”¥ğŸ˜„ğŸ»ğŸ’¥á´ÊÊ€á´‡É´á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢ğŸ˜‹ğŸ‘×©×œ×•××‘×™ğŸ˜±â€¼\\x81ã‚¨ãƒ³ã‚¸æ•…éšœ\\u2009ğŸšŒá´µÍğŸŒŸğŸ˜ŠğŸ˜³ğŸ˜§ğŸ™€ğŸ˜ğŸ˜•\\u200fğŸ‘ğŸ˜®ğŸ˜ƒğŸ˜˜××¢×›×—ğŸ’©ğŸ’¯â›½ğŸš„ğŸ¼à®œğŸ˜–á´ ğŸš²â€ğŸ˜ŸğŸ˜ˆğŸ’ªğŸ™ğŸ¯ğŸŒ¹ğŸ˜‡ğŸ’”ğŸ˜¡\\x7fğŸ‘Œá¼á½¶Î®Î¹á½²Îºá¼€Î¯á¿ƒá¼´Î¾ğŸ™„ï¼¨ğŸ˜ \\ufeff\\u2028ğŸ˜‰ğŸ˜¤â›ºğŸ™‚\\u3000ØªØ­ÙƒØ³Ø©ğŸ‘®ğŸ’™ÙØ²Ø·ğŸ˜ğŸ¾ğŸ‰ğŸ˜\\u2008ğŸ¾ğŸ˜…ğŸ˜­ğŸ‘»ğŸ˜¥ğŸ˜”ğŸ˜“ğŸ½ğŸ†ğŸ»ğŸ½ğŸ¶ğŸŒºğŸ¤”ğŸ˜ª\\x08â€‘ğŸ°ğŸ‡ğŸ±ğŸ™†ğŸ˜¨ğŸ™ƒğŸ’•ğ˜Šğ˜¦ğ˜³ğ˜¢ğ˜µğ˜°ğ˜¤ğ˜ºğ˜´ğ˜ªğ˜§ğ˜®ğ˜£ğŸ’—ğŸ’šåœ°ç„è°·ÑƒĞ»ĞºĞ½ĞŸĞ¾ĞĞğŸ¾ğŸ•ğŸ˜†×”ğŸ”—ğŸš½æ­Œèˆä¼ğŸ™ˆğŸ˜´ğŸ¿ğŸ¤—ğŸ‡ºğŸ‡¸Ğ¼Ï…Ñ‚Ñ•â¤µğŸ†ğŸƒğŸ˜©\\u200ağŸŒ ğŸŸğŸ’«ğŸ’°ğŸ’ÑĞ¿Ñ€Ğ´\\x95ğŸ–ğŸ™…â›²ğŸ°ğŸ¤ğŸ‘†ğŸ™Œ\\u2002ğŸ’›ğŸ™ğŸ‘€ğŸ™ŠğŸ™‰\\u2004Ë¢áµ’Ê³Ê¸á´¼á´·á´ºÊ·áµ—Ê°áµ‰áµ˜\\x13ğŸš¬ğŸ¤“\\ue602ğŸ˜µÎ¬Î¿ÏŒÏ‚Î­á½¸×ª××“×£× ×¨×š×¦×˜ğŸ˜’ÍğŸ†•ğŸ‘…ğŸ‘¥ğŸ‘„ğŸ”„ğŸ”¤ğŸ‘‰ğŸ‘¤ğŸ‘¶ğŸ‘²ğŸ”›ğŸ“\\uf0b7\\uf04c\\x9f\\x10æˆéƒ½ğŸ˜£âºğŸ˜ŒğŸ¤‘ğŸŒğŸ˜¯ĞµÑ…ğŸ˜²á¼¸á¾¶á½ğŸ’ğŸš“ğŸ””ğŸ“šğŸ€ğŸ‘\\u202dğŸ’¤ğŸ‡\\ue613å°åœŸè±†ğŸ¡â”â‰\\u202fğŸ‘ ã€‹à¤•à¤°à¥à¤®à¤¾ğŸ‡¹ğŸ‡¼ğŸŒ¸è”¡è‹±æ–‡ğŸŒğŸ²ãƒ¬ã‚¯ã‚µã‚¹ğŸ˜›å¤–å›½äººå…³ç³»Ğ¡Ğ±ğŸ’‹ğŸ’€ğŸ„ğŸ’œğŸ¤¢ÙÙÑŒÑ‹Ğ³Ñä¸æ˜¯\\x9c\\x9dğŸ—‘\\u2005ğŸ’ƒğŸ“£ğŸ‘¿à¼¼ã¤à¼½ğŸ˜°á¸·Ğ—Ğ·â–±Ñ†ï¿¼ğŸ¤£å–æ¸©å“¥åè®®ä¼šä¸‹é™ä½ å¤±å»æ‰€æœ‰çš„é’±åŠ æ‹¿å¤§åç¨éª—å­ğŸãƒ„ğŸ…\\x85ğŸºØ¢Ø¥Ø´Ø¡ğŸµğŸŒÍŸá¼”æ²¹åˆ«å…‹ğŸ¤¡ğŸ¤¥ğŸ˜¬ğŸ¤§Ğ¹\\u2003ğŸš€ğŸ¤´Ê²ÑˆÑ‡Ğ˜ĞĞ Ğ¤Ğ”Ğ¯ĞœÑĞ¶ğŸ˜ğŸ–‘á½á½»Ïç‰¹æ®Šä½œæˆ¦ç¾¤Ñ‰ğŸ’¨åœ†æ˜å›­×§â„ğŸˆğŸ˜ºğŸŒâá»‡ğŸ”ğŸ®ğŸğŸ†ğŸ‘ğŸŒ®ğŸŒ¯ğŸ¤¦\\u200dğ“’ğ“²ğ“¿ğ“µì•ˆì˜í•˜ì„¸ìš”Ğ–Ñ™ĞšÑ›ğŸ€ğŸ˜«ğŸ¤¤á¿¦æˆ‘å‡ºç”Ÿåœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æğŸ¼ğŸ•ºğŸ¸ğŸ¥‚ğŸ—½ğŸ‡ğŸŠğŸ†˜ğŸ¤ ğŸ‘©ğŸ–’ğŸšªå¤©ä¸€å®¶âš²\\u2006âš­âš†â¬­â¬¯â–æ–°âœ€â•ŒğŸ‡«ğŸ‡·ğŸ‡©ğŸ‡ªğŸ‡®ğŸ‡¬ğŸ‡§ğŸ˜·ğŸ‡¨ğŸ‡¦Ğ¥Ğ¨ğŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹Êğ—ªğ—µğ—²ğ—»ğ˜†ğ—¼ğ˜‚ğ—¿ğ—®ğ—¹ğ—¶ğ˜‡ğ—¯ğ˜ğ—°ğ˜€ğ˜…ğ—½ğ˜„ğ—±ğŸ“ºÏ–\\u2000Ò¯Õ½á´¦á¥Ò»Íº\\u2007Õ°\\u2001É©ï½™ï½…àµ¦ï½ŒÆ½ï½ˆğ“ğ¡ğğ«ğ®ğğšğƒğœğ©ğ­ğ¢ğ¨ğ§Æ„á´¨×Ÿá‘¯à»Î¤á§à¯¦Ğ†á´‘Üğ¬ğ°ğ²ğ›ğ¦ğ¯ğ‘ğ™ğ£ğ‡ğ‚ğ˜ğŸÔœĞ¢á—à±¦ã€”á«ğ³ğ”ğ±ğŸ”ğŸ“ğ…ğŸ‹ï¬ƒğŸ’˜ğŸ’“Ñ‘ğ˜¥ğ˜¯ğ˜¶ğŸ’ğŸŒ‹ğŸŒ„ğŸŒ…ğ™¬ğ™–ğ™¨ğ™¤ğ™£ğ™¡ğ™®ğ™˜ğ™ ğ™šğ™™ğ™œğ™§ğ™¥ğ™©ğ™ªğ™—ğ™ğ™ğ™›ğŸ‘ºğŸ·â„‹ğ€ğ¥ğªğŸš¶ğ™¢á¼¹ğŸ¤˜Í¦ğŸ’¸Ø¬íŒ¨í‹°ï¼·ğ™‡áµ»ğŸ‘‚ğŸ‘ƒÉœğŸ«\\uf0a7Ğ‘Ğ£Ñ–ğŸš¢ğŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ğŸƒğ“¬ğ“»ğ“´ğ“®ğ“½ğ“¼â˜˜ï´¾Ì¯ï´¿â‚½\\ue807ğ‘»ğ’†ğ’ğ’•ğ’‰ğ’“ğ’–ğ’‚ğ’ğ’…ğ’”ğ’ğ’—ğ’ŠğŸ‘½ğŸ˜™\\u200cĞ›â€’ğŸ¾ğŸ‘¹âŒğŸ’â›¸å…¬å¯“å…»å® ç‰©å—ğŸ„ğŸ€ğŸš‘ğŸ¤·æ“ç¾ğ’‘ğ’šğ’ğ‘´ğŸ¤™ğŸ’æ¬¢è¿æ¥åˆ°é˜¿æ‹‰æ–¯×¡×¤ğ™«ğŸˆğ’Œğ™Šğ™­ğ™†ğ™‹ğ™ğ˜¼ğ™…ï·»ğŸ¦„å·¨æ”¶èµ¢å¾—ç™½é¬¼æ„¤æ€’è¦ä¹°é¢áº½ğŸš—ğŸ³ğŸğŸğŸ–ğŸ‘ğŸ•ğ’„ğŸ—ğ ğ™„ğ™ƒğŸ‘‡é”Ÿæ–¤æ‹·ğ—¢ğŸ³ğŸ±ğŸ¬â¦ãƒãƒ«ãƒãƒ‹ãƒãƒ­æ ªå¼ç¤¾â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ğ˜¿ğ™”â‚µğ’©â„¯ğ’¾ğ“ğ’¶ğ“‰ğ“‡ğ“Šğ“ƒğ“ˆğ“…â„´ğ’»ğ’½ğ“€ğ“Œğ’¸ğ“ğ™Î¶ğ™Ÿğ˜ƒğ—ºğŸ®ğŸ­ğŸ¯ğŸ²ğŸ‘‹ğŸ¦Šå¤šä¼¦ğŸ½ğŸ»ğŸ¹â›“ğŸ¹ğŸ·ğŸ¦†ä¸ºå’Œä¸­å‹è°Šç¥è´ºä¸å…¶æƒ³è±¡å¯¹æ³•å¦‚ç›´æ¥é—®ç”¨è‡ªå·±çŒœæœ¬ä¼ æ•™å£«æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©ç›¸ä¿¡è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›æ”¿æ²»é¢˜æ—¶å€™æˆ˜èƒœå› åœ£æŠŠå…¨å ‚ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ğŸ¸ğŸ¤•ğŸ¤’â›‘ğŸæ‰¹åˆ¤æ£€è®¨ğŸğŸ¦ğŸ™‹ğŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ì˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©ğŸ”«ğŸ‘å‡¸á½°ğŸ’²ğŸ—¯ğ™ˆá¼Œğ’‡ğ’ˆğ’˜ğ’ƒğ‘¬ğ‘¶ğ•¾ğ–™ğ–—ğ–†ğ–ğ–Œğ–ğ–•ğ–Šğ–”ğ–‘ğ–‰ğ–“ğ–ğ–œğ–ğ–šğ–‡ğ•¿ğ–˜ğ–„ğ–›ğ–’ğ–‹ğ–‚ğ•´ğ–Ÿğ–ˆğ•¸ğŸ‘‘ğŸš¿ğŸ’¡çŸ¥å½¼ç™¾\\uf005ğ™€ğ’›ğ‘²ğ‘³ğ‘¾ğ’‹ğŸ’ğŸ˜¦ğ™’ğ˜¾ğ˜½ğŸğ˜©ğ˜¨á½¼á¹‘ğ‘±ğ‘¹ğ‘«ğ‘µğ‘ªğŸ‡°ğŸ‡µğŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘á“€á£ğŸ„ğŸˆğŸ”¨ğŸğŸ¤ğŸ¸ğŸ’ŸğŸ°ğŸŒğŸ›³ç‚¹å‡»æŸ¥ç‰ˆğŸ­ğ‘¥ğ‘¦ğ‘§ï¼®ï¼§ğŸ‘£\\uf020ã£ğŸ‰Ñ„ğŸ’­ğŸ¥ÎğŸ´ğŸ‘¨ğŸ¤³ğŸ¦\\x0bğŸ©ğ‘¯ğ’’ğŸ˜—ğŸğŸ‚ğŸ‘³ğŸ—ğŸ•‰ğŸ²Ú†ÛŒğ‘®ğ—•ğ—´ğŸ’êœ¥â²£â²ğŸ‘â°é‰„ãƒªäº‹ä»¶Ñ—ğŸ’Šã€Œã€\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½ã‚·è™šå½å±ç†å±ˆĞ“ğ‘©ğ‘°ğ’€ğ‘ºğŸŒ¤ğ—³ğ—œğ—™ğ—¦ğ—§ğŸŠá½ºá¼ˆá¼¡Ï‡á¿–Î›â¤ğŸ‡³ğ’™ÏˆÕÕ´Õ¥Õ¼Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Õ±å†¬è‡³á½€ğ’ğŸ”¹ğŸ¤šğŸğ‘·ğŸ‚ğŸ’…ğ˜¬ğ˜±ğ˜¸ğ˜·ğ˜ğ˜­ğ˜“ğ˜–ğ˜¹ğ˜²ğ˜«Ú©Î’ÏğŸ’¢ÎœÎŸÎÎ‘Î•ğŸ‡±â™²ğˆâ†´ğŸ’’âŠ˜È»ğŸš´ğŸ–•ğŸ–¤ğŸ¥˜ğŸ“ğŸ‘ˆâ•ğŸš«ğŸ¨ğŸŒ‘ğŸ»ğğğŠğ‘­ğŸ¤–ğŸğŸ˜¼ğŸ•·ï½‡ï½’ï½ï½”ï½‰ï½„ï½•ï½†ï½‚ï½‹ğŸ°ğŸ‡´ğŸ‡­ğŸ‡»ğŸ‡²ğ—ğ—­ğ—˜ğ—¤ğŸ‘¼ğŸ“‰ğŸŸğŸ¦ğŸŒˆğŸ”­ã€ŠğŸŠğŸ\\uf10aáƒšÚ¡ğŸ¦\\U0001f92f\\U0001f92ağŸ¡ğŸ’³á¼±ğŸ™‡ğ—¸ğ—Ÿğ— ğ—·ğŸ¥œã•ã‚ˆã†ãªã‚‰ğŸ”¼'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ãƒ„ãƒªãƒ¼ãƒãƒ³ã‚¯ã‚’å®šç¾©\ntokenizer = TreebankWordTokenizer()\n\n# ãƒ‡ã‚³ãƒ¼ãƒ‰å‡ºæ¥ãªã„æ–‡å­—ã€è¦ã‚‰ãªã„æ–‡å­—ã®è¾æ›¸\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸­ã®è¦ã‚‰ãªã„ãƒ‡ãƒ¼ã‚¿ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’å‰Šé™¤ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ‰å‡ºæ¥ãªã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’æ–‡å­—ã«å¤‰æ›\nx_train = train_df['comment_text'].progress_apply(lambda x:preprocess(x))\n# äºˆæ¸¬ã—ãŸã„ç›®çš„å¤‰æ•°\ny_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸­ã®è¦ã‚‰ãªã„ãƒ‡ãƒ¼ã‚¿ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’å‰Šé™¤ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ‰å‡ºæ¥ãªã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆæ–‡å­—åˆ—ï¼‰ã‚’æ–‡å­—ã«å¤‰æ›\nx_test = test_df['comment_text'].progress_apply(lambda x:preprocess(x))\n\n# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸­ã®èª¬æ˜å¤‰æ•°\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Overall\nweights = np.ones((len(x_train),)) / 4\n# Subgroup(å„è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯ã«æœ‰åŠ¹ãªèª¬æ˜å¤‰æ•°ã®å€‹æ•°åˆ†ã€é‡ã‚Šã‚’åŠ ç®—)\nweights += (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n# Background Positive, Subgroup Negative(è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯ã«æœ‰åŠ¹ãªtargetã‚«ãƒ©ãƒ ã‚’1ã¨ã—ã€æœ‰åŠ¹ã§ãªã„èª¬æ˜å¤‰æ•°ã®å€‹æ•°ã¨è¶³ã—åˆã‚ã›ãŸæ•°ã‚’é‡ã‚Šã«åŠ ç®—)\nweights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n# Background Negative, Subgroup Positive(è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯ã«æœ‰åŠ¹ã§ãªã„targetã‚«ãƒ©ãƒ ã‚’1ã¨ã—ã€æœ‰åŠ¹ãªèª¬æ˜å¤‰æ•°ã®å€‹æ•°ã¨è¶³ã—åˆã‚ã›ãŸæ•°ã‚’é‡ã‚Šã«åŠ ç®—)\nweights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n   (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\nloss_weight = 1.0 / weights.mean()\n\n# å„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã€æœ‰åŠ¹ãªtargetã«ã¯1ã€ãã†ã§ãªã„targetã«ã¯0ã‚’å‰²ã‚ŠæŒ¯ã£ãŸã‚«ãƒ©ãƒ ã¨ã€weightsã‚’é€£çµã—ãŸé…åˆ—ã«å¤‰æ›ã€‚\ny_train = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\n\nmax_features = 410047","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# æ–‡å­—åˆ—ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹å¤‰æ›å™¨ã‚’å®šç¾©\ntokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆæ–‡ç« ï¼‰ã‚’å˜èªæ¯ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’ä½œæˆ\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\n# è¡¨ç¾æ–¹æ³•ãŒé•ã†è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()\n\n# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ç›®çš„å¤‰æ•°ã‚’é€£çµ\ny_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸Šã®å˜èªã‚’TreeBankä¸Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# å„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å˜èªæ•°ã‚’å–å¾—\nlengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n\n# å„ãƒ‡ãƒ¼ã‚¿ã®å˜èªæ•°ãŒ300ã«ãªã‚‹ã‚ˆã†ã«ã€paddingã‚’è¡Œã†\n# paddingã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ‡ãƒ¼ã‚¿ã®å‰å¾Œ('pre' or 'post')ã©ã¡ã‚‰ã‚’paddingã™ã‚‹ã‹ã‚’æ±ºã‚ã‚‰ã‚Œã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å‰'pre'\nmaxlen = 300\nx_train_padded = torch.from_numpy(sequence.pad_sequences(x_train, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_lengths = torch.from_numpy(np.array([len(x) for x in x_test]))\n\nx_test_padded = torch.from_numpy(sequence.pad_sequences(x_test, maxlen=maxlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\n# paddingã—ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨å„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’åŒã˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§å–ã‚Šå‡ºã›ã‚‹Tensorå‹äºŒæ¬¡å…ƒé…åˆ—ã«åŠ å·¥\ntest_dataset = data.TensorDataset(x_test_padded, test_lengths)\n# paddingã—ãŸè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€ãã®ãƒ‡ãƒ¼ã‚¿ã®paddingã™ã‚‹å‰ã®é•·ã•ã¨ç›®çš„å¤‰æ•°ã‚’Tensorå‹ä¸‰æ¬¡å…ƒé…åˆ—ã«åŠ å·¥\ntrain_dataset = data.TensorDataset(x_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\n\n# å„ç¨®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’minibatchæ¯ã«æ•´å½¢\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\n# minibatchæ¯ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨æ­£è§£ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_test_preds = []\n\nfor model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1 + model_idx)\n    model = NeuralNet(embedding_matrix, y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    test_preds = train_model(learn,test_dataset,output_dim=7)    \n    all_test_preds.append(test_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_lstm = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Blending part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\n    \"../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv\"\n)\n\nweights = [0.333, 0.667]\nsubmission[\"prediction\"] = ensemble_predictions(\n    [submission_bert.prediction.values, submission_lstm.prediction.values],\n    weights,\n    type_=\"rank\",\n)\nsubmission.to_csv(\"submission.csv\", index=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}