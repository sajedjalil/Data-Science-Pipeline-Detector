{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import sys\n# copy our file into the working directory\nsys.path.insert(0, \"../input/pytorch-pretrained-BERT/pytorch-pretrained-BERT/pytorch-pretrained-bert/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\n\nn_seeds = 1\nn_splits = 10\nn_epochs = 15\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nENSEMBLE_START_EPOCH = 3\n\nMAX_LEN = 220\n\nEMB_DROPOUT = 0.3\nMIDDLE_DROPOUT = 0.3\n\nBERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n# åˆ†æ•£è¡¨ç¾ãŒuncasedã ã£ãŸå ´åˆã¯ã€å˜èªã‚’å…¨ã¦å°æ–‡å­—ã§æ‰±ã†\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nWORK_DIR = \"../working/\"\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from contextlib import contextmanager\nimport os\n\n# æ¨™æº–å‡ºåŠ›ã‚’nullã«å¤‰æ›\n@contextmanager\ndef suppress_stdout():\n    # nullã®æ›¸ãè¾¼ã¿\n    with open(os.devnull, \"w\") as devnull:\n        # æ¨™æº–å‡ºåŠ›ã‚’nullã«å¤‰æ›\n        old_stdout = sys.stdout\n        sys.stdout = devnull\n        try:  \n            yield\n        finally:\n            sys.stdout = old_stdout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nraw_train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n#test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nAUX_COLUMNS = ['target', 'severe_toxicity','obscene','identity_attack','insult','threat']\ntrain_data = raw_train[:100000]\ntrain_text = train_data['comment_text']\ny_target = (train_data['target'] >= 0.5).astype('float32')\n#print(y_target)\ny_aux_target = (train_data[AUX_COLUMNS] >= 0.5).astype('float32')\n#x_test = test['comment_text'][:300]\n#print(y_aux_target)\n#print(len(y_target.values))\n#print(y_aux_target.shape)\n#y_target = torch.tensor(y_target.values.reshape((-1, 1)), dtype=torch.float32).cuda()\n#y_aux_target = torch.tensor(y_aux_target.values, dtype=torch.float32).cuda()\ny_label = pd.concat([y_target, y_aux_target], axis=1)\n#print(y_label)\n#print(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del raw_train, train_data, y_target, y_aux_target\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport emoji\nimport unicodedata\n\n# ç‰¹å®šæ–‡å­—ã®å¤‰æ›å™¨ã‚’ä½œæˆ\nCUSTOM_TABLE = str.maketrans(\n    {\n        \"\\xad\": None,\n        \"\\x7f\": None,\n        \"\\x10\": None,\n        \"\\x9d\": None,\n        \"\\xa0\": None,\n        \"\\ufeff\": None,\n        \"\\u200b\": None,\n        \"\\u200e\": None,\n        \"\\u202a\": None,\n        \"\\u202c\": None,\n        \"\\uf0d8\": None,\n        \"\\u2061\": None,\n        \"â€˜\": \"'\",\n        \"â€™\": \"'\",\n        \"`\": \"'\",\n        \"â€œ\": '\"',\n        \"â€\": '\"',\n        \"Â«\": '\"',\n        \"Â»\": '\"',\n        \"É¢\": \"G\",\n        \"Éª\": \"I\",\n        \"É´\": \"N\",\n        \"Ê€\": \"R\",\n        \"Ê\": \"Y\",\n        \"Ê™\": \"B\",\n        \"Êœ\": \"H\",\n        \"ÊŸ\": \"L\",\n        \"Ò“\": \"F\",\n        \"á´€\": \"A\",\n        \"á´„\": \"C\",\n        \"á´…\": \"D\",\n        \"á´‡\": \"E\",\n        \"á´Š\": \"J\",\n        \"á´‹\": \"K\",\n        \"á´\": \"M\",\n        \"Îœ\": \"M\",\n        \"á´\": \"O\",\n        \"á´˜\": \"P\",\n        \"á´›\": \"T\",\n        \"á´œ\": \"U\",\n        \"á´¡\": \"W\",\n        \"á´ \": \"V\",\n        \"Ä¸\": \"K\",\n        \"Ğ²\": \"B\",\n        \"Ğ¼\": \"M\",\n        \"Ğ½\": \"H\",\n        \"Ñ‚\": \"T\",\n        \"Ñ•\": \"S\",\n        \"â€”\": \"-\",\n        \"â€“\": \"-\",\n    }\n)\n\n# ä¸‹å“ãªå˜èªã®è¦åˆ¶å¾Œã¨è¦åˆ¶å‰ã®å˜èªã®è¾æ›¸ã‚’ä½œæˆ\nWORDS_REPLACER = [\n    (\"sh*t\", \"shit\"),\n    (\"s**t\", \"shit\"),\n    (\"f*ck\", \"fuck\"),\n    (\"fu*k\", \"fuck\"),\n    (\"f**k\", \"fuck\"),\n    (\"f*****g\", \"fucking\"),\n    (\"f***ing\", \"fucking\"),\n    (\"f**king\", \"fucking\"),\n    (\"p*ssy\", \"pussy\"),\n    (\"p***y\", \"pussy\"),\n    (\"pu**y\", \"pussy\"),\n    (\"p*ss\", \"piss\"),\n    (\"b*tch\", \"bitch\"),\n    (\"bit*h\", \"bitch\"),\n    (\"h*ll\", \"hell\"),\n    (\"h**l\", \"hell\"),\n    (\"cr*p\", \"crap\"),\n    (\"d*mn\", \"damn\"),\n    (\"stu*pid\", \"stupid\"),\n    (\"st*pid\", \"stupid\"),\n    (\"n*gger\", \"nigger\"),\n    (\"n***ga\", \"nigger\"),\n    (\"f*ggot\", \"faggot\"),\n    (\"scr*w\", \"screw\"),\n    (\"pr*ck\", \"prick\"),\n    (\"g*d\", \"god\"),\n    (\"s*x\", \"sex\"),\n    (\"a*s\", \"ass\"),\n    (\"a**hole\", \"asshole\"),\n    (\"a***ole\", \"asshole\"),\n    (\"a**\", \"ass\"),\n]\n\n# ä¸‹å“ãªå˜èªã®è¦åˆ¶éƒ¨åˆ†ã®ç‰¹æ®Šæ–‡å­—ã‚’ç„¡åŠ¹åŒ–ã—ã€å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„åˆ¤åˆ¥å™¨ã‚’ä½œæˆ\nREGEX_REPLACER = [\n    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n    for pat, repl in WORDS_REPLACER\n]\n\n# ç©ºç™½ã®åˆ¤åˆ¥å™¨ã‚’ä½œæˆ\nRE_SPACE = re.compile(r\"\\s\")\nRE_MULTI_SPACE = re.compile(r\"\\s+\")\n\n# Unicodeã‹ã‚‰ç•°ä½“å­—ã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆç›´å‰ã®æ–‡å­—ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹ã‚³ãƒ¼ãƒ‰ï¼‰ã‚’keyã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ\nNMS_TABLE = dict.fromkeys(\n    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n)\n\n# ç‰¹å®šã®Unicodeï¼ˆå­¦ç¿’ã§ä½¿ãˆãªã„æ–‡å­—ï¼Ÿï¼‰ã‚’åˆ¥ã®æ–‡å­—ã§ç½®æ›ã™ã‚‹è¾æ›¸ä½œæˆ\nHEBREW_TABLE = {i: \"×\" for i in range(0x0590, 0x05FF)}\nARABIC_TABLE = {i: \"Ø§\" for i in range(0x0600, 0x06FF)}\nCHINESE_TABLE = {i: \"æ˜¯\" for i in range(0x4E00, 0x9FFF)}\nKANJI_TABLE = {i: \"ãƒƒ\" for i in range(0x2E80, 0x2FD5)}\nHIRAGANA_TABLE = {i: \"ãƒƒ\" for i in range(0x3041, 0x3096)}\nKATAKANA_TABLE = {i: \"ãƒƒ\" for i in range(0x30A0, 0x30FF)}\n\nTABLE = dict()\nTABLE.update(CUSTOM_TABLE)\nTABLE.update(NMS_TABLE)\n# Non-english languages\nTABLE.update(CHINESE_TABLE)\nTABLE.update(HEBREW_TABLE)\nTABLE.update(ARABIC_TABLE)\nTABLE.update(HIRAGANA_TABLE)\nTABLE.update(KATAKANA_TABLE)\nTABLE.update(KANJI_TABLE)\n\n# çµµæ–‡å­—ã®åˆ¤åˆ¥å™¨ã‚’ä½œæˆ\nEMOJI_REGEXP = emoji.get_emoji_regexp()\n\n# çµµæ–‡å­—ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æ–‡ç« åŒ–\nUNICODE_EMOJI_MY = {\n    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n}\n\n# æ–‡ç« å†…ã®çµµæ–‡å­—ã‚’ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«å¤‰æ›\ndef my_demojize(string: str) -> str:\n    # subé–¢æ•°ã§matchã—ãŸå˜èªã‚’ã€ãã‚Œã‚’å«ã‚€çµµæ–‡å­—ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«ç½®æ›\n    def replace(match):\n        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n\n    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ–‡ç« ä¸­ã®çµµæ–‡å­—ã‚’ã€çµµæ–‡å­—ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«ç½®æ›ã—ã€ãã“ã‹ã‚‰ç•°ä½“å­—ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å‰Šé™¤\n    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n\n# æ–‡ç« ä¸­ã®ç‰¹å®šã®ç‰¹æ®Šå˜èªã‚’è§£æç”¨ã«å¤‰æ›\ndef normalize(text: str) -> str:\n    #text_len = len(text)\n    #print(text)\n    # æ–‡ç« å†…ã®çµµæ–‡å­—ã‚’ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«å¤‰æ›\n    text = my_demojize(text)\n\n    # ç©ºç™½ã‚’å…¨ã¦ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›\n    text = RE_SPACE.sub(\" \", text)\n    # æ–‡å­—è¡¨ç¾ï¼ˆUnicodeï¼‰ã®è¦æ ¼ã‚’çµ±ä¸€åŒ–\n    text = unicodedata.normalize(\"NFKD\", text)\n    # æ–‡ç« ä¸­ã®TABLE_keyã‚’valueã«å¤‰æ›\n    text = text.translate(TABLE)\n    # é€£ç¶šã—ãŸç©ºç™½ã‚’ä¸€ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›ã—ã€æ–‡ç« ã®ä¸¡ç«¯ã®ç©ºç™½ã‚’å‰Šé™¤\n    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n\n    # ä¸‹å“ãªå˜èªã®è¦åˆ¶ã•ã‚ŒãŸéƒ¨åˆ†ã‚’ä¿®å¾©\n    for pattern, repl in REGEX_REPLACER:\n        text = pattern.sub(repl, text)\n        \n    #if text_len != len(text):\n        #print(text + \"\\n\")\n    \n    return text\n\n# æ–‡ç« ã®ç‰¹æ®Šå˜èªã‚’ä¸€èˆ¬å˜èªã®å¤‰æ›\ntrain_text = train_text.apply(lambda x: normalize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del UNICODE_EMOJI_MY, EMOJI_REGEXP, TABLE, HEBREW_TABLE, ARABIC_TABLE, CHINESE_TABLE, KANJI_TABLE, HIRAGANA_TABLE, KATAKANA_TABLE, NMS_TABLE, RE_MULTI_SPACE, RE_SPACE, REGEX_REPLACER, WORDS_REPLACER, CUSTOM_TABLE\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if \"EMJ\" in crawl_emb_dict:\n#    print(\"EMJ\")\n#if \"×\" in crawl_emb_dict:\n#    print(\"HEB\")\n#if \"Ø§\" in crawl_emb_dict:\n#    print(\"ARA\")\n#if \"æ˜¯\" in crawl_emb_dict:\n#    print(\"CHI\")\n#if \"ãƒƒ\" in crawl_emb_dict:\n#    print(\"JAP\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', 'â€¢',  '~', '@', 'Â£', \n 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', \n 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', \n 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆ', \n 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš', '\\n', '\\r', \"'\", \"'\", 'Î¸', 'Ã·', 'Î±', 'Î²', 'âˆ…', 'Ï€', 'â‚¹', 'Â´']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            # ç‰¹å®šæ–‡å­—ã®ä¸¡å´ã«ç©ºç™½ä»˜ã‘ã‚‹\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\nimport re\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\n# å¥èª­ç‚¹ã®ä¸¡å´ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä»˜ä¸\ntrain_text = train_text.apply(lambda x: clean_text(x))\n\n# æ–‡ç« ã‹ã‚‰æ•°å­—å‰Šé™¤\ntrain_text = train_text.apply(lambda x: clean_numbers(x))\n\n# Bertã®èªå½™ã‚’èª­ã¿è¾¼ã¿\n#with open(BERT_MODEL_PATH + 'vocab.txt', 'r') as f:\n#    raw_dict = f.readlines()\n\n# ãƒ‡ãƒ¼ã‚¿å†…ã®æ”¹è¡Œã‚’å‰Šé™¤ã¨é‡è¤‡è¡Œã®å‰Šé™¤\n#crawl_emb_dict = set([t.replace('\\n', '') for t in raw_dict])\n#print(len(crawl_emb_dict))\n    \nimport joblib\n# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå˜èªã®åˆ†æ•£è¡¨ç¾ã®è¾æ›¸ï¼‰ã‚’ä¸¦åˆ—å‡¦ç†å½¢å¼ã§ä¿å­˜\nwith open('../input/reducing-oov-of-crawl300d2m-no-appos-result/jigsaw-crawl-300d-2M.joblib', 'rb') as f:\n    crawl_emb_dict = joblib.load(f)\ncrawl_emb_dict = set(crawl_emb_dict.keys())\n#print(crawl_emb_dict)\n#for k in list(crawl_emb_dict)[:10]:\n#    print({k:crawl_emb_dict[k]})\n\n# ã‚°ãƒ¼ã‚°ãƒ«ãŒç¦æ­¢ã—ã¦ã„ã‚‹å˜èªï¼ˆç¦å¥ï¼‰é›†ã‚’å–å¾—\nwith open('../input/googleprofanitywords/google-profanity-words/google-profanity-words/profanity.js', 'r') as f:\n    p_words = f.readlines()\n    \nset_puncts = set(puncts)\n#print(set_puncts)\n\n# ãƒ‡ãƒ¼ã‚¿å†…ã®æ”¹è¡Œã‚’å‰Šé™¤ã¨é‡è¤‡è¡Œã®å‰Šé™¤\np_word_set = set([t.replace('\\n', '') for t in p_words])\n#print(p_word_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del puncts, p_words#, raw_dict\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\nfrom typing import Dict, List\nfrom tqdm import tqdm_notebook as tqdm\n\ndef check_coverage(texts, embeddings_word: Dict) -> List[str]:\n    known_words = []\n    unknown_words = []\n    for text in tqdm(texts):\n        text = text.split()\n        for word in text:\n            if word in embeddings_word:\n                known_words.append(word)\n                #print(word)\n            else:\n                unknown_words.append(word)\n\n    print('Found embeddings for {:.2%} of vocab'.format(float(len(set(known_words))) / (float(len(set(known_words))) + float(len(set(unknown_words))))))\n    print('Found embeddings for {:.2%} of all text'.format(float(len(known_words)) / (float(len(known_words)) + float(len(unknown_words)))))\n\n    return set(unknown_words)\n\n# ï¼“é€šã‚Šã®Stemmerï¼ˆæ¥å°¾è¾é™¤å»=èªå¹¹æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ç”¨ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—\nfrom nltk.stem import PorterStemmer\np_stemmer = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nl_stemmer = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\ns_stemmer = SnowballStemmer(\"english\")\n\nimport copy\ndef edits1(word):\n    \"\"\"\n    wordã®ç·¨é›†è·é›¢1ã®å˜èªã®ãƒªã‚¹ãƒˆã‚’è¿”ã™\n    \"\"\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    # å˜èªã‚’å·¦å³ï¼’ã¤ã«åˆ†å‰²ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    # å˜èªã‹ã‚‰ä¸€æ–‡å­—ã‚’æ¶ˆã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    # å˜èªã‹ã‚‰éš£åŒå£«ã®æ–‡å­—ã‚’ä¸€çµ„å…¥ã‚Œæ›¿ãˆãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    # å˜èªå†…ã®ä¸€æ–‡å­—ã‚’åˆ¥ã®æ–‡å­—ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆä¸­ã®å…¨ã¦ã®æ–‡å­—ï¼‰ã«ãã‚Œãã‚Œå¤‰æ›ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    # å˜èªã®ã©ã“ã‹ã«åˆ¥ã®æ–‡å­—ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆä¸­ã®å…¨ã¦ã®æ–‡å­—ï¼‰ã‚’åŠ ãˆãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef known(words, embed): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    # å˜èªãŒè¾æ›¸å†…ã«å…¥ã£ã¦ã„ãŸã‚‰è¿”ã™\n    return set(w for w in words if w in embed)\n\n# å˜èªã®èª¤å­—è„±å­—ã‚’å¾©å…ƒ\ndef spellcheck(word, word_rank_dict):\n    # å˜èªã«ä¸€æ–‡å­—å…¥ã‚ŒãŸã‚Šã€éš£åŒå£«ã®æ–‡å­—ã‚’å…¥ã‚Œæ›¿ãˆãŸã‚Šã—ãŸç‰©ã®å†…ã€å˜èªè¾æ›¸ã«ã‚ã‚‹ç‰©ã®ä¸­ã‹ã‚‰ã€ä¸€ç•ªæ–‡å­—æ•°ãŒå°‘ãªã„æ–‡å­—åˆ—ã‚’è¿”ã™\n    return min(known(edits1(word), word_rank_dict), key=lambda word_rank_dict:word_rank_dict)\n\n\nimport unicodedata\npunct_mapping = {\"â€˜\": \"'\", \"â‚¹\": \"e\", \"Â´\": \"'\", \"Â°\": \"\",\n                 \"â‚¬\": \"e\", \"â„¢\": \"tm\", \"âˆš\": \" sqrt \", \"Ã—\": \"x\",\n                 \"Â²\": \"2\", \"â€”\": \"-\", \"â€“\": \"-\", \"â€™\": \"'\",\n                 \"_\": \"-\", \"`\": \"'\", 'â€œ': '\"', 'â€': '\"',\n                 'â€œ': '\"', \"Â£\": \"e\", 'âˆ': 'infinity',\n                 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha',\n                 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta',\n                 'âˆ…': '', 'Â³': '3', 'Ï€': 'pi', 'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ğŸ˜‰':'wink','ğŸ˜‚':'joy','ğŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\ndef process_stemmer(texts, embed):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            #print(word)\n            # åˆ†æ•£è¡¨ç¾è¾æ›¸ã‹ã‚‰è‰²ã€…ãªè¡¨ç¾ã§å¤‰æ›ã—ãŸæ–‡ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n            if word in embed:\n                new_text += word + \" \"\n                #print(\"embed:\",word)\n                continue\n\n            # å˜èªã‚’å…¨ã¦å°æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n            if word.lower() in embed:\n                new_text += word.lower() + \" \"\n                #print(\"lower:\",word)\n                continue\n\n            # å˜èªã‚’å…¨ã¦å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n            if word.upper() in embed:\n                new_text += word.upper() + \" \"\n                #print(\"upper:\",word)\n                continue\n\n            # å˜èªã®é ­æ–‡å­—ã ã‘ã‚’å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n            if word.capitalize() in embed:\n                new_text += word.capitalize() + \" \"\n                #print(\"cap:\",word)\n                continue\n\n            # ç‰¹æ®Šæ–‡å­—ã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n            corr_word = punct_mapping.get(word, None)\n            if corr_word is not None:\n                new_text += corr_word + \" \"\n                #print(\"punct:\",word)\n                continue\n\n            try:\n                # PorterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                vector = p_stemmer.stem(word)\n            except:\n                # å¤±æ•—ã—ãŸã‚‰ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œ\n                vector = p_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"p_st:\",vector)\n                continue\n                \n            try:\n                # LancasterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                vector = l_stemmer.stem(word)\n            except:\n                vector = l_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"l_st:\",vector)\n                continue\n\n            try:\n                # SnowballStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                vector = s_stemmer.stem(word)\n            except:\n                vector = s_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"s_st:\",vector)\n                continue\n\n            # å˜èªã®åˆ†æ•£è¡¨ç¾ãŒæ¤œç´¢ã§ããªã‹ã£ãŸå˜èªã‚’è¨˜éŒ²\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef process_small_capital(texts, embed, oov_set):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            #print(word)\n            if word not in oov_set:\n                new_text += word + \" \"\n                continue\n\n            char_list = []\n            any_small_capitial = False\n            # å˜èªã‚’ä¸€æ–‡å­—ãšã¤å–å¾—\n            for char in word:\n                try:\n                    # æ–‡å­—ã«å‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ã„ã‚‹åå‰ï¼ˆ\"a\"=\"LATIN SMALL LETTER A\"ï¼‰ã‚’å–å¾—\n                    uni_name = unicodedata.name(char)\n                except ValueError:\n                    continue\n\n                # æ–‡å­—ãŒãƒ©ãƒ†ãƒ³æ–‡å­—ã ã£ãŸå ´åˆ\n                if 'LATIN SMALL LETTER' or 'LATIN CAPITAL LETTER' in uni_name:\n                    # æ–‡å­—ã«å‰²ã‚ŠæŒ¯ã‚‰ã‚ŒãŸåå‰ã®æœ€å¾Œï¼ˆæ–‡å­—ãŒå¤§æ–‡å­—åŒ–ã—ãŸç‰©ï¼‰ã‚’å–å¾—\n                    char = uni_name[-1]\n                    any_small_capitial = True\n                # ã‚­ãƒªãƒ«æ–‡å­—ã®\"Ò“\"ã¯ã€\"F\"ã«å¤‰æ›\n                if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:\n                    char = 'F'\n                    any_small_capitial = True\n\n                char_list.append(char)\n\n            # å˜èªå†…ã®å…¨ã¦ã®æ–‡å­—ãŒã€åå‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã€ãƒ©ãƒ†ãƒ³æ–‡å­—ã§ã¯ãªãã€ã‚­ãƒªãƒ«æ–‡å­—ã®'Ò“'ã§ã‚‚ãªã„å ´åˆ\n            if not any_small_capitial:\n                oov_word_set.add(word)\n                new_text += word + \" \"\n                #print(\"oov_small_cap:\",word)\n                continue\n\n            # å¤‰æ›ã—ãŸæ–‡å­—ã‚’ä¸€ã¤ã®å˜èªã«æˆ»ã™\n            legit_word = ''.join(char_list)\n\n            if legit_word in embed:\n                new_text += legit_word + \" \"\n                #print(\"embed:\",legit_word)\n                continue\n\n            if legit_word.lower() in embed:\n                new_text += legit_word.lower() + \" \"\n                #print(\"lower:\",legit_word)\n                continue\n\n            if legit_word.upper() in embed:\n                new_text += legit_word.upper() + \" \"\n                #print(\"upper:\",legit_word)\n                continue\n\n            if legit_word.capitalize() in embed:\n                new_text += legit_word.capitalize() + \" \"\n                #print(\"cap:\",legit_word)\n                continue\n\n            corr_word = punct_mapping.get(legit_word, None)\n            if corr_word is not None:\n                new_text += corr_word + \" \"\n                #print(\"punct:\",legit_word)\n                continue\n\n            try:\n                vector = p_stemmer.stem(legit_word)\n            except:\n                vector = p_stemmer.stem(legit_word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"p_st:\",vector)\n                continue\n\n            try:\n                vector = l_stemmer.stem(legit_word)\n            except:\n                vector = l_stemmer.stem(legit_word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"l_st:\",vector)\n                continue\n\n            try:\n                vector = s_stemmer.stem(legit_word)\n            except:\n                vector = s_stemmer.stem(legit_word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"s_st:\",vector)\n                continue\n\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n            \n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n\n        new_texts.append(new_text.strip())\n        \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef process_spellcheck(texts, embed, oov_set):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            if word not in oov_set:\n                new_text += word + \" \"\n                continue\n\n            try:\n                vector = spellcheck(word, embed)\n            except:\n                oov_word_set.add(word)\n                new_text += word + \" \"\n                #print(\"oov:\",word)\n                continue\n            if vector is not None:\n                new_text += vector + \" \"\n                #print(\"original:\",word)\n                #print(\"miss_sp:\",vector)\n                continue\n\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n            \n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\nfrom nltk import TweetTokenizer\n# Tweetå°‚ç”¨ã®è§£æãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ\n# reduce_lenï¼šå˜èªã®é•·ã•ã®æ¨™æº–åŒ–ï¼ˆçŸ­ç¸®åŒ–ï¼‰ã‚’ã™ã‚‹ã‹ã©ã†ã‹ã‚’è¨­å®š\ntknzr = TweetTokenizer(reduce_len=True)\ndef twitter_stemmer(texts, embed, oov_set):\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for word in text:\n            if word not in oov_set:\n                new_text += word + \" \"\n                continue\n            \n            tokens = tknzr.tokenize(word)\n            if (tokens[0] == \"'\" or '\"') and len(tokens) > 1:\n                word = tokens[1]\n                #print(\"tokens_1:\")\n            else:\n                word = tokens[0]\n                #print(\"tokens_0:\")\n            #print(word)\n            # åˆ†æ•£è¡¨ç¾è¾æ›¸ã‹ã‚‰è‰²ã€…ãªè¡¨ç¾ã§å¤‰æ›ã—ãŸæ–‡ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n            if word in embed:\n                new_text += word + \" \"\n                #print(\"embed:\",word)\n                continue\n\n            # å˜èªã‚’å…¨ã¦å°æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n            if word.lower() in embed:\n                new_text += word.lower() + \" \"\n                #print(\"lower:\",word)\n                continue\n\n            # å˜èªã‚’å…¨ã¦å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n            if word.upper() in embed:\n                new_text += word.upper() + \" \"\n                #print(\"upper:\",word)\n                continue\n\n            # å˜èªã®é ­æ–‡å­—ã ã‘ã‚’å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n            if word.capitalize() in embed:\n                new_text += word.capitalize() + \" \"\n                #print(\"cap:\",word)\n                continue\n\n            # ç‰¹æ®Šæ–‡å­—ã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n            corr_word = punct_mapping.get(word, None)\n            if corr_word is not None:\n                new_text += corr_word + \" \"\n                #print(\"punct:\",word)\n                continue\n\n            try:\n                # PorterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                vector = p_stemmer.stem(word)\n            except:\n                # å¤±æ•—ã—ãŸã‚‰ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œ\n                vector = p_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"p_st:\",vector)\n                continue\n                \n            try:\n                # LancasterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                vector = l_stemmer.stem(word)\n            except:\n                vector = l_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"l_st:\",vector)\n                continue\n\n            try:\n                # SnowballStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                vector = s_stemmer.stem(word)\n            except:\n                vector = s_stemmer.stem(word.decode('utf-8'))\n            if vector in embed:\n                new_text += vector + \" \"\n                #print(\"s_st:\",vector)\n                continue\n\n            # å˜èªã®åˆ†æ•£è¡¨ç¾ãŒæ¤œç´¢ã§ããªã‹ã£ãŸå˜èªã‚’è¨˜éŒ²\n            oov_word_set.add(word)\n            new_text += word + \" \"\n            #print(\"oov:\",word)\n        if len(new_text.strip()) == 0:\n            print(\"0:None!\")\n            print(text)\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef bytes_to_unicode():\n    # ãƒ©ãƒ†ãƒ³æ–‡å­—ã‚’è¡¨ã™Unicodeã‚’å–å¾—\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"Â¡\"), ord(\"Â¬\")+1))+list(range(ord(\"Â®\"), ord(\"Ã¿\")+1))\n    cs = bs[:]\n    n = 0\n    # 0~255ã®Unicodeã‚’å–å¾—\n    for b in range(2**8):\n        if b not in bs:\n            # æ–‡å­—ä»¥å¤–ã®Unicodeï¼ˆPCã®å‘½ä»¤ã‚³ãƒ¼ãƒ‰ï¼‰ã‚‚å–å¾—\n            bs.append(b)\n            # ãƒ©ãƒ†ãƒ³æ–‡å­—æ‹¡å¼µAï¼ˆPCã®å‘½ä»¤ã‚³ãƒ¼ãƒ‰+256ã«ä½ç½®ã™ã‚‹æ–‡å­—ï¼‰ã®Unicodeã‚’å–å¾—\n            cs.append(2**8+n)\n            n += 1\n    # é…åˆ—å†…ã®Unicodeã‚’å˜èªã«å¤‰æ›\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n# å˜èªã®å…ˆé ­æ–‡å­—ã‹ã‚‰äºŒæ–‡å­—ãšã¤ã®ã‚¿ãƒ—ãƒ«ã‚’å–å¾—\ndef get_pairs(word):\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n# ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰mergeæ–‡å­—å–å¾—\nMERGES_PATH = \"../input/transformer-tokenizers/gpt2/merges.txt\"\nbpe_data = open(MERGES_PATH, encoding='utf-8').read().split('\\n')[1:-1]\n# mergeæ–‡å­—ã®firstã¨secondã‚’ã‚¿ãƒ—ãƒ«åŒ–\nbpe_merges = [tuple(merge.split()) for merge in bpe_data]\n# keyãŒmergeæ–‡å­—ã®ã‚¿ãƒ–ãƒ«ã€valueãŒ0ã‹ã‚‰ã®idã®è¾æ›¸ä½œæˆ\nbpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\ndef bpe(token):\n    word = tuple(token)\n    # å˜èªã®å…ˆé ­æ–‡å­—ã‹ã‚‰äºŒæ–‡å­—ãšã¤ã®ã‚¿ãƒ—ãƒ«ã‚’å–å¾—\n    pairs = get_pairs(word)\n\n    if not pairs:\n        return token\n\n    while True:\n        # å˜èªä¸­ã§ãƒãƒƒãƒã—ãŸmergeæ–‡å­—ã®ã‚¿ãƒ—ãƒ«ã‹ã‚‰ã€idãŒä¸€ç•ªå°ã•ã„ã‚‚ã®ã‚’å–å¾—\n        bigram = min(pairs, key = lambda pair: bpe_ranks.get(pair, float('inf')))\n        if bigram not in bpe_ranks:\n            break\n        first, second = bigram\n        new_word = []\n        i = 0\n        while i < len(word):\n            try:\n                # iç•ªç›®ã®æ–‡å­—ã‹ã‚‰å¾Œã‚ã®mergeæ–‡å­—ã®firstã®idã‚’å–å¾—\n                j = word.index(first, i)\n                # iç•ªç›®ã®æ–‡å­—ã‹ã‚‰mergeæ–‡å­—ã®firstã®ç›´å‰ã¾ã§ã®æ–‡å­—åˆ—ã‚’å–å¾—\n                new_word.extend(word[i:j])\n                i = j\n            except:\n                # mergeæ–‡å­—ã®firstãŒå˜èªã«å«ã¾ã‚Œã¦ãªã„å ´åˆã¯ã€ãã®ã¾ã¾å–å¾—\n                new_word.extend(word[i:])\n                break\n\n            # mergeæ–‡å­—ã®firstã¨secondã®äºŒæ–‡å­—ãŒé€£ç¶šã§ç¶šã„ãŸæ™‚ã‚‚å–å¾—\n            if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                new_word.append(first+second)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_word = tuple(new_word)\n        word = new_word\n        #print(\"new_word:\",new_word)\n        if len(word) == 1:\n            break\n        else:\n            pairs = get_pairs(word)\n    word = ' '.join(word)\n    return word\n\ndef merge_spellcheck(texts, embed, oov_set):\n    # keyï¼ˆãƒ©ãƒ†ãƒ³æ–‡å­—ã¨PCå‘½ä»¤ã‚³ãƒ¼ãƒ‰ï¼‰ã€valueï¼ˆãƒ©ãƒ†ãƒ³æ–‡å­—ã¨ãƒ©ãƒ†ãƒ³æ–‡å­—æ‹¡å¼µï¼‰ã§ã‚ã‚‹è¾æ›¸ã‚’å–å¾—\n    byte_encoder = bytes_to_unicode()\n    # æ–‡ç« ã‚’å˜èªï¼ˆè§£æã™ã‚‹ãƒ‘ãƒ¼ãƒ„ï¼‰ã«åˆ†ã‘ã‚‹æ–‡å­—ã®åˆ¤åˆ¥å™¨\n    #pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d|[^('s)('t)('re)('ve)('m)('ll)('d)]+\"\"\")\n    oov_word_set = set()\n    new_texts = []\n    for text in tqdm(texts):\n        text = text.split()\n        new_text = \"\"\n        for pre_word in text:\n            if pre_word not in oov_set:\n                new_text += pre_word + \" \"\n                continue\n            #print(\"original:\",pre_word)\n            bpe_tokens = []\n            token_list = re.split(\"('s)|('t)|('re)|('ve)|('m)|('ll)|('d)\", pre_word)\n            token_list = [x for x in token_list if (x is not None) and (x != \"\")]\n            #print(\"token_list:\",token_list)\n            # å˜èªä¸­ã®å˜èªã‚’mergeæ–‡å­—ã§ç´°åˆ†åŒ–\n            for token in token_list:\n                #print(\"token:\",token)\n                # å˜èªä¸­ã®ãƒ©ãƒ†ãƒ³æ–‡å­—ã¨PCå‘½ä»¤ã‚³ãƒ¼ãƒ‰ã‚’ãƒ©ãƒ†ãƒ³æ–‡å­—æ‹¡å¼µAã«å¤‰æ›ã—ãŸæ–‡å­—ã®ã¿ã®å˜èªã‚’ç”Ÿæˆ\n                token = ''.join(byte_encoder[b] for b in token.encode('utf-8'))\n                # mergeæ–‡å­—ã«ã‚ˆã£ã¦åˆ†ã‹ã¡æ›¸ãã—ãŸå˜èªã‚’ä¿å­˜\n                bpe_tokens.extend(bpe_token for bpe_token in bpe(token).split(' '))\n                if (len(bpe_tokens) == 1) and (bpe_tokens[0] == token) and (len(token_list) == 0):\n                    oov_word_set.add(word)\n                    new_text += word + \" \"\n                    #print(\"no_tokens:\")\n                    continue\n                if len(bpe_tokens) == 1:\n                    bpe_tokens.append(\" \")\n\n                for word in bpe_tokens:\n                    #print(\"word:\",word)\n                    # åˆ†æ•£è¡¨ç¾è¾æ›¸ã‹ã‚‰è‰²ã€…ãªè¡¨ç¾ã§å¤‰æ›ã—ãŸæ–‡ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n                    if word in embed:\n                        new_text += word + \" \"\n                        #print(\"embed:\",word)\n                        continue\n\n                    # å˜èªã‚’å…¨ã¦å°æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n                    if word.lower() in embed:\n                        new_text += word.lower() + \" \"\n                        #print(\"lower:\",word)\n                        continue\n\n                    # å˜èªã‚’å…¨ã¦å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n                    if word.upper() in embed:\n                        new_text += word.upper() + \" \"\n                        #print(\"upper:\",word)\n                        continue\n\n                    # å˜èªã®é ­æ–‡å­—ã ã‘ã‚’å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢\n                    if word.capitalize() in embed:\n                        new_text += word.capitalize() + \" \"\n                        #print(\"cap:\",word)\n                        continue\n\n                    # ç‰¹æ®Šæ–‡å­—ã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n                    corr_word = punct_mapping.get(word, None)\n                    if corr_word is not None:\n                        new_text += corr_word + \" \"\n                        #print(\"punct:\",word)\n                        continue\n\n                    try:\n                        # PorterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                        vector = p_stemmer.stem(word)\n                    except:\n                        # å¤±æ•—ã—ãŸã‚‰ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œ\n                        vector = p_stemmer.stem(word.decode('utf-8'))\n                    if vector in embed:\n                        new_text += vector + \" \"\n                        #print(\"p_st:\",vector)\n                        continue\n\n                    try:\n                        # LancasterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                        vector = l_stemmer.stem(word)\n                    except:\n                        vector = l_stemmer.stem(word.decode('utf-8'))\n                    if vector in embed:\n                        new_text += vector + \" \"\n                        #print(\"l_st:\",vector)\n                        continue\n\n                    try:\n                        # SnowballStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢\n                        vector = s_stemmer.stem(word)\n                    except:\n                        vector = s_stemmer.stem(word.decode('utf-8'))\n                    if vector in embed:\n                        new_text += vector + \" \"\n                        #print(\"s_st:\",vector)\n                        continue\n\n                    # å˜èªã®åˆ†æ•£è¡¨ç¾ãŒæ¤œç´¢ã§ããªã‹ã£ãŸå˜èªã‚’è¨˜éŒ²\n                    oov_word_set.add(word)\n                    new_text += word + \" \"\n                    #print(\"oov:\",word)\n        if len(new_text.strip()) == 0:\n            print(text)\n            print(\"0:None!\")\n            new_text += \"0\" + \" \"\n            \n        new_texts.append(new_text.strip())\n            \n        #print(new_texts)\n    return new_texts, oov_word_set\n\ndef head(enumerable, n=10):\n    #print(enumerable)\n    for i, item in enumerate(enumerable):\n        print(str(i) + '\\n',item)\n        if i > n:\n            return\n\nprint(\"only_data:\")\n# åˆ†æ•£è¡¨ç¾ã®è¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ãªã„å˜èªã‚’å–å¾—\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n\n# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å˜èªã«æ¥å°¾è¾é™¤å»ã‚’ç”¨ã„ã¦ã€åˆ†æ•£è¡¨ç¾ã®è¾æ›¸ä½œæˆ\ntrain_text, oov_stemer = process_stemmer(train_text, crawl_emb_dict)\n#print(train_text)\nprint(\"stemmer_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_stemer)\n\n# å˜èªä¸­ã®æ–‡å­—ã‚’è§£æã—ã¦ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\ntrain_text, oov_small_capital = process_small_capital(train_text, crawl_emb_dict, oov_stemer)\nprint(\"small_capital_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_small_capital)\n\n# å˜èªã®èª¤å­—è„±å­—ã‚’è§£æã—ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n#train_text_test = train_text\ntrain_text, oov_spell = process_spellcheck(train_text, crawl_emb_dict, oov_small_capital)\nprint(\"spellcheck_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_spell)\n\n# twitterã®åˆ†ã‹ã¡æ›¸ãã‚’ä½¿ã£ã¦ã€å˜èªã‚’å¤‰æ›ã—ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n#train_text_test = train_text\ntrain_text, oov_twitter = twitter_stemmer(train_text, crawl_emb_dict, oov_spell)\nprint(\"twitter_stemmer_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)\n#print(oov_twitter)\n\n# mergeæ–‡å­—ã‚’ä½¿ã£ã¦ã€å˜èªã‚’å¤‰æ›ã—ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢\n#train_text_test = train_text\ntrain_text, oov_mearge = merge_spellcheck(train_text, crawl_emb_dict, oov_twitter)\nprint(\"merge_data:\")\noov = check_coverage(train_text, crawl_emb_dict)\nhead(oov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del oov, oov_stemer, oov_small_capital, oov_spell, oov_mearge, oov_twitter, bpe_data, bpe_merges, bpe_ranks\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# æ–‡ç« ã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ\ndef sentence_fetures(text):\n    word_list = text.split()\n    #print(word_list)\n    # å˜èªæ•°\n    word_count = len(word_list)\n    # å¤§æ–‡å­—ã‚’å«ã‚€å˜èªã®æ•°\n    n_upper = len([word for word in word_list if any([c.isupper() for c in word])])\n    # å«ã¾ã‚Œã‚‹å˜èªã®ç¨®é¡\n    n_unique = len(set(word_list))\n    # ãƒ“ãƒƒã‚¯ãƒªãƒãƒ¼ã‚¯ã®æ•°\n    n_ex = word_list.count('!')\n    #print(n_ex)\n    # ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ãƒãƒ¼ã‚¯ã®æ•°\n    n_que = word_list.count('?')\n    # ç‰¹æ®Šæ–‡å­—ï¼ˆå¥èª­ç‚¹ï¼‰ã®æ•°\n    n_puncts = len([word for word in word_list if word in set_puncts])\n    # ç¦å¥ã®æ•°\n    n_prof = len([word for word in word_list if word in p_word_set])\n    # unknownå˜èªã®æ•°\n    n_oov = len([word for word in word_list if word not in crawl_emb_dict])\n    \n    return word_count, n_upper, n_unique, n_ex, n_que, n_puncts, n_prof, n_oov\n\nfrom collections import defaultdict\nsentence_feature_cols = ['word_count', 'n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']\n# keyä¸è¦ã®valueãŒlistå‹ã®è¾æ›¸ã‚’ä½œæˆ\nfeature_dict = defaultdict(list)\n#print(raw_train)\nfor text in train_text:\n    #print(text)\n    # æ–‡ç« ã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n    feature_list = sentence_fetures(text)\n    for i_feature, feature_name in enumerate(sentence_feature_cols):\n        # keyã‚’ç‰¹å¾´ã®åå‰ã€valueã‚’æ–‡ç« ã®ç‰¹å¾´å€¤ã¨ã—ãŸè¾æ›¸ã‚’ä½œæˆ\n        feature_dict[sentence_feature_cols[i_feature]].append(feature_list[i_feature])\n        \nsentence_df = pd.DataFrame.from_dict(feature_dict)\n#print(sentence_df['word_count'])\n# å„ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’å˜èªã®æ•°ã§å¹³å‡åŒ–\nfor col in ['n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']:\n    sentence_df[col + '_ratio'] = sentence_df[col] / sentence_df['word_count']\n#print(sentence_df)\n    \n# ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰unknownãƒ¯ãƒ¼ãƒ‰é–¢é€£ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤\nsentence_feature_mat = sentence_df.drop(columns=['n_oov', 'n_oov_ratio']).values\n#print(sentence_feature_mat)\n\n# ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ï¼ˆå¹³å‡0ã€åˆ†æ•£1ã®é›†åˆåŒ–ï¼‰\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(sentence_feature_mat)\nsentence_feature_mat = scaler.transform(sentence_feature_mat)\n#print(sentence_feature_mat)\n#print(scaler.var_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sentence_feature_cols, feature_dict, sentence_df, scaler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentence_feature_mat = torch.tensor(sentence_feature_mat, dtype=torch.float32).cuda()\nsentence_feature_size = sentence_feature_mat.shape[-1]\ny_label_size = y_label.shape[-1]\n#print(y_label_size)\n#print(sentence_feature_mat.shape)\n#print(sentence_feature_size)\nimport numpy as np\nlabel_data = torch.tensor(np.concatenate([y_label, sentence_feature_mat], axis=1), dtype=torch.float32).cuda()\n#print(y_target)\n#print(y_target.shape)\nn_samples = len(train_text)\ntrain_size = int(n_samples * 0.4)\nvalid_size = int(n_samples * 0.3)\n#train_index = list(range(0, train_size))\n#valid_index = list(range(train_size, train_size + valid_size))\n#test_index = list(range(train_size + valid_size, n_samples))\n#print(n_samples)\ntrain = pd.Series(list(train_text[0:train_size]))\ntrain_label = label_data[0:train_size]\nvalid = pd.Series(list(train_text[train_size:train_size + valid_size]))\nvalid_label = label_data[train_size:train_size + valid_size]\ntest = pd.Series(list(train_text[train_size + valid_size:n_samples]))\ntest_label = label_data[train_size + valid_size:n_samples]\n#print(len(train))\n#print(len(valid))\n#print(len(test))\n#print(valid_label)\n#print(label_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del y_label, n_samples, train_size, valid_size, sentence_feature_mat#, train_index, valid_index, test_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’æ•´åœ°\ndef tokenize(text, max_len, tokenizer):\n    # vocab.txtã«ä¹—ã£ã¦ã„ã‚‹æ–‡å­—ã ã‘ã‚’æ–‡ç« å†…ã«å–å¾—\n    tokenized_text = tokenizer.tokenize(text)[:max_len-2]\n    return [\"[CLS]\"]+tokenized_text+[\"[SEP]\"]\n\n# ä»¥å‰ã®pandasãƒ•ã‚¡ã‚¤ãƒ«ã®é€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–\n#import pandas as pd\n#from tqdm import tqdm\n#tqdm.pandas()\n#from tqdm import tqdm_notebook as tqdm\n#from tqdm._tqdm_notebook import tqdm_notebook\n#tqdm_notebook.pandas()\ntrain = train.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))\nvalid = valid.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))\ntest = test.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’vocab.txtå†…ã®åˆ†æ•£è¡¨ç¾ã«å¤‰æ›\ntrain = train.apply(lambda x: tokenizer.convert_tokens_to_ids(x))\nvalid = valid.apply(lambda x: tokenizer.convert_tokens_to_ids(x))\ntest = test.apply(lambda x: tokenizer.convert_tokens_to_ids(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del tokenizer\ngc.collect()\n\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shutil\n# printãªã©ã®æ¨™æº–å‡ºåŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éè¡¨ç¤º\nwith suppress_stdout():\n    # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®Bertãƒ¢ãƒ‡ãƒ«ï¼ˆTendorflowModelï¼‰ã‚’Pytorchä¸Šã«ä½œæˆ\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        # Bertï¼ˆTendorflowModelï¼‰ã®äº‹å‰å­¦ç¿’æ™‚ï¼ˆcheckpointï¼‰ã®é‡ã¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«è¨­å®š\n        # ãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã§å‡ºåŠ›ã¯æ—¢ã«æ±ºå®šã•ã‚Œã¦ã„ã‚‹ï¼Ÿ\n        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å¤‰æ•°åã«'squad'ãŒã‚ã‚‹æ™‚ã€äº‹å‰å­¦ç¿’ã®å‡ºåŠ›ã¯Classificationï¼ˆæœ€åˆã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‡ºåŠ›ï¼‰ã«ãªã‚‹ï¼Ÿ\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n        BERT_MODEL_PATH + 'bert_config.json',\n        # é‡ã¿ã‚’è¨­å®šã—ãŸBERTãƒ¢ãƒ‡ãƒ«ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’WORK_DIRä¸Šã«ä¿å­˜\n        WORK_DIR + 'pytorch_model.bin')\n\n# ä½¿ç”¨ã—ãŸBertã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPytorchModelï¼‰ã‚’åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nclass NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, sentence_feature_size):\n        super(NeuralNet, self).__init__()\n        # WORK_DIRä¸Šã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®BERTã‚’ä½œæˆ\n        #self.bert = BertModel.from_pretrained(WORK_DIR)\n        # bertãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ã—ãªã„\n        #for param in self.bert.parameters():\n            #param.requires_grad=False\n            #print(f'bert-{param.requires_grad}')\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        # Bertã®ç‰¹å¾´ã‚’å­¦ç¿’\n        self.before_linear = nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE)\n        self.before_linear2 = nn.Linear(BERT_HIDDEN_SIZE, 50)\n        \n        # sentence_featureã®ç‰¹å¾´ã‚’å­¦ç¿’\n        self.sentence_feature_linear = nn.Linear(sentence_feature_size, sentence_feature_size)\n        \n        # Bertã¨sentence_featureã®æŠ±ãåˆã‚ã›ã‚’å­¦ç¿’\n        n_hidden = 50 + sentence_feature_size\n        self.mix_linear = nn.Linear(n_hidden, n_hidden)\n        \n        # å‡ºåŠ›\n        self.linear_out = nn.Linear(n_hidden, 1)\n        #nn.init.xavier_uniform_(self.linear_out.weight)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        #nn.init.xavier_uniform_(self.linear_aux_out.weight)\n        \n    def forward(self, bert_output, sentence_feature):\n        # encodeã‚»ãƒ«ã®æœ€åˆã®æ™‚ç³»åˆ—ï¼ˆClassificationï¼‰ä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„Bertãƒ¢ãƒ‡ãƒ«\n        # å¤‰æ•°å‰ã®*ã¯ã€å¤‰æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹ï¼ˆå¼•æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‹ï¼Ÿï¼‰\n        #_, bert_output = self.bert(*x_features, output_all_encoded_layers=False)\n        bert_output = self.dropout(bert_output)\n        \n        bert_relu  = F.relu(bert_output)\n        \n        before_nn = self.before_linear(bert_relu)\n        before_relu = F.relu(before_nn)\n        before_nn2 = self.before_linear2(before_relu)\n        before_relu2 = F.relu(before_nn2)\n        \n        sentence_feature_nn = self.sentence_feature_linear(sentence_feature)\n        sentence_feature_relu = F.relu(sentence_feature_nn)\n        \n        h_cat = torch.cat((before_relu2, sentence_feature_relu), 1)\n        mix_nn = self.mix_linear(h_cat)\n        mix_relu = F.relu(mix_nn)\n        \n        result = self.linear_out(mix_relu)\n        aux_result = self.linear_aux_out(mix_relu)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒæ¯ã«åˆ†ã‘ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿é–¢æ•°\nclass DynamicBucketIterator(object):\n    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒæ¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ†å‰²\n    def __init__(self, data, label, capacity, pad_token, shuffle, length_quantile, max_batch_size, for_bert):\n        self.data = data\n        self.label = label\n        self.pad_token = pad_token\n        self.capacity = capacity\n        self.shuffle = shuffle\n        self.length_quantile = length_quantile\n        self.for_bert = for_bert\n        \n        # æ–‡ç« ãŒçŸ­ã„é †ã«æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®indexã‚’ã‚½ãƒ¼ãƒˆ\n        self.index_sorted = sorted(range(len(self.data)), key=lambda i: len(self.data[i]))\n        \n        old_separator_index = 0\n        self.separator_index_list = [0]\n        for i_sample in range(len(self.data)):\n            # æ–‡ç« ãŒçŸ­ã„é †ã«æ–‡ç« ãƒ‡ãƒ¼ã‚¿å–å¾—\n            sample_index = self.index_sorted[i_sample]\n            sample = self.data[sample_index]\n            current_batch_size = i_sample - old_separator_index + 1\n            if min(len(sample), MAX_LEN) * current_batch_size <= self.capacity and current_batch_size <= max_batch_size:\n                pass\n            else:\n                # ãƒãƒƒãƒã®æœ€å¾Œã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®indexã‚’è¨˜éŒ²\n                old_separator_index = i_sample\n                self.separator_index_list.append(i_sample)\n                \n        # æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®indexã‚’è¨˜éŒ²\n        self.separator_index_list.append(len(self.data)) # [0, ..., start_separator_index, end_separator_index, ..., len(data)]\n        \n        if not self.shuffle:\n            # ãƒãƒƒãƒæ•°å–å¾—\n            self.bucket_index = range(self.__len__())\n        \n        self.reset_index()\n\n    def reset_index(self):\n        self.i_batch = 0\n        \n        if self.shuffle:\n            self.index_sorted = sorted(np.random.permutation(len(self.data)), key=lambda i: len(self.data[i]))\n            self.bucket_index = np.random.permutation(self.__len__())\n    \n    def __len__(self):\n        return len(self.separator_index_list) - 1\n    \n    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿é–¢æ•°ã«ã‚ˆã‚Šå‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°\n    def __iter__(self):\n        return self\n    \n    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã«ãƒãƒƒãƒæ¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n    def __next__(self):\n        # ãƒãƒƒãƒæ•°ãŒå…¨ã¦å‘¼ã³å‡ºã•ã‚Œã¦ã„ãŸã‚‰ã€åˆæœŸåŒ–ã—ã¦ã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†\n        try:\n            i_bucket = self.bucket_index[self.i_batch]\n        except IndexError as e:\n            self.reset_index()\n            raise StopIteration\n            \n        start_index, end_index = self.separator_index_list[i_bucket : i_bucket + 2]\n        \n        # ãƒ‡ãƒ¼ã‚¿ã®indexã‚’ä½¿ç”¨é †ã«ä¿å­˜\n        index_batch = self.index_sorted[start_index : end_index]\n\n        raw_batch_data = [self.data[i] for i in index_batch]\n        \n        batch_label = self.label[index_batch]\n        # ???\n        math.ceil(1)\n        \n        # ãƒãƒƒãƒä¸­ã§ä¸€ç•ªé•·ã„æ–‡ç« ã®å˜èªæ•°ã‚’å–å¾—\n        max_len = int(math.ceil(np.quantile([len(x) for x in raw_batch_data], self.length_quantile)))\n        max_len = min([max_len, MAX_LEN])\n        if max_len == 0:\n            max_len = 1\n        \n        # BERTç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ã—ã¦ã€è¿”ã™\n        if self.for_bert:\n            # ãƒãƒƒãƒã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®ç©ºé…åˆ—\n            segment_id_batch = np.zeros((len(raw_batch_data), max_len))\n            padded_batch = []\n            input_mask_batch = []\n            for sample in raw_batch_data:\n                # ãƒãƒƒãƒå†…ã§æœ€é•·ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦ã€å˜èªãŒå…¥ã£ã¦ã‚‹æ‰€ã«1ã€å…¥ã£ã¦ãªã„æ‰€ã«0\n                input_mask = [1] * len(sample) + [0] * (max_len - len(sample))\n                input_mask_batch.append(input_mask[:max_len])\n\n                # ãƒ‡ãƒ¼ã‚¿å†…ã‚’ãƒãƒƒãƒå†…ã§æœ€é•·ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦ã€0ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            # ãƒãƒƒãƒæ¯ã«ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸæ–‡ç« ãƒ‡ãƒ¼ã‚¿ã€æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®ç©ºé…åˆ—ã€\n            # æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®maskã€æ–‡ç« æ¯ã®å­¦ç¿’ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ã€æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®indexã‚’è¿”ã™\n            return padded_batch, segment_id_batch, input_mask_batch, batch_label, index_batch\n        \n        else:\n            padded_batch = []\n            for sample in raw_batch_data:\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            return padded_batch, batch_label, index_batch\n        \ndef sigmoid(x):\n    return np.where(x<-709.0, 0.0, 1 / (1 + np.exp(-x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport torch.optim as optim\n\ntest_dict = {}\nfold_list = [0]\nepochs = 7\n\nstart_time = time.time()\n\n# encodeã‚»ãƒ«ã®æœ€åˆã®æ™‚ç³»åˆ—ï¼ˆClassificationï¼‰ä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„Bertãƒ¢ãƒ‡ãƒ«\n# WORK_DIRä¸Šã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®BERTã‚’ä½œæˆ\nbert = BertModel.from_pretrained(WORK_DIR).cuda()\n#for param in bert.parameters():\n    #param.requires_grad=False\n    #print(f'bert-{param.requires_grad}')\n\n\nmodel = NeuralNet(y_label_size-1, sentence_feature_size)\n# æœ€é©åŒ–æ‰‹æ³•ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nmodel.cuda()\n\n# ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒ1ã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¹é–¢æ•°ï¼ˆBCEWithLogitsLossï¼‰ã¸ã®å½±éŸ¿åº¦\n#pos_weight = (len(y_targetlabel) - y_target.sum(0)) / y_target.sum(0)\n#pos_weight[pos_weight == float(\"Inf\")] = 1\n#print(pos_weight)\nloss_fn=nn.BCEWithLogitsLoss(reduction='mean')#, pos_weight=pos_weight)\n\n#for param in model.parameters():\n    #print(param.requires_grad)\n\nhighest_accuracy = 0\nlowest_loss = len(valid) * 100\n#print(lowest_loss)\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n\n    train_loader = DynamicBucketIterator(train, \n                                        train_label,\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n    \n    #print(valid_label)\n    valid_loader = DynamicBucketIterator(valid, \n                                         valid_label,\n                                         capacity=MAX_LEN*batch_size,\n                                         pad_token=0,\n                                         shuffle=False,\n                                         length_quantile=1,\n                                         max_batch_size=2048,\n                                         for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    eval_start_time = time.time()\n    for epoch in range(epochs):\n        batch_i = 0\n        train_loss_validation = 0\n        model.train()\n        for batch in train_loader:\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            #print(y_batch)\n            y_targets_batch = y_batch[:, :y_label_size]\n            sentence_feature = y_batch[:, -sentence_feature_size:]\n            #print(y_targets_batch)\n            #print(sentence_feature)\n            index_batch = batch[4]\n            #sample_weight_batch = y_batch[:, len(y_batch[1])-1]\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n\n            with torch.no_grad():\n                # å¤‰æ•°å‰ã®*ã¯ã€å¤‰æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹ï¼ˆå¼•æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‹ï¼Ÿï¼‰\n                _, bert_output = bert(*x_features, output_all_encoded_layers=False)\n            #print(bert_output.grad_fn)\n            # å‹¾é…ã®åˆæœŸåŒ–\n            optimizer.zero_grad()\n            out = model(bert_output, sentence_feature)\n            #print(model.linear_out.weight.grad)\n            #print(y_targets_batch)\n            test_dict[f'{epoch}-{batch_i}'] = sigmoid(out.detach().cpu().numpy())\n            batch_i += 1\n            \n            # ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒ1ã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¹é–¢æ•°ï¼ˆBCEWithLogitsLossï¼‰ã¸ã®å½±éŸ¿åº¦\n            #pos_weight = (len(y_targets_batch) - y_targets_batch.sum(0)) / y_targets_batch.sum(0)\n            #pos_weight[pos_weight == float(\"Inf\")] = 1\n            #print(pos_weight)\n            #loss_fn=nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)\n            \n            loss = loss_fn(out, y_targets_batch)\n            #print(f'{epoch}-{batch_i}:{loss.item() / len(y_batch)}')\n            \n            train_loss_validation += loss.item()\n            # å‹¾é…ã®è¨ˆç®—\n            loss.backward()\n            #print(model.linear_out.weight.grad)\n            #print(bert_output.grad_fn.grad)\n            \n            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°\n            optimizer.step()\n            del x_batch, segment_id_batch, input_mask_batch, y_batch, y_targets_batch, sentence_feature, index_batch, x_features, out, loss, bert_output\n            torch.cuda.empty_cache()\n        print(\"train_loss_validation:\", train_loss_validation)\n        \n        valid_pred = np.zeros(len(valid))\n        batch_i = 0\n        loss_validation = 0\n        model.eval()\n        for batch in valid_loader:\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            #print(y_batch)\n            y_targets_batch = y_batch[:, :y_label_size]\n            sentence_feature = y_batch[:, -sentence_feature_size:]\n            #print(y_targets_batch)\n            #print(sentence_feature)\n            index_batch = batch[4]\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n\n            with torch.no_grad():\n                # å¤‰æ•°å‰ã®*ã¯ã€å¤‰æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹ï¼ˆå¼•æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‹ï¼Ÿï¼‰\n                _, bert_output = bert(*x_features, output_all_encoded_layers=False)\n            #print(bert_output.grad_fn)\n            \n            y_pred = model(bert_output, sentence_feature)\n            \n            #print(\"y_pred:\", y_pred[:, 0])\n            #print(\"y_batch:\", y_batch[:, 0])\n            # ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒ1ã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¹é–¢æ•°ï¼ˆBCEWithLogitsLossï¼‰ã¸ã®å½±éŸ¿åº¦\n            #pos_weight = (len(y_targets_batch) - y_targets_batch.sum(0)) / y_targets_batch.sum(0)\n            #pos_weight[pos_weight == float(\"Inf\")] = 1\n            #print(pos_weight)\n            #loss_fn=nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)\n            \n            loss = loss_fn(y_pred, y_targets_batch)\n            \n            loss_validation += loss.item()\n            #print(y_batch)\n            #test_dict[f'{epoch}-{batch_i}'] = sigmoid(out.detach().cpu().numpy())\n            batch_i += 1\n            \n            valid_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n            \n            del x_batch, segment_id_batch, input_mask_batch, y_batch, y_targets_batch, sentence_feature, index_batch, x_features#, loss_fn\n            torch.cuda.empty_cache()\n        \n        valid_pred = (torch.from_numpy(valid_pred) >= 0.5).to(torch.float32).cuda()\n        correct = (valid_pred == valid_label[:, 0]).to(torch.float32).sum(0)\n        positive_correct = ((valid_pred == valid_label[:, 0]) & (valid_label[:, 0] == 1.0)).to(torch.float32).sum(0)\n        negative_correct = correct - positive_correct\n        accuracy = correct / len(valid_label)\n        positive_sum = valid_label[:, 0].sum(0)\n        positive_accuracy = positive_correct / positive_sum\n        negative_accuracy = negative_correct / (len(valid_label) - positive_sum)\n        if (accuracy >= highest_accuracy) | (lowest_loss >= loss_validation):\n            print(epoch)\n            print(accuracy)\n            print(positive_accuracy)\n            print(negative_accuracy)\n            highest_accuracy = accuracy\n            lowest_loss = loss_validation\n            print(\"lowest_loss:\", lowest_loss)\n            # é‡ã¿ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ã®ã¿ä¿å­˜ï¼ˆä¿å­˜ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªç•ªåœ°ãªã©ã¯ä¿å­˜ã—ãªã„ï¼‰\n            torch.save(model.state_dict(), '../fine-tuning')\n            \n        print(\"loss_validation:\", loss_validation)\n        print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    del train_loader, valid_loader, correct, accuracy, batch_i, positive_correct, negative_correct, positive_sum, positive_accuracy, negative_accuracy\n    gc.collect()\n    torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train, valid, valid_label, train_label, optimizer, loss_fn\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dict = {}\nfold_list = [0]\n#min_out = torch.tensor(-709, dtype=torch.float32).cuda()\n\ni_epoch = 2\nstart_time = time.time()\n\nmodel.load_state_dict(torch.load('../fine-tuning'))\nmodel.eval()\n#for param in model.parameters():\n    #print(param.requires_grad)\n\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n\n    test_loader = DynamicBucketIterator(test, \n                                        test_label,\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    test_pred = np.zeros(len(test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        sentence_feature = y_batch[:, -sentence_feature_size:]\n        index_batch = batch[4]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n        #                 print('x_features', torch.cuda.memory_allocated())\n        \n        with torch.no_grad():\n            _, bert_output = bert(*x_features, output_all_encoded_layers=False)\n        #print(bert_output.grad_fn)\n        \n        y_pred = model(bert_output, sentence_feature)\n        #                 print('after_prediction', torch.cuda.memory_allocated())\n        #y_pred = torch.where(y_pred < min_out, min_out, y_pred)\n        #print(y_pred)\n        test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_batch, segment_id_batch, input_mask_batch, index_batch, y_batch, sentence_feature, x_features, y_pred, bert_output\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = test_pred\n    #print(epoch_test_pred)\n    del model, test_loader\n    gc.collect()\n    torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = (torch.from_numpy(test_pred) >= 0.5).to(torch.float32).cuda()\ncorrect = (test_pred == test_label[:, 0]).to(torch.float32).sum(0)\naccuracy = correct / len(test_label)\nprint(accuracy)\n\ndel accuracy, correct, test_pred\ngc.collect()\ntorch.cuda.empty_cache()\nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#min_out = torch.tensor(-700, dtype=torch.float32)\n#y_pred = torch.tensor([-709, -710], dtype=torch.float32)\n#pred = torch.where(y_pred < min_out, min_out, y_pred)\n#sigmoid(pred.detach().cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')\n#df_submit.prediction = test_dict[0]\n#df_submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}