{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Toxicity EDA, work in progress\nBeginner here, trying to give my 0.02 cents on the data exploratory analysis, any comments/ feedbacks are welcome.\n\n## Content\n- <a href='#1'>Target Distribution</a>\n- <a href='#2'>Comment Length</a>\n- <a href='#3'>No. of Toxicity Annotators vs Comment Length</a>\n- <a href='#4'>Annotators Distribution</a> **(New, added on 29 Apr)**\n- <a href='#5'>Identity Distribution</a> **(New, added on 29 Apr)**\n- <a href='#6'>Toxic vs Non-toxic Words by Identity</a> **(New, added on 29 Apr)**\n\n\n## References:\n- Preprocessing - https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing/notebook (currently not used)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport string\nimport re\nimport time\nimport gc\nimport itertools\n\nfrom tqdm import tqdm\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom multiprocessing import Pool\nfrom matplotlib_venn import venn2\n\nplt.style.use('ggplot')\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Note: I'm using the custom train & test file I created, which contains the original cols + additional columns of POS tags \n\ntrain = pd.read_csv('../input/spacy-pos-tagging-12-workers/jigsaw_train_w_pos_tags.csv')\ntest = pd.read_csv('../input/spacy-pos-tagging-12-workers/jigsaw_test_w_pos_tags.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"toxic_subtypes = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\nidentities = ['asian', 'atheist', 'bisexual',\n       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white']\n\nselected_identities = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"# <a id='1'>Target Distribution</a>\n\nLooking at the raw target output (with bins = 10) distribution, a few things can be observed:\n- Majority of the comments are non-toxic (target = (0.0, 0.1])\n- Interesting bin (0.1 to 0.5) where some annotators find the comments toxic - we aren't sure how toxic they find these comments though.\n\nI'm guessing the bin (0.1 to 0.5) will be a critical area to look at, the comments in this bin are likely prone to misclassifications by the machine learning models. I shall work on a more in-depth analysis on this bin in subsequent versions."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplot = train.target.plot(kind='hist',bins=10)\n\nax = plot.axes\n\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height() * 100 / train.shape[0]:.2f}%',\n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha='center', \n                va='center', \n                fontsize=8, \n                color='black',\n                xytext=(0,7), \n                textcoords='offset points')\nplt.title('Target Distribution (Raw)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"And after binarization, our dataset becomes a **highly imbalaned**!"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Referene: benchmark kernel for the competition\n\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n\ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + selected_identities:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ntrain = convert_dataframe_to_bool(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplot = sns.countplot(x='target', data=pd.DataFrame(train['target'].map({True:'Toxic', False:'Non-toxic'}), columns=['target']))\n\nax = plot.axes\n\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height() * 100 / train.shape[0]:.2f}%',\n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha='center', \n                va='center', \n                fontsize=8, \n                color='black',\n                xytext=(0,7), \n                textcoords='offset points')\n    \nplt.title('Target Distribution (Binary)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>Comment Length</a>\n\nWe will analyse the comment length at word and character level.\n\n**Key Findings:**\n- Similar distribution for word and character level (just the way English is)\n- A peak at character length = 1000, and minimal data with length > 1000. \n\nThis is probably due to different character truncation selection during the data collection. Note that there are still some comments with length > 1000. Perhaps we should take special note of the truncated comments? The toxic may not occure before the truncation."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def preprocessing(titles_array, return_len = False):\n    \n    processed_array = []\n    \n    for title in tqdm(titles_array):\n        \n        # remove other non-alphabets symbols with space (i.e. keep only alphabets and whitespaces).\n        processed = re.sub('[^a-z ]', '', title.lower())\n        \n        words = processed.split()\n        \n        if return_len:\n            processed_array.append(len([word for word in words if word not in eng_stopwords]))\n        else:\n            processed_array.append(' '.join([word for word in words if word not in eng_stopwords]))\n    \n    return processed_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"eng_stopwords = []\n\nfor w in stopwords.words('english'):\n    processed = re.sub('[^a-z ]', '', w.lower())\n    eng_stopwords.append(processed)\n\neng_stopwords = set(eng_stopwords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train['comment_text_len'] = train['comment_text'].progress_apply(len)\ntest['comment_text_len'] = train['comment_text'].progress_apply(len)\n\ntrain['preprocessed_comment_len'] = preprocessing(train['comment_text'], return_len=True)\ntest['preprocessed_comment_len'] = preprocessing(test['comment_text'], return_len=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.subplot(121)\nsns.distplot(train['comment_text_len'], kde=False, bins=150, label='Train Set', norm_hist=True)\nsns.distplot(test['comment_text_len'], kde=False, bins=150, label='Test Set', norm_hist=True)\nplt.legend()\nplt.ylabel('Frequency')\nplt.title('Comment Text Length (char level)')\n\nplt.subplot(122)\nsns.distplot(train['preprocessed_comment_len'], kde=False, bins=150, label='Train Set', norm_hist=True)\nsns.distplot(test['preprocessed_comment_len'], kde=False, bins=150, label='Test Set', norm_hist=True)\nplt.legend()\nplt.ylabel('Frequency')\nplt.title('Comment Text Length (word level, simple preprocessing)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>No. of Toxicity Annotators vs Comment Length</a>\n\nThe main motivation is to check out how ~~diligent the annotators are~~ reliable (or should I say less biased?) the labels of the comments are. I would think that if a comment is labelled by more annotators, its target score is more reliable.\n\n**Key Findings:**\n- As the comment gets longer, the number of annotators get lesser\n\nBe wary of the 'correctness' of long comments, as these tend to have lesser annotators and thus a less reliable score."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Just wondering if how diligent our annotators are lol\n\nplt.figure(figsize=(20,6))\nplt.subplot(121)\nsns.scatterplot(x='preprocessed_comment_len', y='toxicity_annotator_count',data=train)\nplt.title('No. of Toxicity Annotators vs Comment Length (word level, simple pre-processing)')\n\nplt.subplot(122)\nsns.scatterplot(x='comment_text_len', y='toxicity_annotator_count',data=train)\nplt.title('No. of Toxicity Annotators vs Comment Length (char level)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\"\"\"\n# Preprocessing based on https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing/notebook\n# Currently not used\n\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text\n\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\ndef clean_text(text):\n    text = text.lower()\n    text = clean_contractions(text, contraction_mapping)\n    text = clean_special_chars(text, punct, punct_mapping)\n    text = correct_spelling(text, mispell_dict)\n    return text\n    \nn_partitions = 24\nn_workers = 8\n\ndef parallelize_dataframe(df, func):\n    df_split = np.array_split(df, n_partitions)\n    pool = Pool(n_workers)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\ndef text_processing(data):\n    data['processed_text'] = data['comment_text'].apply(clean_text)\n    gc.collect()\n    return data\n\ntrain = parallelize_dataframe(train, text_processing)\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>Annotators Distribution</a>\n\n**Key Findings:**\n- Identity annotator distribution peaks at around 1200, 1500, 1700, probably an artifact of how the annotations were conducted.\n- Toxicity annotator distribution peaks near 0, which means a lot of the comments have very little annotators (and hence less reliable?)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nplt.subplot(121)\nsns.kdeplot(train['toxicity_annotator_count'], color='red')\nplt.title('Toxicity Annotator Distribution')\nplt.subplot(122)\nsns.kdeplot(train['identity_annotator_count'], color='blue')\nplt.title('Identity Annotator Distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='5'>Identity Distribution</a>\n\n**Key Findings:**\n- Female, Male and Christian are the most common identities\n- For samples with identity_annotator_count > 0, 60% of them are not identified with the identities used for metrics\n- Of the remaining 40%, about 28% have a single identity, 9% have double identities\n- From the pair-wise venn diagram, it is observed that 'Male' and 'Female' tend to occur together, 'Black' and 'White' as well.\n\nIf you are going to work on BPSN and BNSP metrics on individual identities, it is wise to spend more time on the related pairs ('Black' & 'White', 'Male' & 'Female') as well as the most common identities."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for identity in selected_identities:\n    counts = train[identity].sum()\n    percentage = train[identity].sum() / train[identity].count() * 100\n    print(f'{identity:<30}: {percentage:.2f}% , {counts}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\ntrain['non_zero_identity_counts'] = np.count_nonzero(train[identities], axis=1)\ntrain.loc[train['identity_annotator_count'] == 0, 'non_zero_identity_counts'] = np.NaN\n\ntrain['non_zero_selected_identity_counts'] = np.count_nonzero(train[selected_identities], axis=1)\ntrain.loc[train['identity_annotator_count'] == 0, 'non_zero_selected_identity_counts'] = np.NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,6))\n\nnon_zero_selected_identity_counts = ~train['non_zero_selected_identity_counts'].isna(),'non_zero_selected_identity_counts'\n\nplot = sns.countplot(train.loc[non_zero_selected_identity_counts])\nax = plot.axes\n\ny_lim = 0\n\nfor p in ax.patches:\n    \n    if p.get_height() > y_lim:\n        y_lim = p.get_height()\n    \n    ax.annotate(f'{p.get_height() * 100 / train.loc[non_zero_selected_identity_counts].shape[0]:.3f}%\\n({p.get_height()})',\n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha='center', \n                va='center', \n                fontsize=11, \n                color='black',\n                xytext=(0,15), \n                textcoords='offset points')\n\nplt.ylim((0,round(y_lim*1.1)))\nplt.title('Number of Non-Zero Selected Identities for Each Sample (identity_annotator > 0)')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Check out how the selected identities are related to each other\n\nselected_identity_corr = train.loc[~train['non_zero_selected_identity_counts'].isna(), selected_identities].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mask = np.zeros_like(selected_identity_corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.style.use('default')\n\nplt.figure(figsize=(6,6))\nsns.heatmap(selected_identity_corr, \n            cmap = sns.diverging_palette(220, 10, as_cmap=True), \n            center=0, mask=mask, vmin=-1, vmax=1, annot=True, fmt='.2f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pair_identity_dict = dict(train.loc[~train['non_zero_selected_identity_counts'].isna(), selected_identities].sum().reset_index().values)\npair_keys = list(pair_identity_dict.keys())\npair_values = list(pair_identity_dict.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def venn_diagram_subplot(id1, id2):\n    overlap = train.loc[~train['non_zero_selected_identity_counts'].isna() & \\\n                                          (train[id1] == True) & \\\n                                          (train[id2] == True),\\\n                                          id1].count()\n    v = venn2(subsets = (pair_identity_dict[id1], pair_identity_dict[id2], overlap), set_labels = (id1[0:10], id2[0:10]))\n\n    return v","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"with plt.rc_context({\"axes.edgecolor\": 'red', \"axes.linewidth\": 3, \"font.size\":'14'}):\n    plt.figure(figsize=(20,40))\n    for i, (id1,id2) in enumerate(itertools.combinations(list(pair_identity_dict.keys()), 2)):\n        plt.subplot(9,4,i+1)\n        venn_diagram_subplot(id1,id2)\n        if ((id1,id2) == ('male','female')) or ((id1,id2) == ('black','white')):\n            plt.gca().set_axis_on()\n\n\n    plt.suptitle(f'Overlap of Identities', fontsize=25, fontweight='heavy')\n    plt.tight_layout(rect=[0, 0, 1, 0.98])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\"\"\"\nidentity = train.loc[~train['non_zero_selected_identity_counts'].isna(),'black']\n\nplot = sns.countplot(identity)\n\nax = plot.axes\ny_lim = 0\n\nfor p in ax.patches:\n    if p.get_height() > y_lim:\n        y_lim = p.get_height()\n\n    ax.annotate(f'{p.get_height() * 100 / identity.shape[0]:.2f}%',\n                (p.get_x() + p.get_width() / 2., p.get_height()), \n                ha='center', \n                va='center', \n                fontsize=11, \n                color='black',\n                xytext=(0,7), \n                textcoords='offset points')\n    \nplt.ylim((0,round(y_lim*1.1)))\nplt.show()\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='6'>Toxic vs Non-toxic Words by Identity</a>\n\nFor each of the selected identities, the word frequency distributions of the **adjectives, nouns, proper nouns and verbs** (which are chosen as they tend to convey more semantics and sentiments) are obtained and plotted. \n\nTop 10 words that are associated with the toxic samples are plotted. To check if the non-toxic word frequency distributions for the identity is similar, the normalized frequency is plotted (out of vocab word frequency are set to 0)\n\n**Key Findings:**\n- 30% of the samples identified with Homosexual_gay_or_lesbian, Black, White are toxic, which is much higher than the remaining identities\n- Toxic and Non-toxic words frequency distribution for Male, Female and Christian have more variations, which means traditional bags-of-words approach (Tf-Idf, Count Vector) can probably distinguish between the toxic and non-toxic samples\n\nFor the remaining identities with similar words frequency distirbution for toxic and non-toxic samples, other strategies will likely be needed if you were to work on the sub-group AUC. Off my mind, I can think of the following methods that could help:\n- Negation tagging \n- Sentence structure analysis\n- Dependency parsing"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_pos_neg_string(identity, pos_query_criteria, neg_query_criteria, return_cols = ['adj','noun','propn','verb']):\n    \n    pos_word_list = [row[0] for row in train.loc[pos_query_criteria,return_cols].values if type(row[0]) != float]\n    neg_word_list = [row[0] for row in train.loc[neg_query_criteria,return_cols].values if type(row[0]) != float]\n    pos_string = ' '.join(pos_word_list)\n    neg_string = ' '.join(neg_word_list)\n    pos_string = re.sub('[^a-zA-Z ]', '', pos_string)\n    neg_string = re.sub('[^a-zA-Z ]', '', neg_string)\n    \n    return pos_string, neg_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_word_freq_and_plot(identity, most_freq_word_count=10):\n\n    # Set the criteria for dataframe query\n    pos_query_criteria = ~train['non_zero_selected_identity_counts'].isna() & train[identity] & train['target']\n    neg_query_criteria = ~train['non_zero_selected_identity_counts'].isna() & train[identity] & ~train['target']\n\n    # get the concatenated string for both positive (toxic) and negative (non-toxic) samples for an identity\n    pos_string, neg_string = get_pos_neg_string(identity, pos_query_criteria, neg_query_criteria)\n    \n    pos_freq_dist = FreqDist([word for word in pos_string.split()])\n    neg_freq_dist = FreqDist([word for word in neg_string.split()])\n    \n    pos_words, pos_word_count = list(zip(*pos_freq_dist.most_common(most_freq_word_count)))\n\n    pos_row_count = train.loc[pos_query_criteria,'target'].shape[0]\n    neg_row_count = train.loc[neg_query_criteria,'target'].shape[0]\n\n    # as negative samples are much larger in population, there is a need to normalize them to the positive sample size\n    neg_word_count_normalized = [int(neg_freq_dist.get(w) * pos_row_count / neg_row_count) \\\n                                 if neg_freq_dist.get(w) != None else 0 \\\n                                 for w in pos_words]\n\n    words_freq_df = pd.DataFrame(list(zip(pos_words,pos_word_count,neg_word_count_normalized)), \n                                 columns=['vocab','pos_freq','neg_freq_norm'])\n    \n    toxic_cloud = WordCloud(background_color='Black', \n                              colormap='Paired', \n                              width=600, \n                              height=700, \n                              random_state=123).generate_from_frequencies(pos_freq_dist)\n\n    non_toxic_cloud = WordCloud(background_color='White', \n                              #colormap='Paired', \n                              width=600, \n                              height=700, \n                              random_state=123).generate_from_frequencies(neg_freq_dist)\n\n    plt.figure(figsize=(18,8))\n    # Word Cloud plot\n    plt.subplot(131)\n    plt.imshow(toxic_cloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Toxic', fontsize=20)\n    \n    plt.subplot(132)\n    plt.imshow(non_toxic_cloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Non-toxic', fontsize=20)\n    \n    # Line plot of the term frequencies\n    plt.subplot(133)\n    sns.lineplot(x='vocab',y='pos_freq',data=words_freq_df, sort=False, marker='o', label='Toxic')\n    sns.lineplot(x='vocab',y='neg_freq_norm',data=words_freq_df, sort=False, marker='o', label='Non-toxic\\n(normalized)', alpha=0.8)\n    plt.legend()\n    plt.xticks(rotation=90)\n    plt.grid(b=True, which='major', axis='x', linestyle='--')\n    plt.ylabel('Term Frequency')\n    plt.title(f'FreqDist (Top {most_freq_word_count} words)', fontsize=20)\n    \n    plt.suptitle(f'Identity : {str.capitalize(identity)}, {pos_row_count/(pos_row_count + neg_row_count)*100:.2f}% toxic', \n                 fontsize=25, fontweight='heavy', ha='center')\n    plt.tight_layout(rect=[0,0,1,0.93])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for identity in selected_identities:\n    get_word_freq_and_plot(identity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### To be continued, let me know your thoughts and give me an upvote if you find the EDA useful/ interesting ;)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}