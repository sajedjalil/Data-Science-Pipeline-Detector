{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport gc\nimport unicodedata\nimport six\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport random\nimport keras\nimport tensorflow as tf\nimport json\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nK.set_epsilon(1e-7)\nimport re\nimport codecs\nimport sys\nimport string\nimport codecs\nimport numpy as np\nimport re\nimport pandas as pd\nfrom tqdm import *\nsys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\nfrom keras_bert.keras_bert.bert import get_model\nimport tokenization \nfrom keras_bert.keras_bert import get_custom_objects\n#from keras_bert.keras_bert.optimizers import AdamWarmup\n\n\ndef bert_get_result():\n    maxlen = 220\n    bsz=512\n    print('begin_build')\n    def checkpoint_loader(checkpoint_file):\n        def _loader(name):\n            return tf.train.load_variable(checkpoint_file, name)\n        return _loader\n\n\n    def load_trained_model_from_checkpoint(config_file,\n                                      #checkpoint_file,\n                                           training=False,\n                                           seq_len=None):\n        \"\"\"Load trained official model from checkpoint.\n        :param config_file: The path to the JSON configuration file.\n        :param checkpoint_file: The path to the checkpoint files, should end with '.ckpt'.\n        :param training: If training, the whole model will be returned.\n                         Otherwise, the MLM and NSP parts will be ignored.\n        :param seq_len: If it is not None and it is shorter than the value in the config file, the weights in\n                        position embeddings will be sliced to fit the new length.\n        :return:\n        \"\"\"\n        with open(config_file, 'r') as reader:\n            config = json.loads(reader.read())\n        if seq_len is None:\n            seq_len = config['max_position_embeddings']\n        else:\n            seq_len = min(seq_len, config['max_position_embeddings'])\n        #loader = checkpoint_loader(checkpoint_file)\n        model = get_model(\n            token_num=config['vocab_size'],\n            pos_num=seq_len,\n            seq_len=seq_len,\n            embed_dim=config['hidden_size'],\n            transformer_num=config['num_hidden_layers'],\n            head_num=config['num_attention_heads'],\n            feed_forward_dim=config['intermediate_size'],\n            training=training,\n        )\n        if not training:\n            inputs, outputs = model\n            model = keras.models.Model(inputs=inputs, outputs=outputs)\n\n        return model\n\n    def convert_lines(example, max_seq_length,tokenizer):\n        max_seq_length -=2\n        all_tokens = []\n        longer = 0\n        for i in tqdm(range(len(example))):\n          tokens_aa = tokenizer.tokenize(example[i])\n          if len(tokens_aa)>max_seq_length:\n            tokens_a = tokens_aa[:int(max_seq_length/2)]+tokens_aa[-int(max_seq_length/2):]\n            longer += 1\n            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n            all_tokens.append(one_token)\n          else:\n            one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_aa+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_aa))\n            all_tokens.append(one_token)\n        print(longer)\n        return np.array(all_tokens)\n\n    test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')#[:1024]#.sample(512*2)\n    #test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    \n      \n    ###\n    ###\n    ## new models\n    print('new models')\n    symbols_to_delete = 'â†’â˜…Â©Â®â—Ëâ˜†Â¶ï¼‰Ð¸Ê¿ã€‚ï¬‚ï¬â‚â™­å¹´â–ªâ†Ê’ã€ï¼ˆæœˆâ– â‡ŒÉ¹Ë¤Â³ã®Â¤â€¿Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ê€É´×©×œ×•××‘×™ã‚¨ãƒ³á´µ××¢×›×—â€Î¹ÎºÎ¾ØªØ­ÙƒØ³Ø©ÙØ²Ø·â€‘åœ°è°·ÑƒÐ»ÐºÐ½Ð¾×”æ­ŒÐ¼Ï…Ñ‚ÑÐ¿Ñ€Ð´Ë¢áµ’Ê³Ê¸á´ºÊ·áµ—Ê°áµ‰áµ˜Î¿Ï‚×ª×ž×“×£× ×¨×š×¦×˜æˆéƒ½ÐµÑ…å°åœŸã€‹à¤•à¤°à¤®à¤¾è‹±æ–‡ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººÐ±ÑŒÑ‹Ð³Ñä¸ã¤Ð·Ñ†ä¼šä¸‹æœ‰çš„åŠ å¤§å­ãƒ„Ø´Ø¡Ê²ÑˆÑ‡ÑŽÐ¶æˆ¦Ñ‰æ˜Ž×§Ñ™Ñ›æˆ‘å‡ºç”Ÿå¤©ä¸€å®¶æ–°ÊÕ½Õ°×ŸØ¬Ñ–â€’å…¬ç¾Žé˜¿×¡×¤ç™½ãƒžãƒ«ãƒãƒ‹ãƒãƒ­ç¤¾Î¶å’Œä¸­æ³•æœ¬å£«ç›¸ä¿¡æ”¿æ²»å ‚ç‰ˆã£Ñ„Ú†ÛŒãƒªäº‹ã€Œã€ã‚·Ï‡ÏˆÕ´Õ¥Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Ú©ã€Šáƒšã•ã‚ˆã†ãªã‚‰Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ê€É´×©×œ×•××‘×™ã‚¨ãƒ³á´µ××¢×›×—â€Î¹ÎºÎ¾ØªØ­ÙƒØ³Ø©ÙØ²Ø·â€‘åœ°è°·ÑƒÐ»ÐºÐ½Ð¾×”æ­ŒÐ¼Ï…Ñ‚ÑÐ¿Ñ€Ð´Ë¢áµ’Ê³Ê¸á´ºÊ·áµ—Ê°áµ‰áµ˜Î¿Ï‚×ª×ž×“×£× ×¨×š×¦×˜æˆéƒ½ÐµÑ…å°åœŸã€‹à¤•à¤°à¤®à¤¾è‹±æ–‡ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººÐ±ÑŒÑ‹Ð³Ñä¸ã¤Ð·Ñ†ä¼šä¸‹æœ‰çš„åŠ å¤§å­ãƒ„Ø´Ø¡Ê²ÑˆÑ‡ÑŽÐ¶æˆ¦Ñ‰æ˜Ž×§Ñ™Ñ›æˆ‘å‡ºç”Ÿå¤©ä¸€å®¶æ–°ÊÕ½Õ°×ŸØ¬Ñ–â€’å…¬ç¾Žé˜¿×¡×¤ç™½ãƒžãƒ«ãƒãƒ‹ãƒãƒ­ç¤¾Î¶å’Œä¸­æ³•æœ¬å£«ç›¸ä¿¡æ”¿æ²»å ‚ç‰ˆã£Ñ„Ú†ÛŒãƒªäº‹ã€Œã€ã‚·Ï‡ÏˆÕ´Õ¥Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Ú©ã€Šáƒšã•ã‚ˆã†ãªã‚‰\\nï¼¼ðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014â‰ \\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶â¤ï¸â˜º\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø£ðŸ˜ðŸ’–Ì¶ðŸ’µâ¥â”â”£â”«Ð•â”—ï¼¯â–ºðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´Êá´‡á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢âœ”\\x96\\x92ðŸ˜‹ðŸ‘ðŸ˜±â€¼\\x81ã‚¸æ•…éšœâž¤\\u2009ðŸšŒÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜â˜•â™¡â—â•‘â–¬ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œÛ©ÛžðŸ˜–á´ ðŸš²âœ’âž¥ðŸ˜ŸðŸ˜ˆâ•ËŒðŸ’ªðŸ™ðŸŽ¯â—„ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®á½²á¼€Î¯á¿ƒá¼´ðŸ™„âœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼¨ï¼©ï¼´ðŸ˜ \\ufeffâ˜»\\u2028ðŸ˜‰ðŸ˜¤â›ºâ™ðŸ™‚\\u3000ðŸ‘®ðŸ’™ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†âœ“â—¾ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08ØŸðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨â¬…ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šç„â„…Ð’ÐŸÐÐðŸ¾ðŸ•â£ðŸ˜†ðŸ”—ðŸš½èˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸â™«Ñ•ï¼£ï¼­â¤µðŸ†ðŸŽƒðŸ˜©â–ˆâ–“â–’â–‘\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’Ž\\x95ðŸ–ðŸ™…â›²ðŸ°â­ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004â§â–°â–”á´¼á´·â—žâ–€\\x13ðŸš¬â–‚â–ƒâ–„â–…â–†â–‡â†™ðŸ¤“\\ue602ðŸ˜µÎ¬ÏŒÎ­á½¸Ì„ðŸ˜’Íâ˜¹âž¡ðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7âœ‹\\uf04c\\x9f\\x10ðŸ˜£âºÌ²Ì…ðŸ˜ŒðŸ¤‘ÌðŸŒðŸ˜¯ðŸ˜²âˆ™â€›á¼¸á¾¶á½ðŸ’žðŸš“â—‡ðŸ””ðŸ“šâœðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613è±†ðŸ¡â–·â”â“â‰â—\\u202fðŸ‘ à¥ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡ðŸŒžËšðŸŽ²ðŸ˜›Ë™å…³ç³»Ð¡ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽâœ¨æ˜¯\\x80\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼â—•à¼½ðŸ˜°á¸·Ð—â–±ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®é™ï¼…ä½ å¤±åŽ»æ‰€é’±æ‹¿åç¨Žéª—ðŸÂ¯ðŸŽ…\\x85ðŸºØ¢Ø¥ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´âŒ Ð˜ÐžÐ Ð¤Ð”Ð¯Ðœâœ˜ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œç¾¤â•ªðŸ’¨åœ†å›­â–¶â„â˜­âœ­ðŸˆðŸ˜ºâ™ªðŸŒâá»‡ðŸ”ðŸ®ðŸâ˜”ðŸ†ðŸ‘ðŸŒ®ðŸŒ¯â˜ ðŸ¤¦\\u200dâ™‚ð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–ÐšðŸ€ðŸ˜«ðŸ¤¤á¿¦åœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºâ˜ƒðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜â˜ŽðŸ¤ ðŸ‘©âœˆðŸ–’âœŒâœ°â†â˜™ðŸšªâš²\\u2006âš­âš†â¬­â¬¯â–â—‹â€£âš“âˆŽâ„’â–™â˜â…›âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹ð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºï½ƒÏ–\\u2000Ò¯ï½á´¦áŽ¥Ò»Íº\\u2007ï½“Ç€\\u2001É©â„®ï½™ï½…àµ¦ï½ŒÆ½Â¸ï½—ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨á‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹âˆ¼ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹â„³ð€ð¥ðªâ„ðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸â˜¼íŒ¨í‹°ï¼·â‹†ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Í¡à¹Ì¯ï´¿âš¾âš½Î¦â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›ðŸŽ¾ðŸ‘¹ï¿¦âŽŒðŸ’â›¸å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ð’‘ð’šð’ð‘´ðŸ¤™ðŸ’â„ƒæ¬¢è¿Žæ¥åˆ°æ‹‰æ–¯ð™«â©â˜®ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»âš ðŸ¦„å·¨æ”¶èµ¢å¾—é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—âœŠðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·âŒâ­•â–¸ð—¢ðŸ³ðŸ±ðŸ¬â¦æ ªå¼â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šâ˜â˜‘å¤šä¼¦âš¡â˜„Ç«ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹â•­â•®ðŸ·ðŸ¦†ä¸ºå‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœä¼ æ•™æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›é¢˜æ—¶å€™ä¾‹æˆ˜èƒœå› åœ£æŠŠå…¨ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ï¼žÊ•Ì£Î”ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©âœžðŸ”«ðŸ‘â”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ˜’å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿â˜ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘âœ…â˜›ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨â™©ðŸŽðŸ¤žâ˜žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ðŸ­ð‘¥ð‘¦ð‘§ï¼¡ï¼®ï¼§ï¼ªï¼¢ðŸ‘£\\uf020â—”â—¡ðŸ‰ðŸ’­ðŸŽ¥â™€ÎžðŸ´ðŸ‘¨ðŸ¤³â¬†ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ì±â„ð‘®ð—•ð—´\\x91ðŸ’â €êœ¥â²£â²â•šðŸ‘â°â†ºâ‡¤âˆé‰„ä»¶âœ¾â—¦â™¬Ñ—ðŸ’Š\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½è™šå½å±ç†å±ˆï½œÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤âˆµâˆ´ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡á¿–Î›Î©â¤ðŸ‡³ð’™ÕÕ¼Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«â˜œÎ’ÏŽðŸ’¢â–²ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´â†³ðŸ’’âŠ˜â–«È»â¬‡ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽâœ§ðŸ˜¼ðŸ•·ï½‡ï½ï½–ï½’ï½Žï½ï½”ï½‰ï½„ï½•ï¼’ï¼ï¼˜ï½†ï½‚ï¼‡ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦âˆ•ðŸŒˆðŸ”­ðŸŠðŸ\\uf10aË†âšœâ˜Ú¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œðŸ”¼'\n    symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±É¡|Â¢`â€•ÉªÂ£â™¥Â´Â¹â‰ˆÃ·â€²É”â‚¬â€ Î¼Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½ÊƒÂ±ÂµÂºÂ¾ï¼ŽÂ»Ð°Ð²â‹…Â¿Â¬Î²â‡’â€ºÂ¡â‚‚â‚ƒÎ³â€³Â«Ï†â…“â€žï¼šÂ¥ÑÉ‘ï¼âˆ’Â²ÊŒÂ¼â´â„â‚„â€šâ€–âŠ‚â…”Â¨Ã—Î¸ï¼Ÿâˆ©ï¼ŒÉâ‚€â‰¥â†‘â†“ï¼âˆšï¼â€°â‰¤'\n\n    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n    \n    abbr_mapping = {'á´€':'a','Ê™':'b','á´„':'c','á´…':'d','á´‡':'e','êœ°':'f','É¢':'g','Êœ':'h',\n                      'Éª':'i','á´Š':'j','á´‹':'k','ÊŸ':'l','á´':'m','É´':'n','á´':'o','á´˜':'p',\n                      'Ç«':'q','Ê€':'r','êœ±':'s','á´›':'t','á´œ':'u','á´ ':'v','á´¡':'w','Ê':'y','á´¢':'z', '\\n':' ',\n                      'u.s.a.': 'usa', 'u.s.a': 'usa', 'u.s.': 'usa',  ' u.s ': ' usa ','u s of a': 'usa',\n                      ' u.k. ': 'uk', ' u.k ': ' uk ', ' yr old ': ' years old ',\n                      ' yrs old ': ' years old ',' ph.d ': ' phd ','kim jong-un': 'the president of north korea',\n                      '#metoo': 'metoo', 'trumpster': 'trump supporter','trumper': 'trump supporter',\n                      'trumpian':'trump supporter','trumpism':'trump supporter',\"trump's\" : 'trump',\n                      ' u r ': ' you are ',  'e.g.': 'for example','i.e.': 'in other words',\n                      'et.al': 'elsewhere', 'antisemitic': 'anti-semitic','sb91':'senate bill',                                   \n                      }\n\n\n    contraction_mapping = {\n        \"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n        'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',\"aren't\": 'are not',\n        'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',\"can't\": 'cannot',\n        \"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have', 'can;t': 'cannot','can;t;ve': 'cannot have',\n        'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',\n        \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\n        \"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n        'couldn;t;ve': 'could not have','couldnÂ´t': 'could not', 'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not',\n        'couldnâ€™tâ€™ve': 'could not have', 'couldÂ´ve': 'could have',\n        'couldâ€™ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',\n        'didnâ€™t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',\n        'doesnâ€™t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',\n        \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n        'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not',\n        'hadnâ€™tâ€™ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not',\n        'hasnâ€™t': 'has not', \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not',\n        'havenâ€™t': 'have not',\"he'd\": 'he would', \"he'd've\": 'he would have',\"he'll\": 'he will','heÂ´ll': 'he will',\n        \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',   \n        'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have',    \n        'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',\n        \"how'd\": 'how did',\"how'll\": 'how will',\"how's\": 'how is','how,d': 'how did','how,ll': 'how will',\n        'how,s': 'how is','how;d': 'how did','how;ll': 'how will','how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will',\n        'howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will','howâ€™s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\n        \"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will','i,m': 'i am','i,ve': 'i have','i;d': 'i would',\n        'i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not','isn,t': 'is not','isn;t': 'is not',\n        'isnÂ´t': 'is not','isnâ€™t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is', \"it's\": 'it is',\n        'it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will', 'it;s': 'it is',\n        'itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is','itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',\n        'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',\n        'iâ€™ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us', 'letâ€™s': 'let us',\n        \"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not', 'mayn;t': 'may not',\n        'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',\"might've\": 'might have',\n        'might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not',\n        'mightn;t': 'might not','mightnÂ´t': 'might not', 'mightnâ€™t': 'might not','mightÂ´ve': 'might have',\n        'mightâ€™ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n        \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not',\n        'mustÂ´ve': 'must have','mustâ€™ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not',\n        'neednÂ´t': 'need not','neednâ€™t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n        'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\n        \"shan't\": 'shall not', 'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not',\n        'shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',\"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is',\n        'she,d': 'she would','she,ll': 'she will', 'she,s': 'she is','she;d': 'she would','she;ll': 'she will',\n        'she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will', 'sheÂ´s': 'she is','sheâ€™d': 'she would',\n        'sheâ€™ll': 'she will','sheâ€™s': 'she is',\"should've\": 'should have','should,ve': 'should have',\n        'should;ve': 'should have', \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not',\n        'shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have', 'shouldâ€™ve': 'should have',\n        \"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n        'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',\n        \"there'd\": 'there had', \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had',\n        'there;s': 'there is', 'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',\n        \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have','they,d': 'they would',\n        'they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will',\n        'they;re': 'they are', 'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are',\n        'theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will','theyâ€™re': 'they are','theyâ€™ve': 'they have',\n        \"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not','wasnâ€™t': 'was not',\n        \"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n        'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n        \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not',\n        'weÂ´d': 'we would','weÂ´ll': 'we will',    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would',\n        'weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\n        \"what's\": 'what is',    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is',\n        'what,ve': 'what have','what;ll': 'what will','what;re': 'what are','what;s': 'what is','what;ve': 'what have',\n        'whatÂ´ll': 'what will', 'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will',\n        'whatâ€™re': 'what are','whatâ€™s': 'what is', 'whatâ€™ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is',\n        'where,d': 'where did','where,s': 'where is','where;d': 'where did','where;s': 'where is','whereÂ´d': 'where did',\n        'whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is', \"who'll\": 'who will',\"who's\": 'who is',\n        'who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is','whoÂ´ll': 'who will','whoÂ´s': 'who is',\n        'whoâ€™ll': 'who will','whoâ€™s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n        'wonÂ´t': 'will not','wonâ€™t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not',\n        'wouldnÂ´t': 'would not','wouldnâ€™t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are',\n        'you,d': 'you would','you,ll': 'you will', 'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n        'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would',\n        'youâ€™ll': 'you will','youâ€™re': 'you are','Â´cause': 'because','â€™cause': 'because',\"you've\": \"you have\",\n        \"could'nt\": 'could not',\"havn't\": 'have not',\"hereâ€™s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\n        \"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\"who're\": \"who are\",\"who've\": \"who have\",\n        \"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n        \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",\n        'don\"\"t': \"do not\",\"dosen't\": \"does not\", \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\n        \"there'll\": \"there will\",\"there're\": \"there are\", \"this'll\": \"this all\",\" u're\": \" you are\", \"ya'll\": \"you all\",\n        \"you'r \": \"you are \",\"youâ€™ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\n        \"dont't\": \"do not\",\"gov't\": \"government\",\"i'ma\": \"i am\",\"is'nt\": \"is not\",\"â€˜i\":'i',  \":)\": ' smile ',\n        \":-)\": ' smile ','â€¦':'...', 'ðŸ˜‰': ' wink ', 'ðŸ˜‚': ' joy ', 'ðŸ˜€': ' stuck out tongue ',  \n         }\n\n    dirty_dict = {      \n                        re.compile( '[^a-zA-Z][uU] of [oO][^a-zA-Z]'): ' you of all ',\n                        re.compile('[wW][hH][^a-zA-Z ][^a-zA-Z ][eE]'):'whore' ,                  #  wh**e\n                        re.compile('[wW][hH][^a-zA-Z ][rR][eE]'):'whore',                         #  wh*re   \n                        re.compile('[wW][^a-zA-Z ][oO][rR][eE]'):'whore',                         #  w*ore  \n                        '[wW] h o r e':'whore',\n                      #  re.compile('[sS][hH][^a-zA-Z ][tT] '):'shit ',                         #   sh*t_\n                        re.compile(' [sS][hH][^a-zA-Z ][tT]'):' shit',                         #   _sh*t\n                        re.compile('[sS][hH][*_x][tT] '):'shit ',                            #   sh*t\n                        re.compile(' [sS][^a-zA-Z ][^a-zA-Z ][tT]'):' shit',                   #   _s**t\n                      #  re.compile('[sS][^a-zA-Z ][^a-zA-Z ][tT] '):'shit ',                   #   s**t_\n                        re.compile('[sS][-*_x][-*_x][tT] '):'shit ',                      #   s**t\n                      #  re.compile('[sS][hH][^a-zA-Z ][^a-zA-Z ] '):'shit ',                    #   sh**_ \n                        re.compile(' [sS][hH][^a-zA-Z ][^a-zA-Z ]'):' shit',                     #   _sh** \n                      #  re.compile('[sS][^a-zA-Z ][iI][tT] '):'shit ',                         #   s*it_   \n                        re.compile(' [sS][^a-zA-Z ][iI][tT]'):' shit',                         #   _s*it \n                        re.compile('[sS][-*_x][iI][tT] '):'shit ',                            #   shit\n                        '[sS] h i t':'shit','5h1t': 'shit',\n                        re.compile(' [fF][^a-zA-Z ][^a-zA-Z ][kK]'):' fuck',                   #   _f**k\n                        re.compile('[fF][^a-zA-Z ][^a-zA-Z ][kK] '):'fuck ',                   #   f**k_\n                        re.compile('[fF][-*_x][-*_x][kK]'):'fuck',                       #   f**k\n                        re.compile(' [fF][^a-zA-Z ][cC][kK]'):' fuck',                         #   _f*ck\n                        re.compile('[fF][^a-zA-Z ][cC][kK] '):'fuck ',                         #   f*ck_\n                        re.compile('[fF][-*_x][cC][kK]'):'fuck',                            #   f*ck\n                        re.compile(' [fF][uU][^a-zA-Z ][kK]'):' fuck',                         #   _fu*k\n                        re.compile('[fF][uU][^a-zA-Z ][kK] '):'fuck ',                         #   fu*k_\n                        re.compile('[fF][uU][-*_x][kK]'):'fuck',                            #   fu*k\n                        '[pP]huk': 'fuck','[pP]huck': 'fuck','[fF]ukk':'fuck','[fF] u c k':'fuck',\n                        '[fF]cuk': 'fuck',' [fF]uks': ' fucks',              \n                        re.compile(' [dD][^a-zA-Z ][^a-zA-Z ][kK]'):' dick',                   #   _d**k\n                        re.compile('[dD][^a-zA-Z ][^a-zA-Z ][kK] '):'dick ',                   #   d**k_\n                        re.compile('[dD][-*_x][-*_x][kK]'):'dick',                       #   d**k\n                        re.compile(' [dD][^a-zA-Z ][cC][kK]'):' dick',                         #   _d*ck\n                        re.compile('[dD][^a-zA-Z ][cC][kK] '):'dick ',                         #   d*ck_\n                        re.compile('[dD][-*_x][cC][kK]'):'dick',                            #   d*ck\n                        re.compile(' [dD][iI][^a-zA-Z ][kK]'):' dick',                         #   _di*k\n                        re.compile('[dD][iI][^a-zA-Z ][kK] '):'dick ',                         #   di*k_\n                        re.compile('[dD][iI][-*_x][kK]'):'dick',                            #   di*k\n\n                        re.compile(' [sS][^a-zA-Z ][cC][kK]'):' suck',                         #   _s*ck\n                        re.compile('[sS][^a-zA-Z ][cC][kK] '):'suck ',                         #   s*ck_\n                        re.compile('[sS][-*_x][cC][kK]'):'suck',                            #   s*ck\n                        re.compile(' [sS][uU][^a-zA-Z ][kK]'):' suck',                         #   _su*k\n                        re.compile('[sS][uU][^a-zA-Z ][kK] '):'suck ',                         #   su*k_\n                        re.compile('[sS][uU][-*_x][kK]'):'suck',                            #   su*k\n\n                        re.compile(' [cC][^a-zA-Z ][nN][tT]'):' cunt',                         #   _c*nt\n                        re.compile('[cC][^a-zA-Z ][nN][tT] '):'cunt ',                         #   c*nt_\n                        re.compile('[cC][-*_x][nN][tT]'):'cunt',                            #   c*nt\n                        re.compile(' [cC][uU][^a-zA-Z ][tT]'):' cunt',                         #   _cu*t\n                        re.compile('[cC][uU][^a-zA-Z ][tT] '):'cunt ',                         #   cu*t_\n                        re.compile('[cC][uU][-*_x][tT]'):'cunt',                            #   cu*t\n\n                        re.compile(' [bB][^a-zA-Z ][tT][cC][hH]'):' bitch',                       #   _b*tch\n                        re.compile('[bB][^a-zA-Z ][tT][cC][hH] '):'bitch ',                       #   b*tch_\n                        re.compile('[bB][-*_x][tT][cC][hH]'):'bitch',                          #   b*tch\n                        re.compile(' [bB][iI][^a-zA-Z ][cC][hH]'):' bitch',                       #   _bi*ch\n                        re.compile('[bB][iI][^a-zA-Z ][cC][hH] '):'bitch ',                       #   bi*ch_\n                        re.compile('[bB][iI][-*_x][cC][hH]'):'bitch',                          #   bi*ch\n                        re.compile(' [bB][iI][tT][^a-zA-Z ][hH]'):' bitch',                       #   _bit*h\n                        re.compile('[bB][iI][tT][^a-zA-Z ][hH]'):'bitch ',                       #   bit*h_\n                        re.compile('[bB][iI][tT][-*_x][hH]'):'bitch',                          #   bit*h\n                        re.compile('[bB][^a-zA-Z ][tT][^a-zA-Z ][hH]'):'bitch',                   #   b*t*h\n                        'b[-*_x][-*_x][-*_x]h':'bitch',                                          #   b***h\n                        '[bB] i t c h':'bitch',\n                        re.compile('[aA][*_]s'):'ass',                                #   a*s\n                        re.compile('[aA][^a-zA-Z ][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',               #   a**hole\n                        re.compile(' [aA][^a-zA-Z ][^a-zA-Z ][hH]'):' assh',                   #   a**h\n                        re.compile('[aA][^a-zA-Z ][sS][hH][oO][lL][eE]'):'asshole',                     #   a*shole\n                        re.compile('[aA][sS][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',                     #   as*hole\n                        ' [aA]s[*]':' ass','[aA] s s': 'ass ','[aA]sswhole': 'ass hole',\n                        re.compile('[aA]ssh[^a-zA-Z ]le'):'asshole',                     #   assh*le\n                        '[hH] o l e':'hole',\n                        '[bB][*]ll': 'bull', \n                        re.compile('[pP][^a-zA-Z ][sS][sS][yY]'):' pussy',                         #   p*ssy\n                        re.compile('[pP][uU][^a-zA-Z ][sS][yY]'):' pussy',                         #   pu*sy\n                        re.compile('[pP][uU][sS][^a-zA-Z ][yY]'):' pussy',                         #   pus*y\n                        re.compile('[pP][uU][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',                   #   pu**y\n                        re.compile('[pP][^a-zA-Z ][^a-zA-Z ][sS][yY]'):' pussy',                   #   p**sy\n                        re.compile(' [pP][^a-zA-Z ][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',            #   _pussy\n                        '[pP]ussi': 'pussy', '[pP]ussies': 'pussy','[pP]ussys': 'pussy', \n                        '[jJ]ack[-]off': 'jerk off','[mM]asterbat[*]': 'masterbate','[gG]od[-]dam': 'god damm',\n\n              }\n\n\n    new_final_mapping = { 'jackoff': 'jerk off','jerkoff':'jerk off','bestial': 'beastial',\n                         'bestiality': 'beastiality', 'd1ck': 'dick', 'lmfao': 'laughing my fucking ass off',\n                          'masturbate': 'masterbate', 'cashap24':'cash app','nurnie':'pussy',\n                         'n1gger': 'nigger', 'nigga': 'nigger', 'niggas': 'niggers',\n                         'clickbait':'click with bait','yuge':'huge','outsider77':'outsider',\n                         'numbnuts': 'noob nuts', 'orgasms': 'orgasm', 'trudope':'the prime minister of canada',\n                          'daesh':'isis', \"qur'an\":'the central religious text of islam','gofundme':'go fund me',\n                         'finicum':'an american spokesman','trumpkins':'trump with pumpkin',\n                           'trumpcare':'trump health care','obamacare':'obama health care','trumpy':'trump',\n                          'trumpster': 'trump supporter','trumper': 'trump supporter','trumpettes':'trump',\n                         'realdonaldtrump':'real donald trump','trumpeteer[s]?':'trump supporter',\n                          'trumpian':'trump supporter','trumpism':'trump supporter',\"trump[']s\" : 'trump',\n                         'trumplethinskin':'trump','trumpo':'trump','trumpies':'trump',\n                          'kim jong([- ]?un)?': 'the president of north korea','cheetolini':'trump',\n                          'trumpland':'trump land','trumpty':'trump','trumpist[s]?':'trump supporter',\n                          ' brotherin ':' brother ', 'beyak':'canadian politician',\n                          'trudeaus':'prime minister of canada ','shibai':'failure',\n                          'tridentinus':'tridentinum','zupta[s]?':'the south african president',\n                           'putrumpski':'putin and trump supporter','twitler':'twitter user',\n                           'antisemitic': 'anti semitic', 'sb91':'senate bill', \n                            'utmterm':' utm term','fakenews':'fake news',  'thedonald':'the donald',               \n                            'washingtontimes':'washington times','garycrum':'gary crum',\n                            'rangermc':'car','tfws':'tuition fee waiver','sjw?':'social justice warrior',\n                            'koncerned':'concerned','vinis':'vinys','Yá´á´œ':'you', 'auwe': 'oh no',\n                            'bigly':'big league','drump[f]?':'trump','brexit':'british exit',\n                            'utilitas':'utilities','justiciaries': 'justiciary','doctrne':'doctrine',\n                           'deplorables': 'deplorable','conartist' : 'con-artist','pizzagate':'pizza gate',\n                           'theglobeandmail': 'the globe and mail', 'howhat': 'how that', ' coz ':' because ',\n                           'civilbeat':'civil beat','gubmit':'submit','financialpost':'financial post',               \n                           'theguardian': 'the guardian','shopo':'shop','fentayal': 'fentanyl',\n                         'designation-': 'designation ','mutilitated' : 'mutilated','dood-': 'dood ',\n                         'irakis' : 'iraki', 'supporter[a-z]?':'supporter',' u ':' you ', \n                        }\n\n\n    def pre_clean_abbr_words(x, dic = abbr_mapping):\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n    def correct_contraction(x, dic = contraction_mapping):\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n\n    def clean_dirty_dict(x, dic = dirty_dict):\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n\n\n\n\n    def handle_punctuation(x):\n        x = x.translate(remove_dict)\n        x = x.translate(isolate_dict)\n        return x\n\n\n    def spacing_punctuation(text): ##clear puncts\n        for punc in new_puncts:\n            if punc in text:\n                text = text.replace(punc, ' ')\n        return text\n\n    '''  \n    def final_contraction(x, dic = final_mapping):\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n    '''\n\n    def new_final_contraction(x, dic = new_final_mapping):\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n    def preprocess(df_comment):\n\n        # lower\n        # clean misspellings\n        df_comment = df_comment.str.lower()\n        df_comment = df_comment.str.replace('[\\'\\\"\\(\\[\\:]?https?:?//[!-z]+',' ')\n        df_comment = df_comment.str.replace('[\\'\\\"\\(\\[\\:]?www[.][!-z]+',' ')\n        df_comment = df_comment.apply(pre_clean_abbr_words)\n        df_comment = df_comment.apply(correct_contraction) \n        df_comment = df_comment.apply(clean_dirty_dict)\n\n        # clean the text\n    #    df_comment = df_comment.apply(spacing_punctuation)\n        df_comment = df_comment.apply(lambda x:handle_punctuation(x))\n        df_comment = df_comment.apply(new_final_contraction)\n\n        return df_comment\n    \n    \n    \n    ## firstlarge models\n    \n    print('bert_large_uncased_wwm')\n    BERT_PRETRAINED_DIR = '../input/bertprototype/wwm_uncased_l-24_h-1024_a-16/wwm_uncased_L-24_H-1024_A-16' \n    print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n    config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n    checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n    dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')  \n    tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n    print('build tokenizer uncased done')\n    modelb = load_trained_model_from_checkpoint(config_file,training=True,seq_len=maxlen)\n    \n    sequence_outputb  = modelb.layers[-6].output\n    pool_outputb = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_outputb)\n    aux_outputb = Dense(6,activation='sigmoid',name = 'aux_output')(sequence_outputb)\n    model2  = Model(inputs=modelb.input, outputs=[pool_outputb,aux_outputb])\n    #model2.compile(optimizer=adamwarm,loss='mse')\n    \n    model2.load_weights('../input/jul2995365ep2bertlarge/95365ep2bertlarge.h5')\n    print('load ba models new')\n    eval_lines = (preprocess(test_df['comment_text'])).values\n    token_input2 = convert_lines(eval_lines,maxlen,tokenizer)\n    seg_input2 = np.zeros((token_input2.shape[0],maxlen))\n    mask_input2 = np.ones((token_input2.shape[0],maxlen))\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertlarge_wwm_uncased',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertlarge.csv', index=False)\n    \n    \n    ##then base uncased models\n    \n    print('bert_based_uncased')\n    BERT_PRETRAINED_DIR = '../input/bertprototype/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/' \n    print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n    config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n    #checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n    #dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')  \n    #tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\n    print('build tokenizer uncased done')\n    modelb = load_trained_model_from_checkpoint(config_file,training=True,seq_len=maxlen)\n    \n    sequence_outputb  = modelb.layers[-6].output\n    pool_outputb = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_outputb)\n    aux_outputb = Dense(6,activation='sigmoid',name = 'aux_output')(sequence_outputb)\n    model2  = Model(inputs=modelb.input, outputs=[pool_outputb,aux_outputb])\n    #model2.compile(optimizer=adamwarm,loss='mse')\n    ##low\n    model2.load_weights('../input/final-model-group2/bertuncasedbase_pre_220_95175_ep2.h5')\n    print('load bert base uncased models low')\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_uncased_low',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_low.csv', index=False)\n    \n    ##high\n    model2.load_weights('../input/95282bertbaseuncased/95282bertbaseuncased.h5')\n    print('load bert base uncased models high')\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_uncased_high',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_high.csv', index=False)\n    \n    \n    ##last base cased models\n    ##first def preprocessing\n    symbols_to_delete = 'â†’â˜…Â©Â®â—Ëâ˜†Â¶ï¼‰Ð¸Ê¿ã€‚ï¬‚ï¬â‚â™­å¹´â–ªâ†Ê’ã€ï¼ˆæœˆâ– â‡ŒÉ¹Ë¤Â³ã®Â¤â€¿Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ê€É´×©×œ×•××‘×™ã‚¨ãƒ³á´µ××¢×›×—â€Î¹ÎºÎ¾ØªØ­ÙƒØ³Ø©ÙØ²Ø·â€‘åœ°è°·ÑƒÐ»ÐºÐ½Ð¾×”æ­ŒÐ¼Ï…Ñ‚ÑÐ¿Ñ€Ð´Ë¢áµ’Ê³Ê¸á´ºÊ·áµ—Ê°áµ‰áµ˜Î¿Ï‚×ª×ž×“×£× ×¨×š×¦×˜æˆéƒ½ÐµÑ…å°åœŸã€‹à¤•à¤°à¤®à¤¾è‹±æ–‡ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººÐ±ÑŒÑ‹Ð³Ñä¸ã¤Ð·Ñ†ä¼šä¸‹æœ‰çš„åŠ å¤§å­ãƒ„Ø´Ø¡Ê²ÑˆÑ‡ÑŽÐ¶æˆ¦Ñ‰æ˜Ž×§Ñ™Ñ›æˆ‘å‡ºç”Ÿå¤©ä¸€å®¶æ–°ÊÕ½Õ°×ŸØ¬Ñ–â€’å…¬ç¾Žé˜¿×¡×¤ç™½ãƒžãƒ«ãƒãƒ‹ãƒãƒ­ç¤¾Î¶å’Œä¸­æ³•æœ¬å£«ç›¸ä¿¡æ”¿æ²»å ‚ç‰ˆã£Ñ„Ú†ÛŒãƒªäº‹ã€Œã€ã‚·Ï‡ÏˆÕ´Õ¥Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Ú©ã€Šáƒšã•ã‚ˆã†ãªã‚‰Ø¹Ø¯ÙˆÙŠÙ‡ØµÙ‚Ù†Ø§Ø®Ù„Ù‰Ø¨Ù…ØºØ±Ê€É´×©×œ×•××‘×™ã‚¨ãƒ³á´µ××¢×›×—â€Î¹ÎºÎ¾ØªØ­ÙƒØ³Ø©ÙØ²Ø·â€‘åœ°è°·ÑƒÐ»ÐºÐ½Ð¾×”æ­ŒÐ¼Ï…Ñ‚ÑÐ¿Ñ€Ð´Ë¢áµ’Ê³Ê¸á´ºÊ·áµ—Ê°áµ‰áµ˜Î¿Ï‚×ª×ž×“×£× ×¨×š×¦×˜æˆéƒ½ÐµÑ…å°åœŸã€‹à¤•à¤°à¤®à¤¾è‹±æ–‡ãƒ¬ã‚¯ã‚µã‚¹å¤–å›½äººÐ±ÑŒÑ‹Ð³Ñä¸ã¤Ð·Ñ†ä¼šä¸‹æœ‰çš„åŠ å¤§å­ãƒ„Ø´Ø¡Ê²ÑˆÑ‡ÑŽÐ¶æˆ¦Ñ‰æ˜Ž×§Ñ™Ñ›æˆ‘å‡ºç”Ÿå¤©ä¸€å®¶æ–°ÊÕ½Õ°×ŸØ¬Ñ–â€’å…¬ç¾Žé˜¿×¡×¤ç™½ãƒžãƒ«ãƒãƒ‹ãƒãƒ­ç¤¾Î¶å’Œä¸­æ³•æœ¬å£«ç›¸ä¿¡æ”¿æ²»å ‚ç‰ˆã£Ñ„Ú†ÛŒãƒªäº‹ã€Œã€ã‚·Ï‡ÏˆÕ´Õ¥Õ¡ÕµÕ«Õ¶Ö€Ö‚Õ¤Ú©ã€Šáƒšã•ã‚ˆã†ãªã‚‰\\nï¼¼ðŸ•\\rðŸµðŸ˜‘\\xa0\\ue014â‰ \\t\\uf818\\uf04a\\xadðŸ˜¢ðŸ¶â¤ï¸â˜º\\uf0e0ðŸ˜œðŸ˜ŽðŸ‘Š\\u200b\\u200eðŸ˜Ø£ðŸ˜ðŸ’–Ì¶ðŸ’µâ¥â”â”£â”«Ð•â”—ï¼¯â–ºðŸ‘ŽðŸ˜€ðŸ˜‚\\u202a\\u202cðŸ”¥ðŸ˜„ðŸ»ðŸ’¥á´Êá´‡á´…á´á´€á´‹Êœá´œÊŸá´›á´„á´˜Ê™Ò“á´Šá´¡É¢âœ”\\x96\\x92ðŸ˜‹ðŸ‘ðŸ˜±â€¼\\x81ã‚¸æ•…éšœâž¤\\u2009ðŸšŒÍžðŸŒŸðŸ˜ŠðŸ˜³ðŸ˜§ðŸ™€ðŸ˜ðŸ˜•\\u200fðŸ‘ðŸ˜®ðŸ˜ƒðŸ˜˜â˜•â™¡â—â•‘â–¬ðŸ’©ðŸ’¯â›½ðŸš„ðŸ¼à®œÛ©ÛžðŸ˜–á´ ðŸš²âœ’âž¥ðŸ˜ŸðŸ˜ˆâ•ËŒðŸ’ªðŸ™ðŸŽ¯â—„ðŸŒ¹ðŸ˜‡ðŸ’”ðŸ˜¡\\x7fðŸ‘Œá¼á½¶Î®á½²á¼€Î¯á¿ƒá¼´ðŸ™„âœ¬ï¼³ï¼µï¼°ï¼¥ï¼²ï¼¨ï¼©ï¼´ðŸ˜ \\ufeffâ˜»\\u2028ðŸ˜‰ðŸ˜¤â›ºâ™ðŸ™‚\\u3000ðŸ‘®ðŸ’™ðŸ˜ðŸ¾ðŸŽ‰ðŸ˜ž\\u2008ðŸ¾ðŸ˜…ðŸ˜­ðŸ‘»ðŸ˜¥ðŸ˜”ðŸ˜“ðŸ½ðŸŽ†âœ“â—¾ðŸ»ðŸ½ðŸŽ¶ðŸŒºðŸ¤”ðŸ˜ª\\x08ØŸðŸ°ðŸ‡ðŸ±ðŸ™†ðŸ˜¨â¬…ðŸ™ƒðŸ’•ð˜Šð˜¦ð˜³ð˜¢ð˜µð˜°ð˜¤ð˜ºð˜´ð˜ªð˜§ð˜®ð˜£ðŸ’—ðŸ’šç„â„…Ð’ÐŸÐÐðŸ¾ðŸ•â£ðŸ˜†ðŸ”—ðŸš½èˆžä¼ŽðŸ™ˆðŸ˜´ðŸ¿ðŸ¤—ðŸ‡ºðŸ‡¸â™«Ñ•ï¼£ï¼­â¤µðŸ†ðŸŽƒðŸ˜©â–ˆâ–“â–’â–‘\\u200aðŸŒ ðŸŸðŸ’«ðŸ’°ðŸ’Ž\\x95ðŸ–ðŸ™…â›²ðŸ°â­ðŸ¤ðŸ‘†ðŸ™Œ\\u2002ðŸ’›ðŸ™ðŸ‘€ðŸ™ŠðŸ™‰\\u2004â§â–°â–”á´¼á´·â—žâ–€\\x13ðŸš¬â–‚â–ƒâ–„â–…â–†â–‡â†™ðŸ¤“\\ue602ðŸ˜µÎ¬ÏŒÎ­á½¸Ì„ðŸ˜’Íâ˜¹âž¡ðŸ†•ðŸ‘…ðŸ‘¥ðŸ‘„ðŸ”„ðŸ”¤ðŸ‘‰ðŸ‘¤ðŸ‘¶ðŸ‘²ðŸ”›ðŸŽ“\\uf0b7âœ‹\\uf04c\\x9f\\x10ðŸ˜£âºÌ²Ì…ðŸ˜ŒðŸ¤‘ÌðŸŒðŸ˜¯ðŸ˜²âˆ™â€›á¼¸á¾¶á½ðŸ’žðŸš“â—‡ðŸ””ðŸ“šâœðŸ€ðŸ‘\\u202dðŸ’¤ðŸ‡\\ue613è±†ðŸ¡â–·â”â“â‰â—\\u202fðŸ‘ à¥ðŸ‡¹ðŸ‡¼ðŸŒ¸è”¡ðŸŒžËšðŸŽ²ðŸ˜›Ë™å…³ç³»Ð¡ðŸ’‹ðŸ’€ðŸŽ„ðŸ’œðŸ¤¢ÙÙŽâœ¨æ˜¯\\x80\\x9c\\x9dðŸ—‘\\u2005ðŸ’ƒðŸ“£ðŸ‘¿à¼¼â—•à¼½ðŸ˜°á¸·Ð—â–±ï¿¼ðŸ¤£å–æ¸©å“¥åŽè®®é™ï¼…ä½ å¤±åŽ»æ‰€é’±æ‹¿åç¨Žéª—ðŸÂ¯ðŸŽ…\\x85ðŸºØ¢Ø¥ðŸŽµðŸŒŽÍŸá¼”æ²¹åˆ«å…‹ðŸ¤¡ðŸ¤¥ðŸ˜¬ðŸ¤§Ð¹\\u2003ðŸš€ðŸ¤´âŒ Ð˜ÐžÐ Ð¤Ð”Ð¯Ðœâœ˜ðŸ˜ðŸ–‘á½á½»Ïç‰¹æ®Šä½œç¾¤â•ªðŸ’¨åœ†å›­â–¶â„â˜­âœ­ðŸˆðŸ˜ºâ™ªðŸŒâá»‡ðŸ”ðŸ®ðŸâ˜”ðŸ†ðŸ‘ðŸŒ®ðŸŒ¯â˜ ðŸ¤¦\\u200dâ™‚ð“’ð“²ð“¿ð“µì•ˆì˜í•˜ì„¸ìš”Ð–ÐšðŸ€ðŸ˜«ðŸ¤¤á¿¦åœ¨äº†å¯ä»¥è¯´æ™®é€šè¯æ±‰è¯­å¥½æžðŸŽ¼ðŸ•ºâ˜ƒðŸ¸ðŸ¥‚ðŸ—½ðŸŽ‡ðŸŽŠðŸ†˜â˜ŽðŸ¤ ðŸ‘©âœˆðŸ–’âœŒâœ°â†â˜™ðŸšªâš²\\u2006âš­âš†â¬­â¬¯â–â—‹â€£âš“âˆŽâ„’â–™â˜â…›âœ€â•ŒðŸ‡«ðŸ‡·ðŸ‡©ðŸ‡ªðŸ‡®ðŸ‡¬ðŸ‡§ðŸ˜·ðŸ‡¨ðŸ‡¦Ð¥Ð¨ðŸŒ\\x1fæ€é¸¡ç»™çŒ´çœ‹ð—ªð—µð—²ð—»ð˜†ð—¼ð˜‚ð—¿ð—®ð—¹ð—¶ð˜‡ð—¯ð˜ð—°ð˜€ð˜…ð—½ð˜„ð—±ðŸ“ºï½ƒÏ–\\u2000Ò¯ï½á´¦áŽ¥Ò»Íº\\u2007ï½“Ç€\\u2001É©â„®ï½™ï½…àµ¦ï½ŒÆ½Â¸ï½—ï½ˆð“ð¡ðžð«ð®ððšðƒðœð©ð­ð¢ð¨ð§Æ„á´¨á‘¯à»Î¤á§à¯¦Ð†á´‘Üð¬ð°ð²ð›ð¦ð¯ð‘ð™ð£ð‡ð‚ð˜ðŸŽÔœÐ¢á—žà±¦ã€”áŽ«ð³ð”ð±ðŸ”ðŸ“ð…ðŸ‹âˆ¼ï¬ƒðŸ’˜ðŸ’“Ñ‘ð˜¥ð˜¯ð˜¶ðŸ’ðŸŒ‹ðŸŒ„ðŸŒ…ð™¬ð™–ð™¨ð™¤ð™£ð™¡ð™®ð™˜ð™ ð™šð™™ð™œð™§ð™¥ð™©ð™ªð™—ð™žð™ð™›ðŸ‘ºðŸ·â„‹â„³ð€ð¥ðªâ„ðŸš¶ð™¢á¼¹ðŸ¤˜Í¦ðŸ’¸â˜¼íŒ¨í‹°ï¼·â‹†ð™‡áµ»ðŸ‘‚ðŸ‘ƒÉœðŸŽ«\\uf0a7Ð‘Ð£ðŸš¢ðŸš‚àª—à«àªœàª°àª¾àª¤à«€á¿†ðŸƒð“¬ð“»ð“´ð“®ð“½ð“¼â˜˜ï´¾Í¡à¹Ì¯ï´¿âš¾âš½Î¦â‚½\\ue807ð‘»ð’†ð’ð’•ð’‰ð’“ð’–ð’‚ð’ð’…ð’”ð’Žð’—ð’ŠðŸ‘½ðŸ˜™\\u200cÐ›ðŸŽ¾ðŸ‘¹ï¿¦âŽŒðŸ’â›¸å¯“å…»å® ç‰©å—ðŸ„ðŸ€ðŸš‘ðŸ¤·æ“ð’‘ð’šð’ð‘´ðŸ¤™ðŸ’â„ƒæ¬¢è¿Žæ¥åˆ°æ‹‰æ–¯ð™«â©â˜®ðŸˆð’Œð™Šð™­ð™†ð™‹ð™ð˜¼ð™…ï·»âš ðŸ¦„å·¨æ”¶èµ¢å¾—é¬¼æ„¤æ€’è¦ä¹°é¢áº½ðŸš—âœŠðŸ³ðŸðŸðŸ–ðŸ‘ðŸ•ð’„ðŸ—ð ð™„ð™ƒðŸ‘‡é”Ÿæ–¤æ‹·âŒâ­•â–¸ð—¢ðŸ³ðŸ±ðŸ¬â¦æ ªå¼â›·í•œêµ­ì–´ã„¸ã…“ë‹ˆÍœÊ–ð˜¿ð™”â‚µð’©â„¯ð’¾ð“ð’¶ð“‰ð“‡ð“Šð“ƒð“ˆð“…â„´ð’»ð’½ð“€ð“Œð’¸ð“Žð™ð™Ÿð˜ƒð—ºðŸ®ðŸ­ðŸ¯ðŸ²ðŸ‘‹ðŸ¦Šâ˜â˜‘å¤šä¼¦âš¡â˜„Ç«ðŸ½ðŸŽ»ðŸŽ¹â›“ðŸ¹â•­â•®ðŸ·ðŸ¦†ä¸ºå‹è°Šç¥è´ºä¸Žå…¶æƒ³è±¡å¯¹å¦‚ç›´æŽ¥é—®ç”¨è‡ªå·±çŒœä¼ æ•™æ²¡ç§¯å”¯è®¤è¯†åŸºç£å¾’æ›¾ç»è®©è€¶ç¨£å¤æ´»æ­»æ€ªä»–ä½†å½“ä»¬èŠäº›é¢˜æ—¶å€™ä¾‹æˆ˜èƒœå› åœ£æŠŠå…¨ç»“å©šå­©ææƒ§ä¸”æ —è°“è¿™æ ·è¿˜â™¾ðŸŽ¸ðŸ¤•ðŸ¤’â›‘ðŸŽæ‰¹åˆ¤æ£€è®¨ðŸðŸ¦ï¼žÊ•Ì£Î”ðŸ™‹ðŸ˜¶ì¥ìŠ¤íƒ±íŠ¸ë¤¼ë„ì„ìœ ê°€ê²©ì¸ìƒì´ê²½ì œí™©ì„ë µê²Œë§Œë“¤ì§€ì•Šë¡ìž˜ê´€ë¦¬í•´ì•¼í•©ë‹¤ìºë‚˜ì—ì„œëŒ€ë§ˆì´ˆì™€í™”ì•½ê¸ˆì˜í’ˆëŸ°ì„±ë¶„ê°ˆë•ŒëŠ”ë°˜ë“œì‹œí—ˆëœì‚¬ìš©âœžðŸ”«ðŸ‘â”ˆâ•±â•²â–â–•â”ƒâ•°â–Šâ–‹â•¯â”³â”Šâ˜’å‡¸á½°ðŸ’²ðŸ—¯ð™ˆá¼Œð’‡ð’ˆð’˜ð’ƒð‘¬ð‘¶ð•¾ð–™ð–—ð–†ð–Žð–Œð–ð–•ð–Šð–”ð–‘ð–‰ð–“ð–ð–œð–žð–šð–‡ð•¿ð–˜ð–„ð–›ð–’ð–‹ð–‚ð•´ð–Ÿð–ˆð•¸ðŸ‘‘ðŸš¿â˜ðŸ’¡çŸ¥å½¼ç™¾\\uf005ð™€ð’›ð‘²ð‘³ð‘¾ð’‹ðŸ’ðŸ˜¦ð™’ð˜¾ð˜½ðŸð˜©ð˜¨á½¼á¹‘âœ…â˜›ð‘±ð‘¹ð‘«ð‘µð‘ªðŸ‡°ðŸ‡µðŸ‘¾á“‡á’§á”­áƒá§á¦á‘³á¨á“ƒá“‚á‘²á¸á‘­á‘Žá“€á£ðŸ„ðŸŽˆðŸ”¨â™©ðŸŽðŸ¤žâ˜žðŸ¸ðŸ’ŸðŸŽ°ðŸŒðŸ›³ç‚¹å‡»æŸ¥ðŸ­ð‘¥ð‘¦ð‘§ï¼¡ï¼®ï¼§ï¼ªï¼¢ðŸ‘£\\uf020â—”â—¡ðŸ‰ðŸ’­ðŸŽ¥â™€ÎžðŸ´ðŸ‘¨ðŸ¤³â¬†ðŸ¦\\x0bðŸ©ð‘¯ð’’ðŸ˜—ðŸðŸ‚ðŸ‘³ðŸ—ðŸ•‰ðŸ²Ì±â„ð‘®ð—•ð—´\\x91ðŸ’â €êœ¥â²£â²â•šðŸ‘â°â†ºâ‡¤âˆé‰„ä»¶âœ¾â—¦â™¬Ñ—ðŸ’Š\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600ç‡»è£½è™šå½å±ç†å±ˆï½œÐ“ð‘©ð‘°ð’€ð‘ºðŸŒ¤âˆµâˆ´ð—³ð—œð—™ð—¦ð—§ðŸŠá½ºá¼ˆá¼¡á¿–Î›Î©â¤ðŸ‡³ð’™ÕÕ¼Õ±å†¬è‡³á½€ð’ðŸ”¹ðŸ¤šðŸŽð‘·ðŸ‚ðŸ’…ð˜¬ð˜±ð˜¸ð˜·ð˜ð˜­ð˜“ð˜–ð˜¹ð˜²ð˜«â˜œÎ’ÏŽðŸ’¢â–²ÎœÎŸÎÎ‘Î•ðŸ‡±â™²ðˆâ†´â†³ðŸ’’âŠ˜â–«È»â¬‡ðŸš´ðŸ–•ðŸ–¤ðŸ¥˜ðŸ“ðŸ‘ˆâž•ðŸš«ðŸŽ¨ðŸŒ‘ðŸ»ðŽððŠð‘­ðŸ¤–ðŸŽŽâœ§ðŸ˜¼ðŸ•·ï½‡ï½ï½–ï½’ï½Žï½ï½”ï½‰ï½„ï½•ï¼’ï¼ï¼˜ï½†ï½‚ï¼‡ï½‹ðŸ°ðŸ‡´ðŸ‡­ðŸ‡»ðŸ‡²ð—žð—­ð—˜ð—¤ðŸ‘¼ðŸ“‰ðŸŸðŸ¦âˆ•ðŸŒˆðŸ”­ðŸŠðŸ\\uf10aË†âšœâ˜Ú¡ðŸ¦\\U0001f92f\\U0001f92aðŸ¡ðŸ’³á¼±ðŸ™‡ð—¸ð—Ÿð— ð—·ðŸ¥œðŸ”¼'\n    symbols_to_isolate = '.,?!-;*\"â€¦:â€”()%#$&_/@ãƒ»Ï‰+=â€â€œ[]^â€“>\\\\Â°<~â€¢â„¢ËˆÊŠÉ’âˆžÂ§{}Â·Ï„Î±É¡|Â¢`â€•ÉªÂ£â™¥Â´Â¹â‰ˆÃ·â€²É”â‚¬â€ Î¼Â½Ê»Ï€Î´Î·Î»ÏƒÎµÏÎ½ÊƒÂ±ÂµÂºÂ¾ï¼ŽÂ»Ð°Ð²â‹…Â¿Â¬Î²â‡’â€ºÂ¡â‚‚â‚ƒÎ³â€³Â«Ï†â…“â€žï¼šÂ¥ÑÉ‘ï¼âˆ’Â²ÊŒÂ¼â´â„â‚„â€šâ€–âŠ‚â…”Â¨Ã—Î¸ï¼Ÿâˆ©ï¼ŒÉâ‚€â‰¥â†‘â†“ï¼âˆšï¼â€°â‰¤'\n    \n    abbr_mapping = {      'á´€':'a','Ê™':'b','á´„':'c','á´…':'d','á´‡':'e','êœ°':'f','É¢':'g','Êœ':'h',\n                      'Éª':'i','á´Š':'j','á´‹':'k','ÊŸ':'l','á´':'m','É´':'n','á´':'o','á´˜':'p',\n                      'Ç«':'q','Ê€':'r','êœ±':'s','á´›':'t','á´œ':'u','á´ ':'v','á´¡':'w','Ê':'y','á´¢':'z', '\\n':' ',\n                       ' yr old ': ' years old ',' yrs old ': ' years old ','coâ‚‚':'carbon dioxide',\n               }     \n\n    regex_mapping = {\n                         '[Uu][.][Ss][.][Aa][.]': 'USA', '[Uu][.][Ss][.][Aa]': 'USA',\n                          '[Uu][.][Ss][.]': 'USA',  ' [Uu][.][Ss] ': ' USA ','[Uu] [Ss] of [Aa]': 'USA',\n                          ' [Uu][.][Kk][.]? ': ' UK ',' [Pp][Hh][.][Dd] ': ' phd ',' [Uu] [Rr] ': ' you are ',\n                         '[Ee][.][Gg][.]': 'for example','[Ii][.][Ee][.]': 'in other words',\n                          '[Ee][Tt][.][Aa][Ll]': 'elsewhere',\"[Gg]ov[']t\": \"government\",\n                         '[Tt][Rr][Uu][Mm][Pp]':'trump','[Oo][Bb][Aa][Mm][Aa]':'obama',\n                    }\n\n\n    new_final_mapping = {  \n                            'jackoff': 'jerk off','jerkoff':'jerk off','bestial': 'beastial',\n                         'bestiality': 'beastiality', 'd1ck': 'dick', 'lmfao': 'laughing my fucking ass off',\n                          'masturbate': 'masterbate', 'cashap24':'cash app','nurnie':'pussy',\n                         'n1gger': 'nigger', 'nigga': 'nigger', 'niggas': 'niggers',\n                         'clickbait':'click with bait','YUGE':'huge','Outsider77':'outsider',\n                         'numbnuts': 'noob nuts', 'orgasms': 'orgasm', 'Trudope':'The prime minister of Canada',\n                          '[Dd]aesh':'ISIS', \"Qur'an\":'the central religious text of Islam','gofundme':'go fund me',\n                         'Finicum':'an American spokesman','trumpkins':'trump with pumpkin','trumpettes':'trump',\n                           'trump[Cc]are':'trump health care','obama[Cc]are':'obama health care','trumpies':'trump',\n                          'trumpster': 'trump supporter','trumper': 'trump supporter', 'trumpy':'trump',\n                          'trumpian':'trump supporter','trumpism':'trump supporter',\"trump[']s\" : 'trump',\n                          '[Kk]im [Jj]ong([- ][Uu]n)?': 'the president of north korea','Cheetolini':'trump',\n                          'trumpland':'trump land','trumpty':'trump','trumpist[s]?':'trump supporter',\n                          'trumpeteer[s]?':'trump supporter','trumplethinskin':'trump','trumpo':'trump',\n                          'realDonaldtrump':'real Donald trump','[Tt]heDonald':'the Donald',\n                          ' brother[Ii]n ':' brother ', 'Beyak':'Canadian politician',\n                          'Trudeaus':'Prime Minister of Canada ','shibai':'failure',\n                          'Tridentinus':'Tridentinum','[Zz]upta[s]?':'the South African President',\n                           '[Pp]utrumpski':'Putin and trump supporter','Twitler':'twitter user',\n                           'antisemitic': 'anti semitic', '[Ss][Bb]91':'senate bill', \n                            'utmterm':' utm term','[Ff]ake[Nn]ews':'fake news', 'Pizzagate':'Pizza gate',                 \n                            '[Ww]ashingtontimes':'washington times','[Gg]arycrum':'gary crum',\n                            'RangerMC':'car','[Tt][Ff][Ww]s':'tuition fee waiver','[Ss][Jj][Ww][Ss]?':'social justice warrior',\n                            'Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you', '[Aa]uwe': 'oh no',\n                            '[Bb]igly':'big league','Drump[f]?':'Trump','[Bb]rexit':'british exit',\n                            'utilitas':'utilities','justiciaries': 'justiciary','doctrne':'doctrine',\n                           '[Dd]eplorables': 'deplorable','[Cc][Oo][Nn]artist' : 'con-artist',\n                           'theglobeandmail': 'the globe and mail', 'howhat': 'how that', ' coz ':' because ',\n                           'civilbeat':'civil beat','gubmit':'submit','financialpost':'financial post',               \n                           'theguardian': 'the guardian','shopo':'shop','SHOPO':'shop','fentayal': 'fentanyl',\n                         'designation-': 'designation ','[Mm]utilitated' : 'Mutilated','dood-': 'dood ',\n                         '[Ii]rakis' : 'iraki', 'supporter[a-z]+':'supporter',' u ':' you ', \n                        }\n\n\n    contraction_mapping = {\n        \"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n        'ain;t': 'am not','ainÂ´t': 'am not','ainâ€™t': 'am not',\"aren't\": 'are not',\n        'aren,t': 'are not','aren;t': 'are not','arenÂ´t': 'are not','arenâ€™t': 'are not',\"can't\": 'cannot',\n        \"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have', 'can;t': 'cannot','can;t;ve': 'cannot have',\n        'canÂ´t': 'cannot','canÂ´tÂ´ve': 'cannot have','canâ€™t': 'cannot','canâ€™tâ€™ve': 'cannot have',\n        \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\n        \"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n        'couldn;t;ve': 'could not have','couldnÂ´t': 'could not', 'couldnÂ´tÂ´ve': 'could not have','couldnâ€™t': 'could not',\n        'couldnâ€™tâ€™ve': 'could not have', 'couldÂ´ve': 'could have',\n        'couldâ€™ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didnÂ´t': 'did not',\n        'didnâ€™t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesnÂ´t': 'does not',\n        'doesnâ€™t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','donÂ´t': 'do not','donâ€™t': 'do not',\n        \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n        'hadn;t;ve': 'had not have','hadnÂ´t': 'had not','hadnÂ´tÂ´ve': 'had not have','hadnâ€™t': 'had not',\n        'hadnâ€™tâ€™ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasnÂ´t': 'has not',\n        'hasnâ€™t': 'has not', \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','havenÂ´t': 'have not',\n        'havenâ€™t': 'have not',\"he'd\": 'he would', \"he'd've\": 'he would have',\"he'll\": 'he will','heÂ´ll': 'he will',\n        \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',   \n        'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','heÂ´d': 'he would','heÂ´dÂ´ve': 'he would have',    \n        'heÂ´s': 'he is','heâ€™d': 'he would','heâ€™dâ€™ve': 'he would have','heâ€™ll': 'he will','heâ€™s': 'he is',\n        \"how'd\": 'how did',\"how'll\": 'how will',\"how's\": 'how is','how,d': 'how did','how,ll': 'how will',\n        'how,s': 'how is','how;d': 'how did','how;ll': 'how will','how;s': 'how is','howÂ´d': 'how did','howÂ´ll': 'how will',\n        'howÂ´s': 'how is','howâ€™d': 'how did','howâ€™ll': 'how will','howâ€™s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\n        \"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will','i,m': 'i am','i,ve': 'i have','i;d': 'i would',\n        'i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not','isn,t': 'is not','isn;t': 'is not',\n        'isnÂ´t': 'is not','isnâ€™t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is', \"it's\": 'it is',\n        'it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will', 'it;s': 'it is',\n        'itÂ´d': 'it would','itÂ´ll': 'it will','itÂ´s': 'it is','itâ€™d': 'it would','itâ€™ll': 'it will','itâ€™s': 'it is',\n        'iÂ´d': 'i would','iÂ´ll': 'i will','iÂ´m': 'i am','iÂ´ve': 'i have','iâ€™d': 'i would','iâ€™ll': 'i will','iâ€™m': 'i am',\n        'iâ€™ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','letÂ´s': 'let us', 'letâ€™s': 'let us',\n        \"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not', 'mayn;t': 'may not',\n        'maynÂ´t': 'may not','maynâ€™t': 'may not','maÂ´am': 'madam','maâ€™am': 'madam',\"might've\": 'might have',\n        'might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not',\n        'mightn;t': 'might not','mightnÂ´t': 'might not', 'mightnâ€™t': 'might not','mightÂ´ve': 'might have',\n        'mightâ€™ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n        \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustnÂ´t': 'must not','mustnâ€™t': 'must not',\n        'mustÂ´ve': 'must have','mustâ€™ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not',\n        'neednÂ´t': 'need not','neednâ€™t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n        'oughtnÂ´t': 'ought not','oughtnâ€™t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\n        \"shan't\": 'shall not', 'shan,t': 'shall not','shan;t': 'shall not','shanÂ´t': 'shall not','shanâ€™t': 'shall not',\n        'shaÂ´nÂ´t': 'shall not','shaâ€™nâ€™t': 'shall not',\"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is',\n        'she,d': 'she would','she,ll': 'she will', 'she,s': 'she is','she;d': 'she would','she;ll': 'she will',\n        'she;s': 'she is','sheÂ´d': 'she would','sheÂ´ll': 'she will', 'sheÂ´s': 'she is','sheâ€™d': 'she would',\n        'sheâ€™ll': 'she will','sheâ€™s': 'she is',\"should've\": 'should have','should,ve': 'should have',\n        'should;ve': 'should have', \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not',\n        'shouldnÂ´t': 'should not','shouldnâ€™t': 'should not','shouldÂ´ve': 'should have', 'shouldâ€™ve': 'should have',\n        \"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n        'that;s': 'that is','thatÂ´d': 'that would','thatÂ´s': 'that is','thatâ€™d': 'that would','thatâ€™s': 'that is',\n        \"there'd\": 'there had', \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had',\n        'there;s': 'there is', 'thereÂ´d': 'there had','thereÂ´s': 'there is','thereâ€™d': 'there had','thereâ€™s': 'there is',\n        \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have','they,d': 'they would',\n        'they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will',\n        'they;re': 'they are', 'they;ve': 'they have','theyÂ´d': 'they would','theyÂ´ll': 'they will','theyÂ´re': 'they are',\n        'theyÂ´ve': 'they have','theyâ€™d': 'they would','theyâ€™ll': 'they will','theyâ€™re': 'they are','theyâ€™ve': 'they have',\n        \"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasnÂ´t': 'was not','wasnâ€™t': 'was not',\n        \"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n        'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n        \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','werenÂ´t': 'were not','werenâ€™t': 'were not',\n        'weÂ´d': 'we would','weÂ´ll': 'we will',    'weÂ´re': 'we are','weÂ´ve': 'we have','weâ€™d': 'we would',\n        'weâ€™ll': 'we will','weâ€™re': 'we are','weâ€™ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\n        \"what's\": 'what is',    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is',\n        'what,ve': 'what have','what;ll': 'what will','what;re': 'what are','what;s': 'what is','what;ve': 'what have',\n        'whatÂ´ll': 'what will', 'whatÂ´re': 'what are','whatÂ´s': 'what is','whatÂ´ve': 'what have','whatâ€™ll': 'what will',\n        'whatâ€™re': 'what are','whatâ€™s': 'what is', 'whatâ€™ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is',\n        'where,d': 'where did','where,s': 'where is','where;d': 'where did','where;s': 'where is','whereÂ´d': 'where did',\n        'whereÂ´s': 'where is','whereâ€™d': 'where did','whereâ€™s': 'where is', \"who'll\": 'who will',\"who's\": 'who is',\n        'who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is','whoÂ´ll': 'who will','whoÂ´s': 'who is',\n        'whoâ€™ll': 'who will','whoâ€™s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n        'wonÂ´t': 'will not','wonâ€™t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not',\n        'wouldnÂ´t': 'would not','wouldnâ€™t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are',\n        'you,d': 'you would','you,ll': 'you will', 'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n        'you;re': 'you are','youÂ´d': 'you would','youÂ´ll': 'you will','youÂ´re': 'you are','youâ€™d': 'you would',\n        'youâ€™ll': 'you will','youâ€™re': 'you are','Â´cause': 'because','â€™cause': 'because',\"you've\": \"you have\",\n        \"could'nt\": 'could not',\"havn't\": 'have not',\"hereâ€™s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\n        \"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\"who're\": \"who are\",\"who've\": \"who have\",\n        \"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\n        \"y'know\": \"you know\",\"you.i\": \"you i\",\n        \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",\n        'don\"\"t': \"do not\",\"dosen't\": \"does not\", \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\n        \"there'll\": \"there will\",\"there're\": \"there are\", \"this'll\": \"this all\", \"ya'll\": \"you all\",\n        \"youâ€™ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\n        \"dont't\": \"do not\",\"i'ma\": \"i am\",\"is'nt\": \"is not\",\"â€˜i\":'i',  \":)\": ' smile ',\";)\": ' smile ',\n        \":-)\": ' smile ',\":(\": ' sad ','â€¦':'...', 'ðŸ˜‰': ' wink ', 'ðŸ˜‚': ' joy ', 'ðŸ˜€': ' stuck out tongue ',  \n         }\n\n    contraction_mapping1 ={\n        \"Agains't\": 'against', \"Ain't\": 'am not', 'Ain,t': 'am not', 'Ain;t': 'am not', 'AinÂ´t': 'am not',\n     'Ainâ€™t': 'am not', \"Aren't\": 'are not', 'Aren,t': 'are not', 'Aren;t': 'are not', 'ArenÂ´t': 'are not',\n     'Arenâ€™t': 'are not', \"Arn't\": 'are not', \"C'mon\": 'common', \"Can't\": 'cannot', \"Can't've\": 'cannot have',\n     'Can,t': 'cannot', 'Can,t,ve': 'cannot have', 'Can;t': 'cannot', 'Can;t;ve': 'cannot have',\n     'CanÂ´t': 'cannot', 'CanÂ´tÂ´ve': 'cannot have', 'Canâ€™t': 'cannot', 'Canâ€™tâ€™ve': 'cannot have',\n     \"Could'nt\": 'could not', \"Could've\": 'could have', 'Could,ve': 'could have', 'Could;ve': 'could have',\n     \"Couldn't\": 'could not', \"Couldn't've\": 'could not have', 'Couldn,t': 'could not',\n     'Couldn,t,ve': 'could not have', 'Couldn;t': 'could not', 'Couldn;t;ve': 'could not have',\n     'CouldnÂ´t': 'could not', 'CouldnÂ´tÂ´ve': 'could not have', 'Couldnâ€™t': 'could not',\n     'Couldnâ€™tâ€™ve': 'could not have', 'CouldÂ´ve': 'could have', 'Couldâ€™ve': 'could have',\n     \"D'int\": 'did not', \"Did'nt\": 'did not', \"Didn't\": 'did not', 'Didn,t': 'did not',\n     'Didn;t': 'did not', 'DidnÂ´t': 'did not', 'Didnâ€™t': 'did not', \"Din't\": 'did not',\n     \"Doens't\": 'does not', \"Doesn't\": 'does not', 'Doesn,t': 'does not', 'Doesn;t': 'does not',\n     'DoesnÂ´t': 'does not', 'Doesnâ€™t': 'does not', 'Don\"\"t': 'do not', \"Don't\": 'do not',\n     'Don,t': 'do not', 'Don;t': 'do not', \"Dont't\": 'do not', 'DonÂ´t': 'do not',\n     'Donâ€™t': 'do not', \"Dosen't\": 'does not', \"Dosn't\": 'does not', \"Hadn't\": 'had not',\n     \"Hadn't've\": 'had not have', 'Hadn,t': 'had not', 'Hadn,t,ve': 'had not have', 'Hadn;t': 'had not',\n     'Hadn;t;ve': 'had not have', 'HadnÂ´t': 'had not', 'HadnÂ´tÂ´ve': 'had not have', 'Hadnâ€™t': 'had not',\n     'Hadnâ€™tâ€™ve': 'had not have', \"Hasn't\": 'has not', 'Hasn,t': 'has not', 'Hasn;t': 'has not',\n     'HasnÂ´t': 'has not', 'Hasnâ€™t': 'has not', \"Haven't\": 'have not', 'Haven,t': 'have not',\n     'Haven;t': 'have not', 'HavenÂ´t': 'have not', 'Havenâ€™t': 'have not', \"Havn't\": 'have not',\n     \"He'd\": 'he would', \"He'd've\": 'he would have', \"He'll\": 'he will', \"He's\": 'he is',\n     'He,d': 'he would', 'He,d,ve': 'he would have', 'He,ll': 'he will', 'He,s': 'he is',\n     'He;d': 'he would', 'He;d;ve': 'he would have', 'He;ll': 'he will', 'He;s': 'he is',\n     'Hereâ€™s': 'here is', 'HeÂ´d': 'he would', 'HeÂ´dÂ´ve': 'he would have', 'HeÂ´ll': 'he will',\n     'HeÂ´s': 'he is', 'Heâ€™d': 'he would', 'Heâ€™dâ€™ve': 'he would have', 'Heâ€™ll': 'he will',\n     'Heâ€™s': 'he is', \"How'd\": 'how did', \"How'll\": 'how will', \"How's\": 'how is', 'How,d': 'how did',\n     'How,ll': 'how will', 'How,s': 'how is', 'How;d': 'how did', 'How;ll': 'how will', 'How;s': 'how is',\n     'HowÂ´d': 'how did', 'HowÂ´ll': 'how will', 'HowÂ´s': 'how is', 'Howâ€™d': 'how did', 'Howâ€™ll': 'how will',\n     'Howâ€™s': 'how is', 'I\"\"m': 'i am', \"I'am\": 'i am', \"I'd\": 'i would', \"I'l\": 'i will',\n     \"I'll\": 'i will', \"I'm\": 'i am', \"I'ma\": 'i am', \"I'v\": 'i have', \"I've\": 'i have',\n     'I,d': 'i would', 'I,ll': 'i will', 'I,m': 'i am', 'I,ve': 'i have', 'I;d': 'i would',\n     'I;ll': 'i will', 'I;m': 'i am', 'I;ve': 'i have', \"Is'nt\": 'is not', \"Isn't\": 'is not',\n     'Isn,t': 'is not', 'Isn;t': 'is not', 'IsnÂ´t': 'is not', 'Isnâ€™t': 'is not', \"It'd\": 'it would',\n     \"It'll\": 'it will', \"It's\": 'it is', 'It,d': 'it would', 'It,ll': 'it will', 'It,s': 'it is',\n     'It;d': 'it would', 'It;ll': 'it will', 'It;s': 'it is', 'ItÂ´d': 'it would', 'ItÂ´ll': 'it will',\n     'ItÂ´s': 'it is', 'Itâ€™d': 'it would', 'Itâ€™ll': 'it will', 'Itâ€™s': 'it is', 'IÂ´d': 'i would',\n     'IÂ´ll': 'i will', 'IÂ´m': 'i am', 'IÂ´ve': 'i have', 'Iâ€™d': 'i would', 'Iâ€™ll': 'i will', 'Iâ€™m': 'i am',\n     'Iâ€™ve': 'i have', \"Let's\": 'let us', 'Let,s': 'let us', 'Let;s': 'let us', 'LetÂ´s': 'let us',\n     'Letâ€™s': 'let us', \"Ma'am\": 'madam', 'Ma,am': 'madam', 'Ma;am': 'madam', \"Mayn't\": 'may not',\n     'Mayn,t': 'may not', 'Mayn;t': 'may not', 'MaynÂ´t': 'may not', 'Maynâ€™t': 'may not',\n     'MaÂ´am': 'madam', 'Maâ€™am': 'madam', \"Might've\": 'might have', 'Might,ve': 'might have',\n     'Might;ve': 'might have', \"Mightn't\": 'might not', 'Mightn,t': 'might not', 'Mightn;t': 'might not',\n     'MightnÂ´t': 'might not', 'Mightnâ€™t': 'might not', 'MightÂ´ve': 'might have', 'Mightâ€™ve': 'might have',\n     \"Must've\": 'must have', 'Must,ve': 'must have', 'Must;ve': 'must have', \"Mustn't\": 'must not',\n     'Mustn,t': 'must not', 'Mustn;t': 'must not', 'MustnÂ´t': 'must not', 'Mustnâ€™t': 'must not',\n     'MustÂ´ve': 'must have', 'Mustâ€™ve': 'must have', \"Needn't\": 'need not', 'Needn,t': 'need not',\n     'Needn;t': 'need not', 'NeednÂ´t': 'need not', 'Neednâ€™t': 'need not', \"Oughtn't\": 'ought not',\n     'Oughtn,t': 'ought not', 'Oughtn;t': 'ought not', 'OughtnÂ´t': 'ought not', 'Oughtnâ€™t': 'ought not',\n     \"Sha'n't\": 'shall not', 'Sha,n,t': 'shall not', 'Sha;n;t': 'shall not', \"Shan't\": 'shall not',\n     'Shan,t': 'shall not', 'Shan;t': 'shall not', 'ShanÂ´t': 'shall not', 'Shanâ€™t': 'shall not',\n     'ShaÂ´nÂ´t': 'shall not', 'Shaâ€™nâ€™t': 'shall not', \"She'd\": 'she would', \"She'll\": 'she will',\n     \"She's\": 'she is', 'She,d': 'she would', 'She,ll': 'she will', 'She,s': 'she is', 'She;d': 'she would',\n     'She;ll': 'she will', 'She;s': 'she is', 'SheÂ´d': 'she would', 'SheÂ´ll': 'she will', 'SheÂ´s': 'she is',\n     'Sheâ€™d': 'she would', 'Sheâ€™ll': 'she will', 'Sheâ€™s': 'she is', \"Shoudn't\": 'should not',\n     \"Should've\": 'should have', 'Should,ve': 'should have', 'Should;ve': 'should have',\n     \"Shouldn't\": 'should not', 'Shouldn,t': 'should not', 'Shouldn;t': 'should not',\n     'ShouldnÂ´t': 'should not', 'Shouldnâ€™t': 'should not', 'ShouldÂ´ve': 'should have',\n     'Shouldâ€™ve': 'should have', \"That'd\": 'that would', \"That'll\": 'that will',\n     \"That's\": 'that is', 'That,d': 'that would', 'That,s': 'that is', 'That;d': 'that would',\n     'That;s': 'that is', 'ThatÂ´d': 'that would', 'ThatÂ´s': 'that is', 'Thatâ€™d': 'that would',\n     'Thatâ€™s': 'that is', \"There'd\": 'there had', \"There'll\": 'there will', \"There're\": 'there are',\n     \"There's\": 'there is', 'There,d': 'there had', 'There,s': 'there is', 'There;d': 'there had',\n     'There;s': 'there is', 'ThereÂ´d': 'there had', 'ThereÂ´s': 'there is', 'Thereâ€™d': 'there had',\n     'Thereâ€™s': 'there is', \"They'd\": 'they would', \"They'll\": 'they will', \"They're\": 'they are',\n     \"They've\": 'they have', 'They,d': 'they would', 'They,ll': 'they will', 'They,re': 'they are',\n     'They,ve': 'they have', 'They;d': 'they would', 'They;ll': 'they will', 'They;re': 'they are',\n     'They;ve': 'they have', 'TheyÂ´d': 'they would', 'TheyÂ´ll': 'they will', 'TheyÂ´re': 'they are',\n     'TheyÂ´ve': 'they have', 'Theyâ€™d': 'they would', 'Theyâ€™ll': 'they will', 'Theyâ€™re': 'they are',\n     'Theyâ€™ve': 'they have', \"This'll\": 'this all', \"Wan't\": 'want', \"Was'nt\": 'was not', \"Wasn't\": 'was not',\n     'Wasn,t': 'was not', 'Wasn;t': 'was not', 'WasnÂ´t': 'was not', 'Wasnâ€™t': 'was not', \"We'd\": 'we would',\n     \"We'll\": 'we will', \"We're\": 'we are', \"We've\": 'we have', 'We,d': 'we would', 'We,ll': 'we will',\n     'We,re': 'we are', 'We,ve': 'we have', 'We;d': 'we would', 'We;ll': 'we will', 'We;re': 'we are',\n     'We;ve': 'we have', \"Weren't\": 'were not', 'Weren,t': 'were not', 'Weren;t': 'were not',\n     'WerenÂ´t': 'were not', 'Werenâ€™t': 'were not', 'WeÂ´d': 'we would', 'WeÂ´ll': 'we will',\n     'WeÂ´re': 'we are', 'WeÂ´ve': 'we have', 'Weâ€™d': 'we would', 'Weâ€™ll': 'we will', 'Weâ€™re': 'we are',\n     'Weâ€™ve': 'we have', \"What'll\": 'what will', \"What're\": 'what are', \"What's\": 'what is',\n     \"What've\": 'what have', 'What,ll': 'what will', 'What,re': 'what are', 'What,s': 'what is',\n     'What,ve': 'what have', 'What;ll': 'what will', 'What;re': 'what are', 'What;s': 'what is',\n     'What;ve': 'what have', 'WhatÂ´ll': 'what will', 'WhatÂ´re': 'what are', 'WhatÂ´s': 'what is',\n     'WhatÂ´ve': 'what have', 'Whatâ€™ll': 'what will', 'Whatâ€™re': 'what are', 'Whatâ€™s': 'what is',\n     'Whatâ€™ve': 'what have', \"Where'd\": 'where did', \"Where's\": 'where is', 'Where,d': 'where did',\n     'Where,s': 'where is', 'Where;d': 'where did', 'Where;s': 'where is', 'WhereÂ´d': 'where did',\n     'WhereÂ´s': 'where is', 'Whereâ€™d': 'where did', 'Whereâ€™s': 'where is', \"Who'd\": 'who would',\n     \"Who'll\": 'who will', \"Who're\": 'who are', \"Who's\": 'who is', \"Who've\": 'who have',\n     'Who,ll': 'who will', 'Who,s': 'who is', 'Who;ll': 'who will', 'Who;s': 'who is',\n     'WhoÂ´ll': 'who will', 'WhoÂ´s': 'who is', 'Whoâ€™ll': 'who will', 'Whoâ€™s': 'who is',\n     \"Why'd\": 'why would', \"Won't\": 'will not', 'Won,t': 'will not', 'Won;t': 'will not',\n     'WonÂ´t': 'will not', 'Wonâ€™t': 'will not', \"Would've\": 'would have', \"Wouldn't\": 'would not',\n     'Wouldn,t': 'would not', 'Wouldn;t': 'would not', 'WouldnÂ´t': 'would not', 'Wouldnâ€™t': 'would not',\n     \"Y'all\": 'you all', \"Y'know\": 'you know', \"Ya'll\": 'you all', \"You'd\": 'you would', \"You'll\": 'you will',\n     \"You're\": 'you are', \"You've\": 'you have', 'You,d': 'you would', 'You,ll': 'you will', 'You,re': 'you are',\n     'You.i': 'you i', 'You;d': 'you would', 'You;ll': 'you will', 'You;re': 'you are',\n     \"Your'e\": 'you are', 'YouÂ´d': 'you would', 'YouÂ´ll': 'you will', 'YouÂ´re': 'you are',\n     'Youâ€™d': 'you would', 'Youâ€™ll': 'you will', 'Youâ€™re': 'you are', 'Youâ€™ve': 'you have'\n    }\n\n    dirty_dict = {      re.compile( '[^a-zA-Z][uU] of [oO][^a-zA-Z]'): ' you of all ',\n                        re.compile('[wW][hH][^a-zA-Z ][^a-zA-Z ][eE]'):'whore' ,                  #  wh**e\n                        re.compile('[wW][hH][^a-zA-Z ][rR][eE]'):'whore',                         #  wh*re   \n                        re.compile('[wW][^a-zA-Z ][oO][rR][eE]'):'whore',                         #  w*ore  \n                        '[wW] h o r e':'whore',\n                      #  re.compile('[sS][hH][^a-zA-Z ][tT] '):'shit ',                         #   sh*t_\n                        re.compile(' [sS][hH][^a-zA-Z ][tT]'):' shit',                         #   _sh*t\n                        re.compile('[sS][hH][*_x][tT] '):'shit ',                            #   sh*t\n                        re.compile(' [sS][^a-zA-Z ][^a-zA-Z ][tT]'):' shit',                   #   _s**t\n                      #  re.compile('[sS][^a-zA-Z ][^a-zA-Z ][tT] '):'shit ',                   #   s**t_\n                        re.compile('[sS][-*_x][-*_x][tT] '):'shit ',                      #   s**t\n                      #  re.compile('[sS][hH][^a-zA-Z ][^a-zA-Z ] '):'shit ',                    #   sh**_ \n                        re.compile(' [sS][hH][^a-zA-Z ][^a-zA-Z ]'):' shit',                     #   _sh** \n                      #  re.compile('[sS][^a-zA-Z ][iI][tT] '):'shit ',                         #   s*it_   \n                        re.compile(' [sS][^a-zA-Z ][iI][tT]'):' shit',                         #   _s*it \n                        re.compile('[sS][-*_x][iI][tT] '):'shit ',                            #   shit\n                        '[sS] h i t':'shit','5h1t': 'shit',\n                        re.compile(' [fF][^a-zA-Z ][^a-zA-Z ][kK]'):' fuck',                   #   _f**k\n                        re.compile('[fF][^a-zA-Z ][^a-zA-Z ][kK] '):'fuck ',                   #   f**k_\n                        re.compile('[fF][-*_x][-*_x][kK]'):'fuck',                       #   f**k\n                        re.compile(' [fF][^a-zA-Z ][cC][kK]'):' fuck',                         #   _f*ck\n                        re.compile('[fF][^a-zA-Z ][cC][kK] '):'fuck ',                         #   f*ck_\n                        re.compile('[fF][-*_x][cC][kK]'):'fuck',                            #   f*ck\n                        re.compile(' [fF][uU][^a-zA-Z ][kK]'):' fuck',                         #   _fu*k\n                        re.compile('[fF][uU][^a-zA-Z ][kK] '):'fuck ',                         #   fu*k_\n                        re.compile('[fF][uU][-*_x][kK]'):'fuck',                            #   fu*k\n                        '[pP]huk': 'fuck','[pP]huck': 'fuck','[fF]ukk':'fuck','[fF] u c k':'fuck',\n                        '[fF]cuk': 'fuck',' [fF]uks': ' fucks',              \n                        re.compile(' [dD][^a-zA-Z ][^a-zA-Z ][kK]'):' dick',                   #   _d**k\n                        re.compile('[dD][^a-zA-Z ][^a-zA-Z ][kK] '):'dick ',                   #   d**k_\n                        re.compile('[dD][-*_x][-*_x][kK]'):'dick',                       #   d**k\n                        re.compile(' [dD][^a-zA-Z ][cC][kK]'):' dick',                         #   _d*ck\n                        re.compile('[dD][^a-zA-Z ][cC][kK] '):'dick ',                         #   d*ck_\n                        re.compile('[dD][-*_x][cC][kK]'):'dick',                            #   d*ck\n                        re.compile(' [dD][iI][^a-zA-Z ][kK]'):' dick',                         #   _di*k\n                        re.compile('[dD][iI][^a-zA-Z ][kK] '):'dick ',                         #   di*k_\n                        re.compile('[dD][iI][-*_x][kK]'):'dick',                            #   di*k\n\n                        re.compile(' [sS][^a-zA-Z ][cC][kK]'):' suck',                         #   _s*ck\n                        re.compile('[sS][^a-zA-Z ][cC][kK] '):'suck ',                         #   s*ck_\n                        re.compile('[sS][-*_x][cC][kK]'):'suck',                            #   s*ck\n                        re.compile(' [sS][uU][^a-zA-Z ][kK]'):' suck',                         #   _su*k\n                        re.compile('[sS][uU][^a-zA-Z ][kK] '):'suck ',                         #   su*k_\n                        re.compile('[sS][uU][-*_x][kK]'):'suck',                            #   su*k\n\n                        re.compile(' [cC][^a-zA-Z ][nN][tT]'):' cunt',                         #   _c*nt\n                        re.compile('[cC][^a-zA-Z ][nN][tT] '):'cunt ',                         #   c*nt_\n                        re.compile('[cC][-*_x][nN][tT]'):'cunt',                            #   c*nt\n                        re.compile(' [cC][uU][^a-zA-Z ][tT]'):' cunt',                         #   _cu*t\n                        re.compile('[cC][uU][^a-zA-Z ][tT] '):'cunt ',                         #   cu*t_\n                        re.compile('[cC][uU][-*_x][tT]'):'cunt',                            #   cu*t\n\n                        re.compile(' [bB][^a-zA-Z ][tT][cC][hH]'):' bitch',                       #   _b*tch\n                        re.compile('[bB][^a-zA-Z ][tT][cC][hH] '):'bitch ',                       #   b*tch_\n                        re.compile('[bB][-*_x][tT][cC][hH]'):'bitch',                          #   b*tch\n                        re.compile(' [bB][iI][^a-zA-Z ][cC][hH]'):' bitch',                       #   _bi*ch\n                        re.compile('[bB][iI][^a-zA-Z ][cC][hH] '):'bitch ',                       #   bi*ch_\n                        re.compile('[bB][iI][-*_x][cC][hH]'):'bitch',                          #   bi*ch\n                        re.compile(' [bB][iI][tT][^a-zA-Z ][hH]'):' bitch',                       #   _bit*h\n                        re.compile('[bB][iI][tT][^a-zA-Z ][hH]'):'bitch ',                       #   bit*h_\n                        re.compile('[bB][iI][tT][-*_x][hH]'):'bitch',                          #   bit*h\n                        re.compile('[bB][^a-zA-Z ][tT][^a-zA-Z ][hH]'):'bitch',                   #   b*t*h\n                        '[bB] i t c h':'bitch',\n                        re.compile('[aA][*_]s'):'ass',                                #   a*s\n                        re.compile('[aA][^a-zA-Z ][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',               #   a**hole\n                        re.compile(' [aA][^a-zA-Z ][^a-zA-Z ][hH]'):' assh',                   #   a**h\n                        re.compile('[aA][^a-zA-Z ][sS][hH][oO][lL][eE]'):'asshole',                     #   a*shole\n                        re.compile('[aA][sS][^a-zA-Z ][hH][oO][lL][eE]'):'asshole',                     #   as*hole\n                        ' [aA]s[*]':' ass','[aA] s s': 'ass ','[aA]sswhole': 'ass hole',\n                        re.compile('[aA]ssh[^a-zA-Z ]le'):'asshole',                     #   assh*le\n                        '[hH] o l e':'hole',\n                        '[bB][*]ll': 'bull', \n                        re.compile('[pP][^a-zA-Z ][sS][sS][yY]'):' pussy',                         #   p*ssy\n                        re.compile('[pP][uU][^a-zA-Z ][sS][yY]'):' pussy',                         #   pu*sy\n                        re.compile('[pP][uU][sS][^a-zA-Z ][yY]'):' pussy',                         #   pus*y\n                        re.compile('[pP][uU][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',                   #   pu**y\n                        re.compile('[pP][^a-zA-Z ][^a-zA-Z ][sS][yY]'):' pussy',                   #   p**sy\n                        re.compile(' [pP][^a-zA-Z ][^a-zA-Z ][^a-zA-Z ][yY]'):' pussy',            #   _pussy\n                        '[pP]ussi': 'pussy', '[pP]ussies': 'pussy','[pP]ussys': 'pussy', \n                        '[jJ]ack[-]off': 'jerk off','[mM]asterbat[*]': 'masterbate','[gG]od[-]dam': 'god damm',\n                }\n    from nltk.tokenize.treebank import TreebankWordTokenizer\n    tokenizer2 = TreebankWordTokenizer()\n\n    isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n    remove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n    def pre_clean_abbr_words(x):\n        dic = abbr_mapping\n        for word in dic.keys():\n            #if word in x:\n            x = x.replace(word, dic[word])\n        return x\n\n    def clean_regex_words(x):\n        dic = regex_mapping\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n\n    def correct_contraction(x):\n        dic = contraction_mapping\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n    def correct_contraction1(x):\n        dic = contraction_mapping1\n        for word in dic.keys():\n            if word in x:\n                x = x.replace(word, dic[word])\n        return x\n\n    def clean_dirty_dict(x):\n        dic = dirty_dict\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x  \n\n\n    def handle_punctuation(x):\n        x = x.translate(remove_dict)\n        x = x.translate(isolate_dict)\n        return x\n\n    def new_final_contraction(x):\n        dic = new_final_mapping\n        for word in dic.keys():\n            x = re.sub(word, dic[word],x)\n        return x \n\n    def handle_contractions(x):\n        x = tokenizer2.tokenize(x)\n        return x\n\n    def fix_quote(x):\n        x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n        x = ' '.join(x)\n        return x\n\n    def preprocess(df_comment):\n\n        # lower\n        # clean misspellings\n        #df_comment = df_comment.str.lower()\n        df_comment = df_comment.apply(pre_clean_abbr_words)\n        df_comment = df_comment.apply(clean_regex_words)\n\n        df_comment = df_comment.apply(correct_contraction) \n        df_comment = df_comment.apply(correct_contraction1) \n        df_comment = df_comment.apply(clean_dirty_dict)\n\n        # clean the text\n        df_comment = df_comment.apply(lambda x:handle_punctuation(x))\n        df_comment = df_comment.apply(new_final_contraction)\n\n        df_comment = df_comment.apply(handle_contractions)\n        df_comment = df_comment.apply(fix_quote)\n\n        return df_comment\n    \n    print('bert_based_cased')\n    BERT_PRETRAINED_DIR = '../input/bertprototype/cased_l-12_h-768_a-12/cased_L-12_H-768_A-12/' \n    print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n    config_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n    checkpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n    dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')  \n    tokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=False)\n    print('build tokenizer done')\n    modelb = load_trained_model_from_checkpoint(config_file,training=True,seq_len=maxlen)\n    \n    sequence_outputb  = modelb.layers[-6].output\n    pool_outputb = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_outputb)\n    aux_outputb = Dense(6,activation='sigmoid',name = 'aux_output')(sequence_outputb)\n    model2  = Model(inputs=modelb.input, outputs=[pool_outputb,aux_outputb])\n    #model2.compile(optimizer=adamwarm,loss='mse')\n    #low\n    model2.load_weights('../input/final-models-group1/bertcased_pre_220_95089.h5')\n    print('load ba models cased')\n    eval_lines = (preprocess(test_df['comment_text'])).values\n    token_input2 = convert_lines(eval_lines,maxlen,tokenizer)\n    print(token_input2[:3])\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_cased_low',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_cased_low.csv', index=False)\n    \n    #high\n    model2.load_weights('../input/final-model-group2/bertcased_pre_220_95108_ep2.h5')\n    print('load ba models cased')\n    hehe_model4 = (model2.predict([token_input2, seg_input2,mask_input2],verbose=1,batch_size=256))[0]#\n    print('bertbase_cased_low',hehe_model4[:5])\n    submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': hehe_model4.flatten()\n    })\n    submission.to_csv('submission_bertbase_cased_high.csv', index=False)\n    \n    \n    K.clear_session()\nbert_get_result()\nK.clear_session()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# gpt2"},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r '../input/keras-gpt-2-latest/keras_gpt_2_latest/' '/kaggle/working'\nprint('gpt2')\nfrom keras_gpt_2_latest.keras_gpt_2.loader import load_trained_model_from_checkpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bsz = 128\nmaxlen=300\n#model_folder = '../input/gpt2-models/'\nconfig_path = '../input/gpt2hparamsjson/hparams.json'#os.path.join(model_folder, 'hparams.json')\ncheckpoint_path = 'anything you like, can be a meme.' #os.path.join(model_folder, 'model.ckpt')#can be anything\nmodel = load_trained_model_from_checkpoint(config_path,\n                                           checkpoint_path,\n                                           seq_len=maxlen,\n                                           fixed_input_shape=True)\nsequence_output  = model.get_layer(index=-2).output\nmaxpool_output = keras.layers.GlobalMaxPooling1D()(sequence_output)\navgpool_output = keras.layers.GlobalAveragePooling1D()(sequence_output)\nconc_output = keras.layers.concatenate([maxpool_output,avgpool_output])\ndropout_output = keras.layers.Dropout(0.4)(conc_output)\nreal_output = keras.layers.Dense(1,activation='sigmoid',name='real_output')(dropout_output)\naux_output = keras.layers.Dense(6,activation='sigmoid',name='aux_output')(dropout_output)\nmodel2  = keras.models.Model(inputs=model.input, outputs=[real_output,aux_output])\n\n##tokenizing\nfrom pytorch_pretrained_bert import BertTokenizer, GPT2Tokenizer\nimport sys\nimport regex as re\ncsv_file = '../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv' # train or test\ndf = pd.read_csv(csv_file)#[:1024]#.sample(512*2,random_state=112)\ndf['comment_text'] = df['comment_text'].astype(str)\ndf[\"comment_text\"] = df[\"comment_text\"].fillna(\"DUMMY_VALUE\")\n\ndef tokenize(self, text):\n    \"\"\" Tokenize a string. \"\"\"\n    bpe_tokens = []\n    for token in re.findall(self.pat, text):\n        # token = ''.join(self.byte_encoder[ord(b)] for b in token.encode('utf-8'))\n        if sys.version_info[0] == 2:\n            token = ''.join(self.byte_encoder[ord(b)] for b in token)\n        else:\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n        bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(' '))\n    return bpe_tokens\n\ndef convert_lines_gpt2(example, max_seq_length, tokenizer):\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        text = re.sub('[ ]+',' ',text)\n        tokens_a = tokenizer.tokenize(tokenizer, text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:int(max_seq_length/2)] + tokens_a[-int(max_seq_length/2):]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0]*(max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef extract_data_gpt2(\n    model_path,\n    csv_file,\n    dataset,\n    max_sequence_length,\n    output_path,\n):\n    os.makedirs(output_path, exist_ok=True)\n    tokenizer = GPT2Tokenizer.from_pretrained(model_path, cache_dir=None)\n    tokenizer.tokenize = tokenize\n    sequences = convert_lines_gpt2(df[\"comment_text\"].values, max_sequence_length, tokenizer)\n    return sequences\n\nmodel_path = '../input/gpt2-models'\ntoken_input2 = extract_data_gpt2(model_path,csv_file,dataset='gpt2',max_sequence_length=maxlen,output_path=' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.load_weights('../input/gpt2-raw-300-tk0-95217-ep2/gpt2_raw_300_tk0_95217_ep2.h5')\nhehe_model4 = (model2.predict(token_input2,verbose=1,batch_size=bsz))[0]\nprint('gpt2',hehe_model4[:5])\nsubmission = pd.DataFrame.from_dict({\n'id': df['id'],\n'prediction': hehe_model4.flatten()\n})\nsubmission.to_csv('submission_gpt2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\nK.clear_session()\ngc.collect()\n%reset -sf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# all"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nresult_bert_df_9 = pd.read_csv('submission_bertlarge.csv')\nresult_bert_9 = result_bert_df_9['prediction'].values.flatten()\nresult_bert_df_6 = pd.read_csv('submission_bertbase_low.csv')\nresult_bert_6 = result_bert_df_6['prediction'].values.flatten()\nresult_bert_df_7 = pd.read_csv('submission_bertbase_high.csv')\nresult_bert_7 = result_bert_df_7['prediction'].values.flatten()\nresult_bert_df_8 = pd.read_csv('submission_bertbase_cased_low.csv')\nresult_bert_8 = result_bert_df_8['prediction'].values.flatten()\nresult_bert_df_5 = pd.read_csv('submission_bertbase_cased_high.csv')\nresult_bert_5 = result_bert_df_5['prediction'].values.flatten()\nresult_gpt2_df = pd.read_csv('submission_gpt2.csv')\nresult_gpt2 = result_gpt2_df['prediction'].values.flatten()\n\nresult_ensemble = (result_bert_9+result_bert_6+result_bert_7+result_bert_8+result_bert_5+result_gpt2)/6.\n\nsubmission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': result_ensemble\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}