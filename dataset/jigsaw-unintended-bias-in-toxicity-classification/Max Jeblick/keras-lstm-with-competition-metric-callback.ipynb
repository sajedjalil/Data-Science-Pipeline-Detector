{"cells":[{"metadata":{},"cell_type":"markdown","source":"**The following notebook adds the competition metric as a callback to the keras baseline lstm with attention kernel.** "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfrom sklearn import metrics\nfrom tqdm import tqdm\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"TEXT_COL = 'comment_text'\nEMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\ntrain = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert taget and identity columns to booleans. This is need to calculate the competition metric (compare to 'subgroup_size': len(dataset[dataset[subgroup]])) within the competition metric\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ntrain = convert_dataframe_to_bool(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(embed_dir=EMB_PATH):\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n    return embedding_index\n\ndef build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n    embedding_matrix = np.zeros((max_features, 300))\n    for word, i in tqdm(word_index.items(),disable = not verbose):\n        if lower:\n            word = word.lower()\n        if i >= max_features: continue\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            embedding_vector = embeddings_index[\"unknown\"]\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix\n\ndef build_matrix(word_index, embeddings_index):\n    embedding_matrix = np.zeros((len(word_index) + 1,300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embeddings_index[word]\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport gc\n\nmaxlen = 220\nmax_features = 100000\nembed_size = 300\ntokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n#tokenizer = text.Tokenizer(num_words=max_features)\nprint('fitting tokenizer')\ntokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\nword_index = tokenizer.word_index\nX_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\ny_train = train['target'].values\nX_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\n\ndel tokenizer\ngc.collect()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = load_embeddings()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = build_matrix(word_index, embeddings_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.layers as L\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\ndef build_model(verbose = False, compile = True):\n    sequence_input = L.Input(shape=(maxlen,), dtype='int32')\n    embedding_layer = L.Embedding(len(word_index) + 1,\n                                300,\n                                weights=[embedding_matrix],\n                                input_length=maxlen,\n                                trainable=False)\n    x = embedding_layer(sequence_input)\n    x = L.SpatialDropout1D(0.2)(x)\n    x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n\n    att = Attention(maxlen)(x)\n    avg_pool1 = L.GlobalAveragePooling1D()(x)\n    max_pool1 = L.GlobalMaxPooling1D()(x)\n\n    x = L.concatenate([att,avg_pool1, max_pool1])\n\n    preds = L.Dense(1, activation='sigmoid')(x)\n\n\n    model = Model(sequence_input, preds)\n    if verbose:\n        model.summary()\n    if compile:\n        model.compile(loss='binary_crossentropy',optimizer=Adam(0.005),metrics=['acc'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Competition metric**\nTHe following code which computes the competition metric is taken from the official benchmark kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\nTOXICITY_COLUMN = 'target'\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\n#Diasbale SettingWithCopyWarning of pandas, see https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\npd.options.mode.chained_assignment = None  # default='warn'\n\nclass CompetitionMetricEarlyStopping(EarlyStopping):\n    \n    def __init__(self, model, df, X_valid, *args, **kwargs):\n        kwargs['monitor'] = 'val_competition_metric'\n        \n        self.model = model\n        self.df = df\n        self.X_valid = X_valid\n        self.bias_metrics_df = None\n        super().__init__(*args, **kwargs)\n            \n    def get_monitor_value(self, logs):\n        preds = self.model.predict(self.X_valid, batch_size=2048)[:, 0]\n        self.df['predictions'] = preds\n        bias_metrics_df = compute_bias_metrics_for_model(self.df, identity_columns, 'predictions', TOXICITY_COLUMN)\n        self.bias_metrics_df = bias_metrics_df\n        val_competition_metric = get_final_metric(bias_metrics_df, calculate_overall_auc(self.df, 'predictions'))\n        logs['val_competition_metric'] = val_competition_metric\n        return val_competition_metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nsplits = list(KFold(n_splits=5).split(X_train,y_train))\n\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\nimport numpy as np\nBATCH_SIZE = 2048\nNUM_EPOCHS = 100\n\noof_preds = np.zeros((X_train.shape[0]))\ntest_preds = np.zeros((X_test.shape[0]))\nfor fold in [0,1,2,3,4]:\n    K.clear_session()\n    tr_ind, val_ind = splits[fold]\n    model = build_model()\n    \n    es = CompetitionMetricEarlyStopping(model, train.iloc[val_ind], X_train[val_ind],\n                                                   restore_best_weights=False, \n                                                   mode='max',\n                                                   verbose=1,\n                                                   patience=2)\n    # CompetitionMetricEarlyStopping writes the val_competition_metric into the logs (logs is a mutable object and hence is passed by name). Thus, logs['val_competition_metric'] can be accesed, if \n    # CompetitionMetricEarlyStopping.on_epoch_end is executed before ModelCheckpoint.on_epoch_end\n    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True, monitor='val_competition_metric', mode='max', verbose=1)\n\n    model.fit(X_train[tr_ind],\n        y_train[tr_ind]>0.5,\n        batch_size=BATCH_SIZE,\n        epochs=NUM_EPOCHS,\n        validation_data=(X_train[val_ind], y_train[val_ind]>0.5), #remove validation data, if you want to improve the runtime of the kernel \n        callbacks = [es, ckpt])\n    model.load_weights(f'gru_{fold}.hdf5')\n    print(es.bias_metrics_df)\n    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n    test_preds += model.predict(X_test)[:,0]\ntest_preds /= 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['predictions'] = oof_preds\nbias_metrics_df = compute_bias_metrics_for_model(train, identity_columns, 'predictions', TOXICITY_COLUMN)\nprint(bias_metrics_df)\noff_competition_metric = get_final_metric(bias_metrics_df, calculate_overall_auc(train, 'predictions')); off_competition_metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train>0.5,oof_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\nsubmission['prediction'] = test_preds\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()\n#%%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}