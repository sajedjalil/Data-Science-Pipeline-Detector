{"cells":[{"metadata":{},"cell_type":"markdown","source":"This kernel is alomost same as [this great kernel](https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming).\nIn this kernel, I'll show you how to save the fine-tuning model.\nmaxlen = 50, sample = 1% because of save time."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport random\nimport keras\nimport tensorflow as tf\nimport json\nsys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\nBERT_PRETRAINED_DIR = '../input/pretrained-bert-including-scripts/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12'\nprint('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\nimport tokenization  #Actually keras_bert contains tokenization part, here just for convenience","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load raw model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert.keras_bert.bert import get_model\nfrom keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\nfrom keras.optimizers import Adam\nadam = Adam(lr=2e-5,decay=0.01)\nmaxlen = 50\nprint('begin_build')\n\nconfig_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\ncheckpoint_file = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nmodel = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True,seq_len=maxlen)\nmodel.summary(line_length=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build classification model\n\nAs the Extract layer extracts only the first token where \"['CLS']\" used to be, we just take the layer and connect to the single neuron output."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nimport re\nimport codecs\n\nsequence_output  = model.layers[-6].output\npool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\nmodel3  = Model(inputs=model.input, outputs=pool_output)\nmodel3.compile(loss='binary_crossentropy', optimizer=adam)\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Data, Training, Predicting\n\nFirst the model need train data like [token_input,seg_input,masked input], here we set all segment input to 0 and all masked input to 1.\n\nStill I am finding a more efficient way to do token-convert-to-ids"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for i in range(example.shape[0]):\n      tokens_a = tokenizer.tokenize(example[i])\n      if len(tokens_a)>max_seq_length:\n        tokens_a = tokens_a[:max_seq_length]\n        longer += 1\n      one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n      all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)\n    \nnb_epochs=1\nbsz = 32\ndict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\ntokenizer = tokenization.FullTokenizer(vocab_file=dict_path, do_lower_case=True)\nprint('build tokenizer done')\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntrain_df = train_df.sample(frac=0.01,random_state = 42)\n#train_df['comment_text'] = train_df['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n\ntrain_lines, train_labels = train_df['comment_text'].values, train_df.target.values \nprint('sample used',train_lines.shape)\ntoken_input = convert_lines(train_lines,maxlen,tokenizer)\nseg_input = np.zeros((token_input.shape[0],maxlen))\nmask_input = np.ones((token_input.shape[0],maxlen))\nprint(token_input.shape)\nprint(seg_input.shape)\nprint(mask_input.shape)\nprint('begin training')\nmodel3.fit([token_input, seg_input, mask_input],train_labels,batch_size=bsz,epochs=nb_epochs)\n\n# you can save the fine-tuning model by this line.\nmodel3.save_weights('bert_weights.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}