{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n### In this kernel, I will demonstrate how to use the [Perspective Comment Analysis API](https://www.perspectiveapi.com/) by Google to predict the toxicity and obscenity of comments in this competition. Click \"REQUEST API ACCESS\" on the homepage in the link above to start the process of accessing the API."},{"metadata":{},"cell_type":"markdown","source":"Perspective is an API that makes it easier to host better conversations. The API uses machine learning models to score the perceived impact a comment might have on a conversation. It can predict quantities such as how toxic or obscene a comment is or whether a it is an identity attack or an insult etc."},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://i.imgur.com/c2kASyT.png\" width=\"500px\"></center>"},{"metadata":{},"cell_type":"markdown","source":"## Acknowledgements\nCredit for the Python Wrapper for the Perspective API goes to Jake Conway for the [perspective package](https://github.com/Conway/perspective)."},{"metadata":{},"cell_type":"markdown","source":"### Import necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom langdetect import detect\n\nimport markdown\nimport json\nimport requests\nimport warnings\nimport time\n\nfrom colorama import Fore, Back, Style, init","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set up language validation and HTML parsing"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    from html.parser import HTMLParser\nexcept ImportError:\n    from HTMLParser import HTMLParser\n\ndef validate_language(language):\n    # ISO 639-1 code validation\n    # language source: https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\n    codes = [\"ab\", \"aa\", \"ae\", \"af\", \"ak\", \"am\", \"an\", \"ar\", \"as\", \"av\", \"ay\",\n             \"az\", \"ba\", \"be\", \"bg\", \"bh\", \"bi\", \"bm\", \"bn\", \"bo\", \"br\", \"bs\",\n             \"ca\", \"ce\", \"ch\", \"co\", \"cr\", \"cs\", \"cu\", \"cv\", \"cy\", \"da\", \"de\",\n             \"dv\", \"dz\", \"ee\", \"el\", \"en\", \"eo\", \"es\", \"et\", \"eu\", \"fa\", \"ff\",\n             \"fi\", \"fj\", \"fo\", \"fr\", \"fy\", \"ga\", \"gd\", \"gl\", \"gn\", \"gu\", \"gv\",\n             \"ha\", \"he\", \"hi\", \"ho\", \"hr\", \"ht\", \"hu\", \"hy\", \"hz\", \"ia\", \"id\",\n             \"ie\", \"ig\", \"ii\", \"ik\", \"io\", \"is\", \"it\", \"iu\", \"ja\", \"jv\", \"ka\",\n             \"kg\", \"ki\", \"kj\", \"kk\", \"kl\", \"km\", \"kn\", \"ko\", \"kr\", \"ks\", \"ku\",\n             \"kv\", \"kw\", \"ky\", \"la\", \"lb\", \"lg\", \"li\", \"ln\", \"lo\", \"lt\", \"lu\",\n             \"lv\", \"mg\", \"mh\", \"mi\", \"mk\", \"ml\", \"mn\", \"mr\", \"ms\", \"mt\", \"my\",\n             \"na\", \"nb\", \"nd\", \"ne\", \"ng\", \"nl\", \"nn\", \"no\", \"nr\", \"nv\", \"ny\",\n             \"oc\", \"oj\", \"om\", \"or\", \"os\", \"pa\", \"pi\", \"ps\", \"pt\", \"qu\", \"rm\",\n             \"rn\", \"ro\", \"ru\", \"rw\", \"sa\", \"sc\", \"sd\", \"se\", \"sg\", \"si\", \"sk\",\n             \"sl\", \"sm\", \"sn\", \"so\", \"sq\", \"sr\", \"ss\", \"st\", \"su\", \"sv\", \"sw\",\n             \"ta\", \"te\", \"tg\", \"th\", \"ti\", \"tk\", \"tl\", \"tn\", \"to\", \"tr\", \"ts\",\n             \"tt\", \"tw\", \"ty\", \"ug\", \"uk\", \"ur\", \"uz\", \"ve\", \"vi\", \"vo\", \"wa\",\n             \"wo\", \"xh\", \"yi\", \"yo\", \"za\", \"zh\", \"zu\"]\n    return language.lower() in codes\n\n\ndef remove_html(text, md=False):\n    if md:\n        text = markdown.markdown(text)\n    # credit: stackoverflow\n    class MLStripper(HTMLParser):\n        def __init__(self):\n            super().__init__()\n            self.reset()\n            self.strict = False\n            self.convert_charrefs= True\n            self.fed = []\n        def handle_data(self, d):\n            self.fed.append(d)\n        def get_data(self):\n            return ''.join(self.fed)\n\n    s = MLStripper()\n    s.feed(text)\n    return s.get_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a Python Wrapper for the Perspective API"},{"metadata":{"trusted":true},"cell_type":"code","source":"# allowed test types\nallowed = [\"TOXICITY\",\n           \"SEVERE_TOXICITY\",\n           \"TOXICITY_FAST\",\n           \"ATTACK_ON_AUTHOR\",\n           \"ATTACK_ON_COMMENTER\",\n           \"INCOHERENT\",\n           \"INFLAMMATORY\",\n           \"OBSCENE\",\n           \"OFF_TOPIC\",\n           \"UNSUBSTANTIAL\",\n           \"LIKELY_TO_REJECT\"]\n\nclass Perspective(object):\n\n    base_url = \"https://commentanalyzer.googleapis.com/v1alpha1\"\n\n    def __init__(self, key):\n        self.key = key\n\n    def score(self, text, tests=[\"TOXICITY\"], context=None, languages=None, do_not_store=False, token=None, text_type=None):\n        # data validation\n        # make sure it's a valid test\n        # TODO: see if an endpoint that has valid types exists\n        if isinstance(tests, str):\n            tests = [tests]\n        if not isinstance(tests, (list, dict)) or tests is None:\n            raise ValueError(\"Invalid list/dictionary provided for tests\")\n        if isinstance(tests, list):\n            new_data = {}\n            for test in tests:\n                new_data[test] = {}\n            tests = new_data\n        if text_type:\n            if text_type.lower() == \"html\":\n                text = remove_html(text)\n            elif text_type.lower() == \"md\":\n                text = remove_html(text, md=True)\n            else:\n                raise ValueError(\"{0} is not a valid text_type. Valid options are 'html' or 'md'\".format(str(text_type)))\n\n        for test in tests.keys():\n            if test not in allowed:\n                warnings.warn(\"{0} might not be accepted as a valid test.\".format(str(test)))\n            for key in tests[test].keys():\n                if key not in [\"scoreType\", \"scoreThreshhold\"]:\n                    raise ValueError(\"{0} is not a valid sub-property for {1}\".format(key, test))\n\n        # The API will only grade text less than 3k characters long\n        if len(text) > 3000:\n            # TODO: allow disassembly/reassembly of >3000char comments\n            warnings.warn(\"Perspective only allows 3000 character strings. Only the first 3000 characters will be sent for processing\")\n            text = text[:3000]\n        new_langs = []\n        if languages:\n            for language in languages:\n                language = language.lower()\n                if validate_language(language):\n                    new_langs.append(language)\n\n        # packaging data\n        url = Perspective.base_url + \"/comments:analyze\"\n        querystring = {\"key\": self.key}\n        payload_data = {\"comment\": {\"text\": text}, \"requestedAttributes\": {}}\n        for test in tests.keys():\n            payload_data[\"requestedAttributes\"][test] = tests[test]\n        if new_langs != None:\n            payload_data[\"languages\"] = new_langs\n        if do_not_store:\n            payload_data[\"doNotStore\"] = do_not_store\n        payload = json.dumps(payload_data)\n        headers = {'content-type': \"application/json\"}\n        response = requests.post(url,\n                            data=payload,\n                            headers=headers,\n                            params=querystring)\n        data = response.json()\n        if \"error\" in data.keys():\n            raise PerspectiveAPIException(data[\"error\"][\"message\"])\n        c = Comment(text, [], token)\n        base = data[\"attributeScores\"]\n        for test in tests.keys():\n            score = base[test][\"summaryScore\"][\"value\"]\n            score_type = base[test][\"summaryScore\"][\"type\"]\n            a = Attribute(test, [], score, score_type)\n            for span in base[test][\"spanScores\"]:\n                beginning = span[\"begin\"]\n                end = span[\"end\"]\n                score = span[\"score\"][\"value\"]\n                score_type = span[\"score\"][\"type\"]\n                s = Span(beginning, end, score, score_type, c)\n                a.spans.append(s)\n            c.attributes.append(a)\n        return c\n\nclass Comment(object):\n    def __init__(self, text, attributes, token):\n        self.text = text\n        self.attributes = attributes\n        self.token = token\n\n    def __getitem__(self, key):\n        if key.upper() not in allowed:\n            raise ValueError(\"value {0} does not exist\".format(key))\n        for attr in self.attributes:\n            if attr.name.lower() == key.lower():\n                return attr\n        raise ValueError(\"value {0} not found\".format(key))\n\n    def __str__(self):\n        return self.text\n\n    def __repr__(self):\n        count = 0\n        num = 0\n        for attr in self.attributes:\n            count += attr.score\n            num += 1\n        return \"<({0}) {1}>\".format(str(count/num), self.text)\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def __len__(self):\n        return len(self.text)\n\nclass Attribute(object):\n    def __init__(self, name, spans, score, score_type):\n        self.name = name\n        self.spans = spans\n        self.score = score\n        self.score_type = score_type\n\n    def __getitem__(self, index):\n        return self.spans[index]\n\n    def __iter__(self):\n        return iter(self.spans)\n\nclass Span(object):\n    def __init__(self, begin, end, score, score_type, comment):\n        self.begin = begin\n        self.end = end\n        self.score = score\n        self.score_type = score_type\n        self.comment = comment\n\n    def __str__(self):\n        return self.comment.text[self.begin:self.end]\n\n    def __repr__(self):\n        return \"<({0}) {1}>\".format(self.score, self.comment.text[self.begin:self.end])\n\nclass PerspectiveAPIException(Exception):\n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Download training data and extract necessary data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ncomments = train_df['comment_text']\ntargets = train_df['target']\nsevere_toxicities = train_df['severe_toxicity']\nobscenities = train_df['obscene']\ndel train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Perspective API Client with Google Cloud API key"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/google-api-information/Google API Key.txt') as f:\n    google_api_key = f.readline()[:-1]\nclient = Perspective(google_api_key)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test the API on a sample of the training data\nI am currently trying the API only on English comments, because it does not work on non-English text."},{"metadata":{},"cell_type":"markdown","source":"### *Note :*\n**I added this line :** *time.sleep((i + 1) - (current - start))* **, so that we make only one API call every second. This is because the free API key comes with a maximum of only 100 requests per 100 seconds.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"toxicity_scores = []\nsevere_toxicity_scores = []\nobscenity_scores = []\n\n\nstart = time.time()\nprint(\"                         EXAMPLE WORKING OF PERSPECTIVE API                          \")\nprint(\"                         ----------------------------------                          \")\nprint(\"\")\nfor i, comment in enumerate(comments[:200]):\n    if detect(comment) == 'en':\n        current = time.time()\n        time.sleep((i + 1) - (current - start)) # limit API calls to 1 per second\n        toxicity = client.score(comment, tests=[\"TOXICITY\", \"SEVERE_TOXICITY\", \"OBSCENE\"])\n        \n        target = targets[i]\n        toxicity_scores.append(toxicity[\"TOXICITY\"].score)\n        severe_toxicity_scores.append(toxicity[\"SEVERE_TOXICITY\"].score)\n        obscenity_scores.append(toxicity[\"OBSCENE\"].score)\n        \n        if i <= 50:\n            print(\"COMMENT :\\n\" + comment)\n            print(\"\")\n            if np.round(toxicity[\"TOXICITY\"].score) == np.round(target):\n                print(\"TOXICITY SCORE : \" + str(toxicity[\"TOXICITY\"].score) +\\\n                      f' {Fore.GREEN}CORRECT \\u2714{Style.RESET_ALL}')\n            else:\n                print(\"TOXICITY SCORE : \" + str(toxicity[\"TOXICITY\"].score) +\\\n                      f' {Fore.RED}WRONG \\u2716{Style.RESET_ALL}')\n            print(\"TARGET : \" + str(target))\n            print(\"\")\n            if np.round(toxicity[\"SEVERE_TOXICITY\"].score) == np.round(severe_toxicities[i]):\n                print(\"SEVERE TOXICITY SCORE : \" + str(toxicity[\"SEVERE_TOXICITY\"].score) +\\\n                      f' {Fore.GREEN}CORRECT \\u2714{Style.RESET_ALL}')\n            else:\n                print(\"SEVERE TOXICITY SCORE : \" + str(toxicity[\"SEVERE_TOXICITY\"].score) +\\\n                      f' {Fore.RED}WRONG \\u2716{Style.RESET_ALL}')\n            print(\"TARGET : \" + str(severe_toxicities[i])) \n            print(\"\")\n            if np.round(toxicity[\"OBSCENE\"].score) == np.round(obscenities[i]):\n                print(\"OBSCENITY SCORE : \" + str(toxicity[\"OBSCENE\"].score) +\\\n                      f' {Fore.GREEN}CORRECT \\u2714{Style.RESET_ALL}')\n            else:\n                print(\"OBSCENITY SCORE : \" + str(toxicity[\"OBSCENE\"].score) +\\\n                      f' {Fore.RED}WRONG \\u2716{Style.RESET_ALL}')\n            print(\"TARGET : \" + str(obscenities[i]))\n            print((\"*********************************************************************\"+\\\n                   \"***********************\").replace('*', '-'))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**If you closely look at the ones which the model gets wrong, thet are generally the comments which have somewhat neutral toxicity (around 0.5) or the ones that seem to be wrongly labeled.**"},{"metadata":{},"cell_type":"markdown","source":"**For example :** \n* **This comment : *\"This bitch is nuts. Who would read a book by a woman.\"* seems severely toxic. The model labeled it as severely toxic, but the actual target says otherwise.**\n* **There are several examples of neutral toxicity (in the range 0.4 to 0.6) in the comments, which the model got narrowly wrong.**"},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Results"},{"metadata":{},"cell_type":"markdown","source":"### Binary Classification Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Toxicity Classification Accuracy : \" +\\\n      str(accuracy_score(np.round(targets[:len(toxicity_scores)]), np.round(toxicity_scores[:len(toxicity_scores)]))))\n\nprint(\"Obscenity Classification Accuracy : \" +\\\n      str(accuracy_score(np.round(obscenities[:len(toxicity_scores)]), np.round(obscenity_scores[:len(toxicity_scores)]))))\n\nprint(\"Severe Toxicity Classification Accuracy : \" +\\\n      str(accuracy_score(np.round(severe_toxicities[:len(toxicity_scores)]), np.round(severe_toxicity_scores[:len(toxicity_scores)]))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Absolute Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Toxicity Mean Absolute Error : \" +\\\n      str(mean_absolute_error(targets[:len(toxicity_scores)], toxicity_scores[:len(toxicity_scores)])))\n\nprint(\"Obscneity Mean Absolute Error : \" +\\\n      str(mean_absolute_error(obscenities[:len(toxicity_scores)], obscenity_scores[:len(toxicity_scores)])))\n\nprint(\"Severe Toxicity Mean Absolute Error : \" +\\\n      str(mean_absolute_error(severe_toxicities[:len(toxicity_scores)], severe_toxicity_scores[:len(toxicity_scores)])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mean Squared Error"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Toxicity Squared Absolute Error : \" +\\\n      str(mean_squared_error(targets[:len(toxicity_scores)], toxicity_scores[:len(toxicity_scores)])))\n\nprint(\"Obscneity Squared Absolute Error : \" +\\\n      str(mean_squared_error(obscenities[:len(toxicity_scores)], obscenity_scores[:len(toxicity_scores)])))\n\nprint(\"Severe Toxicity Squared Absolute Error : \" +\\\n      str(mean_squared_error(severe_toxicities[:len(toxicity_scores)], severe_toxicity_scores[:len(toxicity_scores)])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Perspective API seems to perform pretty well on the data in this competition, but **it probably will not pass stage 2 due to constraints on kernel time limits and API requests per second**. "},{"metadata":{},"cell_type":"markdown","source":"### That's it ! Thanks for reading my kernel ! I hope you found it useful :)"},{"metadata":{},"cell_type":"markdown","source":"### Please drop your comments, feedback or criticism in the comments below."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}