{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\nIn this kernel I'll do EDA and visualization of the data, maybe even modelling, though I plan to do serious modelling in my other kernels.\n\nWe have quite an interesting data. We are challenged to build a model that recognizes toxicity and minimizes unintended bias with respect to mentions of identities.\nFor examplewe need to make sure that a comment like \"I am a gay woman\" is considered to be not toxic.\n\n**Two important points**:\n1. A subset of comments is labeled with identities. Only identities with more than 500 examples in the test set will be included in the evaluation calculation. This means that not all the test data will be included in evaluation. If we can correctly extract identities, then we will know which test samples are evaluated.\n2. Target column was created as a fraction of human raters who believed that the comment is toxic. For evaluation, test set examples with target >= 0.5 will be considered to be in the positive class (toxic). I think that we could try both regression and classification approaches here."},{"metadata":{},"cell_type":"markdown","source":"## Content\n* [1 Data overview](#overview)\n* [1.1 Additional toxic subtypes](#add_toxic)\n* [2 Text overview](#text_overview)\n* [2.1 Text length](#text_l)\n* [2.2 Word count](#word_c)\n* [3 Basic model](#basic_model)\n* [3.1 Validation function](#validation_function)\n* [4 ELI5 for model interpretation](#eli5)\n* [5 Interpreting deep learning models with LIME](#lime)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\npd.set_option('max_columns', 50)\nimport json\nimport altair as alt\nfrom  altair.vega import v3\nfrom IPython.display import HTML\nimport gc\nimport os\nprint(os.listdir(\"../input\"))\nimport lime\nimport eli5\nfrom eli5.lime import TextExplainer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\nvega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"</script>\",\n)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"overview\"></a>\n## Data overview"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\nsub = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, (train['target'] > 0).sum() / train.shape[0], (train['target'] >= 0.5).sum() / train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'].value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['comment_text'] == 'Well said.', 'target'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Rate of unique comments:', train['comment_text'].nunique() / train['comment_text'].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_comments = set(train['comment_text'].values)\ntest_comments = set(test['comment_text'].values)\nlen(train_comments.intersection(test_comments)), len(test.loc[test['comment_text'].isin(list(train_comments.intersection(test_comments)))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We have a lot of data in train - 1.8 mln rows! Test data has less than 100k rows. There are also additional columns in train, we'll look at them later.\n29% samples have value of target higher than 0 and only 7.99% have target higher than 0.5.\n- One more point: ~1.4% of all comments are duplicates and they can have different target values.\n- 1170 unique comments from train data are in test data;"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"hist_df = pd.cut(train['target'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['bins'] = hist_df['bins'].astype(str)\nrender(alt.Chart(hist_df).mark_bar().encode(\n    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n    y=alt.Y('target:Q', axis=alt.Axis(title='Count')),\n    tooltip=['target', 'bins']\n).properties(title=\"Counts of target bins\", width=400).interactive())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of comments aren't toxic. We can also see some spikes in the distribution..."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Do you remember how target was created? This is a fraction of voters who considered the comment to be toxic. Then is is completely normal that 0%, 1/6, 1/5 of voters could think the same."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\ncounts = train.groupby(['created_date'])['target'].mean().sort_index().reset_index()\nmeans = train.groupby(['created_date'])['target'].count().sort_index().reset_index()\nc = alt.Chart(counts).mark_line().encode(\n    x=alt.X(\"created_date:T\", axis=alt.Axis(title='Date')),\n    y=alt.Y('target:Q', axis=alt.Axis(title='Rate')),\n    tooltip=[alt.Tooltip('created_date:T', timeUnit='yearmonth'), alt.Tooltip('target:Q')]\n).properties(title=\"Counts and toxicity rate of comments\", width=800).interactive()\nr = alt.Chart(means).mark_line(color='green').encode(\n    x=alt.X(\"created_date:T\", axis=alt.Axis(title='Date')),\n    y=alt.Y('target:Q', axis=alt.Axis(title='Counts')),\n    tooltip=[alt.Tooltip('created_date:T', timeUnit='yearmonth'), alt.Tooltip('target:Q')],\n).properties().interactive()\nrender(alt.layer(\n    c,\n    r\n).resolve_scale(\n    y='independent'\n))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see how despite the increase of number of comments the toxicity rate is quite stable."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"add_toxic\"></a>\n### Additional toxic subtypes\n\nHere I plot histogram of scores for additional toxicity subtypes **for scores higher that 0**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plot_dict = {}\nfor col in ['severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']:\n    df_ = train.loc[train[col] > 0]\n    hist_df = pd.cut(df_[col], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n    hist_df['bins'] = hist_df['bins'].astype(str)\n    plot_dict[col] = alt.Chart(hist_df).mark_bar().encode(\n        x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n        y=alt.Y(f'{col}:Q', axis=alt.Axis(title='Count')),\n        tooltip=[col, 'bins']\n    ).properties(title=f\"Counts of {col} bins\", width=300, height=200).interactive()\n    \nrender((plot_dict['severe_toxicity'] | plot_dict['obscene']) & (plot_dict['threat'] | plot_dict['insult']) & (plot_dict['identity_attack'] | plot_dict['sexual_explicit']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"text_overview\"></a>\n## Text exploration"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"text_l\"></a>\n### Text length"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hist_df = pd.cut(train['comment_text'].apply(lambda x: len(x)), 10).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['bins'] = hist_df['bins'].astype(str)\nrender(alt.Chart(hist_df).mark_bar().encode(\n    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins'), sort=list(hist_df['bins'].values)),\n    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Count')),\n    tooltip=['comment_text', 'bins']\n).properties(title=\"Counts of target bins of text length\", width=400).interactive())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_length = train['comment_text'].apply(lambda x: len(x)).value_counts(normalize=True).sort_index().cumsum().reset_index().rename(columns={'index': 'Text length'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"render(alt.Chart(text_length).mark_line().encode(\n    x=alt.X(\"Text length:Q\", axis=alt.Axis(title='Text length')),\n    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Cummulative rate')),\n    tooltip=['Text length', 'comment_text']\n).properties(title=\"Cummulative text length\", width=400).interactive())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seeems that there is relatively high number of comments with length 1000. Maybe this is some kind of default max length?"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"word_c\"></a>\n### Word count"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"hist_df = pd.cut(train['comment_text'].apply(lambda x: len(x.split())), 10).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['bins'] = hist_df['bins'].astype(str)\nrender(alt.Chart(hist_df).mark_bar().encode(\n    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins'), sort=list(hist_df['bins'].values)),\n    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Count')),\n    tooltip=['comment_text', 'bins']\n).properties(title=\"Counts of target bins of word count\", width=400).interactive())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"word_count = train['comment_text'].apply(lambda x: len(x.split())).value_counts(normalize=True).sort_index().cumsum().reset_index().rename(columns={'index': 'Word count'})\nrender(alt.Chart(word_count).mark_line().encode(\n    x=alt.X(\"Word count:Q\", axis=alt.Axis(title='Text length')),\n    y=alt.Y('comment_text:Q', axis=alt.Axis(title='Cummulative rate')),\n    tooltip=['Word count:Q', 'comment_text']\n).properties(title=\"Cummulative word cound\", width=400).interactive())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that ~ 90% of all comments have less than 125 words."},{"metadata":{},"cell_type":"markdown","source":"## Identities\n\nSome of the comments are labeled with identities, but only eight of them are included into evaluation: male, female, homosexual_gay_or_lesbian, christian, jewish, muslim, black, white, psychiatric_or_mental_illness."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"basic_model\"></a>\n## Basic model\n\nLet's try building a baseline logistic regression on tf-idf and see what words are considered to be toxic."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll load processed texts from my kernel\ntrain = pd.read_csv('../input/jigsaw-public-files/train.csv')\ntest = pd.read_csv('../input/jigsaw-public-files/test.csv')\ntrain['comment_text'] = train['comment_text'].fillna('')\ntest['comment_text'] = test['comment_text'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\nfor col in identity_columns + ['target']:\n    train[col] = np.where(train[col] >= 0.5, True, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, valid_df = train_test_split(train, test_size=0.1, stratify=train['target'])\ny_train = train_df['target']\ny_valid = valid_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntokenizer = TweetTokenizer()\n\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize, max_features=30000)\nvectorizer.fit(train['comment_text'].values)\ntrain_vectorized = vectorizer.transform(train_df['comment_text'].values)\nvalid_vectorized = vectorizer.transform(valid_df['comment_text'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlogreg = LogisticRegression()\nlogreg.fit(train_vectorized, y_train)\noof_name = 'predicted_target'\nvalid_df[oof_name] = logreg.predict_proba(valid_vectorized)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"validation_function\"></a>\n### Validation function\nI use code from benchmark kernel"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, oof_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[oof_name])\n\ndef compute_bpsn_auc(df, subgroup, label, oof_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[oof_name])\n\ndef compute_bnsp_auc(df, subgroup, label, oof_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[oof_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\noof_name = 'predicted_target'\nbias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, oof_name, 'target')\nbias_metrics_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def calculate_overall_auc(df, oof_name):\n    true_labels = df['target']\n    predicted_labels = df[oof_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total / len(series), 1 / p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n    \nget_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, oof_name))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"eli5\"></a>\n## ELI5 for model interpretation\n\nAnd now let's use ELI5 to see how model makes predictions!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.lime import TextExplainer\n\nte = TextExplainer(random_state=42)\ndef model_predict(x):\n    return logreg.predict_proba(vectorizer.transform(x))\nte.fit(valid_df['comment_text'].values[2:3][0], model_predict)\nte.show_prediction()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te.fit(valid_df['comment_text'].values[12:13][0], model_predict)\nte.show_prediction()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_vectorized = vectorizer.transform(test['comment_text'].values)\nsub['prediction'] = logreg.predict_proba(test_vectorized)[:, 1]\nsub.to_csv('submission.csv', index=False)\ndel logreg, vectorizer, test_vectorized, train_vectorized, valid_vectorized","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting number of words and sequence length\n\nOn of important hyperparameters for our neural nets will be the number of words in tokenizer and the number of words in sequence. Let's compare model AUC for different values of these parameters.\n\nFor preparing data I use code from my kernel: https://www.kaggle.com/artgor/basic-cnn-in-keras\n\nI train the same model on the same data for 3 epochs."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def build_model(X_train, y_train, X_valid, y_valid, max_len, max_features, embedding_matrix, lr=0.0, lr_d=0.0, spatial_dr=0.0,  dense_units=128, dr=0.1):\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                                  save_best_only = True, mode = \"min\")\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n    \n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n    # from benchmark kernel\n    x = Conv1D(128, 2, activation='relu', padding='same')(x1)\n    x = MaxPooling1D(5, padding='same')(x)\n    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n    x = MaxPooling1D(5, padding='same')(x)\n    x = Flatten()(x)\n    \n    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n    x = Dense(2, activation = \"softmax\")(x)\n    \n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size = 128, epochs = 3, validation_data=(X_valid, y_valid), \n                        verbose = 0, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_text = list(train['comment_text'].values) + list(test['comment_text'].values)\nembedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nembed_size = 300\noof_name = 'oof_name'\n\ndef calculate_score(num_words, max_len, full_text, train_df, valid_df, embedding_path, embed_size, identity_columns, oof_name):\n    tk = Tokenizer(lower = True, filters='', num_words=num_words)\n    tk.fit_on_texts(full_text)\n    \n    def get_coefs(word,*arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n    embedding_matrix = np.zeros((num_words + 1, embed_size))\n    for word, i in tk.word_index.items():\n        if i >= num_words: continue\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    del embedding_index\n            \n    train_tokenized = tk.texts_to_sequences(train_df['comment_text'])\n    valid_tokenized = tk.texts_to_sequences(valid_df['comment_text'])\n\n    X_train = pad_sequences(train_tokenized, maxlen = max_len)\n    X_valid = pad_sequences(valid_tokenized, maxlen = max_len)\n    \n    model = build_model(X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, max_len=max_len, max_features=embedding_matrix.shape[0], embedding_matrix=embedding_matrix,\n                        lr = 1e-3, lr_d = 0, spatial_dr = 0.0, dr=0.1)\n    \n    valid_df[oof_name] = model.predict(X_valid)\n    bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, oof_name, 'target')\n    score = get_final_metric(bias_metrics_df, calculate_overall_auc(valid_df, oof_name))\n    del embedding_matrix, tk\n    gc.collect()\n    \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = []\n# for n_words in [50000, 100000]:\n#     for seq_len in [150, 300]:\n#         loc_score = calculate_score(n_words, seq_len, full_text, train_df, valid_df, embedding_path, embed_size, identity_columns, oof_name)\n#         scores.append((n_words, seq_len, loc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running this loop takes a lot of time, so here is the result:\n\n![](https://i.imgur.com/fISAEg7.png)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"lime\"></a>\n## Interpreting deep learning models with LIME\n\nPreviously we were able to interpret logreg predictions, but who uses logreg in such competitions? :)\n\nSo let's try using a similar method to interpret deep learning model prediction! Technically it works almost the same:\n* train DL model with 2 classes;\n* write a function to make prediction on raw texts;\n* use ELI5 with LIME"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = 150000\nmax_len = 220\ntk = Tokenizer(lower = True, filters='', num_words=num_words)\ntk.fit_on_texts(full_text)\n\ndef get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\nembedding_matrix = np.zeros((num_words + 1, embed_size))\nfor word, i in tk.word_index.items():\n    if i >= num_words: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\ndel embedding_index\n\ntrain_tokenized = tk.texts_to_sequences(train_df['comment_text'])\nvalid_tokenized = tk.texts_to_sequences(valid_df['comment_text'])\n\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_valid = pad_sequences(valid_tokenized, maxlen = max_len)\n\nmodel = build_model(X_train=X_train, y_train=pd.get_dummies(y_train), X_valid=X_valid, y_valid=pd.get_dummies(y_valid), max_len=max_len, max_features=embedding_matrix.shape[0],\n                    embedding_matrix=embedding_matrix,\n                    lr = 1e-3, lr_d = 0, spatial_dr = 0.0, dr=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te = TextExplainer(random_state=42)\ndef dl_predict(x):\n    return model.predict(pad_sequences(tk.texts_to_sequences(np.array(x)), maxlen = max_len))\nte.fit(valid_df['comment_text'].values[3:4][0], dl_predict)\nte.show_prediction(target_names=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can see how our neural net makes predictions and use it to improve the model!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}