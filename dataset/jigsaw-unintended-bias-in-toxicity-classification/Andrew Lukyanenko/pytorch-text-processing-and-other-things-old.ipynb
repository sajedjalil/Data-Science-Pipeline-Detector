{"cells":[{"metadata":{},"cell_type":"markdown","source":"## General information\n\n**UPD:** It seems that I messed up: importing scripts is considered to be using other kernel output, which prevents submitting from this kernel. Also this script can't be removed from this kernel, so I can't fix it :( No fun.\n\nSo here is a new kernel where all the code is inside as usual: https://www.kaggle.com/artgor/pytorch-text-processing-and-other-things\n\n\nIn this kernel I wanted to create a full cycle of processing text and training model in Pytorch.\n\nSome time ago Kaggle launched a new feature: possibility to import scripts into kernels and I heavily use this feature in my kernel.\n\nAll functions for preprocessing and training model are defined in the script: https://www.kaggle.com/artgor/jigsaw-utils\n\nThat script is based on ideas from several kernels with my changes and improvements when possible, I want to acknowledge these great works:\n\nhttps://www.kaggle.com/adityaecdrid/public-version-text-cleaning-vocab-65/\nhttps://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version\nhttps://www.kaggle.com/authman/simple-lstm-pytorch-with-batch-loading\nhttps://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n\nIf I missed someone - write to me, I'll add it.\n\nSo my kernel and script contain the following:\n\n* preprocessing texts. Mostly based on adityaecdrid code with some changes. I have a json file with mappings here: https://www.kaggle.com/artgor/jigsaw-public-files\n* text dataset with collating for dynamic length change\n* neural net with two embeddings. The first embedding is fasttext and glove embeddings multiplied by weights. The second one is a small trainable embedding. The idea is that this embedding could get some important information while training model\n* training model on folds\n* to be done - competition metric calculation\n* to be done - weighting loss\n* to be done - saving model while training on folds"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm_notebook\nimport pickle\nimport gc\nfrom sklearn.model_selection import KFold\n\nfrom jigsaw_utils import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting parameters.\nset_seed(42)\n\ncrawl_embedding_path = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\nglove_embedding_path = '../input/glove840b300dtxt/glove.840B.300d.txt'\nnum_models = 2\nlstm_units = 128\ndense_hidden_units = 4 * lstm_units\nmax_len = 220\nembed_size = 300\nmax_features = 120000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data\nI have saved the processed data to the dataset, so I can load it from there."},{"metadata":{"trusted":true},"cell_type":"code","source":"load = True\nif not load:\n    train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n    test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n    train = df_parallelize_run(train, text_clean_wrapper)\n    test = df_parallelize_run(test, text_clean_wrapper)\n    train.to_csv('processed_train.csv', index=False)\n    test.to_csv('processed_test.csv', index=False)\nelse:\n    train = pd.read_csv('../input/jigsaw-public-files/train.csv')\n    test = pd.read_csv('../input/jigsaw-public-files/test.csv')\n    # after processing some of the texts are emply\n    train['comment_text'] = train['comment_text'].fillna('')\n    test['comment_text'] = test['comment_text'].fillna('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking vocab coverage"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nglove_embed = load_embed(glove_embedding_path)\noovs = vocab_check_coverage(train, glove_embed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(oovs[0]['oov_words'][:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of out of vocab words are names, so I suppose there is nothing to do about them."},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not load:\n    tokenizer = text.Tokenizer(lower=False, num_words=max_features)\n    tokenizer.fit_on_texts(list(train['comment_text']) + list(test['comment_text']))\n    \n    # by default tokenizer keeps all words, I leave only top max_features\n    sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n    tokenizer.word_index = {}\n    i = 0\n    for word,count in sorted_by_word_count:\n        if i == max_features:\n            break\n        tokenizer.word_index[word] = i + 1    # <= because tokenizer is 1 indexed\n        i += 1\n    \n    with open(f'tokenizer_{max_features}.pickle', 'wb') as f:\n        pickle.dump(tokenizer, f)\nelse:\n    with open(f'../input/jigsaw-public-files/tokenizer_{max_features}.pickle', 'rb') as f:\n        tokenizer = pickle.load(f)\n    \nX_train = tokenizer.texts_to_sequences(train['comment_text'])\nX_test = tokenizer.texts_to_sequences(test['comment_text'])\nx_train_lens = [len(i) for i in X_train]\nx_test_lens  = [len(i) for i in X_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nfinal_y_train = np.hstack([y_train[:, np.newaxis], y_aux_train])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, crawl_embedding_path, embed_size)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, glove_embedding_path, embed_size)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nembedding_matrix = crawl_matrix * 0.5 +  glove_matrix * 0.5\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a small embedding, which will be trainable."},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_small = np.zeros((embedding_matrix.shape[0], 30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splits for training\nsplits = list(KFold(n_splits=5, shuffle=True, random_state=42).split(X_train, final_y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training model on folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = train_on_folds(X_train, x_train_lens, final_y_train, X_test, x_test_lens,\n                            splits, embedding_matrix, embedding_matrix_small, n_epochs=2, validate=False, debug=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': test_preds.mean(1)\n})\n\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}