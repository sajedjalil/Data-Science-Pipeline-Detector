{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This kernel is an slightly modified version from [Jigsaw Unintended Bias in Toxicity Classification discussion](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/97471#latest-582610) originally wrote by [sakami](https://www.kaggle.com/sakami) meant for let people who access this kernel can have a easy and quick hands on runable version to exprience how Optuna perform hyperparameters tuning in GPT2 and BERT weight blending.","metadata":{}},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install pytorch torchvision torchaudio cudatoolkit=10.2.89 -c pytorch -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(torch.version.cuda)\nprint(torch.__version__)\ntorch.cuda.is_available()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/sakami0000/kaggle_jigsaw.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/apex-master/apex/\n!pip install --no-cache-dir transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from apex import amp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc -V","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-pretrained-bert","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/pronkinnikita/pytorch-pretrained-BERT","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom typing import List\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, Sampler, DataLoader\n\n\nclass TextDataset(Dataset):\n\n    def __init__(self, token_lists: List[List[int]], targets: np.ndarray = None, identities: np.ndarray = None,\n                 annotator_counts: np.ndarray = None):\n        assert targets is None or type(targets) == np.ndarray\n        assert identities is None or type(identities) == np.ndarray\n        super(TextDataset, self).__init__()\n        self.token_lists = token_lists\n        self.targets = targets\n        self.identities = identities\n        self.annotator_counts = annotator_counts\n\n    def __len__(self) -> int:\n        return len(self.token_lists)\n\n    def __getitem__(self, item):\n        if self.targets is None:\n            return self.token_lists[item], item\n        return self.token_lists[item], item, self.annotator_counts[item], self.targets[item], self.identities[item]\n\n    def collate_fn(self, batch):\n        transposed = list(zip(*batch))\n        max_len = max([len(x) for x in transposed[0]])\n        tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n        for i, row in enumerate(transposed[0]):\n            row = np.array(row[:min(max_len, len(row))])\n            tokens[i, :len(row)] = row\n\n        # token_lists, indices\n        tensors = [\n            torch.from_numpy(tokens),\n            torch.Tensor(transposed[1]).type(torch.IntTensor),\n        ]\n        for i in range(2, len(transposed)):\n            tensors.append(torch.Tensor(transposed[i]).type(torch.FloatTensor))\n        return tensors\n\n\nclass LengthBucketingDataLoader(object):\n\n    def __init__(self, dataset: TextDataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None,\n                 num_workers=0, pin_memory=False, drop_last=False,\n                 timeout=0, worker_init_fn=None):\n        self.large_bucket_loader = DataLoader(dataset=dataset, batch_size=batch_size * 100, shuffle=shuffle,\n                                              sampler=sampler, batch_sampler=batch_sampler, num_workers=num_workers,\n                                              collate_fn=self.nop_collate_fn, pin_memory=pin_memory, drop_last=False,\n                                              timeout=timeout, worker_init_fn=worker_init_fn)\n        self.drop_last = drop_last\n        self.batch_size = batch_size\n        self.collate_fn = dataset.collate_fn\n\n    @staticmethod\n    def nop_collate_fn(batch):\n        return batch\n\n    def __iter__(self):\n        for large_batch in self.large_bucket_loader:\n            assert type(large_batch[0])\n            large_batch = sorted(large_batch, key=lambda example: len(example[0]))\n\n            small_batches = []\n            for start_idx in range(0, len(large_batch), self.batch_size):\n                end_idx = min(len(large_batch), start_idx + self.batch_size)\n                small_batch = large_batch[start_idx:end_idx]\n                if end_idx - start_idx == self.batch_size or not self.drop_last:\n                    small_batches.append(self.collate_fn(small_batch))\n            random.shuffle(small_batches)\n\n            for small_batch in small_batches:\n                yield small_batch\n\n\nclass TokenDataset(Dataset):\n\n    def __init__(self, seqs, targets=None, maxlen=200):\n        if targets is not None:\n            self.targets = targets\n        else:\n            self.targets = np.random.randint(2, size=(len(seqs),))\n        \n        self.seqs = seqs\n        self.maxlen = maxlen\n        \n    def __len__(self):\n        return len(self.seqs)\n        \n    def get_keys(self):\n        lens = np.fromiter(\n            ((min(self.maxlen, len(seq))) for seq in self.seqs),\n            dtype=np.int32)\n        return lens\n        \n    def __getitem__(self, index):\n        return index, self.seqs[index], self.targets[index]\n\n\ndef collate_fn(data):\n\n    def _pad_sequences(seqs):\n        lens = [len(seq) for seq in seqs]\n        max_len = max(lens)\n\n        padded_seqs = torch.zeros(len(seqs), max_len).long()\n        for i, seq in enumerate(seqs):\n            start = max_len - lens[i]\n            padded_seqs[i, start:] = torch.LongTensor(seq)\n        return padded_seqs\n\n    index, seqs, targets = zip(*data)\n    seqs = _pad_sequences(seqs)\n    return index, seqs, torch.FloatTensor(targets)\n\n\nclass BucketSampler(Sampler):\n\n    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1048, shuffle_data=True):\n        super().__init__(data_source)\n        self.shuffle = shuffle_data\n        self.batch_size = batch_size\n        self.sort_keys = sort_keys\n        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n        self.weights = None\n\n        if not shuffle_data:\n            self.index = self.prepare_buckets()\n        else:\n            self.index = None\n\n    def set_weights(self, weights):\n        assert weights >= 0\n        total = np.sum(weights)\n        if total != 1:\n            weights = weights / total\n        self.weights = weights\n\n    def __iter__(self):\n        indices = None\n        if self.weights is not None:\n            total = len(self.sort_keys)\n            indices = np.random.choice(total, (total,), p=self.weights)\n        if self.shuffle:\n            self.index = self.prepare_buckets(indices)\n        return iter(self.index)\n\n    def get_reverse_indexes(self):\n        indexes = np.zeros((len(self.index),), dtype=np.int32)\n        for i, j in enumerate(self.index):\n            indexes[j] = i\n        return indexes\n\n    def __len__(self):\n        return len(self.sort_keys)\n        \n    def prepare_buckets(self, indices=None):\n        lens = - self.sort_keys\n        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lens)\n\n        if indices is None:\n            if self.shuffle:\n                indices = shuffle(np.arange(len(lens), dtype=np.int32))\n                lens = lens[indices]\n            else:\n                indices = np.arange(len(lens), dtype=np.int32)\n\n        #  bucket iterator\n        def divide_chunks(l, n):\n            if n == len(l):\n                yield np.arange(len(l), dtype=np.int32), l\n            else:\n                # looping till length l\n                for i in range(0, len(l), n):\n                    data = l[i:i + n]\n                    yield np.arange(i, i + len(data), dtype=np.int32), data\n\n        new_indices = []\n        extra_batch = None\n        for chunk_index, chunk in divide_chunks(lens, self.bucket_size):\n            # sort indices in bucket by descending order of length\n            indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n            batches = []\n            for _, batch in divide_chunks(indices_sorted, self.batch_size):\n                if len(batch) == self.batch_size:\n                    batches.append(batch.tolist())\n                else:\n                    assert extra_batch is None\n                    assert batch is not None\n                    extra_batch = batch\n    \n            # shuffling batches within buckets\n            if self.shuffle:\n                batches = shuffle(batches)\n            for batch in batches:\n                new_indices.extend(batch)\n    \n        if extra_batch is not None:\n            new_indices.extend(extra_batch)\n        return indices[new_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing.pool import Pool\nfrom typing import List, TypeVar\n\nfrom transformers import PreTrainedTokenizer\nfrom tqdm import tqdm\n\nTokenizer = TypeVar('Tokenizer', bound=PreTrainedTokenizer)\n\n\nclass MyTokenizer:\n\n    def __init__(self, tokenizer: Tokenizer, max_len=220, max_head_len=128, mode='bert'):\n        assert max_len >= max_head_len\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.max_head_len = max_head_len\n        self.mode = mode\n        if self.mode == 'bert':\n            self.max_len -= 2\n\n    def _tokenize_one(self, text: str) -> List[int]:\n        \"\"\"\n        when the sentence is longer then `max_len`,\n        the first `max_head_len` and the last `max_len` - `max_head_len` words will \n        be used to train or inference\n        \"\"\"\n        tokens = self.tokenizer.tokenize(text)\n        if len(tokens) > self.max_len:\n            tokens = tokens[:self.max_head_len] + tokens[self.max_head_len - self.max_len:]\n        if self.mode == 'bert':\n            tokens = ['[CLS]'] + tokens + ['[SEP]']\n        return self.tokenizer.convert_tokens_to_ids(tokens)\n\n    def tokenize(self, examples: List[str], num_threads=1, chunksize=1000):\n        if num_threads < 1:\n            raise ValueError('num_threads must be positive integer.')\n        all_tokens = []\n        total = len(examples)\n        if num_threads == 1:\n            for _, text in tqdm(enumerate(examples), total=total):\n                all_tokens.append(self._tokenize_one(text))\n        else:\n            with Pool(num_threads) as pool:\n                for _, tokens in tqdm(enumerate(pool.imap(self._tokenize_one, examples, chunksize=chunksize)),\n                                      total=total):\n                    all_tokens.append(tokens)\n        return all_tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nfrom kaggle_jigsaw.config.base import IDENTITY_COLUMNS\n\n\nclass JigsawEvaluator:\n\n    def __init__(self, y_target: np.ndarray, y_identity: np.ndarray, power=-5, overall_model_weight=0.25):\n        self.y = (y_target >= 0.5).astype(int)\n        self.y_i = (y_identity >= 0.5).astype(int)\n        self.n_subgroups = self.y_i.shape[1]\n        self.power = power\n        self.overall_model_weight = overall_model_weight\n\n    @staticmethod\n    def _compute_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            # return np.nan\n            return 1e-15\n\n    def _compute_subgroup_auc(self, i, y_pred):\n        mask = self.y_i[:, i] == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bpsn_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y == 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bnsp_auc(self, i, y_pred):\n        mask = self.y_i[:, i] + self.y != 1\n        return self._compute_auc(self.y[mask], y_pred[mask])\n\n    def _compute_bias_metrics_for_model(self, y_pred):\n        metrics = np.zeros((3, self.n_subgroups))\n        record = {\n            'subgroup_auc': {},\n            'bpsn_auc': {},\n            'bnsp_auc': {}\n        }\n        for i in range(self.n_subgroups):\n            metrics[0, i] = self._compute_subgroup_auc(i, y_pred)\n            metrics[1, i] = self._compute_bpsn_auc(i, y_pred)\n            metrics[2, i] = self._compute_bnsp_auc(i, y_pred)\n            subgroup_name = IDENTITY_COLUMNS[i]\n            record['subgroup_auc'][subgroup_name] = metrics[0, i]\n            record['bpsn_auc'][subgroup_name] = metrics[1, i]\n            record['bnsp_auc'][subgroup_name] = metrics[2, i]\n        return metrics, record\n\n    def _calculate_overall_auc(self, y_pred):\n        return roc_auc_score(self.y, y_pred)\n\n    def _power_mean(self, array):\n        total = sum(np.power(array, self.power))\n        return np.power(total / len(array), 1 / self.power)\n\n    def get_final_metric(self, y_pred: np.ndarray):\n        bias_metrics, bias_record = self._compute_bias_metrics_for_model(y_pred)\n        bias_score = np.average([\n            self._power_mean(bias_metrics[0]),\n            self._power_mean(bias_metrics[1]),\n            self._power_mean(bias_metrics[2])\n        ])\n        overall_auc = self._calculate_overall_auc(y_pred)\n        overall_score = self.overall_model_weight * overall_auc\n        bias_score = (1 - self.overall_model_weight) * bias_score\n        final_score = overall_score + bias_score\n\n        bias_record['overall_auc'] = overall_auc\n        bias_record['final_score'] = final_score\n        bias_record['mean_subgroup_auc'] = self._power_mean(bias_metrics[0])\n        bias_record['mean_bpsn_auc'] = self._power_mean(bias_metrics[1])\n        bias_record['mean_bnsp_auc'] = self._power_mean(bias_metrics[2])\n        return final_score, bias_record\n\n\ndef accuracy(ys, ps):\n    return torch.mean(((ps >= 0.5) == (ys >= 0.5)).type(torch.FloatTensor)).item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\n\nclass CustomLoss(nn.Module):\n\n    def __init__(self, loss_weight=None, alpha=1, beta=1, use_annotator_counts=False,\n                 weight_from_annotator_counts=None):\n        super(CustomLoss, self).__init__()\n        self.loss_weight = loss_weight\n        self.alpha = alpha\n        self.beta = beta\n        self.use_annotator_counts = use_annotator_counts\n        self.weight_from_annotator_counts = weight_from_annotator_counts\n\n    def forward(self, logits, targets, annotator_counts=None):\n        \"\"\"\n        preds[:, 0] = \"prediction for target labels\"\n        preds[:, 1:] = \"prediction for auxiliary target labels\"\n        targets[:, 0] = \"target labels\"\n        targets[:, 1] = \"instance weight\"\n        targets[:, 2:] = \"auxiliary target labels\"\n        \"\"\"\n        if self.loss_weight is None:\n            weight = None\n            loss_weight = 1\n        else:\n            weight = targets[:, 1:2]\n            loss_weight = self.loss_weight\n    \n        if annotator_counts is None or not self.use_annotator_counts:\n            bce_loss_1 = nn.BCEWithLogitsLoss(weight=weight)(logits[:, :1], targets[:, :1])\n            bce_loss_2 = nn.BCEWithLogitsLoss()(logits[:, 1:], targets[:, 2:])\n            return (bce_loss_1 * loss_weight) + bce_loss_2\n        else:\n            annotator_counts = annotator_counts.view(-1, 1)\n            new_targets = targets.clone()\n            new_targets[:, :1] = (targets[:, :1] * annotator_counts + self.alpha) / (\n                annotator_counts + self.alpha + self.beta)\n\n            num_aux_targets = targets.size()[1] - 1\n            aux_annotator_counts = annotator_counts.view(-1, 1).repeat(1, num_aux_targets)\n            new_targets[:, 1:] = (targets[:, 1:] * aux_annotator_counts + self.alpha) / (\n                aux_annotator_counts + self.alpha + self.beta)\n\n            bce_loss_1 = nn.BCEWithLogitsLoss(weight=weight, reduction='none')(\n                logits[:, :1], new_targets[:, :1])\n            bce_loss_2 = torch.mean(nn.BCEWithLogitsLoss(reduction='none')(\n                logits[:, 1:], new_targets[:, 2:]), 1).view(-1, 1)\n            if self.weight_from_annotator_counts is None:\n                return ((bce_loss_1 * loss_weight) + bce_loss_2).mean()\n            return (((bce_loss_1 * loss_weight) + bce_loss_2) * self.weight_from_annotator_counts(\n                annotator_counts + self.alpha + self.beta)).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install attrdict==2.0.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from contextlib import contextmanager\nimport json\nimport os\nimport random\nimport time\n\nfrom attrdict import AttrDict\nimport numpy as np\n\n@contextmanager\ndef timer(msg):\n    t0 = time.time()\n    print(f'[{msg}] start.')\n    yield\n    elapsed_time = time.time() - t0\n    print(f'[{msg}] done in {elapsed_time / 60:.2f} min.')\n\n\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef load_config(config_path: str) -> AttrDict:\n    with open(config_path) as f:\n        config = json.load(f, object_hook=AttrDict)\n    return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef training_weights(df_train, toxicity_column, identity_columns):\n    subgroup_positive = (df_train[identity_columns].fillna(0).values >= 0.5).sum(axis=1).astype(bool).astype(np.int)\n    subgroup_negative = (df_train[identity_columns].fillna(0).values < 0.5).sum(axis=1).astype(bool).astype(np.int)\n\n    background_positive = (df_train[toxicity_column].values >= 0.5).astype(bool).astype(np.int)\n    background_negative = (df_train[toxicity_column].values < 0.5).astype(bool).astype(np.int)\n\n    weights = np.ones((len(df_train),)) / 4\n    weights += (df_train[identity_columns].fillna(0).values >= 0.5).mean(axis=1) / 4\n    weights += ((background_positive + subgroup_negative) > 1).astype(bool).astype(np.int) / 4\n    weights += ((background_negative + subgroup_positive) > 1).astype(bool).astype(np.int) / 4\n    return weights\n\n\ndef training_weights_s(df_train, toxicity_column, identity_columns):\n    weights = np.ones((len(df_train),))\n    weights += df_train[identity_columns].fillna(0).values.sum(axis=1) * 3\n    weights += df_train[toxicity_column].values * 8\n    weights /= weights.max()\n    return weights","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\ndef get_optimizer_params(model, lr, lr_weight_decay_coef, num_layers):\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    if lr_weight_decay_coef < 1.0:\n        optimizer_grouped_parameters = [\n            {'params': [\n                p for n, p in param_optimizer\n                if 'bert.embeddings' not in n\n                and 'bert.encoder' not in n\n                and not any(nd in n for nd in no_decay)],\n             'weight_decay': 0.01},\n            {'params': [\n                p for n, p in param_optimizer\n                if 'bert.embeddings' not in n\n                and 'bert.encoder' not in n\n                and any(nd in n for nd in no_decay)],\n             'weight_decay': 0.0},\n            {'params': [\n                p for n, p in param_optimizer\n                if 'bert.embeddings' in n\n                and not any(nd in n for nd in no_decay)],\n             'lr': lr * lr_weight_decay_coef ** (num_layers + 1), 'weight_decay': 0.01},\n            {'params': [\n                p for n, p in param_optimizer\n                if 'bert.embeddings' in n\n                and any(nd in n for nd in no_decay)],\n             'lr': lr * lr_weight_decay_coef ** (num_layers + 1), 'weight_decay': 0.0}\n        ]\n        for i in range(num_layers):\n            optimizer_grouped_parameters.append(\n                {'params': [\n                    p for n, p in param_optimizer\n                    if 'bert.encoder.layer.{}.'.format(i) in n\n                    and any(nd in n for nd in no_decay)],\n                 'lr': lr * lr_weight_decay_coef ** (num_layers - i), 'weight_decay': 0.0})\n            optimizer_grouped_parameters.append(\n                {'params': [\n                    p for n, p in param_optimizer\n                    if 'bert.encoder.layer.{}.'.format(i) in n\n                    and any(nd in n for nd in no_decay)],\n                 'lr': lr * lr_weight_decay_coef ** (num_layers - i), 'weight_decay': 0.0})\n    else:\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             'weight_decay': 0.01},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n             'weight_decay': 0.0}\n        ]\n    return optimizer_grouped_parameters\n\n\ndef predict(model: nn.Module, dataset: TextDataset, device, batch_size=32) -> np.ndarray:\n    model.eval()\n    test_ps = []\n    test_is = []\n    with torch.no_grad():\n        for batch in tqdm(LengthBucketingDataLoader(dataset=dataset, batch_size=batch_size,\n                                                    shuffle=False, drop_last=False),\n                          total=len(dataset) // batch_size):\n            x_batch = batch[0]\n            i_batch = batch[1]\n            p_batch = model(x_batch.type(torch.LongTensor).to(device))\n            test_ps.append(p_batch.detach().cpu())\n            test_is.append(i_batch.detach().cpu())\n    test_ps = torch.sigmoid(torch.cat(test_ps, 0)[:, 0]).numpy().ravel()\n    test_is = torch.cat(test_is, 0).numpy().ravel()\n    return np.array(list(map(lambda pi: pi[0], sorted(list(zip(test_ps, test_is)), key=lambda pi: pi[1]))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_pretrained_bert import GPT2Model\n\nclass GPT2ClassificationHeadModel(GPT2Model):\n\n    def __init__(self, config, clf_dropout=0.4, n_class=8):\n        super(GPT2ClassificationHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.dropout = nn.Dropout(clf_dropout)\n        self.linear = nn.Linear(config.n_embd * 3, n_class)\n\n        nn.init.normal_(self.linear.weight, std=0.02)\n        nn.init.normal_(self.linear.bias, 0)\n        \n        self.apply(self.init_weights)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        avg_pool = torch.mean(hidden_states, 1)\n        max_pool, _ = torch.max(hidden_states, 1)\n        h_conc = torch.cat((avg_pool, max_pool, hidden_states[:, -1, :]), 1)\n        logits = self.linear(self.dropout(h_conc))\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nimport warnings\n\nfrom apex import amp\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom kaggle_jigsaw.config.base import (\n    TOXICITY_COLUMN, IDENTITY_COLUMNS, AUX_TOXICITY_COLUMNS,\n    OLD_TOXICITY_COLUMN, OLD_IDENTITY_COLUMNS, OLD_AUX_TOXICITY_COLUMNS,\n    TRAIN_DATA, TEST_DATA, SAMPLE_SUBMISSION,\n    TRAIN_OLD, TEST_OLD, SAMPLE_OLD)\n\nconfig_file = Path('./kaggle_jigsaw/config/bert_large_cased.json')\nconfig = load_config(config_file)\n\nconfig.setdefault('max_len', 220)\nconfig.setdefault('max_head_len', 128)\nconfig.setdefault('epochs', 2)\nconfig.setdefault('down_sample_frac', 0.5)\nconfig.setdefault('lr', 1.5e-5)\nconfig.setdefault('batch_size', 16)\nconfig.setdefault('accumulation_steps', 4)\nconfig.setdefault('lr_weight_decay_coef', 1.0)\nconfig.setdefault('warmup', 0.05)\nconfig.setdefault('old_data', False)\nconfig.setdefault('old_fine_tuned', False)\nconfig.setdefault('device', 'cuda')\nconfig.setdefault('seed', 1234)\n\nassert 'lm_model_name' in config\nassert not (config.old_fine_tuned and config.old_data)\nassert config.max_len >= config.max_head_len\nassert config.epochs <= 2\nvalid = False\nold = False\n\nlm_model_name = config_file.stem\nif config.old_fine_tuned:\n    PRETRAINED_PATH = Path(f'../output/{lm_model_name}_old_fine_tune/')\n    assert PRETRAINED_PATH.exists()\nelse:\n    PRETRAINED_PATH = lm_model_name\nMODE = lm_model_name[:4]\nLOWER_CASE = 'uncased' in lm_model_name\nLARGE_MODEL = 'large' in lm_model_name\nDEVICE = torch.device(config.device)\n\nif config.old_data:\n    lm_model_name += '_old_fine_tune'\n\nif valid:\n    valid_size = 200000\n    shuffle_seed = 1029\n    lm_model_name += '_valid'\nelse:\n    valid_size = 0\n    shuffle_seed = config.seed\n\nOUT_DIR = Path(f'../output/{lm_model_name}/')\nTEST_SUBMISSION = OUT_DIR / 'submission.csv'\nVALID_SUBMISSION = OUT_DIR / 'valid_submission.csv'\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nwarnings.filterwarnings('ignore')\nseed_torch(config.seed)\n\nif not old:\n    train_data = TRAIN_DATA\n    test_data = TEST_DATA\n    sample_submission = SAMPLE_SUBMISSION\n    train_size = 1804874 - valid_size\nelse:\n    train_data = TRAIN_OLD\n    test_data = TEST_OLD\n    sample_submission = SAMPLE_OLD\n    train_size = 159571 - valid_size\n\n    TOXICITY_COLUMN = OLD_TOXICITY_COLUMN\n    IDENTITY_COLUMNS = OLD_IDENTITY_COLUMNS\n    AUX_TOXICITY_COLUMNS = OLD_AUX_TOXICITY_COLUMNS\n\nif MODE == 'bert':\n    from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\n\n    lm_tokenizer = BertTokenizer.from_pretrained('bert-large-cased', cache_dir=None, do_lower_case=LOWER_CASE)\n    model = BertForSequenceClassification.from_pretrained(\"bert-large-cased\", cache_dir=None, num_labels=1 + len(AUX_TOXICITY_COLUMNS))\n    optimizer_class = BertAdam\nelse:\n    from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model\n\n    lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', cache_dir=None)\n    model = GPT2ClassificationHeadModel.from_pretrained(\"gpt2\", clf_dropout=config.get('dropout_rate', 0.1),n_class=1 + len(AUX_TOXICITY_COLUMNS))\n    optimizer_class = OpenAIAdam\n    assert config.lr_weight_decay_coef == 1.0  \n    \nwith timer('preprocess'):\n    tokenizer = MyTokenizer(lm_tokenizer, config.max_len, config.max_head_len, MODE)\n    df_train = pd.read_csv(TRAIN_DATA).sample(frac=1, random_state=shuffle_seed).reset_index(drop=True)\n    df_train['comment_text'] = df_train['comment_text'].astype(str)\n    df_train = df_train.fillna(0)\n    X_train = tokenizer.tokenize(df_train['comment_text'].fillna('DUMMY_VALUE'), num_threads=16, chunksize=5000)\n\n    df_test = pd.read_csv(TEST_DATA)\n    df_test['comment_text'] = df_test['comment_text'].astype(str)\n    df_test = df_test.fillna(0)\n    X_test = tokenizer.tokenize(df_test['comment_text'].fillna('DUMMY_VALUE'), num_threads=16, chunksize=5000)\n\n    df_train.drop(['comment_text'], axis=1, inplace=True)\n    df_test.drop(['comment_text'], axis=1, inplace=True)\n\n    X_valid = X_train[train_size:]\n    X_train = X_train[:train_size]\n\n    y_identity_train = df_train[IDENTITY_COLUMNS].values\n    y_annotator_counts_train = df_train['toxicity_annotator_count'].values\n\n    weights = training_weights(df_train, TOXICITY_COLUMN, IDENTITY_COLUMNS)\n    y_train = np.hstack((df_train[TOXICITY_COLUMN].values.reshape(-1, 1), weights.reshape(-1, 1),df_train[AUX_TOXICITY_COLUMNS].values))\n\n    y_valid = y_train[train_size:]\n    y_train = y_train[:train_size]\n    y_identity_valid = y_identity_train[train_size:]\n    y_identity_train = y_identity_train[:train_size]\n    y_annotator_counts_valid = y_annotator_counts_train[train_size:]\n    y_annotator_counts_train = y_annotator_counts_train[:train_size]\n    loss_weight = 1.0 / weights.mean() if not old else None\n\n        # drop negative samples here\n    frac = config.down_sample_frac\n    target_negative = (y_train > 0.0).sum(axis=1) == 1\n    identity_negative = (y_identity_train > 0.0).sum(axis=1) == 0\n    negative_mask = identity_negative & target_negative\n    negative_indices = np.arange(len(y_train))[negative_mask]\n    drop_indices_0 = set(negative_indices[:int(len(negative_indices) * frac)])\n    drop_indices_1 = set(negative_indices[int(len(negative_indices) * (1 - frac)):])\n    drop_indices_list = [drop_indices_0, drop_indices_1]\n    len_train = len(y_train) - len(drop_indices_0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('train'):\n    model.zero_grad()\n    model = model.to(DEVICE)\n    num_layers = 24 if LARGE_MODEL else 12\n    optimizer_grouped_parameters = get_optimizer_params(model, config.lr, config.lr_weight_decay_coef, num_layers)\n    num_train_optimization_steps = int(config.epochs * len_train / config.batch_size / config.accumulation_steps)\n\n    optimizer = optimizer_class(optimizer_grouped_parameters,\n                                    lr=config.lr,\n                                    warmup=config.warmup,\n                                    t_total=num_train_optimization_steps)\n\n    model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n    model = model.train()\n\n    batch_count = len_train // config.batch_size\n    loss_fn = CustomLoss(loss_weight)\n    for epoch, drop_indices in zip(range(config.epochs), drop_indices_list):\n        sample_indices = np.array([i for i in range(len(y_train)) if i not in drop_indices])\n        X_sampled_train = [X_train[i] for i in sample_indices]\n        y_sampled_train = y_train[sample_indices]\n        y_sampled_identity_train = y_identity_train[sample_indices]\n        y_sampled_annotator_counts_train = y_annotator_counts_train[sample_indices]\n        train_dataset = TextDataset(X_sampled_train, y_sampled_train,\n                                        y_sampled_identity_train, y_sampled_annotator_counts_train)\n        train_loader = LengthBucketingDataLoader(\n                train_dataset, shuffle=True, drop_last=True, batch_size=config.batch_size)\n        tk0 = tqdm(enumerate(train_loader), total=batch_count)\n        optimizer.zero_grad()\n        for i, (x_batch, _, a_batch, y_batch, y_identity_batch) in tk0:\n            y_pred = model(x_batch.to(DEVICE), attention_mask=(x_batch > 0).to(DEVICE), labels=None)\n            loss = loss_fn(y_pred, y_batch.to(DEVICE))\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n            if (i + 1) % config.accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n    model.save_pretrained(OUT_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with timer('evaluate'):\n    if valid:\n        valid_dataset = TextDataset(X_valid, y_valid, y_identity_valid, y_annotator_counts_valid)\n        valid_preds = predict(model, valid_dataset, device=DEVICE)\n\n        df_valid = df_train.tail(valid_size)\n        df_valid['model1'] = valid_preds\n        evaluator = JigsawEvaluator(df_valid[TOXICITY_COLUMN].values, df_valid[IDENTITY_COLUMNS].values)\n        final_score, _ = evaluator.get_final_metric(df_valid['model1'].values)\n\n        valid_prediction = predict(model, TextDataset(X_valid), device=DEVICE)\n        valid_submission = pd.DataFrame({\n                'id': df_valid['id'],\n                'prediction': valid_prediction \n            })\n        valid_submission.to_csv(VALID_SUBMISSION, index=False)\n        print(f'validation score: {final_score:.5f}')\n\n    test_prediction = predict(model, TextDataset(X_test), device=DEVICE)\n    submission = pd.DataFrame({\n            'id': df_test['id'],\n            'prediction': test_prediction \n    })\n    submission.to_csv(TEST_SUBMISSION, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\nimport optuna\nimport pandas as pd\n\nfrom kaggle_jigsaw.config.base import TOXICITY_COLUMN, IDENTITY_COLUMNS\n\nOUT_DIR = Path('../output/')\nVALID_DIR = OUT_DIR.glob('*_valid/')\nmodels = [path.stem[:-6] for path in VALID_DIR]\n\n\ndef objective(trial, train_fold, evaluator):\n    params = {\n        model: trial.suggest_uniform(model, 0.0, 1.0)\n        for model in models\n    }\n    train_fold = np.array(train_fold)\n    train_preds = np.average(train_fold, weights=params.values(), axis=1)\n    score, _ = evaluator.get_final_metric(train_preds)\n    return 1 - score\n\n\n\nconfig = load_config('./kaggle_jigsaw/config/blend.json')\nconfig.setdefault('n_folds', 10)\nconfig.setdefault('n_trials', 300)\nconfig.setdefault('threshold', 0.03)\n\ndf_valid = pd.concat([pd.read_csv(path / 'valid_submission.csv', index_col='id') for path in VALID_DIR], axis=0)\ntrain_scores = []\nvalid_scores = []\nparams = {model: [] for model in models}\n\nfor i in range(config.n_folds):\n    df_valid = df_valid.sample(frac=1, random_state=i).reset_index(drop=True)\n    train_fold = df_valid[:len(df_valid) // 2]\n    valid_fold = df_valid[len(df_valid) // 2:]\n    train_evaluator = JigsawEvaluator(\n        train_fold[TOXICITY_COLUMN].values, train_fold[IDENTITY_COLUMNS].values)\n    valid_evaluator = JigsawEvaluator(\n        valid_fold[TOXICITY_COLUMN].values, valid_fold[IDENTITY_COLUMNS].values)\n        \n    study = optuna.create_study()\n    study.optimize(lambda trial: objective(trial, train_fold.values, train_evaluator),\n                       n_trials=config.n_trials)\n    trial = study.best_trial\n    train_scores.append(1 - trial.value)\n    values = np.array(list(trial.params.values()))\n    values /= values.sum()\n    for key, value in zip(trial.params.keys(), values):\n        params[key].append(value)\n        \n    valid_preds = np.zeros((len(valid_fold)))\n    for key, value in trial.params.items():\n        valid_preds += valid_fold[key].values * value\n    score, _ = valid_evaluator.get_final_metric(valid_preds)\n    valid_scores.append(score)\n\nfor i, (train_score, valid_score) in enumerate(zip(train_scores, valid_scores)):\n    print(f'fold {str(i + 1):2s} - train: {train_score:.5f}, valid: {valid_score:.5f}')\n\nprint('-' * 20)\nprint(f'train mean: {np.mean(train_scores):.5f}, var: {np.var(train_scores):.7f}')\nprint(f'valid mean: {np.mean(valid_scores):.5f}, var: {np.var(valid_scores):.7f}')\n\nprint('-' * 20)\nfor key, values in params.items():\n    print(f'{key:25s} {np.mean(values):.6f} {np.var(values):.6f}')\n\nprint('-' * 20)\nprint(f'robust folds: threshold {config.threshold}')\nrobust_folds = []\nrobust_train_scores = []\nrobust_valid_scores = []\nfor i, (train_score, valid_score) in enumerate(zip(train_scores, valid_scores)):\n    if np.abs(train_score - valid_score) < config.threshold:\n        robust_folds.append(i)\n        robust_train_scores.append(train_score)\n        robust_valid_scores.append(valid_score)\n    print(' '.join(map(str, robust_folds)))\n\nprint('-' * 20)\nprint(f'train mean: {np.mean(robust_train_scores):.5f}, var: {np.var(robust_train_scores):.7f}')\nprint(f'valid mean: {np.mean(robust_valid_scores):.5f}, var: {np.var(robust_valid_scores):.7f}')\n\nprint('-' * 20)\nfor key, values in params.items():\n    robust_values = np.array(values)[robust_folds]\n    print(f'{key:25s} {np.mean(robust_values):.6f} {np.var(robust_values):.6f}')    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:  \nhttps://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/97471#latest-582610","metadata":{}}]}