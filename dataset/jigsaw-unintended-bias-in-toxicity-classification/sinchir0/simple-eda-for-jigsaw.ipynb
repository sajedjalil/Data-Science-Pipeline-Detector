{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center><font size=\"6\">Jigsaw Unintended Bias in Toxicity Classification</font></center></h1>\n<h1><center><font size=\"6\">Simple EDA</font></center></h1>\n\n<center><img src=\"https://d1f5hsy4d47upe.cloudfront.net/36/36f0885cfc92b2d8abf1073f229c0047_w.jpg\" width=\"300\"></img></center>\n\n<br>\n\n# <a id='0'>Content</a>\n\n- <a href='#1'>Introduction</a>  \n- <a href='#2'>Data exploration</a>  \n    - <a href='#20'>Data Overview</a>  \n    - <a href='#21'>ID</a>  \n    - <a href='#22'>target</a>  \n    - <a href='#23'>comment_text</a>  \n    - <a href='#24'>comment_text : value_count</a>  \n    - <a href='#25'>comment_text : comment_length</a>  \n    - <a href='#26'>comment_text : Number of words</a>  \n    - <a href='#27'>comment_text : Number of Capitals</a>\n    - <a href='#28'>comment_text : Number of exclamation marks</a>\n    - <a href='#29'>comment_text : Wordcloud</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='#1'>Introduction</a>   \n\n*Work still progress.*\n\nFirstly,I used  below kernel as a reference. Thanks!\n\n・https://www.kaggle.com/gpreda/jigsaw-eda\n\n・https://www.kaggle.com/nz0722/simple-eda-text-preprocessing-jigsaw\n\n## Backgorund\n\nIn the past competition, we made the model for perdicting \"target\" from Test data, and minimize this type of unintended bias.<br>\n\ne.g.  \"This is so cool. It's like, 'would you want your mother to read this??' Really great idea, well done!\"<br>\n →terget is 0.000000<br>\n \ne.g.  \"haha you guys are a bunch of losers.\"<br>\n →terget is 0.89617, because \"loser\" may be toxicity comment.  <br>\n \n&emsp;\n\nBut,that'model has unintende bias, and we have to remove this.  <br>\n\ne.g.  \"I'm a gay woman.\"<br>\n \n \nThis sentence is not toxic, but old model may be deciding toxicity comment.<br>\n\n&emsp;\n\nSo, let' make better model for removing unintend bias!<br>\n \n## Feature of my kernel\n\nIn this kernel, I conclude whether all variables are useful for making new variable or not as far as possible. \n\n\n\n## Conclusion\n\nIn this paragraph, I summarizes what I found in this kernel.\nIt will be updated as appropriate.\n\n\n・ \"test.csv\" has only comment text. So I think we can make new variables only about comment_text.(I'm not confident・・・)\n\n・New variable　\"Number of words\"　\"Number of Capitals\"　\"Number of exclamation marks\" may be useful.\n\n・In the comment data,there is something frong.For example \"Trump\" at highly insult, \"Damn\" at highly obscene.\n\n・\n"},{"metadata":{"_kg_hide-input":false},"cell_type":"markdown","source":"Import\n------"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\n#from nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)\npd.set_option('max_columns', 50)\nimport json\nimport altair as alt\nfrom  altair.vega import v3\nfrom IPython.display import HTML\nimport gc\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n\nvega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\nvega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\nvega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\nvega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\nnoext = \"?noext\"\n\npaths = {\n    'vega': vega_url + noext,\n    'vega-lib': vega_lib_url + noext,\n    'vega-lite': vega_lite_url + noext,\n    'vega-embed': vega_embed_url + noext\n}\n\nworkaround = \"\"\"\nrequirejs.config({{\n    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n    paths: {}\n}});\n\"\"\"\n\n#------------------------------------------------ Defs for future rendering\ndef add_autoincrement(render_func):\n    # Keep track of unique <div/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n            \n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    chart_str = \"\"\"\n    <div id=\"{id}\"></div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    </script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )\n\nHTML(\"\".join((\n    \"<script>\",\n    workaround.format(json.dumps(paths)),\n    \"</script>\",\n)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>Data exploration</a>  "},{"metadata":{},"cell_type":"markdown","source":"# <a id='20'>Data Overview</a>  "},{"metadata":{},"cell_type":"markdown","source":"Let's have a quick look at the data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Firstly, train_data has **id** and **target**. \n* **id**: unique number for comment.\n* **target**: express toxicity determined by rater\n\nSecondly, the **comment_text** are stored in `train` and `test`.  \n* **comment_text**\n\nIn `train`, following feature expresses aggression of comment.\n* **severe_toxicity**\n* **obsence**\n* **identity_attack**\n* **insult**\n\nThe topic is also related to five categories: race or ethnicity, gender, sexual orientation, religion, disability, as following:\n* **race or ethnicity**: asian, black, jewish, latino, other_race_or_ethnicity, white  \n* **gender**: female, male, transgender, other_gender  \n* **sexual orientation**: bisexual, heterosexual, homosexual_gay_or_lesbian, other_sexual_orientation  \n* **religion**: atheist,buddhist,  christian, hindu, muslim, other_religion  \n* **disability**: intellectual_or_learning_disability, other_disability, physical_disability, psychiatric_or_mental_illness  \n\nWe also have few article/comment identification information:\n* **created_date**  \n* **publication_id**   \n* **parent_id**  \n* **article_id** \n\nSeveral user feedback information associated with the comments are provided:\n* **rating**  \n* **funny**  \n* **wow**  \n* **sad**  \n* **likes**  \n* **disagree**  \n* **sexual_explicit**  \n\nIn the datasets are also 2 fields relative to annotations:\n* **identity_annotator_count**  \n* **toxicity_annotator_count**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We may need Time series analisys because train-data has created_date.\n#What is \"intellectual_or_learning_disability\",\"rating\",\"sexual_explicit\",\"identity_annotator_count\",\"toxicity_annotator_count\",\"publication_id\"? I have to research them.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='21'>ID</a>  "},{"metadata":{},"cell_type":"markdown","source":"## Data exploration\nLet's have a look at the data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"id\"].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"・　ID has unique number."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = train[\"id\"].astype(int)\nplt.subplots(figsize=(10, 7)) \nsns.set_context(\"poster\")\nsns.distplot(train_id,kde =False)\nplt.title('Train ID Histogram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"id\"].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"id\"].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"・　Train ID has nearly 500000 and 5500000"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id = test[\"id\"].astype(int)\nplt.subplots(figsize=(10, 7)) \nsns.set_context(\"poster\")\nsns.distplot(test_id,kde =False)\nplt.title('Test ID Histogram');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context(\"poster\")\ntrain['created_date'] = pd.to_datetime(train['created_date']).values.astype('datetime64[M]')\nplt.subplots(figsize=(15, 6))\nax1 = sns.lineplot(x=train['created_date'], y=train['id'],label='Train ID',color = \"red\");\nax2 = ax1.twinx()\nsns.lineplot(x=train['created_date'], y=train['target'], label='target', ax=ax2);\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nax1.legend(h1+h2, l1+l2, loc='lower right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\nID increase with time, and target also increase, but I think they has no corelation between ID and target."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nI think we can't make new valuable from ID."},{"metadata":{},"cell_type":"markdown","source":"# <a id='22'>target</a>  "},{"metadata":{},"cell_type":"markdown","source":"## Data exploration\nLet's have a look at the data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"hist_df = pd.cut(train['target'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\nhist_df['bins'] = hist_df['bins'].astype(str)\nrender(alt.Chart(hist_df).mark_bar().encode(\n    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target')),\n    y=alt.Y('target:Q', axis=alt.Axis(title='Count')),\n    tooltip=['target', 'bins']\n).properties(title=\"Counts of target bins\", width=1000, height=500).interactive())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"(target=0)/all_target = \",(train['target'] >= 0).sum() / train.shape[0])\nprint(\"(target>0)/all_target = \",(train['target'] > 0).sum() / train.shape[0])\nprint(\"(target>0.25)/all_target = \",(train['target'] >= 0.25).sum() / train.shape[0])\nprint(\"(target>=0.5)/all_target \",(train['target'] >= 0.5).sum() / train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\nWe understood \"target\" distribution.\n\n・There are many 0 target.\n\n・Why are there so many 0.15-0.20 target? We must research it later."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nI think we can't make new valuable from target, because target is objective variable."},{"metadata":{},"cell_type":"markdown","source":"## <a id='23'>comment_text</a>  "},{"metadata":{},"cell_type":"markdown","source":"**Let's check the 'comment_text' about each new variable!**"},{"metadata":{},"cell_type":"markdown","source":"## <a id='24'>comment_text : value_count</a>  "},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'].value_counts().head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'].value_counts().head(50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\nHum・・・.Why so many same comment　\"START WORKING ・・・\" have train and test data? May this be commercial comment？ \n\nWe have to research these later."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nlater."},{"metadata":{},"cell_type":"markdown","source":"## <a id='25'>comment_text : comment_length</a>  "},{"metadata":{},"cell_type":"markdown","source":"## Data exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plus = train\ntrain_plus['train_comment_length'] = train.comment_text.apply(len)\ntest_plus = test\ntest_plus['test_comment_length'] = test.comment_text.apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context(\"poster\");\nplt.figure(figsize=(15, 8));\nsns.distplot(train_plus['train_comment_length'] ,kde=False, rug=False,bins=100,norm_hist=True,label = \"train\");\nsns.distplot(test_plus['test_comment_length'] ,kde=False, rug=False,bins=100,norm_hist=True,label = \"test\",axlabel = \"comment_length\");\nplt.legend();\nplt.title(\"Histgram of train adn test comment_length with normalize\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is histogram of train_comment length.\n\nWe can see rise around 1000."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['train_comment_length'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Train_comment_length');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['train_comment_length'],train_plus['target'])[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next , we check outlier using boxplot."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['train_comment_length'] )\nplt.title(\"Boxplot of train_comment_length\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the Number of 75% point\nq25,q50,q75= np.percentile(train_plus['train_comment_length'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the train_data more than 100% point"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plus[train_plus['train_comment_length'] > 894].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plus['comment_text'][train_plus['train_comment_length'] > 414].value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['train_comment_length'][train_plus['train_comment_length'] > 894], y=train_plus['target'][train_plus['train_comment_length'] > 894],alpha = 0.5);\nplt.title('Target & Train_comment_length');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['train_comment_length'][train_plus['train_comment_length'] > 894],train_plus['target'][train_plus['train_comment_length'] > 894])[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\n\nCorrelation of comment_length is 0.006, and Cor of comment_length > 894 is 0.011.\n\nI think the longer the comment, the more likely it is that the comment is toxic."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nWe can see  defined correlation between \"target\" and \"comment_length\".\n\nSo I think this variable may be useful."},{"metadata":{},"cell_type":"markdown","source":"## <a id='26'>comment_text : Number of words</a>  "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plus['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['num_words'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Number of words');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['num_words'],train_plus['target'])[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['num_words'] )\nplt.title(\"Boxplot of num_words\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the Number of 100% point\nq25,q50,q75= np.percentile(train_plus['num_words'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['num_words'][train_plus['num_words'] > 156],train_plus['target'][train_plus['num_words'] > 156])[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\n\nCorrelation between traget and \"num_words\" is 0.0096, and Cor between target and \"num_words > 156\" is 0.017.\n\nI think the longer the num_words, the more likely it is that the comment is toxic."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nWe can see  defined correlation between \"target\" and \"num_words\".\n\nSo I think this variable may be useful."},{"metadata":{},"cell_type":"markdown","source":"## <a id='27'>comment_text : Number of Capitals</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plus['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['capitals'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Number of capitals');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['capitals'],train_plus['target'])[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['capitals'] )\nplt.title(\"Boxplot of capitals\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the Number of 100% point\nq25,q50,q75= np.percentile(train_plus['capitals'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['capitals'][train_plus['capitals'] > 23],train_plus['target'][train_plus['capitals'] > 23])[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\n\nCorrelation between traget and \"capitals\" is 0.026, and Cor between target and \"capitals > 23\" is 0.064.\n\nI think the longer the number of capitals, the more likely it is that the comment is toxic."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nWe can see  defined correlation between \"target\" and \"capitals\".\n\nSo I think this variable may be useful stongly."},{"metadata":{},"cell_type":"markdown","source":"## <a id='28'>comment_text : Number of exclamation marks</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_plus['num_exclamation_marks'] = train['comment_text'].apply(lambda comment: comment.count('!'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 8));\nsns.scatterplot(x=train_plus['num_exclamation_marks'], y=train_plus['target'],alpha = 0.5);\nplt.title('Target & Number of num_exclamation_marks');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"correlation coefficients is \",np.corrcoef(train_plus['num_exclamation_marks'],train_plus['target'])[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(3, 8));\nsns.boxplot(y=train_plus['num_exclamation_marks'] )\nplt.title(\"Boxplot of num_exclamation_marks\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check the Number of 100% point\nq25,q50,q75= np.percentile(train_plus['num_exclamation_marks'], [25,50,75])\niqr = q75- q25\nq100 = q75 + iqr*1.5\nprint(\"Nuber of 25% point = \",q25)\nprint(\"Nuber of 50% point = \",q50)\nprint(\"Nuber of 75% point = \",q75)\nprint(\"Nuber of 100% point = \",q100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discussion\n\nCorrelation between traget and \"num_exclamation_marks\" is 0.055.\n\nI think the longer the number of num_exclamation_marks, the more likely it is that the comment is toxic."},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nWe can see  defined correlation between \"target\" and \"num_exclamation_marks\".\n\nSo I think this variable may be useful."},{"metadata":{},"cell_type":"markdown","source":"We can see a little correlation between target and New Variable."},{"metadata":{},"cell_type":"markdown","source":"## <a id='29'>comment_text : Wordcloud</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=50,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train['comment_text'].sample(20000), title = 'Prevalent words in comments - train data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(test['comment_text'].sample(20000), title = 'Prevalent words in comments - test data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Politically related words are found*. For example Trump,Republician."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['insult'] < 0.25]['comment_text'].sample(20000), \n               title = 'Prevalent comments with insult score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['insult'] > 0.75]['comment_text'].sample(20000), \n               title = 'Prevalent comments with insult score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many words of direct insult are found. For example,stupid,idot.\n\nBut, We can also find politically related words .for example Trump,President."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['threat'] < 0.25]['comment_text'], \n               title = 'Prevalent words in comments with threat score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['threat'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with threat score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many words of direct threat are found. For kill,shoot,head.\n\nBut, dog is also threat word? I have to research it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['obscene']< 0.25]['comment_text'], \n               title = 'Prevalent words in comments with obscene score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['obscene'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with obscene score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many words of direct obscene are found. ass.\n\nAlso, I was surprised that the following words would become obscene\nDamn,fucking,shit,etc・・・."},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['target'] < 0.25]['comment_text'], \n               title = 'Prevalent words in comments with target score < 0.25')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_wordcloud(train.loc[train['target'] > 0.75]['comment_text'], \n               title = 'Prevalent words in comments with target score > 0.75')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Discussion\n\nFirstly, there is many Politically related words.\n\nSecondly,We found many word along each feature,\nbut there is something frong,for example \"Trump\" at highly insult, \"Damn\" at highly obscene."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nlater."},{"metadata":{},"cell_type":"markdown","source":"*Work still progress*"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}