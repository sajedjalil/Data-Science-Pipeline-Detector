{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Submission: BERT + GPT2"},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"%ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\npackage_dir_a = \"../input/huggingfacepytorchpretrainedbert/pytorch-pretrained-bert-master/pytorch-pretrained-BERT-master\"\nsys.path.insert(0, package_dir_a)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nimport warnings\nimport numpy as np\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\n\n# BERT \nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\n# GPT2\nfrom pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel, GPT2Model, GPT2Config\nfrom pytorch_pretrained_bert import GPT2Tokenizer\nfrom pytorch_pretrained_bert import OpenAIAdam\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pdb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELS_PATH = '../input/submission-toxicity-classification'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_MODEL_PATH = '../input/submission-toxicity-classification/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bert uncased\n!mkdir ../input/bert_uncased\n!cp ../input/submission-toxicity-classification/bert-models/bert-models/bert_config.json  ../input/bert_uncased/bert_config.json\n!cp ../input/submission-toxicity-classification/bert-models/bert-models/pytorch_model.bin ../input/bert_uncased/pytorch_model.bin","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df = test_df.iloc[:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helpers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_get_preds(model, x_test):\n    test_preds = np.zeros((len(x_test)))\n    test = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.long))\n    test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n    tk0 = tqdm(test_loader)\n    for i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n        test_preds[i*32:(i+1)*32] = pred[:, 0].detach().cpu().squeeze().numpy()\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n    return test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gpt2_get_preds(model, x_test):\n    test_preds = np.zeros((len(x_test)))\n    test = torch.utils.data.TensorDataset(torch.tensor(x_test, dtype=torch.long))\n    test_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\n    tk0 = tqdm(test_loader)\n    for i, (x_batch,) in enumerate(tk0):\n        pred = model(x_batch.to(device))\n        test_preds[i*32:(i+1)*32] = pred[:, 0].detach().cpu().squeeze().numpy()\n    test_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()\n    return test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gpt2_convert_lines(example, max_seq_length, tokenizer):\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[-max_seq_length:]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids(tokens_a) + [0]*(max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertHead(nn.Module):\n    \n    def __init__(self, config, hidden_units=256, num_aux_targets=6):\n        super(BertHead, self).__init__()\n        self.hidden_units = hidden_units \n        self.num_aux_targets = num_aux_targets\n        self.config = config\n        \n        self.bert_model = BertForSequenceClassification.from_pretrained(config, cache_dir=None, num_labels=hidden_units)\n        param_optimizer = list(self.bert_model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        \n        self.fc_out = nn.Linear(768, 1)\n        self.fc_aux_out = nn.Linear(768, num_aux_targets)\n        \n        self.fc_dp = nn.Dropout(p=0.4)\n        \n        self.optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n            {'params': [p for p in self.fc_out.parameters()], 'weight_decay': 0.0}, \n            {'params': [p for p in self.fc_aux_out.parameters()], 'weight_decay': 0.0}\n        ]\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert_model.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.bert_model.dropout(pooled_output)\n        logits = self.fc_dp(pooled_output)\n        out = self.fc_out(logits)\n        out_aux = self.fc_aux_out(logits)\n        return torch.cat([out, out_aux], 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_base_models = ['bert_epoch_2_lb_093967.bin', 'bert_epoch_2_lb_093916.bin', 'bert_epoch_1_lb_093970.bin']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = bert_convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), 220, bert_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_base_preds = np.zeros((x_test.shape[0], len(bert_base_models)))\n\nfor i, bert_model in enumerate(bert_base_models):\n    model = BertHead('../input/bert_uncased')\n    model.load_state_dict(torch.load(os.path.join(MODELS_PATH, bert_model)))\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n    preds = bert_get_preds(model, x_test)\n    bert_base_preds[:,i] = preds\n    \n    del model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GPT2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GPT2ClassificationHeadModel(GPT2PreTrainedModel):\n\n    def __init__(self, config, clf_dropout=0.4, num_aux_targets=6):\n        super(GPT2ClassificationHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.dropout = nn.Dropout(clf_dropout)\n        \n        self.fc_out = nn.Linear(config.n_embd * 2, 1)\n        self.fc_aux_out = nn.Linear(config.n_embd * 2, num_aux_targets)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        avg_pool = torch.mean(hidden_states, 1)\n        max_pool, _ = torch.max(hidden_states, 1)\n        logits = torch.cat((avg_pool, max_pool), 1)\n        logits = self.dropout(logits)\n        \n        out = self.fc_out(logits)\n        out_aux = self.fc_aux_out(logits)\n        \n        return torch.cat([out, out_aux], 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpt2_tokenizer = GPT2Tokenizer.from_pretrained('../input/submission-toxicity-classification/gpt2-models/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base"},{"metadata":{"trusted":true},"cell_type":"code","source":"gpt2_base_models = ['gpt2_epoch_1_lb_093912.bin', 'gpt2_epoch_1_lb_093904.bin', 'gpt2_k_epoch_2_lb_093902.bin']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = gpt2_convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), 220, gpt2_tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpt2_base_preds = np.zeros((x_test.shape[0], len(gpt2_base_models)))\n\nfor i, gpt2_model in enumerate(gpt2_base_models):\n    model = GPT2ClassificationHeadModel.from_pretrained('../input/submission-toxicity-classification/gpt2-models/')\n    model.load_state_dict(torch.load(os.path.join(MODELS_PATH, gpt2_model)))\n    model.to(device)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n    preds = gpt2_get_preds(model, x_test)\n    gpt2_base_preds[:,i] = preds\n    \n    del model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"### sample average for now\ntest_pred = ( np.average(bert_base_preds, axis=1) + np.average(gpt2_base_preds, axis=1) ) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}