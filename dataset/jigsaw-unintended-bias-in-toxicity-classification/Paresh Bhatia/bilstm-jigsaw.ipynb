{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport gc\nimport logging\nimport datetime\nimport warnings\nimport pickle\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.compat.v1.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard\nfrom tensorflow.compat.v1.keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.compat.v1.keras.preprocessing import text, sequence\nfrom tensorflow.compat.v1.keras.losses import binary_crossentropy\nfrom tensorflow.compat.v1.keras import backend as K\nimport tensorflow.compat.v1.keras.layers as L\nfrom tensorflow.compat.v1.keras import initializers, regularizers, constraints, optimizers, layers\n\nfrom tensorflow.compat.v1.keras.models import Model\nfrom tensorflow.compat.v1.keras.optimizers import Adam\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom tensorflow.compat.v1.keras.preprocessing.text import Tokenizer\nfrom tensorflow.compat.v1.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"COMMENT_TEXT_COL = 'comment_text'\nEMB_MAX_FEAT = 300\nMAX_LEN = 220\nBATCH_SIZE = 512\nNUM_EPOCHS = 4\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 512\nNUM_MODELS = 2\nEMB_PATHS = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\nJIGSAW_PATH = '../input/jigsaw-unintended-bias-in-toxicity-classification/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    return binary_crossentropy(K.reshape(y_true[:,0],(-1,1)), y_pred) * y_true[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_proc_and_tokenizer(train, test):\n    '''\n        credits go to: https://www.kaggle.com/tanreinama/simple-lstm-using-identity-parameters-solution/ \n    '''\n \n    identity_columns = ['asian', 'atheist',\n       'bisexual', 'black', 'buddhist', 'christian', 'female',\n       'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n       'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n       'muslim', 'other_disability', 'other_gender',\n       'other_race_or_ethnicity', 'other_religion',\n       'other_sexual_orientation', 'physical_disability',\n       'psychiatric_or_mental_illness', 'transgender', 'white']\n       \n    # Overall\n    weights = np.ones((len(train),)) / 4\n    # Subgroup\n    weights += (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) / 4\n    # Background Positive, Subgroup Negative\n    weights += (( (train['target'].values>=0.5).astype(bool).astype(np.int) +\n       (train[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n    # Background Negative, Subgroup Positive\n    weights += (( (train['target'].values<0.5).astype(bool).astype(np.int) +\n       (train[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n    loss_weight = 1.0 / weights.mean()\n    \n    y_train = np.vstack([(train['target'].values>=0.5).astype(np.int),weights]).T\n    y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n    \n    tokenizer = Tokenizer() \n    tokenizer.fit_on_texts(list(train[COMMENT_TEXT_COL]) + list(test[COMMENT_TEXT_COL]))\n    word_index = tokenizer.word_index\n    X_train = tokenizer.texts_to_sequences(list(train[COMMENT_TEXT_COL]))\n    X_test = tokenizer.texts_to_sequences(list(test[COMMENT_TEXT_COL]))\n    X_train = pad_sequences(X_train, maxlen=MAX_LEN)\n    X_test = pad_sequences(X_test, maxlen=MAX_LEN)\n\n    del identity_columns, weights, tokenizer, train, test\n    gc.collect()\n    \n    return X_train, y_train, X_test, y_aux_train, word_index, loss_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embedding_matrix(word_index, path):\n    '''\n     credits to: https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n    '''\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, EMB_MAX_FEAT))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n        except:\n            embedding_matrix[i] = embeddings_index[\"unknown\"]\n            \n    del embedding_index\n    gc.collect()\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_embeddings(word_index):\n    embedding_matrix = np.concatenate(\n        [build_embedding_matrix(word_index, f) for f in EMB_PATHS], axis=-1) \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to get train , test dataset and pre-trained embeddings\ndef load_data():\n    train = pd.read_csv(os.path.join(JIGSAW_PATH,'train.csv'), index_col='id')\n    test = pd.read_csv(os.path.join(JIGSAW_PATH,'test.csv'), index_col='id')\n    y_train = np.where(train['target'] >= 0.5, True, False) * 1\n    X_train, y_train, X_test, y_aux_train, word_index, loss_weight = run_proc_and_tokenizer(train, test)\n    embedding_matrix = build_embeddings(word_index)\n    del train,test\n    gc.collect()\n    return X_train, y_train, X_test, y_aux_train, word_index, embedding_matrix, loss_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_model(embedding_matrix, num_aux_targets, loss_weight):\n    words = Input(shape=(MAX_LEN,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss=[custom_loss,'binary_crossentropy'], loss_weights=[loss_weight, 1.0], \n                  optimizer='adam')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def run_model(X_train, y_train,X_test, y_aux_train, embedding_matrix, word_index, loss_weight):\n  \n    checkpoint_predictions = []\n    weights = []\n\n    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n    file_path = \"best_model.h5\"\n    tensorboard_callback = TensorBoard(\"logs\")\n    model.fit(\n        X_train, [y_train, y_aux_train],\n        batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1,\n        callbacks=[LearningRateScheduler(lambda epoch: 1.1e-3 * (0.55 ** epoch)),tensorboard_callback]\n    )\n    del X_train,y_train,y_aux_train,embedding_matrix\n    gc.collect()\n    \n    predictions =model.predict(X_test, batch_size=2048)[0].flatten()\n    model.save(file_path)\n    del model, X_test\n    gc.collect()\n    \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def submit(sub_preds):\n    submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n    submission['prediction'] = sub_preds\n    submission.reset_index(drop=False, inplace=True)\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def main():\n    X_train, y_train, X_test, y_aux_train, word_index,embedding_matrix, loss_weight = load_data()\n    model = build_model(embedding_matrix, y_aux_train.shape[-1], loss_weight)\n    model.summary()\n    del model\n    gc.collect()\n    sub_preds = run_model(X_train, y_train, X_test, y_aux_train, embedding_matrix, word_index, loss_weight)\n    submit(sub_preds)\n    \nif __name__ == \"__main__\":\n    main()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}