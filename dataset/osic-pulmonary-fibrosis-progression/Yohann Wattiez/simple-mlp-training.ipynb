{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Librairies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport pickle\nimport time\n\nfrom tensorflow import keras as K\nfrom tensorflow.keras import layers as L\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Set seed"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def seed_all(seed = 20):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    # # 5. For layers that introduce randomness like dropout, make sure to set seed values:\n    # model.add(Dropout(0.25, seed=seed_value))\n    # #6 Configure a new global `tensorflow` session: \n    # from keras import backend as K \n    # session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) \n    # sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n    # K.set_session(sess)\n    \nseed_all(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import raw data and copy\n# train = pd.read_csv('/kaggle/input/osic-pulmonary-fibrosis-progression/train.csv')\n# test = pd.read_csv('/kaggle/input/osic-pulmonary-fibrosis-progression/test.csv')\n\n# size_image = pd.read_csv('/kaggle/input/prep-data/size_image.csv')\n# list_files = pd.read_csv('/kaggle/input/prep-data/list_files.csv')\n\ntrain = pd.read_csv('/kaggle/input/data-preparation-for-osic/train.csv')\n\nraw_test = pd.read_csv('/kaggle/input/osic-pulmonary-fibrosis-progression/test.csv')\nX_prediction = pd.read_csv('/kaggle/input/osic-pulmonary-fibrosis-progression/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ID='Patient_Week'\nPINBALL_QUANTILE = [0.255, 0.50, 0.745]\nLAMBDA_LOSS = 0.585\nEPOCH = [54, 55, 20, 60, 23]\nBATCH_SIZE = 128\n\nNFOLD = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n#     y_true=tf.dtypes.cast(y_true, tf.float32)\n#     y_pred=tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.backend.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = PINBALL_QUANTILE\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.backend.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_score(y_true, y_pred):\n    y_true = tf.dtypes.cast(y_true, tf.float32)*(data_prep.fvc_max-data_prep.fvc_min)+data_prep.fvc_min\n    y_pred = tf.dtypes.cast(y_pred, tf.float32)*(data_prep.fvc_max-data_prep.fvc_min)+data_prep.fvc_min\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return -K.backend.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SELECTED_COLUMNS = ['Weeks', 'Percent', 'Age', 'Sex', 'Min_week', 'Base_FVC','Base_week', '_Currently smokes', '_Ex-smoker', '_Never smoked']\ndef create_model(lambda_loss):\n    model_input = K.Input(shape=(len(SELECTED_COLUMNS),))\n    x = L.Dense(500, activation=\"selu\", name='dense_to_freeze1')(model_input)\n    x = L.Dense(100, activation=\"selu\", name='dense_to_freeze2')(x)\n#     FVC = L.Dense(3)(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"selu\", name=\"p2\")(x)\n    FVC = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                         name=\"FVC\")([p1, p2])\n\n    model = K.Model(\n        inputs=model_input,\n        outputs=[FVC],\n    )\n#     boundaries = [150, 250, 350]\n#     values = [0.1, 0.08, 0.01, 0.001]\n#     learning_rate_fn = K.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n#     optimizer=K.optimizers.Adam(learning_rate=learning_rate_fn, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n    model.compile(\n        optimizer=K.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False),\n        loss=mloss(lambda_loss),\n        metrics=score,\n    )\n    return model\n\nmodel = create_model(LAMBDA_LOSS)\n# tf.keras.utils.plot_model(model)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_prediction['Patient'] = X_prediction['Patient_Week'].str.extract(r'(.*)_.*')\nX_prediction['Weeks'] = X_prediction['Patient_Week'].str.extract(r'.*_(.*)').astype(int)\nX_prediction = X_prediction[['Patient', 'Weeks', 'Patient_Week']]\nrename_cols = {'Weeks_y':'Min_week', 'Weeks_x': 'Weeks', 'FVC':'Base_FVC'}\nX_prediction = X_prediction.merge(raw_test, how='left', left_on='Patient', right_on='Patient').rename(columns=rename_cols)[['Patient', 'Min_week', 'Base_FVC', 'Percent', 'Age', 'Sex', 'SmokingStatus', 'Weeks', 'Patient_Week']].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_prediction['Base_week'] = X_prediction['Weeks'] - X_prediction['Min_week']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename_cols = {'Weeks_x':'Base_week', 'FVC_x': 'Base_FVC', 'Percent_x': 'Base_percent', 'Age_x': 'Age', 'SmokingStatus_x': 'SmokingStatus', 'Sex_x':'Sex', 'Weeks_y':'Weeks', 'FVC_y': 'FVC'}\n# drop_cols = ['Age_y', 'Sex_y', 'SmokingStatus_y', 'Percent_y']\n# test = test.merge(test, how='left', left_on='Patient', right_on='Patient').rename(columns=rename_cols).drop(columns=drop_cols)\n# test[ID] = test['Patient'].astype(str) + '_' + test['Weeks'].astype(str)\n# test = test[['Patient', 'Base_week', 'Base_FVC', 'Base_percent', 'Age', 'Sex', 'SmokingStatus', 'Weeks', 'Patient_Week', 'FVC']].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n\n#Override OneHotEncoder to have the column names created automatically (Ex-smoker, Never Smoked...) \nclass OneHotEncoder(SklearnOneHotEncoder):\n    def __init__(self, **kwargs):\n        super(OneHotEncoder, self).__init__(**kwargs)\n        self.fit_flag = False\n\n    def fit(self, X, **kwargs):\n        out = super().fit(X)\n        self.fit_flag = True\n        return out\n\n    def transform(self, X, categories, index='', name='', **kwargs):\n        sparse_matrix = super(OneHotEncoder, self).transform(X)\n        new_columns = self.get_new_columns(X=X, name=name, categories=categories)\n        d_out = pd.DataFrame(sparse_matrix.toarray(), columns=new_columns, index=index)\n        return d_out\n\n    def fit_transform(self, X, categories, index, name, **kwargs):\n        self.fit(X)\n        return self.transform(X, categories=categories, index=index, name=name)\n\n    def get_new_columns(self, X, name, categories):\n        new_columns = []\n        for j in range(len(categories)):\n            new_columns.append('{}_{}'.format(name, categories[j]))\n        return new_columns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.exceptions import NotFittedError\n\ndef standardisation(x, u, s):\n    return (x-u)/s\n\ndef normalization(x, ma, mi):\n    return (x-mi)/(ma-mi)\n\nclass data_preparation():\n    def __init__(self, bool_normalization=True,bool_standard=False):\n        self.enc_sex = LabelEncoder()\n        self.enc_smok = LabelEncoder()\n        self.onehotenc_smok = OneHotEncoder()\n        self.standardisation = bool_standard\n        self.normalization = bool_normalization\n        \n        \n    def __call__(self, data_untransformed):\n        data = data_untransformed.copy(deep=True)\n        \n        #For the test set/Already fitted\n        try:\n            data['Sex'] = self.enc_sex.transform(data['Sex'].values)\n            data['SmokingStatus'] = self.enc_smok.transform(data['SmokingStatus'].values)\n            data = pd.concat([data.drop(columns=['SmokingStatus']), self.onehotenc_smok.transform(data['SmokingStatus'].values.reshape(-1,1), categories=self.enc_smok.classes_, name='', index=data.index).astype(int)], axis=1)\n            \n            #Standardisation\n            if self.standardisation:\n                data['Base_week'] = standardisation(data['Base_week'],self.base_week_mean,self.base_week_std)\n                data['Base_FVC'] = standardisation(data['Base_FVC'],self.base_fvc_mean,self.base_fvc_std)\n                data['Base_percent'] = standardisation(data['Base_percent'],self.base_percent_mean,self.base_percent_std)\n                data['Age'] = standardisation(data['Age'],self.age_mean,self.age_std)\n                data['Weeks'] = standardisation(data['Weeks'],self.weeks_mean,self.weeks_std)\n            \n            #Normalization\n            if self.normalization:\n                data['Base_week'] = normalization(data['Base_week'],self.base_week_max,self.base_week_min)\n                data['Base_FVC'] = normalization(data['Base_FVC'],self.base_fvc_max,self.base_fvc_min)\n                data['Percent'] = normalization(data['Percent'],self.base_percent_max,self.base_percent_min)\n                data['Age'] = normalization(data['Age'],self.age_max,self.age_min)\n                data['Weeks'] = normalization(data['Weeks'],self.weeks_max,self.weeks_min)\n                data['Min_week'] = normalization(data['Min_week'],self.base_week_max,self.base_week_min)\n\n        #For the train set/Not yet fitted    \n        except NotFittedError:\n            data['Sex'] = self.enc_sex.fit_transform(data['Sex'].values)\n            data['SmokingStatus'] = self.enc_smok.fit_transform(data['SmokingStatus'].values)\n            data = pd.concat([data.drop(columns=['SmokingStatus']), self.onehotenc_smok.fit_transform(data['SmokingStatus'].values.reshape(-1,1), categories=self.enc_smok.classes_, name='', index=data.index).astype(int)], axis=1)\n            \n            #Standardisation\n            if self.standardisation:\n                self.base_week_mean = data['Base_week'].mean()\n                self.base_week_std = data['Base_week'].std()\n                data['Base_week'] = standardisation(data['Base_week'],self.base_week_mean,self.base_week_std)\n\n                self.base_fvc_mean = data['Base_FVC'].mean()\n                self.base_fvc_std = data['Base_FVC'].std()\n                data['Base_FVC'] = standardisation(data['Base_FVC'],self.base_fvc_mean,self.base_fvc_std)\n\n                self.base_percent_mean = data['Base_percent'].mean()\n                self.base_percent_std = data['Base_percent'].std()\n                data['Base_percent'] = standardisation(data['Base_percent'],self.base_percent_mean,self.base_percent_std)\n\n                self.age_mean = data['Age'].mean()\n                self.age_std = data['Age'].std()\n                data['Age'] = standardisation(data['Age'],self.age_mean,self.age_std)\n\n                self.weeks_mean = data['Weeks'].mean()\n                self.weeks_std = data['Weeks'].std()\n                data['Weeks'] = standardisation(data['Weeks'],self.weeks_mean,self.weeks_std)\n\n                \n            #Normalization\n            if self.normalization:\n                self.base_week_min = data['Base_week'].min()\n                self.base_week_max = data['Base_week'].max()\n                data['Base_week'] = normalization(data['Base_week'],self.base_week_max,self.base_week_min)\n\n                self.base_fvc_min = data['Base_FVC'].min()\n                self.base_fvc_max = data['Base_FVC'].max()\n                data['Base_FVC'] = normalization(data['Base_FVC'],self.base_fvc_max,self.base_fvc_min)\n\n                self.base_percent_min = data['Percent'].min()\n                self.base_percent_max = data['Percent'].max()\n                data['Percent'] = normalization(data['Percent'],self.base_percent_max,self.base_percent_min)\n\n                self.age_min = data['Age'].min()\n                self.age_max = data['Age'].max()\n                data['Age'] = normalization(data['Age'],self.age_max,self.age_min)\n\n                self.weeks_min = data['Weeks'].min()\n                self.weeks_max = data['Weeks'].max()\n                data['Weeks'] = normalization(data['Weeks'],self.weeks_max,self.weeks_min)\n                \n                self.base_week_min = data['Min_week'].min()\n                self.base_week_max = data['Min_week'].max()\n                data['Min_week'] = normalization(data['Min_week'],self.base_week_max,self.base_week_min)\n\n            \n        return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickefile = open('/kaggle/input/data-preparation-for-osic/data_prep', 'rb')\ndata_prep = pickle.load(pickefile)\npickefile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_prediction = data_prep(X_prediction)\n# test = train[train['Patient']=='ID00009637202177434476278']\n# train = train[~(train['Patient']=='ID00009637202177434476278')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list_patient_score=[]\n# for i in train.Patient.unique():\n#     model = create_model(LAMBDA_LOSS)\n#     print(i)\n#     history = model.fit(x=train[~train.Patient.isin([i])][SELECTED_COLUMNS], y=train[~train.Patient.isin([i])][['FVC']], validation_data=(train[train.Patient.isin([i])][SELECTED_COLUMNS], train[train.Patient.isin([i])][['FVC']]), epochs=250)\n#     list_patient_score.append([i, history.history['val_score']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickefile = open('/kaggle/input/trained-cnn-mlp-for-osic/list_patient_score', 'rb')\nlist_patient_score = pickle.load(pickefile)\npickefile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pickefile = open('list_patient_score', 'wb')\n# pickle.dump(list_patient_score, pickefile)\n# pickefile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list_mean_score=[]\n# for i in list_patient_score:\n#     list_mean_score.append(np.mean(i[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# count=0\n# for i in list_patient_score:\n#     if np.mean(i[1])>8:\n#         plt.plot(i[1])\n#         count+=1    \n# print(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_patient_weight = []\nfor i in list_patient_score:\n    if 6.4>np.mean(i[1]):\n        list_patient_weight.append([i[0], 3])\n    elif 6.8>np.mean(i[1])>6.4:\n        list_patient_weight.append([i[0], 50])\n    elif 7.1>np.mean(i[1])>6.8:\n        list_patient_weight.append([i[0], 100])\n    elif 50>np.mean(i[1])>7.6:\n        list_patient_weight.append([i[0], 3])\n    else:\n        list_patient_weight.append([i[0], 3])\n        \ntrain['Weight'] = train.Patient.map(dict(list_patient_weight))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickefile = open('list_patient_weight', 'wb')\npickle.dump(list_patient_weight, pickefile)\npickefile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selection for KFOLD\nlist_patient_KFOLD=[]\nfor i in list_patient_score:\n    if np.mean(i[1]) < 6.26:\n        list_patient_KFOLD.append([i[0], 0])\n    elif 6.26<=np.mean(i[1]) < 6.43:\n        list_patient_KFOLD.append([i[0], 1])\n    elif 6.43<=np.mean(i[1]) < 6.74:\n        list_patient_KFOLD.append([i[0], 2])\n    elif 6.74<=np.mean(i[1]) < 7.15:\n        list_patient_KFOLD.append([i[0], 3])\n    elif 7.15<=np.mean(i[1]) < 50:\n        list_patient_KFOLD.append([i[0], 4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = create_model(LAMBDA_LOSS)\n# # list_patients = train.Patient.sample(12).to_list()\n# # history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH, sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n# history = model.fit(x=train[SELECTED_COLUMNS], y=train[['FVC']], epochs=EPOCH, sample_weight=train.Weight, verbose=0)\n# model.save('model')\n# # plt.plot(history.history['val_score'])\n# plt.plot(history.history['score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in range(1,6):\n#     model = create_model(LAMBDA_LOSS)\n#     list_patients = train.Patient.unique()[np.random.randint(0, len(train.Patient.unique())-1, size=round(len(train.Patient.unique())/NFOLD))]\n#     history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH, sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n#     # model.save('model')\n#     plt.figure(figsize=(20,20))\n#     plt.subplot(3,3,i)\n#     plt.plot(history.history['val_score'])\n#     plt.plot(history.history['score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pe = np.zeros((X_prediction.shape[0], 3))\n# pred = np.zeros((train.shape[0], 3))\n# i=0\n# EPOCH = [54, 55, 56, 57, 58]\n# for j in range(NFOLD):\n#     print(f\"FOLD {i}\")\n#     model = create_model(LAMBDA_LOSS)\n#     list_patients = [j[0] for j in list_patient_KFOLD if j[1]==i]\n# #     list_patients = train.Patient.unique()[np.random.randint(0, len(train.Patient.unique())-1, size=round(len(train.Patient.unique())/NFOLD))]\n#     history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH[j], sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n#     print(\"train\", model.evaluate(train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[~train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"val\", model.evaluate(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n#     pred[train[train.Patient.isin(list_patients)].index] = model.predict(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)\n#     pe += model.predict(X_prediction[SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0) / NFOLD\n#     model.save('model_' + str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KFOLD_confidence = [0.05, 0.15, 0.2, 0.25, 0.35]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pe = np.zeros((X_prediction.shape[0], 3))\npred = np.zeros((train.shape[0], 3))\nfor i in range(NFOLD):\n    print(f\"FOLD {i}\")\n    model = create_model(LAMBDA_LOSS)\n    list_patients = [j[0] for j in list_patient_KFOLD if j[1]==i]\n#     list_patients = train.Patient.unique()[np.random.randint(0, len(train.Patient.unique())-1, size=round(len(train.Patient.unique())/NFOLD))]\n    history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH[i], sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n    print(\"train\", model.evaluate(train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[~train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", model.evaluate(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE))\n    pred[train[train.Patient.isin(list_patients)].index] = model.predict(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)\n    pe += model.predict(X_prediction[SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)* KFOLD_confidence[i]\n    model.save('model_' + str(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss_value = [0.58, 0.585, 0.59, 0.595]\n# for loss in loss_value:\n#     print('Loss_value; %f' % loss)\n#     res_train = np.zeros((5,1))\n#     res_val = np.zeros((5,1))\n#     for k in range(3):\n#         for i in range(NFOLD):\n#             model = create_model(LAMBDA_LOSS)\n#             list_patients = [j[0] for j in list_patient_KFOLD if j[1]==i]\n#             history = model.fit(x=train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], y=train[~train.Patient.isin(list_patients)][['FVC']], validation_data=(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']]), epochs=EPOCH[i], sample_weight=train[~train.Patient.isin(list_patients)].Weight, verbose=0)\n#             res_train[i] += model.evaluate(train[~train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[~train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE)[1]/3\n#             res_val[i] += model.evaluate(train[train.Patient.isin(list_patients)][SELECTED_COLUMNS], train[train.Patient.isin(list_patients)][['FVC']], verbose=0, batch_size=BATCH_SIZE)[1]/3\n#     for p in range(5):\n#         print('Train')\n#         print(res_train[p])\n#         print('Val')\n#         print(res_val[p])\n            \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NFOLD = 3\n# kf = KFold(n_splits=NFOLD)\n# pe = np.zeros((X_prediction.shape[0], 3))\n# pred = np.zeros((train.shape[0], 3))\n\n# cnt = 0\n# EPOCHS = 200\n# for tr_idx, val_idx in kf.split(train):\n#     cnt += 1\n#     print(f\"FOLD {cnt}\")\n#     net = create_model(LAMBDA_LOSS)\n#     net.fit(train.loc[tr_idx, SELECTED_COLUMNS], train.loc[tr_idx, 'FVC'], batch_size=BATCH_SIZE, epochs=EPOCHS,\n#             validation_data=(train.loc[val_idx, SELECTED_COLUMNS], train.loc[val_idx, 'FVC']), sample_weight=train.loc[tr_idx, 'Weight'], verbose=0)\n#     print(\"train\", net.evaluate(train.loc[tr_idx, SELECTED_COLUMNS], train.loc[tr_idx, 'FVC'], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"val\", net.evaluate(train.loc[val_idx, SELECTED_COLUMNS], train.loc[val_idx, 'FVC'], verbose=0, batch_size=BATCH_SIZE))\n#     print(\"predict val...\")\n#     pred[val_idx] = net.predict(train.loc[val_idx, SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0)\n#     print(\"predict test...\")\n#     pe += net.predict(X_prediction[SELECTED_COLUMNS], batch_size=BATCH_SIZE, verbose=0) / NFOLD\n#     model.save('model_' + str(cnt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigma_opt = mean_absolute_error(train[['FVC']], pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)\n\nX_prediction['FVC1'] = 0.996*pe[:, 1]\nX_prediction['Confidence1'] = pe[:, 2] - pe[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = X_prediction.copy()\nsubm['FVC'] = 3020\nsubm['Confidence'] = 100\n\nsubm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"otest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1\nsubm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SELECTED_COLUMNS = ['Base_week', 'Base_FVC', 'Base_percent', 'Age', 'Sex','Weeks', '_Currently smokes', '_Ex-smoker', '_Never smoked']\n# history = model.fit(x=train[SELECTED_COLUMNS], y=train[['FVC']], validation_data=(test[SELECTED_COLUMNS], test[['FVC']]), epochs=EPOCH)\n# model.save('model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SELECTED_COLUMNS = ['Base_week', 'Base_FVC', 'Base_percent', 'Age', 'Sex','Weeks', '_Currently smokes', '_Ex-smoker', '_Never smoked']\n# history=[]\n# for i in np.linspace(0,1,11):\n#     print('Lambda %f' % i)\n#     model = model = create_model(i)\n#     history.append(model.fit(x=train[SELECTED_COLUMNS], y=train[['FVC']], validation_data=(test[SELECTED_COLUMNS], test[['FVC']]), epochs=200))\n# # model.save('model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# plt.figure(figsize=(30,10))\n# for i in range(1,11):\n#     plt.subplot(3,4,i)\n#     plt.plot(history[i].history['score'])\n#     plt.plot(history[i].history['val_score'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}