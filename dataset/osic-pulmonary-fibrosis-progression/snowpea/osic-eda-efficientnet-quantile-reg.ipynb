{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"overview\"></a>\n# Overview ğŸ§\n[Pulmonary fibrosis is a lung disease that occurs when lung tissue becomes damaged and scarred. This thickened, stiff tissue makes it more difficult for your lungs to work properly. As pulmonary fibrosis worsens, you become progressively more short of breath.](https://www.mayoclinic.org/diseases-conditions/pulmonary-fibrosis/symptoms-causes/syc-20353690)<br>\n<font color=\"RoyalBlue\">è‚ºç·šç¶­ç—‡ã¨ã¯ã€è‚ºã®çµ„ç¹”ãŒå‚·ã¤ãã€å‚·è·¡ãŒæ®‹ã‚‹ã“ã¨ã§èµ·ã“ã‚‹è‚ºã®ç—…æ°—ã§ã™ã€‚ã“ã®è‚¥åšã—ãŸç¡¬ã„çµ„ç¹”ã¯ã€è‚ºã®æ­£å¸¸ãªå‹•ä½œã‚’å›°é›£ã«ã—ã¾ã™ã€‚è‚ºç·šç¶­ç—‡ãŒæ‚ªåŒ–ã™ã‚‹ã¨ã€å¾ã€…ã«æ¯åˆ‡ã‚ŒãŒã²ã©ããªã‚Šã¾ã™ã€‚</font>\n<img src='https://i.imgur.com/edKPRik.png' width=\"600\">\nIn \"OSIC Pulmonary Fibrosis Progression\", we needs to predict a patientâ€™s severity of decline in lung function based on a CT scan of their lungs by using AI machine learning. In detail, we must predict both a Forced vital capacity (FVC) and a confidence measure for each patient.<br>\n<font color=\"RoyalBlue\">ã€ŒOSIC è‚ºç·šç¶­ç—‡ã®é€²è¡Œã€ã§ã¯ã€è‚ºã®CTã‚¹ã‚­ãƒ£ãƒ³ã«åŸºã¥ã„ã¦æ‚£è€…ã®è‚ºæ©Ÿèƒ½ã®ä½ä¸‹ã®é‡ç—‡åº¦ã‚’ AI æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ã¦äºˆæ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</font><br>\n<font color=\"RoyalBlue\">è©³ã—ãè¨€ã†ã¨ã€å„æ‚£è€…ã®åŠªåŠ›è‚ºæ´»é‡ï¼ˆFVCï¼‰ã¨ä¿¡é ¼åº¦ã®ä¸¡æ–¹ã‚’äºˆæ¸¬ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚</font>\n\n# Table of contents ğŸ“–\n* [Overview ğŸ§](#overview)\n* [Acknowledgements ğŸ™‡](#acknowledgements)\n* [Setup ğŸ’»](#setup)\n* [Load the data ğŸ“ƒ](#load)\n* [Explore CSV data ğŸ“Š](#explore)\n    * [Distribution of unique patients data ğŸ˜· (Age, Sex, SmokingStatus)](#unique)\n    * [Weeks distribution ğŸ“…](#weeks)\n    * [FVC & Percent distribution ğŸ’¨](#percent)\n    * [Relationships between FVC and other variables ğŸ¤](#fvc)\n* [Linear Decay (based on EfficientNets) ğŸ“·](#efficient)\n* [Multiple Quantile Regression ğŸŒ’](#quantile)\n    * [Data preprocessing for Multiple Quantile Regression ğŸ§¹](#quantile_d)\n    * [Build the model ğŸ§ ](#quantile_m)\n    * [Cross validation ğŸ’­](#quantile_c)\n* [Ensemble & Submit ğŸ“](#submit)\n\n<a id=\"acknowledgements\"></a>\n# Acknowledgements ğŸ™‡\n\n- Ulrich GOUE's [Osic-Multiple-Quantile-Regression-Starter](https://www.kaggle.com/ulrich07/osic-multiple-quantile-regression-starter)\n- Michael Kazachok's [Linear Decay (based on ResNet CNN)](https://www.kaggle.com/miklgr500/linear-decay-based-on-resnet-cnn)\n- Wei Hao Khoong's [](http://)[EfficientNets + Quantile Regression (Inference)](https://www.kaggle.com/khoongweihao/efficientnets-quantile-regression-inference)\n\n<a id=\"setup\"></a>\n# Setup ğŸ’»\nAll seed values are fixed at 42.<br>\n<font color=\"RoyalBlue\">ã‚·ãƒ¼ãƒ‰å€¤ã¯å…¨ã¦42ã§å›ºå®šã—ã¦ã„ã¾ã™ã€‚</font><br>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n!pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index\n\nimport efficientnet.tfkeras as efn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, train_test_split \nimport tensorflow as tf\nfrom tensorflow.keras import Model, backend\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.utils import Sequence\nfrom keras.utils.vis_utils import plot_model\n\nimport pydicom\nimport cv2\n\ndef seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(42)\n\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\n\npalette_ro = [\"#ee2f35\", \"#fa7211\", \"#fbd600\", \"#75c731\", \"#1fb86e\", \"#0488cf\", \"#7b44ab\"]\n\nROOT = \"../input/osic-pulmonary-fibrosis-progression/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"load\"></a>\n# Load the data ğŸ“ƒ"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(ROOT + \"train.csv\")\ntest = pd.read_csv(ROOT + \"test.csv\")\nsub = pd.read_csv(ROOT + \"sample_submission.csv\")\n\nprint(\"Training data shape: \", train.shape)\nprint(\"Test data shape: \", test.shape)\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Patient` - å„æ‚£è€…å›ºæœ‰ã® IDï¼ˆæ‚£è€…ã® DICOM ãƒ•ã‚©ãƒ«ãƒ€ã®åå‰ã§ã‚‚ã‚ã‚Šã¾ã™ï¼‰\n* `Weeks` - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ CT ã®å‰å¾Œã®ç›¸å¯¾çš„ãªé€±æ•°ï¼ˆè² æ•°ã®å ´åˆã‚‚ï¼‰\n* `FVC` - è¨˜éŒ²ã•ã‚ŒãŸåŠªåŠ›è‚ºæ´»é‡ï¼ˆmLï¼‰\n* `Percent` - %FVC, ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè‚ºæ´»é‡ã€‚å¹´é½¢ãƒ»èº«é•·ãƒ»æ€§åˆ¥ã‹ã‚‰è¨ˆç®—ã—ãŸäºˆæ¸¬ FVC ã«å¯¾ã™ã‚‹å®Ÿéš›ã® FVC ã®å‰²åˆ\n* `Age` - å¹´é½¢\n* `Sex` - æ€§åˆ¥ï¼ˆ`Male` / `Female`ï¼‰\n* `SmokingStatus` - å–«ç…™çŠ¶æ…‹ï¼ˆ`Never smoked` / `Ex-smoker` / `Currently smokes`ï¼‰\n\n<a id=\"explore\"></a>\n# Explore CSV data ğŸ“Š"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in both `train` and `test`.<br>\n<font color=\"RoyalBlue\">train ã¨ test ã®ä¸¡æ–¹ã«æ¬ æå€¤ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚</font><br>\nBefore EDA, we will try to find duplicate rows in the `train` where the `Patient` and `Weeks` elements match.<br>\n<font color=\"RoyalBlue\">EDA ã®å‰ã«ã€train ã®ä¸­ã«ã‚ã‚‹ Patient ã¨ Weeks ã®è¦ç´ ãŒä¸€è‡´ã—é‡è¤‡ã—ã¦ã„ã‚‹è¡Œã‚’æ¢ã—ã¦ã¿ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"dupRows_train = train[train.duplicated(subset=['Patient', 'Weeks'], keep=False)]\n\nprint(\"There are {} duplicate rows here ({:.2f} percent of the total).\".format(len(dupRows_train), len(dupRows_train)/len(train)*100))\ndupRows_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There don't seem to be too many of them. These duplicate rows should be removed.<br>\n<font color=\"RoyalBlue\">æ•°ã¯ã‚ã¾ã‚Šå¤šããªã„ã‚ˆã†ã§ã™ã€‚ã“ã‚Œã‚‰ã®é‡è¤‡ã—ãŸè¡Œã¯å‰Šé™¤ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates(subset=['Patient', 'Weeks'], keep=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following table from [Is this Malware? [EDA, FE and lgb][updated]](https://www.kaggle.com/artgor/is-this-malware-eda-fe-and-lgb-updated)<br>\n\n<font color=\"RoyalBlue\">ã‚«ãƒ©ãƒ å / ã‚«ãƒ©ãƒ ã”ã¨ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•° / æœ€ã‚‚å‡ºç¾é »åº¦ã®é«˜ã„å€¤ / æœ€ã‚‚å‡ºç¾é »åº¦ã®é«˜ã„å€¤ã®å‡ºç¾å›æ•° / æ¬ ææå€¤ã®å‰²åˆ / æœ€ã‚‚å¤šã„ã‚«ãƒ†ã‚´ãƒªã®å‰²åˆ / dtypes ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚<br>\ntrain ã«ãŠã‘ã‚‹ Patient ã®å›ºæœ‰ ID æ•°ã¯176ã®ã‚ˆã†ã§ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = []\nfor col in train.columns:\n    stats.append((col,\n                  train[col].nunique(),\n                  train[col].value_counts().index[0],\n                  train[col].value_counts().values[0],\n                  train[col].isnull().sum() * 100 / train.shape[0],\n                  train[col].value_counts(normalize=True, dropna=False).values[0] * 100,\n                  train[col].dtype))\nstats_df = pd.DataFrame(stats, columns=['Feature', 'Unique values', 'Most frequent item', 'Freuquence of most frequent item', 'Percentage of missing values', 'Percentage of values in the biggest category', 'Type'])\nstats_df.sort_values('Percentage of missing values', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"unique\"></a>\n## Distribution of unique patients data ğŸ˜· (Age, Sex, SmokingStatus)\n<font color=\"RoyalBlue\">ã§ã¯ã€train ã«ãŠã‘ã‚‹ Patient ã®å›ºæœ‰ ID ã”ã¨ã®å¹´é½¢ã€æ€§åˆ¥ã€å–«ç…™çŠ¶æ³ã®åˆ†å¸ƒã‹ã‚‰è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train.groupby(\"Patient\").first().reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(16, 12))\n\nsns.distplot(data[\"Age\"], ax=ax1, bins=data[\"Age\"].max()-data[\"Age\"].min()+1, color=palette_ro[1])\nax1.annotate(\"Min: {:,}\".format(data[\"Age\"].min()), xy=(data[\"Age\"].min(), 0.005), \n             xytext=(data[\"Age\"].min()-8, 0.02),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(data[\"Age\"].max()), xy=(data[\"Age\"].max(), 0.005), \n             xytext=(data[\"Age\"].max()-2, 0.02),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.axvline(x=data[\"Age\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Med: {:.0f}\".format(data[\"Age\"].median()), xy=(data[\"Age\"].median(), 0.056), \n             xytext=(data[\"Age\"].median()-15, 0.065),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=0.25\"))\n\nsns.countplot(x=\"Sex\", ax=ax2, data=data, palette=palette_ro[-2::-4])\nsns.countplot(x=\"SmokingStatus\", ax=ax3, data=data,\n              order=[\"Never smoked\", \"Ex-smoker\", \"Currently smokes\"], palette=palette_ro[-3::-2])\n\nsns.distplot(data[data[\"Sex\"]==\"Male\"].Age, label=\"Male\", ax=ax4, hist=False, color=palette_ro[5])\nsns.distplot(data[data[\"Sex\"]==\"Female\"].Age, label=\"Female\", ax=ax4, hist=False, color=palette_ro[1])\n\nsns.distplot(data[data[\"SmokingStatus\"]==\"Never smoked\"].Age, label=\"Never smoked\", ax=ax5, hist=False, color=palette_ro[4])\nsns.distplot(data[data[\"SmokingStatus\"]==\"Ex-smoker\"].Age, label=\"Ex-smoker\", ax=ax5, hist=False, color=palette_ro[2])\nsns.distplot(data[data[\"SmokingStatus\"]==\"Currently smokes\"].Age, label=\"Currently smokes\", ax=ax5, hist=False, color=palette_ro[0])\n\nsns.countplot(x=\"SmokingStatus\", ax=ax6, data=data, hue=\"Sex\",\n              order=[\"Never smoked\", \"Ex-smoker\", \"Currently smokes\"], palette=palette_ro[-2::-4])\n\nfig.suptitle(\"Distribution of unique patients data\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compared to `male`, `female` seems to be of a wider age range and is less likely to smoke. And, `Never smoked` tend to be younger than `Ex-smoker`.<br>\n<font color=\"RoyalBlue\">ç”·æ€§ã«æ¯”ã¹ã¦å¥³æ€§ã¯å¹´é½¢å±¤ãŒå¹…åºƒãã€å–«ç…™è€…ãŒå°‘ãªã„ã‚ˆã†ã§ã™ã€‚ã¾ãŸã€å–«ç…™æœªçµŒé¨“è€…ã¯å…ƒå–«ç…™è€…ã‚ˆã‚Šã‚‚è‹¥ã„å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚</font>\n\n<a id=\"weeks\"></a>\n## Weeks distribution ğŸ“…"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 6))\n\nsns.distplot(train[\"Weeks\"], ax=ax, color=palette_ro[1], bins=train[\"Weeks\"].max()-train[\"Weeks\"].min()+1)\nax.annotate(\"Min: {:,}\".format(train[\"Weeks\"].min()), xy=(train[\"Weeks\"].min(), 0.005), \n            xytext=(train[\"Weeks\"].min()-8, 0.008),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax.annotate(\"Max: {:,}\".format(train[\"Weeks\"].max()), xy=(train[\"Weeks\"].max(), 0.005), \n            xytext=(train[\"Weeks\"].max()-2, 0.008),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=-0.2\"))\nax.axvline(x=0, color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax.annotate(\"CT Scan\", xy=(0, 0.013), \n            xytext=(-12, 0.016),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax.axvline(x=train[\"Weeks\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax.annotate(\"Med: {:.0f}\".format(train[\"Weeks\"].median()), xy=(train[\"Weeks\"].median(), 0.020), \n            xytext=(train[\"Weeks\"].median()+2, 0.024),\n            bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n            arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\n\nax.set_title(\"Weeks Distribution\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"percent\"></a>\n## FVC & Percent distribution ğŸ’¨"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(train[\"FVC\"], ax=ax1, color=palette_ro[5], hist=False)\nax1.annotate(\"Min: {:,}\".format(train[\"FVC\"].min()), xy=(train[\"FVC\"].min(), 0.00005), \n             xytext=(train[\"FVC\"].min()-300, 0.0001),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax1.annotate(\"Max: {:,}\".format(train[\"FVC\"].max()), xy=(train[\"FVC\"].max(), 0.00005), \n             xytext=(train[\"FVC\"].max()-200, 0.0001),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax1.axvline(x=train[\"FVC\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Med: {:,.0f}\".format(train[\"FVC\"].median()), xy=(train[\"FVC\"].median(), 0.00005), \n             xytext=(train[\"FVC\"].median()-750, 0.0001),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax1.set_title(\"FVC Distribution\", fontsize=16);\n\nsns.distplot(train[\"Percent\"], ax=ax2, color=palette_ro[3], hist=False)\nax2.annotate(\"Min: {:.2f}\".format(train[\"Percent\"].min()), xy=(train[\"Percent\"].min(), 0.0015), \n             xytext=(train[\"Percent\"].min()-8, 0.0040),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"arc3, rad=0.2\"))\nax2.annotate(\"Max: {:.2f}\".format(train[\"Percent\"].max()), xy=(train[\"Percent\"].max(), 0.0015), \n             xytext=(train[\"Percent\"].max()-4, 0.0040),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                             connectionstyle=\"arc3, rad=-0.2\"))\nax2.axvline(x=train[\"Percent\"].median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Med: {:.2f}\".format(train[\"Percent\"].median()), xy=(train[\"Percent\"].median(), 0.0015), \n             xytext=(train[\"Percent\"].median()-17, 0.0040),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax2.set_title(\"Percent Distribution\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"fvc\"></a>\n## Relationships between FVC and other variables ğŸ¤\n\nLet's look at the relationships between the objective variable, `FVC`, and other variables.<br>\n<font color=\"RoyalBlue\">ç›®çš„å¤‰æ•°ã§ã‚ã‚‹ FVC ã¨ä»–ã®å¤‰æ•°ã¨ã®é–¢ä¿‚ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(train[train[\"Sex\"]==\"Male\"].FVC, label=\"Male\", ax=ax1, hist=False, color=palette_ro[5])\nax1.axvline(x=train[train[\"Sex\"]==\"Male\"].FVC.median(), color=palette_ro[5], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Male\\nMed: {:,.0f}\".format(train[train[\"Sex\"]==\"Male\"].FVC.median()), xy=(train[train[\"Sex\"]==\"Male\"].FVC.median(), 0.0006), \n             xytext=(train[train[\"Sex\"]==\"Male\"].FVC.median()+100, 0.00065),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\nsns.distplot(train[train[\"Sex\"]==\"Female\"].FVC, label=\"Female\", ax=ax1, hist=False, color=palette_ro[1])\nax1.axvline(x=train[train[\"Sex\"]==\"Female\"].FVC.median(), color=palette_ro[1], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Female\\nMed: {:,.0f}\".format(train[train[\"Sex\"]==\"Female\"].FVC.median()), xy=(train[train[\"Sex\"]==\"Female\"].FVC.median(), 0.0008), \n             xytext=(train[train[\"Sex\"]==\"Female\"].FVC.median()+100, 0.00085),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\n\nsns.distplot(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC, label=\"Never smoked\", ax=ax2, hist=False, color=palette_ro[4])\nax2.axvline(x=train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), color=palette_ro[4], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Never smoked\\nMed: {:.0f}\".format(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()), xy=(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), 0.0005), \n             xytext=(train[train[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()-1000, 0.00055),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\nsns.distplot(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC, label=\"Ex-smoker\", ax=ax2, hist=False, color=palette_ro[2])\nax2.axvline(x=train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), color=palette_ro[2], linestyle=\"--\", alpha=0.75)\nax2.annotate(\"Ex-smoker\\nMed: {:.0f}\".format(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()), xy=(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), 0.00058), \n             xytext=(train[train[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()-1200, 0.0007),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.25\"))\nsns.distplot(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC, label=\"Currently smokes\", ax=ax2, hist=False, color=palette_ro[0])\nax2.axvline(x=train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Currently smokes\\nMed: {:.0f}\".format(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()), xy=(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), 0.0009), \n             xytext=(train[train[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()+400, 0.00095),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax1.set_title(\"Relationship between FVC and Sex\", fontsize=16)\nax2.set_title(\"Relationship between FVC and SmokingStatus\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that `FVC` of `Female` tends to be much lower than of `Male`, and there is also a difference in FVC by `SmokingStatus`, but this may be because there are more `Female` in `Never smoked`. Let's check it out.<br>\n<font color=\"RoyalBlue\">ç”·æ€§ã¨æ¯”ã¹ã‚‹ã¨å¥³æ€§ã® FVC ã¯ã‹ãªã‚Šä½ããªã‚‹å‚¾å‘ã«ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚Šã¾ã™ã€‚å–«ç…™çŠ¶æ…‹ã«ã‚ˆã£ã¦ã‚‚ FVC ã«å·®ãŒå‡ºã¦ã„ã¾ã™ãŒã€ã“ã‚Œã¯å–«ç…™æœªçµŒé¨“è€…ã«å¥³æ€§ãŒå¤šã„ãŸã‚ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚å®Ÿéš›ã«ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_m = train[train[\"Sex\"]==\"Male\"].reset_index(drop=True)\ntrain_f = train[train[\"Sex\"]==\"Female\"].reset_index(drop=True)\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.distplot(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC, label=\"Never smoked\", ax=ax1, hist=False, color=palette_ro[4])\nax1.axvline(x=train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), color=palette_ro[4], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Never smoked\\nMed: {:.0f}\".format(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()), xy=(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), 0.0005), \n             xytext=(train_m[train_m[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()-1400, 0.0006),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\nsns.distplot(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC, label=\"Ex-smoker\", ax=ax1, hist=False, color=palette_ro[2])\nax1.axvline(x=train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), color=palette_ro[2], linestyle=\"--\", alpha=0.75)\nax1.annotate(\"Ex-smoker\\nMed: {:.0f}\".format(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()), xy=(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), 0.00063), \n             xytext=(train_m[train_m[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()-1400, 0.00045),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\nsns.distplot(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC, label=\"Currently smokes\", ax=ax1, hist=False, color=palette_ro[0])\nax1.axvline(x=train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax1.annotate(\"Currently smokes\\nMed: {:.0f}\".format(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()), xy=(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), 0.00066), \n             xytext=(train_m[train_m[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()+400, 0.00055),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nsns.distplot(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC, label=\"Never smoked\", ax=ax2, hist=False, color=palette_ro[4])\nax2.axvline(x=train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), color=palette_ro[4], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Never smoked\\nMed: {:.0f}\".format(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()), xy=(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median(), 0.001), \n             xytext=(train_f[train_f[\"SmokingStatus\"]==\"Never smoked\"].FVC.median()-600, 0.0015),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.2\"))\nsns.distplot(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC, label=\"Ex-smoker\", ax=ax2, hist=False, color=palette_ro[2])\nax2.axvline(x=train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), color=palette_ro[2], linestyle=\"--\", alpha=0.75)\nax2.annotate(\"Ex-smoker\\nMed: {:.0f}\".format(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()), xy=(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median(), 0.0013), \n             xytext=(train_f[train_f[\"SmokingStatus\"]==\"Ex-smoker\"].FVC.median()+100, 0.0018),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=-0.25\"))\nsns.distplot(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC, label=\"Currently smokes\", ax=ax2, hist=False, color=palette_ro[0])\nax2.axvline(x=train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), color=palette_ro[0], linestyle=\"--\", alpha=0.5)\nax2.annotate(\"Currently smokes\\nMed: {:.0f}\".format(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()), xy=(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median(), 0.0035), \n             xytext=(train_f[train_f[\"SmokingStatus\"]==\"Currently smokes\"].FVC.median()+200, 0.004),\n             bbox=dict(boxstyle=\"round\", fc=\"none\", ec=\"gray\"),\n             arrowprops=dict(arrowstyle=\"->\",\n                           connectionstyle=\"arc3, rad=0.25\"))\n\nax1.set_title(\"Relationship between FVC and SmokingStatus in Male\", fontsize=16)\nax2.set_title(\"Relationship between FVC and SmokingStatus in Female\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When limited to `Male`, `FVC` does not seem to change much with `SmokingStatus`. In the case of `Female`, there is a difference, but this is probably due to the small sample size (especially for `Currently smokes`). It may also be important to consider that patients who are `Currently smokes` are likely to be less severely affected.<br>\n<font color=\"RoyalBlue\">ç”·æ€§ã«é™å®šã—ã¦ã¿ã‚‹ã¨ã€FVC ã¯å–«ç…™çŠ¶æ…‹ã«ã‚ˆã£ã¦ã¯ã‚ã¾ã‚Šå¤‰åŒ–ã—ãªã„ã‚ˆã†ã§ã™ã€‚å¥³æ€§ã®å ´åˆã¯å·®ãŒå‡ºã¦ã„ã¾ã™ãŒã€ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ãŸã‚ã§ã—ã‚‡ã†ï¼ˆç‰¹ã«ç¾å–«ç…™è€…ï¼‰ã€‚ç¾å–«ç…™è€…ã®æ‚£è€…ã«ã¯é‡ç—‡è€…ãŒå°‘ãªã„ã§ã‚ã‚ã†ã“ã¨ã‚‚è€ƒæ…®ã™ã¹ãã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax1,\n                palette=[palette_ro[5], palette_ro[1]], hue=train[\"Sex\"], style=train[\"Sex\"])\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax2,\n                palette=[palette_ro[2], palette_ro[4], palette_ro[0]], hue=train[\"SmokingStatus\"], style=train[\"SmokingStatus\"])\n\nfig.suptitle(\"Correlation between FVC and Age (Pearson Corr: {:.4f})\".format(train[\"FVC\"].corr(train[\"Age\"])), fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There was almost no correlation between `FVC` and `Age`.<br>\n<font color=\"RoyalBlue\">FVC ã¨å¹´é½¢ã«ã¯ç›¸é–¢ã¯ã»ã¼è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Weeks\"], ax=ax1,\n                palette=[palette_ro[5], palette_ro[1]], hue=train[\"Sex\"], style=train[\"Sex\"])\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Weeks\"], ax=ax2,\n                palette=[palette_ro[2], palette_ro[4], palette_ro[0]], hue=train[\"SmokingStatus\"], style=train[\"SmokingStatus\"])\n\nfig.suptitle(\"Correlation between FVC and Weeks (Pearson Corr: {:.4f})\".format(train[\"FVC\"].corr(train[\"Weeks\"])), fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There was almost no correlation between `FVC` and `Weeks` either.<br>\n<font color=\"RoyalBlue\">FVC ã¨é€±æ•°ã«ã‚‚ç›¸é–¢ã¯ã»ã¼è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚</font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax1,\n                palette=[palette_ro[5], palette_ro[1]], hue=train[\"Sex\"], style=train[\"Sex\"])\nsns.scatterplot(x=train[\"FVC\"], y=train[\"Age\"], ax=ax2,\n                palette=[palette_ro[2], palette_ro[4], palette_ro[0]], hue=train[\"SmokingStatus\"], style=train[\"SmokingStatus\"])\n\nfig.suptitle(\"Correlation between FVC and Age (Pearson Corr: {:.4f})\".format(train[\"FVC\"].corr(train[\"Age\"])), fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There was a positive correlation between `FVC` and `Percent`, which is not surprising since `Percent` is a value calculated from `FVC` and other data.<br>\n<font color=\"RoyalBlue\">FVC ã¨ Percent ã«ã¯æ­£ã®ç›¸é–¢ãŒè¦‹ã‚‰ã‚Œã¾ã—ãŸã€‚Percent ã¯ FVC ãªã©ã‹ã‚‰ç®—å‡ºã™ã‚‹å€¤ãªã®ã§å½“ç„¶ã¨ã„ãˆã°å½“ç„¶ã§ã™ã€‚</font>\n\n<a id=\"efficient\"></a>\n# Linear Decay (based on EfficientNets) ğŸ“·\nFirst of all, let's try to make predictions from DICOM and other data. We make a function to put `Age`, `Sex`, and `SmokingStatus` into NumPy array.<br>\n<font color=\"RoyalBlue\">ãã‚Œã§ã¯ã€ã¾ãšã¯ DICOM ã‚’å«ã‚ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äºˆæ¸¬ã‚’å‡ºã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚Age, Sex, SmokingStatus ã‚’å¤‰æ›ã—ã¤ã¤ NumPy array ã«ã¾ã¨ã‚ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tab(df):\n    vector = [(df.Age.values[0] - 30) / 30]\n    \n    if df.Sex.values[0] == 'male':\n       vector.append(0)\n    else:\n       vector.append(1)\n    \n    if df.SmokingStatus.values[0] == 'Never smoked':\n        vector.extend([0,0])\n    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n        vector.extend([1,1])\n    elif df.SmokingStatus.values[0] == 'Currently smokes':\n        vector.extend([0,1])\n    else:\n        vector.extend([1,0])\n    return np.array(vector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we organize the data by Patient's unique ID.<br>\nIn the for statement, we extract `FVC` from `fvc` and `Weeks` from `weeks` in a NumPy array format, respectively.\nWe join weeks and the array with all 1's in the vertical direction and transpose them, and then assign them to `c`.\nThen find the least-squares of `c` and `fvc`, and add the slope to `A`.\nWe then add the value obtained from the above function `get_tab` to `TAB` and the unique ID of Patient to `p`.<br>\n<font color=\"RoyalBlue\">æ¬¡ã«ã€Patient ã®å›ºæœ‰ ID ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¾ã™ã€‚<br>\nfor æ–‡ã®ä¸­ã§ fvc ã« FVC, weeks ã« Weeks ã‚’ãã‚Œãã‚Œ NumPy array å½¢å¼ã§å–ã‚Šå‡ºã—ã¾ã™ã€‚ã€‚<br>\nweeks ã¨è¦ç´ ãŒå…¨ã¦1ã®é…åˆ—ã‚’ç¸¦æ–¹å‘ã«çµåˆã—ã€è»¢ç½®ã—ãŸã‚‚ã®ã‚’ c ã¨ã—ã¾ã™ã€‚<br>\nãã—ã¦ c ã¨ fvc ã®æœ€å°äºŒä¹—è§£ã‚’æ±‚ã‚ã€ãã®å‚¾ãã‚’ A ã«è¿½åŠ ã—ã¾ã™ã€‚<br>\nã•ã‚‰ã«ä¸Šè¨˜ã®ä¸Šè¨˜ã®é–¢æ•° get_tab ã§å–å¾—ã—ãŸå€¤ã‚’ TAB ã«ã€Patient ã®å›ºæœ‰ ID ã‚’ p ã«è¿½åŠ ã—ã¦ã„ãã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"A = {} \nTAB = {} \nP = [] \nfor i, p in tqdm(enumerate(train.Patient.unique())):\n    sub = train.loc[train.Patient == p, :] \n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    c = np.vstack([weeks, np.ones(len(weeks))]).T\n    a, b = np.linalg.lstsq(c, fvc)[0]\n    \n    A[p] = a\n    TAB[p] = get_tab(sub)\n    P.append(p)\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 6))\nsns.distplot(list(A.values()), ax=ax, color=palette_ro[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a function to read a DICOM file from the given path.\nFor ease of use in deep learning we will divide the value by 2048 and then crop and resize the image.<br>\n<font color=\"RoyalBlue\">æ¸¡ã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ DICOM ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚<br>\nãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã§æ‰±ã„ã‚„ã™ãã™ã‚‹ãŸã‚ã«å€¤ã‚’ 2**11 ã§å‰²ã‚Šã€ã•ã‚‰ã«ç”»åƒã‚’ã‚¯ãƒ­ãƒƒãƒ—ãƒ»ãƒªã‚µã‚¤ã‚ºã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_img(path):\n#     d = pydicom.dcmread(path)\n#     return cv2.resize(d.pixel_array / 2**11, (528, 528))    # changed from 512\n\n# https://www.kaggle.com/allunia/pulmonary-dicom-preprocessing\ndef get_img(path, new_shape=(528, 528)):\n    d = pydicom.dcmread(path)\n    scan = d.pixel_array / 2**11\n    \n    left = int((scan.shape[0]-512)/2)\n    right = int((scan.shape[0]+512)/2)\n    top = int((scan.shape[1]-512)/2)\n    bottom = int((scan.shape[1]+512)/2)\n    \n    img = scan[top:bottom, left:right]\n    cropped_resized_scan = cv2.resize(img, new_shape, interpolation=cv2.INTER_LANCZOS4)\n    return cropped_resized_scan\n\n# get_img(\"../input/osic-pulmonary-fibrosis-progression/train/ID00007637202177411956430/1.dcm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class IGenerator(Sequence):\n    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n    def __init__(self, keys, a, tab, batch_size=32):\n        self.keys = [k for k in keys if k not in self.BAD_ID]\n        self.a = a\n        self.tab = tab\n        self.batch_size = batch_size\n        \n        self.train_data = {}\n        for p in train.Patient.values:\n            self.train_data[p] = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/')\n    \n    def __len__(self):\n        return 1000\n    \n    def __getitem__(self, idx):\n        x = []\n        a, tab = [], [] \n        keys = np.random.choice(self.keys, size = self.batch_size)\n        for k in keys:\n            try:\n                i = np.random.choice(self.train_data[k], size=1)[0]\n                img = get_img(f'../input/osic-pulmonary-fibrosis-progression/train/{k}/{i}')\n                x.append(img)\n                a.append(self.a[k])\n                tab.append(self.tab[k])\n            except:\n                print(k, i)\n       \n        x,a,tab = np.array(x), np.array(a), np.array(tab)\n        x = np.expand_dims(x, axis=-1)\n        return [x, tab] , a","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a model.\nWe take image data and run the tensor through `EfficientNetB6` and `GlobalAveragePooling2D`, and we take the pre-processed CSV data and add Gaussian noise to the tensor and concatenate them together to output the model.\nThe model weights are trained.<br>\n<font color=\"RoyalBlue\">ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦ã„ãã¾ã™ã€‚<br>\nç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã£ã¦ EfficientNetB6 ã¨ GlobalAveragePooling2D ã«é€šã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã¨ã€å‰å‡¦ç†æ¸ˆã¿ã® CSV ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã£ã¦ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã‚’åŠ ãˆãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’é€£çµã—ã¦å‡ºåŠ›ã—ã¾ã™ã€‚<br>\nãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯è¨“ç·´æ¸ˆã¿ã®ã‚‚ã®ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef get_efficientnet(model, shape):\n    models_dict = {\n        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n    }\n    return models_dict[model]\n\ndef build_model(shape=(528, 528, 1), model_class=None):    # changed from 512\n    inp = L.Input(shape=shape)\n    base = get_efficientnet(model_class, shape)\n    x = base(inp)\n    x = L.GlobalAveragePooling2D()(x)\n    inp2 = L.Input(shape=(4,))\n    x2 = L.GaussianNoise(0.2)(inp2)\n    x = L.Concatenate()([x, x2]) \n    x = L.Dropout(0.32)(x)    # changed from 0.4\n    x = L.Dense(1)(x)\n    model = Model([inp, inp2] , x)\n    \n    weights = [w for w in os.listdir('../input/osic-model-weights') if model_class in w][0]\n    model.load_weights('../input/osic-model-weights/' + weights)\n    return model\n\nmodel_classes = [\"b6\"] #['b0','b1','b2','b3',b4','b5','b6','b7']    # changed from b5\nmodels = [build_model(model_class=m, shape=(528, 528, 1)) for m in model_classes]    # changed from 512\nprint('Number of models: ' + str(len(models)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(models[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a modified version of the Laplace Log Likelihood function, which is the evaluation function for this competition.<br>\n<font color=\"RoyalBlue\">ã“ã®ã‚³ãƒ³ãƒšã§ã®è©•ä¾¡é–¢æ•°ã§ã‚ã‚‹ä¿®æ­£ç‰ˆãƒ©ãƒ—ãƒ©ã‚¹å¯¾æ•°å°¤åº¦ã®é–¢æ•°ã‚’ä½œæˆã—ã¦ãŠãã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def score(fvc_true, fvc_pred, sigma):\n    sigma_clip = np.maximum(sigma, 70)\n    delta = np.abs(fvc_true - fvc_pred)\n    delta = np.minimum(delta, 1000)\n    sq2 = np.sqrt(2)\n    metric = (delta / sigma_clip)*sq2 + np.log(sigma_clip * sq2)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we will use the model to make predictions.<br>\n<font color=\"RoyalBlue\">ãã‚Œã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_p, vl_p = train_test_split(P, \n                              shuffle=True, \n                              train_size=0.8)\n\nsubs = []\nfor model in models:\n    metric = []\n    for q in tqdm(range(1, 10)):\n        m = []\n        for p in vl_p:\n            x = [] \n            tab = [] \n\n            if p in ['ID00011637202177653955184', 'ID00052637202186188008618']:\n                continue\n\n            ldir = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/')\n            for i in ldir:\n                if int(i[:-4]) / len(ldir) < 0.8 and int(i[:-4]) / len(ldir) > 0.15:\n                    x.append(get_img(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/{i}')) \n                    tab.append(get_tab(train.loc[train.Patient == p, :])) \n            if len(x) < 1:\n                continue\n            tab = np.array(tab) \n\n            x = np.expand_dims(x, axis=-1) \n            _a = model.predict([x, tab]) \n            a = np.quantile(_a, q / 10)\n\n            percent_true = train.Percent.values[train.Patient == p]\n            fvc_true = train.FVC.values[train.Patient == p]\n            weeks_true = train.Weeks.values[train.Patient == p]\n\n            fvc = a * (weeks_true - weeks_true[0]) + fvc_true[0]\n            percent = percent_true[0] - a * abs(weeks_true - weeks_true[0])\n            m.append(score(fvc_true, fvc, percent))\n        print(np.mean(m))\n        metric.append(np.mean(m))\n\n    q = (np.argmin(metric) + 1)/ 10\n\n    sub = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/sample_submission.csv') \n    test = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv') \n    A_test, B_test, P_test, W, FVC = {}, {}, {}, {}, {} \n    STD, WEEK = {}, {} \n    for p in test.Patient.unique():\n        x = [] \n        tab = [] \n        ldir = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/test/{p}/')\n        for i in ldir:\n            if int(i[:-4]) / len(ldir) < 0.8 and int(i[:-4]) / len(ldir) > 0.15:\n                x.append(get_img(f'../input/osic-pulmonary-fibrosis-progression/test/{p}/{i}')) \n                tab.append(get_tab(test.loc[test.Patient == p, :])) \n        if len(x) <= 1:\n            continue\n        tab = np.array(tab) \n\n        x = np.expand_dims(x, axis=-1) \n        _a = model.predict([x, tab]) \n        a = np.quantile(_a, q)\n        A_test[p] = a\n        B_test[p] = test.FVC.values[test.Patient == p] - a*test.Weeks.values[test.Patient == p]\n        P_test[p] = test.Percent.values[test.Patient == p] \n        WEEK[p] = test.Weeks.values[test.Patient == p]\n\n    for k in sub.Patient_Week.values:\n        p, w = k.split('_')\n        w = int(w) \n\n        fvc = A_test[p] * w + B_test[p]\n        sub.loc[sub.Patient_Week == k, \"FVC\"] = fvc\n        sub.loc[sub.Patient_Week == k, \"Confidence\"] = (\n            P_test[p] - A_test[p] * abs(WEEK[p] - w) \n    ) \n\n    _sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()\n    subs.append(_sub)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = len(subs)\nsub = subs[0].copy() # ref\nsub[\"FVC\"] = 0\nsub[\"Confidence\"] = 0\nfor i in range(N):\n    sub[\"FVC\"] += subs[0][\"FVC\"] * (1/N)\n    sub[\"Confidence\"] += subs[0][\"Confidence\"] * (1/N)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub[[\"Patient_Week\", \"FVC\", \"Confidence\"]].to_csv(\"submission_img.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Decay predictions have been made!<br>\n<font color=\"RoyalBlue\">Linear Decay ã§ã®äºˆæ¸¬ãŒä½œæˆã§ãã¾ã—ãŸï¼</font>\n\n<a id=\"quantile\"></a>\n# Multiple Quantile Regression ğŸŒ’\n<a id=\"quantile_d\"></a>\n## Data preprocessing for Multiple Quantile Regression ğŸ§¹\n\nNext, let's pre-process the data for multiple quantile regression. First, check the format of the `sub` (sample_submission.csv).<br>\n<font color=\"RoyalBlue\">æ¬¡ã«ã€é‡åˆ†ä½ç‚¹å›å¸°ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’è¡Œã£ã¦ã„ãã¾ã—ã‚‡ã†ã€‚ã¾ãšã€sub (sample_submission.csv) ã®å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(ROOT + \"sample_submission.csv\")\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split `Patient_Week` in `sub` into `Patient` and `Weeks`, according to the `train` and `test` formats. Then, attach the `Patient` to `Patient` in `sub` and merge it with the `Patient`. This makes it easier to handle the prediction.<br>\n<font color=\"RoyalBlue\">sub ã® Patient_Week ã‚’ Patient ã¨ Weeks ã«åˆ†å‰²ã—ã€train ã‚„ test ã®å½¢å¼ã«åˆã‚ã›ã¾ã™ã€‚ãã—ã¦ã€sub ã« test ã‚’ Patient ã«ç´ã¥ã‘ã¦çµåˆã—ã¾ã™ã€‚ã“ã†ã™ã‚Œã°ã€äºˆæ¸¬æ™‚ã«ç°¡å˜ã«å‡¦ç†ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient', 'Weeks', 'Confidence', 'Patient_Week']]\nsub = sub.merge(test.drop('Weeks', axis=1), on=\"Patient\")\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add `Where` column to all the dataframes.<br>\n<font color=\"RoyalBlue\">ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã« Where åˆ—ã‚’è¿½åŠ ã—ã¾ã™ã€‚</font><br>\nThen, in order to process the `train`, `test` and `sub` at the same time, these three are concatenated vertically into `data`.<br>\n<font color=\"RoyalBlue\">ãã—ã¦ã€train, test, sub ã‚’åŒæ™‚ã«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã™ã‚‹ãŸã‚ã«ã€ç¸¦æ–¹å‘ã«çµåˆã—ã¦ data ã¨ã—ã¦ãŠãã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['WHERE'] = 'train'\ntest['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = train.append([test, sub])\n\nprint(train.shape, test.shape, sub.shape, data.shape)\nprint(train.Patient.nunique(), test.Patient.nunique(), sub.Patient.nunique(), data.Patient.nunique())\n\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Add a `min_week` column for the minimum number of weeks per Patient.<br>\n<font color=\"RoyalBlue\">Patient ã”ã¨ã®æœ€å°ã®é€±æ•°ã‚’ç¤ºã™ min_week åˆ—ã‚’è¿½åŠ ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test', 'min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')\n\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here, calculate `base_FVC` (= the `FVC` of `Patient` at `min_week`) and the `base_week` (= how many weeks have passed since `min_week`).<br>\n<font color=\"RoyalBlue\">ã“ã“ã‹ã‚‰ã€base_FVCï¼ˆï¼Patient ã® min_week æ™‚ã® FVCï¼‰ã¨ base_weekï¼ˆï¼min_week ã‹ã‚‰ä½•é€±çµŒã£ãŸã¨ãã®ãƒ‡ãƒ¼ã‚¿ã‹ï¼‰ã‚’ç®—å‡ºã—ã¦ã„ãã¾ã™ã€‚</font><br>\n\n\nFirst, extract the rows in the data where `Weeks` is `min_week` and set them to `base`.\nExtract only the `Patient` and `FVC` columns from the `base`, and change the column name from `FVC` to `base_FVC`.\nThen create a new `nb` column and set all the values to 1.\nGroup the `base` with `Patient` and compute the cumulative sum with the `nb` column.\nExtract only the rows from `base` that have `nb` columns of 1, and replace `base`.\nThis allows us to eliminate duplicate `Patient` rows from the `base` dataframe with `base_FVC` in it.\nLet's remove the `nb` column.<br>\n<font color=\"RoyalBlue\">ã¾ãšã€data ã® Weeks ãŒ min_week ã§ã‚ã‚‹è¡Œã‚’æŠ½å‡ºã—ã€base ã¨ã—ã¾ã™ã€‚<br>\nbase ã‹ã‚‰ Patient, FVC åˆ—ã ã‘ã‚’æŠœãå‡ºã—ã¾ã™ã€‚<br>\nåˆ—åã‚’ FVC ã‹ã‚‰ base_FVC ã«å¤‰æ›´ã—ã¾ã™ã€‚<br>\nãã—ã¦æ–°ãŸã« nb åˆ—ã‚’ä½œã‚Šã€å€¤ã‚’ã™ã¹ã¦1ã¨ã—ã¾ã™ã€‚<br>\nbase ã‚’ Patient ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€nb åˆ—ã‚’æŒ‡å®šã—ã¦ç´¯ç©å’Œã‚’è¨ˆç®—ã—ã¾ã™ã€‚<br>\nbase ã‹ã‚‰ nb åˆ—ãŒ1ã®è¡Œã®ã¿ã‚’æŠ½å‡ºã—ã€base ã‚’ç½®ãæ›ãˆã¾ã™ã€‚<br>\nã“ã‚Œã«ã‚ˆã‚Šã€base_FVC ãŒè¼‰ã£ã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  base ã‹ã‚‰ Patient ã®é‡è¤‡ã‚’ç„¡ãã™ã“ã¨ãŒã§ãã¾ã™ã€‚<br>\nnb åˆ—ã¯å‰Šé™¤ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"base = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient', 'FVC']].copy()\nbase.columns = ['Patient', 'base_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)\n\nbase.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we associate `base` with `Patient` in `data`.\nWe will create `base_week` column in `data`, which will be `Weeks` minus `min_week`.\nThis will add the `base_FVC` and `base_week` columns to `data`.\nWe should remove the `base` column.<br>\n<font color=\"RoyalBlue\">æ¬¡ã«ã€data ã« base ã‚’ Patient ã«ç´ã¥ã‘ã¦çµåˆã—ã¾ã™ã€‚<br>\ndata ã« base_week åˆ—ã‚’ä½œæˆã—ã€Weeks ã‹ã‚‰ min_week ã‚’å¼•ã„ãŸå€¤ã¨ã—ã¾ã™ã€‚<br>\nã“ã‚Œã§ã€data ã« base_FVC åˆ—ã¨ base_week åˆ—ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚<br>\nbase ã¯å‰Šé™¤ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base\n\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform one-hot-encoding of `Sex` and `SmokingStatus`.<br>\n<font color=\"RoyalBlue\">Sex ã¨ SmokingStatus ã®ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['Sex', 'SmokingStatus']\nfeatures_nn = []\nfor col in categorical_features:\n    for mod in data[col].unique():\n        features_nn.append(mod)\n        data[mod] = (data[col] == mod).astype(int)\n\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalize `Percent`, `Age`, `base_FVC`, and `base_week`.<br>\n<font color=\"RoyalBlue\">Percent, Age, base_FVC, and base_week ã®æ­£è¦åŒ–ã‚’è¡Œã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Percent_n'] = (data['Percent'] - data['Percent'].min() ) / ( data['Percent'].max() - data['Percent'].min() )\ndata['Age_n'] = (data['Age'] - data['Age'].min() ) / ( data['Age'].max() - data['Age'].min() )\ndata['base_FVC_n'] = (data['base_FVC'] - data['base_FVC'].min() ) / ( data['base_FVC'].max() - data['base_FVC'].min() )\ndata['base_week_n'] = (data['base_week'] - data['base_week'].min() ) / ( data['base_week'].max() - data['base_week'].min() )\nfeatures_nn += ['Age_n', 'Percent_n', 'base_week_n', 'base_FVC_n']\n\nprint(features_nn)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the process is done, let's split data into `train`, `test` and `sub` using the `WHERE` column, and remove `data`.<br>\n<font color=\"RoyalBlue\">å‡¦ç†ãŒçµ‚ã‚ã£ãŸã®ã§ã€WHERE åˆ—ã‚’ä½¿ã£ã¦ data ã‚’ train, test, sub ã«åˆ†å‰²ã—ç›´ã—ã¾ã—ã‚‡ã†ã€‚data ã¯å‰Šé™¤ã—ã¦ãŠãã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data.loc[data.WHERE=='train']\ntest = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data\n\ntrain.shape, test.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"quantile_m\"></a>\n## Build the model ğŸ§ \n\nThis competition is evaluated on a modified version of the Laplace Log Likelihood. For each true FVC measurement, you will predict both an FVC and a confidence measure (standard deviation Ïƒ). The metric is computed as:<br>\n<font color=\"RoyalBlue\">ã“ã®ã‚³ãƒ³ãƒšã§ã¯ã€ãƒ©ãƒ—ãƒ©ã‚¹å¯¾æ•°å°¤åº¦ã®ä¿®æ­£ç‰ˆã§è©•ä¾¡ã•ã‚Œã¾ã™ã€‚çœŸã®å„ FVC æ¸¬å®šã«ã¤ã„ã¦ã€FVC ã¨ä¿¡é ¼åº¦ï¼ˆæ¨™æº–åå·® Ïƒï¼‰ã®ä¸¡æ–¹ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ãƒ¡ãƒˆãƒªãƒƒã‚¯ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚</font><br><br>\n\n$\\large \\sigma_{clipped} = max(\\sigma, 70),$<br>\n$\\large \\Delta = min ( |FVC_{true} - FVC_{predicted}|, 1000 ),$<br>\n$\\Large metric = -   \\frac{\\sqrt{2} \\Delta}{\\sigma_{clipped}} - \\ln ( \\sqrt{2} \\sigma_{clipped} ).$<br>\n\nIn the following code, C1 is the value of confidence clipping in the modified Laplace Log Likelihood, an evaluation metric, and C2 is the error threshold.<br>\n<font color=\"RoyalBlue\">ä¸‹è¨˜ã®ã‚³ãƒ¼ãƒ‰ã«ãŠã„ã¦ã€C1 ã¯è©•ä¾¡æŒ‡æ¨™ã§ã‚ã‚‹ä¿®æ­£ç‰ˆãƒ©ãƒ—ãƒ©ã‚¹å¯¾æ•°å°¤åº¦ã«ãŠã‘ã‚‹ä¿¡é ¼åº¦ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®å€¤ã€C2 ã¯èª¤å·®ã®é–¾å€¤ã§ã™ã€‚</font><br>\n\nThe `score` function takes the true and predicted values of the target variable and returns a score based on the modified Laplace Log Likelihood.\nThe `qloss` function is a pinball loss function, which is the loss function used when a multiple-quantile regression prediction is trained.\nThe `mloss` function takes a percentage and returns a function that sums the return values of the `score` function and the `qloss` function according to the percentage.<br>\n<font color=\"RoyalBlue\">score é–¢æ•°ã¯ã“ã®è©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ã§ã™ã€‚ã‚³ãƒ³ãƒšã®ç›®çš„å¤‰æ•°ã®çœŸã®å€¤ã¨äºˆæ¸¬å€¤ã‚’å—ã‘å–ã‚Šã€ä¿®æ­£ç‰ˆãƒ©ãƒ—ãƒ©ã‚¹å¯¾æ•°å°¤åº¦ã«åŸºã¥ã„ãŸã‚¹ã‚³ã‚¢ã‚’è¿”ã—ã¾ã™ã€‚<br>\nqloss é–¢æ•°ã¯ã€é‡åˆ†ä½ç‚¹å›å¸°äºˆæ¸¬ãŒå­¦ç¿’ã™ã‚‹ã¨ãã«ä½¿ç”¨ã™ã‚‹æå¤±é–¢æ•°ã§ã‚ã‚‹ãƒ”ãƒ³ãƒœãƒ¼ãƒ«ãƒ­ã‚¹é–¢æ•°ã§ã™ã€‚<br>\nmloss é–¢æ•°ã¯å‰²åˆ _lambda ã‚’å—ã‘å–ã‚Šã€ãã®å‰²åˆã«å¿œã˜ã¦ score é–¢æ•°ã¨ qloss é–¢æ•°ã®æˆ»ã‚Šå€¤ã‚’åˆè¨ˆã™ã‚‹é–¢æ•°ã‚’è¿”ã—ã¾ã™ã€‚</font><br><br>\nHere, we define confidence as the difference between the predicted values at 0.2 and 0.8 quartiles.<br>\n<font color=\"RoyalBlue\">ã“ã“ã§ã€ä¿¡é ¼åº¦ã‚’0.2åˆ†ä½ç‚¹ã¨0.8åˆ†ä½ç‚¹ã«ãŠã‘ã‚‹äºˆæ¸¬å€¤ã®å·®ã¨ã—ã¦å®šç¾©ã—ã¦ã„ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"C1, C2 = tf.constant(70, dtype=\"float32\"), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip * sq2)\n    return backend.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.5, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return backend.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model():\n    inp = L.Input(len(features_nn), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(inp)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(inp, preds, name=\"NeuralNet\")\n    model.compile(loss=mloss(0.64),    # changed from 0.8\n                  optimizer=tf.keras.optimizers.Adam(lr=0.1, decay=0.01),\n                  metrics=[score])\n    return model\n\nmodel = make_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"quantile_c\"></a>\n## Cross validation ğŸ’­\n\nUse the model we created to cross-validate.<br>\n<font color=\"RoyalBlue\">ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train[features_nn].values\nX_test = sub[features_nn].values\n\ny_train = train['FVC'].values\n\noof_train = np.zeros((X_train.shape[0], 3))\ny_preds = np.zeros((X_test.shape[0], 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 804    # changed from 800\nNFOLD = 5\n\nkf = KFold(n_splits=NFOLD)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor fold_id, (tr_idx, va_idx) in enumerate(kf.split(X_train)):\n    print(f\"FOLD {fold_id+1}\")\n    model = make_model()\n    model.fit(X_train[tr_idx], y_train[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n              validation_data=(X_train[va_idx], y_train[va_idx]), verbose=0)\n    print(\"train\", model.evaluate(X_train[tr_idx], y_train[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", model.evaluate(X_train[va_idx], y_train[va_idx], verbose=0, batch_size=BATCH_SIZE))\n    oof_train[va_idx] = model.predict(X_train[va_idx], batch_size=BATCH_SIZE, verbose=0)\n    y_preds += model.predict(X_test, batch_size=BATCH_SIZE, verbose=0) / NFOLD","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's illustrate the correct and predicted values.<br>\n<font color=\"RoyalBlue\">æ­£è§£å€¤ã¨äºˆæ¸¬å€¤ã‚’å›³ç¤ºã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 12))\n\nidxs = np.random.randint(0, y_train.shape[0], 100)\nax.plot(y_train[idxs], label=\"ground truth\", color=palette_ro[0])\nax.plot(oof_train[idxs, 0], label=\"q20\", color=palette_ro[3], ls=':', alpha=0.5)\nax.plot(oof_train[idxs, 1], label=\"q50\", color=palette_ro[4], ls=':', alpha=0.5)\nax.plot(oof_train[idxs, 2], label=\"q80\", color=palette_ro[5], ls=':', alpha=0.5)\nax.legend(loc=\"best\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We calculate the optimized ğœ (standard deviation) from the `oof_train`. `sigma_opt` is the mean absolute error between the correct value of each fold and the prediction (median), `sigma_unc` is the difference between the prediction (0.2 quantile) and the prediction (0.8 quantile), and `sigma_mean` is the mean value of the difference.<br>\n<font color=\"RoyalBlue\">ã§ã¯ã€oof_train ã‹ã‚‰æœ€é©åŒ–ã•ã‚ŒãŸ ğœï¼ˆæ¨™æº–åå·®ï¼‰ã‚’è¨ˆç®—ã—ã¾ã—ã‚‡ã†ã€‚å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®æ­£è§£å€¤ã¨äºˆæ¸¬å€¤ï¼ˆä¸­å¤®å€¤ï¼‰ã¨ã®å¹³å‡çµ¶å¯¾èª¤å·®ã‚’ sigma_opt, äºˆæ¸¬å€¤ï¼ˆ0.2åˆ†ä½æ•°ï¼‰ã¨äºˆæ¸¬å€¤ï¼ˆ0.8åˆ†ä½æ•°ï¼‰ã¨ã®å·®ã‚’ sigma_unc, ãã®å¹³å‡å€¤ã‚’ sigma_mean ã¨ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sigma_opt = mean_absolute_error(y_train, oof_train[:, 1])\nsigma_unc = oof_train[:, 2] - oof_train[:, 0]\nsigma_mean = np.mean(sigma_unc)\nprint(sigma_opt, sigma_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sigma_unc.min(), sigma_unc.mean(), sigma_unc.max(), (sigma_unc>=0).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(y_train / oof_train[:, 1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16, 6))\n\nsns.distplot(sigma_unc, ax=ax, color=palette_ro[1])\nax.set_title(\"uncertainty in prediction\", fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"submit\"></a>\n# Ensemble & Submit ğŸ“"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare a submission file from the predictions of the neural network.<br>\n<font color=\"RoyalBlue\">ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®äºˆæ¸¬çµæœã‹ã‚‰æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚‹æº–å‚™ã‚’ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['FVC1'] = y_preds[:, 1]\nsub['Confidence1'] = y_preds[:, 2] - y_preds[:, 0]\n\nsub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`subm` is defined by extracting the required columns from the `sub` and leaving only the rows with non-null data in `FVC1`.<br>\n<font color=\"RoyalBlue\">`sub` ã‹ã‚‰å¿…è¦ãªåˆ—ã‚’æŠœãå‡ºã—ã€`FVC1` ã®ãƒ‡ãƒ¼ã‚¿ãŒ `null` ã§ãªã„è¡Œã ã‘ã«ã—ãŸã‚‚ã®ã‚’ `subm` ã¨ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = sub[['Patient_Week', 'FVC', 'Confidence', 'FVC1', 'Confidence1']].copy()\nsubm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']\n\nsubm.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the original `test.csv` and overwrite the `FVC` and `Confidence` in the predicted data to be submitted if they are known in the `test.csv`.<br>\n<font color=\"RoyalBlue\">ã‚ªãƒªã‚¸ãƒŠãƒ«ã® test.csv ã‚’èª­ã¿è¾¼ã¿ã€æŠ•ç¨¿äºˆå®šã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã« test.csv ã§æ—¢çŸ¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã° FVC ã¨ Confidence ã‚’ä¸Šæ›¸ãã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"org_test = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\nfor i in range(len(org_test)):\n    subm.loc[subm['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    subm.loc[subm['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70\n\nsubm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)\nreg_sub = subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ensemble two models.<br>\n<font color=\"RoyalBlue\">ï¼’ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã¾ã™ã€‚</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = img_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\ndf2 = reg_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\n\ndf = df1[['Patient_Week']].copy()\ndf['FVC'] = 0.2*df1['FVC'] + 0.8*df2['FVC']    # changed from 0.25, 0.75\ndf['Confidence'] = 0.0*df1['Confidence'] + 1.0*df2['Confidence']    # changed from 0.26, 0.74\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could create `submission.csv`. Thank you so much for reading!<br>\n<font color=\"RoyalBlue\">submission.csv ã‚’ä½œæˆã§ãã¾ã—ãŸã€‚èª­ã‚“ã§ãã ã•ã‚Šã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}