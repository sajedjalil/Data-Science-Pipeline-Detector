{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n## Image Processing\n\nAn image is a composed of pixels. Every Pixel is assigned a value between 0 and 255 in a rows and columns. So it is easy to represent in matrix format as below.\n\n![img](https://i.imgur.com/L5F39c4.jpg?1)\n\n### Color Image\nA color image is a matrix that specifies the color of various pixels in terms of the amount of red, green and blue components.\n\nA set of one dot of each color form a pixel.\n\n![Imgur2](https://visualled.com/wp-content/uploads/2018/08/pitch.jpg)\nEvery pixel is assigned an RGB value, each components being a value between 0 and 255.\n\nExample: Red is rgb(255,0,0), yellow is rgb(241,252,23) and white is rgb(255,255,255).\n\n\n\n![Imgur1](https://api.intechopen.com/media/chapter/51312/media/fig3.png)\n\nNow will see an example of reading an image and storing in a ndarray. \n\n## OpenCV-Python\n\n* When reading a color image file, OpenCV imread() reads as a NumPy array ndarray of row (height) x column (width) x color (3). The order of color is BGR (blue, green, red).\n* On the other hand, the order of colors is assumed to be RGB (red, green, blue). \n* You can use the OpenCV function cvtColor() or simply change the order of ndarray. \n\n\n**Below image is 768 * 1024 pixels with blue, green and red components.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2\nimg = cv2.imread('../input/image-processing/pichai2.jpg',1)\nplt.imshow(img) # default value is BGR\n\nprint(\"The Dimension of the matrix is \", img.shape)\n# print(\"Matrix value of the blue color \\n\",img[:,:,0], \"\\n Y,X Pixels:\",img[:,:,0].shape) ## 0 refer blue, 1 refer green and 2 refer red\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will convert BGR to RGB component to see the actual image.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig=plt.figure(figsize=(20, 20))\nfig.add_subplot(1, 4, 1)\n#####################################################################\n### Converting BGR to RGB\nplt.imshow(img[:,:,[2,1,0]])\n## im = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # using cvtcolor function also we can change the color\n## plt.imshow(im)\n\nfig.add_subplot(1, 4, 2)\n## Select the pixels of the face\nplt.imshow(img[40:275,500:750,[2,1,0]])\n\n\nfig.add_subplot(1, 4, 3)\n## Select the pixels of the eyes\nplt.imshow(img[120:160,550:600,[2,1,0]])\n\nfig.add_subplot(1, 4, 4)\n## Select the pixels of the nose\nplt.imshow(img[140:190,600:650,[2,1,0]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"color = ('r','g','b')\n\nfor i,col in enumerate(color):\n    histogram2 = cv2.calcHist([img],[i],None,[256],[0,256])\n    plt.plot(histogram2,color = col)\n    plt.xlim([0,256])\nplt.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_features = ['left_eye_center_x', 'left_eye_center_y',\n            'right_eye_center_x','right_eye_center_y',\n            'nose_tip_x', 'nose_tip_y',\n            'mouth_center_bottom_lip_x',\n            'mouth_center_bottom_lip_y', 'Image']\n# from IPython.display import clear_output\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\ntraining = pd.read_csv('../input/facial-keypoints-detection/training.zip')\ntest = pd.read_csv('../input/facial-keypoints-detection/test.zip')\nlookid_data = pd.read_csv('../input/facial-keypoints-detection/IdLookupTable.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convolutions\n\nYou don't directly choose the numbers to go into your convolutions for deep learning... instead the deep learning technique determines what convolutions will be useful from the data (as part of model-training). We'll come back to how the model does that soon.\n\n![Imgur](https://i.imgur.com/op9Maqr.png)\n\nBut looking closely at convolutions and how they are applied to your image will improve your intuition for these models, how they work, and how to debug them when they don't work.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![img4](https://pathmind.com/images/wiki/convgaus.gif)\n![img5](https://cdn-media-1.freecodecamp.org/images/gb08-2i83P5wPzs3SL-vosNb6Iur5kb5ZH43)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Feature Maps\nFeature maps are the results we get after applying the filters.<br>\n\nThe shape of the feature map is influenced by:<br><br>\n    1) Filter/Kernals<br>\n    2) Padding<br>\n    3) Striding<br>\n    \n### Filter/Kernals:\n![filters](https://miro.medium.com/max/875/1*cfO8kMdUGG-X33TZa2h-vQ.gif)\n- The filters are the ___neurons___ of the convolutional layers. \n- They are used for ___feature detection___. \n- They are represented in the form of square matrix. \n\nInitial weights of the filters are assigned randomly, and during the training phase these weights gets updated based on backward propagations.<br>\n<br>\n\nExamples of image after we apply a filter:<br>\n![img5](https://i.imgur.com/6KrzpuC.jpg)\n\n\n    \n#### Striding\n   The amount of movement between applications of the filter to the input image is referred to as the stride, and it is almost always symmetrical in height and width dimensions.<br>\n   The default stride or strides in two dimensions is (1,1) for the height and the width movement, performed when needed. And this default works well in most cases.<br>\n<br>\nThe stride can be changed, which has an effect both on how the filter is applied to the image and, in turn, the size of the resulting feature map.\n![imgstr](https://miro.medium.com/max/588/1*BMngs93_rm2_BpJFH2mS0Q.gif)\n\n#### Padding:\n\n\n   The pixels on the edge of the input are only ever exposed to the edge of the filter. By starting the filter outside the frame of the image, it gives the pixels on the border of the image more of an opportunity for interacting with the filter, more of an opportunity for features to be detected by the filter, and in turn, an output feature map that has the same shape as the input image. This process of creating extra layers/borders in image is known as padding.<br>\n   Padding are extremely usefull when the input dimension are small and when we don't want to have any information leakage.<br>\n\n![padding](https://miro.medium.com/max/430/1*KGrCz7aav02KoGuO6znO0w.gif)","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport sys, requests, shutil, os\nfrom urllib import request, error\n\n\ndef fetch_image(image_url,name):\n    img_data = requests.get(image_url).content\n    with open(f'../input/{name}', 'wb') as handler:\n        handler.write(img_data)\n\ndef plot_image(image):\n    plt.imshow(image,cmap='gray')\n    plt.xticks([])\n    plt.yticks([])\n\n    \ndef pooling(image, kernel_shape=3):\n    #showing max_pooling\n    print (\"shape before pooling\",image.shape)\n    y, x = image.shape\n    new_image = []\n    for i in range(0,y,kernel_shape):\n        temp = []\n        for j in range(0,x,kernel_shape):\n            temp.append(np.max(image[i:i+kernel_shape, j:j+kernel_shape]))\n        new_image.append(temp)\n    new_image = np.array(new_image)\n    print (\"shape after pooling\",new_image.shape)\n    return (new_image)\n\ndef padding(image,top=1,bottom=1,left=1,right=1,values=0):\n  # Create new rows/columns in the matrix and fill those with some values\n  #return cv2.copyMakeBorder(image,top,bottom,left,right,cv2.BORDER_CONSTANT,value=values)\n    \n    x,y = image.shape\n    #print (image.shape)\n    arr = np.full((x+top+bottom,y+left+right),values,dtype=float)\n    #print(image[0])\n    #print (arr.shape)\n    #print (top,x-bottom)\n    #print (y,y-bottom)\n    arr[top:x+top,left:y+left] = image\n    #print(arr[top])\n    return arr\n\ndef convolution2d(image, kernel, bias=0,strid=1,pad_val=()):\n  #including padding,striding and convolution\n    print (\"shape before padding/striding\",image.shape)\n    if not pad_val:\n        print (pad_val)\n    image = padding(image,*pad_val)#(how many rows, columns to be padded, and of what type)\n    m, n = kernel.shape\n    y, x = image.shape\n    y = y - m + 1\n    x = x - m + 1\n    new_image = []\n    for i in range(0,y,strid):\n        temp = []\n        for j in range(0,x,strid):\n            temp.append(np.sum(image[i:i+m, j:j+m]*kernel) + bias)\n        new_image.append(temp)\n    new_image = np.array(new_image)\n    print (\"shape after padding/striding\",new_image.shape)\n    return (new_image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting pixel value into image","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below is the image pixel value represent in 96*96 matrix.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)\nimg_txt_input = training['Image'][0]\nprint(\"No of pixel values is :\",len(img_txt_input.split(' ')),\"Converting the pixel values into rows and column:\",np.sqrt(len(img_txt_input.split(' '))),\"*\",np.sqrt(len(img_txt_input.split(' '))),\"\\n\")\nfn_reshape = lambda a: np.fromstring(a, dtype=int, sep=' ').reshape(96,96)\nimg = fn_reshape(img_txt_input)\nprint(\"Below is the pixel value conveted into an image\")\nplt.imshow(img,cmap='gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Filters on image","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"samp_imag = img.copy()\nsamp_imag = samp_imag/255.\nh_kernal = np.array([[1,1,1],[0,0,0],[-1,-1,-1]])\nv_kernal = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nh_image = cv2.filter2D(samp_imag,-1, h_kernal)\nv_image = cv2.filter2D(samp_imag,-1, v_kernal)\n#Laplacian filter\nlap_filter = np.array([[0,1,0],[1,-4,1],[0,1,0]])\nlap_image = cv2.filter2D(samp_imag,-1, lap_filter)\n\nprint (\"Shapes before applying the filter:{} \".format(samp_imag.shape))\nprint (\"Shapes after applying the filter:{} \".format(lap_image.shape))\nplt.figure(figsize=(10,10))\nplt.suptitle(\"Image and its transformations after applying filters\")\nplt.subplot(221)\nplt.title(\"Actual Gray Scale Image\")\nplot_image(samp_imag)\nplt.subplot(222)\nplt.title(\"Horizontal Filter applied\")\nplot_image(h_image)\nplt.subplot(223)\nplt.title(\"Vertical Filter applied\")\nplot_image(v_image)\nplt.subplot(224)\nplt.title(\"Laplacian Filter applied\")\nplot_image(lap_image)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying padding on image","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# print (\"shape of actual image: {}\".format(samp_imag.shape))\npadded_image_5 = padding(samp_imag,*(5,5,5,5,1))\npadded_image_10 = padding(samp_imag,*(10,10,10,10,0))\npadded_bimage_10 = padding(samp_imag,*(10,10,10,10,1))\n# # print (\"shape of padded image: {}\".format(padded_image.shape))\n\nplt.figure(figsize=(10,10))\nplt.suptitle(\"Padding\")\nplt.subplot(2,2,1)\nplt.imshow(samp_imag,cmap='gray')\nplt.title(\"Actual Image\")\nplt.subplot(2,2,2)\nplt.imshow(padded_image_5,cmap='gray')\nplt.title(\"Padding 5 border with white\")\nplt.subplot(2,2,3)\nplt.imshow(padded_image_10,cmap='gray')\nplt.title(\"Padding 10 border with black\")\nplt.subplot(2,2,4)\nplt.imshow(padded_bimage_10,cmap='gray')\nplt.title(\"Padding 10 border with white\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Striding on image","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print (\"Padding used is 1 for all the borders\\nVertical filter is used\")\nfig, ax = plt.subplots(2, 2,figsize=(10,10))\nplt.suptitle(\"Affect of Stride\")\nax[0,0].set_title(\"Actual Image\")\nax[0,0].imshow(samp_imag,cmap='gray')\nrave = ax.ravel()\nfor i in range(1,4):\n    print (\"\")\n    print(f\"striding value = {i}\")\n    custom_conv = convolution2d(samp_imag,v_kernal,strid=i,pad_val=(1,1,1,1,0))\n    rave[i].set_title(f\"striding value = {i}\")\n    rave[i].imshow(custom_conv,cmap='gray')\n#print (custom_conv.shape)\n#plt.imshow(custom_conv,cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pooling Layer\nIts function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network.<br>\nPooling layer operates on each feature map independently<br>\nThe most common approach used in pooling is max pooling.<br>\n\n![pooling](https://miro.medium.com/max/875/1*jU_Mp73fXzh9_ffvtnbrDQ.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Applying MaxPooling on image","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print (\"Pooling example\")\nfig, ax = plt.subplots(2, 2,figsize=(10,10))\nplt.suptitle(\"Affect of Pooing\")\nax[0,0].set_title(\"Actual Image\")\nax[0,0].imshow(samp_imag,cmap='gray')\nrave = ax.ravel()\nfor i in range(2,5):\n    print (\"\")\n    print(f\"Pooling and striding value = {i}\")\n    custom_conv = pooling(samp_imag,i)\n    rave[i-1].set_title(f\"striding value = {i}\")\n    rave[i-1].imshow(custom_conv,cmap='gray')\n#print (custom_conv.shape)\n#plt.imshow(custom_conv,cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Facial Keypoints Detection competition\nThe goal of the competition is to locate specific keypoints on face images. Build a model that, given an image of a face, automatically locates where these keypoints are located.\n\n## Given\n1. training.csv - It contains (x,y) coordinates of 30 facial keypoints(both left and right) and pixel values of Images.\n2. test.csv - It contains pixel values of images\n3. IdLookupTable.csv - It contains required Feature Names along with ImageId for submission.\n\n**Important points:** In total, we have 7049 rows, each one with 31 columns. The first 30 columns are keypoint locations, which python correctly identified as numbers. The last one is a string representation of the image, identified as a string.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_columns = training.columns[:-1].values\ntraining.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"training[training.columns[:-1]].describe(percentiles = [0.05,0.1,.25, .5, .75,0.9,0.95]).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data and outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"whisker_width = 1.5\ntotal_rows = training.shape[0]\nmissing_col = 0\nfor col in training[training.columns[:-1]]:\n    count = training[col].count()\n    q1 = training[col].quantile(0.25)\n    q3 = training[col].quantile(0.75)\n    iqr = q3 - q1\n    outliers = training[(training[col] < q1 - whisker_width*iqr)\n                       | (training[col] > q3 + whisker_width*iqr)][col].count()\n    print (f\"dv:{col}, dv_rows:{count}, missing_pct:{round(100.*(1-count/total_rows),2)}%, outliers:{outliers}, outlier_pct:{round(100.*outliers/count,2)}%\")\n    if (100.*(1-count/total_rows)>65):\n        missing_col+=1\n\nprint(f\"DVs containing more than 65% of data missing : {missing_col} out of {len(training.columns[:-1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above analysis we can see that, around 2-5% of the data are prone to outliers.\nAnd for detailed DV such as mouth_right_corner,left_eyebrow_outer_end_y etc, >65% of the data are missing.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_loss(hist,name,plt,RMSE_TF=False):\n    '''\n    RMSE_TF: if True, then RMSE is plotted with original scale \n    '''\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    if RMSE_TF:\n        loss = np.sqrt(np.array(loss))*48 \n        val_loss = np.sqrt(np.array(val_loss))*48 \n        \n    plt.plot(loss,\"--\",linewidth=3,label=\"train:\"+name)\n    plt.plot(val_loss,linewidth=3,label=\"val:\"+name)\n\ndef plot_sample_val(X,y,axs,pred):\n    '''\n    kaggle picture is 96 by 96\n    y is rescaled to range between -1 and 1\n    '''\n    \n    axs.imshow(X.reshape(96,96),cmap=\"gray\")\n    axs.scatter(48*y[0::2]+ 48,48*y[1::2]+ 48, label='Actual')\n    axs.scatter(48*pred[0::2]+ 48,48*pred[1::2]+ 48, label='Prediction')\n\ndef plot_sample(X,y,axs):\n    '''\n    kaggle picture is 96 by 96\n    y is rescaled to range between -1 and 1\n    '''\n    \n    axs.imshow(X.reshape(96,96),cmap=\"gray\")\n    axs.scatter(48*y[0::2]+ 48,48*y[1::2]+ 48)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get only the data with keypoints","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Manually splitting the training and validation data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def data_loader(data_frame):\n    \n    # Load dataset file\n   \n    data_frame['Image'] = data_frame['Image'].apply(lambda i: np.fromstring(i, sep=' '))\n    data_frame = data_frame.dropna()  # Get only the data with 15 keypoints\n   \n    # Extract Images pixel values\n    imgs_array = np.vstack(data_frame['Image'].values)/ 255.0\n    imgs_array = imgs_array.astype(np.float32)    # Normalize, target values to (0, 1)\n    imgs_array = imgs_array.reshape(-1,96, 96, 1)\n        \n    # Extract labels (key point cords)\n    labels_array = data_frame[data_frame.columns[:-1]].values\n    labels_array = (labels_array - 48) / 48    # Normalize, traget cordinates to (-1, 1)\n    labels_array = labels_array.astype(np.float32) \n    \n    # shuffle the train data\n#     imgs_array, labels_array = shuffle(imgs_array, labels_array, random_state=9)  \n    \n    return imgs_array, labels_array\n\ndef data_loader_test(data_frame):\n    \n    # Load dataset file\n   \n    data_frame['Image'] = data_frame['Image'].apply(lambda i: np.fromstring(i, sep=' '))\n  \n    # Extract Images pixel values\n    imgs_array = np.vstack(data_frame['Image'].values)/ 255.0\n    imgs_array = imgs_array.astype(np.float32)    # Normalize, target values to (0, 1)\n    imgs_array = imgs_array.reshape(-1, 96, 96, 1)\n    \n    return imgs_array\n\n\nX,Y = data_loader(training)\nX_test = data_loader_test(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\nprint(\"Train sample:\",X_train.shape,\"Val sample:\",X_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data augmentation\n\nImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nThe Keras deep learning neural network library provides the capability to fit models using image data augmentation via the ImageDataGenerator class.\n\n### Horizontal and Vertical Flip Augmentation\n\nAn image flip means reversing the rows or columns of pixels in the case of a vertical or horizontal flip respectively.\n\n### Horizontal and Vertical Shift Augmentation\n\nA shift to an image means moving all pixels of the image in one direction, such as horizontally or vertically, while keeping the image dimensions the same.\n\nFlipping pictures can double the number of pictures twice. If we allow the pictures to shift by some pixcels within frames, this can increase the number of pictures substantially!","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class DataModifier(object):\n    def fit(self,X_,y_):\n        return(NotImplementedError)\n    \nclass FlipPic(DataModifier):\n    def __init__(self,flip_indices=None):\n        if flip_indices is None:\n            flip_indices = [\n                (0, 2), (1, 3),\n                (4, 8), (5, 9), (6, 10), (7, 11),\n                (12, 16), (13, 17), (14, 18), (15, 19),\n                (22, 24), (23, 25)\n                ]\n        \n        self.flip_indices = flip_indices\n        \n    def fit(self,X_batch,y_batch):\n\n        batch_size = X_batch.shape[0]\n        indices = np.random.choice(batch_size, batch_size//2, replace=False)\n\n        X_batch[indices] = X_batch[indices, :, ::-1,:]\n        y_batch[indices, ::2] = y_batch[indices, ::2] * -1\n\n        # flip left eye to right eye, left mouth to right mouth and so on .. \n        for a, b in self.flip_indices:\n            y_batch[indices, a], y_batch[indices, b] = (\n                    y_batch[indices, b], y_batch[indices, a]\n                )\n        return X_batch, y_batch\n\nclass FlipPic_8(DataModifier):\n    def __init__(self,flip_indices=None):\n        if flip_indices is None:\n            flip_indices = [\n                (0, 2), (1, 3)\n                ]\n        \n        self.flip_indices = flip_indices\n\n    def fit(self,X_batch,y_batch):\n\n        batch_size = X_batch.shape[0]\n        indices = np.random.choice(batch_size, batch_size//2, replace=False)\n\n        X_batch[indices] = X_batch[indices, :, ::-1,:]\n        y_batch[indices, ::2] = y_batch[indices, ::2] * -1\n\n        # flip left eye to right eye, left mouth to right mouth and so on .. \n        for a, b in self.flip_indices:\n            y_batch[indices, a], y_batch[indices, b] = (\n                    y_batch[indices, b], y_batch[indices, a]\n                )\n        return X_batch, y_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"class ShiftFlipPic(FlipPic):\n    def __init__(self,flip_indices=None,prop=0.1):\n        super(ShiftFlipPic,self).__init__(flip_indices)\n        self.prop = prop\n        \n    def fit(self,X,y):\n        X, y = super(ShiftFlipPic,self).fit(X,y)\n        X, y = self.shift_image(X,y,prop=self.prop)\n        return(X,y)\n    def random_shift(self,shift_range,n=96):\n        '''\n        :param shift_range: \n        The maximum number of columns/rows to shift\n        :return: \n        keep(0):   minimum row/column index to keep\n        keep(1):   maximum row/column index to keep\n        assign(0): minimum row/column index to assign\n        assign(1): maximum row/column index to assign\n        shift:     amount to shift the landmark\n\n        assign(1) - assign(0) == keep(1) - keep(0)\n        '''\n        shift = np.random.randint(-shift_range,\n                                  shift_range)\n        def shift_left(n,shift):\n            shift = np.abs(shift)\n            return(0,n - shift)\n        def shift_right(n,shift):\n            shift = np.abs(shift)\n            return(shift,n)\n\n        if shift < 0:\n            keep = shift_left(n,shift) \n            assign = shift_right(n,shift)\n        else:\n            assign = shift_left(n,shift) ## less than 96\n            keep = shift_right(n,shift)\n\n        return((keep,  assign, shift))\n\n    def shift_single_image(self,x_,y_,prop=0.1):\n        '''\n        :param x_: a single picture array (96, 96, 1)\n        :param y_: 15 landmark locations \n                   [0::2] contains x axis values\n                   [1::2] contains y axis values \n        :param prop: proportion of random horizontal and vertical shift\n                     relative to the number of columns\n                     e.g. prop = 0.1 then the picture is moved at least by \n                     0.1*96 = 8 columns/rows\n        :return: \n        x_, y_\n        '''\n        w_shift_max = int(x_.shape[0] * prop)\n        h_shift_max = int(x_.shape[1] * prop)\n\n        w_keep,w_assign,w_shift = self.random_shift(w_shift_max)\n        h_keep,h_assign,h_shift = self.random_shift(h_shift_max)\n\n        x_[w_assign[0]:w_assign[1],\n           h_assign[0]:h_assign[1],:] = x_[w_keep[0]:w_keep[1],\n                                           h_keep[0]:h_keep[1],:]\n\n        y_[0::2] = y_[0::2] - h_shift/float(x_.shape[0]/2.)\n        y_[1::2] = y_[1::2] - w_shift/float(x_.shape[1]/2.)\n        return(x_,y_)\n\n    def shift_image(self,X,y,prop=0.1):\n            ## This function may be modified to be more efficient e.g. get rid of loop?\n            for irow in range(X.shape[0]):\n                x_ = X[irow]\n                y_ = y[irow]\n                X[irow],y[irow] = self.shift_single_image(x_,y_,prop=prop)\n            return(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Flipping pictures","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ngenerator = ImageDataGenerator()\nmodifier = FlipPic_8()\nfig = plt.figure(figsize=(20,20))\ncount = 1\nfor batch in generator.flow(X[:4],Y[:4]):\n    X_batch, y_batch = modifier.fit(*batch)\n    ax = fig.add_subplot(5,4, count,xticks=[],yticks=[])  \n    plot_sample(X_batch[0],y_batch[0],ax)\n    count += 1\n    if count == 10:\n        break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Shifting pictures","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ngenerator = ImageDataGenerator()\nshiftFlipPic = ShiftFlipPic(prop=0.1)\n\nfig = plt.figure(figsize=(20,20))\n\ncount = 1\nfor batch in generator.flow(X[:4],Y[:4]):\n    X_batch, y_batch = shiftFlipPic.fit(*batch)\n\n    ax = fig.add_subplot(5,4, count,xticks=[],yticks=[])  \n    plot_sample(X_batch[0],y_batch[0],ax)\n    count += 1\n    if count == 10:\n        break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from math import sin, cos, pi\n\ndef rotate_augmentation(images, keypoints):\n    rotated_images = []\n    rotated_keypoints = []\n#     print(\"Augmenting for angles (in degrees): \")\n    for angle in rotation_angles:    # Rotation augmentation for a list of angle values\n        for angle in [angle,-angle]:\n#             print(f'{angle}', end='  ')\n            M = cv2.getRotationMatrix2D((48,48), angle, 1.0)\n            angle_rad = -angle*pi/180.     # Obtain angle in radians from angle in degrees (notice negative sign for change in clockwise vs anti-clockwise directions from conventional rotation to cv2's image rotation)\n            # For train_images\n            for image in images:\n                rotated_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                rotated_images.append(rotated_image)\n            # For train_keypoints\n            for keypoint in keypoints:\n                rotated_keypoint = (keypoint+1)-1    # Subtract the middle value of the image dimension\n                for idx in range(0,len(rotated_keypoint),2):\n                    # https://in.mathworks.com/matlabcentral/answers/93554-how-can-i-rotate-a-set-of-points-in-a-plane-by-a-certain-angle-about-an-arbitrary-point\n                    rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n                    rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n                rotated_keypoint = (rotated_keypoint-1)+1   # Add the earlier subtracted value\n                rotated_keypoints.append(rotated_keypoint)\n\n    return np.reshape(rotated_images,(-1,96,96,1)), rotated_keypoints\n\nrotation_angles=[6,12] # Rotation angle in degrees (includes both clockwise & anti-clockwise rotations)\n\n\n\ndef alter_brightness(images, keypoints):\n    altered_brightness_images = []\n    inc_brightness_images = np.clip(images*2, 0.0, 1.0)    # Increased brightness by a factor of 1.2 & clip any values outside the range of [-1,1]\n    dec_brightness_images = np.clip(images*0.1, 0.0, 1.0)    # Decreased brightness by a factor of 0.6 & clip any values outside the range of [-1,1]\n    altered_brightness_images.extend(inc_brightness_images)\n    altered_brightness_images.extend(dec_brightness_images)\n    return altered_brightness_images, np.concatenate((keypoints, keypoints))\n\n\ndef add_blur(images,keypoints,blur_val=3):\n    noisy_images = []\n    for image in images:\n#         kernel = np.ones((5,5),np.float32)/25\n        image = cv2.blur(image, (blur_val, blur_val), cv2.BORDER_DEFAULT).reshape(96,96,1)\n        noisy_images.append(image)\n    return noisy_images, keypoints\n\ndef add_noise(images,keypoints,noise_val):\n    noisy_images = []\n    for image in images:\n#         kernel = np.ones((5,5),np.float32)/25\n        gauss = np.random.normal(0,1,image.size)\n        gauss = gauss.reshape(image.shape[0],image.shape[1],image.shape[2]).astype('uint8')\n        # Add the Gaussian noise to the image\n        img_gauss = cv2.add(img,gauss)\n        noisy_images.append(img_gauss).reshape(96,96,1)\n    return noisy_images, keypoints","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image rotation","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\ngenerator = ImageDataGenerator()\n\nfig = plt.figure(figsize=(20,20))\n\ncount = 1\nfor batch in generator.flow(X[:4],Y[:4]):\n    X_batch, y_batch = add_blur(*batch,3)\n    ax = fig.add_subplot(5,4, count,xticks=[],yticks=[])  \n    plot_sample(X_batch[0],y_batch[0],ax)\n    count += 1\n    if count == 10:\n        break\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Augmentation With ImageDataGenerator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"modifier = FlipPic()\ngenerator = ImageDataGenerator()\nshiftFlipPic_1 = ShiftFlipPic(prop=0.02)\nshiftFlipPic_2 = ShiftFlipPic(prop=0.03)\nshiftFlipPic_3 = ShiftFlipPic(prop=0.04)\nshiftFlipPic_4 = ShiftFlipPic(prop=0.05)\nshiftFlipPic_5 = ShiftFlipPic(prop=0.06)\nshiftFlipPic_6 = ShiftFlipPic(prop=0.07)\nshiftFlipPic_7 = ShiftFlipPic(prop=0.1)\nbatches = 0\nfor batch in generator.flow(X_train,y_train):\n    X_batch, y_batch = add_blur(*batch,5)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = add_blur(*batch,3)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = add_blur(*batch,2)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = rotate_augmentation(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = rotate_augmentation(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = rotate_augmentation(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = rotate_augmentation(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = alter_brightness(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = alter_brightness(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = alter_brightness(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = alter_brightness(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = modifier.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = modifier.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = modifier.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = modifier.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = shiftFlipPic_1.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = shiftFlipPic_2.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = shiftFlipPic_3.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n    X_batch, y_batch = shiftFlipPic_4.fit(*batch)\n    y_train = np.concatenate((y_train,y_batch))\n    X_train = np.concatenate((X_train,X_batch))\n#     X_batch, y_batch = shiftFlipPic_5.fit(*batch)\n#     y_train = np.concatenate((y_train,y_batch))\n#     X_train = np.concatenate((X_train,X_batch))\n#     X_batch, y_batch = shiftFlipPic_6.fit(*batch)\n#     y_train = np.concatenate((y_train,y_batch))\n#     X_train = np.concatenate((X_train,X_batch))\n#     X_batch, y_batch = shiftFlipPic_7.fit(*batch)\n#     y_train = np.concatenate((y_train,y_batch))\n#     X_train = np.concatenate((X_train,X_batch))\n    batches += 1\n    if batches >= 6:\n#         len(X_train) / 32\n        # we need to break the loop by hand because\n        # the generator loops indefinitely\n        break  \n\nprint(\"Train sample:\",X_train.shape,\"Val sample:\",X_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom Metrics in Keras\n\nKaggle submissions are scored on the root mean squared error. RMSE is very common and is a suitable general-purpose error metric. Compared to the Mean Absolute Error, RMSE punishes large errors:\n\n![rmse_img](https://www.includehelp.com/ml-ai/Images/rmse-1.jpg)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend\n \ndef rmse(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, TimeDistributed,Convolution2D, MaxPooling2D, BatchNormalization, AvgPool2D, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D,GRU, LSTM\nfrom keras.optimizers import Adam, SGD\nfrom keras import regularizers\nimport keras.backend as K\nfrom keras.initializers import GlorotNormal\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\nimport numpy as np\nfrom keras.layers.advanced_activations import LeakyReLU\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define simple CNN function using Keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cnn(n_out):\n    model = Sequential()\n    model = Sequential()\n    model.add(TimeDistributed(Convolution2D(32, (3,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal, activation='relu',input_shape=(96,96,1))))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 96, 96, 32)\n    model.add(TimeDistributed(Convolution2D(32, (3,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n#     model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n\n    # Input dimensions: (None, 48, 48, 32)\n    model.add(TimeDistributed(Convolution2D(64, (3,1), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 48, 48, 64)\n    model.add(TimeDistributed(Convolution2D(64, (1,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n#     model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n\n    # Input dimensions: (None, 24, 24, 64)\n    model.add(TimeDistributed(Convolution2D(96, (3,1), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 24, 24, 96)\n    model.add(TimeDistributed(Convolution2D(96, (1,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n#     model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n\n    # Input dimensions: (None, 12, 12, 96)\n    model.add(TimeDistributed(Convolution2D(128, (3,1),padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 12, 12, 128)\n    model.add(TimeDistributed(Convolution2D(128, (1,3),padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n\n    # Input dimensions: (None, 6, 6, 128)\n    model.add(TimeDistributed(Convolution2D(256, (3,1),padding='same',use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 6, 6, 256)\n    model.add(TimeDistributed(Convolution2D(256, (1,3),padding='same',use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n\n    # Input dimensions: (None, 3, 3, 256)\n    model.add(TimeDistributed(Convolution2D(512, (3,1), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 3, 3, 512)\n    model.add(TimeDistributed(Convolution2D(512, (1,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n\n    # Input dimensions: (None, 48, 48, 32)\n    model.add(TimeDistributed(Convolution2D(64, (3,1), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 48, 48, 64)\n    model.add(TimeDistributed(Convolution2D(64, (1,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n#     model.add(TimeDistributed(MaxPool2D(pool_size=(2, 2))))\n    \n    # Input dimensions: (None, 3, 3, 32)\n    model.add(TimeDistributed(Convolution2D(32, (3,1), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n    # Input dimensions: (None, 3, 3, 32)\n    model.add(TimeDistributed(Convolution2D(32, (1,3), padding='same', use_bias=False, kernel_initializer=GlorotNormal)))\n    model.add(TimeDistributed(LeakyReLU(alpha = 0.1)))\n    model.add(TimeDistributed(BatchNormalization()))\n\n    \n    model.add(TimeDistributed(Flatten()))\n\n    model.add(LSTM(1024, activation='relu', return_sequences=False))\n    model.add(Dense(1024,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(n_out))\n    return model\n# model.compile(A(lr=0.0001),loss='mean_squared_error',  metrics=[rmse,mae,own_loss])\n\n# filepath=\"weights-improvement_1.hdf5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='min')\n# callbacks_list = [checkpoint]\n# Fit the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_30 = cnn(30)\n# model_30.compile(optimizer=SGD(lr=0.01,momentum = 0.9,decay=0.00006, nesterov=True), loss=rmse,metrics=[rmse,'mse', 'mae'])\nmodel_30.compile(optimizer=Adam(lr=0.001),loss=rmse, metrics=[rmse,'mse', 'mae'])\n# print(model_30.summary())\n# LR_callback_30 = ReduceLROnPlateau(monitor='val_loss',  verbose=10, factor=.4, min_lr=.00001)\nEarlyStop_callback_30 = EarlyStopping(restore_best_weights=True,mode='min')\n# print(model_30.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_30.fit(X_train,y_train,validation_data=(X_val, y_val), epochs = 120) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\ntrain_sequences = TimeseriesGenerator(X_train, y_train, length=5, batch_size=20)\ntest_sequences = TimeseriesGenerator(X_val, y_val, length=5, batch_size=20)\n\n# fit model using fit_generator instead of fit\nmodel_30.fit_generator(train_sequences,validation_data = test_sequences,epochs=120)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN Model fitting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# # instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\n#     model_30 = cnn(30)\n# # model_30.compile(optimizer=SGD(lr=0.01,momentum = 0.9,decay=0.00006, nesterov=True), loss=rmse,metrics=[rmse,'mse', 'mae'])\n#     model_30.compile(optimizer=Adam(lr=0.001),loss=rmse, metrics=[rmse,'mse', 'mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model_30.fit(X_train,y_train,validation_data=(X_val, y_val), epochs = 120, batch_size=20) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluting model results on train and val sample","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# scores = model_30.evaluate(train_sequences, verbose=0)\nprint(\" Train %s: %.2f%% %s: %.2f%% %s: %.2f%%\" % (model_30.metrics_names[1], scores[1]*100,model_30.metrics_names[2], scores[2]*100,model_30.metrics_names[3], scores[3]*100))\nscores = model_30.evaluate(test_sequences, verbose=0)\nprint(\" Val %s: %.2f%% %s: %.2f%% %s: %.2f%%\" % (model_30.metrics_names[1], scores[1]*100,model_30.metrics_names[2], scores[2]*100,model_30.metrics_names[3], scores[3]*100))\n# test1_sequences = TimeseriesGenerator(X_test, length=5, batch_size=20)\n# y_hat_30 = model_30.predict_generator(X_test,350) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1783/5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot val and train rmse on each epoch","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plot_loss(hist,name,plt,RMSE_TF=False):\n    '''\n    RMSE_TF: if True, then RMSE is plotted with original scale \n    '''\n    loss = hist['rmse']\n    val_loss = hist['val_rmse']\n    if RMSE_TF:\n        loss = np.sqrt(np.array(loss))*48 \n        val_loss = np.sqrt(np.array(val_loss))*48 \n        \n    plt.plot(loss,\"--\",linewidth=3,label=\"train:\"+name)\n    plt.plot(val_loss,linewidth=3,label=\"val:\"+name)\n\nplot_loss(hist.history,\"model 1\",plt)\nplt.legend()\nplt.grid()\nplt.xlabel(\"epoch\")\nplt.ylabel(\"RMSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot actual and prediction keypoints on val sample","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"  \npred = model_30.predict(X_val)\n\nfig = plt.figure(figsize=(7, 7))\nfig.subplots_adjust(hspace=0.13,wspace=0.0001,\n                    left=0,right=1,bottom=0, top=1)\nNpicture = 9\ncount = 1\nfor irow in range(Npicture):\n    ipic = np.random.choice(X_val.shape[0])\n    ax = fig.add_subplot(Npicture/3 , 3, count,xticks=[],yticks=[])        \n    plot_sample_val(X_val[ipic],y_val[ipic], ax,pred[ipic])\n    ax.legend( ncol = 1)\n    ax.set_title(\"picture \"+ str(ipic))\n    count += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model_30.predict(X_test)\nlabel_points = (np.squeeze(pred)*48)+48\n\nfeature_names = list(lookid_data['FeatureName'])\nimage_ids = list(lookid_data['ImageId']-1)\nrow_ids = list(lookid_data['RowId'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))\n    \npredictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(label_points[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('face_key_detection_submission_12.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Model with less feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ntraining_8 = pd.read_csv('../input/facial-keypoints-detection/training.zip',usecols = main_features).dropna()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_8.drop(training_8[\n(training_8.nose_tip_x < training_8.nose_tip_x.quantile(0.10)) & \n(training_8.nose_tip_x > training_8.nose_tip_x.quantile(0.90)) & \n(training_8.nose_tip_y < training_8.nose_tip_y.quantile(0.10)) & \n(training_8.nose_tip_y > training_8.nose_tip_y.quantile(0.90)) & \n(training_8.mouth_center_bottom_lip_x > training_8.mouth_center_bottom_lip_x.quantile(0.10)) & \n(training_8.mouth_center_bottom_lip_x < training_8.mouth_center_bottom_lip_x.quantile(0.90))\n(training_8.mouth_center_bottom_lip_y > training_8.mouth_center_bottom_lip_y.quantile(0.05)) & \n(training_8.mouth_center_bottom_lip_y < training_8.mouth_center_bottom_lip_y.quantile(0.95))].index, axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_8,Y_8 = data_loader(training_8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_8, X_val_8, y_train_8, y_val_8 = train_test_split(X_8, Y_8, test_size=0.2, random_state=42)\nprint(\"Train sample:\",X_train_8.shape,\"Val sample:\",X_val_8.shape, y_train_8.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modifier = FlipPic_8()\ngenerator = ImageDataGenerator()\nshiftFlipPic_1 = ShiftFlipPic(prop=0.02)\nshiftFlipPic_2 = ShiftFlipPic(prop=0.03)\nshiftFlipPic_3 = ShiftFlipPic(prop=0.04)\nshiftFlipPic_4 = ShiftFlipPic(prop=0.05)\nshiftFlipPic_5 = ShiftFlipPic(prop=0.06)\nshiftFlipPic_6 = ShiftFlipPic(prop=0.07)\nshiftFlipPic_7 = ShiftFlipPic(prop=0.1)\nbatches = 0\nfor batch in generator.flow(X_train_8,y_train_8):\n    X_batch_8, y_batch_8 = add_blur(*batch,5)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = add_blur(*batch,3)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = add_blur(*batch,2)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = rotate_augmentation(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = rotate_augmentation(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = rotate_augmentation(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = rotate_augmentation(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = alter_brightness(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = modifier.fit(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = modifier.fit(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = modifier.fit(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = alter_brightness(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = alter_brightness(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    X_batch_8, y_batch_8 = alter_brightness(*batch)\n    y_train_8 = np.concatenate((y_train_8,y_batch_8))\n    X_train_8 = np.concatenate((X_train_8,X_batch_8))\n    batches += 1\n    if batches >= 6:\n#         len(X_train_8) / 32\n        # we need to break the loop by hand because\n        # the generator loops indefinitely\n        break  \n\nprint(\"Train sample:\",X_train_8.shape,\"Val sample:\",X_val_8.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_8 = cnn(8)\n# model_8.compile(optimizer=Adam(lr=0.001),loss=rmse, metrics=[rmse,'mse', 'mae'])\n# # model_8.compile(optimizer=SGD(lr=0.01,momentum = 0.9,decay=0.000125, nesterov=True), loss=rmse,metrics=[rmse,'mse', 'mae'])\n# print(model_8.summary())\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model_8 = cnn(8)\n# model_30.compile(optimizer=SGD(lr=0.01,momentum = 0.9,decay=0.00006, nesterov=True), loss=rmse,metrics=[rmse,'mse', 'mae'])\n    model_8.compile(optimizer=Adam(lr=0.001),loss=rmse, metrics=[rmse,'mse', 'mae'])\n\nLR_callback_8 = ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=10, factor=.4, min_lr=.00001)\nEarlyStop_callback_8 = EarlyStopping(patience=15, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist1 = model_8.fit(X_train_8,y_train_8,validation_data=(X_val_8, y_val_8), epochs = 120) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot_features = ['nose_tip_x', 'mouth_center_bottom_lip_x',\n                    'nose_tip_y',\n            'mouth_center_bottom_lip_y']\n\nremove_outlier.boxplot(column=boxplot_features )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_8.boxplot(column=boxplot_features )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_8 = model_8.evaluate(X_train_8, y_train_8, verbose=0)\nprint(\" Train %s: %.2f%% %s: %.2f%% %s: %.2f%%\" % (model_8.metrics_names[1], scores_8[1]*100,model_8.metrics_names[2], scores_8[2]*100,model_8.metrics_names[3], scores_8[3]*100))\nscores_8 = model_8.evaluate(X_val_8, y_val_8, verbose=0)\nprint(\" Val %s: %.2f%% %s: %.2f%% %s: %.2f%%\" % (model_8.metrics_names[1], scores_8[1]*100,model_8.metrics_names[2], scores_8[2]*100,model_8.metrics_names[3], scores_8[3]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(hist,name,plt,RMSE_TF=False):\n    '''\n    RMSE_TF: if True, then RMSE is plotted with original scale \n    '''\n    loss = hist['rmse']\n    val_loss = hist['val_rmse']\n    if RMSE_TF:\n        loss = np.sqrt(np.array(loss))*48 \n        val_loss = np.sqrt(np.array(val_loss))*48 \n        \n    plt.plot(loss,\"--\",linewidth=3,label=\"train:\"+name)\n    plt.plot(val_loss,linewidth=3,label=\"val:\"+name)\n\nplot_loss(hist1.history,\"model 1\",plt)\nplt.legend()\nplt.grid()\nplt.xlabel(\"epoch\")\nplt.ylabel(\"RMSE\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  \npred = model_8.predict(X_val)\n\nfig = plt.figure(figsize=(7, 7))\nfig.subplots_adjust(hspace=0.13,wspace=0.0001,\n                    left=0,right=1,bottom=0, top=1)\nNpicture = 9\ncount = 1\nfor irow in range(Npicture):\n    ipic = np.random.choice(X_val.shape[0])\n    ax = fig.add_subplot(Npicture/3 , 3, count,xticks=[],yticks=[])        \n    plot_sample_val(X_val_8[ipic],y_val_8[ipic], ax,pred[ipic])\n    ax.legend( ncol = 1)\n    ax.set_title(\"picture \"+ str(ipic))\n    count += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create .csv files to submit to kaggle competition","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_8 = model_8.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat_8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Predictions shape', y_hat_30.shape)\nprint('Predictions shape', y_hat_8.shape)\nfeature_8_ind = [0, 1, 2, 3, 20, 21, 28, 29]\n#Merge 2 prediction from y_hat_30 and y_hat_8.\nfor i in range(8):\n    print('Copy \"{}\" feature column from y_hat_8 --> y_hat_30'.format(main_features[i]))\n    y_hat_30[:,feature_8_ind[i]] = (y_hat_8[:,i]*0.8+y_hat_30[:,feature_8_ind[i]]*0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_points_30 = (np.squeeze(y_hat_30)*48)+48\n\nfeature_names = list(lookid_data['FeatureName'])\nimage_ids = list(lookid_data['ImageId']-1)\nrow_ids = list(lookid_data['RowId'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))\n    \npredictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(label_points_30[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('face_key_detection_submission_combine_16.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"## Saving CNN model architecture and model weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import model_from_json\n\ndef save_model(model,name):\n    '''\n    save model architecture and model weights\n    '''\n    json_string = model.to_json()\n    open(name+'_architecture.json', 'w').write(json_string)\n    model.save_weights(name+'_weights.h5')\n    \ndef load_model(name):\n    model = model_from_json(open(name+'_architecture.json').read())\n    model.load_weights(name + '_weights.h5')\n    return(model)\n\nsave_model(model_30,\"model30\")\nmodel = load_model(\"model30\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}