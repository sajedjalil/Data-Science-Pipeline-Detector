{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Facial keypoints detection\n# Preparation\n## Libraries import","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import models\nfrom torchvision import transforms\nfrom torch import nn, optim\nfrom torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\nimport time\nimport cv2\nimport os\nfrom zipfile import ZipFile\nimport gc\n\ngc.collect()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:05:56.993603Z","iopub.execute_input":"2021-12-11T21:05:56.994055Z","iopub.status.idle":"2021-12-11T21:05:59.137544Z","shell.execute_reply.started":"2021-12-11T21:05:56.993937Z","shell.execute_reply":"2021-12-11T21:05:59.136806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Environment settings","metadata":{}},{"cell_type":"code","source":"if os.environ.get('KAGGLE_KERNEL_RUN_TYPE',''):\n    print(\"Running a Kaggle Notebook/Script - Could be Interactive or Batch Mode\")\n    host = 'Kaggle'\n    inp = '../input/'\n    dir = ''\n    frame_color = 'black'\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive':\n        print(\"Running a Kaggle Notebook/Script - Interactive Mode\")\n\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Batch':\n        print(\"Running a Kaggle Notebook/Script - Batch Mode\")\n\nelse:\n    try:\n        import google.colab\n        host = 'Colab'\n        drive.mount('/content/drive')\n        dir = 'drive/MyDrive/studies/My projects/Kaggle/Facial_keypoints/'\n        frame_color = 'black'\n        inp = ''\n        print(\"Running on Colab\")\n    except ModuleNotFoundError:\n        host = None\n        dir = ''\n        inp = ''\n        frame_color = 'white'\n        print(\"Running on Localhost\")\n\ngpu = torch.cuda.is_available()\nprint('GPU: ' + str(gpu))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:05:59.140176Z","iopub.execute_input":"2021-12-11T21:05:59.140685Z","iopub.status.idle":"2021-12-11T21:05:59.195418Z","shell.execute_reply.started":"2021-12-11T21:05:59.140656Z","shell.execute_reply":"2021-12-11T21:05:59.194626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read the data","metadata":{}},{"cell_type":"code","source":"with ZipFile(inp + 'facial-keypoints-detection/test.zip', 'r') as zipObj:\n   zipObj.extractall()\nwith ZipFile(inp + 'facial-keypoints-detection/training.zip', 'r') as zipObj:\n   zipObj.extractall()\ntrain_and_val_csv = pd.read_csv(dir + 'training.csv')\ntest_csv = pd.read_csv(dir + 'test.csv')\nprint('Train and validation set length {}'.format(len(train_and_val_csv)))\nprint('Test set length {}'.format(len(test_csv)))","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:05:59.196776Z","iopub.execute_input":"2021-12-11T21:05:59.197213Z","iopub.status.idle":"2021-12-11T21:06:06.343737Z","shell.execute_reply.started":"2021-12-11T21:05:59.197174Z","shell.execute_reply":"2021-12-11T21:06:06.34294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train and test set info","metadata":{}},{"cell_type":"code","source":"print(train_and_val_csv.info())\nprint(test_csv.info())\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:06.345764Z","iopub.execute_input":"2021-12-11T21:06:06.346333Z","iopub.status.idle":"2021-12-11T21:06:06.381472Z","shell.execute_reply.started":"2021-12-11T21:06:06.346291Z","shell.execute_reply":"2021-12-11T21:06:06.380548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## New train set","metadata":{}},{"cell_type":"code","source":"auto_fill = train_and_val_csv.ffill()\nfull_only = train_and_val_csv.dropna()\nmissing_only = train_and_val_csv[train_and_val_csv.isna().sum(axis=1) != 0]\n# missing_only = missing_only.fillna(method='ffill').fillna(method='bfill')\n\nprint(auto_fill.shape)\nprint(missing_only.shape)\nprint(full_only.shape)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:06.382645Z","iopub.execute_input":"2021-12-11T21:06:06.382927Z","iopub.status.idle":"2021-12-11T21:06:06.403428Z","shell.execute_reply.started":"2021-12-11T21:06:06.382886Z","shell.execute_reply":"2021-12-11T21:06:06.402561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DataLoader","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def get_keypoints_features(keypoint_data):\n    keypoint_features = []\n    for _, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n\n    keypoint_features = np.array(keypoint_features, dtype=\"float\")\n    return keypoint_features\n\nclass FaceKeypointDataSet(torch.utils.data.Dataset):\n    def __init__(self, data, transformer=None, transformer_factor=None, is_test_set=False):\n        imgs = data.Image\n        imgs = np.array(imgs)\n        for j in range(len(imgs)):\n            imgs[j] = np.fromstring(imgs[j], sep = ' ')\n        self.image_data = imgs\n        self.feature_data = data.drop(['Image'], axis=1)\n        self.transformer = transformer\n        self.transformer_factor = transformer_factor\n        self.is_test_set = is_test_set\n        self.transform = transforms.Compose([transforms.ToPILImage(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\n    def __len__(self):\n        return len(self.image_data)\n\n    def __getitem__(self,index):\n        img =  self.image_data[index]\n        img = img.astype(np.uint8).reshape(96,96)\n        img = self.transform(img)\n        if self.is_test_set:\n                return img\n        feature = np.array(self.feature_data.iloc[index])\n        if self.transformer is not None:\n            img, feature = self.transformer(img,feature,self.transformer_factor)\n        return img, feature\n\noriginal_train_data = FaceKeypointDataSet(train_and_val_csv, transformer=None)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:06.404923Z","iopub.execute_input":"2021-12-11T21:06:06.405662Z","iopub.status.idle":"2021-12-11T21:06:22.369693Z","shell.execute_reply.started":"2021-12-11T21:06:06.405624Z","shell.execute_reply":"2021-12-11T21:06:22.368909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation\n### Help functions","metadata":{}},{"cell_type":"code","source":"all_datasets = []\ndef create_aug_sets(aug_transformer, params):\n    aug_sets = []\n    for param in params:\n        aug_data = FaceKeypointDataSet(train_and_val_csv, transformer=aug_transformer,\n                                       transformer_factor=param)\n        aug_sets.append(aug_data)\n    show_aug(aug_sets)\n    global all_datasets\n    all_datasets += aug_sets\n    return aug_sets\n\ndef show_img_and_features(data, img_index):\n    plt.imshow(data[img_index][0].reshape(96, 96), cmap='gray')\n    plt.scatter(data[img_index][1][::2], data[img_index][1][1::2], marker='o', s=100)\n\ndef show_aug(datasets):\n    '''\n    Show the diffrence between the augmented dataset to the original\n    '''\n    fig = plt.figure(figsize=(10, 20))\n    plt.tight_layout()\n    num_of_datasets = len(datasets)\n    rand_img = np.random.randint(0, len(original_train_data))\n    for index, aug_data in enumerate(datasets):\n        fig.add_subplot(num_of_datasets, 2, (index + 1) * 2 - 1)\n        show_img_and_features(original_train_data, rand_img)\n        fig.add_subplot(num_of_datasets, 2, (index + 1) * 2)\n        show_img_and_features(aug_data, rand_img)\n    plt.show()\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:22.371827Z","iopub.execute_input":"2021-12-11T21:06:22.372159Z","iopub.status.idle":"2021-12-11T21:06:22.382196Z","shell.execute_reply.started":"2021-12-11T21:06:22.372122Z","shell.execute_reply":"2021-12-11T21:06:22.381478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Horizontal flip","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"original_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\nflip_index = [2,3,0,1,8,9,10,11,4,5,6,7,16,17,18,19,12,13,14,15,20,21,24,25,22,23,26,27,28,29]\n\ndef flip_aug(img, fea, factor):\n    img = np.array(img)\n    img = img[:,:,::-1]\n    img = torch.tensor(img.copy()).reshape(1,96,96)\n    fea = fea[flip_index]\n    fea[::2] = 96 - fea[::2]\n    return img, fea\n\ntransformer_params = [None]\naug_sets = create_aug_sets(flip_aug, transformer_params)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:22.383648Z","iopub.execute_input":"2021-12-11T21:06:22.384544Z","iopub.status.idle":"2021-12-11T21:06:39.264258Z","shell.execute_reply.started":"2021-12-11T21:06:22.384507Z","shell.execute_reply":"2021-12-11T21:06:39.263522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add noise","metadata":{}},{"cell_type":"code","source":"def noise_aug(img, fea, factor):\n    img = np.array(img)\n    img = img + 0.008 * np.random.randn(1,96,96)\n    img = torch.tensor(img.copy(),dtype=torch.float32).reshape(1,96,96)\n    return img, fea\n\ntransformer_params = [None]\naug_sets = create_aug_sets(noise_aug, transformer_params)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:39.265299Z","iopub.execute_input":"2021-12-11T21:06:39.265532Z","iopub.status.idle":"2021-12-11T21:06:55.444353Z","shell.execute_reply.started":"2021-12-11T21:06:39.265504Z","shell.execute_reply":"2021-12-11T21:06:55.44369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Brightness trim","metadata":{}},{"cell_type":"code","source":"def brightness_aug(img, fea, factor):\n    img = np.clip(img + factor, -1, 1)\n    img = img.reshape(1,96,96)\n    return img, fea\n\ntransformer_params = [1, -1, 0.5, -0.5]\naug_sets = create_aug_sets(brightness_aug, transformer_params)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:06:55.447081Z","iopub.execute_input":"2021-12-11T21:06:55.447713Z","iopub.status.idle":"2021-12-11T21:08:02.892324Z","shell.execute_reply.started":"2021-12-11T21:06:55.447674Z","shell.execute_reply":"2021-12-11T21:08:02.89171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rotate","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def rotate_aug(img, fea, factor=-30):\n    rad = -factor/180 * np.pi\n    rot = cv2.getRotationMatrix2D((48,48),factor,1)\n    img = cv2.warpAffine(np.array(img).reshape(96,96),rot,(96,96),flags=cv2.INTER_CUBIC)\n    img = torch.tensor(img).reshape(1,96,96)\n    fea -= 48\n    for index in range(0,len(fea),2):\n        x = fea[index]\n        y = fea[index + 1]\n        fea[index] = x * np.cos(rad) - y * np.sin(rad)\n        fea[index + 1] = x * np.sin(rad) + y * np.cos(rad)\n    fea += 48\n    return img,fea\n\ntransformer_params = [30, -30, 15, -15]\naug_sets = create_aug_sets(rotate_aug, transformer_params)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:08:02.893529Z","iopub.execute_input":"2021-12-11T21:08:02.896006Z","iopub.status.idle":"2021-12-11T21:09:10.219357Z","shell.execute_reply.started":"2021-12-11T21:08:02.895967Z","shell.execute_reply":"2021-12-11T21:09:10.216117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Arrange the data","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"tav_split = 0.65\nindices = list(range(len(full_only)))\nnp.random.shuffle(indices)\nsplit = int(np.floor((1 - tav_split) * len(full_only)))\ntrain_indices, val_indices = indices[split:], indices[:split]\ntrain_full = full_only.iloc[train_indices]\nval_sampler =  SequentialSampler(val_indices)\nmissing_set = FaceKeypointDataSet(missing_only, transformer=None)\ntrain_full_set = FaceKeypointDataSet(train_full, transformer=None)\nfull_only_set = FaceKeypointDataSet(full_only,transformer=None)\nall_datasets.append(missing_set)\nall_datasets.append(train_full_set)\ntrain_data = torch.utils.data.ConcatDataset(all_datasets)\ntrain_sampler = SubsetRandomSampler(range(len(train_data)))\nprint(f\"Train set length: {len(train_data)}\")\nprint(f\"Validation set length: {len(val_indices)}\")\ndel indices, train_indices, train_full, train_full_set, val_indices,\\\n    missing_set, all_datasets, original_train_data\ngc.collect()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:09:10.220901Z","iopub.execute_input":"2021-12-11T21:09:10.221395Z","iopub.status.idle":"2021-12-11T21:09:30.381952Z","shell.execute_reply.started":"2021-12-11T21:09:10.221346Z","shell.execute_reply":"2021-12-11T21:09:30.381196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constract the loader","metadata":{}},{"cell_type":"code","source":"if gpu:\n    batch_size = 256\nelse:\n    batch_size = 32\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(full_only_set, batch_size=batch_size, sampler=val_sampler)\ndel full_only_set, train_data,val_sampler,train_sampler\ngc.collect()\nprint('Data loader is ready, wait for saving')\ntorch.save(val_loader, 'val_loader.pt')\nprint('Val loader is saved')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T21:09:30.383385Z","iopub.execute_input":"2021-12-11T21:09:30.3839Z","iopub.status.idle":"2021-12-11T21:09:31.405404Z","shell.execute_reply.started":"2021-12-11T21:09:30.383838Z","shell.execute_reply":"2021-12-11T21:09:31.404564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualization","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"## Help function","metadata":{}},{"cell_type":"code","source":"def visualize_examples(images, features, pred_labels=None):\n    '''\n    Make 16 images and labels examplesf= from the input,\n    if the input includes predicted labels - show them with x marks\n    '''\n    fig = plt.figure(figsize=(20,10))\n    for i in range(16):\n        fig.add_subplot(4, 4, i + 1)\n        plt.imshow(images[i].reshape(96, 96), cmap='gray')\n        plt.axis('off')\n        plt.tight_layout()\n        plt.scatter(features[i][::2], features[i][1::2], marker='o', s=100)\n        if pred_labels is not None:\n                plt.scatter(pred_labels[i][::2], pred_labels[i][1::2], marker='x',color='red', s=100)\n\n    plt.show()","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:09:31.406601Z","iopub.execute_input":"2021-12-11T21:09:31.406952Z","iopub.status.idle":"2021-12-11T21:09:31.417482Z","shell.execute_reply.started":"2021-12-11T21:09:31.406914Z","shell.execute_reply":"2021-12-11T21:09:31.416641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some examples","metadata":{}},{"cell_type":"code","source":"print('Training set')\nimages, features = next(iter(train_loader))\nvisualize_examples(images, features)\nprint('Validation set')\nimages, features = next(iter(val_loader))\nvisualize_examples(images, features)\n\n","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-12-11T21:09:31.418673Z","iopub.execute_input":"2021-12-11T21:09:31.419519Z","iopub.status.idle":"2021-12-11T21:09:34.625437Z","shell.execute_reply.started":"2021-12-11T21:09:31.41948Z","shell.execute_reply":"2021-12-11T21:09:34.624675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"out_features = 30\nclass Basic(nn.Module):\n    '''\n    Some self ensmbeled model for fun\n    '''\n    def __init__(self):\n        super(Basic, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(1,4,(3,3),padding=1),\n                                   nn.ReLU(),\n                                   nn.Conv2d(4,16,(3,3),padding=1),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(16))\n        self.conv2 = nn.Sequential(nn.Conv2d(16,32,(3,3)),\n                                   nn.ReLU(),\n                                   nn.Conv2d(32,64,(3,3)),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(64))\n        self.conv3 = nn.Sequential(nn.Conv2d(64,128,(5,5)),\n                                   nn.ReLU(),\n                                   nn.Conv2d(128,256,(5,5)),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(256))\n        self.conv4 = nn.Sequential(nn.Conv2d(256,526,(3,3)),\n                                   nn.ReLU(),\n                                   nn.Conv2d(526,1024,(3,3)),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(1024))\n        self.max_pool = nn.Sequential(nn.MaxPool2d(2))\n        self.fc1 = nn.Linear(9216,1024)\n        self.fc2 = nn.Linear(1024,256)\n        self.fc3 = nn.Linear(256,out_features)\n\n    def forward(self, x):\n        x = self.conv1(x) + x\n        x = self.max_pool(x)\n        x = self.conv2(x)\n        x = self.max_pool(x)\n        x = self.conv3(x)\n        x = self.max_pool(x)\n        x = self.conv4(x)\n        x = x.view(x.shape[0],-1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:09:34.626845Z","iopub.execute_input":"2021-12-11T21:09:34.627171Z","iopub.status.idle":"2021-12-11T21:09:34.644141Z","shell.execute_reply.started":"2021-12-11T21:09:34.627136Z","shell.execute_reply":"2021-12-11T21:09:34.64342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training parameters","metadata":{}},{"cell_type":"code","source":"def RMSELoss(pred,y):\n    return torch.sqrt(torch.mean((pred-y)**2))\n\ndef RMSELoss_custom(pred,y):\n    not_nan = (batch_size * 30 - y.isnan().sum())*0.0001\n    return torch.sqrt(torch.mean((pred-y).nan_to_num()**2)) / not_nan\nepochs = 150\n\nresnet50 = models.resnet50(num_classes = 30)\nresnet50.inplanes = 96\nresnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\nmodel = resnet50\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:09:34.645548Z","iopub.execute_input":"2021-12-11T21:09:34.645804Z","iopub.status.idle":"2021-12-11T21:09:35.006587Z","shell.execute_reply.started":"2021-12-11T21:09:34.645767Z","shell.execute_reply":"2021-12-11T21:09:35.005858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train!\n","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.001\ncriterion = RMSELoss_custom\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',verbose=True, patience=5)\nif gpu:\n    model.cuda()\n\ntrain_losses, val_losses = [], []\nval_loss_min = np.inf\nfor e in range(1, epochs + 1):\n    start = time.perf_counter()\n    model.float().train()\n    train_loss = 0\n    for images, labels in train_loader:\n        if gpu:\n            images = images.cuda()\n            labels = labels.cuda()\n        optimizer.zero_grad()\n        prediction = model(images)\n        loss = criterion(prediction, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    val_loss = 0\n    with torch.no_grad():\n        model.eval()\n        for images, labels in val_loader:\n            if gpu:\n                images = images.cuda()\n                labels = labels.cuda()\n            prediction = model(images)\n            loss = criterion(prediction, labels)\n            val_loss += loss.item()\n        scheduler.step(val_loss)\n    train_losses.append(train_loss/len(train_loader))\n    val_losses.append(val_loss/len(val_loader))\n    print(\"Epoch: {}/{} \".format(e, epochs),\n                  \"Training Loss: {:.4f}\".format(train_losses[-1]),\n                  \"Val Loss: {:.4f}\".format(val_losses[-1]))\n    if val_loss < val_loss_min:\n        val_loss_min = val_loss\n        torch.save(model.state_dict(), dir + 'model_so_far.pt')\n        print('Detected network improvement, saving current model')\n    end = time.perf_counter()\n    total = (end - start)*(epochs - e)\n    print('Estimated time: {} hours, '\n          '{} minutes, {} seconds'.format(total//3600,\n                                          total%3600//60,int(total%60)), end=\"\\r\")\n\nmodel = resnet50\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncheckpoint = torch.load(dir + 'model_so_far.pt')\nmodel.load_state_dict(checkpoint)\n\nplt.tick_params(colors=frame_color)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.legend(frameon=False)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-11T21:09:35.008093Z","iopub.execute_input":"2021-12-11T21:09:35.008335Z","iopub.status.idle":"2021-12-12T02:55:46.553474Z","shell.execute_reply.started":"2021-12-11T21:09:35.008301Z","shell.execute_reply":"2021-12-12T02:55:46.552689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load the model","metadata":{"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"model = resnet50\n\ncheckpoint = torch.load(dir + 'model_so_far.pt',map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint)\nmodel.cpu()\nprint('Model is ready!')","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-12T02:55:46.555025Z","iopub.execute_input":"2021-12-12T02:55:46.555534Z","iopub.status.idle":"2021-12-12T02:55:46.836478Z","shell.execute_reply.started":"2021-12-12T02:55:46.555493Z","shell.execute_reply":"2021-12-12T02:55:46.8357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation prediction visualization\n## Some examples","metadata":{}},{"cell_type":"code","source":"val_loader = torch.load(dir + 'val_loader.pt')\nfig = plt.figure(figsize=(20,10))\nimages, labels = next(iter(val_loader))\nwith torch.no_grad():\n    model.eval()\n    predicted_labels = model(images)\nvisualize_examples(images, labels, predicted_labels)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-12T02:55:46.8402Z","iopub.execute_input":"2021-12-12T02:55:46.840407Z","iopub.status.idle":"2021-12-12T02:55:59.46874Z","shell.execute_reply.started":"2021-12-12T02:55:46.840381Z","shell.execute_reply":"2021-12-12T02:55:59.468105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nimages, labels = next(iter(train_loader))\nwith torch.no_grad():\n    model.eval()\n    predicted_labels = model(images)\nvisualize_examples(images, labels, predicted_labels)","metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2021-12-12T02:55:59.469986Z","iopub.execute_input":"2021-12-12T02:55:59.470356Z","iopub.status.idle":"2021-12-12T02:56:11.668431Z","shell.execute_reply.started":"2021-12-12T02:55:59.470322Z","shell.execute_reply":"2021-12-12T02:56:11.667679Z"},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test prediction\n## Predict","metadata":{}},{"cell_type":"code","source":"test_csv = pd.read_csv(dir + 'test.csv')\ntest_data = FaceKeypointDataSet(test_csv,is_test_set=True)\ntest_sampler =  SequentialSampler(range(len(test_data)))\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), sampler=test_sampler)\nimages = next(iter(test_loader))\nwith torch.no_grad():\n    model.eval()\n    predicted_labels = model(images)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-12T02:56:11.669761Z","iopub.execute_input":"2021-12-12T02:56:11.670181Z","iopub.status.idle":"2021-12-12T02:57:29.155425Z","shell.execute_reply.started":"2021-12-12T02:56:11.670148Z","shell.execute_reply":"2021-12-12T02:57:29.154552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"print('test set')\nvisualize_examples(images,predicted_labels)\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-12T02:57:29.156958Z","iopub.execute_input":"2021-12-12T02:57:29.15722Z","iopub.status.idle":"2021-12-12T02:57:30.557446Z","shell.execute_reply.started":"2021-12-12T02:57:29.157185Z","shell.execute_reply":"2021-12-12T02:57:30.556807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"keypts_labels = train_and_val_csv.columns.tolist()\nid_lookup = pd.read_csv(inp + 'facial-keypoints-detection/IdLookupTable.csv')\nid_lookup_features = list(id_lookup['FeatureName'])\nid_lookup_image = list(id_lookup['ImageId'])\n\nfor i in range(len(id_lookup_features)):\n    id_lookup_features[i] = keypts_labels.index(id_lookup_features[i])\n\nlocation = []\nfor i in range(len(id_lookup_features)):\n    value = float(predicted_labels[id_lookup_image[i]-1][id_lookup_features[i]])\n    if value < 0:\n        print(id_lookup_image[i] - 1)\n        value = 0\n    if value > 96:\n        print(id_lookup_image[i] - 1)\n        value = 96\n    location.append(value)\nid_lookup['Location'] = location\nsubmission = id_lookup[['RowId', 'Location']]\nsubmission.to_csv('submission.csv',index=False)\nprint('Total test images labeled:')\nprint(len(submission) - 1)\nprint('Submission file is ready')\n","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n","is_executing":true},"jupyter":{"outputs_hidden":false},"scrolled":true,"execution":{"iopub.status.busy":"2021-12-12T02:57:30.558617Z","iopub.execute_input":"2021-12-12T02:57:30.558978Z","iopub.status.idle":"2021-12-12T02:57:30.824823Z","shell.execute_reply.started":"2021-12-12T02:57:30.558943Z","shell.execute_reply":"2021-12-12T02:57:30.824095Z"},"trusted":true},"execution_count":null,"outputs":[]}]}