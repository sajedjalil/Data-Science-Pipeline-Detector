{"nbformat":4,"cells":[{"outputs":[],"cell_type":"code","metadata":{},"source":"import pandas as pd\nimport numpy as np\nimport os,glob\nimport lightgbm as lgb\nfile_list = []\nfor file in glob.glob(\"../input/*.csv\"):\n    file_list.append(file)\n    print(file)\n","execution_count":9},{"outputs":[],"cell_type":"code","metadata":{},"source":"store_relation = pd.read_csv(file_list[1])\nstore_relation.head()","execution_count":15},{"outputs":[],"cell_type":"code","metadata":{},"source":"hpg_store_info = pd.read_csv(file_list[-2])\nprint(hpg_store_info.head())\nhpg_store_info = to_radians(hpg_store_info,'longitude')\n\nprint(len(set(hpg_store_info['hpg_area_name'].values)))","execution_count":16},{"outputs":[],"cell_type":"code","metadata":{},"source":"hpg_store_info.at[0,'latitude'] = math.radians(hpg_store_info['latitude'][0])","execution_count":17},{"outputs":[],"cell_type":"code","metadata":{},"source":"hpg_reserve = pd.read_csv(file_list[5])\nhpg_reserve.head()","execution_count":18},{"outputs":[],"cell_type":"code","metadata":{},"source":"air_store_info = pd.read_csv(file_list[-1])\nprint(air_store_info.head())\nprint(len(set(air_store_info['air_area_name'].values)))","execution_count":19},{"outputs":[],"cell_type":"code","metadata":{},"source":"air_reserve = pd.read_csv(file_list[2])\nair_reserve.head()","execution_count":20},{"outputs":[],"cell_type":"code","metadata":{},"source":"air_visit = pd.read_csv(file_list[4])\nair_visit.head()","execution_count":21},{"outputs":[],"cell_type":"code","metadata":{},"source":"date_data = pd.read_csv(file_list[0])\ndate_data.head()","execution_count":22},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"from sklearn import preprocessing\nimport re\nimport numpy as np\nimport pandas as pd\nimport math\ndef to_radians(df,col_name):\n    for i in range(len(df)):\n        df.at[i,col_name] = math.radians(df[col_name][i])\n    return df\n\ndef merge_dataset(data):\n    # merge hpg reserve and id relation according to hpg store id\n    data['hpg_reserve'] = pd.merge(data['hpg_reserve'],data['id_relation'],how='inner',on=['hpg_store_id'])\n    for df in ['air_reserve','hpg_reserve']:\n        data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n        data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n        data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n        data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n        data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (\n            r['visit_datetime'] - r['reserve_datetime']).days,axis=1)\n        tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[\n            ['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'\n                                                                                 , 'reserve_datetime_diff': 'rs1'\n                                                                                 , 'reserve_visitors':'rv1'})\n        tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[\n            ['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date',\n                                                                                  'reserve_datetime_diff': 'rs2',\n                                                                                  'reserve_visitors':'rv2'})\n        data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n    for df in ['train','test']:\n        if df is 'test':\n            data[df]['visit_date'] = data['test']['id'].map(lambda x: str(x).split('_')[2])\n        data[df]['visit_date'] = pd.to_datetime(data[df]['visit_date'])\n        data[df]['dow'] = data[df]['visit_date'].dt.dayofweek\n        data[df]['year'] = data[df]['visit_date'].dt.year\n        data[df]['month'] = data[df]['visit_date'].dt.month\n        data[df]['visit_date'] = data[df]['visit_date'].dt.date\n    data['test']['air_store_id'] = data['test']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n    unique_stores = data['test']['air_store_id'].unique()\n    stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, \n                                      'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, \n                                   ignore_index=True).reset_index(drop=True)\n    for col in ['min_visitors','mean_visitors','median_visitors','max_visitors','count_observations']:\n        temp = data['train'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={\n            'visitors':col})\n        stores = pd.merge(stores, temp, how='left', on=['air_store_id','dow']) \n    stores = pd.merge(stores, data['air_store'], how='left', on=['air_store_id']) \n    stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n    stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n    \n    encode_label = preprocessing.LabelEncoder()\n    for i in range(10):\n        stores['air_genre_name'+str(i)] = encode_label.fit_transform(\n            stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n        stores['air_area_name'+str(i)] = encode_label.fit_transform(\n            stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_genre_name'] = encode_label.fit_transform(stores['air_genre_name'])\n    stores['air_area_name'] = encode_label.fit_transform(stores['air_area_name'])\n    \n    data['holiday']['visit_date'] = pd.to_datetime(data['holiday']['visit_date'])\n    data['holiday']['day_of_week'] = encode_label.fit_transform(data['holiday']['day_of_week'])\n    data['holiday']['visit_date'] = data['holiday']['visit_date'].dt.date\n    train = pd.merge(data['train'], data['holiday'], how='left', on=['visit_date']) \n    test = pd.merge(data['test'], data['holiday'], how='left', on=['visit_date']) \n\n    train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n    test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\n    for df in ['air_reserve','hpg_reserve']:\n        train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n        test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n\n    train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\n    train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n    train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n    train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\n    test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n    test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n    test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n    train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n    test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n\n    train = to_radians(train,'latitude')\n    train = to_radians(train,'longitude')\n    test = to_radians(test,'latitude')\n    test = to_radians(test,'longitude')\n\n    train['var_max_lat'] = train['latitude'].max() - train['latitude']\n    train['var_max_long'] = train['longitude'].max() - train['longitude']\n    test['var_max_lat'] = test['latitude'].max() - test['latitude']\n    test['var_max_long'] = test['longitude'].max() - test['longitude']\n    encoder2 = preprocessing.LabelEncoder()\n    train['air_store_id2'] = encoder2.fit_transform(train['air_store_id'])\n    test['air_store_id2'] = encoder2.transform(test['air_store_id'])\n    \n    return train,test\n\ndef RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":24},{"outputs":[],"cell_type":"code","metadata":{},"source":"data = {\n    'train':pd.read_csv('../input/air_visit_data.csv'),\n    'air_store':pd.read_csv('../input/air_store_info.csv'),\n    'hpg_store':pd.read_csv('../input/hpg_store_info.csv'),\n    'air_reserve':pd.read_csv('../input/air_reserve.csv'),\n    'hpg_reserve':pd.read_csv('../input/hpg_reserve.csv'),\n    'id_relation':pd.read_csv('../input/store_id_relation.csv'),\n    'test':pd.read_csv('../input/sample_submission.csv'),\n    'holiday':pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'}),\n}","execution_count":26},{"outputs":[],"cell_type":"code","metadata":{},"source":"train,test = merge_dataset(data)\nprint(train.head())\nprint(test.head())","execution_count":27},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"import pickle\npickle.dump(train,open('preprocessed_train.pkl','wb'))\npickle.dump(test,open('preprocessed_test.pkl','wb'))","execution_count":28},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"import pickle\nimport numpy as np\ntrain = pickle.load(open('preprocessed_train.pkl','rb'))\ntest = pickle.load(open('preprocessed_test.pkl','rb'))","execution_count":29},{"outputs":[],"cell_type":"code","metadata":{},"source":"from sklearn import *\ndef RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":30},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)","execution_count":31},{"outputs":[],"cell_type":"code","metadata":{},"source":"import lightgbm as lgbm\n\nparams = {\n        'boosting_type': 'gbdt', 'objective': 'regression', 'nthread': -1, 'verbose': 1,\n        'num_leaves': 700, 'learning_rate': 0.02, 'max_depth': -1,\n        'subsample': 0.8, 'subsample_freq': 1, 'colsample_bytree': 0.6,\n        'reg_alpha': 1, 'reg_lambda': 0.001, 'metric': 'rmse',\n        'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 20, 'scale_pos_weight': 1}\n    \n#kf = KFold(n_splits=5, shuffle=True, random_state=seed_val)\n# pred_test_y = np.zeros(test[col].shape[0])\n\ntrain_set = lgbm.Dataset(train[col], np.log1p(train['visitors'].values), silent=True)\nmodel = lgbm.train(params, train_set=train_set, num_boost_round=100,feature_name=col)\nprint('RMSE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), model.predict(train[col])))","execution_count":32},{"outputs":[],"cell_type":"code","metadata":{},"source":"print('Feature names:', model.feature_name())\n\n# feature importances\nprint('Feature importances:', list(model.feature_importance()))\nmodel.save_model('model.txt')","execution_count":33},{"outputs":[],"cell_type":"code","metadata":{},"source":"test.head()\nprint (test.head())\ntrain.head()\nprint (train.head())\n\ny_pred = model.predict(test[col])\nsubmission = pd.read_csv('../input/sample_submission.csv')  # check where you place the submission.csv\nsubmission['visitors'] = np.asarray(y_pred)\nsubmission.to_csv(\"lightgbm_baseline.csv\",index=False)","execution_count":34},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"collapsed":true},"source":"","execution_count":null}],"nbformat_minor":1,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python","file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}}}