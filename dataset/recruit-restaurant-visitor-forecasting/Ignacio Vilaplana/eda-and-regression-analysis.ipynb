{"cells":[{"metadata":{"_cell_guid":"3c6a7d23-9756-43a9-af67-76af62f96d97","_uuid":"555897e4c690981c2dc0f035d47e426a29d8ef07"},"cell_type":"markdown","source":"# Restaurant visitors forecasting\n\n## Data\n\nAll the datasets are provided by the Kaggle competition \"Recruit Restaurant Visitor Forecasting\" (https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting).These are:\n\n- air_reserve.csv.csv\n- air_store_info.csv\n- air_visit_data.csv\n- date_info.csv\n- hpg_reserve.csv\n- hpg_store_info.csv\n- store_id_relation.csv\n\nThere's a final dataset, included for the purpose of defining a common format for prediction submission:\n\n- sample_submission.csv\n\n\n## Goal\n\nWe will try to perform an EDA of the provided data by loading, cleaning and merging the files, and finally visualizing the final dataframes. We'll also take advantage of the time series data provided to visualize an example, and finally we'll use different regression techniques to see how could we predict the visitors of a restaurant using only the regression information.\n\n\n## Index\n\n1- Importing libraries and loading the datasets\n\n2- Data preparation: Exploring, cleaning and merging datasets\n\n3- Visualizing the data\n\n4- Modeling the data\n\n5- Time series forecasting\n\n6- Generation of predictions and submission file"},{"metadata":{"_cell_guid":"7faf4649-9609-4da7-a64a-4a116d3e8201","_uuid":"bffc012f08e41c07fd1bb05bcf6c74b772aa731f"},"cell_type":"markdown","source":"### 1- Importing libraries and loading the datasets\n\n- Importing the necessary libraries"},{"execution_count":null,"metadata":{"_cell_guid":"12d6e0e3-b2aa-4f19-bcc4-e352ef3d9aa1","_uuid":"587730476eaee807105f7066736ceb7d9fb8a11c","collapsed":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport seaborn as sns\nimport pandas_profiling\nimport datetime\nimport sqlite3\nimport calendar\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython.display import display\npd.set_option('display.float_format', lambda x: '%.5f' % x)","outputs":[]},{"metadata":{"_cell_guid":"662d6c52-5c94-47fb-b408-485d9f50940c","_uuid":"f99e04a9995ea241f9d0f679ede51d0c8692512e"},"cell_type":"markdown","source":"- Loading the data files"},{"execution_count":null,"metadata":{"_cell_guid":"04bcbc98-ed32-4d93-86c8-408c1529865e","_uuid":"60652f84d0ed7371dc490508c09f41c26fd82160","collapsed":true},"cell_type":"code","source":"#We willl load all the csv files into Pandas dataframes, properly parsing dates\n\nair_reserve = pd.read_csv('../input/air_reserve.csv',parse_dates=['visit_datetime','reserve_datetime'])\nhpg_reserve = pd.read_csv('../input/hpg_reserve.csv',parse_dates=['visit_datetime','reserve_datetime'])\nair_store_info = pd.read_csv('../input/air_store_info.csv')\nhpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\nstore_relation = pd.read_csv('../input/store_id_relation.csv')\ndate_info = pd.read_csv('../input/date_info.csv',parse_dates=['calendar_date'])\nair_visit = pd.read_csv('../input/air_visit_data.csv',parse_dates=['visit_date'])\nsample_submission = pd.read_csv('../input/sample_submission.csv')","outputs":[]},{"metadata":{"_cell_guid":"8074d7dd-4e2d-4024-87e8-13a8ae936602","_uuid":"3cd3a9676bb00184a8aa66bded83b800f6731c08"},"cell_type":"markdown","source":"### Data preparation: Exploring, cleaning and merging datasets\n\n- Exploring the data\n\nWe will use here Pandas Profiling, as it is a nice way to understand the content of each file and all their important characteristics. It's going to be a little painful, since there are 7 files to profile, but it will definitely worth it."},{"execution_count":null,"metadata":{"_cell_guid":"1935ea03-f8c4-48cb-92b7-fa29bb7275be","_uuid":"bdd98d2adfe1c6b28fe5c6376f50af657ac885af","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(air_reserve)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"b53aac56-7f64-47ed-a5b9-e63c7e50ba30","_uuid":"0973a7c020efcbe4a9aaffe2dedd9e74e34996a5","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(hpg_reserve)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"780cb59b-33a4-4594-b2c8-7955bfe2e523","_uuid":"5a4ed8bcabee348212918883ca74171c0c408067","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(air_store_info)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"d8619229-5471-468c-bc65-c798866dbfa6","_uuid":"ff7c7a9ae2f1900df08f930c365774a4bfddd873","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(hpg_store_info)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"95145345-0f61-4373-a222-fcfe8057cfe8","_uuid":"4adacc2cb42adee365f9f13a7d65e83e65cb5215","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(store_relation)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"75580945-d5ee-4749-a4eb-2dad515f3f96","_uuid":"3fb1809f83c1c7a81164bdf6c5e9581b833b47db","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(date_info)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"37e2a3d3-fa88-4deb-a287-6bca0f1ce37e","_uuid":"d22bbd222c6b9a3db08c508b4f8748ba133056cd","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(air_visit)","outputs":[]},{"metadata":{"_cell_guid":"bac3a141-dbed-4f05-871b-a5bc7f9e24ad","_uuid":"607f40c9e5bd5ffdff87e01ea618792c604f7199"},"cell_type":"markdown","source":"- Cleaning the data\n\nAs we can see from the Pandas profiles above, the data is so clean that fortunately it will take low to no work cleaning it up. None of the files have missing data and only two files have duplicates (air_reserve & hpg_reserve), which are considered to be valid duplicates since there can be multiple reserves records for the same restaurant, day and hour, made in the same hour range. We'll just need to downsample those to daily reserves to get rid of the duplicates.\n\nOne thing we can do is check is if there are reserves made after the visit time, which shouldn't be possible."},{"execution_count":null,"metadata":{"_cell_guid":"d97c3409-c705-4738-85d0-3f6f3a40120d","_uuid":"b9a72663451484939534043cca0d63379b03b351"},"cell_type":"code","source":"print((air_reserve['reserve_datetime']>air_reserve['visit_datetime']).value_counts())\n\nprint((hpg_reserve['reserve_datetime']>hpg_reserve['visit_datetime']).value_counts())","outputs":[]},{"metadata":{"_cell_guid":"8e9ce210-de6f-4578-beb4-fbb9da87efbc","_uuid":"ccb26790b8c633b6ae02ea3e5d3425c55bf24bc4"},"cell_type":"markdown","source":"We can see that there are no trues in the above statement, so no errors were found.\n\nNow, it would be usefull to translate all the hpg ids to air ids, to have a common ground before merging dataframes. To get that, we need to downsample the reserves in the hpg system in order to have a single row when we do the merge with the air system."},{"execution_count":null,"metadata":{"_cell_guid":"596a9973-6485-48cf-9ac1-8f4359dadf06","_uuid":"cdbfd3c9c113a975ca9e1e6b65270f0790d6327f","collapsed":true},"cell_type":"code","source":"hpg_reserve['visit_year'] = hpg_reserve['visit_datetime'].dt.year\nhpg_reserve['visit_month'] = hpg_reserve['visit_datetime'].dt.month\nhpg_reserve['visit_day'] = hpg_reserve['visit_datetime'].dt.day\nhpg_reserve['reserve_year'] = hpg_reserve['reserve_datetime'].dt.year\nhpg_reserve['reserve_month'] = hpg_reserve['reserve_datetime'].dt.month\nhpg_reserve['reserve_day'] = hpg_reserve['reserve_datetime'].dt.day\n\nhpg_reserve.drop(['visit_datetime','reserve_datetime'], axis=1, inplace=True)\n\nhpg_reserve = hpg_reserve.groupby(['hpg_store_id', 'visit_year', 'visit_month',\\\n                                   'visit_day','reserve_year','reserve_month','reserve_day'], as_index=False).sum()","outputs":[]},{"metadata":{"_cell_guid":"19d5127f-c043-4f9e-9920-54a777caaef4","_uuid":"142fc39858f8700fb26b41a6d17c9da8ebeee1fa"},"cell_type":"markdown","source":"We should also prepair the rest of the files to get merged by visit day."},{"execution_count":null,"metadata":{"_cell_guid":"730b55a1-1f4a-42bd-a868-9f601a4832c0","_uuid":"b4b5ec778252fb6bc34039a81a1eda1a7dec5621","collapsed":true},"cell_type":"code","source":"air_reserve['visit_year'] = air_reserve['visit_datetime'].dt.year\nair_reserve['visit_month'] = air_reserve['visit_datetime'].dt.month\nair_reserve['visit_day'] = air_reserve['visit_datetime'].dt.day\nair_reserve['reserve_year'] = air_reserve['reserve_datetime'].dt.year\nair_reserve['reserve_month'] = air_reserve['reserve_datetime'].dt.month\nair_reserve['reserve_day'] = air_reserve['reserve_datetime'].dt.day\n\nair_reserve.drop(['visit_datetime','reserve_datetime'], axis=1, inplace=True)\n\ndate_info['calendar_year'] = date_info['calendar_date'].dt.year\ndate_info['calendar_month'] = date_info['calendar_date'].dt.month\ndate_info['calendar_day'] = date_info['calendar_date'].dt.day\n\ndate_info.drop(['calendar_date'], axis=1, inplace=True)\n\nair_visit['visit_year'] = air_visit['visit_date'].dt.year\nair_visit['visit_month'] = air_visit['visit_date'].dt.month\nair_visit['visit_day'] = air_visit['visit_date'].dt.day\n\nair_visit.drop(['visit_date'], axis=1, inplace=True)","outputs":[]},{"metadata":{"_cell_guid":"64535d7c-bfc4-4cff-946f-97a742e5a7ad","_uuid":"fb84c9bdabc7c6cac1ece1376fdf5e9eedcad700"},"cell_type":"markdown","source":"- Merging the data\n\nNow that the data is prepared to be merged, we need to add all the columns to the air_reserve file, as it is the file connected to all the rest of them by one or other way.\n\nFirst, we merge all the reserves from both systems into the air_reserve file."},{"execution_count":null,"metadata":{"_cell_guid":"5020b1f5-d5e6-44e9-bc75-1ba5441470ff","_uuid":"501afb9119c473055642be43cf2c208764f99f81","collapsed":true},"cell_type":"code","source":"hpg_reserve = pd.merge(hpg_reserve, store_relation, on='hpg_store_id', how='inner')\nhpg_reserve.drop(['hpg_store_id'], axis=1, inplace=True)\n\nair_reserve = pd.concat([air_reserve, hpg_reserve])","outputs":[]},{"metadata":{"_cell_guid":"52ebe1cd-8fdd-4205-976b-2b22ea571073","_uuid":"5c9ccf680689847ff96e0fdbcd78fc560bf456f0"},"cell_type":"markdown","source":"Now we can downsalmple to daily visit days, adding all the reserves made for that specific date."},{"execution_count":null,"metadata":{"_cell_guid":"3c27e26b-db64-4fb3-8180-e9e4500badd3","_uuid":"baccc95b2af7544e060f0834f85e7343b258fbf4","collapsed":true},"cell_type":"code","source":"air_reserve = air_reserve.groupby(['air_store_id', 'visit_year', 'visit_month','visit_day'],\\\n                as_index=False).sum().drop(['reserve_day','reserve_month','reserve_year'], axis=1)","outputs":[]},{"metadata":{"_cell_guid":"5b124f45-c7bc-435e-98cc-cb0f83f4d29f","_uuid":"50e713e5f37801410c4b29316d0920b2fcef079c"},"cell_type":"markdown","source":"We can easily add the holiday info to our dataframe."},{"execution_count":null,"metadata":{"_cell_guid":"9cd9be4f-d1ee-4771-ad14-d9b264a7e783","_uuid":"e57fdaee69275b5cb25fdfc57e189c4b6bbf4a7a","collapsed":true},"cell_type":"code","source":"air_reserve = pd.merge(air_reserve, date_info, left_on=['visit_year','visit_month','visit_day'], right_on=['calendar_year','calendar_month','calendar_day'], how='left')\nair_reserve.drop(['calendar_year','calendar_month','calendar_day'], axis=1, inplace=True)","outputs":[]},{"metadata":{"_cell_guid":"8683d32c-c44c-4416-8c30-de4df6a3f63a","_uuid":"513c7d9c3665a8373d3d571321a855127654b47d"},"cell_type":"markdown","source":"And merge also the store information and the restaurant visits per day. At this point, we're going to create a new dataframe, df, to mark the moment where we have all the data together."},{"execution_count":null,"metadata":{"_cell_guid":"fef05215-1805-4362-b9a2-df9847559185","_uuid":"43a69ca57b233cafc11e5d253bde1f3c2f329144","collapsed":true},"cell_type":"code","source":"air_reserve = pd.merge(air_reserve, air_store_info, on='air_store_id', how='left')\n\ndf = pd.merge(air_reserve, air_visit, on=['air_store_id','visit_year','visit_month','visit_day'], how='left')","outputs":[]},{"metadata":{"_cell_guid":"c60268f2-1309-43a3-b91f-5fd1c69d75b5","_uuid":"acc7e42d95a8bf2691f7187b3c652641b2aa5a99"},"cell_type":"markdown","source":"Let's see what is the result with this complete dataframe with a Pandas profile."},{"execution_count":null,"metadata":{"_cell_guid":"3d271b84-c690-4568-8d59-33db031927ca","_uuid":"63c5927c2499edcd37a135d4cb3eee74e5a0ef67","scrolled":true},"cell_type":"code","source":"pandas_profiling.ProfileReport(df)","outputs":[]},{"metadata":{"_cell_guid":"6024ff08-c344-4e16-9d23-288644e3aa13","_uuid":"43e9a706353701f5b89adcc910dc3b4319d925e3"},"cell_type":"markdown","source":"There missings values found in the visits variable will make our test dataset, as that is the variable that we want to predict. Hence, the rest of rows will be our train data. Let's create both train and test dataframes."},{"execution_count":null,"metadata":{"_cell_guid":"ed354641-2502-4294-b44a-62d8a288923c","_uuid":"132988a5e390550d16fb1affd683f207e3d5086f","collapsed":true},"cell_type":"code","source":"df.air_genre_name = df.air_genre_name.replace(' ', '_', regex=True)\ndf.air_genre_name = df.air_genre_name.replace('/', '_', regex=True)\ndf=df.rename(columns = {'air_genre_name':'genre','day_of_week':'dow'})\n\ndf.sort_values(by=['visit_year','visit_month','visit_day','air_store_id'],\\\n               ascending=[True,True,True,True], inplace=True)\n\ndata_train = df[df.visitors.notnull()]\ndata_test = df[df.visitors.isnull()]","outputs":[]},{"metadata":{"_cell_guid":"56665511-373f-4a7f-9612-9c42d9d17a8c","_uuid":"da7622e2b0edca7bd736e588ec22e427301f69a8"},"cell_type":"markdown","source":"We will create a new column with the natural logarithm of the visitor numbers, in case that helps us in the forecasting section later."},{"execution_count":null,"metadata":{"_cell_guid":"b0cb8e90-322d-427c-9a17-aa7b5f4967aa","_uuid":"1c9348641cc6f962d94f46f7d3f73db7a6b1f86b","collapsed":true},"cell_type":"code","source":"data_train['log_visitors'] = data_train.visitors.apply(lambda x: np.log(x))","outputs":[]},{"metadata":{"_cell_guid":"9d043574-2d9b-4445-9f0a-43fcee746799","_uuid":"f85ad727150b28372cb28ec5732bab1e4054cfe1"},"cell_type":"markdown","source":"### 3- Visualizing the data\n\n- We can start by visualizing violin plots of the visitors distribution for each day of the week, differentiating if it is a holiday or not."},{"execution_count":null,"metadata":{"_cell_guid":"43b4a1bf-fdf2-4e3e-ae97-f6d405a013d4","_uuid":"d751ded062dcfd7aa7d2126dac0837e1db20a0d7"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(14,12));\nax = sns.violinplot(x='dow', y=\"visitors\", hue='holiday_flg',data=df, palette=\"muted\", split=True)","outputs":[]},{"metadata":{"_cell_guid":"54598151-6cb5-47fc-9bb8-ae0abb516911","_uuid":"b9d95323bcfc38f48773c3a5c4881c5643d7186b"},"cell_type":"markdown","source":"We can see that, as expected, Monday through Thursday, the distribution of visitors is much lower than Friday to Sunday. Also, the holiday flag plays a big role in the visitor number, but that role seems to have a bigger effect on weekdays.\n\n- We should explore now the relationship between the reserve visitors and the actual visitors."},{"execution_count":null,"metadata":{"_cell_guid":"12750e6d-1465-467a-929f-77e82f6adc16","_uuid":"9a7c160f80920c4c22c08778b901a649b01392ef"},"cell_type":"code","source":"sns.jointplot(x='visitors', y='reserve_visitors', data=data_train, color='navy',\\\n              size=10, space=0, kind='reg',marginal_kws={'hist_kws': {'log': True}})","outputs":[]},{"metadata":{"_cell_guid":"0e8a2a4c-f231-4498-9790-dc92d1ce9d4a","_uuid":"6f2a5486f99a7b32992d163414c8cf847d171188"},"cell_type":"markdown","source":"There seems to be a strong relationship, with a p of 0, so we can reject the null hypothesis of both variables being independent, and a Pearson correlation coefficient of 0.42.\n\n- We should also inspect the visitors affluence to the restaurants depending on the month of the year."},{"execution_count":null,"metadata":{"_cell_guid":"3325fa94-112c-4a52-90c6-0d34f46aa6c3","_uuid":"ff3ce7a39a34dbf60b4fa62c09b1463ab9e8b8c9"},"cell_type":"code","source":"data_train_month = data_train[['visit_month','visitors','visit_year']].groupby(['visit_year','visit_month']).sum()\n\ndata_train_month.plot(kind =\"bar\", y='visitors')","outputs":[]},{"metadata":{"_cell_guid":"abddcc51-c710-44df-8369-b8fe35df50e1","_uuid":"34039f8cb4e0305d86bb5522e3f2aaa38e7821fe"},"cell_type":"markdown","source":"In the previous graph we can see every month in the dataset, as there are not so many months. We can see how this kind of graph could be skewed, as the visitors data has a big jump in November 2016 and it stays high until the end of the graph. This could be that there are more restaurants added to the database, or multiple other reasons. Let's check the month average."},{"execution_count":null,"metadata":{"_cell_guid":"70d86420-ea5f-4ee1-8e0d-ab22af6640a7","_uuid":"102c85feb97bb3cfcb5d15f62df01b6998571638"},"cell_type":"code","source":"data_train_month_av = data_train[['visit_month','visitors','visit_year']].groupby(['visit_month']).mean()\n\ndata_train_month_av.plot(kind =\"bar\", y='visitors')","outputs":[]},{"metadata":{"_cell_guid":"b809f7b9-480d-4c2a-b3fe-ff1bb9ba0f23","_uuid":"fa986f1e95ee1dd89f807391a276184e1bb7d651"},"cell_type":"markdown","source":"As we can see, the average of restaurant visitors for each month doesn't show those big jumps.\n\n- We've seen the strong relationship between reserve visitors and visitors. Is there any other strong correlation in the dataset?"},{"execution_count":null,"metadata":{"_cell_guid":"1a18dca3-e67c-4661-b570-e47097a24d88","_uuid":"8598bdbc48e09022a299762b14bcff51a1998a3c"},"cell_type":"code","source":"cor = data_train.corr()\nplt.figure(figsize=(14,3))\nsns.heatmap(cor.loc[['visitors'], list(df)[:-1]]);","outputs":[]},{"metadata":{"_cell_guid":"9b936144-28df-4c04-9c77-5113b67ba330","_uuid":"cac2805e423d2cfcf626f50c52c91292c65522ea"},"cell_type":"markdown","source":"We can see that the reserve visitors variable is the strongest with a great difference. After it, we could use holiday flag, visit day and visit month. We'll check on those later.\n\n- Finally, what is the evolution of visitors per day of the week for each month?"},{"execution_count":null,"metadata":{"_cell_guid":"640d632a-49cf-4c9f-ba75-975f3b8b3451","_uuid":"fc08f8447848dd612abf0b3581f0159cb4db426c"},"cell_type":"code","source":"data_train_Pivot = pd.pivot_table(data_train, values='visitors', columns='dow', index='visit_month')\ndata_train_Pivot.plot();\nplt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")","outputs":[]},{"metadata":{"_cell_guid":"766c50cc-ee45-4cf2-b090-5c7aabf7d8b3","_uuid":"4daad050a9a7f283e1b49b174c5600a70f1b889f"},"cell_type":"markdown","source":"### 4- Modeling the data\n\nNow that we have a clean dataframe and that we've inspected the variables and their relationship, let's start trying out some models to find out their behaviour.\n\n- First, we'll start by predicting always the average number of visitors of all restaurants. Any future model behaving worse than this one will be useless."},{"execution_count":null,"metadata":{"_cell_guid":"0dff6c9a-2e25-4894-ac87-539171b3f4c9","_uuid":"3ba495bcca0b950563cdb1fe10409b3a6754cf2a","collapsed":true},"cell_type":"code","source":"#Definition of the formula that will show the goodness of the model.\n\ndef RMSLE(predicted, actual):\n    msle = (np.log(predicted+1) - np.log(actual+1))**2\n    rmsle = np.sqrt(msle.sum()/msle.count())\n    return rmsle","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"46b7d21e-287d-415f-a55a-de403a5ceca7","_uuid":"a2d8c15d683264dec01b7b0d37b4059632d2b7b6"},"cell_type":"code","source":"data_train = pd.get_dummies(data_train, columns=['genre','dow'])\n\n#We will use the log of the visitors to get a more useful mean.\nmodel_mean_pred = data_train.log_visitors.mean()\n\n# And we'll store this value in the dataframe\ndata_train['visitors_mean'] = np.exp(model_mean_pred)\n\ndata_train.loc[:, ['visitors','visitors_mean']].plot(color=['#bbbbbb','r'], figsize=(16,8));","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"4075e184-e35d-4e1a-ba83-2d0f9e0eef1f","_uuid":"a4bb0b4a3061a8525c5f6e962bbaa27394d51f76"},"cell_type":"code","source":"model_mean_RMSLE = RMSLE(data_train.visitors_mean, data_train.visitors)\n\nresults_df = pd.DataFrame(columns=[\"Model\", \"RMSLE\"])\n\nresults_df.loc[0,\"Model\"] = \"Mean\"\nresults_df.loc[0,\"RMSLE\"] = model_mean_RMSLE\nresults_df.head()","outputs":[]},{"metadata":{"_cell_guid":"8805e9f2-4c25-4cf3-a10c-2791979ca969","_uuid":"768e736117322a6b0e57abf44cf3264bb5cb046d"},"cell_type":"markdown","source":"- Let's now see if and how much the model would enhance if we predicted always the mean number of visitors of the restaurant being predicted."},{"execution_count":null,"metadata":{"_cell_guid":"130b06c3-1219-4d33-acfd-d8281cb0370c","_uuid":"e98ca8b2395ce22b46cc30a43d6084b56a60cd08"},"cell_type":"code","source":"data_train = pd.merge(data_train, data_train[['air_store_id','visitors']].groupby(['air_store_id'], as_index=False).mean(), on='air_store_id', how='left')\n\ndata_train=data_train.rename(columns = {'visitors_y':'visitors_rest_mean','visitors_x':'visitors'})\n\nmodel_mean_rest_RMSLE = RMSLE(data_train.visitors_rest_mean, data_train.visitors)\n\nresults_df.loc[1,\"Model\"] = \"Mean_by_rest\"\nresults_df.loc[1,\"RMSLE\"] = model_mean_rest_RMSLE\nresults_df.head()","outputs":[]},{"metadata":{"_cell_guid":"7d26cc39-f727-49b5-a1db-436463d3d204","_uuid":"7252b397bd372f716da139a66db60a8b33aa2c27"},"cell_type":"markdown","source":"- Let's start creating the models with linear and polynomial regression. Starting with a model with multiple linear regressors, one for each variable in the data."},{"execution_count":null,"metadata":{"_cell_guid":"b38487e4-6cd5-4f85-9f3b-bf104c1191e9","_uuid":"4d44d9f62906fecc9eb80e3d8d0ce33bb8a036ed","scrolled":true},"cell_type":"code","source":"model = sm.OLS.from_formula('visitors ~ ' + '+'.join(data_train.columns.difference(['visitors', \\\n                            'log_visitors', 'air_store_id','visitors_mean'])), data_train)\nresult = model.fit()\nprint(result.summary())","outputs":[]},{"metadata":{"_cell_guid":"013e2825-4d2e-4a29-af08-f1ff6710709e","_uuid":"202c332f503462c20a474ad53ae3fd2039fa8439"},"cell_type":"markdown","source":"We can see how the null hypothesis of independence can't be rejected for none of the dummy variables (genres, areas and day of week), as can't be for latitude and longitude. However, the holiday flag, the reserve visitors and the visit date, as well as the own mean visitors number for the restaurant, help to get a better prediction."},{"execution_count":null,"metadata":{"_cell_guid":"7132762c-c521-4719-a0a7-4afb8c02234f","_uuid":"31ebb2848a4313321b8ac871cd3f0103ea862419"},"cell_type":"code","source":"data_train[\"linear_regr\"] = result.predict()\n\nmodel_lin_RMSLE = RMSLE(data_train.linear_regr, data_train.visitors)\n\nresults_df.loc[2,\"Model\"] = \"Multiple linear regressors\"\nresults_df.loc[2,\"RMSLE\"] = model_lin_RMSLE\nresults_df","outputs":[]},{"metadata":{"_cell_guid":"af2fa7a7-afb4-4ed6-bbc8-96290c5b5e48","_uuid":"a8212a13fca5a3b7f326b217a80ad436aeb54f96"},"cell_type":"markdown","source":"- We'll try and perform now some sort of random walk model: We'll just take the visitors of the restaurant from the previous similar day of the week, as this could be a good fit that includes seasonality for each restaurant. For that, we'll create 7 new columns containing the value of previous similar dow visitors and then create a new column, \"past_dow_visitors\", with the appropriate number for the specific day."},{"execution_count":null,"metadata":{"_cell_guid":"c6ba0174-2db1-4bba-a376-7a660663a905","_uuid":"de955c8f66c1c2225e3619338c8cabbd05538d01","collapsed":true},"cell_type":"code","source":"dows = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n\nfor dow in dows:\n    data_train['past_'+dow]= 0\n    \ndata_train.sort_values(by=['air_store_id','visit_year','visit_month','visit_day'], ascending=[True,True,True,True], inplace=True)\n\ndata_train['store_change'] = (data_train.air_store_id!=data_train.air_store_id.shift())\ndata_train['past_dow_visitors'] = data_train['visitors_rest_mean']\ndata_train.reset_index(drop=True, inplace=True)\n\nfor index, row in data_train.iterrows():\n    if not row.store_change:\n        for dow in dows:\n            if data_train.iloc[index-1, data_train.columns.get_loc('dow_'+dow)]:\n                data_train.set_value(index,'past_'+dow,data_train.iloc[index-1, data_train.columns.get_loc('visitors')])\n            else:\n                data_train.set_value(index,'past_'+dow,data_train.iloc[index-1, data_train.columns.get_loc('past_'+dow)])","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"04f8ab7e-9cd5-4654-ab9d-1ba9e2a5cf79","_uuid":"c5a8e187612fbe03fc7997ac38422b252486ef04","collapsed":true},"cell_type":"code","source":"for index, row in data_train.iterrows():\n    for dow in dows:\n        if row['dow_'+dow] and row['past_'+dow]>0:\n            data_train.set_value(index,'past_dow_visitors', row['past_'+dow])\n\nfor dow in dows:\n    data_train.drop(['past_'+dow], axis=1, inplace=True)","outputs":[]},{"metadata":{"_cell_guid":"4cca4ec1-58ee-41ec-844a-fc42a9821f5e","_uuid":"54e58b563ad7f4958a64e3a4ded72db0d95ff1bc"},"cell_type":"markdown","source":"The \"random walk\" model will include this new variable and the two other most powerful ones, the reserve visitors and wether if it's a holiday or not. We'll also include the intercept between the variables this time."},{"execution_count":null,"metadata":{"_cell_guid":"411d40c7-66e6-4c94-a6c0-6871703fb1fc","_uuid":"54d330d02d17c552c46c6893eb7370631471afef","scrolled":true},"cell_type":"code","source":"model = sm.OLS.from_formula('visitors ~ past_dow_visitors * reserve_visitors * holiday_flg',data_train)\nresult = model.fit()\nprint(result.summary())","outputs":[]},{"metadata":{"_cell_guid":"52db3cfc-c832-407a-978b-abd3f84a63f5","_uuid":"b1b71c24775f0eac8446285c848de832808482b5"},"cell_type":"markdown","source":"This time, all the variables have strong predictive power, being the newly created column of past day of week visitors the one with a higher t statistic (>100)"},{"execution_count":null,"metadata":{"_cell_guid":"b928550b-5bd6-491e-9882-d3a9f0f2e397","_uuid":"e64b8d566459b3a8d3190beb6f4a385f94828ef2"},"cell_type":"code","source":"model_pred = result.predict()\ndata_train['past_dow_predict'] = model_pred\n\nmodel_past_dow_RMSLE = RMSLE(data_train.past_dow_predict, data_train.visitors)\n\nresults_df.loc[3,\"Model\"] = \"Past_DoW\"\nresults_df.loc[3,\"RMSLE\"] = model_past_dow_RMSLE\nresults_df","outputs":[]},{"metadata":{"_cell_guid":"a8d220ac-cbbe-4000-a23b-5434197f201e","_uuid":"7fad8f725482de40ed375ea19cfbde4bfc13c8ce"},"cell_type":"markdown","source":"Nevertheless, this model does not outperform the multiple linear regressors obtained previously.\n\nResiduals:"},{"execution_count":null,"metadata":{"_cell_guid":"3d0bfbf4-f091-42fe-bc89-1517dc5221b9","_uuid":"f6a7b29b2fac087931669cc2e6c6a81d2f5b905f"},"cell_type":"code","source":"s_residuals = pd.Series(result.resid_pearson, name=\"S. Residuals\")\nfitted_values = pd.Series(result.fittedvalues, name=\"Fitted Values\")\nsns.regplot(fitted_values, s_residuals,  fit_reg=False)","outputs":[]},{"metadata":{"_cell_guid":"a5bdb7f1-c5b7-4a8c-be38-a21d612ee12c","_uuid":"948c0459fa19ce3e1ecb048599f10163e17a8057"},"cell_type":"markdown","source":"The residuals seem to be in a random distribution, and we can't observe a curvature in the data nor heteroskedasticity.\n\n- Let's create a more efficient model by using forward subsetting, using all the variables in the dataframe, including the newly created past dow visitors. Let's start by defining the needed functions."},{"execution_count":null,"metadata":{"_cell_guid":"5eb0ff83-03e4-46b0-b887-11df7e99f6ca","_uuid":"a660a6b107bac21ed30050391b326d1bf8f131a6","collapsed":true},"cell_type":"code","source":"def forward(predictors):\n    remaining_predictors = [p for p in X.columns if p not in predictors]    \n    results = []\n    \n    for p in remaining_predictors:\n        results.append(processSubset(predictors + [p]))\n    \n    models = pd.DataFrame(results)\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors.\")\n    return models.loc[models['RSS'].argmin()]\n\ndef processSubset(feature_set):\n    model = sm.OLS(y, X[list(feature_set)])\n    regr = model.fit()\n    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n    return {\"model\":regr, \"RSS\":RSS}","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"2b724a9c-324a-4fb5-a0aa-05b644a6e201","_uuid":"7bdcb14bfc47756a4780be8d121e8152a1ee3d4e"},"cell_type":"code","source":"models = pd.DataFrame(columns=[\"RSS\", \"model\"])\n\npredictors = []\ny=data_train.visitors\nX = data_train[['visit_year', 'visit_month', 'visit_day', 'reserve_visitors','holiday_flg','latitude','longitude',\\\n                'dow_Friday','dow_Monday','dow_Tuesday','dow_Wednesday','dow_Thursday','dow_Saturday','dow_Sunday',\\\n                 'visitors_rest_mean','past_dow_visitors']].astype('float64')\n\nfor i in range(1, len(X.columns) + 1):    \n    models.loc[i] = forward(predictors)\n    predictors = models.loc[i][\"model\"].model.exog_names","outputs":[]},{"metadata":{"_cell_guid":"5d034d7c-84af-483d-acaf-93132d3c9467","_uuid":"13892c713dd8a4fcf5988bc74c456bcc10bb31c4"},"cell_type":"markdown","source":"Let's inspect the correlation coefficient for each of the best possible models with the different number of predictors."},{"execution_count":null,"metadata":{"_cell_guid":"0fe387c7-3b91-4fc5-92c1-09641a266b4f","_uuid":"2fe0eef500edc257deeffc1b86683089c71aedba","scrolled":false},"cell_type":"code","source":"models.apply(lambda row: row[1].rsquared, axis=1)","outputs":[]},{"metadata":{"_cell_guid":"3fdb9d56-2e9c-4fb6-8bc7-5153a628b12e","_uuid":"555aa8fc6f49ef4b9deff9d274e9703c10a5f5a5"},"cell_type":"markdown","source":"Let's show some graphs to see how these models compare to each other."},{"execution_count":null,"metadata":{"_cell_guid":"34b3c3bd-dc34-4401-ab40-022e681b9884","_uuid":"fdd53d4ae1aed4011ab14d3df3576946c2fa51a6"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\nplt.subplot(4, 1, 1)\n\nplt.plot(models[\"RSS\"])\nplt.xlabel('# Predictors')\nplt.ylabel('RSS')\n\nrsquared_adj = models.apply(lambda row: row[1].rsquared_adj, axis=1)\n\nplt.subplot(4, 1, 2)\nplt.plot(rsquared_adj)\nplt.plot(rsquared_adj.argmax(), rsquared_adj.max(), \"ob\")\nplt.xlabel('# Predictors')\nplt.ylabel('adjusted rsquared')\n\naic = models.apply(lambda row: row[1].aic, axis=1)\n\nplt.subplot(4, 1, 3)\nplt.plot(aic)\nplt.plot(aic.argmin(), aic.min(), \"ob\")\nplt.xlabel('# Predictors')\nplt.ylabel('AIC')\n\nbic = models.apply(lambda row: row[1].bic, axis=1)\n\nplt.subplot(4, 1, 4)\nplt.plot(bic)\nplt.plot(bic.argmin(), bic.min(), \"ob\")\nplt.xlabel('# Predictors')\nplt.ylabel('BIC')","outputs":[]},{"metadata":{"_cell_guid":"870e61c9-eee9-49ba-8555-01b223d12792","_uuid":"6864f1ad8852d59ee24c49db3aad71c3b4412373"},"cell_type":"markdown","source":"We can see from the first graph that the RSS of the models is decreasing as the number of predictors increase. Also, from the second graph, we can see that the adjusted r sqared increases only up to a point of around 0.85, as we saw in the previous list, but it decreases sharply with all the predictors. The maximum adjusted r squared is marked with a point, as is the model with the lowest AIC and BIC.\n\nWe'll chose the model with 8 predictors to try to keep it simple, as after this point, the models only preform slightly better."},{"execution_count":null,"metadata":{"_cell_guid":"3f2cafc3-27c8-4e97-bfad-2f39998993bf","_uuid":"3b7dc938ddfa2fff3700402beb2cc63e2ae635ae"},"cell_type":"code","source":"data_train[\"subset_selection\"] = models.loc[8, \"model\"].predict()\nmodel_subset_RMSLE = RMSLE(data_train.subset_selection, data_train.visitors)\n\nresults_df.loc[4,\"Model\"] = \"Subset selection\"\nresults_df.loc[4,\"RMSLE\"] = model_subset_RMSLE\nresults_df","outputs":[]},{"metadata":{"_cell_guid":"1d0561fa-e37b-4418-a591-1374625cb3ba","_uuid":"0da757be369bb616df2cc71f26386e2db3b218ef"},"cell_type":"markdown","source":"This last model is the best up until now by RMSLE standards.\n\n- Let's try a polynomial regression model with the past dow visitors variable, as it is the one with the highest t statistic, up to a 5th degree polynomial."},{"execution_count":null,"metadata":{"_cell_guid":"87973db8-266f-4ab2-afd6-4b9e80cbeb19","_uuid":"2aeea8c05c29a694107ca33c350b22681e35a522","collapsed":true},"cell_type":"code","source":"poly_1 = smf.ols(formula='visitors ~ 1 + past_dow_visitors', data=data_train).fit()\n\npoly_2 = smf.ols(formula='visitors ~ 1 + past_dow_visitors + I(past_dow_visitors ** 2.0)', data=data_train).fit()\n\npoly_3 = smf.ols(formula='visitors ~ 1 + past_dow_visitors + I(past_dow_visitors ** 2.0) \\\n+ I(past_dow_visitors ** 3.0)', data=data_train).fit()\n\npoly_4 = smf.ols(formula='visitors ~ 1 + past_dow_visitors + I(past_dow_visitors ** 2.0) \\\n+ I(past_dow_visitors ** 3.0) + I(past_dow_visitors ** 4.0)', data=data_train).fit()\n\npoly_5 = smf.ols(formula='visitors ~ 1 + past_dow_visitors + I(past_dow_visitors ** 2.0) \\\n+ I(past_dow_visitors ** 3.0) + I(past_dow_visitors ** 4.0) + I(past_dow_visitors ** 5.0)', data=data_train).fit()","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"f3592cdc-0725-49a4-b731-f7d874378c74","_uuid":"bf5eda4929ea71302d9deb72c2c40fb25c313b8e"},"cell_type":"code","source":"print(sm.stats.anova_lm(poly_1, poly_2, poly_3, poly_4, poly_5, typ=1))","outputs":[]},{"metadata":{"_cell_guid":"814bbb4f-daa9-4573-9764-b37c76488e26","_uuid":"eb08ce25b114385ab4ae361c4690056148fe4d9d"},"cell_type":"markdown","source":"The model is ever increasing in goodness of fit, but actually doing so by just a little. Let's see it plotted to better understand it."},{"execution_count":null,"metadata":{"_cell_guid":"6fe9767d-1767-4d9a-aa14-2bd4c685ee4d","_uuid":"5177fe3a9a749ce2001f7b39a247831b47b107e4"},"cell_type":"code","source":"plt.figure(figsize=(6 * 1.618, 6))\nplt.scatter(data_train.past_dow_visitors, data_train.visitors, s=10, alpha=0.3)\nplt.xlabel('past_dow_visitors')\nplt.ylabel('visitors')\n\nx = pd.DataFrame({'past_dow_visitors': np.linspace(data_train.past_dow_visitors.min(), data_train.past_dow_visitors.max(), 100)})\nplt.plot(x.past_dow_visitors, poly_1.predict(x), 'b-', label='Poly n=1 $R^2$=%.2f' % poly_1.rsquared, alpha=0.9)\nplt.plot(x.past_dow_visitors, poly_2.predict(x), 'g-', label='Poly n=2 $R^2$=%.2f' % poly_2.rsquared, alpha=0.9)\nplt.plot(x.past_dow_visitors, poly_3.predict(x), 'r-', alpha=0.9,label='Poly n=3 $R^2$=%.2f' % poly_3.rsquared)\nplt.plot(x.past_dow_visitors, poly_4.predict(x), 'y-', alpha=0.9,label='Poly n=4 $R^2$=%.2f' % poly_4.rsquared)\nplt.plot(x.past_dow_visitors, poly_5.predict(x), 'k-', alpha=0.9,label='Poly n=5 $R^2$=%.2f' % poly_5.rsquared)\n\nplt.legend()","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"f28cf2fb-57e0-42a9-b45e-174a8353554d","_uuid":"0d7f32924d980d6bc362af0d2c393ce5388be9a1"},"cell_type":"code","source":"data_train[\"poly_regr\"] = poly_5.predict()\nmodel_poly_RMSLE = RMSLE(data_train.poly_regr, data_train.visitors)\n\nresults_df.loc[5,\"Model\"] = \"Polynomial Regressor\"\nresults_df.loc[5,\"RMSLE\"] = model_poly_RMSLE\nresults_df","outputs":[]},{"metadata":{"_cell_guid":"b771a09d-de91-4dd0-9c7f-8ce9c13a0e3b","_uuid":"6ef1f1c84dc208e2544be0c811a02af0c745ff5d"},"cell_type":"markdown","source":"The polynomial regression wasn't actually an improvement over the linear regression."},{"metadata":{"_cell_guid":"bbc8859b-4cb4-4cc6-aae3-0d87226c5351","_uuid":"766dd5d632a35a9e0e537e5738a2755b17ed7ab2"},"cell_type":"markdown","source":"### 5- Time series forecasting\n\nWe will use a single restaurant id (air_6b15edd1b4fbb96a) to evaluate it's time evolution data and use that to forecast the visitors.\n\n- Let's first explore the chosen id creating a time index."},{"execution_count":null,"metadata":{"_cell_guid":"6b5cf473-ecbe-443c-bfb1-c8c6acc3716d","_uuid":"0532ea4dee0f552ce39364470736845a7b697729"},"cell_type":"code","source":"df_time = data_train[data_train.air_store_id == 'air_6b15edd1b4fbb96a']\n\ndf_time.set_index(pd.to_datetime(df_time.visit_year*10000+df_time.visit_month*100\\\n                                 +df_time.visit_day,format='%Y%m%d'), inplace=True)\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(16, 7))\n\naxes[0].plot(df_time.visitors, color='navy', linewidth=4)\naxes[1].plot(df_time.visitors[df_time.visit_month > 10], color='navy', linewidth=4)","outputs":[]},{"metadata":{"_cell_guid":"b7f5ca04-1aa0-424b-b0b4-eb1a7832cfe1","_uuid":"7e31ed99a964729294d9071dffe7612136316e35"},"cell_type":"markdown","source":"We can see that there is a spike for each Saturday in the time series, and after the spike come the low values for Mondays through Wednesdays. The frequency of the series is hence weekly.\n\n- Let's inspect the shape of the visitors distribution for this restaurant."},{"execution_count":null,"metadata":{"_cell_guid":"0f55568f-1958-4dbe-9263-12e4f94fcf05","_uuid":"d2c6e44f96203ce3d59c61dd50fbab8b09232c98","scrolled":false},"cell_type":"code","source":"df_time.visitors.plot(kind = \"hist\", bins = 30)","outputs":[]},{"metadata":{"_cell_guid":"06980eb2-308e-4819-99a8-66527671bd59","_uuid":"a89267bb8e9be302f794e7ce4f67d541e83a326f"},"cell_type":"markdown","source":"We can see a skewed-right distribution, as there are no values under 0 visitors. Let's see the shape for the logarithmic visitor number."},{"execution_count":null,"metadata":{"_cell_guid":"a94fa7ef-2bf1-483f-80d3-ee8e7e1a2157","_uuid":"342cd4e6361e15ebb86ce36054f0e3797e2aad80"},"cell_type":"code","source":"df_time.log_visitors.plot(kind = \"hist\", bins = 30);","outputs":[]},{"metadata":{"_cell_guid":"325cca6b-947b-4efb-9b96-ddb040525b3c","_uuid":"ac57dae846bdcb5fbedc2e2339f354d9ec90a624"},"cell_type":"markdown","source":"Let's try now to forecast this time series using several methods.\n\n- First, let's see how the first model from the previous section would behave, the ones with the total average visitors and the specific restaurant average visitors."},{"execution_count":null,"metadata":{"_cell_guid":"b35a67a1-2a36-4adb-9b72-745c4f3a102b","_uuid":"e6f1ca7f4c8b254cbb7d49a46afc7722225b5baf"},"cell_type":"code","source":"model_mean_RMSLE = RMSLE(df_time.visitors_mean, df_time.visitors)\nmodel_rest_mean_RMSLE = RMSLE(df_time.visitors.mean(), df_time.visitors)\n\nresults_df_time = pd.DataFrame(columns=[\"Model\", \"RMSLE\"])\nresults_df_time.loc[0,\"Model\"] = \"Total Mean\"\nresults_df_time.loc[0,\"RMSLE\"] = model_mean_RMSLE\nresults_df_time.loc[1,\"Model\"] = \"Restaurant Mean\"\nresults_df_time.loc[1,\"RMSLE\"] = model_rest_mean_RMSLE\n\nresults_df_time","outputs":[]},{"metadata":{"_cell_guid":"d78bc59b-2460-49f3-a7f0-383f4f0f8433","_uuid":"9dd848b3fb6c09f9a2e990c05ecfc2f769962fbe"},"cell_type":"markdown","source":"This is in line with the numbers obtained in the previous section, so let's now do something new: Time Series Decomposition.\n\n- We will decompose the time series into trend and seasonality"},{"execution_count":null,"metadata":{"_cell_guid":"121d4d88-650d-4ea3-87c3-f139bf7c331d","_uuid":"d311c8d4305dbed261755c453ac6a9cd53bdd684"},"cell_type":"code","source":"decomposition = seasonal_decompose(df_time.log_visitors, model=\"additive\", freq=6)\ndecomposition.plot();","outputs":[]},{"metadata":{"_cell_guid":"21bd89d0-8ec2-4714-ad9a-b610f898a009","_uuid":"632f4b865094168ed97b7499f90a5bcd5a3b41ee"},"cell_type":"markdown","source":"Let's store this information into the dataframe and predict the visitors using them."},{"execution_count":null,"metadata":{"_cell_guid":"95c54d64-a64e-4671-86b6-c7a9881342af","_uuid":"5202fb54e8580e10d0e50aaecd5463e7fb64ff81","collapsed":true},"cell_type":"code","source":"trend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\ndf_time['power_decomp'] = np.exp(trend + seasonal)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"3f659d9c-960d-4b5c-b7be-63b53d95a8d2","_uuid":"d620383f3b7ea14e585db2dbdc0825804febd91f"},"cell_type":"code","source":"model_Decomp_RMSLE = RMSLE(df_time.power_decomp, df_time.visitors)\n\nresults_df_time.loc[2,\"Model\"] = \"Time Decomposition\"\nresults_df_time.loc[2,\"RMSLE\"] = model_Decomp_RMSLE\nresults_df_time","outputs":[]},{"metadata":{"_cell_guid":"93e89968-065f-4e96-afec-901901e09557","_uuid":"7593b017887333151b573e38c62497fee95e167f"},"cell_type":"markdown","source":"OK, this is not a bad number having seen the numbers from the previous section, but we still have to create the multiple linear regressors for this specific restaurant.\n\n- Let's start by forward subsetting the predictors."},{"execution_count":null,"metadata":{"_cell_guid":"49f67a12-4f99-475d-83db-1520105515e0","_uuid":"5126c2b4dfca07038f437a5c5ad423a36e7b4e7f"},"cell_type":"code","source":"models_time = pd.DataFrame(columns=[\"RSS\", \"model\"])\n\npredictors = []\ny=df_time.visitors\nX = df_time[['visit_year', 'visit_month', 'visit_day', 'reserve_visitors','holiday_flg','latitude','longitude',\\\n                'dow_Friday','dow_Monday','dow_Tuesday','dow_Wednesday','dow_Thursday','dow_Saturday','dow_Sunday',\\\n                 'visitors_rest_mean','past_dow_visitors']].astype('float64')\n\nfor i in range(1, len(X.columns) + 1):    \n    models_time.loc[i] = forward(predictors)\n    predictors = models_time.loc[i][\"model\"].model.exog_names","outputs":[]},{"metadata":{"_cell_guid":"ac2614df-189f-4f54-860a-acb25af2e7eb","_uuid":"052c588178e82965ce6632d3adcdb7525be281b4"},"cell_type":"markdown","source":"Again, let's plot them to choose an appropriate number of predictors."},{"execution_count":null,"metadata":{"_cell_guid":"11741c41-c058-4807-9cec-39fdd826e4ed","_uuid":"97b56b1ba4648af5a272c3ce24ddcadd32eb6ca4"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.rcParams.update({'font.size': 18, 'lines.markersize': 10})\nplt.subplot(4, 1, 1)\n\nplt.plot(models_time[\"RSS\"])\nplt.xlabel('# Predictors')\nplt.ylabel('RSS')\n\nrsquared_adj = models_time.apply(lambda row: row[1].rsquared_adj, axis=1)\n\nplt.subplot(4, 1, 2)\nplt.plot(rsquared_adj)\nplt.plot(rsquared_adj.argmax(), rsquared_adj.max(), \"ob\")\nplt.xlabel('# Predictors')\nplt.ylabel('adjusted rsquared')\n\naic = models_time.apply(lambda row: row[1].aic, axis=1)\n\nplt.subplot(4, 1, 3)\nplt.plot(aic)\nplt.plot(aic.argmin(), aic.min(), \"ob\")\nplt.xlabel('# Predictors')\nplt.ylabel('AIC')\n\nbic = models_time.apply(lambda row: row[1].bic, axis=1)\n\nplt.subplot(4, 1, 4)\nplt.plot(bic)\nplt.plot(bic.argmin(), bic.min(), \"ob\")\nplt.xlabel('# Predictors')\nplt.ylabel('BIC')","outputs":[]},{"metadata":{"_cell_guid":"2b1544b6-968a-4840-9eb8-ca5a89456b7b","_uuid":"313ae9f032280f8c548ad598b2c12a3da878973c"},"cell_type":"markdown","source":"Although the best adjusted r squared is obtained with 2 models, the RSS is still too high, so we'll chose 10 predictors as a compromise solution."},{"execution_count":null,"metadata":{"_cell_guid":"88d1aece-bc72-4720-b6e2-ad20934d71f7","_uuid":"a58cafd8d05560f4521418e6ee6befd2c620518c"},"cell_type":"code","source":"df_time[\"subset_selection\"] = models_time.loc[10, \"model\"].predict()\nmodel_subset_RMSLE = RMSLE(df_time.subset_selection, df_time.visitors)\n\nresults_df_time.loc[3,\"Model\"] = \"Subset selection\"\nresults_df_time.loc[3,\"RMSLE\"] = model_subset_RMSLE\nresults_df_time","outputs":[]},{"metadata":{"_cell_guid":"095b0ad8-4e35-4487-aca6-87ab5bde80c8","_uuid":"b36e0e427c10455cdd2cea78b7b47a721146a574"},"cell_type":"markdown","source":"This is a great improvement from the previous models.\n\n- Let's see now how a multiple linear regression model would perform."},{"execution_count":null,"metadata":{"_cell_guid":"fda4ed1f-c6f5-4ed5-b8e8-50a8ea8ba150","_uuid":"7762d07c7e4005ce776ae5858dc8e8f3d9d46ffd"},"cell_type":"code","source":"#We get rid of the genres, as they do not help making a better model\ndf_time.drop(list(df_time.filter(regex = 'genre_')), axis = 1, inplace = True)\ndf_time.dropna(axis=0,how='any',inplace=True)\n\nmodel = sm.OLS.from_formula('visitors ~ ' + '+'.join(df_time.columns.difference(['visitors', 'log_visitors',\\\n'air_store_id','visitors_mean', 'subset_selection','past_dow_predict','power_decomp','poly_regr'])), df_time)\n\nresult = model.fit()\nprint(result.summary())","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"7d1e89de-38a3-4b2e-9ccb-0062a4f293b4","_uuid":"80740e068e3599826fc0e05d04882cda9a383f0f"},"cell_type":"code","source":"df_time[\"linear_regr\"] = result.predict()\n\n# RMSLE for linear regressor\nmodel_lin_RMSLE = RMSLE(df_time.linear_regr, df_time.visitors)\n\nresults_df_time.loc[4,\"Model\"] = \"Linear Regressor\"\nresults_df_time.loc[4,\"RMSLE\"] = model_lin_RMSLE\nresults_df_time","outputs":[]},{"metadata":{"_cell_guid":"840239d8-2793-4d02-a5cc-3eeac03ce5ef","_uuid":"042bde119fbcb3725b26cf66ebb45d418a70b776"},"cell_type":"markdown","source":"So we have a winner. Compairing the multiple linear regressor RMSLE with the RMSLEs obtained in the previous section, we can conclude that having a model for each restaurant will improve the prediction of the visitors for that restaurant. \n\nThe only problem now is that not all restaurants in the test data have enough information in the train data, there are restaurants that are not even included in the train data, so we'll have to make just the best possible model for each group of them.\n\n- We'll start by creating a multiple linear regression model for each restaurant in the train data."},{"execution_count":null,"metadata":{"_cell_guid":"e3e5d09c-0c70-460a-91c2-cce41c6183ea","_uuid":"0514a14d73967d1008fcbffcd847529098c5b807","collapsed":true},"cell_type":"code","source":"#Let's get rid of the columns that won't be used in the final predictions.\ndata_train.drop(data_train[['air_area_name', 'latitude','past_dow_visitors','longitude','visitors_mean',\\\n'linear_regr','store_change','past_dow_predict','subset_selection','poly_regr','log_visitors']], axis=1, inplace=True)\ndata_train.drop(list(data_train.filter(regex = 'genre_')), axis = 1, inplace = True)","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"909eca77-7e4f-457a-8925-c4bc37100fc3","_uuid":"c5818270fb0705e3eec72c98fc21d6b05e655779","scrolled":false},"cell_type":"code","source":"restaurants = data_train.air_store_id.unique()\nRMSLEs = []\nmodels_dict = {}\n\nfor i,restaurant in enumerate(restaurants):\n    if i%100 == 0 or i==(len(restaurants)-1):\n        print(\"Model {} of {}\".format(i+1,len(restaurants)))\n        \n    df_temp = data_train[data_train.air_store_id == restaurant]\n    df_temp.dropna(axis=0,how='any',inplace=True)\n    model = sm.OLS.from_formula('visitors ~ ' + '+'.join(df_temp.columns.difference(['visitors',\\\n                                'air_store_id'])), df_temp).fit()\n    RMSLEs.append(RMSLE(model.predict(), df_temp.visitors))\n    models_dict[restaurant] = model","outputs":[]},{"metadata":{"_cell_guid":"fca3110c-af2b-4e92-94f3-76316d372b37","_uuid":"cd85a3d4fc25654fa83b9b7a8ff5566d973a590d"},"cell_type":"markdown","source":"We'll create now the models for the restaurants with no reserved visitors info, as this data is not complete for the forecasted weeks."},{"execution_count":null,"metadata":{"_cell_guid":"9ffb4b49-0369-4799-bbc7-4ec2f80e327c","_uuid":"bdc37ba746bdd3361bd9eee5a8d8a562dc39a9b4"},"cell_type":"code","source":"RMSLEhalf = []\nhalf_models_dict = {}\n\nfor i,restaurant in enumerate(restaurants):\n    if i%100 == 0 or i==(len(restaurants)-1):\n        print(\"Model {} of {}\".format(i+1,len(restaurants)))\n        \n    df_temp = data_train[data_train.air_store_id == restaurant]\n    df_temp.dropna(axis=0,how='any',inplace=True)\n    model = sm.OLS.from_formula('visitors ~ ' + '+'.join(df_temp.columns.difference(['visitors',\\\n                                'air_store_id','reserve_visitors'])), df_temp).fit()\n    RMSLEhalf.append(RMSLE(model.predict(), df_temp.visitors))\n    half_models_dict[restaurant] = model","outputs":[]},{"metadata":{"_cell_guid":"6a4f1197-7cb3-44a6-b36b-9cd4027af64c","_uuid":"f2aafe18ad3a78389bab06ae97dde4094cadf6c0"},"cell_type":"markdown","source":"And finally, a last model for those restaurants that are new in the test dataframe."},{"execution_count":null,"metadata":{"_cell_guid":"a958e788-fdbf-4c12-9060-8afd13454ea3","_uuid":"fb472c56ec315d9492a384991fdefb148821d511","collapsed":true},"cell_type":"code","source":"nodata_model = sm.OLS.from_formula('visitors ~ ' + '+'.join(data_train.columns.difference(['visitors',\\\n                                   'air_store_id','reserve_visitors','visitors_rest_mean'])), data_train).fit()\nRMSLE_rest = RMSLE(nodata_model.predict(), data_train.visitors)","outputs":[]},{"metadata":{"_cell_guid":"2610f177-ff9b-4ec5-a544-5df150053ccb","_uuid":"cc3e198da2f7c9b60b719cad2406733820d97467"},"cell_type":"markdown","source":"- Let's see how these newly created models compare with the ones obtained in the modeling section."},{"execution_count":null,"metadata":{"_cell_guid":"d146f163-f5b1-4c98-a415-78cb16e48bab","_uuid":"5b6ad485652951bf6c920136d81d33911098b368"},"cell_type":"code","source":"results_df.loc[6,\"Model\"] = \"Regressor per id\"\nresults_df.loc[6,\"RMSLE\"] = np.mean(RMSLEs)\nresults_df.loc[7,\"Model\"] = \"Regressor per id w/o reserves\"\nresults_df.loc[7,\"RMSLE\"] = np.mean(RMSLEs)\nresults_df.loc[8,\"Model\"] = \"New id model\"\nresults_df.loc[8,\"RMSLE\"] = RMSLE_rest\n\nresults_df","outputs":[]},{"metadata":{"_cell_guid":"fe167778-4495-4434-9d99-b2cfe58865b8","_uuid":"6446da4a083b40034662617cd3a06ee4b54edc45"},"cell_type":"markdown","source":"We can see that the models for the ids in the train data will perform much better than the model for the new restaurants that will appear in the test dataset."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","file_extension":".py","mimetype":"text/x-python","version":"3.6.3","pygments_lexer":"ipython3"}},"nbformat_minor":1,"nbformat":4}