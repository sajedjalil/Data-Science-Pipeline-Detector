{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a></a>\n# Introduction\n\nWhen someone opens a restaurant, their focus is likely on making high-quality food that will make their customers happy. However, this does not cover all the problems they encounter. How do they effectively schedule the staff? How do they know the quantity of ingredients to order? If restaurants cannot solve these problems, their business will be hurt. \n\nIf restaurants can predict how many visitors will be in one day, it's easier for them to make the arrangement. However, forecasting the number of visits is hard because it might be influenced by countless factors  (eg weather, holiday, and location). It's even harder for new restaurants with little historical data to make accurate predictions.\n\nWe are going to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be more efficient and allow them to focus on creating an enjoyable dining experience for their customers.\n\nThe first steps were to understand the main problem, get familiar with the structure of the data and decide what features we need. This is a Time-Series prediction, so we need to be careful about the sequence of the data. In any modeling process, the data that goes into a model plays a big role in ensuring accurate results. Therefore, the relevant features that helped achieve the objective were defined and an initial feature set was selected. \n\nFollowing this, some noisy and missing data was removed. We had access to data on restaurant reservations, but the dataset was incomplete and not of much use to us. Thus we didn't take this factor into account. Too many missing values might lead to a worse result even if it's a good predictor.\n\nSome fields’ values were imputed - for example, those of extreme actual visitor numbers on a specific day. After the data engineering, the core modeling process was started. Different algorithms were implemented on the feature set, along with cyclical addition and removal of features depending on performance and complexity of the features and the model used. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D\nimport scikitplot as skplt\nimport numpy as np\nimport pandas as pd\nimport datetime\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, LSTM, GRU, TimeDistributed, Input\nfrom keras.optimizers import SGD\n\nimport xgboost as xgb\nimport lightgbm as lgbm\n\nfrom sklearn import tree, neighbors, datasets, linear_model, svm, naive_bayes, ensemble, metrics, model_selection\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_validate,GridSearchCV\nfrom sklearn.utils.multiclass import unique_labels\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 999","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata_dir = '/kaggle/input/recruit-restaurant-visitor-forecasting/'\nair_visit = pd.read_csv(data_dir + 'air_visit_data.csv.zip')\nair_store_info = pd.read_csv(data_dir + 'air_store_info.csv.zip')\nhpg_reserve = pd.read_csv(data_dir + 'hpg_reserve.csv.zip')\nstore_id_relation = pd.read_csv(data_dir + 'store_id_relation.csv.zip')\nhpg_store_info = pd.read_csv(data_dir + 'hpg_store_info.csv.zip')\nair_reserve = pd.read_csv(data_dir + 'air_reserve.csv.zip')\ndate_info = pd.read_csv(data_dir + 'date_info.csv.zip')\n\n\n# This is the file that we submit with our results\nsubmission = pd.read_csv(data_dir + 'sample_submission.csv.zip')\nsubmission['air_store_id'] = submission['id'].str.slice(0, 20)\nsubmission['visit_date'] = submission['id'].str.slice(21)\nsubmission['is_test'] = True\nsubmission['visitors'] = np.nan\nsubmission['test_number'] = range(len(submission))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n**Our original file that includes our training and test data. This is the file we will be editing and adding on to.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"air_visit['id'] = np.nan\nair_visit['is_test'] = False\nair_visit['test_number'] = np.nan\nair_visit = air_visit[['id', 'visitors', 'air_store_id', 'visit_date', 'is_test','test_number']]\nair_visit = pd.concat([air_visit, submission])\n\n# We are combining the training data with the \"submission\" file\n# There is no test data, just the NaN with where our prediction will go\nair = pd.merge(air_store_info, store_id_relation, on='air_store_id', how='left')\nair_visit = pd.merge(air, air_visit, on='air_store_id')\nair_visit['visit_date'] = pd.to_datetime(air_visit['visit_date'])\nair_visit = air_visit.drop(columns=['air_genre_name', 'hpg_store_id'])\nair_visit.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n\n<a></a>\n# Feature Engineering\n\nThere are 3 parts in our feature engineering that we will cover:\n- 1. Time series info\n- 2. Japan specific info\n- 3. Store specific info\n\n\n## **Time Series**\n\n - **Prior Year Mapping**\n  One obvious solution to predicting visitors would be to look at how many customers came the year before. For example, if we are predicting for Jan 10, we might want to look at Jan 10th of the prior year.\n  However, this is a slight problem here: what if Jan 10th of the previous year fell on a Saturday, but this year it falls on a Monday?  To adjust this, we instead will take the number of visitors from the matching `week of the year` & the `day of the week` together.\n  After running our LightGBM model, this feature was in the top 5 in terms of importance.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prior year mapping - previous year Monday to this year Monday\nair_visit['prev_visitors'] = air_visit.groupby(\n    [air_visit['visit_date'].dt.week,\n     air_visit['visit_date'].dt.weekday])['visitors'].shift()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - **Day of week** - day of the week. Use numeric value to represent. If the day is weekend, it might have more traffic than normal weekdays.\n\n - **Month of the Year** - different months also has different volume. This variable is kind of similar to season which showcase the seasonality of the time in a year\n\n - **Seasons** - Japan has four distinct seasons: March to May is spring; June to August is summer; September to November is autumn; and December to February is winter. Each season has very different temperatures and climates which might affect the traffic. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# year / month / day_of_week\ndef seperate_date(data):\n    data['dow'] = data['visit_date'].dt.dayofweek\n    data['year'] = data['visit_date'].dt.year\n    data['month'] = data['visit_date'].dt.month\n    data['day'] = data['visit_date'].dt.day\n    return data\nair_visit = seperate_date(air_visit)\nair_visit['Weekend'] = np.where(air_visit['dow'] == (0,1), 1, 0)\n\n\n# Seasons\ndef seasonLabel(row):\n    if row['month'] in [3,4,5]:\n        return 'spring'\n    if row['month'] in [6,7,8]:\n        return 'summer'\n    if row['month'] in [9,10,11]:\n        return 'autumn'\n    if row['month'] in [12,1,2]:\n        return 'winter'\nair_visit[\"season\"] = air_visit.apply(lambda row:seasonLabel(row), axis=1) \nair_visit['summer_yes'] = np.where(air_visit['season'] == 'summer', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"\n\n## **Japan Specific**\n - **Holiday Status** - whether a day is holiday. This vairable can indicate the holiday flag. If it's holiday time, there might be higher traffic outside and restaurant might also have more number of visitors. Thus this shoud be a good predictor\n\n - **Next day holiday** - whether the next day is holiday. Days around holiday might also play a role in attracting visitors to restaurants. If the next day is holiday, people may be tried and don't hand out in the previous\n\n - **Previous day holiday** - Same to the above two predictors  \n\n - **Consecutive Holidays** - other than normal holiday flag data in the `data_info.csv`, we believe consective holidays and the length of days off-work also have a say in restaurant visiting patterns. For example, if the holiday is Friday, we will mark Friday, and the followed weekend with 3. Same goes when Monday is holiday and so on.   \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename the columns to make the column name match other dataset\ndate_info.rename(columns={'holiday_flg': 'is_holiday', 'calendar_date': 'visit_date'}, inplace=True)\n\n# previous days holiday flag. 1 means holiday, 0 means not\ndate_info['prev_day'] = date_info['is_holiday'].shift().fillna(0)\n\n# following days holiday flag, 1 means holiday, 0 means not\ndate_info['next_day'] = date_info['is_holiday'].shift(-1).fillna(0)\ndate_info['visit_date'] = pd.to_datetime(date_info['visit_date'])\nair_visit = pd.merge(air_visit, date_info, on='visit_date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" - **Days since 25th** - The next feature calculates how many days it has been since the previous 25th of the month. The 25th is special because this is when most Japanese people receive their monthly paycheck ([Japan Visa]([http://www.japanvisa.com/news/japan-payroll-%E2%80%93-introduction](http://www.japanvisa.com/news/japan-payroll-–-introduction))). In a country like the United States, this may not play too large of a role as people simply use a credit card. In Japan, however, people seem to be averse to debt and prefer cash over credit cards ([Business in Japan](https://blog.btrax.com/japanese-hold-as-many-credit-cards-as-americans-but-do-they-use-them/)). After running our LightGBM model, this feature was in the top 5 in terms of importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# days since 25th\nair_visit[\"dayofmonth\"] = air_visit[\"visit_date\"].dt.day    \nair_visit[\"daysinPrevmonth\"] = (air_visit[\"visit_date\"] - pd.DateOffset(months=1)).dt.daysinmonth \n\ndef daysToPrev25th(row):\n    TARGET_DATE = 25\n    if row['dayofmonth'] >= 25:\n        return row['dayofmonth'] - TARGET_DATE\n    else:\n        return row['daysinPrevmonth'] - TARGET_DATE + row['dayofmonth']\n\nair_visit[\"daysToPrev25th\"] = air_visit.apply(lambda row:daysToPrev25th(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n - **City** - We weren't provided with columns such as `region`, `city`, `neighorhood`, etc. Instead, we had one column that included 5 levels of detail. Our dataset was in English, but the cities retained their original Japanese symbols. We had longitude and latitude data from 2 different systems to 6 decimal places, so the numbers sometimes did not match up exactly. So we need to split the column and select the information we need.\n\n - **Population and Density** - To go along with this, we have also added the population for each of the cities. This information comes from both [Simple Maps](https://simplemaps.com/data/jp-cities) and verified with [Wikipedia](https://en.wikipedia.org/wiki/List_of_cities_in_Japan). \n  This is the dataset that we combined with our training set. We made sure that every city was included and that there were no missing values. \n  *Population is in millions, population density is in thousands.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting up locations\nair_df = pd.DataFrame(air_visit[\"air_area_name\"].str.split(' ', expand=True))\n\n# Naming the columns created from the split\nair_df.columns = ['air_geo1','air_geo2','air_geo3','air_geo4','air_geo5']\n\n# Only keeping relevant info from the split\nair_df = air_df[['air_geo1','air_geo2']]\n\n# Removing Japanese characters / symbols\nair_df['geo1'] = air_df['air_geo1'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\nair_df['geo2'] = air_df['air_geo2'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n\n# Joining back with original df\nair_visit = pd.concat([air_visit, air_df], axis=1)\nair_visit = air_visit.drop(columns=['air_geo1', 'air_geo2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Population and Density\n\ncities = ['Tokyo-to','Osaka-fu','Fukuoka-ken','Hyogo-ken','Hokkaido','Hiroshima-ken','Shizuoka-ken','Miyagi-ken','Niigata-ken','Osaka','Kanagawa-ken', 'Saitama-ken']\npopulation = [2.7, 1.2, 8.6, 1.1, 1.0, 1.2, 1.5,3.7, 1.5, .8, 2.6, 1.6]\ndensity = [11.9, 5.4, 13.9, 1.3, 1.3, 1.7, .5, 8.5,2.7, .7, 11.9, 4.4]\n\npop = pd.DataFrame(list(zip(cities, population, density)),\n            columns=['cities','population', 'density'])\n\nair_visit = pd.merge(air_visit, pop, left_on = 'geo1', right_on= 'cities', how='left')\nair_visit = air_visit.drop(columns=['cities', 'air_area_name'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Per restaurant visit features**\n\nNote that `visitors` does not exist in test set. Features such as locations and genres are categorical attributes and many missing values are involved. In this case, we need to create features on `visitors` , which will rely on the number of `visitors` to calculate. Therefore, we can feed visitors related features into our models to provide clues regarding stores. \n\nNote that each of the following are on a \"day of the week\" level for each store. For example if we are looking at a specific store, we only compare Mondays to other Mondays.\n\n - **max visitors**: maximum number of visitors for each store for a specific weekday\n - **min visitors**: minimum number of visitors for each store for a specific weekday\n - **average visitors**: average number of visitors for each store for a specific weekday\n - **count observations**: number of observations for each store in a spe**cific weekday. We take this feature into consideration because a number of stores don't open to business everyday\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# =============================================================================\n# Adding visitors related features\n# =============================================================================\n\n# Min\ntmp = air_visit.groupby(['air_store_id', 'dow'], as_index=False)['visitors'].min().rename(\n    columns={'visitors': 'min_visitors'})\nair_visit = pd.merge(air_visit, tmp, how='left', on=['air_store_id', 'dow'])\n\n# Avg\ntmp = air_visit.groupby(['air_store_id', 'dow'], as_index=False)['visitors'].mean().rename(\n    columns={'visitors': 'mean_visitors'})\nair_visit = pd.merge(air_visit, tmp, how='left', on=['air_store_id', 'dow'])\n\n# Median\ntmp = air_visit.groupby(['air_store_id', 'dow'], as_index=False)['visitors'].median().rename(\n     columns={'visitors': 'median_visitors'})\nair_visit = pd.merge(air_visit, tmp, how='left', on=['air_store_id', 'dow'])\n\n# Max\ntmp = air_visit.groupby(['air_store_id', 'dow'], as_index=False)['visitors'].max().rename(\n     columns={'visitors': 'max_visitors'})\nair_visit = pd.merge(air_visit, tmp, how='left', on=['air_store_id', 'dow'])\n\n# Count of groups of people\ntmp = air_visit.groupby(['air_store_id', 'dow'], as_index=False)['visitors'].count().rename(\n    columns={'visitors': 'count_observations'})\nair_visit = pd.merge(air_visit, tmp, how='left', on=['air_store_id', 'dow'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing some outliers from our training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/MaxHalford/kaggle-recruit-restaurant/blob/master/Solution.ipynb\n\ndef find_outliers(series):\n    return (series - series.mean()) > 2.4 * series.std()\n\ndef cap_values(series):\n    outliers = find_outliers(series)\n    max_val = series[~outliers].max()\n    series[outliers] = max_val\n    return series\n\n# Identify outliers\nstores = air_visit.groupby('air_store_id')\nair_visit['is_outlier'] = stores.apply(lambda g: find_outliers(g['visitors'])).values\nair_visit['visitors_capped'] = stores.apply(lambda g: cap_values(g['visitors'])).values\nair_visit['visitors_capped_log1p'] = np.log1p(air_visit['visitors_capped'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split up\nair_visit_train = air_visit[air_visit['is_test'] == False]\nair_visit_test = air_visit[air_visit['is_test'] == True]\n\n# Filter train\nair_visit_train = air_visit_train[air_visit_train['is_outlier'] == False]\nair_visit_train = air_visit_train[air_visit_train['visitors'] < 300]\n\n\n# Bring them back together, drop extra columns\nair_visit = pd.concat([air_visit_train, air_visit_test])\nair_visit = air_visit.drop(columns = ['is_outlier', 'visitors_capped', 'visitors_capped_log1p', 'geo2'])\nair_visit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is what our final training data looks like before we begin to deal with data types."},{"metadata":{"trusted":true},"cell_type":"code","source":"air_visit['visitors'] = air_visit['visitors'].astype(float)\nair_visit['visit_date'] = air_visit['visit_date'].astype(np.str)\nair_visit = pd.get_dummies(air_visit, columns=['season', 'dow', 'geo1', 'day_of_week'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = air_visit[air_visit.is_test == False]\ntest = air_visit[air_visit.is_test == True]\n\ntrain.index = train[['visit_date']] # , 'air_store_id'\ntrain_x = train.drop(columns = ['visitors'])\ntrain_y = train[['visitors']]\n\ntest.index = test[['visit_date']] # , 'air_store_id'\ntest_x = test.drop(columns = ['visitors'])\ntest_y = test[['visitors']]\n\ntrain_x = train_x.drop(columns=['air_store_id', 'id', 'visit_date'])\ntest_x = test_x.drop(columns=['air_store_id', 'id', 'visit_date'])\n\nprint(len(train_x.columns),len(test_x.columns))\ntrain_x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#LGBM\nnp.random.seed(42)\n\nmodel = lgbm.LGBMRegressor(\n    objective='regression',\n    max_depth=5,\n    num_leaves=5 ** 2 - 1,\n    learning_rate=0.007,\n    n_estimators=30000,\n    min_child_samples=80,\n    subsample=0.8,\n    colsample_bytree=1,\n    reg_alpha=0,\n    reg_lambda=0,\n    random_state=np.random.randint(10e6)\n)\n\nn_splits = 8\ncv = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\nval_scores = [0] * n_splits\n\nsub = submission['id'].to_frame()\nsub['visitors'] = 0\n\nfeature_importances = pd.DataFrame(index=train_x.columns)\n\nfor i, (fit_idx, val_idx) in enumerate(cv.split(train_x, train_y)):\n    \n    X_fit = train_x.iloc[fit_idx]\n    y_fit = train_y.iloc[fit_idx]\n    X_val = train_x.iloc[val_idx]\n    y_val = train_y.iloc[val_idx]\n    \n    model.fit(\n        X_fit,\n        y_fit,\n        eval_set=[(X_fit, y_fit), (X_val, y_val)],\n        eval_names=('fit', 'val'),\n        eval_metric='l2',\n        early_stopping_rounds=200,\n        feature_name=X_fit.columns.tolist(),\n        verbose=False\n    )\n    \n    val_scores[i] = np.sqrt(model.best_score_['val']['l2'])\n    sub['visitors'] += model.predict(test_x, num_iteration=model.best_iteration_)\n    feature_importances[i] = model.feature_importances_\n    \n    print('Fold {} RMSLE: {:.5f}'.format(i+1, val_scores[i]))\n    \nsub['visitors'] /= n_splits\nsub['visitors'] = np.expm1(sub['visitors'])\n\nval_mean = np.mean(val_scores)\nval_std = np.std(val_scores)\n\nprint('Local RMSLE: {:.5f} (±{:.5f})'.format(val_mean, val_std))\n\nfeature_importances.sort_values(0, ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM\n\nLSTM is a great solution for relatively short sequences, up to 100-300 items. On longer sequences LSTM still works, but can gradually forget information from the oldest items. In our dataset, the timeseries is up to 478 days long, so we decided to implement encoder and decoder to \"strengthen\" LSTM memory. \n\nWe are using the encoder and decoder structure in the **Sequence-to-sequence learning (Seq2Seq)**  concept, which is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n\nWe used one LSTM RNN layer as \"**encoder**\": it processes the input sequence and returns its own internal state. Note  that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n\nAnd then we used another RNN layer with 2 hidden LSTM model acts as \"**decoder**\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic tidying of the training and test table\n\n# Drop unnecessary columns\ntrain_df = train_df.drop(columns=[ 'population', 'reserve_visitors', 'days_diff', 'day', 'season'])\n\ntest = test.drop(columns=['population', 'reserve_visitors','days_diff', 'day', 'season'])\n# Refine column names\ntrain_df = train_df.rename({'visitors_x': 'visitors'}, axis = 1)\ntrain_df = train_df.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\ntrain_df = train_df.rename({'month_y': 'month'}, axis = 1)\ntrain_df = train_df.rename({'longitude_y': 'longitude'}, axis = 1)\ntrain_df = train_df.rename({'latitude_y': 'latitude'}, axis = 1)\ntest = test.rename({'latitude_y': 'latitude'}, axis = 1)\ntest = test.rename({'longitude_y': 'longitude'}, axis = 1)\ntest = test.rename({'month_y': 'month'}, axis = 1)\ntest = test.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\n\n# Clean unnecessary columns\ntrain_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\ntest = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n# Fill the cells of missing values with -1\ntrain_df = train_df.fillna(-1)\ntest = test.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are several categorical columns in the dataset, which are  'Food_Type', 'day_of_week', 'air_store_id' that needs to be transferred. One-hot encoding may provide better result, but we applied labels encoding to avoid high dimensional feature space. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encode categorical columns\n\n# Weekday\nle_weekday = LabelEncoder()\nle_weekday.fit(train_df['day_of_week'])\ntrain_df['day_of_week'] = le_weekday.transform(train_df['day_of_week'])\ntest['day_of_week'] = le_weekday.transform(test['day_of_week'])\n\n# id\nle_id = LabelEncoder()\nle_id.fit(train_df['air_store_id'])\ntrain_df['air_store_id'] = le_id.transform(train_df['air_store_id'])\ntest['air_store_id'] = le_id.transform(test['air_store_id'])\n\n# food type\nle_ftype = LabelEncoder()\nle_ftype.fit(train_df['Food_Type'])\ntrain_df['Food_Type'] = le_ftype.transform(train_df['Food_Type'])\ntest['Food_Type'] = le_ftype.transform(test['Food_Type'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering the input data structure the LSTM RNN model needed, we filled up all the dates within the whole time span (2016-01-01 ~ 2017-05-31) for each stores with number of visitors as 0 on those dates, and the time-independent features (food types, longitude, latitude, etc) are \"stretched\" to timeseries length. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simultaneous transformation of Train and test sets\n\n# combine train and test sets\nX_all = train_df.append(test)\n# date table (includes all dates for training and test period)\ndates = np.arange(np.datetime64(X_all.visit_date.min()),\n                  np.datetime64(X_all.visit_date.max()) + 1,\n                  datetime.timedelta(days=1))\nids = X_all['air_store_id'].unique()\ndates_all = dates.tolist()*len(ids)\nids_all = np.repeat(ids, len(dates.tolist())).tolist()\ndf_all = pd.DataFrame({\"air_store_id\": ids_all, \"visit_date\": dates_all})\ndf_all['visit_date'] = df_all['visit_date'].copy().apply(lambda x: str(x)[:10])\n\n# create copy of X_all with data relevant to 'visit_date'\nX_dates = X_all[['visit_date', 'year','month','week',\\\n                 'is_holiday','next_day','prev_day',\\\n                 'daysToPrev25th','day_of_week','Consecutive_holidays']].copy()\n\n# remove duplicates to avoid memory issues\nX_dates = X_dates.drop_duplicates('visit_date')\n\n# merge dataframe that represents all dates per each restaurant with information about each date\ndf_to_reshape = df_all.merge(X_dates,\n                             how = \"left\",\n                             left_on = 'visit_date',\n                             right_on = 'visit_date')\n\n# create copy of X_all with data relevant to 'air_store_id'\nX_stores = X_all[['air_store_id', 'Food_Type', 'latitude','longitude']].copy()       \n\n# remove duplicates to avoid memory issues\nX_stores = X_stores.drop_duplicates('air_store_id')\n\n# merge dataframe that represents all dates per each restaurant with information about each restaurant\ndf_to_reshape = df_to_reshape.merge(X_stores,\n                                    how = \"left\",\n                                    left_on = 'air_store_id',\n                                    right_on = 'air_store_id')\n# merge dataframe that represents all dates per each restaurant with inf. about each restaurant per specific date\ndf_to_reshape = df_to_reshape.merge(X_all[['air_store_id', 'visit_date',\\\n                                           'prev_visitors', 'mean_visitors',\\ \n                                       'median_visitors', 'max_visitors', \\\n                                           'min_visitors','count_observations'\\\n                                           ,'visitors']],\n                                    how = \"left\",\n                                    left_on = ['air_store_id', 'visit_date'],\n                                    right_on = ['air_store_id', 'visit_date'])\n\n# separate 'visitors' into output array\nY_lstm_df = df_to_reshape[['visit_date', 'air_store_id', 'visitors']].copy().fillna(0)\n\n# take log(y+1)\nY_lstm_df['visitors'] = np.log1p(Y_lstm_df['visitors'].values)\n\n# add flag for days when a restaurant was closed\ndf_to_reshape['closed_flag'] = np.where(df_to_reshape['visitors'].isnull() &\n                                       df_to_reshape['visit_date'].isin(train_df['visit_date']).values,1,0)\n\n# drop 'visitors' and from dataset\ndf_to_reshape = df_to_reshape.drop(['visitors'], axis = 1)\n\n# fill in NaN values\ndf_to_reshape = df_to_reshape.fillna(-1)\n\n# list of df_to_reshape columns without 'air_store_id' and 'visit_date'\ncolumns_list = [x for x in list(df_to_reshape.iloc[:,2:])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We bounded all numerical values between -1 and 1. To avoid data leakage 'fit' should be made on train data and 'transform' on train and test data in this case all data in test set is taken from train set, thus fit/transform on all data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler.fit(df_to_reshape[columns_list])\ndf_to_reshape[columns_list] = scaler.transform(df_to_reshape[columns_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape\n\n# reshape X into (samples, timesteps, features)\nX_all_lstm = df_to_reshape.values[:,2:].reshape(len(ids),\n                                                len(dates),\n                                                df_to_reshape.shape[1]-2)\n\n# isolate output for train set and reshape it for time series\nY_lstm_df = Y_lstm_df.loc[Y_lstm_df['visit_date'].isin(train_df['visit_date'].values) &\n                          Y_lstm_df['air_store_id'].isin(train_df['air_store_id'].values),]\nY_lstm = Y_lstm_df.values[:,2].reshape(len(train_df['air_store_id'].unique()),\n                                       len(train_df['visit_date'].unique()),\n                                       1)\n# test dates\nn_test_dates = len(test['visit_date'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two ways to split timeseries into training and validation datasets:\n\n1. Walk-forward split. This is not actually a split: we train on full dataset and validate on full dataset, using different timeframes. Timeframe for validation is shifted forward by one prediction interval relative to timeframe for training.\n2. Side-by-side split. This is traditional split model for mainstream machine learning. Dataset splits into independent parts, one part used strictly for training and another part used strictly for validation.\n\nWalk-forward is preferable, because it directly relates to the competition goal: predict future values using historical values. But this split consumes data points at the end of timeseries, thus making hard to train model to precisely predict the future.\n\nWe used validation (with walk-forward split) only for model tuning. Final model to predict future values was trained in blind mode, without any validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train test split\n\n# make additional features for number of visitors in t-1, t-2, ... t-7\nt_minus = np.ones([Y_lstm.shape[0],Y_lstm.shape[1],1])\nfor i in range(1,8):\n    temp = Y_lstm.copy()\n    temp[:, i:, :] = Y_lstm[:,0:-i,:].copy()\n    t_minus = np.concatenate((t_minus[...], temp[...]), axis = 2)\nt_minus = t_minus[:,:,1:]\nprint (\"t_minus shape\", t_minus.shape)\n\n\n# split X_all into training and test data\nX_lstm = X_all_lstm[:,:-n_test_dates,:]\nX_lstm_test = X_all_lstm[:,-n_test_dates:,:]\n\n# add t-1, t-2 ... t-7 visitors to feature vector\nX_lstm = np.concatenate((X_lstm[...], t_minus[...]), axis = 2)\n\n# split training set into train and validation sets\nX_tr = X_lstm[:,39:-140,:]\nY_tr = Y_lstm[:,39:-140,:]\n\nX_val = X_lstm[:,-140:,:]\nY_val = Y_lstm[:,-140:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The encoder takes input features of 39 days (t*1, t*2 … t39) and encode their hidden states through LSTM neural network. Then it pass the hidden states to decoder. Decoder use them with the features of 39 days shifted 1 day forward (t*2, t*3 … T40) to predict number of visitors per each of 829 restaurants in t_40.\n\nMethods used to address overfitting: we applied dropout and recurrent dropout regularization in all RNN layers and adjust the epoch size to prevent overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model\n\n# MODEL FOR ENCODER AND DECODER -------------------------------------------\nnum_encoder_tokens = X_lstm.shape[2]\nlatent_dim = 256 \n\n# encoder training\nencoder_inputs = Input(shape = (None, num_encoder_tokens))\nencoder = LSTM(latent_dim, \n               batch_input_shape = (1, None, num_encoder_tokens),\n               stateful = False,\n               return_sequences = True,\n               return_state = True,\n               recurrent_initializer = 'glorot_uniform')\n\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\nencoder_states = [state_h, state_c] # 'encoder_outputs' are ignored and only states are kept.\n\n# Decoder training, using 'encoder_states' as initial state.\ndecoder_inputs = Input(shape=(None, num_encoder_tokens))\n\ndecoder_lstm_1 = LSTM(latent_dim,\n                      batch_input_shape = (1, None, num_encoder_tokens),\n                      stateful = False,\n                      return_sequences = True,\n                      return_state = False,\n                      dropout = 0.4,\n                      recurrent_dropout = 0.4) # True\n\ndecoder_lstm_2 = LSTM(128, \n                     stateful = False,\n                     return_sequences = True,\n                     return_state = True,\n                     dropout = 0.4,\n                     recurrent_dropout = 0.4)\n\ndecoder_outputs, _, _ = decoder_lstm_2(\n    decoder_lstm_1(decoder_inputs, initial_state = encoder_states))\n\ndecoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# training model\ntraining_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\ntraining_model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# GENERATOR APPLIED TO FEED ENCODER AND DECODER ---------------------------\n# generator that randomly creates times series of 39 consecutive days\n# theses time series has following 3d shape: 829 restaurants * 39 days * num_features \ndef dec_enc_n_days_gen(X_3d, Y_3d, length):\n    while 1:\n        decoder_boundary = X_3d.shape[1] - length - 1\n        \n        encoder_start = np.random.randint(0, decoder_boundary)\n        encoder_end = encoder_start + length\n        \n        decoder_start = encoder_start + 1\n        decoder_end = encoder_end + 1\n        \n        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n        \n        yield([X_to_conc,\n               X_to_decode],\n               Y_decoder)\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our  generator that randomly creates times series of 39 consecutive days. And those time series has following 3-D shape: 829 restaurants * 39 days * num_features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generator\ndef dec_enc_n_days_gen(X_3d, Y_3d, length):\n    while 1:\n        decoder_boundary = X_3d.shape[1] - length - 1\n        \n        encoder_start = np.random.randint(0, decoder_boundary)\n        encoder_end = encoder_start + length\n        \n        decoder_start = encoder_start + 1\n        decoder_end = encoder_end + 1\n        \n        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n        \n        yield([X_to_conc,\n               X_to_decode],\n               Y_decoder)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training on X_tr/Y_tr and validate with X_val/Y_val. To perform validation training on validation data should be made instead of training on full data set. Then validation check is made on period outside of training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntraining_model.fit_generator(dec_enc_n_days_gen(X_tr, Y_tr, 39),\n                             validation_data = dec_enc_n_days_gen(X_val, Y_val, 39),\n                             steps_per_epoch = X_lstm.shape[0],\n                             validation_steps = X_val.shape[0],\n                             verbose = 1,\n                             epochs = 1)\n'''\n\n# Training on full dataset\ntraining_model.fit_generator(dec_enc_n_days_gen(X_lstm[:,:,:], Y_lstm[:,:,:], 39),\n                            steps_per_epoch = X_lstm[:,:,:].shape[0],\n                            verbose = 1,\n                            epochs = 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function takes 39 days before first prediction day (input_seq), then using encoder to identify hidden states for these 39 days. Next, decoder takes hidden states provided by encoder, and predicts number of visitors from day 2 to day 40. Day 40 is the first day of target_seq.\n\nPredicted value for day 40 is appended to features of day 41. Then function takes period from day 2 to day 40 and repeat the process unil all days in target sequence get their predictions. \n\nThe output of the function is the vector with predictions that has following shape: 820 restaurants * 39 days * 1 predicted visitors amount"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_sequence(inf_enc, inf_dec, input_seq, Y_input_seq, target_seq):\n    # state of input sequence produced by encoder\n    state = inf_enc.predict(input_seq)\n    \n    # restrict target sequence to the same shape as X_lstm_test\n    target_seq = target_seq[:,:, :X_lstm_test.shape[2]]\n    \n    \n    # create vector that contains y for previous 7 days\n    t_minus_seq = np.concatenate((Y_input_seq[:,-1:,:], input_seq[:,-1:, X_lstm_test.shape[2]:-1]), axis = 2)\n    \n    # current sequence that is going to be modified each iteration of the prediction loop\n    current_seq = input_seq.copy()\n    \n    \n    # predicting outputs\n    output = np.ones([target_seq.shape[0],1,1])\n    for i in range(target_seq.shape[1]):\n        # add visitors for previous 7 days into features of a new day\n        new_day_features = np.concatenate((target_seq[:,i:i+1,:], t_minus_seq[...]), axis = 2)\n        \n        # move prediction window one day forward\n        current_seq = np.concatenate((current_seq[:,1:,:], new_day_features[:,]), axis = 1)\n        \n        \n        # predict visitors amount\n        pred = inf_dec.predict([current_seq] + state)\n        \n        # update t_minus_seq\n        t_minus_seq = np.concatenate((pred[:,-1:,:], t_minus_seq[...]), axis = 2)\n        t_minus_seq = t_minus_seq[:,:,:-1]        \n        \n        # update predicitons list\n        output = np.concatenate((output[...], pred[:,-1:,:]), axis = 1)\n        \n        # update state\n        state = inf_enc.predict(current_seq)\n    \n    return output[:,1:,:]\n\n# inference encoder\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# inference decoder\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs,_,_ = decoder_lstm_2(decoder_lstm_1(decoder_inputs,\n                                                    initial_state = decoder_states_inputs))\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs,\n                      [decoder_outputs])\n\n# Predicting test values\nenc_dec_pred = predict_sequence(encoder_model,\n                                decoder_model,\n                                X_lstm[:,-X_lstm_test.shape[1]:,:],\n                                Y_lstm[:,-X_lstm_test.shape[1]:,:],\n                                X_lstm_test[:,:,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Business Insight"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manager intuition\n\n# Using the previous day\ndf['prev_day_visitors'] = df['visitors_x'].shift(1)\ndf = df.groupby('air_store_id').apply(lambda group: group.iloc[1:, ])\n\n# Using the same day of the previous week\ndf['prev_week_visitors'] = df.groupby([df['visit_date'].dt.weekday])['visitors_x'].shift()\ndf.groupby('air_store_id').apply(lambda group: group.iloc[7:, ])\n\n# Error\ndf['difference_decimal'] = abs(\n    df['visitors_x'] - df['prev_day_visitors']) / df['visitors_x']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normally we would provide recommendations in this section. Because we are only looking to sell our system in this project, we are instead providing reasons for why someone should buy our product.\n\nWe are looking to persuade a restaurant owner to purchase our product. For them, the decision ultimately comes down to: \"How much will these predictions actually help me out? I believe I know the business and have good intuition about how many people will come to the restaurant. Are their predictions any better than me? How much money will they save for me?\"\n\nTo quantify this, we decided to run a couple of simulations on our training data based on intuition to see how good our results would be. For reference, our daily percent error (using MAPE) was 50%.\n\n1) Using the previous day, the manager would be off by 113% on average. \n\n2) Using the same weekday from the previous week, the manager would be off by 94% on average.\n\n**Staffing**\n\nThe hourly minimum wage in Japan translates to roughly [8.30 USD](https://tradingeconomics.com/japan/minimum-wages). Overstaffing by 2 people for a given 8-hour day equates to roughly 130USD in unnecessary expenses. On the flip side, understaffing means a poor customer experience as wait time is longer. We believe this provides strong support for the purchase of our system.\n\n**Supply Chain** \n\nAlthough we are not able to quantify the supply chain as easily as we can with staffing costs, owners are able to reduce expenses if they have a more accurate picture of how many customers they expect to see. Purchasing too much leads to waste, and purchasing too little means running out of ingredients and making your customers upset. Using our system provides a more stable data-driven approach to this problem.\n\n**Smoothing Demand**\n\nAlthough not covered in our project, an individual will be able to look at their past data and gain an unbiased view of seasonality that occurs throughout the year. If they are looking for stability week-by-week or month-by-month to even out their supply purchases or keep the correct number of people on board, they can use their past data to aid in offering of incentives to drive customers where they see fit.\n\n**Important Features**\n\nAnother insight we can provide the manager is an understanding of the most important features that drives their business. In particular, we found the following 5 to be most important when we ran our LightGBM model:\n\n(1) Previous year mapping  \n(2) Days since previous 25th  \n(3) Holiday flag  \n(4) Aggregated visitors per day of week  \n(5) Exponential Moving Average"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}