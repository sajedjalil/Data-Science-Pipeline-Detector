{"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","version":"3.6.3","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1,"cells":[{"outputs":[],"execution_count":null,"cell_type":"code","source":"# based on: https://www.kaggle.com/the1owl/surprise-me/code\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_cell_guid":"a4c6baee-2808-4dee-baa8-6f5979890f52","_uuid":"a4bda34ebeeb78f4342979aaf05067e9890a4fcc"}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'), #need to create reservation features\n    'hr': pd.read_csv('../input/hpg_reserve.csv'), #need to create reservation features\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['tra'].head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['id'].head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"unique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) ","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"stores.head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":" stores.air_genre_name.value_counts()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"# stores[\"air_genre_name_0\"] = stores.air_genre_name.str.split(\"/\",expand=True)[0]","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"lbl = preprocessing.LabelEncoder()\n# stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n# stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) \ntest = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow']) \n","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.head()","metadata":{}},{"cell_type":"markdown","source":"## remaining data:","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['hs'].head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['hr'].head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['ar'].head()","metadata":{"scrolled":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['id'].head()","metadata":{"scrolled":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"data['id'].nunique()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.shape","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"# map IDs\n## https://stackoverflow.com/questions/36971661/python-pandas-map-using-2-columns-as-reference?noredirect=1&lq=1\n# train.set_index(\"air_store_id\").join(data[\"id\"]).tail()\n# train.set_index(\"air_store_id\").join(data[\"id\"].set_index([\"air_store_id\",\"hpg_store_id\"])).isnull().sum()\n\ntrain = train.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\"))\n# train.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\")).isnull().sum()\n# train.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\")).nunique()\ntest = test.set_index(\"air_store_id\").join(data[\"id\"].set_index(\"air_store_id\"))","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.head(3)","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.info()","metadata":{}},{"cell_type":"markdown","source":"### add a aniave regression model (doesn\\'t even have OHE for the ID or recent average. mainly just Day of week)\n* would be best to do temporal split but that's trickeier wit hcross val predict. for something this simple, who cares","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.reset_index().select_dtypes(['number']).iloc[:,1:7]","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"lr = linear_model.LinearRegression(normalize=True, n_jobs=-1)\n# lr.fit(train[col], np.log1p(train['visitors'].values))\n# lr.fit(train.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1), np.log1p(train['visitors'].values))","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train[\"pred_lr_naive\"] = model_selection.cross_val_predict(lr,train.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1),np.log1p(train['visitors'].values  ))","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"lr.fit(train.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1), np.log1p(train['visitors'].values))\n\ntest[\"pred_lr_naive\"] = lr.predict(test.reset_index().select_dtypes(['number']).iloc[:,1:7].fillna(-1))","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"test.head()","metadata":{}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"train.to_csv(\"train_partmerged_v1.csv.gz\",compression=\"gzip\")\ntest.to_csv(\"test_partmerged_v1.csv.gz\",compression=\"gzip\")","metadata":{"collapsed":true}},{"outputs":[],"execution_count":null,"cell_type":"code","source":"","metadata":{"collapsed":true}}]}