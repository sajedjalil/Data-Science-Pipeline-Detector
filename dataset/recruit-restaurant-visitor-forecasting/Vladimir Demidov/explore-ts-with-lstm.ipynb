{"cells":[{"source":"In this notebook I'll try the approach, which discovered in one tutorial about multivariate time series forecasting using LSTM.\n","cell_type":"markdown","metadata":{"_cell_guid":"8bd41f5e-8931-42ef-a440-072078a72531","_uuid":"b86dc252887403d87ab2d04cf965801ffc41a3b1"}},{"source":"import pandas as pd\nimport numpy as np\nnp.random.seed(10)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"725ec533-357d-45cb-ba91-6af034696543","_uuid":"e2058e19bb6a2b4a8bfdb0740a1b371092b0b476"},"execution_count":1},{"source":"## **Data Aggregation**\n\n\nFeatures from **the1owl**'s kernel https://www.kaggle.com/the1owl/surprise-me","cell_type":"markdown","metadata":{"_cell_guid":"5f9b6c6e-bc43-411e-a9d8-0ed4cb746074","_uuid":"604b6bd57b7c254f7014abb079667f5a350f2694"}},{"source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }\n\ndata['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"cac9e094-3b2f-471e-8bb2-5a6967bb3790","_uuid":"98e8cdbccba65e193d36603a77d483e00480e949","collapsed":true},"execution_count":2},{"source":"for df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date    \n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    data[df] = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"09486f7b-8b1f-47a3-b2b8-17fefc08ae0f","_uuid":"623d2a29ababd1a2ff8ade01bcec2af814d4f33b","collapsed":true},"execution_count":3},{"source":"data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"2eefb47d-8081-4741-8662-6b52235c1f92","_uuid":"e499470cea771207c9cea5543bf3ae8d5b69343e"},"execution_count":4},{"source":"unique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"31b66338-3871-4183-b342-292e64430f4f","_uuid":"50f7dab23a845ef129a48bf31ba5d66bd9327c60","collapsed":true},"execution_count":5},{"source":"stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \nlbl = LabelEncoder()\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) ","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"38da6714-9617-481a-a133-560319af5a6f","_uuid":"338259bf538ed869f0b9c7c7e2bc56d496980cfb","collapsed":true},"execution_count":6},{"source":"train = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) \ntest = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n    \ntrain = train.fillna(-1)\ntest = test.fillna(-1)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b40d2db7-d960-4b43-ae5c-5939e68befc4","_uuid":"2010505626053fbae1b852e34c32050cab9599b1","collapsed":true},"execution_count":7},{"source":"def RMSLE(y, pred):\n    return mean_squared_error(y, pred)**0.5","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6b5bb4d2-197e-4a1e-bd5b-37334c45137d","_uuid":"d57e38ca429843983116df1943e4468b192231bf","collapsed":true},"execution_count":8},{"source":"train.head()","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a2194f79-07c5-4184-8396-098ccea92a98","scrolled":true,"_uuid":"b3d392b9506abc14db46706fe84a86c589e0344f"},"execution_count":9},{"source":"# **Part 1** \n## **Visitors as a feature to fit LSTM**\n\nFunctions from https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras","cell_type":"markdown","metadata":{"_cell_guid":"7f9178b2-e59f-492b-b56f-98ef997dbd09","_uuid":"1a8a720ac5590e1097eb362cb8d0c0f77a201fa8"}},{"source":"### *Normalize feature*","cell_type":"markdown","metadata":{"_cell_guid":"4318487b-3c93-47eb-90cd-111fd903392d","_uuid":"e32336ef9ff1a8f4c8fd69e016113325bd24437f"}},{"source":"train = train.sort_values('visit_date')\nvalues = np.log1p(train['visitors'].values).reshape(-1,1)\nvalues = values.astype('float32')\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"efa3fe0a-771e-4d4b-9e57-80d888b1789d","_uuid":"62cddb6514b7153a906a804c40b66da627592525","collapsed":true},"execution_count":10},{"source":"### *Split into train and test sets*","cell_type":"markdown","metadata":{"_cell_guid":"14290506-1f5e-4fd6-bb25-06ff50a09146","_uuid":"ef66d00ed041acb0f254ae4fa0256274b005d6c3"}},{"source":"train_size = int(len(scaled) * 0.7)\ntest_size = len(scaled) - train_size\n\nV_train, V_test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\nprint(len(V_train), len(V_test))","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"51cc721e-6f40-4cdc-b03c-27abcb5ce5c7","_uuid":"e0593a65c332c528b40f9faf5c75d72e48e5421d"},"execution_count":11},{"source":"### *Convert an array of values into a dataset matrix*","cell_type":"markdown","metadata":{"_cell_guid":"01b4dea0-5c79-4145-8d69-dc5ca71cb342","_uuid":"5ea9b627b68aa452bee41448dbcaa9b98d255507"}},{"source":"def create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset) - look_back):\n        a = dataset[i:(i + look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    print(len(dataY))\n    return np.array(dataX), np.array(dataY)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c46462f1-6818-4d04-8f39-50b382990393","_uuid":"b1d03dd3506b5c48149f5aed24caa92d06d16465","collapsed":true},"execution_count":12},{"source":"### *Create dataset with look back*","cell_type":"markdown","metadata":{"_cell_guid":"4178114a-7e11-4302-885f-873781cf681b","_uuid":"85ad747825bf1ccf3ec2e87a4f4f76e0b29da18f"}},{"source":"look_back = 1\ntrainX, trainY = create_dataset(V_train, look_back)\ntestX, testY = create_dataset(V_test, look_back)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"74438c7a-45ea-4ef0-9973-e2d4f7935192","_uuid":"a068488ea3bef9affc7d1052f05bdb719a6383a5"},"execution_count":13},{"source":"### *Reshape X for model training*","cell_type":"markdown","metadata":{"_cell_guid":"1cb7592e-fe3d-468d-91d7-9622ff3b9e26","_uuid":"a150742a7687f1a4f70df6b0c45e2c7094037a9c"}},{"source":"trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"7a2e23fa-a184-4f09-be5e-07364b4f902e","_uuid":"c53740ea7194f31e45ca96ac464d79c18a9e9209","collapsed":true},"execution_count":14},{"source":" ### *Train LSTM with 3 epochs*","cell_type":"markdown","metadata":{"_cell_guid":"79e6cd0e-2a38-46c6-aa8c-3506fbe177ee","_uuid":"9a5ce2f4b30c1ad5992bd51ad997471682507a91"}},{"source":"model = Sequential()\nmodel.add(LSTM(4, input_shape=(trainX.shape[1], trainX.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\nhistory = model.fit(trainX, trainY, epochs=3, batch_size=100,\n                            validation_data=(testX, testY), verbose=1, shuffle=False) ","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"68511997-bb73-4b3b-96fc-91db08025f4c","_uuid":"f2063c1c8bc8f73d97840e437010d7c8f1760eef"},"execution_count":15},{"source":"### *Make prediction and apply invert scaling*","cell_type":"markdown","metadata":{"_cell_guid":"1e135933-aefb-482f-991c-fae28736125e","_uuid":"7b9a6cd42e0d82b105dd1244a06588a899014d3f"}},{"source":"yhat = model.predict(testX)\n\nyhat_inverse = scaler.inverse_transform(yhat.reshape(-1, 1))\ntestY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6bac9566-3ce3-45cd-9de3-680220c418b6","_uuid":"0e66c5594b0f96320d29ed724d32e052d953c059","collapsed":true},"execution_count":16},{"source":"### *RMSLE*","cell_type":"markdown","metadata":{"_cell_guid":"3d2cdd33-4cb1-4776-b5cc-9b8ab4829401","_uuid":"ef8133a54f36a3e21370c1e636a94918aaa9513b"}},{"source":"rmsle = RMSLE(testY_inverse, yhat_inverse)\nprint('Test RMSLE: %.3f' % rmsle)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"4d9d99f7-8f52-4518-9e39-f9306759e466","_uuid":"d7a0a497c8430e36cc97a075df0312ea8660a27c"},"execution_count":17},{"source":"# **Part 2**\n## **Multivariate Forecast**\n\nFunctions from https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras","cell_type":"markdown","metadata":{"_cell_guid":"addf84e4-7235-4ca6-b8a9-f631ade22fc1","_uuid":"00ef42aef7ecc896c47423ad1190116fc0345f13"}},{"source":"### *Using all features for model training*","cell_type":"markdown","metadata":{"_cell_guid":"38878ce6-98da-406d-ab3d-c5bee4e1d238","_uuid":"c48e420633e0bf91cde66d03ac57eafcf229a783"}},{"source":"train = train.sort_values('visit_date')\ntarget_train = np.log1p(train['visitors'].values)\n\ncol = [c for c in train if c not in ['id', 'air_store_id', 'visitors']]\n\ntrain = train[col]\ntrain.set_index('visit_date', inplace=True)\n\ntrain.head()","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6d7d4fc4-3526-47f2-9069-2783e7c43ff8","_uuid":"f8264be6aa6785f04a87a4ca8155281c840c7bdd"},"execution_count":18},{"source":"### *Function to convert series to supervised learning*","cell_type":"markdown","metadata":{"_cell_guid":"7b4fa080-c750-43fd-ab31-2dafb71ef0ed","_uuid":"22a7e3a4b35aba652f9e500675707d4561e4aa83"}},{"source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    \"\"\"\n    Frame a time series as a supervised learning dataset.\n    Arguments:\n        data: Sequence of observations as a list or NumPy array.\n        n_in: Number of lag observations as input (X).\n        n_out: Number of observations as output (y).\n        dropnan: Boolean whether or not to drop rows with NaN values.\n    Returns:\n        Pandas DataFrame of series framed for supervised learning.\n    \"\"\"\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # Forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"1ebfdd4c-24ad-4280-bbe7-63234d71bfc7","_uuid":"1690ee4ad571a4978b358daa6f9cbbb4be41fcd9","collapsed":true},"execution_count":19},{"source":"### *Normalize features*","cell_type":"markdown","metadata":{"_cell_guid":"64f79c05-b4aa-46a1-9efd-69db23447f48","_uuid":"dfb94265d258324c6cd183826da54afbfa89171d"}},{"source":"train['visitors'] = target_train\nvalues = train.values\nvalues = values.astype('float32')\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c4e4f56e-9f43-4ca2-88a8-28f7f22ec7ed","_uuid":"2b2b4c58989fd03bade2c8bd227602a1c8d7a5da","collapsed":true},"execution_count":20},{"source":"### *Frame as supervised learning*","cell_type":"markdown","metadata":{"_cell_guid":"a71d31a6-b169-4a2b-9b50-849bf3c34f24","_uuid":"3ca96959d0de395f6df36b4113e9f9762c0fa4d6"}},{"source":"reframed = series_to_supervised(scaled, 1, 1)\nreframed.head()","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a0c1f374-dad5-42fe-a33f-874b951e934e","_uuid":"e49faf36f97ff0bd128a1fa9fb2549b3d454b42b"},"execution_count":21},{"source":"### *Drop unncessary columns*","cell_type":"markdown","metadata":{"_cell_guid":"4527e2c0-c138-4d8a-a446-4927973ed61d","_uuid":"23891f7fb6d0b1362b463c945ade07c6d70c2403"}},{"source":"reframed.drop(reframed.columns[[i for i in range(17,33)]], axis=1, inplace=True)\nreframed.head()","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"283d2ccc-bb49-4cba-aa6f-8cc4eb220f91","scrolled":false,"_uuid":"a98b0b425675269218b81e9a27215996a0678b43"},"execution_count":22},{"source":"### *Split into train and test sets*","cell_type":"markdown","metadata":{"_cell_guid":"01e41387-5d43-4bc3-8399-07888fa69acd","_uuid":"9fdd0ba18aa6c252248ea216ee339988669515f4"}},{"source":"values = reframed.values\nn_train_days = int(len(values) * 0.7)\ntrain = values[:n_train_days, :]\ntest = values[n_train_days:, :]\n# Split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# Reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b79a3851-b0c5-4a0c-acaa-9436ed229439","_uuid":"c9dea01c0946587f54f6fdd0d2049bed41f0a907"},"execution_count":23},{"source":" ### *Train LSTM with 3 epochs*","cell_type":"markdown","metadata":{"_cell_guid":"9b37acff-e975-4c11-914a-9b8b090747ca","_uuid":"ba44e1fad5384246cc42831740c9e50b39322856"}},{"source":"multi_model = Sequential()\nmulti_model.add(LSTM(4, input_shape=(train_X.shape[1], train_X.shape[2])))\nmulti_model.add(Dense(1))\nmulti_model.compile(loss='mse', optimizer='adam')\nmulti_history = multi_model.fit(train_X, train_y, epochs=3,\n                                batch_size=100, validation_data=(test_X, test_y),\n                                verbose=1, shuffle=False)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"67593e30-d79a-469a-b373-387cd5bafe39","_uuid":"ab4349824bd3859cfb232e6e46de90fad4b42f2e"},"execution_count":24},{"source":"### *Make prediction*","cell_type":"markdown","metadata":{"_cell_guid":"ff2f075c-b0b9-4c3f-9587-83dd613d6e3f","_uuid":"ba033e07a4b21b9b89f493dc23924d4999afe150"}},{"source":"yhat = multi_model.predict(test_X)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a106cb5a-a304-4939-9642-8924cba32273","_uuid":"9bc02f954b5a61e508067f3cf9604d68182dde49","collapsed":true},"execution_count":25},{"source":"### *Apply invert scaling*","cell_type":"markdown","metadata":{"_cell_guid":"f3a9ab8f-1fff-4b70-876e-ec83a0dca450","_uuid":"0a6ed5c118a4a723152ec0674d8687567f697cf7"}},{"source":"test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n# Invert scaling for forecast\ninv_yhat = np.concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# Invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"abd97ae2-7c9c-48b2-ac5b-46a1ad3f1c10","_uuid":"279fb937c1d1462df8e3be8bea5aa4d6a8fb6017"},"execution_count":26},{"source":"### *RMSLE*","cell_type":"markdown","metadata":{"_cell_guid":"8a67ef42-bc43-4453-bf5a-75bddcec54a0","_uuid":"8c39ade35c980876f0aa094916a524571bc974cb"}},{"source":"rmsle = RMSLE(inv_y, inv_yhat)\nprint('Test RMSLE: %.3f' % rmsle)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"2e87a7ba-1b8d-496e-a5ee-cc64d25f8187","_uuid":"7781a4c953d7159899c1d2a1a77e9d55c7c1b8b3"},"execution_count":27},{"source":"Slight improve, not enough, however, to beat benchmarks. I would add that the LSTM may not be suited for autoregression type problems (at least with such set of features, window and LSTM-configuration) and that maybe better off exploring an MLP with a large window. But i think it's good demo how to fit neural network to a multivariate time series forecasting problem. \nSpecifically:\n\n* How to transform a raw dataset into something we can use for time series forecasting.\n* How to prepare data and fit an LSTM for a multivariate time series forecasting problem.\n* How to make a forecast and rescale the result back into the original units.","cell_type":"markdown","metadata":{}}],"nbformat":4,"metadata":{"language_info":{"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.3","name":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1}