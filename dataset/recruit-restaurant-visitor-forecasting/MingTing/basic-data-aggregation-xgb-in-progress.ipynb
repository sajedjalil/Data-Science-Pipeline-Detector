{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python"}},"cells":[{"metadata":{"_cell_guid":"3cde9785-0990-4a4c-81f5-587aa665127d","_uuid":"f5249e7a1a1b5f74219c3d567a50b0a2c5f437ff","collapsed":true},"outputs":[],"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, XGBClassifier, plot_importance\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\nair_reserve = pd.read_csv('../input/air_reserve.csv')\nair_store_info= pd.read_csv('../input/air_store_info.csv')\nair_visit_data = pd.read_csv('../input/air_visit_data.csv')\ndate_info = pd.read_csv('../input/date_info.csv')\nhpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\nstore_id_relation = pd.read_csv('../input/store_id_relation.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')\nhpg_reserve = pd.read_csv('../input/hpg_reserve.csv')\n\nair_combined = pd.merge(air_reserve, air_store_info, on='air_store_id', how='outer')\nhpg_combined = pd.merge(hpg_reserve, hpg_store_info, on='hpg_store_id', how='left')\n\ndf = store_id_relation.merge(hpg_combined, on='hpg_store_id', how='left')\ndf2 = air_combined.merge(df, on='air_store_id', how='left')","execution_count":1},{"metadata":{},"cell_type":"markdown","source":"## Combine air and hpg files together then merge on store_id_relation, 'air_store_id' is primary key."},{"metadata":{},"outputs":[],"cell_type":"code","source":"master_col = df2[['air_store_id']]\ndf2.head(5)","execution_count":3},{"metadata":{},"cell_type":"markdown","source":"## 11171047 entries in 'air_store_id', with 829 unique restaurants."},{"metadata":{},"outputs":[],"cell_type":"code","source":"print(df2.info())\nprint(len(df2['air_store_id'].value_counts()))","execution_count":5},{"metadata":{},"cell_type":"markdown","source":"## Add features to date_info"},{"metadata":{},"outputs":[],"cell_type":"code","source":"date_info['calendar_date'] = pd.to_datetime(date_info['calendar_date']).dt.date.astype(str)\n#date_info['holiday_flg'] = date_info['holiday_flg'].map({1: 'Yes', 0: 'No'})\ndate_info['MTWTF'] = date_info['day_of_week'].map({'Monday': 1, \n                                                   'Tuesday': 2, \n                                                   'Wednesday': 3, \n                                                   'Thursday': 4, \n                                                   'Friday': 5,\n                                                   'Saturday': 6, \n                                                   'Sunday': 7})\n\ndate_info['weekend_or_weekday'] = date_info['day_of_week'].map({'Monday': 0, \n                                                                'Tuesday': 0, \n                                                                'Wednesday': 0, \n                                                                'Thursday': 0, \n                                                                'Friday': 0,\n                                                                'Saturday': 1, \n                                                                'Sunday': 1})\ndate_info2 = date_info.drop(['day_of_week'], axis=1)\ndate_info2.head(5)","execution_count":7},{"metadata":{},"cell_type":"markdown","source":"## Join sample_submission with date_info"},{"metadata":{},"outputs":[],"cell_type":"code","source":"sub_store = sample_submission['id'].apply(lambda x: str(x).split('_', 2)[:2])\nsub_dates = pd.to_datetime(sample_submission['id'].apply(lambda x: str(x).split('_', 2)[2]).rename('Date'))\nsub_stores = pd.Series(['_'.join(x) for x in sub_store]).rename('air_store_id')\n\nsub_dt = pd.DataFrame({\n        'air_store_id': sub_stores,\n        'date': sub_dates.dt.date.astype(str),\n        #'year': sub_dates.dt.year,\n        'month': sub_dates.dt.month,\n        'day': sub_dates.dt.day })\n\nsub_df = pd.concat([sample_submission, sub_dt], axis=1)\nsub_df2 = sub_df.merge(date_info2, left_on= sub_df['date'], right_on=date_info['calendar_date'])\nsub_df3 = sub_df2.drop(['id', 'calendar_date', 'date'], axis =1)\nsub_df3.head(5)","execution_count":8},{"metadata":{},"cell_type":"markdown","source":"## Split datetime columns, and join with date_info"},{"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":"a = pd.to_datetime(df2['visit_datetime_x'])\n#b = pd.to_datetime(df2['reserve_datetime_x'])\n#c = pd.to_datetime(df2['visit_datetime_y'])\n#d = pd.to_datetime(df2['reserve_datetime_y'])\n\ndatetime_df =pd.DataFrame({\n        'air_visit_date': a.dt.date.astype(str),\n        #'year': a.dt.year,\n        'month': a.dt.month,\n        'day': a.dt.day,\n        \n        #'air_visit_hour': a.dt.hour,\n        #'air_visit_date': a.dt.date,\n        #'air_visit_year': b.dt.year,\n        \n        #'air_reserve_month': b.dt.month,\n        #'air_reserve_day': b.dt.day,\n        #'air_reserve_hour': b.dt.hour,\n         \n        #'hpg_visit_date': c.dt.date,\n        #'hpg_visit_year': c.dt.year,\n        #'hpg_visit_month': c.dt.month,\n        #'hpg_visit_day': c.dt.day,\n        #'hpg_visit_hour': c.dt.hour,\n        \n        #'hpg_reserve_date': d.dt.date,\n        #'hpg_reserve_year': d.dt.year,\n        #'hpg_reserve_month': d.dt.month,\n        #'hpg_reserve_day': d.dt.day,\n        #'hpg_reserve_hour': d.dt.hour        \n    }).fillna(0)\n\ndatetime_df2 = datetime_df.merge(date_info, left_on=datetime_df['air_visit_date'], \n                                 right_on=date_info['calendar_date'])","execution_count":9},{"metadata":{},"cell_type":"markdown","source":"## Encode all categorical variable, then join with sample_submission and training set"},{"metadata":{},"outputs":[],"cell_type":"code","source":"lbl = LabelEncoder()\ncategorical_df = pd.concat([master_col, df2[['air_genre_name','air_area_name','latitude_x','longitude_x',\n                      'hpg_genre_name','hpg_area_name']].fillna('None_Stated').apply(lbl.fit_transform)], axis=1)\n\n#categorical_dummy = pd.get_dummies(df2[['air_genre_name','air_area_name','latitude','longitude', 'hpg_genre_name','hpg_area_name']].fillna('None_Stated'))\n\nsub_df4 = sub_df3.merge(categorical_df, left_on=sub_df3['air_store_id'], right_on=categorical_df['air_store_id'])  \n\ncombined_visitors = pd.Series(df2['reserve_visitors_x'].fillna(0) + df2['reserve_visitors_y'].fillna(0)).rename('visitors')\n\n\ntrain_df = pd.concat([datetime_df2, combined_visitors, categorical_df], axis=1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Encode all air_store_id together, then drop all unneeded columns "},{"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":"lbl= LabelEncoder()\nids = lbl.fit_transform(feats['air_store_id'].append(sub_df4['air_store_id_x']))\ntrain_df['air_store_id_num'] = pd.Series(ids[:1171046])\nsub_df4['air_store_id_num'] = pd.Series(ids[1171047:])\n\ntrain_df2 = train_df.drop(['air_store_id', 'air_visit_date', 'calendar_date', 'day_of_week'], axis=1)\nsub_df5 = sub_df4.drop(['air_store_id_x', 'air_store_id_y'], axis=1).reindex(columns=list(train_df2.columns.values))","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##  Statistics of visitors grouped by store, and day of week."},{"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":"vis_store = train_df2['visitors'].groupby(train_df2['air_store_id_num']).describe()\nvis_dow = train_df2['visitors'].groupby(train_df2['MTWTF']).describe()\nvis_store_dow = train_df2['visitors'].groupby([train_df2['air_store_id_num'], train_df2['MTWTF']]).describe()","execution_count":3},{"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":"vis_store_dow.head(15)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## XGB Regressor without  tuning"},{"metadata":{"collapsed":true},"outputs":[],"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_df2, train_df2['visitors'], test_size=0.1, random_state=7)\n\nxgbR = XGBRegressor(learning_rate=0.1,\n                   objective='reg:linear')\nxgbR.fit(X_train, y_train)\n\ndef rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n\n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n\n\npred = xgbR.predict(X_test)\nmse = rmsle(pred, y_test)\nprint(mse)","execution_count":null}],"nbformat":4,"nbformat_minor":1}