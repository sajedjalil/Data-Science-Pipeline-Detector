{"nbformat_minor":1,"cells":[{"cell_type":"code","source":"import datetime\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3ad8b141-0c5a-4710-bd08-0fa04b707e6d","_uuid":"6ee9ce5c4dea2be04fc2a9c0ee6d7c36429a5e28","collapsed":true}},{"cell_type":"code","source":"def LeaveOneOut(data1, data2, groupcolumns, columnName, useLOO=False, cut=1, addNoise=False):\n    features = list([])\n    for a in groupcolumns:\n        features.append(a)\n    if(columnName is not None):\n        features.append(columnName)\n       \n    grpCount = data1.groupby(features)['visitors'].count().reset_index().rename(columns={'visitors': 'Count'})\n    if(useLOO):\n        grpCount = grpCount[(grpCount.Count > cut)]\n    grpMean = data1.groupby(features)['visitors'].mean().reset_index().rename(columns={'visitors': 'Mean'})\n    grpMedian = data1.groupby(features)['visitors'].median().reset_index().rename(columns={'visitors': 'Median'})\n    grpMin = data1.groupby(features)['visitors'].min().reset_index().rename(columns={'visitors': 'Min'})\n    grpMax = data1.groupby(features)['visitors'].max().reset_index().rename(columns={'visitors': 'Max'})\n    grpStd = data1.groupby(features)['visitors'].std().reset_index().rename(columns={'visitors': 'Std'})\n        \n    grpOutcomes = grpCount.merge(grpMean, on=features)\n    grpOutcomes = grpOutcomes.merge(grpMedian, on=features)\n    grpOutcomes = grpOutcomes.merge(grpMin, on=features)\n    grpOutcomes = grpOutcomes.merge(grpMax, on=features)\n    grpOutcomes = grpOutcomes.merge(grpStd, on=features)\n    \n    x = pd.merge(data2[features], grpOutcomes,\n                 suffixes=('x_', ''),\n                 how='left',\n                 on=features,\n                 left_index=True)[['Count','Mean','Median','Max','Min','Std']]\n    x['Outcomes'] = data2['visitors'].values\n    \n    if(useLOO):\n        nonnulls = (~x.Count.isnull())\n        x.loc[nonnulls,'Mean'] = ((x[nonnulls].Mean*x[nonnulls].Count)-x[nonnulls].Outcomes)\n        x.loc[nonnulls,'Median'] = ((x[nonnulls].Median*x[nonnulls].Count)-x[nonnulls].Outcomes)\n        if(addNoise is True):\n            x.loc[nonnulls&(x.Std>0),'Mean'] += np.random.normal(0,x[nonnulls&(x.Std>0)].Std,x[nonnulls&(x.Std>0)].shape[0])\n            x.loc[nonnulls&(x.Std>0),'Median'] += np.random.normal(0,x[nonnulls&(x.Std>0)].Std,x[nonnulls&(x.Std>0)].shape[0])\n        else:\n            x.loc[nonnulls,'Count'] -= 1\n        x.loc[nonnulls,'Mean'] /=  (x[nonnulls].Count)\n        x.loc[nonnulls,'Median'] /= (x[nonnulls].Count)\n    x.Count = np.log1p(x.Count)\n    x = x.replace(np.inf, np.nan)\n    x = x.replace(-np.inf, np.nan)\n    #x = x.fillna(x.mean()) \n    \n    return x[['Count','Mean','Median','Max','Min', 'Std']]\n\n\ndef MungeTrain():\n    air_visit_data = pd.read_csv('../input/air_visit_data.csv',parse_dates=['visit_date'])\n    air_store_info = pd.read_csv('../input/air_store_info.csv')\n    \n    hpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\n    hpg_store_info.drop(['latitude','longitude'],inplace=True,axis=1)\n    store_id_relation = pd.read_csv('../input/store_id_relation.csv')\n    storeinfo = air_store_info.merge(store_id_relation,on='air_store_id',how='left')\n    storeinfo = storeinfo.merge(hpg_store_info,on='hpg_store_id',how='left')\n    air_reserve = pd.read_csv('../input/air_reserve.csv',parse_dates=['visit_datetime'])\n    air_reserve['visit_date'] = air_reserve.visit_datetime.apply( lambda df : \n    datetime.datetime(year=df.year, month=df.month, day=df.day))\n    hpg_reserve =pd.read_csv('../input/hpg_reserve.csv',parse_dates=['visit_datetime'])\n    hpg_reserve['visit_date'] = hpg_reserve.visit_datetime.apply( lambda df : \n    datetime.datetime(year=df.year, month=df.month, day=df.day))\n    date_info = pd.read_csv('../input/date_info.csv',parse_dates=['calendar_date']).rename(columns={'calendar_date':'visit_date'})\n    air_reserve_by_date = air_reserve.groupby(['air_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n    hpg_reserve_by_date = hpg_reserve.groupby(['hpg_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n    \n    train = air_visit_data.merge(storeinfo,on='air_store_id',how='left')\n    train = train.merge(air_reserve_by_date,on=['air_store_id','visit_date'],how='left')\n    train = train.merge(hpg_reserve_by_date,on=['hpg_store_id','visit_date'],how='left')\n    train = train.merge(date_info,on='visit_date',how='left')\n    train['year'] = train.visit_date.dt.year\n    train['month'] = train.visit_date.dt.month\n    train.reserve_visitors_x = train.reserve_visitors_x.fillna(0)\n    train.reserve_visitors_y = train.reserve_visitors_y.fillna(0)\n    train.reserve_visitors_x = np.log1p(train.reserve_visitors_x)\n    train.reserve_visitors_y = np.log1p(train.reserve_visitors_y)\n    train.visitors = np.log1p(train.visitors)\n    #train.drop(['latitude','longitude'],inplace=True,axis=1)\n    train = train.fillna(-1)\n    train = train.sort_values(by=['visit_date','air_store_id'],ascending=False)\n    train = train.reset_index(drop=True)\n    return train\n\n\ndef MungeTest(columns):\n    air_visit_data = pd.read_csv('../input/sample_submission.csv')\n    air_visit_data['visit_date'] = air_visit_data.id.apply(lambda x: datetime.datetime(year=int(x[-10:-6]), month=int(x[-5:-3]), day=int(x[-2:])))\n    air_visit_data['air_store_id'] = air_visit_data.id.apply(lambda x: x[:-11])\n    \n    air_store_info = pd.read_csv('../input/air_store_info.csv')\n    hpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\n    hpg_store_info.drop(['latitude','longitude'],inplace=True,axis=1)\n    store_id_relation = pd.read_csv('../input/store_id_relation.csv')\n    storeinfo = air_store_info.merge(store_id_relation,on='air_store_id',how='left')\n    storeinfo = storeinfo.merge(hpg_store_info,on='hpg_store_id',how='left')\n    air_reserve = pd.read_csv('../input/air_reserve.csv',parse_dates=['visit_datetime'])\n    air_reserve['visit_date'] = air_reserve.visit_datetime.apply( lambda df : \n    datetime.datetime(year=df.year, month=df.month, day=df.day))\n    hpg_reserve =pd.read_csv('../input/hpg_reserve.csv',parse_dates=['visit_datetime'])\n    hpg_reserve['visit_date'] = hpg_reserve.visit_datetime.apply( lambda df : \n    datetime.datetime(year=df.year, month=df.month, day=df.day))\n    date_info = pd.read_csv('../input/date_info.csv',parse_dates=['calendar_date']).rename(columns={'calendar_date':'visit_date'})\n    air_reserve_by_date = air_reserve.groupby(['air_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n    hpg_reserve_by_date = hpg_reserve.groupby(['hpg_store_id','visit_date']).reserve_visitors.sum().reset_index(drop=False)\n    \n    test = air_visit_data.merge(storeinfo,on='air_store_id',how='left')\n    test = test.merge(air_reserve_by_date,on=['air_store_id','visit_date'],how='left')\n    test = test.merge(hpg_reserve_by_date,on=['hpg_store_id','visit_date'],how='left')\n    test = test.merge(date_info,on='visit_date',how='left')\n    test['year'] = test.visit_date.dt.year\n    test['month'] = test.visit_date.dt.month\n    test.reserve_visitors_x = test.reserve_visitors_x.fillna(0)\n    test.reserve_visitors_y = test.reserve_visitors_y.fillna(0)\n    test.reserve_visitors_x = np.log1p(test.reserve_visitors_x)\n    test.reserve_visitors_y = np.log1p(test.reserve_visitors_y)\n    test.visitors = np.log1p(test.visitors)\n    #test.drop(['latitude','longitude'],inplace=True,axis=1)\n    test = test.fillna(-1)\n    test = test.sort_values(by=['visit_date','air_store_id'],ascending=False)\n    test = test.reset_index(drop=True)\n    return test[list(['id'])+list(columns)]","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"374a6697-b6f9-44dc-b2ed-74cf492f43d1","_uuid":"4c22ee168e7a9aa24a18a487f4d0d58c3bfb1195","collapsed":true}},{"cell_type":"code","source":"train = MungeTrain()\ndelta = train.visit_date.max()-pd.Timedelta(weeks=5)\nlastfiveweekstrain = train[train.visit_date>=delta].copy()\nlastfiveweekstrain = lastfiveweekstrain.reset_index(drop=True)\ntrain = train[train.visit_date<delta].copy()\ntrain = train.reset_index(drop=True)\ntest = MungeTest(train.columns)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"86e06ff3-fa8e-4a9f-bd73-f7dcb4a3fa3c","_uuid":"70cb7c59ba22ef894084bead3feb5e4b8c12b1f2","collapsed":true}},{"cell_type":"code","source":"# Time to do predictions for 60 weeks total\ndeltavals = [1,2,3,4,5]\nxgbtrainpreds = None\nxgbtestpreds = None\nfor i,deltaweek in enumerate(deltavals):\n    print(i)\n    delta = pd.Timedelta(weeks=deltaweek)\n    blindmin = train.visit_date.max()-delta\n    blindmax = train.visit_date.max()\n    vismin = blindmin-delta-pd.Timedelta(days=1)\n    vismax = blindmax-delta-pd.Timedelta(days=1)\n    btrain = None\n\n    for x in range(int(60./deltaweek)):\n        vistrain = train[(train.visit_date<=vismax)].copy()\n        blindtrain = train[(train.visit_date>=blindmin)&(train.visit_date<=blindmax)].copy()\n        features = ['air_genre_name',\n                    'air_area_name', 'hpg_store_id',\n                    'hpg_genre_name', 'hpg_area_name', \n                    'day_of_week', 'holiday_flg']\n        for c in features:\n            blindtrain[c+'_Count_Store'] = np.nan\n            blindtrain[c+'_Mean_Store'] = np.nan\n            blindtrain[c+'_Median_Store'] = np.nan\n            blindtrain[c+'_Max_Store'] = np.nan\n            blindtrain[c+'_Min_Store'] = np.nan\n            blindtrain[c+'_Std_Store'] = np.nan\n\n\n            blindtrain[[c+'_Count_Store',c+'_Mean_Store',\n                        c+'_Median_Store',c+'_Max_Store',\n                        c+'_Min_Store', c+'_Std_Store']] =  LeaveOneOut(vistrain,\n                                                                        blindtrain,\n                                                                        list(['air_store_id']),\n                                                                        c,\n                                                                        useLOO=False,\n                                                                        cut=0).values\n\n        features = ['air_store_id',\n                    'air_genre_name',\n                    'air_area_name', 'hpg_store_id',\n                    'hpg_genre_name', 'hpg_area_name', \n                    'day_of_week', 'holiday_flg']\n\n        for c in features:\n            blindtrain[c+'_Count'] = np.nan\n            blindtrain[c+'_Mean'] = np.nan\n            blindtrain[c+'_Median'] = np.nan\n            blindtrain[c+'_Max'] = np.nan\n            blindtrain[c+'_Min'] = np.nan\n            blindtrain[c+'_Std'] = np.nan\n\n            blindtrain[[c+'_Count',c+'_Mean',\n                        c+'_Median',c+'_Max',\n                        c+'_Min', c+'_Std']] =  LeaveOneOut(vistrain,\n                                                            blindtrain,\n                                                            list([]),\n                                                            c,\n                                                            useLOO=False,\n                                                            cut=0,\n                                                            addNoise=False).values\n\n\n            if('air_store_id'!=c):\n                blindtrain.drop(c,inplace=True,axis=1)\n        if(btrain is None):\n            btrain = blindtrain.copy()\n        else:\n            btrain = pd.concat([btrain,blindtrain])\n        vismax -= pd.Timedelta(weeks=deltaweek)\n        vismin -= pd.Timedelta(weeks=deltaweek)\n        blindmin -= pd.Timedelta(weeks=deltaweek)\n        blindmax -= pd.Timedelta(weeks=deltaweek)\n\n    vistrain = train.copy()\n    features = ['air_genre_name',\n                'air_area_name', 'hpg_store_id',\n                'hpg_genre_name', 'hpg_area_name', \n                'day_of_week', 'holiday_flg']\n    for c in features:\n        lastfiveweekstrain[c+'_Count_Store'] = np.nan\n        lastfiveweekstrain[c+'_Mean_Store'] = np.nan\n        lastfiveweekstrain[c+'_Median_Store'] = np.nan\n        lastfiveweekstrain[c+'_Max_Store'] = np.nan\n        lastfiveweekstrain[c+'_Min_Store'] = np.nan\n        lastfiveweekstrain[c+'_Std_Store'] = np.nan\n\n\n        lastfiveweekstrain[[c+'_Count_Store',c+'_Mean_Store',\n                            c+'_Median_Store',c+'_Max_Store',\n                            c+'_Min_Store', c+'_Std_Store']] =  LeaveOneOut(vistrain,\n                                                                            lastfiveweekstrain,\n                                                                            list(['air_store_id']),\n                                                                            c,\n                                                                            useLOO=False,\n                                                                            cut=0).values\n        \n        test[c+'_Count_Store'] = np.nan\n        test[c+'_Mean_Store'] = np.nan\n        test[c+'_Median_Store'] = np.nan\n        test[c+'_Max_Store'] = np.nan\n        test[c+'_Min_Store'] = np.nan\n        test[c+'_Std_Store'] = np.nan\n\n\n        test[[c+'_Count_Store',c+'_Mean_Store',\n              c+'_Median_Store',c+'_Max_Store',\n              c+'_Min_Store', c+'_Std_Store']] =  LeaveOneOut(vistrain,\n                                                              test,\n                                                              list(['air_store_id']),\n                                                              c,\n                                                              useLOO=False,\n                                                              cut=0).values\n\n    features = ['air_store_id',\n                'air_genre_name',\n                'air_area_name', 'hpg_store_id',\n                'hpg_genre_name', 'hpg_area_name', \n                'day_of_week', 'holiday_flg']\n\n    for c in features:\n        lastfiveweekstrain[c+'_Count'] = np.nan\n        lastfiveweekstrain[c+'_Mean'] = np.nan\n        lastfiveweekstrain[c+'_Median'] = np.nan\n        lastfiveweekstrain[c+'_Max'] = np.nan\n        lastfiveweekstrain[c+'_Min'] = np.nan\n        lastfiveweekstrain[c+'_Std'] = np.nan\n\n        lastfiveweekstrain[[c+'_Count',c+'_Mean',\n                            c+'_Median',c+'_Max',\n                            c+'_Min', c+'_Std']] =  LeaveOneOut(vistrain,\n                                                                lastfiveweekstrain,\n                                                                list([]),\n                                                                c,\n                                                                useLOO=False,\n                                                                cut=0,\n                                                                addNoise=False).values\n        \n        test[c+'_Count'] = np.nan\n        test[c+'_Mean'] = np.nan\n        test[c+'_Median'] = np.nan\n        test[c+'_Max'] = np.nan\n        test[c+'_Min'] = np.nan\n        test[c+'_Std'] = np.nan\n\n        test[[c+'_Count',c+'_Mean',\n              c+'_Median',c+'_Max',\n              c+'_Min', c+'_Std']] =  LeaveOneOut(vistrain,\n                                                  test,\n                                                  list([]),\n                                                  c,\n                                                  useLOO=False,\n                                                  cut=0,\n                                                  addNoise=False).values\n    \n    d_train = xgb.DMatrix(btrain[btrain.columns[3:]], label=btrain.visitors)\n    d_valid = xgb.DMatrix(lastfiveweekstrain[btrain.columns[3:]], label=lastfiveweekstrain.visitors)\n    d_test = xgb.DMatrix(test[btrain.columns[3:]])\n    params = {}\n    params['objective'] = 'reg:linear'\n    params['eval_metric'] = 'rmse'\n    params['eta'] = 0.1\n    params['max_depth'] = 7\n    params['subsample']= 0.8 \n    params['colsample_bytree']= 0.8\n    params['silent'] = 1\n    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n    clf = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=20, verbose_eval=10)\n    xgbx = clf.predict(d_valid,ntree_limit=clf.best_iteration+1)\n    xgby = clf.predict(d_test,ntree_limit=clf.best_iteration+1)\n    trainpreds = pd.DataFrame()\n    trainpreds['visit_date'] = lastfiveweekstrain.visit_date.values\n    trainpreds['air_store_id'] = lastfiveweekstrain.air_store_id.values\n    trainpreds['visitors'] = lastfiveweekstrain.visitors.values\n    trainpreds['weeks_'+str(deltaweek)] = xgbx\n    testpreds = pd.DataFrame()\n    testpreds['visit_date'] = test.visit_date.values\n    testpreds['air_store_id'] = test.air_store_id.values\n    testpreds['weeks_'+str(deltaweek)] = xgby\n    if(xgbtrainpreds is None):\n        xgbtrainpreds = trainpreds.copy()\n        xgbtestpreds = testpreds.copy()\n    else:\n        xgbtrainpreds = xgbtrainpreds.merge(trainpreds[['air_store_id','visit_date','weeks_'+str(deltaweek)]],on=['air_store_id','visit_date'])\n        xgbtestpreds = xgbtestpreds.merge(testpreds[['air_store_id','visit_date','weeks_'+str(deltaweek)]],on=['air_store_id','visit_date'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"91c86994-644d-45e8-b4b5-533b22a386eb","_uuid":"621145b81ea2fcc2850220ef93975c9621077ee8","collapsed":true}},{"cell_type":"code","source":"xgbtrainpreds.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3dc3d012-a336-4873-8132-19bf661a92df","_uuid":"57a25f80a29f0b1346fee4010f1bacde5be28c7b","collapsed":true}},{"cell_type":"code","source":"xgbtestpreds.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"935d7bd9-18bd-4c6f-84f8-aa382aee4365","_uuid":"001559b21a87996ae8093d03117b0a7df6d5d0e9","collapsed":true}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(xgbtrainpreds[['weeks_1', 'weeks_2', 'weeks_3', 'weeks_4', 'weeks_5']],\n                                                    xgbtrainpreds.visitors, test_size=0.2, random_state=42)\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_valid = xgb.DMatrix(X_test, label=y_test)\nd_test = xgb.DMatrix(xgbtestpreds[['weeks_1', 'weeks_2', 'weeks_3', 'weeks_4', 'weeks_5']])\nparams = {}\nparams['objective'] = 'reg:linear'\nparams['eval_metric'] = 'rmse'\nparams['eta'] = 0.2\nparams['max_depth'] = 3\nparams['subsample']=0.8 \nparams['colsample_bytree']=0.8\nparams['silent'] = 1\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nclf = xgb.train(params, d_train, 250, watchlist, early_stopping_rounds=20, verbose_eval=10)\nxgbx = clf.predict(d_valid,ntree_limit=clf.best_iteration+1)\nxgby = clf.predict(d_test,ntree_limit=clf.best_iteration+1)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"0d5520e3-ae79-4246-9b47-58e53be38990","_uuid":"5b82bc0ee578360b61d0e69d625aae8f349bd087","collapsed":true}},{"cell_type":"code","source":"bestrounds = clf.best_iteration+1\nd_train = xgb.DMatrix(xgbtrainpreds[['weeks_1', 'weeks_2', 'weeks_3', 'weeks_4', 'weeks_5']], label=xgbtrainpreds.visitors)\nclf = xgb.train(params, d_train, int((bestrounds)*1.2), verbose_eval=10)\nxgbx = clf.predict(d_train,ntree_limit=bestrounds)\nxgby = clf.predict(d_test,ntree_limit=bestrounds)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"bd5caedc-b5a7-4eb5-89ce-a70caead6b75","_uuid":"865787909862732589494199c1a3fb2e10b2cf86","collapsed":true}},{"cell_type":"code","source":"print(np.sqrt(mean_squared_error(xgbtrainpreds.visitors.ravel(),\n                                 xgbx)))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e0fe9e2b-1913-4912-bd47-cbab0c756adf","_uuid":"40c99701626fb87a9db42067da7cb73d6d0a6b6c","collapsed":true}},{"cell_type":"code","source":"xgbtestpreds['id'] =  xgbtestpreds[\"air_store_id\"]+'_'+xgbtestpreds[\"visit_date\"].map(str)\nxgbtestpreds['id'] = xgbtestpreds.id.str[:-9]\nxgbtestpreds['visitors'] = np.expm1(xgby)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"cf5848e8-b8e7-4209-b3f9-3dc6b9c57573","_uuid":"ff409c9b4262e5f65d33e5eade5a8ba72e71cb4b","collapsed":true}},{"cell_type":"code","source":"xgbtestpreds[['id','visitors']].to_csv('xgbtimeseries.csv',index=False)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"016af5ac-db44-43f0-afc4-064310412c89","_uuid":"b5af8522830e3925f3427abf9451ddf16baf9711","collapsed":true}}],"nbformat":4,"metadata":{"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.3","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}