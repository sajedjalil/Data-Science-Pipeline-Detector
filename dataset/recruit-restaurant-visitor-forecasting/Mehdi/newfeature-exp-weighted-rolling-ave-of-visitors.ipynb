{"nbformat":4,"metadata":{"language_info":{"version":"3.6.4","file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"56c256b4-7de9-4617-937c-92878bebfc26","_uuid":"52ec7acbe234e42574ce9b28ccc4047e9e0ac268"},"source":"This is my first forcasting problem. My goals in this Kernel are as follows:\n\n* I will use the awesome kernels published by great Kagglers for this competition. I will try to explain why each step is neccessary, especially for data preparation. I have used the amazing work of following users:  \n-[the1owe](https://www.kaggle.com/the1owl/surprise-me)  \n-[Bojan Tunguz](https://www.kaggle.com/tunguz/surprise-me-2/code)  \n* I will add a new feature based on the discussions made [here](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179)\n* I will test several algorithms and I will  take advantage of `RandomizedSearchCV` class of `Scikit-learn` for hyper parameter optimization. Randomized search perfomrs drastically better than grid search. Please see [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html).\n\n**Update** First, I have run the model excluding the new feature. I then used the predictions on test set to obtain new `ewm` for test set. The RMSLE of fitting **without** `ewm` are as follows:\n- RMSE GradientBoostingRegressor:  0.3498713372707779\n- RMSE KNeighborsRegressor:  0.41941643714148896\n- RMSE XGBRegressor:  0.4367730415059621\n\nThe RMSLE of XGBRegressor has **dropped** after inserting the new feature (Please see model fitting section below). This model has achieved **LB score of 0.48x** alone. I imagine one could establish a learning scheme where each time a new `ewm` is generated for the newer prediction, and the process is repeated to reach higher accuracy.\n\n---\n**Please Note** This is a work in progress. Currently, I am using the hyperparameters indicated in the source scripts. I will keep this notebook updated in time by adding `gridsearchcv`. I also plan to include percipitation and temperature from [this](https://www.kaggle.com/huntermcgushion/rrv-weather-data) dataset. Your comments are much appreciated."},{"source":"import glob,re,os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom xgboost import XGBRegressor\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"61fceb69-a3c2-48e3-9d03-766332dd59ad","_uuid":"c6f23a7b6a6012e3976954bd961571865bf05092","collapsed":true},"outputs":[]},{"source":"#Assiging a name to each data frame\ndata={\n    'tra':pd.read_csv('../input/recruit-restaurant-visitor-forecasting/air_visit_data.csv'),\n    'as':pd.read_csv('../input/recruit-restaurant-visitor-forecasting/air_store_info.csv'),\n    'hs':pd.read_csv('../input/recruit-restaurant-visitor-forecasting/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/recruit-restaurant-visitor-forecasting/air_reserve.csv'),\n    'hr': pd.read_csv('../input/recruit-restaurant-visitor-forecasting/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/recruit-restaurant-visitor-forecasting/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/recruit-restaurant-visitor-forecasting/sample_submission.csv'),\n    'hol':pd.read_csv('../input/recruit-restaurant-visitor-forecasting/date_info.csv').rename(columns={'calendar_date':'visitor_date'})\n}","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"9a10c987-66c1-4f01-af18-179147d64845","_uuid":"76f3d9ea7d130182137601d5231d5e3273044005","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"7e4501c1-2125-4c2d-81d1-468d19a55688","_uuid":"9a40a22f874f8f676a5b8ca167e84b811846c934"},"source":"Regression works most reliably when the inputs come in a form that is well-known. The “form” we’re talking about is the distribution of the data. If the distribution of your data approximates that of a theoretical probability distribution, we can perform calculations on the data that are based on assumptions we can make about the well-known theoretical distribution.  \nIn order to make the variable better fit the assumptions underlying regression, we need to transform it. There are a number of ways to do this, but the most common for our purposes is to take the log of `visitors`. Let's check this:"},{"source":"plt.subplots(figsize=(12,6))\nplt.subplot(1, 2, 1)\ndata['tra']['visitors'].hist()\nplt.title('Histogram of visitors')\nplt.grid(False)\n\nplt.subplot(1, 2, 2)\nplt.title('Histogram of log of visitors')\ndata['tra']['visitors'].map(pd.np.log1p).hist()\nplt.grid(False)","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"756c5c5d-099d-4e20-bb71-3376c7391b4a","_uuid":"7b50656408b721d01142ab16602ff02d7ca6b714","collapsed":true},"outputs":[]},{"source":"# Now let's add to the hpg_reserve the ids from id dataset\ndata['hr']=data['hr'].merge(data['id'],on=['hpg_store_id'],how='inner')\n\ndata['ar'].head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"06056b6a-c209-4d00-83ce-89993108c587","_uuid":"3cf67f6e0622e68cb40ea09edbc5cf41eecf18ad","collapsed":true},"outputs":[]},{"source":"#let's tranfrom date to datetime objects. Please note, to_datetime also includes the actual time. Using .dt.date we only capture date.\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime']).dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime']).dt.date\n    \n    #here, we are actually engineering a new feature that captures the difference between visit and reserve times\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    \n    #let's group datasets by id and visit date, then get the sum and mean of reserve and reserve differnce, then rename the columns\n    temp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    \n    temp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    #now let's merge these two new temp dataframes.\n    data[df]=temp1.merge(temp2,how='inner',on=['air_store_id','visit_date'])","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"9d6f9aa1-a0dd-4ce1-9ebd-8d0e475ee443","_uuid":"276a52b033c9731512e0cca9253aa0c09be5e84f","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"1c9a3b18-481b-4304-826d-a55b56c5d82c","_uuid":"d493b78ac55ceab8199a5be0adef8ec75dc898ef"},"source":"> Experienced users of relational databases like SQL will be familiar with the terminology used to describe join operations between two SQL-table like structures (DataFrame objects). There are several cases to consider which are very important to understand:\n* one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values)\n* many-to-one joins: for example when joining an index (unique) to one or more columns in a DataFrame\n* many-to-many joins: joining columns on columns.\n\n**Note** When joining columns on columns (potentially a many-to-many join), any indexes on the passed DataFrame objects will be discarded."},{"source":"#let's take a look at hr and see what has happened to it.\ndata['hr'].head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"1309f3d8-1957-44e2-abed-21f949a679e4","_uuid":"a0dbeccef5b9120f200174d3c514b3d9619eae38","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"6ecf7c7a-aaf5-488e-bcb9-8d247fe0601a","_uuid":"fb1a5c3752b3006151a4630d1d3d68f5995e45cb"},"source":"**rs1** : sum of reserve_datetime_diff  \n**rv1** : sum of reserve_visitors  \n**rs2** : mean of reserve_datetime_diff  \n**rv2** : mean of reserve_visitors\n\nNow' let's engineer more features. pllease note our main training set is air_visit_data.  \nWe create daya of week, year, and month of visit as new features."},{"source":"data['tra']['visit_date']=pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow']=data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year']=data['tra']['visit_date'].dt.year\ndata['tra']['month']=data['tra']['visit_date'].dt.month\ndata['tra']['visit_date']=data['tra']['visit_date'].dt.date","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"90f8a41a-28f8-4137-aa79-b4c6c8e51bb1","_uuid":"948211eaf45911508d91fb00ab0da039609f2e65","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"be977952-1720-45a3-a051-0a164d70148d","_uuid":"b5b374fdc925453932e8f0c7a7e0e14c47f94c70"},"source":"We do the same thing for test set. Please note, we should first split the test ids and get dates and ids seperately."},{"source":"data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"aca4e819-b199-4c4d-bf3f-d6930d3f511a","_uuid":"ac2b5e144fc8e0a4bf2ae33b33406d238a4cbd06","collapsed":true},"outputs":[]},{"source":"unique_stores=data['tes']['air_store_id'].unique()\nprint('The number of unique stores is:', unique_stores.shape[0])\nprint('total number of data records in test set is',data['tes'].shape[0])","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"25a8ed7b-854a-4a98-a305-03c112c6c801","_uuid":"4164a87bf86e064cb17c39d471158aed50f60a87","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"d5e9d455-6ed1-4701-9eec-09e0946c22d5","_uuid":"d4644de1fcff2404cf20af35ca7df2af86c86958"},"source":"Now, we'd like to create a new dataframe, that has 7\\*821 rows, for each 7 days of the week. Later, we will add values to this new dataframe."},{"source":"stores=pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)],\n            axis=0,ignore_index=True).reset_index(drop=True)\n\nstores.shape","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"8d5aefbd-b990-4282-9c02-084a733e2749","_uuid":"baeae469922e4d1a497e272bd29ca0c1d83b5135","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"732664a6-6fb4-44a8-b8e1-00f096b947ed","_uuid":"572c9d1a49aa96178411a21601e961bb6754d69f"},"source":"Now, We will add a new feature to the train set. This feature is taken from the competition conversations. Please see [here](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46179).\nThis feature is an exponentially weighted rolling average of the number of visitors.\n\n**Note** `visitors` does not exist in test set. This new feature relies on the number of `visitors` to calculate the exp() rolling average. Therefore, I have made predictions on the test set without ewm, and will use these predictions in this section for calculating `ewm` for test set."},{"source":"#loading the test set with first round of projections.\ndata['test_ewm']=pd.read_csv('../input/first-round-predictions/test_ewm.csv')\n\n#defining a function that calculates the exponential weighted value of a series. alpha is the smoothing factor.\ndef calc_shifted_ewm(series, alpha, adjust=True):\n    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n\nfor df in ['tra','test_ewm']:\n    data[df]['ewm'] = data[df].groupby(['air_store_id', 'dow'])\\\n                  .apply(lambda g: calc_shifted_ewm(g['visitors'], 0.1)).sort_index(level=['air_store_id','dow']).values","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"d32de48e-db2d-4941-a6a6-2bea0e843f0b","_uuid":"c1bf0cff9d42ad9d06e3835cf004af81ea0a0b11","scrolled":false,"collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"a6672956-d893-49ec-acf8-62fbb3170af5","_uuid":"92b13068e81e0414e47eba7d9d1d9ce670432624"},"source":"We should find a way to replace the missing values. For this purpose, I first calculate the mean of ewm, then create a new id for both mean_ewm and data['tra'] datasets, and then set the index to the new id, and then replace the null values with the mean of ewm.\n\n**Note** please let me know if you think of any better way to perform this."},{"source":"#finding the mean of ewm\nmean_ewm_train=data['tra'].groupby(['air_store_id','dow']).mean().reset_index()\nmean_ewm_test=data['test_ewm'].groupby(['air_store_id','dow']).mean().reset_index()\n\n#setting new index for new_ewm_train\nmean_ewm_train['id_dow']=mean_ewm_train.apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\nmean_ewm_train=mean_ewm_train.set_index('id_dow')\n\nmean_ewm_test['id_dow']=mean_ewm_test.apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\nmean_ewm_test=mean_ewm_test.set_index('id_dow')","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"28d245de-cf9d-41e2-be9a-6a417d832208","_uuid":"19f5d18188c8f376c6ac290c2faee02e858ca2c8","scrolled":false,"collapsed":true},"outputs":[]},{"source":"#setting new index for data['tra']\ndata['tra']['id_dow']=data['tra'].apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\ndata['tra']=data['tra'].set_index('id_dow')\n\n#setting new index for data['test_ewm']\ndata['test_ewm']['id_dow']=data['test_ewm'].apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\ndata['test_ewm']=data['test_ewm'].set_index('id_dow')","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"913ab456-f7b6-41c7-9500-f1763a6bf033","_uuid":"e4b2c7eb39889d5e33ce8c78a61200060de0e21d","collapsed":true},"outputs":[]},{"source":"#filling na\ndata['tra']['ewm']=data['tra']['ewm'].fillna(mean_ewm_train['ewm'])\ndata['test_ewm']['ewm']=data['test_ewm']['ewm'].fillna(mean_ewm_test['ewm'])\n\n#making sure there are no missing values.\ndata['test_ewm'].isnull().sum()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"40b031e1-0f10-496e-8bb3-36a124c96c4e","_uuid":"bdf2b83a172dbce2ee841c9bbaf61a981ad6e2df","collapsed":true},"outputs":[]},{"source":"#merging new ewm with test set.\ndata['tes']=data['tes'].merge(data['test_ewm'],on=['id'],how='left')\ndata['tes']=data['tes'][['id','visitors_x','visit_date','air_store_id_x','dow_x','year','month','ewm']]\ndata['tes']=data['tes'].rename(columns={'visitors_x':'visitors','air_store_id_x':'air_store_id','dow_x':'dow'})","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"013efb54-3f60-4916-beb2-889298350ef4","_uuid":"b7949f4a5189102ca43b500f00f0380cf9ff771a","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"87b06355-7706-4798-b0ab-5b6a3e349792","_uuid":"648b08da37a9ecbfaba24d1aa15cf344fc26995b"},"source":"Here, we calculate min, max, median, mean, and the number of times each store has been visited per each day of the week. The following code might sound complicated but we are using the aggregate method of groupby."},{"source":"temp=data['tra'].groupby(['air_store_id','dow']).agg({'visitors':[np.min, np.mean, np.median, np.max, np.size]}).reset_index()\n\ntemp.head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"479c5b22-6a04-4393-ba82-f1876d9545ea","_uuid":"e18582d45dee43e90f34819fdbe4ea0360ac7c14","collapsed":true},"outputs":[]},{"source":"temp.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors','max_visitors','count_observations']\n\nstores=stores.merge(temp, on=['air_store_id','dow'],how='left')\nstores.head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"e0d980cd-d3d7-493c-94e5-3a1f45c6d1fe","_uuid":"37fe877d09d2dbbb7fd86b8aa8fb6acd2a118569","scrolled":false,"collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"c683ae42-d907-4595-a069-3ec1e1c04574","_uuid":"112fd7c301db2b1fba7b4a09233835c6b4317766"},"source":"It's getting more and more exciting. Now, let's add the store information to this dataframe. Including, genre, name, latitude, and longtitude."},{"source":"stores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])\n\nstores.head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"43989760-178d-45ed-8ff7-9958b1b78d1e","_uuid":"6a977fc693215c5448f46b95649e9b798cdb448c","scrolled":true,"collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"301d7934-fbc6-4930-859f-8ba4ff618e7c","_uuid":"5106fecd533ced9cda3d5a3083f631a966d6feaf"},"source":"Let's create new features based on name and area. We use [`LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class of preprocessing in python."},{"source":"stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n\n#list of unique genres\nstores['air_genre_name'].unique()\nstores['air_genre_name'].unique().shape[0]","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"79837ea4-4e9a-4294-8e15-65c19698a532","_uuid":"51801bcf5e09a43ab09ca1fbd4043cec4d2a8449","collapsed":true},"outputs":[]},{"source":"stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n\n#number of unique areas\nstores['air_area_name'].unique().shape[0]","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"840d3059-7075-4285-8db7-f09811068747","_uuid":"8661b0f0f3da4a26ffb1ef59b50a73555b9c4f5f","collapsed":true},"outputs":[]},{"source":"lbl = preprocessing.LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\nstores['air_genre_name'].unique()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"03ab71e4-e08d-4822-96ef-4ab54c2a04a7","_uuid":"2f9dbc59091b97f671dee72d275c2f22dc032c8d","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"d573e4ed-1e53-4bad-870c-b8dcedb2b6f8","_uuid":"e2c2951e811c30c5e77f02eebba8efff3e034bd7"},"source":"Successfully label encoded.\n\nLet's also label holidays. "},{"source":"data['hol'].head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"53ae4177-798c-4388-948a-ea291916bfbc","_uuid":"b813a8b4d1c24d3bc08f9bee19501cc27e9f6e87","collapsed":true},"outputs":[]},{"source":"data['hol']['visit_date']=pd.to_datetime(data['hol']['visitor_date'])\ndata['hol']['day_of_week']=lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date']=data['hol']['visit_date'].dt.date\ndata['hol']=data['hol'].drop('visitor_date',axis=1)\n\n#merge the holiday flags to train and test sets.\ntrain=data['tra'].merge(data['hol'],on=['visit_date'],how='left')\ntest=data['tes'].merge(data['hol'],on=['visit_date'],how='left')","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"e4e08738-7e1a-4c85-acb6-a7bb7573255c","_uuid":"e1f270b49584adae36698e0198eb7a38e227ae92","collapsed":true},"outputs":[]},{"source":"#merge stores\ntrain=train.merge(stores,how='left',on=['air_store_id','dow'])\ntest=test.merge(stores,how='left',on=['air_store_id','dow'])","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"496967d5-4bf9-4909-9f2d-c12f669db1e5","_uuid":"4202f02534d50fa4575ef04eafffe091a2631d5c","collapsed":true},"outputs":[]},{"source":"for df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5533a9ab-550f-4cb6-841b-ab7b3b1ff95e","_uuid":"19eba6f70965153d6291cb19b40e63b72896f7d7","collapsed":true},"outputs":[]},{"source":"train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\n#engineering new features\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\ntrain.head()","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5f426cfa-b0ac-4496-a847-66c4878b05bb","_uuid":"a20e70ccf60d5ce020918a6f8ba31f8dd18793b2","collapsed":true},"outputs":[]},{"source":"# engineeirng new features, Please refer to original codes mentioned in the introduction for more info.\n\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"9e1f906e-181c-44e0-b47e-6c26b03760fe","_uuid":"1aa53f7e4227557bd01608f9ae290f6802a6041e","collapsed":true},"outputs":[]},{"source":"train['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"2f22bcd6-ea27-4453-9b4e-156d2d92abda","_uuid":"c328430a7e2572700042e4b7ad6a5736c8cfa47f","collapsed":true},"outputs":[]},{"source":"lbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"d694d789-cae7-4f77-ac33-10950ee07109","_uuid":"9d0e91ff8c776804461fd7d8e946c24c3bce007d","collapsed":true},"outputs":[]},{"source":"col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\n#let's see how many features are we traning on\nprint('number of features are: ', len(col))","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"9b8cec31-b1fe-4cf0-abde-a850dcb5a33c","_uuid":"5c6df8fd4bdf6e295e58b79cb1026b79ed8fc56d","collapsed":true},"outputs":[]},{"source":"# XGB starter template borrowed from @anokas: https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n\nfor c, dtype in zip(train.columns, train.dtypes):\n    if dtype == np.float64:\n        train[c] = train[c].astype(np.float32)\n\nfor c, dtype in zip(test.columns, test.dtypes):\n    if dtype == np.float64:\n        test[c] = test[c].astype(np.float32)","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"fae898c9-f095-4ae7-a619-d987d5b624da","_uuid":"b68fd108343128e7b49004d0a896ea5b36b4ebf4","collapsed":true},"outputs":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"433fccef-e0b2-4d77-97ba-0671c91ec692","_uuid":"37074a8dac87bebfceb90120a649930f7c778c05"},"source":"Okay! our data preparation is done. Let's move on to model fitting."},{"cell_type":"markdown","metadata":{"_cell_guid":"d89591c9-bced-4a06-8503-fc795f3d20fb","_uuid":"d19d1ddea592664439ed0cc603581604cea78e09"},"source":"## Model fitting"},{"source":"#error metric\ndef RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"1c08bde2-f16d-450f-9927-b45231193fee","_uuid":"ab7f3e82025fc56afb7d2e6437617cb454d45c5a","collapsed":true},"outputs":[]},{"source":"model1 = ensemble.GradientBoostingRegressor(learning_rate=0.2, random_state=3, n_estimators=200, subsample=0.8, \n                      max_depth =10)\nmodel2 = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\nmodel3 = XGBRegressor(learning_rate=0.2, random_state=3, n_estimators=250, subsample=0.8, \n                      colsample_bytree=0.8, max_depth =10)\n\nmodel1.fit(train[col], np.log1p(train['visitors'].values))\nmodel2.fit(train[col], np.log1p(train['visitors'].values))\nmodel3.fit(train[col], np.log1p(train['visitors'].values))\n\npreds1 = model1.predict(train[col])\npreds2 = model2.predict(train[col])\npreds3 = model3.predict(train[col])\n\nprint('RMSLE GradientBoostingRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds1))\nprint('RMSLE KNeighborsRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds2))\nprint('RMSLE XGBRegressor: ', RMSLE(np.log1p(train['visitors'].values), preds3))\npreds1 = model1.predict(test[col])\npreds2 = model2.predict(test[col])\npreds3 = model3.predict(test[col])","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"98d4fd1a-39b1-47f0-95eb-0653d6924083","_uuid":"79557ef0542886bdb37154639377cffb601ce5bf","collapsed":true},"outputs":[]},{"source":"test['visitors'] = 0.3*preds1+0.3*preds2+0.4*preds3\ntest['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\nsub1 = test[['id','visitors']].copy()\ndel train; del data;","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"55c61cd6-cba9-4474-9b82-36516fbdc8d1","_uuid":"5c63b48838f45962acdfe29cce94fa7b0892ddd7","collapsed":true},"outputs":[]},{"source":"# from hklee\n# https://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st/code\ndfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):\n    pd.read_csv(fn)for fn in glob.glob('../input/recruit-restaurant-visitor-forecasting/*.csv')}\n\nfor k, v in dfs.items(): locals()[k] = v\n\nwkend_holidays = date_info.apply(\n    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') and x.holiday_flg==1), axis=1)\ndate_info.loc[wkend_holidays, 'holiday_flg'] = 0\ndate_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n\nvisit_data = air_visit_data.merge(date_info, left_on='visit_date', right_on='calendar_date', how='left')\nvisit_data.drop('calendar_date', axis=1, inplace=True)\nvisit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n\nwmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\nvisitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\nvisitors.rename(columns={0:'visitors'}, inplace=True) # cumbersome, should be better ways.\n\nsample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\nsample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\nsample_submission.drop('visitors', axis=1, inplace=True)\nsample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\nsample_submission = sample_submission.merge(visitors, on=[\n    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[visitors.holiday_flg==0], on=('air_store_id', 'day_of_week'), \n    how='left')['visitors_y'].values\n\nmissings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n    on='air_store_id', how='left')['visitors_y'].values\n\nsample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\nsub2 = sample_submission[['id', 'visitors']].copy()\nsub_merge = pd.merge(sub1, sub2, on='id', how='inner')\n\nsub_merge['visitors'] = 0.7*sub_merge['visitors_x'] + 0.3*sub_merge['visitors_y']* 1.2\nsub_merge[['id', 'visitors']].to_csv('submission.csv', index=False)","execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"859d88bd-c557-4959-8edf-bb0b188bfaea","_uuid":"f161e04941f4adc83ff057bd17616ee23a13bcc4","collapsed":true},"outputs":[]}]}