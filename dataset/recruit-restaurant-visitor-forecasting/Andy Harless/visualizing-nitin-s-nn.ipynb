{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","name":"python","nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.4","pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"source":"Not necessarily useful, but I found this [visualization code](https://github.com/philipperemy/keras-visualize-activations) on Github and wanted to try it out.  This example is based on [Nitin Surya's neural network](https://www.kaggle.com/nitinsurya/surprise-me-2-neural-networks-keras).","metadata":{"_uuid":"1d9fa65697c084b8ea20545c64e430bf044db6ba","collapsed":true,"_cell_guid":"b8654a8b-643b-4d2b-bf4a-b1fc6c06f2a9"},"cell_type":"markdown"},{"source":"NWEEKS_TO_EXCLUDE = 1","metadata":{"_uuid":"556618fcb333d6b8e68e7a26ede7d349c10a4536","collapsed":true,"_cell_guid":"1666bebe-2dc7-4128-a699-827233b25ca7"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"\"\"\"\nContributions from:\nDSEverything - Mean Mix - Math, Geo, Harmonic (LB 0.493) \nhttps://www.kaggle.com/dongxu027/mean-mix-math-geo-harmonic-lb-0-493\nJdPaletto - Surprised Yet? - Part2 - (LB: 0.503)\nhttps://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503\nhklee - weighted mean comparisons, LB 0.497, 1ST\nhttps://www.kaggle.com/zeemeen/weighted-mean-comparisons-lb-0-497-1st\ntunguz - Surprise Me 2!\nhttps://www.kaggle.com/tunguz/surprise-me-2/code\n\nAlso all comments for changes, encouragement, and forked scripts rock\n\nKeep the Surprise Going\n\"\"\"\n\nimport glob, re\nimport numpy as np\nimport pandas as pd\nfrom sklearn import *\nfrom datetime import datetime\nfrom xgboost import XGBRegressor\n\nfrom keras.layers import Embedding, Input, Dense\nimport keras\nimport keras.backend as K\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"6ae68db95c99261f9d9235185aaaf2344c335169","collapsed":true,"_cell_guid":"dd50fef0-4659-49ce-9e19-a8f3c16b9542"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"# From Philippe RÃ©my's Github  https://github.com/philipperemy\n# Revised to print layer names when displaying\n\ndef get_activations(model, model_inputs, print_shape_only=False, layer_name=None):\n    print('----- activations -----')\n    activations = []\n    inp = model.input\n\n    model_multi_inputs_cond = True\n    if not isinstance(inp, list):\n        # only one input! let's wrap it in a list.\n        inp = [inp]\n        model_multi_inputs_cond = False\n\n    outputs = [layer.output for layer in model.layers if\n               layer.name == layer_name or layer_name is None]  # all layer outputs\n\n    funcs = [K.function(inp + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n\n    if model_multi_inputs_cond:\n        list_inputs = []\n        list_inputs.extend(model_inputs)\n        list_inputs.append(0.)\n    else:\n        list_inputs = [model_inputs, 0.]\n\n    # Learning phase. 0 = Test mode (no dropout or batch normalization)\n    # layer_outputs = [func([model_inputs, 0.])[0] for func in funcs]\n    layer_outputs = [func(list_inputs)[0] for func in funcs]\n    for layer_activations in layer_outputs:\n        activations.append(layer_activations)\n        if print_shape_only:\n            print(layer_activations.shape)\n        else:\n            print(layer_activations)\n    return activations\n\n\ndef display_activations_with_names(activation_maps, layer_names):\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \"\"\"\n    (1, 26, 26, 32)\n    (1, 24, 24, 64)\n    (1, 12, 12, 64)\n    (1, 12, 12, 64)\n    (1, 9216)\n    (1, 128)\n    (1, 128)\n    (1, 10)\n    \"\"\"\n    batch_size = activation_maps[0].shape[0]\n    assert batch_size == 1, 'One image at a time to visualize.'\n    for i, activation_map in enumerate(activation_maps):\n        print('Displaying activation map {}'.format(layer_names[i]))\n        shape = activation_map.shape\n        if len(shape) == 4:\n            activations = np.hstack(np.transpose(activation_map[0], (2, 0, 1)))\n        elif len(shape) == 2:\n            # try to make it square as much as possible. we can skip some activations.\n            activations = activation_map[0]\n            num_activations = len(activations)\n            if num_activations > 1024:  # too hard to display it on the screen.\n                square_param = int(np.floor(np.sqrt(num_activations)))\n                activations = activations[0: square_param * square_param]\n                activations = np.reshape(activations, (square_param, square_param))\n            else:\n                activations = np.expand_dims(activations, axis=0)\n        else:\n            raise Exception('len(shape) = 3 has not been implemented.')\n        plt.imshow(activations, interpolation='None', cmap='jet')\n        plt.show()","metadata":{"_uuid":"d25e1479527e1d07555b3cb7c239c9725d115581","collapsed":true,"_cell_guid":"5bc330b2-44cc-4ea8-9fad-bec74c521225"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"data = {\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n    }\n\ndata['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n\nfor df in ['ar','hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_dow'] = data[df]['visit_datetime'].dt.dayofweek\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    # Exclude late reservations\n    ne = 7*(NWEEKS_TO_EXCLUDE - 1)\n    data[df] = data[df][data[df]['reserve_datetime_diff'] > data[df]['visit_dow']+ne]\n    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n\ndata['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n\ndata['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n\nunique_stores = data['tes']['air_store_id'].unique()\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n\n#sure it can be compressed...\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\ntmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\nstores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n\nstores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n# NEW FEATURES FROM Georgii Vyshnia\nstores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\nstores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\nlbl = preprocessing.LabelEncoder()\nfor i in range(10):\n    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\nstores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\nstores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ntrain = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \ntest = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n\ntrain = pd.merge(train, stores, how='inner', on=['air_store_id','dow']) \ntest = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n\nfor df in ['ar','hr']:\n    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n\ntrain['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n\ntrain['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\ntrain['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\ntrain['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n\ntest['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\ntest['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\ntest['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n\n# NEW FEATURES FROM JMBULL\ntrain['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntest['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\ntrain['var_max_lat'] = train['latitude'].max() - train['latitude']\ntrain['var_max_long'] = train['longitude'].max() - train['longitude']\ntest['var_max_lat'] = test['latitude'].max() - test['latitude']\ntest['var_max_long'] = test['longitude'].max() - test['longitude']\n\n# NEW FEATURES FROM Georgii Vyshnia\ntrain['lon_plus_lat'] = train['longitude'] + train['latitude'] \ntest['lon_plus_lat'] = test['longitude'] + test['latitude']\n\nlbl = preprocessing.LabelEncoder()\ntrain['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\ntest['air_store_id2'] = lbl.transform(test['air_store_id'])\n\ncol = [c for c in train if c not in ['id', 'air_store_id', 'visit_date','visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)","metadata":{"_uuid":"4e1d66863fc7513cbc78f9a392da989994e62c1c","collapsed":true,"_cell_guid":"ba6c11e9-8f34-44c8-88f8-36f6b055caab"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","metadata":{"_uuid":"d9d356a59c967c4a4d4c6a481f5fbbd1da1243c9","collapsed":true,"_cell_guid":"381168b6-ea8b-4335-a5c2-5eebd2c853d9"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"Here we prepare data required for the neural network model.\n\n**value_col**:  taken as float input(which are normalized)\n\n**nn_col - value_col**: taken as categorical inputs(embedding layers used)","metadata":{"_uuid":"52e4a8487080e0d8f347774ffdcf67307584fa83","_cell_guid":"aa727038-6fd5-47c0-9cda-197c4016ff8a"},"cell_type":"markdown"},{"source":"value_col = ['holiday_flg','min_visitors','mean_visitors','median_visitors','max_visitors','count_observations',\n'rs1_x','rv1_x','rs2_x','rv2_x','rs1_y','rv1_y','rs2_y','rv2_y','total_reserv_sum','total_reserv_mean',\n'total_reserv_dt_diff_mean','date_int','var_max_lat','var_max_long','lon_plus_lat']\n\nnn_col = value_col + ['dow', 'year', 'month', 'air_store_id2', 'air_area_name', 'air_genre_name',\n'air_area_name0', 'air_area_name1', 'air_area_name2', 'air_area_name3', 'air_area_name4',\n'air_area_name5', 'air_area_name6', 'air_genre_name0', 'air_genre_name1',\n'air_genre_name2', 'air_genre_name3', 'air_genre_name4']\n\n\nX = train.copy()\nX_test = test[nn_col].copy()\n\nvalue_scaler = preprocessing.MinMaxScaler()\nfor vcol in value_col:\n    X[vcol] = value_scaler.fit_transform(X[vcol].values.astype(np.float64).reshape(-1, 1))\n    X_test[vcol] = value_scaler.transform(X_test[vcol].values.astype(np.float64).reshape(-1, 1))\n\nX_train = list(X[nn_col].T.as_matrix())\nY_train = np.log1p(X['visitors']).values\nnn_train = [X_train, Y_train]\nnn_test = [list(X_test[nn_col].T.as_matrix())]\nprint(\"Train and test data prepared\")","metadata":{"_uuid":"a5de3f5c884c46772f44e76c2ff6053fbc9b2b4c","collapsed":true,"_cell_guid":"20cd2b5a-990f-4ad7-a043-b17d70e49b08"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"Following function implements the Keras neural network model.\n\nBasic structure:\n- categorical columns get independent inputs, passed through embedding layer and then flattened.\n- numeric columns are simply taken as float32 inputs\n- the final tensors of categorical and numerical are then concatenated together\n- following the concatenated layer and simple feed forward neural network is implemented.\n- output layer has 'ReLU' activation function","metadata":{"_uuid":"fb1b7bd33d1a74ac26abe06bccc747fa930a5722","_cell_guid":"0f3c1ff0-c4be-4732-81a5-db1a250e5296"},"cell_type":"markdown"},{"source":"def get_nn_complete_model(train, hidden1_neurons=35, hidden2_neurons=15):\n    \"\"\"\n    Input:\n        train:           train dataframe(used to define the input size of the embedding layer)\n        hidden1_neurons: number of neurons in the first hidden layer\n        hidden2_neurons: number of neurons in the first hidden layer\n    Output:\n        return 'keras neural network model'\n    \"\"\"\n    K.clear_session()\n\n    air_store_id = Input(shape=(1,), dtype='int32', name='air_store_id')\n    air_store_id_emb = Embedding(len(train['air_store_id2'].unique()) + 1, 15, input_shape=(1,),\n                                 name='air_store_id_emb')(air_store_id)\n    air_store_id_emb = keras.layers.Flatten(name='air_store_id_emb_flatten')(air_store_id_emb)\n\n    dow = Input(shape=(1,), dtype='int32', name='dow')\n    dow_emb = Embedding(8, 3, input_shape=(1,), name='dow_emb')(dow)\n    dow_emb = keras.layers.Flatten(name='dow_emb_flatten')(dow_emb)\n\n    month = Input(shape=(1,), dtype='int32', name='month')\n    month_emb = Embedding(13, 3, input_shape=(1,), name='month_emb')(month)\n    month_emb = keras.layers.Flatten(name='month_emb_flatten')(month_emb)\n\n    air_area_name, air_genre_name = [], []\n    air_area_name_emb, air_genre_name_emb = [], []\n    for i in range(7):\n        area_name_col = 'air_area_name' + str(i)\n        air_area_name.append(Input(shape=(1,), dtype='int32', name=area_name_col))\n        tmp = Embedding(len(train[area_name_col].unique()), 3, input_shape=(1,),\n                        name=area_name_col + '_emb')(air_area_name[-1])\n        tmp = keras.layers.Flatten(name=area_name_col + '_emb_flatten')(tmp)\n        air_area_name_emb.append(tmp)\n\n        if i > 4:\n            continue\n        area_genre_col = 'air_genre_name' + str(i)\n        air_genre_name.append(Input(shape=(1,), dtype='int32', name=area_genre_col))\n        tmp = Embedding(len(train[area_genre_col].unique()), 3, input_shape=(1,),\n                        name=area_genre_col + '_emb')(air_genre_name[-1])\n        tmp = keras.layers.Flatten(name=area_genre_col + '_emb_flatten')(tmp)\n        air_genre_name_emb.append(tmp)\n\n    air_genre_name_emb = keras.layers.concatenate(air_genre_name_emb)\n    air_genre_name_emb = Dense(4, activation='sigmoid', name='final_air_genre_emb')(air_genre_name_emb)\n\n    air_area_name_emb = keras.layers.concatenate(air_area_name_emb)\n    air_area_name_emb = Dense(4, activation='sigmoid', name='final_air_area_emb')(air_area_name_emb)\n    \n    air_area_code = Input(shape=(1,), dtype='int32', name='air_area_code')\n    air_area_code_emb = Embedding(len(train['air_area_name'].unique()), 8, input_shape=(1,), name='air_area_code_emb')(air_area_code)\n    air_area_code_emb = keras.layers.Flatten(name='air_area_code_emb_flatten')(air_area_code_emb)\n    \n    air_genre_code = Input(shape=(1,), dtype='int32', name='air_genre_code')\n    air_genre_code_emb = Embedding(len(train['air_genre_name'].unique()), 5, input_shape=(1,),\n                                   name='air_genre_code_emb')(air_genre_code)\n    air_genre_code_emb = keras.layers.Flatten(name='air_genre_code_emb_flatten')(air_genre_code_emb)\n\n    \n    holiday_flg = Input(shape=(1,), dtype='float32', name='holiday_flg')\n    year = Input(shape=(1,), dtype='float32', name='year')\n    min_visitors = Input(shape=(1,), dtype='float32', name='min_visitors')\n    mean_visitors = Input(shape=(1,), dtype='float32', name='mean_visitors')\n    median_visitors = Input(shape=(1,), dtype='float32', name='median_visitors')\n    max_visitors = Input(shape=(1,), dtype='float32', name='max_visitors')\n    count_observations = Input(shape=(1,), dtype='float32', name='count_observations')\n    rs1_x = Input(shape=(1,), dtype='float32', name='rs1_x')\n    rv1_x = Input(shape=(1,), dtype='float32', name='rv1_x')\n    rs2_x = Input(shape=(1,), dtype='float32', name='rs2_x')\n    rv2_x = Input(shape=(1,), dtype='float32', name='rv2_x')\n    rs1_y = Input(shape=(1,), dtype='float32', name='rs1_y')\n    rv1_y = Input(shape=(1,), dtype='float32', name='rv1_y')\n    rs2_y = Input(shape=(1,), dtype='float32', name='rs2_y')\n    rv2_y = Input(shape=(1,), dtype='float32', name='rv2_y')\n    total_reserv_sum = Input(shape=(1,), dtype='float32', name='total_reserv_sum')\n    total_reserv_mean = Input(shape=(1,), dtype='float32', name='total_reserv_mean')\n    total_reserv_dt_diff_mean = Input(shape=(1,), dtype='float32', name='total_reserv_dt_diff_mean')\n    date_int = Input(shape=(1,), dtype='float32', name='date_int')\n    var_max_lat = Input(shape=(1,), dtype='float32', name='var_max_lat')\n    var_max_long = Input(shape=(1,), dtype='float32', name='var_max_long')\n    lon_plus_lat = Input(shape=(1,), dtype='float32', name='lon_plus_lat')\n\n    date_emb = keras.layers.concatenate([dow_emb, month_emb, year, holiday_flg])\n    date_emb = Dense(5, activation='sigmoid', name='date_merged_emb')(date_emb)\n\n    cat_layer = keras.layers.concatenate([holiday_flg, min_visitors, mean_visitors,\n                    median_visitors, max_visitors, count_observations, rs1_x, rv1_x,\n                    rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y,\n                    total_reserv_sum, total_reserv_mean, total_reserv_dt_diff_mean,\n                    date_int, var_max_lat, var_max_long, lon_plus_lat,\n                    date_emb, air_area_name_emb, air_genre_name_emb,\n                    air_area_code_emb, air_genre_code_emb, air_store_id_emb])\n\n    m = Dense(hidden1_neurons, name='hidden1',\n             kernel_initializer=keras.initializers.RandomNormal(mean=0.0,\n                            stddev=0.05, seed=None))(cat_layer)\n    m = keras.layers.PReLU()(m)\n    m = keras.layers.BatchNormalization()(m)\n    \n    m1 = Dense(hidden2_neurons, name='sub1')(m)\n    m1 = keras.layers.PReLU()(m1)\n    m = Dense(1, activation='relu')(m1)\n\n    inp_ten = [\n        holiday_flg, min_visitors, mean_visitors, median_visitors, max_visitors, count_observations,\n        rs1_x, rv1_x, rs2_x, rv2_x, rs1_y, rv1_y, rs2_y, rv2_y, total_reserv_sum, total_reserv_mean,\n        total_reserv_dt_diff_mean, date_int, var_max_lat, var_max_long, lon_plus_lat,\n        dow, year, month, air_store_id, air_area_code, air_genre_code\n    ]\n    inp_ten += air_area_name\n    inp_ten += air_genre_name\n    model = keras.Model(inp_ten, m)\n    model.compile(loss='mse', optimizer='rmsprop', metrics=['acc'])\n\n    return model","metadata":{"_uuid":"6df537d60eca75ceb2445359aa67ba8c1688ccda","collapsed":true,"_cell_guid":"282b5af0-fcfc-419e-9afa-6ca86756173c"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"model4 = get_nn_complete_model(train, hidden2_neurons=12)\nfor i in range(5):\n    model4.fit(nn_train[0], nn_train[1], epochs=3, verbose=1,\n        batch_size=256, shuffle=True, validation_split=0.15)\n    model4.fit(nn_train[0], nn_train[1], epochs=8, verbose=0,\n        batch_size=256, shuffle=True)\nprint(\"Model4 trained\")","metadata":{"_uuid":"9dc55a87c7bb1b2c7c4f599e3f58bd35c9a6d6d7","collapsed":true,"_cell_guid":"4694a89d-0629-4d6e-b646-79ad0b5a7b83"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"preds4 = pd.Series(model4.predict(nn_train[0]).reshape(-1)).clip(0, 6.8).values\n\nprint('RMSE NeuralNetwork: ', RMSLE(np.log1p(train['visitors'].values), preds4))","metadata":{"_uuid":"513635a989d672572fb3e30740ebc33785beade7","collapsed":true,"scrolled":true,"_cell_guid":"45f736a7-30df-4339-a803-251e6e818734"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"model4.summary()","metadata":{"_uuid":"02cec58e287996c25bc64249b4bbd212ddb9666b","collapsed":true,"_cell_guid":"ada1ed4f-b6a4-4232-ade4-2e20dc2886fc"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"layer_names = {  # layers to display\n               '  Area Embedding':    'air_area_code_emb_flatten',\n               '  Genre Embedding':   'air_genre_code_emb_flatten', \n               '  Store Embedding':   'air_store_id_emb_flatten',\n               '  Concatenated Data': 'concatenate_4',\n               '  1st Dense Input':   'hidden1',\n               '  1st Dense Output':  'p_re_lu_1', \n               '  2nd Dense Input':   'sub1', \n               '  2nd Dense Output':  'p_re_lu_2'\n              }","metadata":{"_uuid":"4d658f4bcba886c22e1993a3d82edc34b390ac86","collapsed":true,"_cell_guid":"c0884d20-45f1-407e-9d65-30fdcfa89894"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"for i in [0,5,13,14,15]:       # Visualize several test data points\n   # Get test data point\n    inp = []\n    for arr in nn_test[0]:\n        inp.append( arr[i].reshape(1, 1) )\n\n    print( '\\nCalculating activations for test case ', i)\n    activs = list()\n    for n in layer_names.keys():\n        activs += get_activations(model4, inp, print_shape_only=True, layer_name=layer_names[n])\n    \n    result = get_activations(model4, inp, print_shape_only=True, layer_name='dense_1')\n\n    display_activations_with_names(activs, [n for n in layer_names.keys()])\n    print( 'Final result for test case', i, ': ', result[0].item(0), '\\n\\n\\n')","metadata":{"_uuid":"4074fcef371e628088ce7f3793e57764f55a0bcd","collapsed":true,"scrolled":false,"_cell_guid":"88f68b6f-014f-4454-93ba-6e4e81124f97"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"# Final layer connections\nmodel4.get_weights()[-2]","metadata":{"_uuid":"4713f26686446549e2a9eec88b91a79723a8f0c1","collapsed":true,"_cell_guid":"4dc90b4a-abfe-4802-9dec-109a16eb3010"},"cell_type":"code","execution_count":null,"outputs":[]},{"source":"The specific arrangement of weights depends randomly on the particular training run, so I can't make specific comments, but it's interesting to look at how the weights map the final hidden layer to the result.","metadata":{"_uuid":"f17f25b44de0bccc30f5b62dbe43e6924b03f3e3","_cell_guid":"be15d9bc-1c50-47a7-8eef-a84667f36978"},"cell_type":"markdown"}],"nbformat_minor":1,"nbformat":4}