{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6f156cfe-a1a7-0f4b-99e9-967689747c66"},"source":"Whoa CT data. Very interesting! Let's take a quick look and see if we can visualize it in any way that makes sense."},{"cell_type":"markdown","metadata":{"_cell_guid":"abee07fa-4cd8-faa4-d632-63a4f3b8d264"},"source":"### Loading the required packages:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73a7ee02-4a2b-47d8-72f7-46a23f19b4b6"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport dicom\nimport cv2\nfrom skimage import data, io, filters\nimport os\nimport matplotlib.pyplot as plt\nimport pylab\n\nos.chdir(\"../input/sample_images\")\nfiles = os.listdir()\n\n%config InlineBackend.figure_format = 'retina'"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed9c1f87-d6a1-ac05-c822-93542a86b5ae"},"source":"Let's build a helper function that will allow us to look at the CT scans. We will use the color map recommended by the python package [dicom](http://pydicom.readthedocs.io/en/stable/viewing_images.html):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d96524d5-c982-e813-2fac-028233a4ab75"},"outputs":[],"source":"def show(slice):\n    plt.imshow(slice, cmap=plt.cm.bone)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5f268856-6b14-08c2-dc98-69eaa3f0a7a9"},"source":"### Let's for now focus on one patient:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52ecc9db-55ad-0dd2-c854-7e5a0329188a"},"outputs":[],"source":"os.chdir('0d941a3ad6c889ac451caf89c46cb92a')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d4724ea-8616-3adb-788f-499f3a9cb1fe"},"outputs":[],"source":"os.listdir()[:5]"},{"cell_type":"markdown","metadata":{"_cell_guid":"70821929-e688-0d72-e32f-fff59e05fa57"},"source":"So it looks like we have a bunch of .dcm files for each patient. Let's read them in using the dicom package and try to extract the actual image arrays. Since we have multiple images we have to figure out a way to sort them in the order they were taken. r4m0n's excellent script [here](https://www.kaggle.com/mumech/data-science-bowl-2017/loading-and-processing-the-sample-images ) figured out that sorting based on Instance number is the way to go:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eeae6b64-42fd-b787-c1cb-bf97b7290ccf"},"outputs":[],"source":"files = os.listdir()\n\nimgs = []\nfor i in files:\n        ds = dicom.read_file(i)\n        imgs.append(ds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1397a07-0b4f-a62f-5f4e-7e66940f773f"},"outputs":[],"source":"#sorting based on InstanceNumber stolen from r4m0n's script: \nimgs.sort(key = lambda x: int(x.InstanceNumber))\nfull_img = np.stack([s.pixel_array for s in imgs])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2ae47c3-c424-9cba-d979-077fc0f61ac3"},"outputs":[],"source":"full_img.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"51946509-befe-9e12-ba52-fd57e8184b6b"},"source":"We get a 3-D array (or tensor) that should roughly mimic one's torso.\n\nWe can try to visualize it by doing sweeps of the images in one of the three dimensions. Note that the image dimensions are techically (z, x, y) here. I am sure these have specific names in the medical field but we will just refer to them by their coordinates. Let's see what insights we can get when doing this:"},{"cell_type":"markdown","metadata":{"_cell_guid":"f6161d33-dbd5-5530-0f5a-3e13e21ac0c0"},"source":"### Z-axis (height) sweep:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45790697-d1f4-00ff-2d0c-dde472739fd0"},"outputs":[],"source":"#from https://www.kaggle.com/z0mbie/data-science-bowl-2017/chest-cavity-animation-with-pacemaker\n#not working..hmm\n%matplotlib nbagg\nimport matplotlib.animation as animation\nfig = plt.figure() # make figure\nfrom IPython.display import HTML\n\nim = plt.imshow(full_img[0], cmap=pylab.cm.bone)\n\n# function to update figure\ndef updatefig(j):\n    # set the data in the axesimage object\n    im.set_array(full_img[j])\n    # return the artists set\n    return im,\n# kick off the animation\nani = animation.FuncAnimation(fig, updatefig, frames=range(len(full_img)), \n                              interval=50, blit=True)\nani.save('Chest_Cavity.gif', writer='imagemagick')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86f5fe5a-530a-790a-213f-3fe860f6e40f"},"outputs":[],"source":"%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n\n\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    show(full_img[4*i,:,:])    \n    plt.xticks([])\n    plt.yticks([])"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ec9735d-ce81-18e6-74de-961d407c403c"},"source":"### X-axis (frontal) sweep:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4765f34e-7e69-db68-b59f-7ec8aa8a42ca"},"outputs":[],"source":"for i in range(36):\n    plt.subplot(6,6,i+1)\n    img = cv2.resize(full_img[:,20+ 12*i,:], (256, 256))\n    show(img)    \n    plt.xticks([])\n    plt.yticks([])"},{"cell_type":"markdown","metadata":{"_cell_guid":"663203ad-9a44-6ee3-5419-55d08d59fb1f"},"source":"### Y-axis (lateral) sweep:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4d5c0ed-e5ae-c634-6727-fe14a204c099"},"outputs":[],"source":"for i in range(36):\n    plt.subplot(6,6,i+1)\n    img = cv2.resize(full_img[:,:,20+ 12*i], (256, 256))\n    show(img)    \n    plt.xticks([])\n    plt.yticks([])"},{"cell_type":"markdown","metadata":{"_cell_guid":"04fc9507-b316-cabc-0a12-737f949f4a0c"},"source":"Pretty fascinating stuff - in particular it's easy to see the 3 different perspectives. Another neat thing is that once we got the scans we managed to visualize them using very basic python open source tools - basically numpy and matplotlib. No need for fancy medical imaging tools.\n\nI wish I could say something more interesting about these images but honestly I have no medical expertise whatsoever so I'll just stare at them for a bit and maybe read more on human anatomy :)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e52f66c-e637-5521-243d-20973f6be9a5"},"source":"### Edge detection and other convolutional filters:\n\nSince we're interested in detecting whether a patient will be diagnosed with lung cancer, we can try to see if we can detect pulmonary nodules using something like edge detection. This can be done using a sobel filter (aka hand crafted one-filter CNN). "},{"cell_type":"markdown","metadata":{"_cell_guid":"48ff3a62-4f1a-7fba-f294-d65458d73fd9"},"source":"Let's also take a look at the distribution of the pixels values in an image first:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bbcd2a5-a0e1-e942-3a03-820a75c4c12a"},"outputs":[],"source":"full_img[100]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da06ffcd-058d-ec3d-56dd-0defc29237ed"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npd.Series(full_img[10].ravel()).hist(bins = 100)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b415b98-1024-f001-6baf-33db29f50996"},"source":"Interesting - the distribution seems to be roughly bimodal with a bunch of pixels set at - 2000 - probably for missing values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"842a0cd7-1677-471b-0c19-30cf7c8276f8"},"outputs":[],"source":"edge_img = filters.sobel(full_img[90,:,:]/2500)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a880eaeb-37c6-6fba-4152-414dec726d03"},"outputs":[],"source":"plt.imshow(edge_img)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d9bb0eb3-2b5c-3fc8-ab51-2291e94db745"},"source":"The Sobel filter does find the edges but the image is very low intensity. One thing we can do is to simply treshold the image to see the segmentation better:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b574ebc-d448-d769-6154-53fea5bfdbe6"},"outputs":[],"source":"plt.imshow(edge_img > 0.04, cmap = \"viridis\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f7f2497-285c-dee4-adba-c5e12db04c9a"},"outputs":[],"source":"matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n\n\nfor i in range(36):\n    plt.subplot(6,6,i+1)\n    edge_img = filters.sobel(full_img[4*i,:,:]/2500)\n    img = edge_img > 0.04\n    plt.imshow(img, cmap = \"viridis\")    \n    plt.xticks([])\n    plt.yticks([])"},{"cell_type":"markdown","metadata":{"_cell_guid":"94f43827-84c8-20b4-f812-a6616530cb66"},"source":"Interesting results, however the issue here is that the filter will also detect the blood vessels in the lung. So some sort of 3-D surface detection that differentiates between spheres and tubes would be more suitable for this situation."},{"cell_type":"markdown","metadata":{"_cell_guid":"bdb89799-41c6-568f-9697-81ae8739812b"},"source":"### Future Ideas:\n\n- creating a 3-D representation of the lungs\n- try other filters\n- analyze the metadata more closely.\n\nThanks for reading!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ef7bc4e-1ab5-79c5-03bb-29f93c12abb0"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}