{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"76b2ded9-4e62-1ca5-8b35-40409cdb7f69"},"source":"In this kernel, I will be talking about the methods that will help in better understanding of the problem statement and visualisation of the data. I will also provide links of useful resources and information.\n\nThe script is written in Python. I recommend people to install anaconda on their desktop because of its advantages mentioned [here][1]. The libraries that will be used in this tutorial for reading, processing and visualisation of data are matplotlib, numpy, skimage and [pydicom][2]. \n\nThe images are of size (z, 512, 512) where z is the number of slices in the CT Scan and varies depending on the resolution of the scanner. Such large images cant be fed directly into a Convolution Network architectures because of the limit on the computation power. Thus we will have to find the regions that are more probable of having cancer. We will be reducing our search space by first segmenting the lungs and then removing the low intensity regions. \n\nIn this tutorial, we will first start with reading the dataset and visualising it. After that, we will be segmenting the lung structures and then find the region of interest(possible cancer regions) in the CT Scans using Image processing methods. Then I will talk about how to preprocess LUNA16 dataset for training architectures like UNet for segmentation and candidate classification.\n\nThe segmentation of lung structures is very challenging problem because homogeneity is not present in the lung region, similar densities in the pulmonary structures, different scanners and scanning protocols. The segmented lungs can be further used to find the lung nodule candidates and regions of interest which may help in better classification of the CT Scans. Finding the lung nodule regions is a very hard problem because there are nodules that are attached to the blood vessels or are present at the boundary of the lung region. The lung nodule candidates can be further used for classification by cutting 3D voxels around them and passing it through a 3D CNNs which can be trained on LUNA16 dataset. The LUNA 16 dataset has the location of the nodules in each CT Scan thus will be useful for training the classifier. \n\n  [1]: https://www.reddit.com/r/Python/comments/3t23vv/what_advantages_are_there_of_using_anaconda/\n  [2]: http://pydicom.readthedocs.io/en/stable/getting_started.html"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a8d62b0-fa29-942d-8970-9f6e86e7bec0"},"source":"**Reading a CT Scan**\n-----------------\nThe input folder has three things, one is the sample_images folders which has the sample CT Scans. The `stage1_labels.csv` contains the cancer ground truth for the stage 1 training set images and `stage1_sample_submission.csv` shows the submission format for stage 1. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4a71922-604e-55df-e49c-e51e3f330b34"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport skimage, os\nfrom skimage.morphology import ball, disk, dilation, binary_erosion, remove_small_objects, erosion, closing, reconstruction, binary_closing\nfrom skimage.measure import label,regionprops, perimeter\nfrom skimage.morphology import binary_dilation, binary_opening\nfrom skimage.filters import roberts, sobel\nfrom skimage import measure, feature\nfrom skimage.segmentation import clear_border\nfrom skimage import data\nfrom scipy import ndimage as ndi\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nimport dicom\nimport scipy.misc\nimport numpy as np\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"60d13984-09b5-4615-b864-7b885dba7bd9"},"source":"Each 3D CT Scan consists of many slices, whose number depends on the resolution of the scanner and each slice has a Instance Number associated with it which tells the index of the slice from the top. All the dicom files for a CT Scan are inside one folder having the CT Scan's name. Now we will read all the dicom slices for a scan and then stack them with respect to their Instance Number to get the 3D Lung CT Scanned Image."},{"cell_type":"markdown","metadata":{"_cell_guid":"14d123c3-81e1-db58-5c4c-127f484ad02d"},"source":"The sample_images folder has around 20 folders each corresponding to one CT Scan. Inside the folder there are many dicom files. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"740305a3-05d6-4ac1-c4b5-6bb80e9a7631"},"outputs":[],"source":"print(check_output([\"ls\", \"../input/sample_images/\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6f84e58c-9c9f-9671-95b6-003c4c13e30e"},"source":"Each CT Scan consists of multiple 2D slices which are provided in a DICOM format. At first, I will read the random dicom file of a CT Scan. After reading the image file, we will update the intensity values of -2000 with 0 because they are the pixels that fall outside of the scanner bounds."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ec6d6a0-a7ce-1b99-d5e6-67f62793c7c3"},"outputs":[],"source":"# Any results you write to the current directory are saved as output.\nlung = dicom.read_file('../input/sample_images/00cba091fa4ad62cc3200a657aeb957e/38c4ff5d36b5a6b6dc025435d62a143d.dcm')\n\nslice = lung.pixel_array\nslice[slice == -2000] = 0\nplt.imshow(slice, cmap=plt.cm.gray)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fe3f348-0b2c-a67e-80c9-602b3713d4db"},"outputs":[],"source":"def read_ct_scan(folder_name):\n        # Read the slices from the dicom file\n        slices = [dicom.read_file(folder_name + filename) for filename in os.listdir(folder_name)]\n        \n        # Sort the dicom slices in their respective order\n        slices.sort(key=lambda x: int(x.InstanceNumber))\n        \n        # Get the pixel values for all the slices\n        slices = np.stack([s.pixel_array for s in slices])\n        slices[slices == -2000] = 0\n        return slices"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8da9d5aa-fe8e-f4d2-1363-f092b6fa8116"},"outputs":[],"source":"ct_scan = read_ct_scan('../input/sample_images/00cba091fa4ad62cc3200a657aeb957e/') "},{"cell_type":"markdown","metadata":{"_cell_guid":"073aac07-d63c-d0b9-51d9-ce17dce987ce"},"source":"To visualise the slices, we will have to plot them. `matplotlib` is used for plotting the slices. The `plot_ct_scan` function takes a 3D CT Scanned Image array  as input and plots equally spaced slices. The CT Scans are grayscale images i.e. the value of each pixel is a single sample, which means it carries only intensity information."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25258e5c-c625-209a-d4e8-dd3a6ab11586"},"outputs":[],"source":"def plot_ct_scan(scan):\n    f, plots = plt.subplots(int(scan.shape[0] / 20) + 1, 4, figsize=(25, 25))\n    for i in range(0, scan.shape[0], 5):\n        plots[int(i / 20), int((i % 20) / 5)].axis('off')\n        plots[int(i / 20), int((i % 20) / 5)].imshow(scan[i], cmap=plt.cm.bone) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15c592a0-6a06-4e50-7daa-6a0d663a756b"},"outputs":[],"source":"plot_ct_scan(ct_scan)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6eb619f3-c336-c8f3-963e-da7d70d3c89d"},"source":"## Segmentation of Lungs ##\nAfter reading the CT Scan, the first step in preprocessing is the segmentation of lung structures because it is obvious that the regions of interests lies inside the lungs. It is visible that the lungs are the darker regions in the CT Scans. The bright region inside the lungs are the blood vessels or air. A threshold of 604(-400 HU) is used at all places because it was found in experiments that it works just fine. We segment lung structures from each slice of the CT Scan image and try not to loose the possible region of interests attached to the lung wall. There are some nodules which may be attached to the lung wall.\n\nI will first explain a common method using simple Image Processing and Morphological operations to segment the lungs and then will give references and summaries to good links of papers. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b050dfab-7f5a-eb0a-825f-84e8166fba92"},"outputs":[],"source":"def get_segmented_lungs(im, plot=False):\n    \n    '''\n    This funtion segments the lungs from the given 2D slice.\n    '''\n    if plot == True:\n        f, plots = plt.subplots(8, 1, figsize=(5, 40))\n    '''\n    Step 1: Convert into a binary image. \n    '''\n    binary = im < 604\n    if plot == True:\n        plots[0].axis('off')\n        plots[0].imshow(binary, cmap=plt.cm.bone) \n    '''\n    Step 2: Remove the blobs connected to the border of the image.\n    '''\n    cleared = clear_border(binary)\n    if plot == True:\n        plots[1].axis('off')\n        plots[1].imshow(cleared, cmap=plt.cm.bone) \n    '''\n    Step 3: Label the image.\n    '''\n    label_image = label(cleared)\n    if plot == True:\n        plots[2].axis('off')\n        plots[2].imshow(label_image, cmap=plt.cm.bone) \n    '''\n    Step 4: Keep the labels with 2 largest areas.\n    '''\n    areas = [r.area for r in regionprops(label_image)]\n    areas.sort()\n    if len(areas) > 2:\n        for region in regionprops(label_image):\n            if region.area < areas[-2]:\n                for coordinates in region.coords:                \n                       label_image[coordinates[0], coordinates[1]] = 0\n    binary = label_image > 0\n    if plot == True:\n        plots[3].axis('off')\n        plots[3].imshow(binary, cmap=plt.cm.bone) \n    '''\n    Step 5: Erosion operation with a disk of radius 2. This operation is \n    seperate the lung nodules attached to the blood vessels.\n    '''\n    selem = disk(2)\n    binary = binary_erosion(binary, selem)\n    if plot == True:\n        plots[4].axis('off')\n        plots[4].imshow(binary, cmap=plt.cm.bone) \n    '''\n    Step 6: Closure operation with a disk of radius 10. This operation is \n    to keep nodules attached to the lung wall.\n    '''\n    selem = disk(10)\n    binary = binary_closing(binary, selem)\n    if plot == True:\n        plots[5].axis('off')\n        plots[5].imshow(binary, cmap=plt.cm.bone) \n    '''\n    Step 7: Fill in the small holes inside the binary mask of lungs.\n    '''\n    edges = roberts(binary)\n    binary = ndi.binary_fill_holes(edges)\n    if plot == True:\n        plots[6].axis('off')\n        plots[6].imshow(binary, cmap=plt.cm.bone) \n    '''\n    Step 8: Superimpose the binary mask on the input image.\n    '''\n    get_high_vals = binary == 0\n    im[get_high_vals] = 0\n    if plot == True:\n        plots[7].axis('off')\n        plots[7].imshow(im, cmap=plt.cm.bone) \n        \n    return im"},{"cell_type":"markdown","metadata":{"_cell_guid":"519aca96-b180-bde5-70d7-8b5bcf1beedf"},"source":"The `get_segmented_lungs` function segments a 2D slice of the CT Scan. I have outputted the slice after all steps for better visualisation and understanding of the code and applied operations."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77669c2c-4605-a181-2e72-33d63ab75e2c"},"outputs":[],"source":"get_segmented_lungs(ct_scan[71], True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"21972b9a-3374-1a31-1763-4ad7090e4e01"},"source":"Now, I will segment the whole CT Scan slice by slice and show some slices of the CT Scan."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b6fb3b9-fba4-fb83-9cc4-7b2f08727421"},"outputs":[],"source":"def segment_lung_from_ct_scan(ct_scan):\n    return np.asarray([get_segmented_lungs(slice) for slice in ct_scan])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40c6c98d-35c3-7511-9020-24398b6b2193"},"outputs":[],"source":"segmented_ct_scan = segment_lung_from_ct_scan(ct_scan)\nplot_ct_scan(segmented_ct_scan)"},{"cell_type":"markdown","metadata":{"_cell_guid":"13f4ee23-96e6-79fd-8780-3a954d3236f0"},"source":"Nodule Candidate/Region of Interest Generation\n---------------------------\nAfter segmenting the lung structures from the CT Scanned images, our task is to find the candidate regions with nodules since the search space is very large. Also, whole image can't be classified directly using 3D CNNs due to limit on computation, we need to find possible regions of cancer and then classify them. It was found in experiments that all the region of interests have intensity >  604(-400 HU). So, we used this threshold to filter the darker regions. This reduces the number of candidates by a large number and preserves all the important regions with high recall. We then classify all the candidate points to reduce the False Positives."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"529c0891-5476-83a0-bc42-35c2697bd31f"},"outputs":[],"source":"segmented_ct_scan[segmented_ct_scan < 604] = 0\nplot_ct_scan(segmented_ct_scan)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b10b58a9-b62f-a6b6-3fd5-74747eea565f"},"source":"After filtering, there are still lot of noise because of blood vessels. Thus we further remove the two largest connected component."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfccfed2-24da-a5dc-a421-81f9d934bf8e"},"outputs":[],"source":"selem = ball(2)\nbinary = binary_closing(segmented_ct_scan, selem)\n\nlabel_scan = label(binary)\n\nareas = [r.area for r in regionprops(label_scan)]\nareas.sort()\n\nfor r in regionprops(label_scan):\n    max_x, max_y, max_z = 0, 0, 0\n    min_x, min_y, min_z = 1000, 1000, 1000\n    \n    for c in r.coords:\n        max_z = max(c[0], max_z)\n        max_y = max(c[1], max_y)\n        max_x = max(c[2], max_x)\n        \n        min_z = min(c[0], min_z)\n        min_y = min(c[1], min_y)\n        min_x = min(c[2], min_x)\n    if (min_z == max_z or min_y == max_y or min_x == max_x or r.area > areas[-3]):\n        for c in r.coords:\n            segmented_ct_scan[c[0], c[1], c[2]] = 0\n    else:\n        index = (max((max_x - min_x), (max_y - min_y), (max_z - min_z))) / (min((max_x - min_x), (max_y - min_y) , (max_z - min_z)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"28422634-588e-2d23-6c2d-437b449c3925"},"source":"The `plot_3d` function plots the 3D numpy array of CT Scans. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"573ae277-f18c-246b-d7b0-b8d590156737"},"outputs":[],"source":"def plot_3d(image, threshold=-300):\n    \n    # Position the scan upright, \n    # so the head of the patient would be at the top facing the camera\n    p = image.transpose(2,1,0)\n    p = p[:,:,::-1]\n    \n    verts, faces = measure.marching_cubes(p, threshold)\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n    mesh = Poly3DCollection(verts[faces], alpha=0.1)\n    face_color = [0.5, 0.5, 1]\n    mesh.set_facecolor(face_color)\n    ax.add_collection3d(mesh)\n\n    ax.set_xlim(0, p.shape[0])\n    ax.set_ylim(0, p.shape[1])\n    ax.set_zlim(0, p.shape[2])\n\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7720ade7-d544-08d6-8e60-ffacb72266a6"},"outputs":[],"source":"plot_3d(segmented_ct_scan, 604)"},{"cell_type":"markdown","metadata":{"_cell_guid":"efb46d44-5528-cd02-e5ff-36e1ce7cdbbe"},"source":"There are more methods that can be tried to get the regions of interest.\n\n 1.  The left blood vessels can further be filtered using shape properties because we know that nodules are spherical in shape.\n 2. Super pixel segmentation can be further used and shape properties can be applied on segmented regions.\n 3. CNN architectures like UNet can also be used to generate candidate regions of interest. \n\nThere are also a lot of good papers on Lung Segmentation and Nodule Candidate generation using Image Processing methods. I am posting the methods with a little summary of all of them:\n\n 1. [Automatic segmentation of lung nodules with growing neural gas and support vector machine][1]- The proposed method consists of the acquisition of computerized tomography images of the lung, the reduction of the volume of interest through techniques for the extraction of the thorax, extraction of the lung, and reconstruction of the original shape of the parenchyma. After that, growing neural gas (GNG) is applied to constrain even more the structures that are denser than the pulmonary parenchyma (nodules, blood vessels, bronchi, etc.). The next stage is the separation of the structures resembling lung nodules from other structures, such as vessels and bronchi. Finally, the structures are classified as either nodule or non-nodule, through shape and texture measurements together with support vector machine.\n 2. [Automated Segmentation of Lung Regions using Morphological Operators in CT scan][2]- The `get_segmented_lungs` method is a little modification of this paper. The threshold of 604 was taken from this paper.\n 3. [Pre-processing methods for nodule detection in lung CT][3]- This paper has used dot enhancement\nfilter applied to the 3D matrix of voxel data. This 3D filter attempts to determine local geometrical characteristics for each voxel, computing the eigenvalues of the Hessian matrix and evaluating a  \"likelihood\" function that was purposely built to discriminate between local morphology of linear, planar and spherical objects, modeled as having 3D Gaussian sections (Q. Li, S. Sone and K. Doi [6]). By applying this 3D filter to artificial images, we have verified the efficiency in detecting the Gaussian-like regions even in the cases were they are superimposed to non-Gaussian ones.\n 4. [Lung Nodule Detection using a Neural Classifier][4]: This paper discusses a dot-enhancement filter for nodule candidate selection and a neural classifier for false-positive finding reduction. The  performance is evaluated as a fully automated computerized method for the detection of lung nodules in screening CT in the identification of lung cancers that may be missed during visual interpretation. \n\nNext I will be discussing about preprocessing of the LUNA16 dataset and using it to train the UNet model.\n\n\n  [1]: http://www.sciencedirect.com/science/article/pii/S0010482512001412\n  [2]: http://www.ijser.org/onlineResearchPaperViewer.aspx?Automated-Segmentation-of-Lung-Regions-using-Morphological-Operators-in-CT-scan.pdf\n  [3]: https://arxiv.org/pdf/physics/0507153.pdf\n  [4]: http://www.ijetch.org/papers/136-D058.pdf"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d43d7eb-85a8-acec-2884-b1678f11c0b1"},"source":"UNET for Candidate Point Generation\n-----------------------------------\n\nNowadays, Deep Learning methods are achieving good results in Segmentation problems in medical imaging. One very famous architecture is UNET, which can be used for Nodule Candidates Points Generation in our case. Training of these networks is done using annotated datasets. The image processing methods used for candidate points generation above does not need any training data. We use the LUNA16 dataset for training our UNET model. \n\nIn the LUNA16 dataset, each CT Scan is annotated with nodule points and the radius of the nodule which is used to generate the binary mask.  I will first talk about preprocessing of the LUNA16 dataset. In the dataset, the CT Scans are saved in '.mhd' files and SimpleITK is used to read the image.  I have defined three functions:\n\n - **load_itk** - Used to read a CT_Scan for the '.mhd' file.\n - **world_2_voxel**- Convert world coordinates to voxel coordinates.\n - **voxel_2_world**- Convert voxel coordinates to world coordinates."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe22f727-6d92-e67f-7e6a-9f4ef6b221b4"},"outputs":[],"source":"'''\nThis funciton reads a '.mhd' file using SimpleITK and return the image array, \norigin and spacing of the image.\n'''\ndef load_itk(filename):\n    # Reads the image using SimpleITK\n    itkimage = sitk.ReadImage(filename)\n    \n    # Convert the image to a  numpy array first and then shuffle the dimensions to get axis in the order z,y,x\n    ct_scan = sitk.GetArrayFromImage(itkimage)\n    \n    # Read the origin of the ct_scan, will be used to convert the coordinates from world to voxel and vice versa.\n    origin = np.array(list(reversed(itkimage.GetOrigin())))\n    \n    # Read the spacing along each dimension\n    spacing = np.array(list(reversed(itkimage.GetSpacing())))\n    \n    return ct_scan, origin, spacing\n\n'''\nThis function is used to convert the world coordinates to voxel coordinates using \nthe origin and spacing of the ct_scan\n'''\ndef world_2_voxel(world_coordinates, origin, spacing):\n    stretched_voxel_coordinates = np.absolute(world_coordinates - origin)\n    voxel_coordinates = stretched_voxel_coordinates / spacing\n    return voxel_coordinates\n\n'''\nThis function is used to convert the voxel coordinates to world coordinates using \nthe origin and spacing of the ct_scan.\n'''\ndef voxel_2_world(voxel_coordinates, origin, spacing):\n    stretched_voxel_coordinates = voxel_coordinates * spacing\n    world_coordinates = stretched_voxel_coordinates + origin\n    return world_coordinates"},{"cell_type":"markdown","metadata":{"_cell_guid":"b6098f54-d336-e3bb-b822-145f53a83804"},"source":"After reading the 3D CT Scans, we will first segment the lungs and then generate the binary mask of nodule regions. This will be done by the `create_nodule_mask` function.  The `draw_circle` function is used to mark the nodule regions in the binary mask. 'cands' are the list of nodule points with the radius given in the `annotation.csv` file of LUNA16 dataset. At the end we save the resized CT Scan with its segmented lungs and binary mask of nodules."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8fc04a3-04db-046d-02c2-9b8f156df8da"},"outputs":[],"source":"def seq(start, stop, step=1):\n\tn = int(round((stop - start)/float(step)))\n\tif n > 1:\n\t\treturn([start + step*i for i in range(n+1)])\n\telse:\n\t\treturn([])\n\n'''\nThis function is used to create spherical regions in binary masks\nat the given locations and radius.\n'''\ndef draw_circles(image,cands,origin,spacing):\n\t#make empty matrix, which will be filled with the mask\n\tRESIZE_SPACING = [1, 1, 1]\n    image_mask = np.zeros(image.shape)\n\n\t#run over all the nodules in the lungs\n\tfor ca in cands.values:\n\t\t#get middel x-,y-, and z-worldcoordinate of the nodule\n\t\tradius = np.ceil(ca[4])/2\n\t\tcoord_x = ca[1]\n\t\tcoord_y = ca[2]\n\t\tcoord_z = ca[3]\n\t\timage_coord = np.array((coord_z,coord_y,coord_x))\n\n\t\t#determine voxel coordinate given the worldcoordinate\n\t\timage_coord = world_2_voxel(image_coord,origin,spacing)\n\n\t\t#determine the range of the nodule\n\t\tnoduleRange = seq(-radius, radius, RESIZE_SPACING[0])\n\n\t\t#create the mask\n\t\tfor x in noduleRange:\n\t\t\tfor y in noduleRange:\n\t\t\t\tfor z in noduleRange:\n\t\t\t\t\tcoords = world_2_voxel(np.array((coord_z+z,coord_y+y,coord_x+x)),origin,spacing)\n\t\t\t\t\tif (np.linalg.norm(image_coord-coords) * RESIZE_SPACING[0]) < radius:\n\t\t\t\t\t\timage_mask[np.round(coords[0]),np.round(coords[1]),np.round(coords[2])] = int(1)\n\t\n\treturn image_mask\n\n'''\nThis function takes the path to a '.mhd' file as input and \nis used to create the nodule masks and segmented lungs after \nrescaling to 1mm size in all directions. It saved them in the .npz\nformat. It also takes the list of nodule locations in that CT Scan as \ninput.\n'''\ndef create_nodule_mask(imagePath, maskPath, cands):\n\t#if os.path.isfile(imagePath.replace('original',SAVE_FOLDER_image)) == False:\n\timg, origin, spacing = load_itk(imagePath)\n\n\t#calculate resize factor\n    RESIZE_SPACING = [1, 1, 1]\n\tresize_factor = spacing / RESIZE_SPACING\n\tnew_real_shape = img.shape * resize_factor\n\tnew_shape = np.round(new_real_shape)\n\treal_resize = new_shape / img.shape\n\tnew_spacing = spacing / real_resize\n\t\n\t#resize image\n\tlung_img = scipy.ndimage.interpolation.zoom(img, real_resize)\n    \n    # Segment the lung structure\n\tlung_img = lung_img + 1024\n\tlung_mask = segment_lung_from_ct_scan(lung_img)\n\tlung_img = lung_img - 1024\n\n\t#create nodule mask\n\tnodule_mask = draw_circles(lung_img,cands,origin,new_spacing)\n\n\tlung_img_512, lung_mask_512, nodule_mask_512 = np.zeros((lung_img.shape[0], 512, 512)), np.zeros((lung_mask.shape[0], 512, 512)), np.zeros((nodule_mask.shape[0], 512, 512))\n\n\toriginal_shape = lung_img.shape\t\n\tfor z in range(lung_img.shape[0]):\n\t\toffset = (512 - original_shape[1])\n\t\tupper_offset = np.round(offset/2)\n\t\tlower_offset = offset - upper_offset\n\n\t\tnew_origin = voxel_2_world([-upper_offset,-lower_offset,0],origin,new_spacing)\n\n\t\tlung_img_512[z, upper_offset:-lower_offset,upper_offset:-lower_offset] = lung_img[z,:,:]\n\t\tlung_mask_512[z, upper_offset:-lower_offset,upper_offset:-lower_offset] = lung_mask[z,:,:]\n\t\tnodule_mask_512[z, upper_offset:-lower_offset,upper_offset:-lower_offset] = nodule_mask[z,:,:]\n\n    # save images.    \n\tnp.save(imageName + '_lung_img.npz', lung_img_512)\n\tnp.save(imageName + '_lung_mask.npz', lung_mask_512)\n\tnp.save(imageName + '_nodule_mask.npz', nodule_mask_512)\n        "},{"cell_type":"markdown","metadata":{"_cell_guid":"a7bb0fd3-919c-a7bd-f6c6-888b752b4882"},"source":"After preprocessing the dataset, the next thing is to train the model for segmentation. The model is written in keras in the `unet_model` function. It takes a 2D slice as input and returns a 2D slice of the same size as output. There are few things to be kept in mind while training\n\n - We wont use the slices that has no nodule region in the mask for training.\n - Dataset augmentation is very important because are nodules are generally circular or spherical in shape and are of different radius. \n - Since the nodule regions are very less, the dataset is skewed. Thus, we should weight the loss function accordingly.\n - The model may overfit on the training dataset. Thus, Dropout or Spatial Dropout are used to avoid overfitting."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b0d0538b-6f58-3ee1-0329-cc03eb7177b6"},"outputs":[],"source":"# change the loss function\ndef dice_coef(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n\treturn -dice_coef(y_true, y_pred)\n\n'''\nThe UNET model is compiled in this function.\n'''\ndef unet_model():\n\tinputs = Input((1, 512, 512))\n\tconv1 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(inputs)\n\tconv1 = Dropout(0.2)(conv1)\n\tconv1 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv1)\n\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n\tconv2 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(pool1)\n\tconv2 = Dropout(0.2)(conv2)\n\tconv2 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv2)\n\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n\tconv3 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(pool2)\n\tconv3 = Dropout(0.2)(conv3)\n\tconv3 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv3)\n\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n\tconv4 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(pool3)\n\tconv4 = Dropout(0.2)(conv4)\n\tconv4 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv4)\n\tpool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n\tconv5 = Convolution2D(1024, 3, 3, activation='relu', border_mode='same')(pool4)\n\tconv5 = Dropout(0.2)(conv5)\n\tconv5 = Convolution2D(1024, 3, 3, activation='relu', border_mode='same')(conv5)\n\n\tup6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n\tconv6 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(up6)\n\tconv6 = Dropout(0.2)(conv6)\n\tconv6 = Convolution2D(512, 3, 3, activation='relu', border_mode='same')(conv6)\n\n\tup7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n\tconv7 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(up7)\n\tconv7 = Dropout(0.2)(conv7)\n\tconv7 = Convolution2D(256, 3, 3, activation='relu', border_mode='same')(conv7)\n\n\tup8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n\tconv8 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(up8)\n\tconv8 = Dropout(0.2)(conv8)\n\tconv8 = Convolution2D(128, 3, 3, activation='relu', border_mode='same')(conv8)\n\n\tup9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n\tconv9 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(up9)\n\tconv9 = Dropout(0.2)(conv9)\n\tconv9 = Convolution2D(64, 3, 3, activation='relu', border_mode='same')(conv9)\n\n\tconv10 = Convolution2D(1, 1, 1, activation='sigmoid')(conv9)\n\n\tmodel = Model(input=inputs, output=conv10)\n\tmodel.summary()\n\tmodel.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, metrics=[dice_coef])\n\n\treturn model"},{"cell_type":"markdown","metadata":{"_cell_guid":"3b5c9580-5c23-3827-5006-68a5840ea2ff"},"source":"## Classification of Lung Nodules ##\n\nThe candidate regions generated still has a lot of noise. Thus we need a classifier to classify the candidates as either Nodule or Non-Nodule.  This is also referred to as False Positive Reduction step.  I am sharing a list of useful resources with a summary of all of them:\n\n 1. [Pulmonary Nodule Classification with Convolutional Neural Networks][1]: They use a CNN to predict\nwhether the image contains a pulmonary lesion. As baselines, they also look at using SVM, kNN, and logistic regression to perform the same task. LIDC dataset is used for training and the classification results are very good.\n 2. [Pulmonary Nodule Classification with Deep Convolutional Neural Networks on Computed Tomography Images][2]: They design a deep convolutional neural networks method for nodule classification, which has an advantage of autolearning representation and strong generalization ability. A specified network structure for nodule images is proposed to solve the recognition of three types of nodules, that is, solid, semisolid, and ground glass opacity (GGO). Deep convolutional neural networks are trained by 62,492 regions-of-interest (ROIs) samples including 40,772 nodules and 21,720 nonnodules from the Lung Image Database Consortium (LIDC) database. Experimental results demonstrate the effectiveness of the proposed method in terms of sensitivity and overall accuracy and that it consistently outperforms the competing methods.\n 3. [Deep Learning for the Classification of Lung Nodules][3]: They have trained a CNN and tested the results on different datasets.  In order to understand the features of lung nodules, they further construct new datasets, based on the combination of artificial geometric nodules and some transformations of the original images, as well as a stochastic nodule shape model.\n 4. [Lung Nodule Classification Based on Deep Convolutional Neural Networks][4]: In this work, they present a method for classifying lung nodules based on CNNs. Training is performed by balancing the mini-batches on each stochastic gradient descent (SGD) iteration to address the lack of nodule samples compared to background samples. They show that the method outperforms a base feature-engineering method using the same techniques for other stages of lung nodule detection, and show that CNNs obtain competitive results.\n\n\n  [1]: http://cs231n.stanford.edu/reports2016/324_Report.pdf\n  [2]: https://www.hindawi.com/journals/cmmm/2016/6215085/\n  [3]: https://arxiv.org/abs/1611.06651\n  [4]: http://www.liv.ic.unicamp.br/~juliomb/resources/2016-lnc-ciarp.pdf"},{"cell_type":"markdown","metadata":{"_cell_guid":"e6a41966-4946-15d3-bbd5-94c493b107e4"},"source":"Due to computational limitations the size of each cannot very large too. I will be considering cubic voxels of size 36. In the dataset, we have 1187 nodule points, which is a very less number for training a deep network. Thus we can cut many voxels around a nodule center and increase the size of the dataset for training. We will also sample equal number of negative examples from the image for training. The code for preprocessing the LUNA16 dataset is below:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61793d31-ef9a-2692-dfa4-d597ae25cb68"},"outputs":[],"source":"'''\nThis function takes the numpy array of CT_Scan and a list of coords from\nwhich voxels are to be cut. The list of voxel arrays are returned. We keep the \nvoxels cubic because per pixel distance is same in all directions.\n'''\ndef get_patch_from_list(lung_img, coords, window_size = 10):\n\tshape = lung_img.shape\n\toutput = []\n\tlung_img = lung_img + 1024\n\tfor i in range(len(coords)):\n\t\tpatch =   lung_img[coords[i][0] - 18: coords[i][0] + 18,\n\t\t\t\t\t\t   coords[i][1] - 18: coords[i][1] + 18,\n\t\t\t\t\t\t   coords[i][2] - 18: coords[i][2] + 18]\t\t\t   \n\t\toutput.append(patch)\n\t\n\treturn output\n\n'''\nSample a random point from the image and return the coordinates. \n'''\ndef get_point(shape):\n\tx = randint(50, shape[2] - 50)\n\ty = randint(50, shape[1] - 50)\n\tz = randint(20, shape[0] - 20)\n\treturn np.asarray([z, y, x])\n\n'''\nThis function reads the training csv file which contains the CT Scan names and\nlocation of each nodule. It cuts many voxels around a nodule and takes random points as \nnegative samples. The voxels are dumped using pickle. It is to be noted that the voxels are \ncut from original Scans and not the masked CT Scans generated while creating candidate\nregions.\n'''\ndef create_data(path, train_csv_path):\n\tcoords, trainY = [], []\n\t\n\twith open(train_csv_path, 'rb') as f:\n\t\tlines = f.readlines()\n\t\t\n\t\tfor line in lines:\n\t\t\trow = line.split(',')\n\t\t\t\n\t\t\tif os.path.isfile(path + row[0] + '.mhd') == False:\n\t\t\t\tcontinue\n\n\t\t\tlung_img = sitk.GetArrayFromImage(sitk.ReadImage(path + row[0] + '.mhd'))\n\n\t\t\tfor i in range(-5, 5, 3):\n\t\t\t\tfor j in range(-5, 5, 3):\n\t\t\t\t\tfor k in range(-2, 3, 2):\n\t\t\t\t\t\tcoords.append([int(row[3]) + k, int(row[2]) + j, int(row[1]) + i])\n\t\t\t\t\t\ttrainY.append(True)\n\n\t\t\tfor i in range(60):\t\t\t\n\t\t\t\tcoords.append(get_point(lung_img.shape))\n\t\t\t\ttrainY.append(False)\n\n\t\t\ttrainX = get_patch_from_list(lung_img, coords)\n\n\t\t\tprint (trainX[0].shape, len(trainX))\n\n\t\t\tpickle.dump(np.asarray(trainX), open('traindata_' + str(counter) + '_Xtrain.p', 'wb'))\n\t\t\tpickle.dump(np.asarray(trainY, dtype = bool),  open('traindata_' + str(counter) + '_Ytrain.p', 'wb'))\n\n\t\t\tcounter = counter + 1\n\t\t\tcoords, trainY = [], []"},{"cell_type":"markdown","metadata":{"_cell_guid":"a98f6934-7a12-317c-0072-847a909f902e"},"source":"In the processing, there are few things that must be tried:\n\n 1. Do not sample negative examples randomly from CT Scan. Select using the candiate points or the False Positives Generated, because it will train the model to discriminate non-nodule regions better.\n 2. Dataset Augmentation should be done while training because the nodules are symmetric regions and of varying sizes, thus dataset augmentation is very likely to help in such cases. Dataset augmentation can implemented using keras given at [this][1] link.\n\nNext is the script for training a classifier with 3D Convolutional Neural Networks. In the preprocessed pickle data files, randomly select 20% files and put it in a seperate folder as test set. The model is trained in keras. \n\n\n  [1]: https://keras.io/preprocessing/image/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"023cadff-bfa2-dd6d-0bd2-4bae8e518bd0"},"outputs":[],"source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution3D, MaxPooling3D\nfrom keras.utils import np_utils\nfrom keras import backend as K\n\n'''\nCreates a keras model with 3D CNNs and returns the model.\n'''\ndef classifier(input_shape, kernel_size, pool_size):\n\tmodel = Sequential()\n\n\tmodel.add(Convolution3D(16, kernel_size[0], kernel_size[1], kernel_size[2],\n\t                        border_mode='valid',\n\t                        input_shape=input_shape))\n\tmodel.add(Activation('relu'))\n\tmodel.add(MaxPooling3D(pool_size=pool_size))\n\tmodel.add(Convolution2D(32, kernel_size[0], kernel_size[1], kernel_size[2]))\n\tmodel.add(Activation('relu'))\n\tmodel.add(MaxPooling3D(pool_size=pool_size))\n\tmodel.add(Convolution3D(64, kernel_size[0], kernel_size[1], kernel_size[2]))\n\tmodel.add(Activation('relu'))\n\tmodel.add(MaxPooling3D(pool_size=pool_size))\n\tmodel.add(Dropout(0.25))\n\n\tmodel.add(Flatten())\n\tmodel.add(Dense(512))\n\tmodel.add(Activation('relu'))\n\tmodel.add(Dropout(0.5))\n\tmodel.add(Dense(128))\n\tmodel.add(Activation('relu'))\n\tmodel.add(Dropout(0.5))\n\tmodel.add(Dense(2))\n\tmodel.add(Activation('softmax'))\n\n\treturn model\n\ndef train_classifier(input_shape):\n\tmodel = classifier(input_shape, (3, 3, 3), (2, 2, 2))\n\tmodel.compile(loss='categorical_crossentropy',\n\t\t  optimizer='adadelta',\n\t\t  metrics=['accuracy'])\n\t'''\n\tRead the preprocessed datafiles chunk by chunnk and train the model on that batch (trainX, trainY) using:'''\n\tmodel.train_on_batch(trainX, trainY, sample_weight=None)\n\t'''The model can be trained on many epochs using for loops'''\n\t\n\t'''\n\tAFter training the dataset we test our model of the test dataset, read the test file chunk by chunk and \n\ttest on each chunk (trainX, trainY) using:'''\n\tprint (model.test_on_batch(trainX, trainY, sample_weight=None))\n\t'''The average of all of this can be taken to obtain the final test score.'''\n\t\n\t'''After testing save the model using'''\n\tmodel.save('my_model.h5')\n    "},{"cell_type":"markdown","metadata":{"_cell_guid":"972a24f0-de40-1dcf-980b-5a157cf1ae8c"},"source":"This is a simple 3D CNN architecture for classification. There are few things that anyone should try to improve the accuracy:\n1) Use of multi-scale CNNs to capture more features for classification.\n2) Extract the features of fully connected layers and use xgboost on it to improve classification accuracy.\n\nIn summary, we first generate candidates using Image Processing or UNet and then cut voxels around all candidate points and classify them. The points are clustered using DBScan algorithm and probability of a cluster is the mean of probabilities of all points in that cluster. The activation of the fully connected network of the classifier is extracted and this features is created for each CT Scan in our dataset. There is one more xgboost or SVM classifier on our dataset to classify the samples. We take the mean of the probabilities of this classifier as the final output probability.\n\nThe code of xgboost classifier for use with trainX containing features and trainY containing labels is below."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b4bc2c9-1647-38ee-88bb-f3319709fe26"},"outputs":[],"source":"'''\nA sample code to train xgboost. \nThe OptTrain class takes trainX(features) and trainY(labels) as input and initialise the class for \ntraining the classifier. The optimize function then trains the classifier and the best model is saved\nin 'model_best.pkl'.\n\nA sample code is below:\ntrainX = np.array([[1,1,1], [2,3,4], [2,3,2], [1,1,1], [2,3,4], [2,3,2]])\ntrainY = np.array([0,1,0,0,0,1])\na = OptTrain(trainX, trainY)\na.optimize()\n'''\nimport xgboost as xgb\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.externals import joblib\nfrom sklearn import model_selection\nimport numpy as np\n\nclass OptTrain:\n    def __init__(self, trainX, trainY):\n        self.trainX = trainX\n        self.trainY = trainY\n\n        self.level0 = xgb.XGBClassifier(learning_rate=0.325,\n                                       silent=True,\n                                       objective=\"binary:logistic\",\n                                       nthread=-1,\n                                       gamma=0.85,\n                                       min_child_weight=5,\n                                       max_delta_step=1,\n                                       subsample=0.85,\n                                       colsample_bytree=0.55,\n                                       colsample_bylevel=1,\n                                       reg_alpha=0.5,\n                                       reg_lambda=1,\n                                       scale_pos_weight=1,\n                                       base_score=0.5,\n                                       seed=0,\n                                       missing=None,\n                                       n_estimators=1920, max_depth=6)\n        self.h_param_grid = {'max_depth': hp.quniform('max_depth', 1, 13, 1),\n                        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n                        'learning_rate': hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n                        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n                        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n                        'n_estimators': hp.quniform('n_estimators', 100, 1000, 5),\n                        }\n        self.to_int_params = ['n_estimators', 'max_depth']\n\n    def change_to_int(self, params, indexes):\n        for index in indexes:\n            params[index] = int(params[index])\n\n    # Hyperopt Implementatation\n    def score(self, params):\n        self.change_to_int(params, self.to_int_params)\n        self.level0.set_params(**params)\n        score = model_selection.cross_val_score(self.level0, self.trainX, self.trainY, cv=5, n_jobs=-1)\n        print('%s ------ Score Mean:%f, Std:%f' % (params, score.mean(), score.std()))\n        return {'loss': score.mean(), 'status': STATUS_OK}\n\n    def optimize(self):\n        trials = Trials()\n        print('Tuning Parameters')\n        best = fmin(self.score, self.h_param_grid, algo=tpe.suggest, trials=trials, max_evals=200)\n\n        print('\\n\\nBest Scoring Value')\n        print(best)\n\n        self.change_to_int(best, self.to_int_params)\n        self.level0.set_params(**best)\n        self.level0.fit(self.trainX, self.trainY)\n        joblib.dump(self.level0,'model_best.pkl', compress=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b3c58fb1-6aa9-4fc6-b9e7-7189c188317b"},"source":"I am currently working on training a classifier on the LUNA16 dataset. I will make the results available with dataset augmentation and about making a sample submission on the kernel as soon as it I complete it.\n\nI will post the dataset augmentation functions and Spatial Dropout very soon. I will post more methods as soon I implement them.\n\nAfter getting the regions of interest, 3D voxels can be cut around regions of interest and used for classification.\n\n**Please upvote or leave a comment, if you liked the tutorial.**"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}