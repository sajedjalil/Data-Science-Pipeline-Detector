{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d4986e65-7d43-4c78-af4d-2a3592ace5bd"},"source":"Hello Everyone. The following code is a work on image-processing which aims to make understanding and working with DICOM data-set simple for any user. As you will go ahead you will find that minute details such as why to convert to HU values ,what DICOM data-set looks like, what is there in DICOM's Pixel Data attribute, what does those value represent corresponding to CT Scan etc. are emphasized for making readers know the background flow of steps. This is  basically our the first submission on Kaggle platform. Thanks to you guys for bringing up such a good problem definition which made us ponder on learning new concepts and techniques in the field of computer aided image processing."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d4f04802-89b7-5b16-52a2-7c742a7ae5f8"},"outputs":[],"source":"# All imports and data path\n\nimport os\nimport pandas as pds\nimport dicom\nimport numpy as npy\nimport matplotlib.pyplot as ppt\nimport scipy.ndimage\n\nfrom skimage import measure, morphology\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n\n\nmain_dir ='../input/sample_images/' # Input DIR which contains data of all the patients\npatients_data = os.listdir(main_dir) # List of folders of all patients which conatins scan data\n#label_set = pds.read_csv('../input/stage1_labels.csv', index_col=0)\n\n#label_set.head() # or alternatively one can use \"print(label_set)\" if one wishes to see entire file for all the patients  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0039d5f6-5567-f987-2871-dfd519babfaa"},"outputs":[],"source":"# 'Voxels' is a term used more often in the entire code from here onwards and it represents each slice of the thoracic volume\n\n# Reading all voxels of each patient in a list named voxels, this list will be manipulated here onwards\n\ndef voxel_read(list_Patients_Data):\n    \n    # thanks to --> https://www.kaggle.com/sentdex/data-science-bowl-2017/first-pass-through-data-w-3d-convnet for precise and dynamic idea of reading\n\n    for patient_data in patients_data [:1]:\n        #label = label_set.get_value(patient_data, 'cancer')\n        path_each_pat = main_dir + patient_data\n\n        \n    \n        voxels= [dicom.read_file(path_each_pat + '/' + img) for img in os.listdir(path_each_pat)]\n        voxels.sort(key = lambda x:int(x.InstanceNumber)) # Instance Number is an attribute of DICOM, it identifies images in series (Image ID)\n                                                          #or\n                                                          #voxels.sort(key=lambda x:int(x.ImagePositionPatient[2]))\n    \n        #Calculating the attribute SliceThickness and adding it in the list voxels\n\n        slice_thickness= npy.abs(voxels[0].ImagePositionPatient[2]-voxels[1].ImagePositionPatient[2]) #one of the important attributes which will used in later processing\n                                                                                                            \n        print(\"Thickness of Each slice: %f \" %slice_thickness)\n        \n        for img in voxels:\n            img.SliceThickness=slice_thickness\n            \n        print(\"No. of VOXELS : \", len(voxels),\"\\t\")     \n            \n    \n        return npy.array(list(voxels))\n    \nv=voxel_read(patients_data) \nprint(\"\\nDICOM file of a slice:\\n\\n\")  # This gives an actual DICOM file of one of the scanned image.\nprint(v[10])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb8fc46b-d174-f2b5-c8bc-82e4eec27468"},"outputs":[],"source":"#Gray-scale plot of some of the slices of a patient\n\n\"\"\"X=ppt.figure(figsize=(20,20))\n\n\nfor i,image in enumerate (v [50:74]):\n    X.add_subplot(6,4,i+1)\n    img1= image.pixel_array\n    ppt.imshow(img1,cmap=ppt.cm.gray)\nppt.show()\"\"\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cbbb989d-8709-efc2-4b61-c403c4540696"},"outputs":[],"source":"# understanding and visualizing values contained in DICOM data which corresponds to pixel data\n\n# NOTE: for purpose of efficient storage values stored in Pixel Data are Attenuation Coefficient\n# Lets visualize these data\n\nppt.imshow(v[100].pixel_array,cmap=ppt.cm.gray)\nppt.show()\nprint(v[100].pixel_array[10]) # pixel_array is a numpy function to read data in to nD-array"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e74604e-23f9-de76-26ec-619251977d96"},"outputs":[],"source":"#HounsField Conversion fun\n\ndef attValues_to_HU(voxels):\n\n    #Getting a join sequence of all the voxels(slices) in a patient \n    #NOTE: Each voxel which is a list contains atteunation coefficent of scanned thoraic portion respectively\n    Array_of_voxels = npy.stack([v.pixel_array for v in voxels])\n    #print(Array_of_voxels)  #this print stt. is only for once to check the output\n    \n    #Attenuation values are effficent for storage on disk, but in memory compution requires HU values\n    #HU ranges from nearly -1000 to some thousand on +ve scale, \n    #Thus conversion to type interger 16 bit is necessary for attenutation coefficient values\n    Array_of_voxels=Array_of_voxels.astype(npy.int16)\n    # print(Array_of_voxels[10][10])\n    \n    #omitting unwanted -2000 att. coeff. for box shaped images\n    Array_of_voxels[Array_of_voxels==-2000]=0\n    #print(Array_of_voxels[10][10])  #this print stt. is only for once to check the output\n    \n    \n    #Any voxel or sclice can be chosen for a patient, they will contain same SLOPE and INTERCEPT VALUE\n    Slope=voxels[0].RescaleSlope\n    Intercept=voxels[0].RescaleIntercept\n    \n    print(\"\\n\\nRESCALE SLOPE \\t RESCALE INTERCEPT: \",Slope,\"\\t\",Intercept)\n\n    if Slope != 1: #which is generally 1 in most of the case \n        Array_of_voxels= Slope * Array_of_voxels.astype(npy.float64)\n        Array_of_voxels= Array_of_voxels.astype(npy.int16)\n    \n\n    Array_of_voxels= npy.int16(Intercept) + Array_of_voxels   \n    #print(Array_of_voxels[100])\n\n    return npy.array(Array_of_voxels, dtype=npy.int16)\n\n\nVoxel_HU_List=attValues_to_HU(v)\n\nprint(\"\\n\\n Array contaning HU values of PIXEL DATA: \\n\\n\")\nprint(Voxel_HU_List)\nprint(\"\\n\\n Single row values of a VOXEL: \\n\\n\")\nprint(Voxel_HU_List[100][10])\n\n#Histogram Plotting of one patient\n\nlist_of_voxels= Voxel_HU_List\n\nprint(\"\\n\\n HISTOGRAM PLOT OF HU VALUES OF A PATIENT: \\n\\n\")\n\nppt.hist(list_of_voxels.flatten(), bins=50, color='crimson',histtype='bar')\nppt.xlabel(\"Hounsfield Units (HU)\")\nppt.ylabel(\"Frequency\")\nppt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e671bc9e-34e4-e553-9f9c-2efe79cd16e0"},"outputs":[],"source":"#Experimental code for Sampling\n\nPixel_Spacing = map(float, ([v[0].SliceThickness] + v[0].PixelSpacing))\nPixel_Spacing = npy.array(list(Pixel_Spacing))\nprint(\"\\n\\nPixel Spacing values as --> [SLICE_THICKNESS\\t PIXEL_SPACING_x\\t PIXEL_SPACING_y]  :\\n\")\nprint(Pixel_Spacing)\nprint(\"\\n\\nShape of the scanned thorax --> (LENGHT, WIDTH, HEIGHT) in terms of pixels: \\n\")\nprint(Voxel_HU_List.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60740405-5857-f308-2f28-cf90a8af5e45"},"outputs":[],"source":"# RESAMPLING (basically resizing the no of pixels keeping dimensional measurement same)\n\ndef Resample(slices, images, new_pixel_spacing=[1,1,1]):\n    \n    Pixel_Spacing = map(float, ([slices[0].SliceThickness] + slices[0].PixelSpacing))\n    Pixel_Spacing = npy.array(list(Pixel_Spacing))\n    \n     # some lines from: https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial\n         # The mathematical computation is same all other way, which very precisely is described below   \n    \n    resize_factor = Pixel_Spacing / new_pixel_spacing\n    new_real_shape = images.shape * resize_factor\n    new_shape = npy.round(new_real_shape)\n    real_resize_factor = new_shape / images.shape\n    new_pixel_spacing = Pixel_Spacing / real_resize_factor\n\n    Array_of_voxels = scipy.ndimage.interpolation.zoom(images, real_resize_factor)\n    \n    return Array_of_voxels, new_pixel_spacing\n\nimg2,res=Resample(v,Voxel_HU_List,[1,1,1])\nprint(\"\\n\\n New SHAPE , NEW SPACING:\", img2.shape,res)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a427649-a405-b43e-c9c9-403710a05423"},"outputs":[],"source":"# Lets get the Pixel values standardized or basically normalized to a standarad\n# This step is crucial as it used for feature scaling which later will help machince learning algos. like CLASSIFERS for better categorization\n#There are two of such methods namely: Rescaling and Standardization , here later is preferred as we deal with muti-D array values.\n# For more info on Standardization one can look for Wikipedia :)\n\n\nfor img in Voxel_HU_List[:100]:\n    # getting Standard Scores for feature sampling\n    mean = npy.mean(img)\n    std = npy.std(img)\n    img = img- mean\n    img = img/std      # from formula x'= (x-x_avg)/s\n    \nprint(img[200])\n\nppt.hist(img.flatten(), bins=50, color='crimson',histtype='bar')\nppt.xlabel(\"Standardized Values of HU\")\nppt.ylabel(\"Frequency\")\nppt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94e0341e-520f-4e6f-74f1-0f26bac9916c"},"outputs":[],"source":"# Filter operation for distinction of lung area\n\n#from skimage import filters\n\n#edge_img = filters.sobel(Voxel_HU_List[55])\n#ppt.imshow(edge_img,cmap=ppt.cm.gray)\n#ppt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f838a8f-25b4-4c1a-a429-9569092811d3"},"outputs":[],"source":"#SEGMENTATION using Global Thresholding method\n\nimg=Voxel_HU_List[56]\nmax_ = npy.max(img)\nprint(max_)\nmin_ = npy.min(img)\nprint(min_)\n#thres=(max_+min_)/2     #Initial threshold using max and min brightness values\n\nthres=npy.mean(img)\n\nprint(thres)\nth0=0\n\narray1=img[img<=thres]\nprint(len(array1))      # First segmented cluster\narray2=img[img>thres]\nprint(len(array2))      # Second segmented cluster\nprint(array1)\nprint(array2)\n\nmean1= npy.mean(array1)\nmean2= npy.mean(array2)\n\n\n#while abs(thres-th0)<10^-6:\n    #th0=thres\n    #thres=(mean1+mean2)/2\n    #print (thres)\n    #array1=img[img<=thres]\n    #array2=img[img>thres]\n    \n     #mean1= npy.mean(array1)\n    #mean2= npy.mean(array2)\n    \n    \n#print(thres)   \n\nthres_img = npy.where(img<thres,1.0,0.0)\nppt.imshow(thres_img,cmap=ppt.cm.gray)\nppt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7c093200-4362-f279-a966-2dab7d6d1ded"},"outputs":[],"source":"import cv2\n\nimg=Voxel_HU_List[56]\nimg = npy.array(img * 255, dtype = npy.uint8)\nret,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\nth2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,\\\n            cv2.THRESH_BINARY,11,2)\nth3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n            cv2.THRESH_BINARY,11,2)\ntitles = ['Original Image', 'Global Thresholding (v = 127)',\n            'Adaptive Mean Thresholding', 'Adaptive Gaussian Thresholding']\nimages = [img, th1, th2, th3]\n\nX=ppt.figure(figsize=(10,10))\nfor i in range(4):\n    X.add_subplot(2,2,i+1),\n    ppt.imshow(images[i],'gray')\n    ppt.title(titles[i])\n    ppt.xticks([]),ppt.yticks([])\nppt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da91e4e6-7b7d-4911-b26c-d54c4dafb880"},"outputs":[],"source":"def plot_3d(image, threshold=-300):\n    \n    # Position the scan upright, \n    # so the head of the patient would be at the top facing the camera\n    p = npy.transpose(image, axes=(2,1,0)) # image.transpose(1,0,2)\n    \n    verts, faces = measure.marching_cubes(p, threshold)\n\n    fig = ppt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n    mesh = Poly3DCollection(verts[faces], alpha=0.70)\n    face_color = [0.45, 0.45, 0.75]\n    mesh.set_facecolor(face_color)\n    ax.add_collection3d(mesh)\n\n    ax.set_xlim(0, p.shape[0])\n    ax.set_ylim(0, p.shape[1])\n    ax.set_zlim(0, p.shape[2])\n\n "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7052d4a8-b781-2258-4dc4-51cc212311d7"},"outputs":[],"source":"##plot_3d(img2, 400)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"367aeffd-b192-39b7-3bf2-12085a7aa656"},"outputs":[],"source":"def largest_label_volume(im, bg=-1):\n    vals, counts = npy.unique(im, return_counts=True)\n\n    counts = counts[vals != bg]\n    vals = vals[vals != bg]\n\n    if len(counts) > 0:\n        return vals[npy.argmax(counts)]\n    else:\n        return None\n\ndef segment_lung_mask(image, fill_lung_structures=True):\n    \n    # not actually binary, but 1 and 2. \n    # 0 is treated as background, which we do not want\n    binary_image = npy.array(image > -320, dtype=npy.int8)+1\n    labels = measure.label(binary_image)\n    \n    # Pick the pixel in the very corner to determine which label is air.\n    #   Improvement: Pick multiple background labels from around the patient\n    #   More resistant to \"trays\" on which the patient lays cutting the air \n    #   around the person in half\n    background_label = labels[0,0,0]\n    \n    #Fill the air around the person\n    binary_image[background_label == labels] = 2\n    \n    \n    # Method of filling the lung structures (that is superior to something like \n    # morphological closing)\n    if fill_lung_structures:\n        # For every slice we determine the largest solid structure\n        for i, axial_slice in enumerate(binary_image):\n            axial_slice = axial_slice - 1\n            labeling = measure.label(axial_slice)\n            l_max = largest_label_volume(labeling, bg=0)\n            \n            if l_max is not None: #This slice contains some lung\n                binary_image[i][labeling != l_max] = 1\n\n    \n    binary_image -= 1 #Make the image actual binary\n    binary_image = 1-binary_image # Invert it, lungs are now 1\n    \n    # Remove other air pockets insided body\n    labels = measure.label(binary_image, background=0)\n    l_max = largest_label_volume(labels, bg=0)\n    if l_max is not None: # There are air pockets\n        binary_image[labels != l_max] = 0\n \n    return binary_image\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14381a29-09bc-70b9-c40a-aa370f7b4f5a"},"outputs":[],"source":"\n\nsegmented_lungs = segment_lung_mask(img2, False)\nsegmented_lungs_fill = segment_lung_mask(img2, True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ba12d82-26b5-8277-44dd-5d4105b47724"},"outputs":[],"source":"\n\n#plot_3d(segmented_lungs, 0)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"060dd2bf-5262-5d0c-5a35-91b51b70503a"},"outputs":[],"source":"images= segmented_lungs_fill - segmented_lungs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bb99fcd-4143-19dd-5d1d-e5c3b7d2c74b"},"outputs":[],"source":"\n\nMIN_BOUND = -1000.0\nMAX_BOUND = 400.0\n    \ndef normalize(images):\n    images = (images - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n    images[images>1] = 1.\n    images[images<0] = 0.\n    return images\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"810d0448-3679-40da-393f-d95ad698937c"},"outputs":[],"source":"PIXEL_MEAN = 0.25\n\ndef zero_center(images):\n    images = images - PIXEL_MEAN\n    return images"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32594774-2982-ad55-3d7b-f354804ad944"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}