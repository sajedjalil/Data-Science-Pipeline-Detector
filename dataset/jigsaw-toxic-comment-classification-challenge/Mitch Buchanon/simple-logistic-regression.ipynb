{"nbformat":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.3","pygments_lexer":"ipython3","name":"python"}},"cells":[{"metadata":{"run_control":{"read_only":false},"_cell_guid":"37e09e49-234c-4ac5-96e6-0e9e5254f920","_uuid":"11d8d14230a24771fe29a46e4a05017e6d0a4955","button":false,"new_sheet":false},"source":"# Logistic regression-based classifier\n\n\n### Loading of the data\n\nThis notebook shows how to train and evaluate a simple logistic regression-based classifier. Thanks to High Flying Bird for providing the program, I merely translated it from French to English!","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"4fce0abf-780e-4401-a80a-a993b75f25b0","new_sheet":false,"run_control":{"read_only":false},"_uuid":"736cb1ce51df87be84fe3f0d2bcf46faf0880d43","button":false,"collapsed":true},"source":"import pandas as pd\ndata = pd.read_csv('../input/train.csv')","cell_type":"code","outputs":[]},{"metadata":{"run_control":{},"_cell_guid":"c9a6ecbb-15c1-42a6-8c1b-84ab828b9ffc","_uuid":"4e438d86096cb5530805155427769cbd325e0ac6"},"source":"### Comments vectorization\nWe consider a vocabulary of 1000 unigrams and bigrams at most. The vocabulary is only composed of elements present at least 4 times and in less than 50% of the comments. Every comment is projected in this space according to the tf-idf score.","cell_type":"markdown"},{"execution_count":null,"metadata":{"run_control":{},"_cell_guid":"dc16bb91-b2a8-4476-a248-f9d038f2d95b","_uuid":"d7e8422402b2ef14e0cbfd2861df6134ca49b7c6","collapsed":true},"source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2),\n                             max_df=0.5,\n                             min_df=4,\n                             max_features=1000)\nvector_space_model = vectorizer.fit_transform(data['comment_text'].tolist())\nn_comments = vector_space_model.shape[0]\nprint('%d comments total' % n_comments)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f6a757aa-2fc2-4ace-bc93-4893ef592a81","_uuid":"a85d31d0c6f8654bc38e0cea3eec191e8345d07a"},"source":"### Preparation of the data for the learning and evaluation of the classifier\nWe split the data in two subsets, with 1/3 of the comments used for the learning, and the other 2/3 being used for the evaluation.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"f1567229-040a-4a56-88d6-53252b29c348","_uuid":"35c04aa151e6bef59f2576ec76782fd29facc56e","collapsed":true},"source":"training_set_size = int(n_comments * 0.33)\nX = vector_space_model[:training_set_size,:]\nZ = vector_space_model[training_set_size:vector_space_model.shape[0]-1,:]\nprint('%d comments for the estimation of the parameters and %d for the evaluation' % \n      (X.shape[0], Z.shape[0]))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"8188d298-0d21-41a9-87c7-a7133a391dfa","_uuid":"3dd3546cb72c5113e433e002607d13dcedf2b541"},"source":"### Training of the classifer for the \"toxic\" category\nWe estimate the parameters of the logistic regression, with L2 penalty (i.e. Ridge), for the \"toxic\" category. Beforehand we assign X of a dense representation instead of a sparse one. ","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"ac1761ca-fa05-4d16-aa14-9600f8bfc958","_uuid":"f3d7c64bee8baa82e00d73470dbe61680a0b022f","collapsed":true},"source":"from sklearn import linear_model\nX = X.toarray()\nY = data['toxic'][:training_set_size]\nmodel = linear_model.BayesianRidge(verbose=True)\nmodel.fit(X, Y)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"202fd3e8-b2b7-40d4-9dc6-9af679f1fdff","_uuid":"011cc6dff3a20def3f31fad610d04f254e648d34"},"source":"### Evaluation of the classifier\nWe measure the likelihood of each comment to belong to the \"toxic\" category. The comments for which the likelihood is strictly over 1/2 are considered as \"toxic\". We sum up the results thanks to the confusion matrix.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"ec97dd86-7176-486d-952e-a142f2452e34","_uuid":"782316a144b32d0426913fa550f253677857a85d","collapsed":true},"source":"from sklearn.preprocessing import binarize\nfrom sklearn.metrics import confusion_matrix\nground_truth = data['toxic'][training_set_size:vector_space_model.shape[0]-1]\nprediction = model.predict(Z)\nprediction = binarize(prediction.reshape(-1, 1), 0.5)\nconfusion_matrix(ground_truth, prediction)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"51758c42-baba-42cd-9318-4f9589339dfd","_uuid":"682ae22238f3e0026db752012afa41e7cbb32bcf"},"source":"### Analysis of the predictions","cell_type":"markdown"},{"execution_count":null,"metadata":{"_cell_guid":"bb7f9d27-6be1-4c66-b76d-0703c0bff457","_uuid":"ae03c33b06e443ff6c306365aec9b570384b88f2","collapsed":true},"source":"toxic_ids = [i for i, c in enumerate(prediction) if c == 1]","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"9922cec0-f5d3-4474-993f-2668dd442da0","_uuid":"e68ad48a289b4c36b19c3ceabe82be42de8c4e03","collapsed":true},"source":"comment_id = toxic_ids[0]\nprint('Content of the comment: \\n%s\\n' % data['comment_text'][training_set_size+comment_id])\nprint('Is this comment \"toxic\" according to the model?\\n%s' % str(model.predict(Z[comment_id,:]) >0.5))","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_cell_guid":"3c056c67-6350-466b-b876-96a49a2f13d6","_uuid":"5763ebdc56f792d0ee945c30d6737f7871cf1ed6","collapsed":true},"source":"","cell_type":"code","outputs":[]}],"nbformat_minor":1}