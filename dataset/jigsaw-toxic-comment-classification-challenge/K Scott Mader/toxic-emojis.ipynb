{"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"# Overview\nHere we try to focus on just the emojis to figure out which of them are toxic","metadata":{"_cell_guid":"88b36a6b-6bb5-4cdd-8fd4-0f5c43bf1903","_uuid":"3ae4f06c309793cb1b206300f9c0f56c5fd2e85c"}},{"cell_type":"code","execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing import text as keras_text, sequence as keras_seq\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","outputs":[],"metadata":{"_cell_guid":"d06b6ce7-d93b-424b-ace1-2464e13f90ae","_uuid":"e41e62413248c1a87934d5c5e8bb0390b2481add"}},{"cell_type":"markdown","source":"# Load and Preprocessing Steps\nHere we load the data and fill in the misisng values","metadata":{"_cell_guid":"6ddb5d07-801c-4c82-8fb4-24fd71567dc0","_uuid":"96d5f7c706b6a11fbc2a2530c97dfefb4faf209d"}},{"cell_type":"code","execution_count":null,"source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain = train.sample(frac=1)\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values\n\nall_sentences = np.concatenate([list_sentences_train, list_sentences_test])","outputs":[],"metadata":{"_cell_guid":"c8bbaffa-b53b-4840-b47f-2d5bb117fc53","_uuid":"d6b8d61f1c16e0a7e1713d31428c5d01097a985c","collapsed":true}},{"cell_type":"code","execution_count":null,"source":"# We add all of the possible categories together as a proxy for level of toxicity\ntotal_toxicity = np.sum(y,1)\nprint('Distribution of Total Toxicity Labels (important for validation)')\nprint(pd.value_counts(total_toxicity))","outputs":[],"metadata":{"_cell_guid":"a121ad8b-d39d-4f38-8f0d-06a50710689a","_uuid":"84805fa62ace58a65ff7d2b26dfa5b9fadad4678"}},{"cell_type":"markdown","source":"## Sequence Generation\nHere we take the data and generate sequences from the data. We have an unusual preprocessing step because we want to remove everything that is normal text or number","metadata":{"_cell_guid":"ebe64376-9126-4f7f-b10d-388a8f3bc77d","_uuid":"ee41735cbf0933aec3590d0da6a8b6d59c7ea608"}},{"cell_type":"code","execution_count":null,"source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re, string\nnums_chars = ''.join(['{}'.format(i) for i in range(10)])\nre_prep = re.compile(f'([{string.ascii_letters}{nums_chars} ;:.,\\t!?\\-_\\(\\)\\[\\]])') # non-character things\nre_prep = re.compile(r'[^\\u263a-\\U0001f645]') # just emojis\ndef prep_func(s): return re_prep.sub('', s)\nprint('Verify we are keeping the right things:',\n      prep_func('Hello my name (Âµ-998)\\t and I [ðŸ˜] emojis ;-)'))\nvec = CountVectorizer(preprocessor=prep_func, \n                      analyzer = 'char', \n                     binary = True)","outputs":[],"metadata":{"_cell_guid":"2d715de6-d361-4a47-9082-76b3e6c1b3c9","_uuid":"f312edcbb9afbe08523e3fdeb08e558a1ef1fa87"}},{"cell_type":"code","execution_count":null,"source":"%%time\nvec.fit(all_sentences)\nvocab_lookup = {idx: k for k,idx in vec.vocabulary_.items()}\nprint(len(vocab_lookup), 'unique characters found')","outputs":[],"metadata":{"_cell_guid":"7a472046-efc7-4941-a084-7d240b9de292","_uuid":"de0935723205e23a7b7838c396eef9debc984929"}},{"cell_type":"code","execution_count":null,"source":"X_train = vec.transform(list_sentences_train)\nX_test = vec.transform(list_sentences_test)","outputs":[],"metadata":{"_cell_guid":"d20fc51b-a2af-435b-9baf-47c60d954407","_uuid":"674a4fe6d153f1cfc8f2e29ae2fbaa351a18641b","collapsed":true}},{"cell_type":"markdown","source":"# Simple Model\nHere we create a simple random forest model for determining which characters relate to toxicity. We make a simple split to start and then fit the model on one group and then validate it on the other","metadata":{"_cell_guid":"a6c4a179-4049-44bf-87c0-5e095670cce9","_uuid":"c133153f5b6b8200878f22bce39a216d479c017a"}},{"cell_type":"code","execution_count":null,"source":"from sklearn.model_selection import train_test_split\nX_t_train, X_t_test, y_train, y_test = train_test_split(X_train, \n                                                        total_toxicity, \n                                                        test_size = 0.2, \n                                                        stratify = total_toxicity,\n                                                       random_state = 2017)\nprint('Training:', X_t_train.shape)\nprint('Testing:', X_t_test.shape)","outputs":[],"metadata":{"_cell_guid":"00db0eea-f8c1-4d74-82ee-499d8d63ab3c","_uuid":"0c7d698963639d37463d7256438dc85518865706"}},{"cell_type":"code","execution_count":null,"source":"from sklearn.ensemble import RandomForestRegressor\nbasic_rf = RandomForestRegressor()\nbasic_rf.fit(X_t_train, y_train)","outputs":[],"metadata":{"_cell_guid":"41cd3c6b-72c1-48d3-b6b1-6b6e33772d97","_uuid":"fce7b3b19204451da59402cdc1452db6addb64e6"}},{"cell_type":"code","execution_count":null,"source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, roc_auc_score\ny_pred = basic_rf.predict(X_t_test)\nfig, ax1 = plt.subplots(1,1)\ntpr, fpr, _ = roc_curve(y_test>0, y_pred)\nax1.plot(tpr, fpr, 'b.', label = 'ROC Curve')\nax1.plot(tpr, tpr, 'r-', label = 'Random Guessing')\nax1.set_ylabel('True Positive Rate')\nax1.set_xlabel('False Positive Rate')","outputs":[],"metadata":{"_cell_guid":"1c4ce644-94cc-487e-9971-3096699ce34d","_uuid":"6f5b4848c9eadc7c179c226f745897013eb85a58"}},{"cell_type":"markdown","source":"# Character Importance\nWe can now show the importance of each character for deciding if a message is toxic or not","metadata":{"_cell_guid":"fdb59682-a1d7-4eda-8c28-5309beab8549","_uuid":"2d57d28db3dfe93a3a282dacec6dc4f050b0d0c6"}},{"cell_type":"code","execution_count":null,"source":"show_characters = 100\nfor i in np.argsort(-1*basic_rf.feature_importances_)[:show_characters]:\n    print(vocab_lookup[i], '\\t%2.2f%%' % (100*basic_rf.feature_importances_[i]))","outputs":[],"metadata":{"_cell_guid":"9f1ff657-6129-4c00-9756-1915b6db7389","_uuid":"467cc9819d7545bff9eb2a18dc51780d92408f8e"}},{"cell_type":"markdown","source":"# Logistic Regression\nHere we apply cross-validated logistic regression to determine which characters are important","metadata":{"_cell_guid":"05032baa-bade-426a-b87c-d8d64300256e","_uuid":"4de322cf1453258d0019c9b0a9b70ad77cfbb743"}},{"cell_type":"code","execution_count":null,"source":"%%time\nfrom sklearn.linear_model import LogisticRegressionCV\nbasic_logreg = LogisticRegressionCV()\nbasic_logreg.fit(X_train, total_toxicity>0)","outputs":[],"metadata":{"_cell_guid":"90de75ae-3a9c-4b62-b165-ced65afbe463","_uuid":"1dbe8f9f111c55e385d917c591a6c27fcdcc1906"}},{"cell_type":"markdown","source":"# Show the most significant characters\nHere we show the characters by the ones with the largest coefficients","metadata":{"_cell_guid":"6fb8d7a0-7229-4f39-b56b-bdac0425cdac","_uuid":"c681624bd3b66835f846df047f843dc492f2bf71"}},{"cell_type":"code","execution_count":null,"source":"show_characters = 100\nfor i in np.argsort(-1*np.abs(basic_logreg.coef_[0,:]))[:show_characters]:\n    print(vocab_lookup[i], '\\t%03.2f%%' % (100*basic_logreg.coef_[0,i]))","outputs":[],"metadata":{"_cell_guid":"b08d6f92-990d-4eff-a1dd-584ad2704416","scrolled":false,"_uuid":"4007ed23f0065529f39279ed1c0cb275ed414dbc"}},{"cell_type":"code","execution_count":null,"source":"","outputs":[],"metadata":{"_cell_guid":"b4ac252d-7daa-4ab6-a9cf-38fc6fa88223","_uuid":"a80ee3203a429bdcd13fd5765ff4f0fdcfc7dbbe","collapsed":true}}],"nbformat":4,"metadata":{"language_info":{"file_extension":".py","name":"python","mimetype":"text/x-python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}}}