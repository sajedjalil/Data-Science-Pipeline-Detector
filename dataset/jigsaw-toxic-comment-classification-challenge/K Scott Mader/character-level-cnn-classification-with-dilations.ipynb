{"metadata":{"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","version":"3.6.3","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"metadata":{"_uuid":"3ae4f06c309793cb1b206300f9c0f56c5fd2e85c","_cell_guid":"88b36a6b-6bb5-4cdd-8fd4-0f5c43bf1903"},"source":"# Overview\nThe basic idea of this notebook is to transform the data from a sequence of letters into possible categories using a CNN. We use letters instead of words since we say in the [mothjer](https://www.kaggle.com/fcostartistican/don-t-mess-with-my-mothjer) notebook that words are often misspelled or written differently so looking at character level correlations might work better.  We utilize Atrous Convolutions since they can account for larger spacings between relevant words and ideas. For the model we focus on individual letters and ngrams sized 1-10, but the model could easily be expanded to handle larger differences.","cell_type":"markdown"},{"metadata":{"_uuid":"e41e62413248c1a87934d5c5e8bb0390b2481add","collapsed":true,"_cell_guid":"d06b6ce7-d93b-424b-ace1-2464e13f90ae"},"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\nfrom keras.preprocessing import text as keras_text, sequence as keras_seq\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"f99670815ef29c169168b34a817b60783421e8d1","collapsed":true,"_cell_guid":"71ec20e2-3f9b-49f8-81ea-479747ab3728"},"execution_count":null,"source":"# define network parameters\nmax_features = 64\nmaxlen = 512","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"96d5f7c706b6a11fbc2a2530c97dfefb4faf209d","_cell_guid":"6ddb5d07-801c-4c82-8fb4-24fd71567dc0"},"source":"# Load and Preprocessing Steps\nHere we load the data and fill in the misisng values","cell_type":"markdown"},{"metadata":{"_uuid":"d6b8d61f1c16e0a7e1713d31428c5d01097a985c","collapsed":true,"_cell_guid":"c8bbaffa-b53b-4840-b47f-2d5bb117fc53"},"execution_count":null,"source":"%%time\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain = train.sample(frac=1)\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ee41735cbf0933aec3590d0da6a8b6d59c7ea608","_cell_guid":"ebe64376-9126-4f7f-b10d-388a8f3bc77d"},"source":"## Sequence Generation\nHere we take the data and generate sequences from the data","cell_type":"markdown"},{"metadata":{"_uuid":"bfde767157b1af853bf66ae4a99ec551b486e7d8","collapsed":true,"_cell_guid":"6c84aaf5-155d-475c-be20-d69fa2b7e8db"},"execution_count":null,"source":"tokenizer = keras_text.Tokenizer(char_level = True)\ntokenizer.fit_on_texts(list(list_sentences_train))\n# train data\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nX_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n# test data\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"467cc9819d7545bff9eb2a18dc51780d92408f8e","collapsed":true,"_cell_guid":"9f1ff657-6129-4c00-9756-1915b6db7389"},"execution_count":null,"source":"def build_model(conv_layers = 2, \n                dilation_rates = [0, 2, 4, 8, 16], \n                embed_size = 256):\n    inp = Input(shape=(None, ))\n    x = Embedding(input_dim = len(tokenizer.word_counts)+1, \n                  output_dim = embed_size)(inp)\n    prefilt_x = Dropout(0.25)(x)\n    out_conv = []\n    # dilation rate lets us use ngrams and skip grams to process \n    for dilation_rate in dilation_rates:\n        x = prefilt_x\n        for i in range(2):\n            if dilation_rate>0:\n                x = Conv1D(16*2**(i), \n                           kernel_size = 3, \n                           dilation_rate = dilation_rate,\n                          activation = 'relu',\n                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n                          )(x)\n            else:\n                x = Conv1D(16*2**(i), \n                           kernel_size = 1,\n                          activation = 'relu',\n                          name = 'word_fcl_{}'.format(i))(x)\n        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n    x = concatenate(out_conv, axis = -1)    \n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(32, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(6, activation='sigmoid')(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model\n\nmodel = build_model()\nmodel.summary()","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"dcd322903096a49e1401bd1bf5fa073e9ecf0b11","_cell_guid":"3052969a-a72b-49c2-a595-e75b8573c369"},"source":"# Train the Model\nHere we train the model and use model checkpointing and early stopping to keep only the best version of the model","cell_type":"markdown"},{"metadata":{"_uuid":"a4e639ce318affd65cb2dfc141fd76ac60bccbbf","_cell_guid":"11dd7653-7bda-4611-9eb1-beeb24f06cbd"},"source":"## Hold-out\nWe create a hold-out group of data for having a set of data the model was never exposed to for testing it. We add all of the possible categories together as a cheap hack for ensuring groups are somewhat stratified.","cell_type":"markdown"},{"metadata":{"_uuid":"a7e9d58c9698de39f32e5c0600ea474bd9f5d1cb","collapsed":true,"_cell_guid":"2d550f74-32c7-44e6-8398-8dfb4d9b762e"},"execution_count":null,"source":"from sklearn.model_selection import train_test_split\nany_category_positive = np.sum(y,1)\nprint('Distribution of Total Positive Labels (important for validation)')\nprint(pd.value_counts(any_category_positive))\nX_t_train, X_t_test, y_train, y_test = train_test_split(X_t, y, \n                                                        test_size = 0.2, \n                                                        stratify = any_category_positive,\n                                                       random_state = 2017)\nprint('Training:', X_t_train.shape)\nprint('Testing:', X_t_test.shape)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"2c60302e2756b9fb5137503e29db220ffce955bf","collapsed":true,"_cell_guid":"7b5e74a1-d92e-4705-b478-606121443909"},"execution_count":null,"source":"batch_size = 128 # large enough that some other labels come in\nepochs = 1\n\nfile_path=\"best_weights.h5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n\ncallbacks_list = [checkpoint, early] #early\nmodel.fit(X_t_train, y_train, \n          validation_data=(X_t_test, y_test),\n          batch_size=batch_size, \n          epochs=epochs, \n          shuffle = True,\n          callbacks=callbacks_list)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"58de3d5b2443643c4548e5a6c6dc256b7c49ce49","_cell_guid":"f6792563-9c96-4e2f-b9a9-9d5dd53b2665"},"source":"# Make Predictions\nLoad the model and make predictions on the test dataset","cell_type":"markdown"},{"metadata":{"_uuid":"92532dfaa0211e9b01d9ee3d78f86b3f1e570a3c","collapsed":true,"_cell_guid":"a3442437-db15-4e64-b377-c8bd010e7378"},"execution_count":null,"source":"model.load_weights(file_path)\ny_test = model.predict(X_te)\nsample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nsample_submission[list_classes] = y_test\nsample_submission.to_csv(\"predictions.csv\", index=False)","outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"1dbe8f9f111c55e385d917c591a6c27fcdcc1906","collapsed":true,"_cell_guid":"90de75ae-3a9c-4b62-b165-ced65afbe463"},"execution_count":null,"source":"","outputs":[],"cell_type":"code"}],"nbformat":4,"nbformat_minor":1}