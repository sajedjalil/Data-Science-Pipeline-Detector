{"metadata":{"language_info":{"version":"3.6.3","file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"version":3,"name":"ipython"}},"anaconda-cloud":{},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd \nimport numpy as np\nfrom string import punctuation\nfrom collections import Counter\nfrom tqdm import tqdm\n%matplotlib inline","outputs":[],"metadata":{"_cell_guid":"ca5062ab-58a3-483e-85b9-c78fb4541678","_uuid":"7c00afcb40878f45ca6453a456805850cc569077"},"execution_count":1},{"cell_type":"markdown","source":"## Step 1. Dataset preparation\n\n\n#### Step 1.1 Loading data","metadata":{"_cell_guid":"e15d3267-dcdd-40e7-9016-5e66923f98aa","_uuid":"b9d998bd6d0aab05ee7d538982c276db977a7054"}},{"cell_type":"code","source":"sentiment_data = pd.read_csv('../input/train.csv')\n","outputs":[],"metadata":{"_cell_guid":"e3abd6f8-f237-47dc-b54b-ef04d457d8ff","_uuid":"58bbf8f841851fdf01d0226d19b4f4ce9251e0e5","collapsed":true},"execution_count":2},{"cell_type":"code","source":"sentiment_data.head()","outputs":[],"metadata":{"_cell_guid":"5b79409a-7e8b-4582-bed0-40361f246f9e","_uuid":"2c0c327118cb3809b56e77772588ae11295880c3"},"execution_count":3},{"cell_type":"markdown","source":"#### Step 1.2 Shuffling data","metadata":{"_cell_guid":"ab621cec-0959-4fe3-8775-f640462ba868","_uuid":"7847b984ba922aedc4bc9d367492699629ba624e"}},{"cell_type":"code","source":"from sklearn.utils import shuffle\nsentiment_data = shuffle(sentiment_data)","outputs":[],"metadata":{"_cell_guid":"1323b905-9029-4733-a54f-7d6f43b84659","_uuid":"cddf7a1386265e58044ce3f50578a7148745cb10","collapsed":true},"execution_count":4},{"cell_type":"markdown","source":"#### Step 1.3 Creating the Vocab and the vocab2int","metadata":{"_cell_guid":"1cc4980c-eaa0-432d-b71f-65cfdd568f41","_uuid":"06503c17fc9613024ce0c49b765be9c645dba1d6"}},{"cell_type":"code","source":"labels = sentiment_data.iloc[:, 0].values\nreviews = sentiment_data.iloc[:, 1].values","outputs":[],"metadata":{"_cell_guid":"02eb2fd3-38de-4539-b221-fce114861d1a","_uuid":"75d202724514b34253a758f5a697dd4d6303ecd0","collapsed":true},"execution_count":5},{"cell_type":"code","source":"reviews_processed = []\nunlabeled_processed = [] \nfor review in reviews:\n    review_cool_one = ''.join([char for char in review if char not in punctuation])\n    reviews_processed.append(review_cool_one)","outputs":[],"metadata":{"_cell_guid":"0fefd71d-19ab-48c6-aa8d-2582e116e167","_uuid":"1da5a2cf03119c344ab7c794d5059a558305a5d9","collapsed":true},"execution_count":6},{"cell_type":"code","source":"word_reviews = []\nall_words = []\nfor review in reviews_processed:\n    word_reviews.append(review.lower().split())\n    for word in review.split():\n        all_words.append(word.lower())\n    \ncounter = Counter(all_words)\nvocab = sorted(counter, key=counter.get, reverse=True)\nvocab_to_int = {word: i for i, word in enumerate(vocab, 1)}","outputs":[],"metadata":{"_cell_guid":"698bd5e8-8450-4e8e-977a-d46a90208384","_uuid":"13363a209b55c41a8be3e94f3c51c6534dfac569","collapsed":true},"execution_count":7},{"cell_type":"markdown","source":"#### Step 1.4 Encoding words to ints","metadata":{"_cell_guid":"4caaf351-6499-4b93-a97b-05bf5d534250","_uuid":"d5dddbaf623c23ba5b634244e1929ea68be2f105"}},{"cell_type":"code","source":"reviews_to_ints = []\nfor review in word_reviews:\n    reviews_to_ints.append([vocab_to_int[word] for word in review])","outputs":[],"metadata":{"_cell_guid":"202339ee-07bb-49da-8fde-7c703f93931e","_uuid":"899d1178d64afaf9cd61256316768c96e74b6483","collapsed":true},"execution_count":8},{"cell_type":"markdown","source":"#### Step 1.5 Checking if there was any review with length == 0","metadata":{"_cell_guid":"ea09f4d4-2791-4b60-aa87-1e7cd83690ad","_uuid":"b1959cde73e5ee7f23a0da5829b7b49a7335b4f8"}},{"cell_type":"code","source":"reviews_lens = Counter([len(x) for x in reviews_to_ints])\nprint('Zero-length {}'.format(reviews_lens[0]))\nprint(\"Max review length {}\".format(max(reviews_lens)))","outputs":[],"metadata":{"_cell_guid":"ea05a229-801d-4dc2-b570-b34a16bf9200","_uuid":"1ef44eb0c41433e9456ab9927124c2051453d2e7"},"execution_count":9},{"cell_type":"markdown","source":"#### Step 1.6 Padding the data to the same sequence length","metadata":{"_cell_guid":"ea667d07-ab1b-451f-a70a-bf396d85dfe9","_uuid":"2ad88fe184c6a33b02844e4c55692a6203bcd351"}},{"cell_type":"code","source":"seq_len = 250\n\nfeatures = np.zeros((len(reviews_to_ints), seq_len), dtype=int)\nfor i, review in enumerate(reviews_to_ints):\n    features[i, -len(review):] = np.array(review)[:seq_len]","outputs":[],"metadata":{"_cell_guid":"a9f2b8b3-aefc-4d90-a685-cd7d71441248","_uuid":"5e4935bcdf905e668b918b69087b8043b8db7c10","collapsed":true},"execution_count":10},{"cell_type":"markdown","source":"#### Step 1.7 Creating training and testing sets","metadata":{"_cell_guid":"0e5df796-44c1-4b6d-8e55-ef62ec4e43dd","_uuid":"500de716e9329962da7079ac4cc3873fcc1e3275"}},{"cell_type":"code","source":"X_train = features[:6400]\ny_train = labels[:6400]\n\nX_test = features[6400:]\ny_test = labels[6400:]\n\nprint('X_trian shape {}'.format(X_train.shape))","outputs":[],"metadata":{"_cell_guid":"a5c56a32-19aa-44fb-a6fb-fdf11454e33e","_uuid":"d6bba777a2b2dd7f650f4ca2061b078be6d24d81"},"execution_count":11},{"cell_type":"markdown","source":"## Step 2 Define a model\n\n\n#### Step 2.1 Define functions for creating weights and biases","metadata":{"_cell_guid":"bc3f5dac-3d8c-4694-be50-2bbc63b259b8","_uuid":"97ffbd7cdf7b6c0bea08da2843ac9f4bec943226"}},{"cell_type":"code","source":"def weights_init(shape):\n    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))","outputs":[],"metadata":{"_cell_guid":"1f2a72f8-3482-47d7-aca7-86513049c341","_uuid":"ed98934ddd2ca97b4e547fad30f56cbf95044645","collapsed":true},"execution_count":12},{"cell_type":"code","source":"def bias_init(shape):\n    return tf.Variable(tf.zeros(shape=shape))","outputs":[],"metadata":{"_cell_guid":"ee2ed01a-07ae-4d2e-9d43-35b7ffbc03fb","_uuid":"f80a047d0b91c82d80611b0ca0a7b65f0008ccbb","collapsed":true},"execution_count":13},{"cell_type":"markdown","source":"#### Step 2.2 Define helper functions for the model","metadata":{"_cell_guid":"01a3a5b4-e20e-433f-aaeb-506bebf76b06","_uuid":"d78bcf5e1d81e0d856976f263cc71d745867003e"}},{"cell_type":"code","source":"def define_inputs(batch_size, sequence_len):\n    '''\n    This function is used to define all placeholders used in the network.\n    \n    Input(s): batch_size - number of samples that we are feeding to the network per step\n              sequence_len - number of timesteps in the RNN loop\n              \n    Output(s): inputs - the placeholder for reviews\n               targets - the placeholder for classes (sentiments)\n               keep_probs - the placeholder used to enter value for dropout in the model    \n    '''\n    inputs = tf.placeholder(tf.int32, [batch_size, sequence_len], name='inputs_reviews')\n    targets = tf.placeholder(tf.float32, [batch_size, 1], name='target_sentiment')\n    keep_probs = tf.placeholder(tf.float32, name='keep_probs')\n    \n    return inputs, targets, keep_probs","outputs":[],"metadata":{"_cell_guid":"d1149031-eba7-492f-a72a-1a8f40df2d42","_uuid":"1d4831c851e2dd0041a8a5cd5b7d1b9af9022e96","collapsed":true},"execution_count":14},{"cell_type":"code","source":"def embeding_layer(vocab_size, embeding_size, inputs):\n    '''\n    Function used for creating word embedings (word vectors)\n    \n    Input(s): vocab_size - number of words in the vocab\n              embeding_size - length of a vector used to represent a single word from vocab\n              inputs - inputs placeholder\n    \n    Output(s): embed_expended -  word embedings expended to be 4D tensor so we can perform Convolution operation on it\n    '''\n    word_embedings = tf.Variable(tf.random_uniform([vocab_size, embeding_size]))\n    embed = tf.nn.embedding_lookup(word_embedings, inputs)\n    embed_expended = tf.expand_dims(embed, -1) #expend dims to 4d for conv layer\n    return embed_expended","outputs":[],"metadata":{"_cell_guid":"4b728176-bfd3-4045-bd76-6a8858b5fce1","_uuid":"3d0a80bf8dc03fac30d5022b7a038426004b1531","collapsed":true},"execution_count":15},{"cell_type":"code","source":"def text_conv(input, filter_size, number_of_channels, number_of_filters, strides=(1, 1), activation=tf.nn.relu, max_pool=True):\n    '''\n    This is classical CNN layer used to convolve over embedings tensor and gether useful information from it.\n    \n    Input(s): input - word_embedings\n              filter_size - size of width and height of the Conv kernel\n              number_of_channels - in this case it is always 1\n              number_of_filters - how many representation of the input review are we going to output from this layer \n              strides - how many pixels does kernel move to the side and up/down\n              activation - a activation function\n              max_pool - boolean value which will trigger a max_pool operation on the output tensor\n    \n    Output(s): text_conv layer\n    \n    '''\n    weights = weights_init([filter_size, filter_size, number_of_channels, number_of_filters])\n    bias = bias_init([number_of_filters])\n    \n    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n    \n    if activation != None:\n        layer = activation(layer)\n    \n    if max_pool:\n        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n    \n    return layer","outputs":[],"metadata":{"_cell_guid":"a484f01d-b043-42bd-9d7c-dda2b979e5e0","_uuid":"4d9ee289e9d00056b3b3cbd24abe60aa503fe951","collapsed":true},"execution_count":16},{"cell_type":"code","source":"def lstm_layer(lstm_size, number_of_layers, batch_size, dropout_rate):\n    '''\n    This method is used to create LSTM layer/s for PixelRNN\n    \n    Input(s): lstm_cell_unitis - used to define the number of units in a LSTM layer\n              number_of_layers - used to define how many of LSTM layers do we want in the network\n              batch_size - in this method this information is used to build starting state for the network\n              dropout_rate - used to define how many cells in a layer do we want to 'turn off'\n              \n    Output(s): cell - lstm layer\n               init_state - zero vectors used as a starting state for the network\n    '''\n    def cell(size, dropout_rate=None):\n        layer = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n        \n        return tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n            \n    cell = tf.contrib.rnn.MultiRNNCell([cell(lstm_size, dropout_rate) for _ in range(number_of_layers)])\n    \n    init_state = cell.zero_state(batch_size, tf.float32)\n    return cell, init_state","outputs":[],"metadata":{"_cell_guid":"9bce5266-b5e3-4d20-82cd-9f002853583f","_uuid":"d9da952a296b67623c2e27710217f0c211bb19c1","collapsed":true},"execution_count":17},{"cell_type":"code","source":"def flatten(layer, batch_size, seq_len):\n    '''\n    Used to transform/reshape 4d conv output to 2d matrix\n    \n    Input(s): Layer - text_cnn layer\n              batch_size - how many samples do we feed at once\n              seq_len - number of time steps\n              \n    Output(s): reshaped_layer - the layer with new shape\n               number_of_elements - this param is used as a in_size for next layer\n    '''\n    dims = layer.get_shape()\n    number_of_elements = dims[2:].num_elements()\n    \n    reshaped_layer = tf.reshape(layer, [batch_size, int(seq_len/2), number_of_elements])\n    return reshaped_layer, number_of_elements","outputs":[],"metadata":{"_cell_guid":"c6a3bc7f-754f-4ef9-a748-9a42f58d6e09","_uuid":"653e0d182fbc8ebe1686db953d506088bbd3b795","collapsed":true},"execution_count":18},{"cell_type":"code","source":"def dense_layer(input, in_size, out_size, dropout=False, activation=tf.nn.relu):\n    '''\n    Output layer for the lstm netowrk\n    \n    Input(s): lstm_outputs - outputs from the RNN part of the network\n              input_size - in this case it is RNN size (number of neuros in RNN layer)\n              output_size - number of neuros for the output layer == number of classes\n              \n    Output(s) - logits, \n    '''\n    weights = weights_init([in_size, out_size])\n    bias = bias_init([out_size])\n    \n    layer = tf.matmul(input, weights) + bias\n    \n    if activation != None:\n        layer = activation(layer)\n    \n    if dropout:\n        layer = tf.nn.dropout(layer, 0.5)\n        \n    return layer","outputs":[],"metadata":{"_cell_guid":"54bc9c0d-04ba-479e-b97e-d4c7609fa4be","_uuid":"39fcf3ef8923aa6bcfcd656a0f4609a228aa6ce1","collapsed":true},"execution_count":19},{"cell_type":"code","source":"def loss_optimizer(logits, targets, learning_rate, ):\n    '''\n    Function used to calculate loss and minimize it\n    \n    Input(s): rnn_out - logits from the fully_connected layer\n              targets - targets used to train network\n              learning_rate/step_size\n    \n    \n    Output(s): optimizer - optimizer of choice\n               loss - calculated loss function\n    '''\n    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n    \n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    return loss, optimizer","outputs":[],"metadata":{"_cell_guid":"6fc1ffce-1ed8-41bb-890d-be2056be4ac9","_uuid":"f3fad6dd3d45171a0865ef502e9c74180f399fd2","collapsed":true},"execution_count":20},{"cell_type":"code","source":"class SentimentCNN(object):\n    \n    def __init__(self, learning_rate=0.001, batch_size=100, seq_len=250, vocab_size=10000, embed_size=300,\n                conv_filters=32, conv_filter_size=5, number_of_lstm_layers=1, lstm_units=128):\n        \n        \n        '''\n        To created Sentiment embed network CNN-LSTM create object of this class.\n        \n        Input(s): learning_rate/step_size - how fast are we going to find global minima\n                  batch_size -  the nuber of samples to feed at once\n                  seq_len - the number of timesteps in unrolled RNN\n                  vocab_size - the number of nunique words in the vocab\n                  embed_size - length of word embed vectors\n                  conv_filters - number of filters in output tensor from CNN layer\n                  conv_filter_size - height and width of conv kernel\n                  number_of_lstm_layers - the number of layers used in the LSTM part of the network\n                  lstm_units - the number of neurons/cells in a LSTM layer\n        \n        '''\n        tf.reset_default_graph()\n        self.inputs, self.targets, self.keep_probs = define_inputs(batch_size, seq_len)\n        \n        embed = embeding_layer(vocab_size, embed_size, self.inputs)\n        \n        #Building the network\n        convolutional_part = text_conv(embed, conv_filter_size, 1, conv_filters)\n        conv_flatten, num_elements = flatten(convolutional_part, batch_size, seq_len)\n        \n        cell, init_state = lstm_layer(lstm_units, number_of_lstm_layers, batch_size, self.keep_probs)\n        \n        outputs, states = tf.nn.dynamic_rnn(cell, conv_flatten, initial_state=init_state)\n        \n        review_outputs = outputs[:, -1, :]\n        \n        logits = dense_layer(review_outputs, lstm_units, 1, activation=None)\n        \n        self.loss, self.opt = loss_optimizer(logits, self.targets, learning_rate)\n        \n        preds = tf.nn.sigmoid(logits)\n        currect_pred = tf.equal(tf.cast(tf.round(preds), tf.int32), tf.cast(self.targets, tf.int32))\n        self.accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))","outputs":[],"metadata":{"_cell_guid":"f1ce760e-9da7-4efd-8b34-5d44cf4fb249","_uuid":"74ede758801cada2b6b54230e1688a5adca4f9ac","collapsed":true},"execution_count":21},{"cell_type":"markdown","source":"## Step 3 Training and testing","metadata":{"_cell_guid":"ce9a258d-f0b3-407d-9604-5cfa80bb0458","_uuid":"36baae535afea8cb57dd42d2cb594fe5a6191abf"}},{"cell_type":"code","source":"model = SentimentCNN(learning_rate=0.001, \n                     batch_size=50, \n                     seq_len=250, \n                     vocab_size=len(vocab_to_int) + 1, \n                     embed_size=300,\n                     conv_filters=32, \n                     conv_filter_size=5, \n                     number_of_lstm_layers=1, \n                     lstm_units=128)","outputs":[],"metadata":{"_cell_guid":"449450ec-459f-4f2c-b3d2-ea70480f3a7f","_uuid":"1c71ce777be8f32f9f0e2dc113ed6e90e6bb3910","collapsed":true},"execution_count":22},{"cell_type":"code","source":"session = tf.Session()","outputs":[],"metadata":{"_cell_guid":"bdce7439-b8ad-4652-8f8e-20e617777fe7","_uuid":"53f672b4bdd7f3e43da11f09b8c8d5a70533fd58","collapsed":true},"execution_count":23},{"cell_type":"code","source":"session.run(tf.global_variables_initializer())","outputs":[],"metadata":{"_cell_guid":"381e8d66-eae6-4a6a-bc01-9c06a686d086","_uuid":"a6f41197c6d8ae09cc1f6b56bf5f4cfc2e5d8c90","collapsed":true},"execution_count":24},{"cell_type":"code","source":"epochs = 5\nbatch_size = 50\ndrop_rate = 0.7","outputs":[],"metadata":{"_cell_guid":"14a7c9d5-1962-4d9b-9541-1f0aa7bcb3b8","_uuid":"f218965a7934400a38f441f07c1b8277047ea4e8","collapsed":true},"execution_count":null},{"cell_type":"markdown","source":"#### Step 3.1 Training process","metadata":{"_cell_guid":"7df4d495-f6b9-4d7d-899f-71e894326dee","_uuid":"6ac025e0c4456cc574003f1dcaa859187346c258"}},{"cell_type":"code","source":"for i in range(epochs):\n    epoch_loss = []\n    train_accuracy = []\n    for ii in tqdm(range(0, len(X_train), batch_size)):\n        X_batch = X_train[ii:ii+batch_size]\n        y_batch = y_train[ii:ii+batch_size].reshape(-1, 1)\n        \n        c, _, a = session.run([model.loss, model.opt, model.accuracy], feed_dict={model.inputs:X_batch, \n                                                                                  model.targets:y_batch,\n                                                                                  model.keep_probs:drop_rate})\n        \n        epoch_loss.append(c)\n        train_accuracy.append(a)\n        \n    \n    print(\"Epoch: {}/{}\".format(i, epochs), \" | Epoch loss: {}\".format(np.mean(epoch_loss)), \n          \" | Mean train accuracy: {}\".format(np.mean(train_accuracy)))","outputs":[],"metadata":{"_cell_guid":"73a5229b-c1d5-4bf7-96b1-57ef25840c6c","_uuid":"a367f9d87d7ef592c84295f29371b20ba5dd37ee"},"execution_count":null},{"cell_type":"markdown","source":"#### Step 3.2 Testing process","metadata":{"_cell_guid":"0f338f38-d9e7-4a9b-8f7a-fff832fc0c7f","_uuid":"9a6fe82c27efb5392b83b14302732b4f1b091903"}},{"cell_type":"code","source":"test_accuracy = []\n\nii = 0\nwhile ii + batch_size <= len(X_test):\n    X_batch = X_test[ii:ii+batch_size]\n    y_batch = y_test[ii:ii+batch_size].reshape(-1, 1)\n\n    a = session.run([model.accuracy], feed_dict={model.inputs:X_batch, \n                                                 model.targets:y_batch, \n                                                 model.keep_probs:1.0})\n    \n    test_accuracy.append(a)\n    ii += batch_size","outputs":[],"metadata":{"_cell_guid":"a8c79fe4-875e-4686-9c7e-037ad4edd393","_uuid":"3829858836c94ee424461fa11fd29f7927509005","collapsed":true},"execution_count":null},{"cell_type":"code","source":"print(\"Test accuracy: {}\".format(np.mean(test_accuracy)))","outputs":[],"metadata":{"_cell_guid":"3515473b-6e6a-454d-9ccc-241fa1b6f553","_uuid":"f8a985f71a1c851feccdf355febedcb3f454f486","collapsed":true},"execution_count":null},{"cell_type":"code","source":"session.close()","outputs":[],"metadata":{"_cell_guid":"30552f06-4463-4592-b8cc-ceac18f33490","_uuid":"021e48993338ac23d5547874b38b2a4ad88b782e","collapsed":true},"execution_count":null}],"nbformat_minor":1,"nbformat":4}