{"cells":[{"metadata":{},"cell_type":"markdown","source":"* Analyzed Twitter comments to detect and classify them\n  into different types of toxicity like threats, obscenity,\n  insults and hate.\n* Applied deep learning techniques i.e. RNN, to\n  understand and classify the given sentence into one of\n  the 6 types of toxicity.\n* Achieved 96% accuracy in test dataset.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n#from textblob import TextBlob\n#from spellchecker import SpellChecker\n#from autocorrect import spell\n#from gingerit.gingerit import GingerIt\n#from symspellpy.symspellpy import SymSpell, Verbosity\n#from wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import Word2Vec\n#from glove import Glove, Corpus\nimport tensorflow\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Flatten, SimpleRNN, RNN,GRU, SpatialDropout1D, Dropout\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Dataset into Kernel","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\nX = dataset.iloc[:,1].values\ny = dataset.iloc[:,2:].values\n\ndataset_test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\nX_test = dataset_test.iloc[:,1].values\nX_test = X_test.reshape(153164,1)\n\ntest_labels = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip')\nY_test = test_labels.iloc[:,1:].values\n\nmerged = pd.merge(dataset_test, test_labels, how=\"left\", on=\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged.head()\nY_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged['sum'] = merged['toxic'] + merged['severe_toxic'] + merged['obscene'] + merged['threat'] + merged['insult'] + merged['identity_hate']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged.drop('id',axis=1, inplace=True)\nmerged.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge = merged[merged['sum'] != -6]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = merge.iloc[:,1:7].values\nX_test = merge.iloc[:,0].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization of Data and using Regular Expressions to remove all characters except alphabets and lowercasing them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = []\ntokens = [word_tokenize(str(sentence)) for sentence in X]\nrm = []\nfor w in tokens:\n    sm = re.sub('[^A-Za-z]',' ', str(w))\n    x = re.split(\"\\s\", sm)\n    rm.append(x)\n\n#Removing whitespaces\nfor sent in rm:\n    while '' in sent:\n        sent.remove('')\n\n# Lowercasing\nlow = []\nfor i in rm:\n    i = [x.lower() for x in i]\n    low.append(i)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatization and Removal of Stopwords\nUsing WordNetLemmatizer to obtain the root form of all the words in the dataset. For eg, reducing increased to its root form increase to reduce the number of redundant words and reducing dimensionality of the dataset. Also removed all the stopwords like a, an, the, and, not etc., since all of them are useless words and do not influence the predictions that much.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemma = []\nwnl = WordNetLemmatizer()\nfor doc in low:\n    tokens = [wnl.lemmatize(w) for w in doc]\n    lemma.append(tokens)\n\n# Removing Stopwords\nfilter_words = []\nStopwords = set(stopwords.words('english'))\n\n#ab = spell('nd')\nfor sent in lemma:\n    tokens = [w for w in sent if w not in Stopwords]\n    filter_words.append(tokens)\n\nspace = ' ' \nsentences = []\nfor sentence in filter_words:\n    sentences.append(space.join(sentence))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_words = []\nfor sent in filter_words:\n    token = [word for word in sent if len(word)>2]\n    filtered_words.append(token)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Embedding\nUsing Word2Vec to calculate the relationship between words. Word2Vec gives the value of correlation between two words and formed a pre-trained matrix.\n\n### Word2Vec\nWord2Vec is a group of related models that are used to produce web embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic context of words. Word2Vec takes as input a large corpus and produces a vector space, of several hundred dimensions, with each unique word in corpus being assigned a vector in space. Words are placed such that words having similar meaning in the corpus are located in close proximity to each other. The cosine of angle between vectors of words having similar context should be close to 1, i.e., angle close to 0.\n\nHere comes the idea of distributed representations. Intuitively, we introduce some dependence of one word on the other words. The words in context of the particular word would get a greater share of this dependence.\n\nWord2Vec can utilize either of the two model architectures to produce a distributed representation of words: continuous bag of words (CBOW) or continuous skip gram\n\n#### Continuous Bag of Words (CBOW)\nThis method takes the context of each word as input and tries to predict the word corresponding to the context from a window of surrounding context words. The order of context words does not influence the predictions. We learn the vector representation of the target word.\n \nIn CBOW, the input given is the context word and we try to predict the center words so that the cosine between context word and center word approaches to one. The input layer is the one hot encoding of the context words. The input layer is then multiplied with hidden layer which is the weight matrix and as output we get word vector representation. This hidden input layer is then multiplied to the output layer, i.e., weight matrix Wâ€™ and then the output is fed to the softmax function which calculates the probabilities and the vector with highest probability is chosen to be the vector closest to the input context word.\n\nIn multiple context words, the vectors obtained in hidden input layer are averaged before going further.\n \n#### Skip gram\nSkip gram is just opposite to CBOW. In this algorithm, target word is fed as input to the network. It weighs the nearby context words more heavily than more distant context words. It learns by predicting the surrounding words given a target value.\n \nBoth models are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words. Continuous bag of words is considered to be faster than skip gram but skip gram performs well for small data is found to represent rare words well. CBOW has better representation for more frequent words.\nWord2vec is implemented using Genism package.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cbow = Word2Vec(filtered_words)\nword_vectors = model_cbow.wv\nvocabulary = word_vectors.vocab.items()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cbow.most_similar('mother')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word_vectors.vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling of Unknown words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = list(word_vectors.vocab.keys())\nunk = 0\ntotal = 0\n\nembedding_matrix = word_vectors.vectors\n## Word with their index values\nword2id = {k:v.index for k,v in word_vectors.vocab.items()}\n\n## Unknown values\nUNK_INDEX = 0\nUNK_TOKEN = 'UNK'\nunk_vector = embedding_matrix.mean(0)\n\n## Inserting row for unknown words \nembedding_matrix = np.insert(embedding_matrix, [UNK_INDEX], [unk_vector], axis=0)\nword2id = {word:(index+1) if index >= UNK_INDEX else index for word, index in \n           word2id.items()}\nword2id[UNK_TOKEN] = UNK_INDEX\n\n## Replacing words in x_train with their respective indices and replacing each unknown \n## word with index 0\nL = []\nfor sent in filter_words:\n    Z = []\n    for word in sent:\n        if word in word2id:\n            Z.append(word2id.get(word))\n        else:\n            Z.append(UNK_INDEX)\n            unk+=1\n    L.append(Z)\nX_train = pad_sequences(L, maxlen=100, padding='post',\n                        dtype='float')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing RNN using GRU/LSTM using Keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Implementing RNN using GRU/LSTM\nvocab_len = len(embedding_matrix)\nmodel = Sequential()\nmodel.add(Embedding(vocab_len, 100, input_length = 100,weights=[embedding_matrix]))\nmodel.add(GRU(units=100, activation='tanh'))\n#model.add(LSTM(units=120, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(50,activation='tanh'))\nmodel.add(Dense(6,activation='softmax'))\nmodel.compile(optimizer='adam',loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit(X_train,y,batch_size=1000,epochs=10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing model on test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nx_test = []\nfor sentence in X_test:\n    x_test.append(text_to_word_sequence(str(sentence),filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' '))\n\nfilter_test = []\nfor sent in x_test:\n    tokens = [w for w in sent if w not in Stopwords]\n    filter_test.append(tokens)\n\n## Converting the text into sequences using ids\nL = []\nfor sent in x_test:\n    Z = []\n    for word in sent:\n        if word in word2id and len(word)>2:\n            Z.append(word2id.get(word))\n        else:\n            Z.append(UNK_INDEX)\n            unk+=1\n    L.append(Z)\n\nX_test = pad_sequences(L, maxlen=100, padding= 'post',dtype='float')\ny_pred = model.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X_test,Y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}