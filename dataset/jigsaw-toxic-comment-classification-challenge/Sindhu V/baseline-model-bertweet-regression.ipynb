{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Approach\n\nWill work to evaluate the model. Last competition's public NB that uses combined submission also has a binary output, and a score of 0.97. We'll use those probability scores with weights for each comment type to generate a final regressor score. Bertweet model will get preprocessed inputs (except url and twitter handle preprocessing) and a regressor head on top of its model is finetuned.\n\nThis is Roberta. So we can easily squeeze in 5 checkpoints in 20GB. ","metadata":{"_uuid":"e7e37167-d065-4e32-8ab1-f52b8b35a07e","_cell_guid":"90677e36-9e1b-4c54-ae59-3c26239a1881","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"N_EPOCHS = 1\n\nmname = \"vinai/bertweet-large\"\nCFG = {\"lr\": 1e-5, \"batch_size\": 4, \\\n       \"grad_acc_steps\": 8, \"fp16\": False, \\\n       \"eval_steps\": 250, \"wandb_logging_steps\": 30}\n\nrun_id = \"bertweet_lg_better_baseline\"\nkaggle_source = \"jdoesv/baseline-model-bertweet-regression, V2\"","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:14:07.461914Z","iopub.execute_input":"2021-11-19T19:14:07.462219Z","iopub.status.idle":"2021-11-19T19:14:07.482058Z","shell.execute_reply.started":"2021-11-19T19:14:07.462143Z","shell.execute_reply":"2021-11-19T19:14:07.481429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data prep\n- Training data: Weighted scores from that prediction notebook\n- Validation data: Jigsaw validation data min-max scaled.\n- Both have sentence, label as inputs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n#Sampled in source NB. TODO: Remove that and use weighted DL here. \ntraindf = pd.read_csv(\"../input/baseline-data-prep/cleaned_train_actual.csv\")\ntraindf[\"normed_text\"].fillna(\"\", inplace=True)\ntraindf = traindf.sample(frac=1).reset_index(drop=True)\ntraindf = traindf[[\"normed_text\", \"new_wscore\"]] #Now we need to sample\ntraindf.rename(columns={\"normed_text\": \"sentence\", \"new_wscore\": \"label\"}, inplace=True)\ndisplay(traindf[\"label\"].describe())\n\nvaliddf = pd.read_csv(\"../input/wranged-validation-data/eval_regression.csv\")\nvaliddf = validdf[[\"tclean\", \"reg_rank\"]]\nvaliddf[\"reg_rank\"] = (validdf[\"reg_rank\"]-validdf[\"reg_rank\"].min())/(validdf[\"reg_rank\"].max()-validdf[\"reg_rank\"].min())\nvaliddf.rename(columns={\"tclean\": \"sentence\", \"reg_rank\": \"label\"}, inplace=True)\ndisplay(validdf[\"label\"].describe())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:14:07.48449Z","iopub.execute_input":"2021-11-19T19:14:07.484884Z","iopub.status.idle":"2021-11-19T19:14:08.37617Z","shell.execute_reply.started":"2021-11-19T19:14:07.484846Z","shell.execute_reply":"2021-11-19T19:14:08.375451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training prep\n\nPitfalls\n1. Token length > 512. For now, adopt truncation\n2. Other language data. For now, ignore. \n3. Not a rankable. TODO: Custom loss function: Take 10 pivot elements (1 per decimal range). Contrastive loss against each of these pivot and aggregate to generate loss fn. Should teach the model a measure of relative-toxicity. With 50 steps itself, model is at a 0.03 MSE loss (=== 0.17 RMSE) already. For now ignore.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = AutoTokenizer.from_pretrained(mname)\nmodel = AutoModelForSequenceClassification.from_pretrained(mname, num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:14:08.377548Z","iopub.execute_input":"2021-11-19T19:14:08.377987Z","iopub.status.idle":"2021-11-19T19:15:53.619548Z","shell.execute_reply.started":"2021-11-19T19:14:08.377946Z","shell.execute_reply":"2021-11-19T19:15:53.618879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_ds = Dataset.from_pandas(traindf)\nvalid_ds = Dataset.from_pandas(validdf)\n\nkey1, key2 = \"sentence\", None\ndef preprocess_function(examples):\n    if key2 is None:\n        return tokenizer(examples[key1], padding=True, truncation=True, max_length=512) \n    return tokenizer(examples[key1], examples[key2], padding=True, truncation=True, max_length=512) #We allow truncation. So encoded ds and actual ds should share the same size. \n\ntrain_ds = train_ds.map(preprocess_function, batched=True)\nvalid_ds = valid_ds.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:15:53.641881Z","iopub.execute_input":"2021-11-19T19:15:53.642442Z","iopub.status.idle":"2021-11-19T19:16:10.075159Z","shell.execute_reply.started":"2021-11-19T19:15:53.642401Z","shell.execute_reply":"2021-11-19T19:16:10.074504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nwandbkey = user_secrets.get_secret(\"wandbkey\")\nwandb.login(key=wandbkey)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:16:10.076448Z","iopub.execute_input":"2021-11-19T19:16:10.07704Z","iopub.status.idle":"2021-11-19T19:16:11.425154Z","shell.execute_reply.started":"2021-11-19T19:16:10.077002Z","shell.execute_reply":"2021-11-19T19:16:11.424424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb_kwargs = {\"project\":\"kaggle_jigsaw\", \"tags\": [\"bertweet-lg\", \"baseline\"], \"name\": run_id, \"reinit\": True, \"notes\": kaggle_source}","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:16:11.426603Z","iopub.execute_input":"2021-11-19T19:16:11.427009Z","iopub.status.idle":"2021-11-19T19:16:11.431601Z","shell.execute_reply.started":"2021-11-19T19:16:11.426972Z","shell.execute_reply":"2021-11-19T19:16:11.430907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, default_data_collator\ndata_collator = default_data_collator\n\nrun = wandb.init(**wandb_kwargs)\n\nargs = TrainingArguments(\n    run_id,\n    evaluation_strategy=\"steps\",\n    eval_steps=CFG[\"eval_steps\"],\n    save_strategy = \"steps\",\n    save_steps=CFG[\"eval_steps\"],\n    learning_rate=CFG[\"lr\"],\n    per_device_train_batch_size=CFG[\"batch_size\"],\n    gradient_accumulation_steps=CFG[\"grad_acc_steps\"],\n    per_device_eval_batch_size=64,\n    num_train_epochs=N_EPOCHS,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    report_to=[\"wandb\"],\n    run_name=run_id,\n    logging_steps=CFG[\"wandb_logging_steps\"],\n    #seed=4142\n)\n\ntrainer = Trainer(\n    args=args,\n    model=model,\n    train_dataset=train_ds,\n    eval_dataset=valid_ds,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\nrun.finish() ","metadata":{"execution":{"iopub.status.busy":"2021-11-19T19:16:11.433261Z","iopub.execute_input":"2021-11-19T19:16:11.433651Z"},"trusted":true},"execution_count":null,"outputs":[]}]}