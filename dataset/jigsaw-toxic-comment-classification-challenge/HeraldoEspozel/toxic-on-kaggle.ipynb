{"cells":[{"metadata":{"trusted":true,"_uuid":"92c3f76ef6a7463ed19576361755eccb98c59db4"},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:120% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9ffdac82ab8a043fd7974284004b6b5f88c0b08"},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f8acfc335e80016070acf91777b36cc825c6f32"},"cell_type":"code","source":"import multiprocessing\n#from util.utils import timer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7aeba7854195f9f66e420b5499d3d618e542efaa","scrolled":true},"cell_type":"code","source":"import os\ndpath=\"../input/jigsaw-toxic-comment-classification-challenge/\"\nprint(os.listdir(\"../input/jigsaw-toxic-comment-classification-challenge\"))\nprint(os.listdir(\"../\"))\nos.mkdir(\"../testpred\")\nos.mkdir(\"../testpred/sub\")\nos.mkdir(\"../trainpred\")\nos.mkdir(\"../trainpred/sub\")\nos.mkdir(\"../model\")\nos.mkdir(\"../model/final\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cdc2bf81300717d1db01fd18bce272d104330c1"},"cell_type":"code","source":"'''\nSingle model may achieve LB scores at around 0.043\nDon't need to be an expert of feature engineering\nAll you need is a GPU!!!!!!!\n\nThe code is tested on Keras 2.0.0 using Theano backend, and Python 3.5\n\nreferrence Code:https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n'''\n\n########################################\n## import packages\n########################################\nimport os\nimport re\nimport csv\nimport sys\nfrom datetime import datetime\nimport codecs\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom string import punctuation\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,GlobalMaxPooling1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU,CuDNNLSTM,CuDNNGRU, Reshape, MaxPooling1D,AveragePooling1D\nfrom keras.optimizers import RMSprop, SGD\n\nimport colorama\nfrom colorama import Fore\n\n\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport matplotlib.pyplot as plt\n\n########################################\n## set directories and parameters\n########################################\n\n\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\n#from keras import initializations\nfrom keras import initializers, regularizers, constraints","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80ff61c4aac2ecbc366d1d3be18d29609ce14228"},"cell_type":"code","source":"path = '../'\nTRAIN_DATA_FILE=path+'input/jigsaw-toxic-comment-classification-challenge/train.csv'\nTEST_DATA_FILE=path+'input/jigsaw-toxic-comment-classification-challenge/test.csv'\n\nmaxlen = 200   # Maximum Sequence Size \nmax_features = 250000 # Maximum Number of Words in Dictionary\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.1\n\nnum_lstm = 300 #300\nnum_dense = 256 # 256\nrate_drop_lstm = 0.05\nrate_drop_dense = rate_drop_lstm\n\nloss=\"val_acc\"\n#loss=\"val_loss\"\nopt='rmsprop'\n#opt='adam'\n\nlr=0.01\nfrom keras.optimizers import RMSprop, SGD, Nadam, Adamax, Adam\n#opto = SGD(lr=lr, clipvalue=0.5)\n#opto=  Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n#opto = RMSprop (lr=lr)\n#opto = Nadam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n#opto = Adamax(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n##\n \nact = 'relu'\nTrainable=True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a141c3ac1771acec19edaf6b09921e2f687a9c2"},"cell_type":"code","source":"from contextlib import contextmanager\nfrom datetime import datetime\n@contextmanager\ndef timer(name):\n    \"\"\"\n    Taken from Konstantin Lopuhin https://www.kaggle.com/lopuhin\n    in script named : Mercari Golf: 0.3875 CV in 75 LOC, 1900 s\n    https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s\n    \"\"\"\n    start_time = datetime.now()\n    print(f'[{name}] Started : '+start_time.strftime(\"%d-%m-%Y %H:%M\"))\n    yield\n    thour, temp_sec = divmod( (datetime.now() - start_time).total_seconds(), 3600)\n    tmin, tsec = divmod(temp_sec, 60)\n    print(f'[{name}] Done in :', end=\"\");\n    print(' %i h %i m and %s seconds.' % (thour, tmin, round(tsec, 2)), end=\"\");\n    print(f' Ended : '+datetime.now().strftime(\"%d-%m-%Y %H:%M\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8c7e0395d25fa2b4c58481a391c8c77a4af9d2e"},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        \"\"\"\n        Keras Layer that implements an Attention mechanism for temporal data.\n        Supports Masking.\n        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, features)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(Attention())\n        \"\"\"\n        self.supports_masking = True\n        #self.init = initializations.get('glorot_uniform')\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        # eij = K.dot(x, self.W) TF backend doesn't support it\n\n        # features_dim = self.W.shape[0]\n        # step_dim = x._keras_shape[1]\n\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        #print weigthted_input.shape\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        #return input_shape[0], input_shape[-1]\n        return input_shape[0],  self.features_dim\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5893043a51f60932aa27e49e8af7cce54997ace"},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    #    print(\"Normalized confusion matrix\")\n    # else:\n    #    print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    plt.draw()\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63dbf4b345f1e75f8f45d043aac8b3b892a9425a"},"cell_type":"code","source":"def multi_roc_auc_score(y_true, y_pred):\n    assert y_true.shape == y_pred.shape\n    columns = y_true.shape[1]\n    scores = []\n    for i in range(0, columns):\n        scr=roc_auc_score(y_true[:, i], y_pred[:, i])\n        print (\"Class:\",list_classes[i],\" -roc:{:.5f}\".format(scr))\n        scores.append(scr)\n    return np.array(scores).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9f9fb6d0388398a261dc1ed885b7eb678ef26ad"},"cell_type":"code","source":"def multi_confusion(y_true, y_pred):\n\n    assert y_true.shape == y_pred.shape\n    columns = y_true.shape[1]\n    scores = []\n    for i in range(0, columns):\n        rounded_predictions = np.round(y_pred[:, i], 0) \n        scr=roc_auc_score(y_true[:, i], y_pred[:, i])\n        cm = confusion_matrix(y_true[:, i], rounded_predictions)\n        cm_plot_labels = ['NO-'+list_classes[i],list_classes[i]]\n        plot_confusion_matrix(cm, cm_plot_labels, title='Confusion '+list_classes[i]+\" -roc:{:.5f}\".format(scr))\n    return ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57aeaa36c47731d6f1790edea897ddccae4ec11d"},"cell_type":"code","source":"import re\ndef remove_urls (vTEXT):\n    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n    return(vTEXT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ffb5ba9b22d0d73ded0e5a63dc4c1b50b8c31c3"},"cell_type":"code","source":"def ReplaceThreeOrMore(s):\n    # pattern to look for three or more repetitions of any character, including\n    # newlines.\n    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n    return pattern.sub(r\"\\1\", s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55c2eea91a124128c83da5fd457c93170ec49b10"},"cell_type":"code","source":"def splitstring(s):\n    # searching the number of characters to split on\n    proposed_pattern = s[0]\n    for i, c in enumerate(s[1:], 1):\n        if c != \" \":\n            if proposed_pattern == s[i:(i+len(proposed_pattern))]:\n                # found it\n                break\n            else:\n                proposed_pattern += c\n    else:\n        exit(1)\n\n    return proposed_pattern","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3849a8d7872e7d731f9393d02939649c2fde7590"},"cell_type":"code","source":"########################################\n## process texts in datasets\n########################################\n\n#Regex to remove all Non-Alpha Numeric and space\nspecial_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n\n#regex to replace all numerics\nreplace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n\ndef text_to_wordlist(text,to_lower=False, rem_urls=False, rem_3plus=False, \\\n                     split_repeated=False, rem_special=False, rep_num=False,\n                     man_adj=True, rem_stopwords=False, stem_snowball=False,\\\n                     stem_porter=False, lemmatize=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    if rem_urls:\n        text = remove_urls(text)\n    if to_lower:    \n        text = text.lower()\n    if rem_3plus:    \n        text = ReplaceThreeOrMore(text)\n\n    if man_adj: \n        # Clean the text\n        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n        text = re.sub(r\"what's\", \"what is \", text)\n        text = re.sub(r\"\\'s\", \" \", text)\n        text = re.sub(r\"\\'ve\", \" have \", text)\n        text = re.sub(r\"can't\", \"cannot \", text)\n        text = re.sub(r\"n't\", \" not \", text)\n        text = re.sub(r\"i'm\", \"i am \", text)\n        text = re.sub(r\"\\'re\", \" are \", text)\n        text = re.sub(r\"\\'d\", \" would \", text)\n        text = re.sub(r\"\\'ll\", \" will \", text)\n        text = re.sub(r\",\", \" \", text)\n        text = re.sub(r\"\\.\", \" \", text)\n        text = re.sub(r\"!\", \" ! \", text)\n        text = re.sub(r\"\\/\", \" \", text)\n        text = re.sub(r\"\\^\", \" ^ \", text)\n        text = re.sub(r\"\\+\", \" + \", text)\n        text = re.sub(r\"\\-\", \" - \", text)\n        text = re.sub(r\"\\=\", \" = \", text)\n        text = re.sub(r\"'\", \" \", text)\n        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n        text = re.sub(r\":\", \" : \", text)\n        text = re.sub(r\" e g \", \" eg \", text)\n        text = re.sub(r\" b g \", \" bg \", text)\n        text = re.sub(r\" u s \", \" american \", text)\n        text = re.sub(r\"\\0s\", \"0\", text)\n        text = re.sub(r\" 9 11 \", \"911\", text)\n        text = re.sub(r\"e - mail\", \"email\", text)\n        text = re.sub(r\"j k\", \"jk\", text)\n        text = re.sub(r\"\\s{2,}\", \" \", text)\n\n    # split them into a list\n    text = text.split()\n    \n    if split_repeated:\n        for i, c in enumerate(text):\n            text[i]=splitstring(c)\n    \n    # Optionally, remove stop words\n    if rem_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n    \n    #Remove Special Characters\n    if rem_special: \n        text=special_character_removal.sub('',text)\n    \n    #Replace Numbers\n    if rep_num:     \n        text=replace_numbers.sub('n',text)\n\n    # Optionally, shorten words to their stems\n    if stem_snowball:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    if stem_porter:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in text.split()])\n        \n    if lemmatize:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])   \n \n    # Return a list of words\n    return(text)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a46196489953296d0dd072027066b8fa681c9207"},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA_FILE)\ntest_df = pd.read_csv(TEST_DATA_FILE)\nlist_sentences_train = train_df[\"comment_text\"].fillna(\"NA\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train_df[list_classes].values\nyaux=train_df[\"toxic\"].values\nlist_sentences_test = test_df[\"comment_text\"].fillna(\"NA\").values\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"135208322b9070ad2928cfe227392e4edb46c99e"},"cell_type":"code","source":"comments = []\nfor text in list_sentences_train:\n    comments.append(text_to_wordlist(text))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6deb22bb9c53566dbcf9009a7a728b62276b35c8"},"cell_type":"code","source":"\ntest_comments=[]\nfor text in list_sentences_test:\n    test_comments.append(text_to_wordlist(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1042778ca4497c696a9b237b2276ba8c4b7d28a"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(comments + test_comments)\n\nsequences = tokenizer.texts_to_sequences(comments)\ntest_sequences = tokenizer.texts_to_sequences(test_comments)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3e5dd1a05067929775036a74bb5bf0cdd364069"},"cell_type":"code","source":"print(len(sequences), 'train sequences')\nprint(len(test_sequences), 'test sequences')\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, sequences)), dtype=int)))\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, test_sequences)), dtype=int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46cd84528e91ea1e08e6e5dc33fab9f788162837"},"cell_type":"code","source":"print(len(sequences), 'train sequences')\nprint(len(test_sequences), 'test sequences')\nprint('Max train sequence length: {}'.format(np.max(list(map(len, sequences)))))\nprint('Max test sequence length: {}'.format(np.max(list(map(len, test_sequences)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29be31dfd82202bcb30ede6b9fb8af55028793a5"},"cell_type":"code","source":"print(len(sequences), 'train sequences')\nprint(len(test_sequences), 'test sequences')\nprint('Min train sequence length: {}'.format(np.min(list(map(len, sequences)))))\nprint('Min test sequence length: {}'.format(np.min(list(map(len, test_sequences)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bed08c9f1abb024f891225a697ebca50217f6612"},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', y.shape)\n\ntest_data = pad_sequences(test_sequences, maxlen=maxlen)\nprint('Shape of test_data tensor:', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ad064c3a15713c39af8f2ab9dcb4be19b6133da"},"cell_type":"code","source":"                                                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1da754806181cc68ba64336f7ef8d6be1dc65a54"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"15048142c5275720fe1610660a49e66ad70a2e53"},"cell_type":"raw","source":"filext = '_ml_' + str(maxlen) + '_mf_' + str(max_features) + '_eb_' + str(EMBEDDING_DIM) + \".npy\"  \nnp.save(path+'prodata/train_data'+filext, data)\nnp.save(path+'prodata/test_data'+filext,test_data)\n"},{"metadata":{"trusted":true,"_uuid":"a72e3ea1a3bc19d1297131d4eeae45f138cb8dd9"},"cell_type":"code","source":"del comments\ndel test_sequences\ndel sequences\ndel list_sentences_train\ndel list_sentences_test\ndel train_df\ndel test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ecd42c012ab948135f36263e72a33b7ac71ee53"},"cell_type":"code","source":"########################################\n## index GLOVE  word vectors\n########################################\nEMBEDDING_FILE=path+'input/glove840b300dtxt/glove.840B.300d.txt'\net=\"GLOVE-840B\"\n\n#EMBEDDING_FILE=path+'glove/glove.6B.300d.txt'\n#et=\"GLOVE-6B\"\n\n#EMBEDDING_FILE= path+'prodata/toxic_clean_300d.txt'\n#et='TOXIC-TXT'\n\n#EMBEDDING_FILE= path+'fasttext/crawl-300d-2M.vec'\n#et=\"FASTTEXT\"\n\n\nprint('Indexing '+et+' vectors')\nprint(\"Vector\",EMBEDDING_FILE )\n#Glove Vectors\n\nembeddings_index = {}\nf = open(EMBEDDING_FILE,  encoding='utf8')\nfor line in f:\n    values = line.split()\n    word = ' '.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    #word = values[0]\n    #coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Total %s word vectors.' % len(embeddings_index))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65d5998c9059c7e88c2460fe93fe7c49500f7c09"},"cell_type":"code","source":"#########\n## Glove\n#########\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std\n\n########################################\n## GLOVE prepare embeddings\n########################################\nprint('Preparing embedding matrix')\nnb_words = min(max_features, len(word_index))+1\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\n#embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\nnl=0\ngd=0\nfor word, i in word_index.items():\n    if i >= max_features:\n        #print ('Over: ',word)\n        nl = nl +1 \n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        gd=gd+1\n    else:\n        #print (word)\n        nl = nl +1 \n\n         \ndel embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be1bbaf2649bf154608a26896f5b28d1d6476ff4"},"cell_type":"raw","source":"########################################\n## WORD2VEC word vectors\n########################################\n\nfrom gensim.models import KeyedVectors\n\n#model_name = path+'prodata/toxic_clean_1000d.bin'\n#et='TOXIC'\nbf=True\n\nmodel_name =path+'google/GoogleNews-vectors-negative300.bin'\net='GOOGLE'\nbf=True\n\n#model_name = path+'fasttext/crawl-300d-2M.vec'\n#et='FASTTEXT'\n#bf=False\n\nprint('Indexing '+et+' word vectors-'+model_name) \nload_vec = KeyedVectors.load_word2vec_format(model_name, binary=bf)\n\n\nword_vector = load_vec.wv\ndel load_vec\nprint('Found %s word vectors of word2vec' % len(word_vector.vocab))\n\nn_features = len(word_index)\nfeatureVec = np.zeros((n_features,EMBEDDING_DIM),dtype=\"float32\")\nindex2word_list = list(word_vector.index2word)\nfeatureVec=np.array(word_vector[index2word_list])\nall_embs = np.stack(featureVec)\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std\n\n\n# prepare embedding matrix\nnb_words = min(max_features, len(word_index))\n#embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, EMBEDDING_DIM))\nnl=0\ngd=0\nfor word, i in word_index.items():\n    if i >= max_features:\n        #print ('Over: ',word)\n        nl = nl +1 \n        continue\n    if word in word_vector.vocab:\n        embedding_matrix[i] = word_vector.word_vec(word)\n        #embedding_matrix[i] = word_vector.wv[word]\n        gd=gd+1\n    else:\n        #print (word)\n        nl = nl +1   \ndel word_vector"},{"metadata":{"trusted":true,"_uuid":"ae765839d81a82ff59ecbfc7fba1dbd2bead9b3b"},"cell_type":"code","source":"#print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0)) \nprint( \"Matrix\", embedding_matrix.shape)\nprint( \"Tamanho Vocabulario\", len(word_index), \"Maximo de Features\",max_features)\nprint('Null word embeddings: %d' % nl)\nprint('Good word embeddings: %d' % gd)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9750f0e71332581ddafd198d724cc3cd1b1f6e41"},"cell_type":"raw","source":"#\n# Model Concatenating\n#\n\n#def get_model(embedding_matrix, sequence_length, rate_drop_dense, num_lstm, num_dense):\nnum_lstm = 300\nnum_dense = 256\nrate_drop_lstm = 0.25\nrate_drop_dense = 0.25\n\nembedding_input = Input(shape=(max_features,), name='embedding_input')\nnumerical_input = Input(shape=[X_train_['numerical_input'].shape[1]], name='numerical_input')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], \n                            embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False)(embedding_input)\n\n#embedding_layer = Embedding(nb_words,\n#        EMBEDDING_DIM,\n#        weights=[embedding_matrix],\n#        input_length=maxlen,\n#        trainable=True)\n\nbidirecti_layer = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\nbidirecti_layer = Dropout(rate_drop_lstm)(bidirecti_layer)\nbidirecti_layer = Bidirectional(CuDNNGRU(num_lstm, return_sequences=False))(bidirecti_layer)\nbidirecti_layer = Dense(num_dense, activation=\"relu\")(bidirecti_layer)\nbidirecti_layer = BatchNormalization()(bidirecti_layer)\n\nmain_layer = concatenate([\n    bidirecti_layer,\n    numerical_input\n])\n\nmain_layer = BatchNormalization()(main_layer)\nmain_layer = Dense(64, activation=\"relu\")(main_layer)\nmain_layer = Dropout(rate_drop_dense)(main_layer)\nmain_layer = Dense(32, activation=\"relu\")(main_layer)\nmain_layer = Dropout(rate_drop_dense)(main_layer)\noutput_layer = Dense(6, activation=\"sigmoid\")(main_layer)\n\nmodel = Model([embedding_input, numerical_input], outputs=output_layer)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=RMSprop(clipvalue=1, clipnorm=1),\n              metrics=['accuracy'])\n\n"},{"metadata":{"collapsed":true,"_uuid":"12c1c0aa2a84ea2b9f95bf3383b4c5a41a129271"},"cell_type":"raw","source":"def create_model6():\n    ########################################\n    ## Model - Keras tutorial - https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n    ########################################\n    mdln=\"06-3CONV\"\n    embedding_layer = Embedding(nb_words,\n            EMBEDDING_DIM,\n            weights=[embedding_matrix],\n            input_length=maxlen,\n            trainable=True)\n\n    sequence_input = Input(shape=(maxlen,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n    x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n    x = MaxPooling1D(5)(x)\n    x = Conv1D(128, 5, activation='relu')(x)\n    x = MaxPooling1D(5)(x)\n    x = Conv1D(128, 5, activation='relu')(x)\n    x = MaxPooling1D(35)(x)  # global max pooling\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    preds = Dense(6, activation='softmax')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['acc'])\n    return model, mdln\n\n"},{"metadata":{"trusted":true,"_uuid":"be47095f8aa94db740a624d71490eb8eb17d0e18"},"cell_type":"code","source":" def create_model0():\n    \n    mdln=\"00-max-d-dr-d-dr-d-dr\"\n    comment_input = Input((maxlen,))\n\n    # we start off with an efficient embedding layer which maps\n    # our vocab indices into embedding_dims dimensions\n    comment_emb = Embedding(max_features, EMBEDDING_DIM, input_length=maxlen, \n                            embeddings_initializer=\"uniform\")(comment_input)\n\n    # we add a GlobalMaxPooling1D, which will extract features from the embeddings\n    # of all words in the comment\n    m = GlobalMaxPooling1D()(comment_emb)\n    d = Dense(1024, activation=act)(m)\n    d = Dropout(rate_drop_dense)(d)\n    d = Dense(512, activation=act)(d)\n    d = Dropout(rate_drop_dense)(d)\n    d = Dense(256, activation=act)(d)\n    d = Dropout(rate_drop_dense)(d)\n    d = Dense(128, activation=act)(d)\n    d = Dropout(rate_drop_dense)(d)\n    # We project onto a six-unit output layer, and squash it with a sigmoid:\n    output = Dense(6, activation='sigmoid')(d)\n\n    model = Model(inputs=comment_input, outputs=output)\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n    \n    #for layer in model.layers:\n     #   weights = layer.get_weights()\n    #print(weights)\n    \n    return model,mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd83c1649f01bc7e6834fb0c67d960380c3ea5f3"},"cell_type":"code","source":"# ## Modelo LSTM Base Line \n########################################\ndef create_model1():\n\n    mdln=\"01-lstm-att-d-b\"\n    embedding_layer = Embedding(nb_words,\n            EMBEDDING_DIM,\n            weights=[embedding_matrix],\n            input_length=maxlen,\n            trainable=Trainable)\n\n    lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n\n    #lstm_layer = Bidirectional(LSTM(num_lstm, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))\n#    CuDNNLSTM\n\n    comment_input = Input(shape=(maxlen,), dtype='int32')\n    embedded_sequences= embedding_layer(comment_input)\n    x = lstm_layer(embedded_sequences)\n    x = Dropout(rate_drop_dense)(x)\n    merged = Attention(maxlen)(x)\n    merged = Dense(num_dense, activation=act)(merged)\n    merged = Dropout(rate_drop_dense)(merged)\n    merged = BatchNormalization()(merged)\n    preds = Dense(6, activation='sigmoid')(merged)\n\n    model = Model(inputs=[comment_input], \\\n            outputs=preds)\n    model.compile(loss='binary_crossentropy',\n            optimizer=opt,\n            metrics=['accuracy'])\n    \n    \n    #for layer in model.layers:\n        #weights = layer.get_weights()\n    #print(weights)\n    \n    return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac75b28843070bb7eae9c46dfb1f18ca182d4c9d"},"cell_type":"code","source":"########################################\n## BASE Model\n########################################\ndef create_model2( ):\n\n    mdln=\"02-base-bilstm-max-dd\"\n    embedding_layer = Embedding(nb_words,\n            EMBEDDING_DIM,\n            weights=[embedding_matrix],\n            input_length=maxlen,\n            trainable=Trainable)\n\n    sequence_input = Input(shape=(maxlen,))\n    embedded_sequences = embedding_layer(sequence_input)\n    x = Bidirectional(LSTM(num_dense, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))(embedded_sequences)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    preds = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=sequence_input, outputs=preds)\n\n    model.compile(loss='binary_crossentropy',\n            optimizer=opt,\n            metrics=['accuracy'])\n    #print(model.summary())\n    \n    return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5029667f5d22999704715579c5fb851906695a8"},"cell_type":"code","source":"########################################\n## Modelo CONV - https://www.kaggle.com/cdubuz/keras-cnn-rnn-0-051-lb\n########################################\n\ndef create_model3():\n\n        \n    \n        mdln=\"03-conv-max-conv-max-gru-d\"\n        embedding_layer = Embedding(nb_words,\n                EMBEDDING_DIM,\n                weights=[embedding_matrix],\n                input_length=maxlen,\n                trainable=Trainable)\n\n        sequence_input = Input(shape=(maxlen,))\n        embedded_sequences = embedding_layer(sequence_input)\n\n        main = Dropout(rate_drop_dense)(embedded_sequences)\n        main = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')(main)\n        main = MaxPooling1D(pool_size=2)(main)\n        main = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')(main)\n        main = MaxPooling1D(pool_size=2)(main)\n        main = GRU(64)(main)\n        main = Dense(32, activation=\"relu\")(main)\n        main = Dense(6, activation=\"sigmoid\")(main)\n        model = Model(inputs=sequence_input, outputs=main)\n        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n        \n        return model, mdln\n\n  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85d4d69d3f01099310d1ed8192a0739a0b2e2b1d"},"cell_type":"code","source":"from keras.models import Sequential\n\ndef create_model4():\n\n    mdln=\"04-conv-max-conv-max-bulstm-max-dd\"\n\n\n    embedding_layer = Embedding(nb_words,\n            EMBEDDING_DIM,\n            weights=[embedding_matrix],\n            input_length=maxlen,\n            trainable=Trainable)\n\n    comment_input = Input(shape=(maxlen,), dtype='int32')\n    embedded_sequences= embedding_layer(comment_input)\n    x = Dropout(0.2)(embedded_sequences)\n    x = Conv1D(filters=EMBEDDING_DIM, kernel_size=4, padding='same', activation='relu')(x)\n\n    x = MaxPooling1D(pool_size=2)(x)\n    x = Conv1D(filters=EMBEDDING_DIM, kernel_size=4, padding='same', activation='relu')(x)\n\n    x = MaxPooling1D(pool_size=2)(x)\n    x = Bidirectional(LSTM(num_dense, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    preds = Dense(6, activation='sigmoid')(x)\n\n    model = Model(inputs=[comment_input], \\\n            outputs=preds)\n    model.compile(loss='binary_crossentropy',\n            optimizer=opt,\n            metrics=['accuracy'])\n    #print(model.summary())\n\n    return model, mdln\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcea9c5861fa1a4ff53e572738c50ea8412b7514"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\ndef create_model5():\n    et='NONE'\n    mdln=\"05-tridense\"\n    model = Sequential()\n    sequence_input = Input(shape=(maxlen,))\n    # Dense(64) is a fully-connected layer with 64 hidden units.\n    # in the first layer, you must specify the expected input data shape:\n    # here, 20-dimensional vectors.\n    model.add(Dense(64, input_dim=maxlen, init='uniform'))\n    model.add(Activation('tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(64, init='uniform'))\n    model.add(Activation('tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(6, init='uniform'))\n    model.add(Activation('softmax'))\n\n    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(loss='mean_squared_error', optimizer=sgd)\n    return model, mdln\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bb72dbbefade3b283dd069b8c80a6e1380a3e7c"},"cell_type":"code","source":"####\n####        \n\ndef create_model6( ):\n\n    mdln=\"06-bicugru-con-max-avg-d\"\n\n    input_layer = Input(shape=(maxlen,))\n    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n\n\n    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n  \n\n    tower_1 = GlobalMaxPool1D()(x)\n    tower_2 = GlobalAveragePooling1D()(x)\n    \n    output = concatenate([  tower_1, tower_2])\n\n    x = Dense(num_dense, activation=\"relu\")(output)\n    x = Dropout(rate_drop_dense)(x)\n    preds = Dense(6, activation=\"sigmoid\")(x)                         \n\n    model = Model(inputs=input_layer, outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                 # optimizer=RMSprop(clipvalue=1, clipnorm=1),\n                  metrics=['accuracy'])\n\n   \n    return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f9cdd7f267188f78e5e90f79123b7babaf1d0d0"},"cell_type":"code","source":"def create_model7():\n    et='NONE'\n    mdln=\"07-biconv-mas-gru-d\"\n    embed_size = 256\n    inp = Input(shape=(maxlen, ))\n    main = Embedding(max_features, embed_size)(inp)\n    main = Dropout(0.2)(main)\n    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n    main = MaxPooling1D(pool_size=2)(main)\n    main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n    main = MaxPooling1D(pool_size=2)(main)\n    main = GRU(32)(main)\n    main = Dense(16, activation=\"relu\")(main)\n    main = Dense(6, activation=\"sigmoid\")(main)\n    model = Model(inputs=inp, outputs=main)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    #model.summary()     \n    return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0879a446a38cd4a6476d8912b88d6bc4351e7ced"},"cell_type":"code","source":"####\n####        \n\ndef create_model8( ):\n\n    # PV = 0.9875  (PROC FALSE) / PUBLIC:0.9844\n    # PV = 0.98587 (PROC TRUE)  / PUBLIC: ?\n    mdln=\"08-bicugru-max-dd\"\n\n\n #   embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n #                               weights=[embedding_matrix], trainable=True)(input_layer)\n    \n    embedding_layer = Embedding(nb_words,\n        EMBEDDING_DIM,\n        weights=[embedding_matrix],\n        input_length=maxlen,\n        trainable=Trainable)\n\n    input_layer = Input(shape=(maxlen,))\n    embedded_sequences = embedding_layer(input_layer)\n    x = Bidirectional(CuDNNGRU(num_dense, return_sequences=True))(embedded_sequences)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    preds = Dense(6, activation=\"sigmoid\")(x)                         \n\n    model = Model(inputs=input_layer, outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    return model, mdln\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"928048206e0c563fc8cc9c457c7605b52f6c98f2"},"cell_type":"code","source":"def create_model9( ):\n\n        mdln=\"09-bi-lstm-max-dd\"\n        embedding_layer = Embedding(nb_words,\n                EMBEDDING_DIM,\n                weights=[embedding_matrix],\n                input_length=maxlen,\n                trainable=Trainable)\n\n        sequence_input = Input(shape=(maxlen,))\n        embedded_sequences = embedding_layer(sequence_input)\n        x = Bidirectional(LSTM(num_dense, return_sequences=True, dropout=rate_drop_dense, recurrent_dropout=rate_drop_lstm))(embedded_sequences)\n        x = GlobalMaxPool1D()(x)\n        x = Dense(num_dense, activation=\"relu\")(x)\n        x = Dropout(rate_drop_dense)(x)\n        x = Dense(num_dense, activation=\"relu\")(x)\n        x = Dropout(rate_drop_dense)(x)\n        preds = Dense(6, activation=\"sigmoid\")(x)\n        model = Model(inputs=sequence_input, outputs=preds)\n\n        model.compile(loss='binary_crossentropy',\n                optimizer=opt,\n                metrics=['accuracy'])\n        #print(model.summary())\n        return model, mdln  \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9009545f78a6435e3342c80f0ee2ed9c6d764c5c"},"cell_type":"code","source":"def create_model10( ):\n\n    # PV = 0.98734 TRUE / PUBLIC: 0.9852\n    # PV = 0.98603 FALSE/ PUBLIC: 0.9837\n    \n    mdln=\"10-bibi-cugru-dbdd\"\n\n\n    input_layer = Input(shape=(maxlen,))\n    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n\n    main_layer = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n    main_layer = Dropout(rate_drop_lstm)(main_layer)\n    main_layer = Bidirectional(CuDNNGRU(num_lstm, return_sequences=False))(main_layer)\n    main_layer = Dense(num_dense, activation=\"relu\")(main_layer)\n    main_layer = BatchNormalization()(main_layer)\n    main_layer = Dense(64, activation=\"relu\")(main_layer)\n    main_layer = Dropout(rate_drop_dense)(main_layer)\n    main_layer = Dense(32, activation=\"relu\")(main_layer)\n    main_layer = Dropout(rate_drop_dense)(main_layer)\n    preds    = Dense(6, activation=\"sigmoid\")(main_layer)\n    \n    model = Model(inputs=input_layer, outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    #print(model.summary())\n\n    return model, mdln  \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d81dcd2abb6fbd67f0e96a9b84f210517c2fcc10"},"cell_type":"code","source":"def create_model11( ):\n\n    # PV = 0.98734 TRUE / PUBLIC: 0.9852\n    # PV = 0.98603 FALSE/ PUBLIC: 0.9837\n    \n    mdln=\"11-bicgru-dr-bicgru-dr-d-dr\"\n\n    input_layer = Input(shape=(maxlen,))\n    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n\n\n    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n    x = Dropout(rate_drop_lstm)(x)\n    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=False))(x)\n#    x = GlobalMaxPool1D()(x)\n    x = Dropout(rate_drop_dense)(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    preds = Dense(6, activation=\"sigmoid\")(x)                         \n\n    model = Model(inputs=input_layer, outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n    #print(model.summary())\n\n    return model, mdln    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f4e47b63580c81870f6b8444b1aba2ccdd45572"},"cell_type":"code","source":"def create_model12():\n\n    mdln=\"12-bicgru-con-conv-max-avg-d-dr\"\n\n    input_layer = Input(shape=(maxlen,))\n    embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=Trainable)(input_layer)\n\n\n    x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(embedding_layer)\n    \n    tower_1 = Conv1D(filters=num_lstm, kernel_size=2, padding='same', activation='relu')(x)\n    tower_1 = GlobalMaxPool1D()(tower_1)\n    tower_2 = Conv1D(filters=num_lstm, kernel_size=2, padding='same', activation='relu')(x)\n    tower_2 = GlobalAveragePooling1D()(tower_2)\n    \n    output = concatenate([  tower_1, tower_2])\n    #, axis = 3\n    \n#    output = Flatten()(output)\n#    out    = Dense(10, activation='softmax')(output)\n    out = Dense(num_dense, activation=\"relu\")(output)\n    out = Dropout(rate_drop_dense)(out)\n    preds = Dense(6, activation=\"sigmoid\")(out)                         \n\n    model = Model(inputs=input_layer, outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n                       \n\n\n    return model, mdln    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cee2ac7cca48f36b655430eaaecd6590655ce5b2"},"cell_type":"code","source":"# ## Modelo LSTM Base Line \n########################################\ndef create_model13():\n        et='NONE'\n        mdln=\"13-dr-conv-max-conv-max-gru-dr\"\n        model = Sequential()\n        inp = Input(shape=(maxlen,))\n        main = Embedding(max_features, EMBEDDING_DIM)(inp)\n        main = Dropout(0.2)(main)\n        main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n        main = MaxPooling1D(pool_size=2)(main)\n        main = Conv1D(filters=32, kernel_size=2, padding='same', activation='relu')(main)\n        main = MaxPooling1D(pool_size=2)(main)\n        main = GRU(32)(main)\n        main = Dense(16, activation=\"relu\")(main)\n        main = Dense(6, activation=\"sigmoid\")(main)\n        model = Model(inputs=inp, outputs=main)\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n        return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17a09f34911294aa8fcb803214b09d57eac3ddd9"},"cell_type":"code","source":"def create_model14():\n\n        mdln=\"14-culstm-sdr-bn-d-dr\"\n        \n        comment_input = Input((maxlen,))\n\n        # we start off with an efficient embedding layer which maps\n        # our vocab indices into embedding_dims dimensions\n        comment_emb =Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=Trainable)(comment_input)\n\n        # we add a GlobalMaxPool1D, which will extract information from the embeddings\n        # of all words in the document\n        x = CuDNNLSTM(num_dense, return_sequences=True)(comment_emb)\n        comment_emb = SpatialDropout1D(0.25)(x)\n        max_emb = GlobalMaxPool1D()(comment_emb)\n\n        # normalized dense layer followed by dropout\n        main = BatchNormalization()(max_emb)\n        main = Dense(64)(main)\n        main = Dropout(0.5)(main)\n\n        # We project onto a six-unit output layer, and squash it with sigmoids:\n        output = Dense(6, activation='sigmoid')(main)\n\n        model = Model(inputs=comment_input, outputs=output)\n\n        model.compile(loss='binary_crossentropy',\n                      optimizer=opt,\n                      metrics=['accuracy'])\n                                         \n        \n        return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbc21da20a305e55af6831d9eadeae28acc06b69"},"cell_type":"code","source":"def create_model15():\n### https://www.kaggle.com/yekenot/pooled-gru-fasttext\n        mdln=\"15-sdr-bigru-con-max-avg\"\n        inp = Input(shape=(maxlen,))\n        embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                weights=[embedding_matrix], trainable=Trainable)(inp)\n   \n        x = SpatialDropout1D(rate_drop_dense)(embedding_layer)\n        #x = Bidirectional(GRU(500, return_sequences=True))(x)\n        x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(x)\n        avg_pool = GlobalAveragePooling1D()(x)\n        max_pool = GlobalMaxPooling1D()(x)        \n        conc = concatenate([avg_pool, max_pool])\n        x = Dense(num_dense, activation=\"relu\")(conc)\n        x = Dropout(rate_drop_dense)(x)\n        outp = Dense(6, activation=\"sigmoid\")(x)\n\n        model = Model(inputs=inp, outputs=outp)\n        model.compile(loss='binary_crossentropy',\n                      optimizer=opt,\n                      metrics=['accuracy'])\n     \n        return model, mdln","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b32e43872323a141537924730e1b0cfecc7a6d00"},"cell_type":"code","source":"########################################\n## BASE Model\n########################################\ndef create_model16( ):\n\n    mdln=\"16-culstm-max-d-dr\"\n    embedding_layer = Embedding(nb_words,\n            EMBEDDING_DIM,\n            weights=[embedding_matrix],\n            input_length=maxlen,\n            trainable=Trainable)\n\n    sequence_input = Input(shape=(maxlen,))\n    embedded_sequences = embedding_layer(sequence_input)\n    x = Bidirectional(CuDNNLSTM(num_dense, return_sequences=True))(embedded_sequences)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(num_dense, activation=\"relu\")(x)\n    x = Dropout(rate_drop_dense)(x)\n    preds = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=sequence_input, outputs=preds)\n\n    model.compile(loss='binary_crossentropy',\n            optimizer=opt,\n            metrics=['accuracy'])\n    #print(model.summary())\n    \n    return model, mdln\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7440ab53eb01889fc75bb04d999fc3116dfe8548"},"cell_type":"code","source":"def create_model17( ):\n    \n        mdln=\"17-biclstm-max-d-dr-d-dr\"\n        embedding_layer = Embedding(nb_words,\n                EMBEDDING_DIM,\n                weights=[embedding_matrix],\n                input_length=maxlen,\n                trainable=Trainable)\n\n        sequence_input = Input(shape=(maxlen,))\n        embedded_sequences = embedding_layer(sequence_input)\n        x = Bidirectional(CuDNNLSTM(num_dense, return_sequences=True))(embedded_sequences)\n        x = GlobalMaxPool1D()(x)\n        x = Dense(num_dense, activation=\"relu\")(x)\n        x = Dropout(rate_drop_dense)(x)\n        x = Dense(num_dense, activation=\"relu\")(x)\n        x = Dropout(rate_drop_dense)(x)\n        preds = Dense(6, activation=\"sigmoid\")(x)\n        model = Model(inputs=sequence_input, outputs=preds)\n\n        model.compile(loss='binary_crossentropy',\n                optimizer=opt,\n                metrics=['accuracy'])\n        #print(model.summary())\n        return model, mdln  \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3388f5724f8f2adfdf1604142276b754dae1cba"},"cell_type":"code","source":"def create_model18( ):\n    \n        mdln=\"18-dr-bicgru-max-d-dr\"\n        embedding_layer = Embedding(nb_words,\n                EMBEDDING_DIM,\n                weights=[embedding_matrix],\n                input_length=maxlen,\n                trainable=Trainable)\n\n        sequence_input = Input(shape=(maxlen,))\n        embedded_sequences = embedding_layer(sequence_input)\n        x = Dropout(rate_drop_dense)(embedded_sequences)\n        x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(x)\n        x = GlobalMaxPool1D()(x)\n        x = Dense(num_dense, activation=\"relu\")(x)\n        x = Dropout(rate_drop_dense)(x)\n        preds = Dense(6, activation=\"sigmoid\")(x)\n        model = Model(inputs=sequence_input, outputs=preds)\n\n        model.compile(loss='binary_crossentropy',\n                optimizer=opt,\n                metrics=['accuracy'])\n        #print(model.summary())\n        return model, mdln  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c538f6cffb4a51aa196167ca613e93b4b519e9e"},"cell_type":"code","source":"def create_model19( ):\n    \n    \n        mdln=\"19-sdr-bicgru-con-conv-max-avg-d-dr\"\n        \n        input_layer = Input(shape=(maxlen,))\n        embedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                                    weights=[embedding_matrix], trainable=Trainable)(input_layer)\n\n        x = SpatialDropout1D(0.2)(embedding_layer)\n        x = Bidirectional(CuDNNGRU(num_lstm, return_sequences=True))(x)\n\n        #x = Conv1D(filters=num_lstm, kernel_size=2, padding='same', activation='relu')(x)\n        x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n        tower_1 = GlobalMaxPool1D()(x)\n        tower_2 = GlobalAveragePooling1D()(x)\n\n        output = concatenate([  tower_1, tower_2])\n\n        out = Dense(num_dense, activation=\"relu\")(output)\n        out = Dropout(rate_drop_dense)(out)\n        preds = Dense(6, activation=\"sigmoid\")(out)                         \n\n        model = Model(inputs=input_layer, outputs=preds)\n        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\n        #model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])               \n\n\n        return model, mdln    \n        \n       \n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aa83b488308f4f0c2bafa53ec68f9f2ba9631f0"},"cell_type":"code","source":"def dict_to_list(d):\n    ret = []\n    for i in d.items():\n        ret.append(i[1])\n    return ret\n\n\ndef merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a += np.array(data[i])\n    a /= nfolds\n    return a.tolist()\n\ndef get_validation_predictions(train_data, predictions_valid):\n    pv = []\n    for i in range(len(train_data)):\n        pv.append(predictions_valid[i])\n    return pv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf08ffa9fa4c8291c304b4ec4940b15aca7292ca"},"cell_type":"code","source":"from keras import backend as K\nfrom keras.layers import Dense\n\n\ndef reset_weights(model):\n    session = K.get_session()\n    for layer in model.layers: \n         for v in layer.__dict__:\n             v_arg = getattr(layer,v)\n             if (v != \"embeddings\"):\n                 if hasattr(v_arg,'initializer'):\n                     initializer_method = getattr(v_arg, 'initializer')\n                     initializer_method.run(session=session)\n                     print('reinitializing layer {}.{}'.format(layer.name, v))\n             else :\n                  print('keeping layer {}.{}'.format(layer.name, v)) \n            \n                        \n    print (\"reinitializing layers...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44599a485e2d5c1ce2c9f19ba16193f82b6e4399"},"cell_type":"code","source":"def evaluate_model ( label, X_train, X_valid, Y_train, Y_valid, STAMP):\n\n            modelx=None\n            modelx,mdln = create_model(label)\n           \n\n            bst_model_path=path+ \"model/\"+\"{val_acc:.5f}-{epoch:02d}-{val_loss:.5f}_\"+ STAMP + '.h5'\n            #print(bst_model_path)\n\n            #early_stopping =EarlyStopping(monitor='val_loss', patience=patience, verbose=1)\n            early_stopping =EarlyStopping(monitor=loss, patience=patience, verbose=1)\n           \n\n            model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=False, save_weights_only=False, mode='auto')\n\n            callbacks = [ early_stopping, model_checkpoint]\n\n            \n            hist=modelx.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n                  shuffle=True, verbose=0, validation_data=(X_valid, Y_valid),\n                  callbacks=callbacks)\n            \n            index_val_loss = hist.history['val_loss'].index(min(hist.history['val_loss']))\n            bst_model_path=path+ \"model/\"+\"{:.5f}-\".format(hist.history['val_acc'][index_val_loss])+\"{:02d}\".format(index_val_loss+1)+\"-{:.5f}\".format(hist.history['val_loss'][index_val_loss])+\"_\"+STAMP+'.h5'\n \n            #reset_weights(model)\n            K.clear_session()\n            modelx=None\n            del modelx\n            return bst_model_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"507cde2434fc6819c77d7394cf18c320a38ea0a5"},"cell_type":"code","source":"def run_cross_validation_create_models(label,nsplits=10,epochs=3,patience=3):\n        random_state = 999\n\n        yfull_train = dict()\n        train_full=  []\n        test_full = []\n\n        kf = skf = StratifiedKFold(n_splits=nsplits, shuffle=True, random_state=random_state)\n        num_fold = 0\n        sum_score = 0\n        bestmodel,mdln=create_model(label)\n        print (Fore.GREEN+\"\\n\"+mdln,\" ***********************************************************\")\n        print (bestmodel.summary() )\n        print (Fore.BLACK)\n        saved_models = []\n        score = np.zeros(nsplits)\n        score_partial = np.zeros(nsplits)\n        for i, (train_index, test_index) in enumerate(kf.split(data,yaux)):\n            num_fold += 1\n            print('Fold:',num_fold)\n\n            X_train, X_valid = data[train_index],data[test_index]\n            Y_train, Y_valid = y[train_index], y[test_index]\n            print('Start KFold number {} from {} - Fitting'.format(num_fold, nsplits))\n\n           \n            bst_model_path=evaluate_model (label,X_train, X_valid, Y_train, Y_valid, STAMP)\n            \n          \n            print (\"Validating\")\n\n            ### Getting the Best Model\n            bestmodel,mdln=create_model(label)\n            bestmodel.load_weights(bst_model_path)          \n            \n            #predictions_valid = bestmodel.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n            predictions_valid = bestmodel.predict(X_valid, batch_size=batch_size, verbose=0)\n\n            score_partial[i] = multi_roc_auc_score(Y_valid, predictions_valid)\n            print(Fore.BLUE +'Partial Score roc_auc: {:.5f}\\n'.format(score_partial[i])+Fore.BLACK)\n            info_string = '{:.5f}'.format(score_partial[i]) +\"_fl_\"+'{:02d}'.format(num_fold) +\"_\"+STAMP\n \n\n            train_pred = bestmodel.predict(data, batch_size=batch_size, verbose=1)\n            score[i] = multi_roc_auc_score(y, train_pred)\n            print(Fore.GREEN +'Full Score roc_auc: {:.5f}\\n'.format(score[i])+Fore.BLACK)\n            train_full.append(train_pred)\n            \n            test_pred = bestmodel.predict(test_data, batch_size=batch_size, verbose=1)\n            test_full.append(test_pred)\n                       \n            #newfile= path+\"model/\"+\"{:.5f}-roc-\".format(score_partial[i])+bst_model_path[-(len(STAMP)+21):]\n            newfile= path+\"model/final/\"+\"{:.5f}-roc-\".format(score_partial[i])+bst_model_path[35:]\n            os.rename(bst_model_path, newfile)\n            \n            sum_score += score_partial[i]*len(test_index)\n\n            saved_models.append(bestmodel)\n\n            del bestmodel\n        scoreF = sum_score/len(data)\n        \n        train_res = np.array( merge_several_folds_mean(train_full, nsplits))  \n        #scoreT = multi_roc_auc_score(y, train_res)\n        \n        print(Fore.RED +'roc_uac train independent: {:.5f}\\n'.format(scoreF))\n        print (\"Teste Internal Score {:.5f}\".format(score_partial.mean()) )\n        print (\"Teste External Score {:.5f}\\n\".format(score.mean()) )\n        #print (\"Teste Full     Score {:.5f}\\n\".format(scoreT) )\n\n            \n        #test_res  = merge_several_folds_mean(test_full, nsplits)\n        #info_string = '{:.5f}'.format(scoreF) +'_final_'+STAMP\n        #test_submission = pd.read_csv(dpath+\"sample_submission.csv\")\n        #test_submission[list_classes] = test_res\n  \n            \n        print(Fore.BLACK)\n          \n        K.clear_session()   \n        #info_string = mdln+'_roc_{:.5f}'.format(score1)  + '_folds_' + str(nsplits) + '_ep_' + str(epochs) \n\n        return info_string, saved_models \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5fc98342442b74b1001e853e6d5175d8a1a8e31"},"cell_type":"code","source":"def create_model (label):\n    \n            if (label==\"0\"):\n                mdn,mdname = create_model0()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"1\"):\n                mdn,mdname = create_model1()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"2\"):\n                mdn,mdname = create_model2()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"3\"):\n                mdn,mdname = create_model3()\n                print(mdn.summary())\n                return mdn, mdname          \n            if (label==\"4\"):\n                mdn,mdname = create_model4()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"5\"):\n                mdn,mdname = create_model5()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"6\"):\n                mdn,mdname = create_model6()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"7\"):\n                mdn,mdname = create_model7()\n                print(mdn.summary())\n                return mdn, mdname \n            if (label==\"8\"):\n                mdn,mdname = create_model8()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"9\"):\n                mdn,mdname = create_model9()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"10\"):\n                mdn,mdname = create_model10()\n                print(mdn.summary())\n                return mdn, mdname            \n            if (label==\"11\"):\n                mdn,mdname = create_model11()\n                print(mdn.summary())\n                return mdn, mdname                \n            if (label==\"12\"):\n                mdn,mdname = create_model12()\n                print(mdn.summary())\n                return mdn, mdname                \n            if (label==\"13\"):\n                mdn,mdname = create_model13()\n                print(mdn.summary())\n                return mdn, mdname               \n            if (label==\"14\"):\n                mdn,mdname = create_model14()\n                print(mdn.summary())\n                return mdn, mdname              \n            if (label==\"15\"):\n                mdn,mdname = create_model15()\n                print(mdn.summary())\n                return mdn, mdname                \n            if (label==\"16\"):\n                mdn,mdname = create_model16()\n                print(mdn.summary())\n                return mdn, mdname \n            if (label==\"17\"):\n                mdn,mdname = create_model17()\n                print(mdn.summary())\n                return mdn, mdname \n            if (label==\"18\"):\n                mdn,mdname = create_model18()\n                print(mdn.summary())\n                return mdn, mdname\n            if (label==\"19\"):\n                mdn,mdname = create_model19()\n                print(mdn.summary())\n                return mdn, mdname\n\n            return None,\"None\"\n            \n            \n            \n            \n            \n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d05ad93e4b7e23236978eb12e52a45451f711e67"},"cell_type":"code","source":"nsplits = 3\nepochs=1\npatience=3\ntrainpred=False\ntestpred=False\nbatch_size=512\nSTAMP=\"STAMP\"\nbest_models= [\"15\",\"16\"]\n\n#for clf, label in zip([clf0,clf8,clf9,clf1,clf2,clf6,clf12,clf11],\n#                      [\"6\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\",\"1\"]):\n1\nfor label in  [\"12\"]:\n    \n       with timer(label):\n             info_string, modelos = run_cross_validation_create_models(label, nsplits,epochs, patience)\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}