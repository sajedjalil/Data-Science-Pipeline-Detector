{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Table of Contents:\n\n[0. Introduction](#0)\n\n[1. Importing Libraries](#1)\n\n[2. Reading Dataset](#2)\n\n[3. Text Preprocessing](#3)\n\n[4. Defining a Multi-Label LSTM model](#4)\n\n[5. Compile and train the LSTM model](#5)\n\n[6. Predicting and Submitting for Test Data](#6)\n\n[7. TODOs](#7)"},{"metadata":{},"cell_type":"markdown","source":"## 0. Introduction <a class=\"anchor\" id=\"0\"></a>\nIn earlier notebook(https://www.kaggle.com/anirbansen3027/jtcc-cnn) we used g Keras Library (which is a wrapper over tensorflow) for creating 1-D Convolutional Neural Networks(CNNs) for multi-label text classification on output variables - toxic, severe_toxic, obscene, threat, insult, identity_hate.\n\nIn this one, we will be using the same Keras Library (which is a wrapper over tensorflow) for creating Long Short Term Memory (LSTM) which is an improvement over regular RNNs for multi-label text classification. We will be first going through a bit of intuition of how RNNs and LSTM work and then implement it using a minimalistic single output layer network for multilabel classification (instead of creating 6 separate networks for each type of toxicity or creating a multiple output layer network). We will be just using a single LSTM layer and in just a single epoch it gives ~96 AUC on leaderboard \n\n### Why do we need for RNNs?\nIn a traditional neural network we assume that all inputs (and outputs) are independent of each other. They dont share features learnt across different positions of text. This might be an issue for sequential information such as text data or time-series data where each instance is also dependent on the previous ones. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a ‚Äúmemory‚Äù which captures information about what has been calculated so far. \n\n<img src=\"https://i.imgur.com/FQyAYBP.png\" title=\"source: imgur.com\" width = 700/>\n\n### What is the architecture of RNNs?\nThe overall architecture of the RNN depends on the task in hand. For this task which is a classification task, we will be using the 3rd one: many-to-one. But for intuition purpose, let's look at the 5th one which is a more generalised notation for RNNs. If we know how the 5th notation work, it will be just a matter to change a small part.\n\n<img src=\"https://www.di.ens.fr/~lelarge/dldiy/slides/lecture_8/images/rnn_variants_4.png\" width = 500/>\nInput vectors are in red, output vectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video)\n\n<img src=\"https://i.imgur.com/S7AKfYi.png\"/>\n\n### What is vanishing gradients?\nThe vanishing gradient problem arises in very deep Neural Networks, typically Recurrent Neural Networks, that use activation functions whose gradients tend to be small (in the range of 0 from 1). Because these small gradients are multiplied during backpropagation, they tend to ‚Äúvanish‚Äù or reduce to 0 throughout the layers, preventing the network from learning long-range dependencies. As the sequence gets longer, the gradients/ derivatives passed to the previous states become smaller and smaller.There are many solutions to this problem. One of that is using LSTM.\n\n\n### What is an LSTM?\nLong Short Term Memory networks ‚Äì usually just called ‚ÄúLSTMs‚Äù ‚Äì are a special kind of RNN, capable of learning long-term dependencies.All RNNs have the form of a chain of repeating modules of neural network. LSTMs also have this chain like structure, but instead of the hidden layer we have something called LSTM cell and we have another connection that runs through all the time steps along with the hidden state.This is the called the \"Cell State\" vector from which information can be retrieved and removed as and when required.\n\n<img src=\"https://i.imgur.com/utWg9yZ.png\"/></a>\n\nLet's look at the 6 steps:\n\n1. This is the forget gate which is responsible for how much to forget and since it passes through a sigmoid function, it will give a value of 0 to 1 which is the amount of memory to be retained.\n2. This is the input gate which is responsible for how much new information is to be added to the cell state. Similar to forget gate this will also give a value of 0 to 1 which is the amount of new memory to be added\n3. This is the creation of new candidate vector/ cell state \n4. This is where the cell state is updated which is a combination of previos cell state and current cell state, the contribution of each is controlled using the forget gate and input gate respectively.\n5. This is the output gate which is responsible for what part of the updated cell state is to be remembered in the hidden state having a value between 0 and 1\n6. This is the updated hidden state which will be the input for next cell and is based on cell state controlled by output gate \n\nThis is an awesome link to deep dive further into LSTM http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\n### How does LSTM solve vanishing gradients?\n* The LSTM architecture makes it easier for the RNN to preserve information over many timesteps e.g. if the forget gate is set to remember everything on every timestep, then the info in the cell is preserved indefinitely\n* By contrast, it‚Äôs harder for vanilla RNN to learn a recurrent weight matrix Wh that preserves info in hidden state\n* LSTM doesn‚Äôt guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies\n\nBefore the birth of Transformers, LSTMs ruled the world of NLP. Even today it is used in many places\n\n**2015:**\nGoogle started using an LSTM for speech recognition on Google Voice. According to the official blog post, the new model cut transcription errors by 49%.\n\n**2016:**\n2016: Google started using an LSTM to suggest messages in the Allo conversation app. In the same year, Google released the Google Neural Machine Translation system for Google Translate which used LSTMs to reduce translation errors by 60%.\n\nApple announced in its Worldwide Developers Conference that it would start using the LSTM for quicktype in the iPhone and for Siri.\n\nAmazon released Polly, which generates the voices behind Alexa, using a bidirectional LSTM for the text-to-speech technology.\n\n**2017:**\nFacebook performed some 4.5 billion automatic translations every day using long short-term memory networks.\n\nEnough of context, let's dive into the code üë®‚Äçüíª"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Libraries <a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#sklearn libraries\nfrom sklearn.model_selection import train_test_split\n#keras libraries\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, Input\n#Constants\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000 \nEMBEDDING_DIM = 100 \nVALIDATION_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Reading Dataset <a class=\"anchor\" id=\"2\"></a>\nAll the datasets are provided as zipped files. First we will have to unzip them and then read them into dataframes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#unzipping all the zip folders and saving it /kaggle/working and saving the verbose in /dev/null to keep it quiet\n# -o for overwrite -d for destination directory of unzipped file\n!unzip -o '/kaggle/input/jigsaw-toxic-comment-classification-challenge/*.zip' -d /kaggle/working > /dev/null\n\n#Reading input csv files\ndf_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")\n\nprint(df_train.shape, df_test.shape, sample_submission.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assinging the texts to list of strings\ntrain_texts= df_train.comment_text.values\ntest_texts= df_test.comment_text.values\n#Assignings the labels as a separate df\ntrain_labels = df_train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]]\n#Printing the list of strings\nprint(\"First comment text in training set:\\n\\n\", train_texts[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Text Preprocessing <a class=\"anchor\" id=\"3\"></a>\nThe preprocessing for the LSTM model is pretty much same as the CNN one. We use the Tokenizer class from Keras to tokenize the strings into a sequence of numbers by mapping each word to a number based of frequency. We also use pad_sequences from Keras to pad the tokenized sequence of integers to make all the sequences of same size as the ANN be it CNN or LSTM will be expecting a fixed sized input each time for vectorized calculations. I would recommend to look at the notebook for an elaborate read (https://www.kaggle.com/anirbansen3027/jtcc-cnn#3.-Text-Preprocessing)\n\n*We will follow these steps going ahead for Multi-Label text classification using LSTM:*\n\n**Input String -> Tokenization -> Padding -> Embedding -> LSTM -> Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initializing the class\ntokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n#Updates internal vocabulary based on a list of texts.\ntokenizer.fit_on_texts(train_texts)\n#Transforms each text in texts to a sequence of integers.\ntrain_sequences = tokenizer.texts_to_sequences(train_texts)\ntest_sequences = tokenizer.texts_to_sequences(test_texts)\nword_index = tokenizer.word_index\nprint(\"Length of word Index:\", len(word_index))\nprint(\"First 5 elements in the word_index dictionary:\", dict(list(word_index.items())[0: 5]) )\nprint(\"First comment text in training set:\\n\", train_sequences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pad tokenized sequences\ntrainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint(\"Shape of padded sequence list:\\n\", trainvalid_data.shape)\nprint(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", trainvalid_data[0][-50:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Defining a Multi-Label LSTM model <a class=\"anchor\" id=\"4\"></a>\n\nIn keras, the easiest way to define a model is initiate a Sequential model class and keep adding required layers. A Sequential model is a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nIn this NN model, a new paramter called dropout is being used:\n\n**Dropout**\n\nDropout is a technique for addressing the problem of overfitting. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much.\nA new hyperparameter is introduced that specifies the probability at which outputs of the layer are dropped out.\n\nRecurrent dropout masks (or \"drops\") the connections between the recurrent units.\n\n#### Important Note: In general,\n\n***For binary classification, we can have 1 output units, use sigmoid activation in the output layer and use binary cross entropy loss**\n\n***For multi class classification, we can have N output units, use softmax activation in the output layer and use categorical cross entropy loss**\n\n***For multi label classification, we can have N output units, use sigmoid activation in the output layer and use binary cross entropy loss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_model = Sequential()\nrnn_model.add(Embedding(MAX_NUM_WORDS, 128))\nrnn_model.add(LSTM(units = 128, dropout = 0.2, recurrent_dropout = 0.2))\nrnn_model.add(Dense(units = 6, activation = 'sigmoid'))\nprint(rnn_model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Compile and train the LSTM model <a class=\"anchor\" id=\"5\"></a>\nThe compiling and training/fitting code is also pretty much same as the CNN model.\n\nBefore starting to train the model, we need to configure it. We need to mention the loss fucntion which will be used to calculate the error at each iteration, optimizer which will specify how the weights will be updated and the metrics which is to be evaluated by the model during training and testing\n\nWhile fitting/ training the model, along with the training set we also pass the following parameters:\n\nbatch_size = Number of samples that goes through the network at a time and updates the network parameters by calculating loss (in Mini Batch Gradient Descent)\n\nepochs = Number of times the whole set of training samples goes through the network\n\nvalidation_data = the dataset that will be used to evaluate the loss and any model metrics at the end of each epoch. This set will not be used for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Configures the model for training.\nrnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"AUC\"])\n\n#Split the dataset into train and validation set for training and evaludating the model\nX_train, X_val, y_train, y_val = train_test_split(trainvalid_data, train_labels, shuffle = True, random_state = 123)\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n\n#Trains the model for a fixed number of epochs (iterations on a dataset)\nhistory = rnn_model.fit(X_train, y_train, batch_size = 128, epochs = 1, validation_data = (X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Predicting and Submitting for Test Data <a class=\"anchor\" id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the test dataset with sample_submission to have all the columns:\n#id,text_data and the target variables in one dataframe\ndf_test = pd.merge(df_test, sample_submission, on = \"id\")\n#Use the CNN model to output probabilities on test data\ny_preds = rnn_model.predict(test_data)\n#Assign the predictions by the model in the final test dataset\ndf_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = y_preds\n#Drop Comment Text as the sample submission doesnt have it and wouldnt be expected\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Save the dataset as a csv to submit it\ndf_test.to_csv(\"sample_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. TODOs <a class=\"anchor\" id=\"7\"></a>\n* Stack more LSTM layers \n* Hyperparameter Tune the parameters\n\nDo upvote if you find it helpful üòÅ"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}