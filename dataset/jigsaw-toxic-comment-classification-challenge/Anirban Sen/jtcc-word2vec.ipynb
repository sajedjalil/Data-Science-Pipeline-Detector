{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 0. Introduction\nIn earlier notebook(https://www.kaggle.com/anirbansen3027/jtcc-bag-of-words) we used CountVectorizer (an sklearn implementation of Bag-of-Words) model to convert the texts to a numerical dataset, mapped against the output variables \"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\" and used Multi Output Classifier wrapper from sklearn to create Logistic Regression models for all the 6 output variables.\n\nIn this one, we will be replacing the first part with a Word2Vec model to create an embedding instead of the BagOfWords vector and then input that to a Logistic Regression Model (Any ML/DL model can be built on top of the Word2Vec embedding).\n\n### Brief Intuition:\n#### What is Word Embedding?\nWord embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify).\n<img src=\"https://www.researchgate.net/profile/Ali_Basirat/publication/327074728/figure/fig1/AS:678946643386368@1538884902625/A-two-dimensional-representation-of-word-embeddings-Words-with-similar-meanings-are.png\" width=\"500\">\n\nAbove is a 2-dimensional word embedding where Sunday has more similar values to other weekdays than members of a family\n\n#### What is Word2Vec?\nWord2Vec is one of the oldest methods to create/learn this embeddings. Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks like Text Classification, Question Answering. The papers proposed two methods for learning representations of words:\n\n**Continuous Bag-of-Words Model** which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n\n**Continuous Skip-gram Model** which predict words within a certain range before and after the current word in the same sentence.\n\nArchitecture Diagrams:\n<img src=\"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Word2Vec-Training-Models.png\" width=\"500\">\n\nExamples:\n![](https://1.bp.blogspot.com/-Vz5pLuZ49K8/XV0ErlMtdDI/AAAAAAAAB0A/FIM74z__LAUkCqpW12ViAnGX8Br56W2PQCEwYBhgL/s1600/image001.png)\nIn CBOW, given the words (the quick brown box, over the lazy log), we would want to predict jump.\nIn Skipgram just the opposite given the word jump, we would want to predict (the quick brown box, over the lazy log)\n\n#### But how does the models learn?\nI tried to make an image as that would be easier to grasp. It might look scary at first but I will try my best to explain.\nLets's start with CBOW, we take the sentence \"Natural Language Processing\" where \"Natural\" and \"Processing\" are context words and \"Language\" is the target word. We have a shallow network as shown above with a single hidden layer. \n\nSo the input is a one-hot encoded vector of V terms, V being the size of vocabulary (total number of unique words) with only single 1. So let's say we have only 5 words in vocabulary (Natural, Language, Processing, is, great). The vector for Natural will be [1, 0, 0, 0, 0]. Similarly for Processing it will be [0, 0, 1, 0, 0].\nNow, we have a randomly initialised Embedding vector(E) with size V * D where D is the dimension size of the vector which you can choose on. This is the weight matrix for the input layer.\nSo, we multiply the input one-hot encoded vector with the weights/embedding vector. This gives the embedding vectors for the context words of size 1 * D.\n\nNow in the hidden layer, we average the emedding vectors for the context words which forms the input for this layer of size 1* D. This is multiplied by another Vector called Context Vector (E') with size D * V. This gives us a vector of 1 * V which is then passed through a sigmoid function to get the final output.\n\nThe final output is compared with the one-hot encoded vector of Language (the middle word) [0, 1, 0, 0, 0] and loss function is calculated. This loss is back propogated and the model is trained using Gradient Descent\n\nThe final o\n<img src=\"https://i.imgur.com/JsCPzSX.png\" title=\"source: imgur.com\" width=\"1000\"/>\n\n#### How will be get the embeddings?\nGensim library enables us to develop word embeddings.Gensim gives you an option to choose the either CBOW or Skipgram while training your own embeddings.(Default is CBOW). Along with it, Gensim also has a directory of pretrained embeddings which are trained on several documents like wiki pages, google news, twitter tweets etc. In this example, we will be using a pretrained embedding based on Google News corpus (3 billion running words) word vector model (3 million 300-dimension English word vectors). \n\nOk enough of definitions. Let's dive into the code\n\n### Table of Contents:\n[1. Importing Libraries](#1)\n\n[2. Reading Dataset](#2)\n\n[3. Basic preprocessing](#3)\n\n[4. Load pretrained embeddings](#4)\n\n[5. Convert text inputs to embeddings using pretrained models](#5)\n\n[6. Train and Validate a Multi-Output Classifier](#6)\n\n[7. Predicting and Submitting for Test Data](#7)\n\n[8. TODOs](#8)\n\n**N.B.: I haven't covered Logistic Regression and Feature Importance/ Model Intrepretation in this notebook as I have covered it in the last notebook : https://www.kaggle.com/anirbansen3027/jtcc-bag-of-words**"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Libraries <a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ipython-autotime \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom gensim.models import Word2Vec, KeyedVectors\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom statistics import mean\n%load_ext autotime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Reading Dataset <a class=\"anchor\" id=\"2\"></a>\nAll the datasets are provided as zipped files. First we will have to unzip them and then read them into dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#unzipping all the zip folders and saving it /kaggle/working and saving the verbose in /dev/null to keep it quiet\n# -o for overwrite -d for destination directory of unzipped file\n!unzip -o '/kaggle/input/jigsaw-toxic-comment-classification-challenge/*.zip' -d /kaggle/working > /dev/null\n#Reading input csv files\ndf_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")\nprint(df_train.shape, df_test.shape, sample_submission.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts = list(df_train[\"comment_text\"].values)\ntrain_labels = df_train[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']].to_numpy()\ntest_texts = list(df_test[\"comment_text\"].values)\nprint(\"Example Training Text:\\n\\n\",train_texts[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have around 160k training texts and about 153k test texts"},{"metadata":{},"cell_type":"markdown","source":"## 3. Basic preprocessing <a class=\"anchor\" id=\"3\"></a>\nIn this case, we remove stopwords and digits, lowercase all the texts and tokenize(break into individual tokens/words) the texts using word_tokenize from NLTK library"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_corpus(texts):\n    #importing stop words like in, the, of so that these can be removed from texts\n    #as these words dont help in determining the classes(Whether a sentence is toxic or not)\n    mystopwords = set(stopwords.words(\"english\"))\n    def remove_stops_digits(tokens):\n        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n        return [token.lower() for token in tokens if token not in mystopwords and not token.isdigit()\n               and token not in punctuation]\n    #This return statement below uses the above function and tokenizes output further. \n    return [remove_stops_digits(word_tokenize(text)) for text in tqdm(texts)]\n\n#Preprocess both for training and test data\ntrain_texts_processed = preprocess_corpus(train_texts)\ntest_texts_processed = preprocess_corpus(test_texts)\nprint(\"Example Training Prepocessed Text\\n\\n\", train_texts_processed[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Load pretrained embeddings <a class=\"anchor\" id=\"4\"></a>\nWe use Gensim Library to load pretrained embeddings for words trained on Google News dataset. The Google New model/ embedding vector is of 300 dimension"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Path for the models/ embedding vector\ngoogle_news_model = '../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim'\n#Loading the models/ embedding vector using KeyedVectors.load function from gensim\nw2v_google_news = KeyedVectors.load(google_news_model)\n#Print lengths/number of words in the embedding\nprint(len(w2v_google_news.vocab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Goggle News model/ embedding vector has about 3 M words. Let's have a look at an example of an embedding which is essentialy a dictionary where the key is the word and value is the embedding vector for that word."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print Shape of the embedding\nprint(\"Shape of embedding vector\", w2v_google_news[\"Natural\"].shape)\n#Let's print first 20 dimensions rather than all 300\nprint(\"First 20 numbers in the embedding of the word Natural\\n\\n\", w2v_google_news[\"Natural\"][:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how the embedding for the word \"Natural\" looks like."},{"metadata":{},"cell_type":"markdown","source":"## 5. Convert text inputs to embeddings using pretrained models <a class=\"anchor\" id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Here we take the input tokenized texts from earlier and get the embeddings for each word in texts from the pretrained embedding vector. This will give us the final input dataset in form of an embedding per sentence which can be used to train along with the output variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function that takes in the input text dataset in form of list of lists where each sentence is a list of words all the sentences are \n#inside a list\ndef embedding_feats(list_of_lists, DIMENSION, w2v_model):\n    zeros_vector = np.zeros(DIMENSION)\n    feats = []\n    missing = set()\n    missing_sentences = set()\n    #Traverse over each sentence\n    for tokens in tqdm(list_of_lists):\n        # Initially assign zeroes as the embedding vector for the sentence\n        feat_for_this = zeros_vector\n        #Count the number of words in the embedding for this sentence\n        count_for_this = 0\n        #Traverse over each word of a sentence\n        for token in tokens:\n            #Check if the word is in the embedding vector\n            if token in w2v_model:\n                #Add the vector of the word to vector for the sentence\n                feat_for_this += w2v_model[token]\n                count_for_this +=1\n            #Else assign the missing word to missing set just to have a look at it\n            else:\n                missing.add(token)\n        #If no words are found in the embedding for the sentence\n        if count_for_this == 0:\n            #Assign all zeroes vector for that sentence\n            feats.append(feat_for_this)\n            #Assign the missing sentence to missing_sentences just to have a look at it\n            missing_sentences.add(' '.join(tokens))\n        #Else take average of the values of the embedding for each word to get the embedding of the sentence\n        else:\n            feats.append(feat_for_this/count_for_this)\n    return feats, missing, missing_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Embeddings for the train dataset\ntrain_vectors, missing, missing_sentences = embedding_feats(train_texts_processed, 300, w2v_google_news)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the final embeddings for the sentences\", np.array(train_vectors).shape)\nprint(\"First 20 numbers in the embedding of the first train sentence\\n\\n\", np.array(train_vectors)[0][:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To summarize, each sentence will have one 300 dimensional embedding vector which will be an average of the word embeddings present in that sentence. The word embeddings are taken from the pretrained word embeddings that was trained on google news to find the embedding."},{"metadata":{},"cell_type":"markdown","source":"## 6. Train and Validate a Multi-Output Classifier <a class=\"anchor\" id=\"6\"></a>\nSince we need to classify each sentence as toxic or not, severe_toxic or not, obscene or not, threat or not, insult or not and identity_hate or not, we need to classify the sentence against 6 output variables (This is called Multi-Label Classification which is different from mult-class classification where a target variable has more than 2 options e.g. a sentence can be positive, negative and neutral)\n\nFor the same, we will be using MultiOutputClassifier from sklearn which as mentioned earlier is a wrapper.This strategy consists of fitting one classifier per target.\nSo, this segment will deal with 5 things\n\n1. Getting the embedding vector for the training dataset\n2. Split the embedding vector and output variables into train and validation set\n3. Fit a Logistic Regression model on training embedding vector and output variables\n*(I have covered Logistic Regression in the previous notebook https://www.kaggle.com/anirbansen3027/jtcc-bag-of-words)*\n4. Make predictions on the validation embedding vectors\n5. Measure performance in the terms of ROC-AUC"},{"metadata":{},"cell_type":"markdown","source":"Since, the competition uses mean ROC-AUC as the evaluation metric, we will be using the same in the notebook. We will compare the mean ROC-AUC across all the 3 models we have trained. We will be using predict_proba function of models instead of predict which gives us the probability scores instead of predicted value based on a threshold of 0.5, as it is used by the roc_auc_measure."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for calculating roc auc with given actual binary values across target variables\n#and the probability score made by the model\ndef accuracy(y_test, y_pred):\n    aucs = []\n    #Calculate the ROC-AUC for each of the target column\n    for col in range(y_test.shape[1]):\n        aucs.append(roc_auc_score(y_test[:,col],y_pred[:,col]))\n    return aucs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(DIMENSION, model):\n    #Get the embedding vector for the training data\n    train_vectors, missing, missing_sentences = embedding_feats(train_texts_processed, DIMENSION, model)\n    \n    #Split the embedding vector for the training data along with the output variables\n    #into train and validation sets\n    train_data, val_data, train_cats, val_cats = train_test_split(train_vectors, train_labels)\n    \n    #Logistic Regression Model (As we have unbalanced dataset, we use class_weight which will use inverse\n    #of counts of that class. It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1)\n    lr = MultiOutputClassifier(LogisticRegression(class_weight='balanced', max_iter=3000)).fit(train_data, train_cats)\n    \n    #Actuals for the validation data\n    y_vals = val_cats\n    #Prediction probability for the validation dataset by the model for class 1\n    y_preds = np.transpose(np.array(lr.predict_proba(val_data))[:,:,1])\n    #Calculate the Mean ROC_AUC \n    mean_auc = mean(accuracy(y_vals,y_preds))\n    return mean_auc, lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_auc, lr = train_model(300, w2v_google_news)\nprint(mean_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model turns out to be pretty moderate. This might be because of the pretrained embeddings not correctly capturing the details. We could instead train an embedding of our own using Word2Vec."},{"metadata":{},"cell_type":"markdown","source":"## 7. Predicting and Submitting for Test Data <a class=\"anchor\" id=\"7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the test dataset with sample_submission to have all the columns:\n#id,text_data and the target variables in one dataframe\ndf_test = pd.merge(df_test, sample_submission, on = \"id\")\n#Getting the embedding matrix for test texts \ntest_vectors, _, _ = embedding_feats(test_texts_processed, 300, w2v_google_news)\n#Use the Logistic Regression model to output probabilities and take the probability for class 1\ny_preds = np.transpose(np.array(lr.predict_proba(test_vectors))[:,:,1])\n#Assign the predictions by the model in the final test dataset\ndf_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = y_preds\n#Drop Comment Text as the sample submission doesnt have it and wouldnt be expected\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Save the dataset as a csv to submit it\ndf_test.to_csv(\"sample_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. TODOs <a class=\"anchor\" id=\"8\"></a>\n1. Train a Word2Vec model from scratch \n2. Try ensemble models instead of Vanilla ML models Bagging and Boosting models give better results than classic ML techniques in most cases\n3. Better Text Preprocessing Typo correction etc can be done to further improve the model\n\n***Do upvote if you find it helpful üòÅ***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}