{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 0. Introduction\n\nIn earlier notebook(https://www.kaggle.com/anirbansen3027/jtcc-fasttext-supervised) we used fastText library for both generating embeddings for the sentences as well as multi-label text classification on output variables - toxic, severe_toxic, obscene, threat, insult, identity_hate.\n\nIn this one, we will be using Keras Library (which is a wrapper over tensorflow) for creating 1-D Convolutional Neural Networks(CNNs) for multi-label text classification.\n\n**Intuition:**\n\n**How did it start ?**\n\nCNNs were first introduced in the 1980s by Yann LeCun to recognize handwritten digits. But ConvNets remained on the sidelines of computer vision because they faced a serious problem: They could not scale. CNNs needed a lot of data and compute resources to work efficiently for large images. In 2012, AlexNet showed that perhaps the time had come to revisit deep learning as it had won various competitions. The availability of large sets of data, namely the ImageNet dataset with millions of labeled pictures, and vast compute resources enabled researchers to create complex CNNs that could perform computer vision tasks that were previously impossible.\n\n**What is a CNN ??**\n\n<img src = https://marketing3.topcoder.com/wp-content/uploads/2019/08/image-15-1024x450.png width = 500> </img>\n\nThere are four main operations in the ConvNet shown in the image above:\n\n**1. Convolution**\n<img src = https://miro.medium.com/max/1920/1*D6iRfzDkz-sEzyjYoVZ73w.gif width = 400> </img>\n\nThis layer is the heart of CNNs. CNNs use feature maps/kernels to learn features of the input. For e.g. the above kernel [[1, 0, -1], [1, 0, -1], [1, 0, -1]] detects vertical lines in images. \nThe magic is that, we dont need to specify the numbers in kernels. We just need to mention the number of kernels and the model will learn on itself the kernels, just like weights in a normal ANN. The general idea is that, as we keep on increasing the number of Conv and Pool layers, the more complex features the model will be able to detect. The 1st layers recognize simple things like lines/colors and subsequent layers recognize more complex patterns.\n\n**2. Non Linearity (ReLU)**\n\nAn artificial neuron without an activation function will just produce the sum of dot products between all inputs and their weights. By using appropriate nonlinear activation function we can help the neural networks to understand this nonlinear relationship. Here is an indepth blog on activations ([Activation Functions](https://machinelearningknowledge.ai/activation-functions-neural-network/#Why_we_need_Activation_Functions_in_Neural_Network))\n\nSigmoid function, is used in output neurons in case of binary classification problem to convert the incoming signal into a range of 0 to 1 so that it can be interpreted as a probability.\n\nWe have used ReLU or rectified linear unit, which applies the non-saturating activation function f(x)=max(0,x).ReLU is often preferred in the hidden layers to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.It also does not suffer from phenomena of vanishing gradient like Sigmoid and Tanh activation functions\n\n\n\n<img src=\"https://i.imgur.com/qsAiZ5G.jpg\" width = 700/>\n\n**3. Pooling or Sub Sampling**\n<img src = https://developers.google.com/machine-learning/practica/image-classification/images/maxpool_animation.gif width = 200> </img>\n\nPooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n\nHere is an indepth blog on types of pooling layers ([Pooling](https://www.machinecurve.com/index.php/2020/01/30/what-are-max-pooling-average-pooling-global-max-pooling-and-global-average-pooling/))\n\nIn this notebook, we will be using MaxPooling and GlobalMaxPooling\n\nThe above image is for MaxPooling - the operation simply involves computing the max value over a block at a time. \n\nAnother type is Global Max Pooling layer. Here, we set the pool size equal to the input size, so that the max of the entire input is computed as the output value. For the above image, if we would have applied Global Max Pooling, we would get 9 as the output.\n\n**4. Classification (Fully Connected Layer)**\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Fully Connected layers perform classification based on the features extracted by the previous layers. Typically, this layer is a traditional ANN which multiplies weights with inputs and pass through an activation to give an output\n\nLet's dive into the code then\n\n### Table of Contents:\n\n[1. Importing Libraries](#1)\n\n[2. Reading Dataset](#2)\n\n[3. Text Preprocessing](#3)\n\n[4. Defining a 1D CNN model](#4)\n\n[5. Compile and fit the CNN model](#5)\n\n[6. Predicting and Submitting for Test Data](#6)\n\n[7. TODOs](#7)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Libraries <a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, Embedding, Input\n\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000 \nEMBEDDING_DIM = 100 \nVALIDATION_SPLIT = 0.2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Reading Dataset <a class=\"anchor\" id=\"2\"></a>\nAll the datasets are provided as zipped files. First we will have to unzip them and then read them into dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#unzipping all the zip folders and saving it /kaggle/working and saving the verbose in /dev/null to keep it quiet\n# -o for overwrite -d for destination directory of unzipped file\n!unzip -o '/kaggle/input/jigsaw-toxic-comment-classification-challenge/*.zip' -d /kaggle/working > /dev/null\n#Reading input csv files\ndf_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assinging the texts to list of strings\ntrain_texts= df_train.comment_text.values\ntest_texts= df_test.comment_text.values\n#Assignings the labels as a separate df\ntrain_labels = df_train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]]\n#Printing the list of strings\nprint(\"First comment text in training set:\\n\\n\", train_texts[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Text Preprocessing<a class=\"anchor\" id=\"3\"></a>\n\nText data must be encoded as numbers to be used as input or output for ML/DL models. The Keras library provides some basic tools to help us prepare our text data. We will be using Tokenizer class, a Text tokenization utility class that allows to vectorize a text corpus, by turning each text to a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf. \nThis will be a 3-step process:\n\n**1. Initializing the Tokenizer class** \n\n* By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the ' character). These sequences are then split into lists of tokens. They will then be indexed or vectorized. 0 is a reserved index that won't be assigned to any word.\n* We set num_words to MAX_NUM_WORDS (20000) which is the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n\n**2. Calling the fit_on_texts function - Updates internal vocabulary based on a list of texts**\n\nThis method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot). \n\n**3. Calling the texts_to_sequences function - Transforms each text in texts to a sequence of integers**\n\nSo it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n\n***N.B.***\n*After fit_on_texts, which is essentially creating a word_index matrix for the vocabulary, we could do 2 things*\n\n*texts_to_sequences which is used when we use a embedding layer otherwise we can call*\n\n*text_to_matrix which converts the texts to a bag of words*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initializing the class\ntokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n#Updates internal vocabulary based on a list of texts.\ntokenizer.fit_on_texts(train_texts)\n#Transforms each text in texts to a sequence of integers.\ntrain_sequences = tokenizer.texts_to_sequences(train_texts)\ntest_sequences = tokenizer.texts_to_sequences(test_texts)\nword_index = tokenizer.word_index\nprint(\"Length of word Index:\", len(word_index))\nprint(\"First 5 elements in the word_index dictionary:\", dict(list(word_index.items())[0: 5]) )\nprint(\"First comment text in training set:\\n\", train_sequences[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have tokenized the comment texts, we need to pad the sentences to make all the sentences of equal length.\n\n**Why So?**\n\nDeep learning libraries assume a vectorized representation of your data. In the case of variable length sequence prediction problems, this requires that your data be transformed such that each sequence has the same length. This vectorization allows code to efficiently perform the matrix operations in batch for your chosen deep learning algorithms. \n\nThis is also done in Computer Vision, where we generally tend to resize all the images to a fixed size which will be the input size of the Neural Network."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pad tokenized sequences\ntrainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint(\"Shape of padded sequence list:\\n\", trainvalid_data.shape)\nprint(\"First comment text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", trainvalid_data[0][-50:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Defining a 1D CNN model<a class=\"anchor\" id=\"4\"></a>\n\nIn keras, the easiest way to define a model is initiate a Sequential model class and keep adding required layers. A Sequential model is a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nA standard model for document classification is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer. We used 1 embedding layer, 3 sets of Convolution and Pooling layer and 2 sets of Dense layer. We can either use a pre-trained embedding (like Word2Vec) to generate a embedding matrix of size Vocabulary * Dimension of embedding or train a fresh embedding putting it as an input layer along with other weights.\n\n**Conv1D**\n\nConvolutional Neural Network (CNN)  models were developed for image classification, in which the model accepts a two-dimensional input representing an image‚Äôs pixels and color channels. This same process can be applied to 1D sequences of data. The model extracts features from sequences data and maps the internal features of the sequence. CNNs take into account the proximity of words to create trainable patterns.\nThe kernel size/height in the convolutional layer defines the number of words to consider as the convolution is passed across the input text document, providing a grouping parameter. In our case, it will consider 5 words at a time and in the image it will consider 2 words at a time\n\n<img src=\"https://i.imgur.com/zEapf5O.png\" width = 300/>\n\n**Max Pooling 1D**\n\nMax Pooling layer will consolidate the output from the convolutional layer. We had earlier seen MaxPooling 2D. In Maxpooling 1D, the same thing happens only in 1 direction.\n\nWe use sigmoid activation in the output layer. Sigmoid function gives us a probability score between 0 and 1 from each out of the output node. If we would have used softmax it gives a probability distribution across the output nodes that adds to 1.\n\nIn general,\n* For binary classification, we can have 1 output units, use sigmoid activation in the output layer and use binary cross entropy loss\n* For multi class classification, we can have N output units, use softmax activation in the output layer and use categorical cross entropy loss\n* For multi label classification, we can have N output units, use sigmoid activation in the output layer and use binary cross entropy loss"},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = Sequential()\ncnn_model.add(Embedding(MAX_NUM_WORDS, 128))\ncnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\ncnn_model.add(MaxPooling1D(pool_size = 5))\ncnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\ncnn_model.add(MaxPooling1D(pool_size = 5))\ncnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\ncnn_model.add(GlobalMaxPool1D())\ncnn_model.add(Dense(units = 128, activation = 'relu'))\ncnn_model.add(Dense(units = 6, activation = 'sigmoid'))\n\nprint(cnn_model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Compile and fit the CNN model <a class=\"anchor\" id=\"5\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Before starting to train the model, we need to configure it. We need to mention the loss fucntion which will be used to calculate the error at each iteration, optimizer which will specify how the weights will be updated and the metrics which is to be evaluated by the model during training and testing\n\nWhile fitting/ training the model, along with the training set we also pass the following parameters:\n\nbatch_size = Number of samples that goes through the network at a time and updates the network parameters by calculating loss (in Mini Batch Gradient Descent)\n\nepochs = Number of times the whole set of training samples goes through the network\n\nvalidation_data = the dataset that will be used to evaluate the loss and any model metrics at the end of each epoch. This set will not be used for training."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Configures the model for training.\ncnn_model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"AUC\"])\n\n#Split the dataset into train and validation set for training and evaludating the model\nX_train, X_val, y_train, y_val = train_test_split(trainvalid_data, train_labels, shuffle = True, random_state = 123)\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n#Trains the model for a fixed number of epochs (iterations on a dataset)\nhistory = cnn_model.fit(X_train, y_train, batch_size = 128, epochs = 1, validation_data = (X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 6. Predicting and Submitting for Test Data <a class=\"anchor\" id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the test dataset with sample_submission to have all the columns:\n#id,text_data and the target variables in one dataframe\ndf_test = pd.merge(df_test, sample_submission, on = \"id\")\n#Use the CNN model to output probabilities on test data\ny_preds = cnn_model.predict(test_data)\n#Assign the predictions by the model in the final test dataset\ndf_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = y_preds\n#Drop Comment Text as the sample submission doesnt have it and wouldnt be expected\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Save the dataset as a csv to submit it\ndf_test.to_csv(\"sample_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. TODOs <a class=\"anchor\" id=\"7\"></a>\n* Use a multichannel CNN which would combine looking at different length (e.g. kernel size of 3, 5 and 7) of a sentences at a time\n* Tune the model layers and hyperparameters to improve the performance\n\n***Do upvote if you find it helpful üòÅ***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}