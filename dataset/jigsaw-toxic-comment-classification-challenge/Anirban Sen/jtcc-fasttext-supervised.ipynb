{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 0. Introduction\nIn earlier notebook(https://www.kaggle.com/anirbansen3027/jtcc-word2vec) we used Gensim for getting pretrained Word2Vec models/embedding vectors for the words used in the sentences, mapped them against the output variables \"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\" and used Multi Output Logistic Regression Classifier wrapper from sklearn to create Logistic Regression models for all the 6 output variables.\n\nIn this one, we will be using fastText library for both generating embeddings for the sentences as well as text classification. Infact, it gives us an option of doing both in one go. The intuition part is going to be shorter than other methods as the documentation/ readings on the fastText are limited.\n\n**What is fastText?**\n\nIn 2016, Facebook AI Research (FAIR) is open-sourced fastText, a library designed to help build scalable solutions for text representation and classification. fastText take the idea of word embeddings in Word2Vec a step ahead and learns representations\nfor character n-grams, and to represent words as the sum of the n-gram vectors. Taking the word \"where\" and n = 3 as an example, it will be represented by the character n-grams: <wh, whe, her, ere, re>. Apart from txet representations in form of sub-word embeddings, it also provides an off-the-shelf classfication model which is optimized to work with these embedding and give fast results.\n\n**Why is fastText required?**\n\nfastText has 2 benifits over regular word2vec embeddings:\n\n*1. fastText helps in dealing with Out of Vocabulary(OOV) problem:*\n\nWord2Vec faces the problem of Out of vocabulary. Lets say we are training a Word2Vec model from scratch, we setup a vocabulary which contains of all the words in the training data. Now if we have a new word in the test data for which we might be needing embedding, the new missing word will be OOV. In word2vec we completely ignored such words. By using sub-word embeddings in fastText, we try to get an embedding for a word which is OOV as well\n\n*2. By using a distinct vector representation for each word, the Word2Vec model ignores the internal structure of words:*\n\nIn word2vec each word is learned uniquely based on the context it appears in. For example  boxer and boxing are used in different contexts and there is no way we can capture the underlying similarity. Breaking it down to character n-gram helps\n\n**What are additional benifits of fastText?**\n\nAlthough fastText goes beyond word level to character-ngram level, It is extremely fast (and hence the name).Experiments show that fastText is often on par with deeplearning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.\n\nAdditionally, fastText provides word vectors for 157 languages trained on Wikipedia and Crawl (which is amazing).\n\n**How does fastText work?**\n\n*1. Creation of word embeddings*\n\nThe subword model is based on the skipgram model from Word2Vec and instead of using the vector representations of words, an average of the vector representations of the character n-grams are used. Eveything else is quite similar to the skipgram model. \n\n*2. Text Classification*\n<img src=\"https://i.imgur.com/Gl1PiFO.png\" title=\"source: imgur.com\" width = 300/> \nAbove image is taken from the actual paper \"Bag of Tricks for Efficient Text Classification\" where fastText for classification was introduced. fastText uses a shallow neural network similar to Word2Vec networks. We use the softmax function f to compute the probability distribution over the predefined classes.\n\nInfact, it uses something called Hierarchical softmax based on the Huffman coding tree (the most common word/alphabet is given the smallest code in short). So, in Hierarchical softmax probability of a node is always lower than the one of its parent. This helps both when we have a large number of classes and at testing time where we are searching for the most likely class. \n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200921040227/treec.png\" width = 300/> \nFor example, if Travel, Food and Indian Cuisines are 3 classes, and if Travel has a higher probability, we dont even need to calculate the probability for Food and Indian Cuisines. This saves reduces the complexity.\n\nAdditionally it also uses word n-grams as additional features other than the embeddings to capture some partial information about the local word order which is otherwise very computationally expensive to cature in normal BagOfWords.\n\nLet's dive into the code then \n\n### Table of Contents:\n\n[1. Importing Libraries](#1)\n\n[2. Reading Dataset](#2)\n\n[3. Splitting Dataset into training and validation sets](#3)\n\n[4. Basic Preprocessing](#4)\n\n[5. Training and Validating fastText Classifier](#5)\n\n[6. Predicting and Submitting for Test Data](#6)\n\n[7. TODOs](#7)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Libraries <a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom statistics import mean\n\nfrom fasttext import train_supervised\n\n#Sklearn Library\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Reading Dataset <a class=\"anchor\" id=\"2\"></a>\nAll the datasets are provided as zipped files. First we will have to unzip them and then read them into dataframes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#unzipping all the zip folders and saving it /kaggle/working and saving the verbose in /dev/null to keep it quiet\n# -o for overwrite -d for destination directory of unzipped file\n!unzip -o '/kaggle/input/jigsaw-toxic-comment-classification-challenge/*.zip' -d /kaggle/working > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Reading input csv files\ntrain_text = pd.read_csv(\"train.csv\")\ntest_text = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")\n\nprint(train_text.shape, test_text.shape, sample_submission.shape)\ntrain_text.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Splitting Dataset into training and validation sets <a class=\"anchor\" id=\"3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\nX = train_text.comment_text\ny = train_text[y_cols]\n\ntrain, val = train_test_split(train_text, shuffle = True, random_state = 123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Basic Preprocessing <a class=\"anchor\" id=\"4\"></a>"},{"metadata":{},"cell_type":"markdown","source":"In terms of preprocessing we are doing the following steps:\n\n1. We are introducing spaces for some punctuations like ?, ., ) , ( , ! , and removed some just to make the text cleaner\n\n2. We have removed \\n as we can already see a lot of it in the texts\n\n3. We did some unicode normalization it order to handle some unicode issues that might arise\n\n4. This one being the most important, converting the binary labels for all output variables from 0 and 1 to \\__clas__0 and \\__class__1 as the fastText classifier needs it that way. We only need to do this for the training_data as the classifier will not look at the real labels of the validation data or the test data\n\n5. Shuffling the dataset to introduce some randomness and remove ordering (if present)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets do some cleaning of this text\ndef clean_it(text,normalize=True):\n    # Replacing possible issues with data. We can add or reduce the replacemtent in this chain\n    s = str(text).replace(',',' ').replace('\"','').replace('\\'',' \\' ').replace('.',' . ').replace('(',' ( ').\\\n            replace(')',' ) ').replace('!',' ! ').replace('?',' ? ').replace(':',' ').replace(';',' ').lower()\n    s = s.replace(\"\\n\",\" \")\n    \n    # normalizing / encoding the text\n    if normalize:\n        s = s.normalize('NFKD').str.encode('ascii','ignore').str.decode('utf-8')\n    \n    return s\n\n# Now lets define a small function where we can use above cleaning on datasets\ndef clean_df(data, cleanit= False, shuffleit=False, encodeit=False, label_prefix='__class__'):\n    # Defining the new data\n    df = data[['comment_text']].copy(deep=True)\n    for col in y_cols:\n        df[col] = label_prefix + data[col].astype(str) + ' '\n    \n    # cleaning it\n    if cleanit:\n        df['comment_text'] = df['comment_text'].apply(lambda x: clean_it(x,encodeit))\n    \n    # shuffling it\n    if shuffleit:\n        df.sample(frac=1).reset_index(drop=True)\n            \n    return df\n\n# Transform the datasets using the above clean functions\ndf_train_cleaned = clean_df(train, True, True)\ndf_val_cleaned = clean_df(val, True, True, label_prefix='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, as mentioned earlier we have changed the label classes for the training dataset but not for the validation set as they wouldn't be looked at by the fastText classifier"},{"metadata":{},"cell_type":"markdown","source":"## 5. Training and Validating fastText Classifier <a class=\"anchor\" id=\"5\"></a>\n\n* Since fastText classifier takes input a csv file with the text data and the class label, we can't use the Multi-Output Classifier wrapper we were using in earlier notebooks. So we will have to run a for loop to train separate models for each output variable and store predictions for each output variable in the validation set.\n\n* train_supervised is the function that is used for fastText classification. We can tune the learning parameters to improve the model.\n\n* There is no API till date which can take a validation set and give out probabilities for the positive case. We can only get probabilities for one sentence at a time. So, we run a for loop around each of the validation sentence and store the probabilities in a list. We need probabilties as the performance metric is ROC-AUC"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Will contain all the predictions for validation set for all the output variables\nall_preds = []\n#Iterating over all output variables to create separate models\nfor col in tqdm(y_cols):\n    #Path for saving the training dataset\n    train_file = '/kaggle/working/final_train.csv'\n    #Saving the Output Variable and the text data to a csv\n    df_train_cleaned[[col, \"comment_text\"]].to_csv(train_file, header=None, index=False, columns=[col, \"comment_text\"]) \n    #Training the model\n    model = train_supervised(input=train_file, label=\"__class__\", lr=1.0, epoch=2, loss='ova', wordNgrams=2, dim=200, thread=2, verbose=100)\n    #Predictions for validation sets for that ouput variable\n    col_preds = []\n    #Iterating over each sentence in the validation set\n    for text in df_val_cleaned[\"comment_text\"].values:\n        #Get the prediction for class 1\n        pred = model.predict(text, k = 2)[1][1]\n        #Append the prediction to the list of predictions for that output variable\n        col_preds.append(pred)\n    #Append the list of predictions for a output variable to the overall set of predictions for all columns\n    all_preds.append(col_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have got the predictions from the model on the validation set, let's look at the results/performance. Since, the competition uses mean ROC-AUC as the evaluation metric, we will be using the same in the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for calculating roc auc with given actual binary values across target variables and the probability score made by the model\ndef accuracy(y_test, y_pred):\n    aucs = []\n    #Calculate the ROC-AUC for each of the target column\n    for col in range(y_test.shape[1]):\n        aucs.append(roc_auc_score(y_test[:,col],y_pred[:,col]))\n    return aucs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Actual Labels\ny_val_actuals = df_val_cleaned[y_cols].astype(\"int\").to_numpy()\n#Prediction probability - minor ordering\nall_preds_array = np.transpose(np.array(all_preds))\n#Calculate the mean of the ROC-AUC for each of the ouput variable\nmean_auc = mean(accuracy(y_val_actuals,all_preds_array))\nmean_auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"ROC-AUC on the validation set tends to be around 0.77 which is much better than the Word2Vec model. Also, these are early results just on 2 epochs and not-tuned parameters. I think we can get even better results with better tuning. And also the learning and predictions were pretty quick.Apparently, Logistic Regression model that we used with BagOfWords took more time to converge while training than fastText."},{"metadata":{},"cell_type":"markdown","source":"## 6. Predicting and Submitting for Test Data <a class=\"anchor\" id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the test dataset with sample_submission to have all the columns:\n#id,text_data and the target variables in one dataframe\ndf_test = pd.merge(test_text, sample_submission, on = \"id\")\n# Preprocessing the test dataset as well\ndf_test_cleaned = clean_df(df_test, True, True, label_prefix='')\n#Will contain all the predictions for validation set for all the output variables\nall_test_preds = []\nfor col in tqdm(y_cols):\n    #Predictions for test sets for that ouput variable\n    col_preds = []\n    #Iterating over each sentence in the test set\n    for text in df_test_cleaned[\"comment_text\"].values:\n        #Get the prediction for class 1\n        pred = model.predict(text, k = 2)[1][1]\n        #Append the prediction to the list of predictions for that output variable\n        col_preds.append(pred)\n    #Append the list of predictions for a output variable to the overall set of predictions for all columns\n    all_test_preds.append(col_preds)\n#Prediction probability - minor ordering\nall_test_preds_array = np.transpose(np.array(all_test_preds))\n#Assign the predictions by the model in the final test dataset\ndf_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = all_test_preds_array\n#Drop Comment Text as the sample submission doesnt have it and wouldnt be expected\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Save the dataset as a csv to submit it\ndf_test.to_csv(\"sample_submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. TODOs <a class=\"anchor\" id=\"7\"></a>\n* Better Text Preprocessing Typo correction etc can be done to further improve the model\n* Try tuning the hyperparameters to get better results\n\n***Do upvote if you find it helpful üòÅ***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}