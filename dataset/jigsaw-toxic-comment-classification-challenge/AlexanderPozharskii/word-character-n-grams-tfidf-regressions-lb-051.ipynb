{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","name":"python","file_extension":".py","version":"3.6.4","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"source":"# Data reading","cell_type":"markdown","metadata":{"_uuid":"0359f1c686397bc52cbb38393ab6d04c74f2267b","_cell_guid":"5dd5d0cc-e7a4-482f-9a37-f1ad8880e599"}},{"source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom scipy import sparse","cell_type":"code","outputs":[],"metadata":{"_uuid":"032617e230e6d70f8d56bc77d8839c6235411201","_cell_guid":"c8d8555e-a045-4615-b01f-8acd77af2400","collapsed":true},"execution_count":1},{"source":"train = pd.read_csv('../input/train.csv')\ntrain.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"23e1665eaddab6e6273d2cdca9001386e918b361","_cell_guid":"fed5896b-4c49-4fc6-a7a2-221c4769dde9"},"execution_count":2},{"source":"train['comment_text'] = train['comment_text'].fillna('nan')","cell_type":"code","outputs":[],"metadata":{"_uuid":"609ddd0d9bdf50012fa57a9f33bcdb93effc9a39","_cell_guid":"2ea66e7b-fdad-4bac-94d3-da8dcd9aa1ac","collapsed":true},"execution_count":4},{"source":"test = pd.read_csv('../input/test.csv')\ntest.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"2c17f0326289dcecac1bc27e677cf60a2637a4a3","_cell_guid":"027d7a98-ecae-4c9f-9781-24f370a62a5e"},"execution_count":5},{"source":"test['comment_text'] = test['comment_text'].fillna('nan')","cell_type":"code","outputs":[],"metadata":{"_uuid":"0798606a5475744a50534ad040d16e212221095d","_cell_guid":"c8a8db93-5427-410f-a1c7-7925b360523c","collapsed":true},"execution_count":6},{"source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"1d854336015dec939acd711eece59f9cd073a013","_cell_guid":"45bf9df5-f4d0-46f2-914d-81dfa99c5ba5"},"execution_count":7},{"source":"# Basic analysis\nWe have multilabel classification task. So let's check proportion of each label:","cell_type":"markdown","metadata":{"_uuid":"5e779c2dedc6b0f26bc1acb1cb61068e0993c021","_cell_guid":"b8a43edd-30fe-4ef7-be13-ccc465f1f22c"}},{"source":"for label in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(label, (train[label] == 1.0).sum() / len(train))","cell_type":"code","outputs":[],"metadata":{"_uuid":"c84337c7b285de1a82cbf9b057b62171c06fbb4c","_cell_guid":"e53562a3-9dd9-4550-a545-ecb27ff6e2c5"},"execution_count":8},{"source":"and correlation between target variables (maybe we'l could build some kind of hierarchy classification or something like it).","cell_type":"markdown","metadata":{"_uuid":"0aca9b0ee990724bdc48c263914adf2a6fec0126","_cell_guid":"09f6ed4a-9690-4d7c-abd7-038018eb56d3"}},{"source":"train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].corr()","cell_type":"code","outputs":[],"metadata":{"_uuid":"312210b20caa443bc92881cfde50a93540093936","_cell_guid":"3d4ba7eb-54fc-4d9b-8b38-22c83868ce3c"},"execution_count":9},{"source":"# Text postprocessing\n\nI'll try models with:\n- text as is\n- stemmed text\n- lemmatized text","cell_type":"markdown","metadata":{"_uuid":"c1e6613d05570503af75068afb387caaad2a4a99","_cell_guid":"6723e986-3e8f-4257-94a0-4b93ba7ccf51"}},{"source":"stemmer = EnglishStemmer()\n\n@lru_cache(30000)\ndef stem_word(text):\n    return stemmer.stem(text)\n\n\nlemmatizer = WordNetLemmatizer()\n\n@lru_cache(30000)\ndef lemmatize_word(text):\n    return lemmatizer.lemmatize(text)\n\n\ndef reduce_text(conversion, text):\n    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n\n\ndef reduce_texts(conversion, texts):\n    return [reduce_text(conversion, str(text))\n            for text in tqdm(texts)]","cell_type":"code","outputs":[],"metadata":{"_uuid":"d66ab0611d7903b3569c479dd928763078d73be6","_cell_guid":"acde9eca-f73a-489b-a1e6-958f8f9e3f95","collapsed":true},"execution_count":10},{"source":"train['comment_text_stemmed'] = reduce_texts(stem_word, train['comment_text'])\ntest['comment_text_stemmed'] = reduce_texts(stem_word, test['comment_text'])\ntrain['comment_text_lemmatized'] = reduce_texts(lemmatize_word, train['comment_text'])\ntest['comment_text_lemmatized'] = reduce_texts(lemmatize_word, test['comment_text'])","cell_type":"code","outputs":[],"metadata":{"_uuid":"ca6fbd9b79c406d4ce52984532c9b88eecdb8742","_cell_guid":"bf68cf71-17f5-4636-aef7-524a17f98856"},"execution_count":11},{"source":"train.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"ce9988ddbf0efc7ca6cbe1b5619e0a119f191ee8","_cell_guid":"aae35ff5-0d66-4cb9-82e8-0f2ccf168f8f"},"execution_count":14},{"source":"test.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"1c59d1e3ff66e37cc15a6edd1fa202e575792312","_cell_guid":"a4c64f6e-3703-4e8f-aac3-561b4a329ffa"},"execution_count":15},{"source":"# Validation\n\nOur metric is collumn-average of collumn log_loss values. So let's define custom metric based on binary log loss and define cross-validation function:","cell_type":"markdown","metadata":{"_uuid":"c81b72ec6de3b86280ff32c665103e3a144f02d4","_cell_guid":"c1715403-26ee-4531-8eca-8357ebd8b202"}},{"source":"def metric(y_true, y_pred):\n    assert y_true.shape == y_pred.shape\n    columns = y_true.shape[1]\n    column_losses = []\n    for i in range(0, columns):\n        column_losses.append(log_loss(y_true[:, i], y_pred[:, i]))\n    return np.array(column_losses).mean()","cell_type":"code","outputs":[],"metadata":{"_uuid":"863c73537f29824bd19cc530a7c5b25d1b0066b5","_cell_guid":"8901a23a-9307-4360-85da-b86fa3ea05af","collapsed":true},"execution_count":16},{"source":"## Cross-validation\n\nI don't found quickly a way to stratified split for multilabel case.\n\nSo I used next way for stratified splitting:\n\n- define ordered list of all possible label combinations. E.g.\n\n    - 0 = [\"toxic\"=0, \"severe_toxic\"=0, \"obscene\"=0, \"threat\"=0, \"insult\"=0, \"identity_hate\"=0]\n    - 1 = [\"toxic\"=0, \"severe_toxic\"=0, \"obscene\"=0, \"threat\"=0, \"insult\"=1, \"identity_hate\"=0]\n    - 2 = [\"toxic\"=0, \"severe_toxic\"=0, \"obscene\"=0, \"threat\"=0, \"insult\"=1, \"identity_hate\"=1]\n\n- for each row replace label combination with combination index \n- use StratifiedKFold on this\n- train and test model by train/test indices from StratifiedKFold\n\nBasic idea is next:\n- we can present label combination as class for multiclass classification - at least for some cases\n- we can stratified split by combination indices\n    - so in each split distribution of combination indices will be similar to full set\n    - so source label distribution also will be similar\n    \nBut I don't sure that all my assumpions are fully correct - at least, for common case.","cell_type":"markdown","metadata":{"_uuid":"1be09a4b666a18d411ce5e4d302c22fd871d0d36","_cell_guid":"fe62a951-1d2e-40e7-bcf2-783cc104dd9a"}},{"source":"def cv(model, X, y, label2binary, n_splits=3):\n    def split(X, y):\n        return StratifiedKFold(n_splits=n_splits).split(X, y)\n    \n    def convert_y(y):\n        new_y = np.zeros([len(y)])\n        for i, val in enumerate(label2binary):\n            idx = (y == val).max(axis=1)\n            new_y[idx] = i\n        return new_y\n    \n    X = np.array(X)\n    y = np.array(y)\n    scores = []\n    for train, test in tqdm(split(X, convert_y(y)), total=n_splits):\n        fitted_model = model(X[train], y[train])\n        scores.append(metric(y[test], fitted_model(X[test])))\n    return np.array(scores)","cell_type":"code","outputs":[],"metadata":{"_uuid":"5d0aaafc562f06ee8194668781180d4f07773653","_cell_guid":"7c4a3e83-6dc6-420b-a8f0-4f4c617034bf","collapsed":true},"execution_count":17},{"source":"Let's define possible label combinations:","cell_type":"markdown","metadata":{"_uuid":"ca12d3e7a55daec036b96a957ef80d3b5285554c","_cell_guid":"648ec9f8-c9c7-4ac3-b011-56fa2bbb35fd"}},{"source":"label2binary = np.array([\n    [0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 1],\n    [0, 0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1, 1],\n    [0, 0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0, 1],\n    [0, 0, 0, 1, 1, 0],\n    [0, 0, 0, 1, 1, 1],\n    [0, 0, 1, 0, 0, 0],\n    [0, 0, 1, 0, 0, 1],\n    [0, 0, 1, 0, 1, 0],\n    [0, 0, 1, 0, 1, 1],\n    [0, 0, 1, 1, 0, 0],\n    [0, 0, 1, 1, 0, 1],\n    [0, 0, 1, 1, 1, 0],\n    [0, 0, 1, 1, 1, 1],\n    [0, 1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0, 1],\n    [0, 1, 0, 0, 1, 0],\n    [0, 1, 0, 0, 1, 1],\n    [0, 1, 0, 1, 0, 0],\n    [0, 1, 0, 1, 0, 1],\n    [0, 1, 0, 1, 1, 0],\n    [0, 1, 0, 1, 1, 1],\n    [0, 1, 1, 0, 0, 0],\n    [0, 1, 1, 0, 0, 1],\n    [0, 1, 1, 0, 1, 0],\n    [0, 1, 1, 0, 1, 1],\n    [0, 1, 1, 1, 0, 0],\n    [0, 1, 1, 1, 0, 1],\n    [0, 1, 1, 1, 1, 0],\n    [0, 1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 0, 0],\n    [1, 0, 0, 0, 0, 1],\n    [1, 0, 0, 0, 1, 0],\n    [1, 0, 0, 0, 1, 1],\n    [1, 0, 0, 1, 0, 0],\n    [1, 0, 0, 1, 0, 1],\n    [1, 0, 0, 1, 1, 0],\n    [1, 0, 0, 1, 1, 1],\n    [1, 0, 1, 0, 0, 0],\n    [1, 0, 1, 0, 0, 1],\n    [1, 0, 1, 0, 1, 0],\n    [1, 0, 1, 0, 1, 1],\n    [1, 0, 1, 1, 0, 0],\n    [1, 0, 1, 1, 0, 1],\n    [1, 0, 1, 1, 1, 0],\n    [1, 0, 1, 1, 1, 1],\n    [1, 1, 0, 0, 0, 0],\n    [1, 1, 0, 0, 0, 1],\n    [1, 1, 0, 0, 1, 0],\n    [1, 1, 0, 0, 1, 1],\n    [1, 1, 0, 1, 0, 0],\n    [1, 1, 0, 1, 0, 1],\n    [1, 1, 0, 1, 1, 0],\n    [1, 1, 0, 1, 1, 1],\n    [1, 1, 1, 0, 0, 0],\n    [1, 1, 1, 0, 0, 1],\n    [1, 1, 1, 0, 1, 0],\n    [1, 1, 1, 0, 1, 1],\n    [1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 1, 0, 1],\n    [1, 1, 1, 1, 1, 0],\n    [1, 1, 1, 1, 1, 1],\n])","cell_type":"code","outputs":[],"metadata":{"_uuid":"a775bd7990da167fd6f923263e88a3cf143cbfc6","_cell_guid":"0c1d97b8-67e2-464c-90c1-62c19f182920","collapsed":true},"execution_count":18},{"source":"# Dummy model\n\nLet's build dummy model that always return 0.5 and compare score on cross-validation with test-set public leatherboard \"All 0.5s Benchmark\" (score - 0.693)","cell_type":"markdown","metadata":{"_uuid":"0117807ff37c2807c929e870730c7ae42fbb95bd","_cell_guid":"c0e974ab-af1a-40e4-b839-1592b118048c"}},{"source":"def dummy_model(X, y):\n    def _predict(X):\n        return np.ones([X.shape[0], 6]) * 0.5\n    \n    return _predict\n\ncv(dummy_model,\n   train['comment_text'],\n   train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n   label2binary)","cell_type":"code","outputs":[],"metadata":{"_uuid":"b167c4e627cc3629ba0c39e8f45f068f8788601b","_cell_guid":"fe679495-7f7e-4416-ba32-d2dbd3aa07cc"},"execution_count":19},{"source":"seems like we built metric correctly, so let's go to baseline building\n\n# Baseline (binary logistic regression over word-based tf-idf)\n\nLet's build model that:\n- compute tf-idf for given train texts\n- train 6 logistic regressions (one for each label)\n- compute tf-idf on test texts\n- compute probability of \"1\" class for all 6 regressions","cell_type":"markdown","metadata":{"_uuid":"45ddeeb39d8252016c019add0ed24b128435cb56","_cell_guid":"3297eb2e-66db-4d1e-9a28-f83484f4b0b2"}},{"source":"def regression_baseline(X, y):\n    tfidf = TfidfVectorizer()\n    X_tfidf = tfidf.fit_transform(X)\n    columns = y.shape[1]\n    regressions = [\n        LogisticRegression().fit(X_tfidf, y[:, i])\n        for i in range(columns)\n    ]\n    \n    def _predict(X):\n        X_tfidf = tfidf.transform(X)\n        predictions = np.zeros([len(X), columns])\n        for i, regression in enumerate(regressions):\n            regression_prediction = regression.predict_proba(X_tfidf)\n            predictions[:, i] = regression_prediction[:, regression.classes_ == 1][:, 0]\n        return predictions\n    \n    return _predict","cell_type":"code","outputs":[],"metadata":{"_uuid":"3adcd67349e34f957bd974886457b21f8c00a1bc","_cell_guid":"f2932f23-6129-4082-90d3-09238fb2b6b7","collapsed":true},"execution_count":20},{"source":"Now let's check model on source texts/stemmed texts/lemmatized texts","cell_type":"markdown","metadata":{"_uuid":"aa6a106976a09922d3b94c588cbc1db7da0ccce7","_cell_guid":"c4783bb9-9cf4-4758-8a74-7008c20c5fa0"}},{"source":"cv(regression_baseline,\n   train['comment_text'],\n   train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n   label2binary)","cell_type":"code","outputs":[],"metadata":{"_uuid":"22b4fab009572ec1c24e66126d59930e92f6c820","_cell_guid":"9b13057c-323f-497b-bb56-96036a73700b"},"execution_count":21},{"source":"cv(regression_baseline,\n   train['comment_text_stemmed'],\n   train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n   label2binary)","cell_type":"code","outputs":[],"metadata":{"_uuid":"7bf420163021df472b63071e6c14f697a2e8f2eb","_cell_guid":"84882d30-8883-450b-b5f3-227b737586e1"},"execution_count":22},{"source":"cv(regression_baseline,\n   train['comment_text_lemmatized'],\n   train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n   label2binary)","cell_type":"code","outputs":[],"metadata":{"_uuid":"780c3c85e55675a114635b32f54bf8a50759072b","_cell_guid":"ff732007-2228-47be-96d2-479ffba58862"},"execution_count":23},{"source":"As you can see - this baseline gives best score on stemmed texts.\nAnyway - let's  try to add character-level features:\n\n# Regressions over tfidf over words and character n-grams\n\nLet's build model that:\n- compute tfidf of words of stemmed texts\n- compute tfidf of character n-grams from source text\n- train/predict regressions on computed tfidf-s.","cell_type":"markdown","metadata":{"_uuid":"b5dfd28d9d63b0ac8648af712d91479c6c1a7da0","_cell_guid":"a94e8b0e-751d-44f1-a6d2-e56cd8fd2d6e"}},{"source":"def regression_wordchars(X, y):\n    tfidf_word = TfidfVectorizer()\n    X_tfidf_word = tfidf_word.fit_transform(X[:, 1])\n    tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)\n    X_tfidf_char = tfidf_char.fit_transform(X[:, 0])\n    X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n    \n    columns = y.shape[1]\n    regressions = [\n        LogisticRegression().fit(X_tfidf, y[:, i])\n        for i in range(columns)\n    ]\n    \n    def _predict(X):\n        X_tfidf_word = tfidf_word.transform(X[:, 1])\n        X_tfidf_char = tfidf_char.transform(X[:, 0])\n        X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n        predictions = np.zeros([len(X), columns])\n        for i, regression in enumerate(regressions):\n            regression_prediction = regression.predict_proba(X_tfidf)\n            predictions[:, i] = regression_prediction[:, regression.classes_ == 1][:, 0]\n        return predictions\n    \n    return _predict","cell_type":"code","outputs":[],"metadata":{"_uuid":"b165a7eb2bc448c7d1fa191e4579a51fd1a7eced","_cell_guid":"8bd7fa08-43f0-46f9-8870-499174905a67","collapsed":true},"execution_count":25},{"source":"cv(regression_wordchars,\n   train[['comment_text', 'comment_text_stemmed']],\n   train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']],\n   label2binary)","cell_type":"code","outputs":[],"metadata":{"_uuid":"e268457a05af6da5da8e4b6a2240252f22b5d12b","_cell_guid":"47b8f303-ed0b-4dd9-998f-947d708249ac"},"execution_count":26},{"source":"# Prediction\n\nLet's use our best model - regression over word&chars tfidf to build submission:","cell_type":"markdown","metadata":{"_uuid":"695aca3ecc8f779222b5f2415c6b81b6920d740d","_cell_guid":"f9912856-a38c-4368-84ca-a66645159941"}},{"source":"%%time\nmodel = regression_wordchars(np.array(train[['comment_text', 'comment_text_stemmed']]),\n                             np.array(train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))","cell_type":"code","outputs":[],"metadata":{"_uuid":"c3a7529e9153e2855f2150f708e679540ec1df3a","_cell_guid":"968580ed-eb6a-4c20-9c98-4991c159a4fa"},"execution_count":27},{"source":"%%time\nprediction = model(np.array(test[['comment_text', 'comment_text_stemmed']]))","cell_type":"code","outputs":[],"metadata":{"_uuid":"448dc9b6f3282119059522962b9a8153de9757aa","_cell_guid":"056ee24d-4a16-41c0-bdf9-19097bfa6868"},"execution_count":28},{"source":"for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n    submission[label] = prediction[:, i]\nsubmission.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"e21dc7e6f3049cd7bc5dc3fda34f27c4f830fddf","_cell_guid":"7f7f873d-b29c-4cd0-95d7-8bf820f77cf4"},"execution_count":30},{"source":"submission.to_csv('output.csv', index=None)","cell_type":"code","outputs":[],"metadata":{"_uuid":"3c8a093eb8ddc8102039bfc0536cffd04eac443c","_cell_guid":"f0aa24bd-de50-4e5d-b739-4f7245037522","collapsed":true},"execution_count":29},{"source":"","cell_type":"code","outputs":[],"metadata":{"_uuid":"66c51ff3cde21d04365f57e98167684b930b2157","_cell_guid":"18b224db-0c66-4438-92a2-1221f5e43989","collapsed":true},"execution_count":null}]}