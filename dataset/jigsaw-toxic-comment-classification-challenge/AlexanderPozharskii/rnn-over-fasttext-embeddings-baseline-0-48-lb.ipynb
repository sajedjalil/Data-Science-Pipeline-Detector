{"cells":[{"cell_type":"markdown","source":"In this notebook I'll try to use RNN on fasttext embeddings (seems like for this case it'll may be good idea to use char-based embedding - at lear as I can see by mine previous kernel with logisitc regressions over word / chars tf-idfs).\nSo you'll need fasttext installed (for Windows I used this build - http://cs.mcgill.ca/~mxia3/FastText-for-Windows/).\n\n**Also - there is no fasttext installed at Kaggle, so you'll need to run notebook on your machine.**\n\n# Data import","metadata":{"_cell_guid":"448bfb2b-b05d-49da-99de-229ce294a20d","_uuid":"8ab3a26879ccb6ba1939efa2a43384d8224908bb"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"6c13feb2-1234-4dc5-aa8b-ad438236a44a","_uuid":"94f7a22e45a5a0163ee30c5d732c76732c618115"},"source":"import pandas as pd\nimport numpy as np\nfrom itertools import chain\nfrom nltk.tokenize import wordpunct_tokenize\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import Dense, Embedding, Dropout, LSTM, Bidirectional, GlobalMaxPool1D, InputLayer, BatchNormalization, Activation\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom subprocess import call\nfrom sklearn.utils import compute_sample_weight\nfrom sklearn.metrics import confusion_matrix, log_loss\nfrom collections import OrderedDict","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"0d90b317-42b2-4486-8051-28284853ec32","_uuid":"22dcdf2010fccf25945187cf45634a562c22ca47"},"source":"train = pd.read_csv(\"../input/train.csv\")\ntrain.fillna(\"nan\")\ntrain.head()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"51432730-607b-483a-9deb-98245f8f865f","_uuid":"799800abdeb3b8f64f7b7d159bc97744eb653264"},"source":"test = pd.read_csv(\"../input/test.csv\")\ntest.fillna(\"nan\")\ntest.head()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"832c4a28-394b-4c9d-9c27-571050534bcb","_uuid":"722d3d53ae18044a451c6222da24cee306186c91"},"source":"submission = pd.read_csv('../input/sample_submission.csv')\nsubmission.head()","outputs":[]},{"cell_type":"markdown","source":"Some values interpreted as float, so I'll convert it to strings:","metadata":{"_cell_guid":"c5791ee8-919a-42df-9b4e-c26f97713ee0","_uuid":"7d574a17ff913dbbd1deeecb309198bb0b489534"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"675c8a41-49da-4fa1-934b-68c6f89566db","_uuid":"0cb38ad3d25ac6d7354088dde5f18e4951d42f5e"},"source":"train['comment_text'] = train['comment_text'].apply(str)\ntest['comment_text'] = test['comment_text'].apply(str)","outputs":[]},{"cell_type":"markdown","source":"# train/validation split\nLet's split data to train/validation set.\n\nI used next method so save each class distribution:\n\n- build all possible labels combinations\n- excluded combination that seen l;east then 2 times\n- replaced label combination with combination index\n- build indices for stratified split based on combination indices","metadata":{"_cell_guid":"e634d87f-754d-4804-8959-0cc66da42fea","_uuid":"a98745649d60d06d9bb2b946a4618bf5eeaf840a"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"0baafe20-b15f-47f6-94d8-3b190770f93e","_uuid":"e95420630fcc2179c6ef4cfc48f8a756500267a2"},"source":"targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"6c424491-2cc9-44fd-92ad-8eb4347e12d2","_uuid":"20096b1e8a5b6ba101e84a13dea9e959dc17318e"},"source":"y = np.array(train[targets])\ntexts = np.array(train['comment_text'])\ntexts_test = np.array(test['comment_text'])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"54b3a248-d0ee-4f41-8463-1875fb47aa6f","_uuid":"331c0f7aa53471da60e811d8b585657ff10fd8f1"},"source":"# Some mappings exlucded because have only 1 sample.\nlabel_mapping = np.array([\n    [0, 0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0, 1],\n    [0, 0, 0, 0, 1, 0],\n    [0, 0, 0, 0, 1, 1],\n    [0, 0, 0, 1, 0, 0],\n    [0, 0, 0, 1, 0, 1],\n    [0, 0, 0, 1, 1, 0],\n    [0, 0, 0, 1, 1, 1],\n    [0, 0, 1, 0, 0, 0],\n    #[0, 0, 1, 0, 0, 1],\n    [0, 0, 1, 0, 1, 0],\n    [0, 0, 1, 0, 1, 1],\n    #[0, 0, 1, 1, 0, 0],\n    [0, 0, 1, 1, 0, 1],\n    #[0, 0, 1, 1, 1, 0],\n    [0, 0, 1, 1, 1, 1],\n    [0, 1, 0, 0, 0, 0],\n    [0, 1, 0, 0, 0, 1],\n    [0, 1, 0, 0, 1, 0],\n    [0, 1, 0, 0, 1, 1],\n    [0, 1, 0, 1, 0, 0],\n    [0, 1, 0, 1, 0, 1],\n    [0, 1, 0, 1, 1, 0],\n    [0, 1, 0, 1, 1, 1],\n    [0, 1, 1, 0, 0, 0],\n    [0, 1, 1, 0, 0, 1],\n    [0, 1, 1, 0, 1, 0],\n    [0, 1, 1, 0, 1, 1],\n    [0, 1, 1, 1, 0, 0],\n    [0, 1, 1, 1, 0, 1],\n    [0, 1, 1, 1, 1, 0],\n    [0, 1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 0, 0],\n    [1, 0, 0, 0, 0, 1],\n    [1, 0, 0, 0, 1, 0],\n    [1, 0, 0, 0, 1, 1],\n    [1, 0, 0, 1, 0, 0],\n    [1, 0, 0, 1, 0, 1],\n    [1, 0, 0, 1, 1, 0],\n    [1, 0, 0, 1, 1, 1],\n    [1, 0, 1, 0, 0, 0],\n    [1, 0, 1, 0, 0, 1],\n    [1, 0, 1, 0, 1, 0],\n    [1, 0, 1, 0, 1, 1],\n    [1, 0, 1, 1, 0, 0],\n    [1, 0, 1, 1, 0, 1],\n    [1, 0, 1, 1, 1, 0],\n    [1, 0, 1, 1, 1, 1],\n    [1, 1, 0, 0, 0, 0],\n    [1, 1, 0, 0, 0, 1],\n    [1, 1, 0, 0, 1, 0],\n    [1, 1, 0, 0, 1, 1],\n    [1, 1, 0, 1, 0, 0],\n    [1, 1, 0, 1, 0, 1],\n    [1, 1, 0, 1, 1, 0],\n    [1, 1, 0, 1, 1, 1],\n    [1, 1, 1, 0, 0, 0],\n    [1, 1, 1, 0, 0, 1],\n    [1, 1, 1, 0, 1, 0],\n    [1, 1, 1, 0, 1, 1],\n    [1, 1, 1, 1, 0, 0],\n    [1, 1, 1, 1, 0, 1],\n    [1, 1, 1, 1, 1, 0],\n    [1, 1, 1, 1, 1, 1],\n])\ny_converted = np.zeros([len(y)])\nfor i in range(len(label_mapping)):\n    idx = (y == label_mapping[i]).sum(axis=1) == 6\n    y_converted[idx] = i\ntrain_indices, val_indices, _, _ = train_test_split(np.fromiter(range(len(y)), dtype=np.int32),\n                                                    y_converted,\n                                                    test_size=0.1,\n                                                    stratify=y_converted)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"b2946515-6297-4960-90d2-8a8d59799461","_uuid":"096550f7553a611b4e53e808bacb4be87ce7034b"},"source":"texts_train, texts_val = texts[train_indices], texts[val_indices]\ny_train, y_val = y[train_indices], y[val_indices]","outputs":[]},{"cell_type":"markdown","source":"# Embedding training\n\nNow I'll prepare texts from train subset to use in fasttext train:","metadata":{"_cell_guid":"90bd2f50-2815-4463-a079-3349e9599c80","_uuid":"7d81a479a5f596fdb1b355c4cc08f7037b7aece2"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4decd929-045c-4b92-81d6-0aca53799e0c","_uuid":"58294f30891b179c1e272fe36165502ac1f1532f"},"source":"with open('fasttext-embedding-train.txt', 'w', encoding='utf-8') as target:\n    for text in texts_train:\n        target.write('__label__0\\t{0}\\n'.format(text.strip()))","outputs":[]},{"cell_type":"markdown","source":"And - with next command I'll start fasttext model train:\n\nFor linux system similar command will be \n\n    fasttext skipgram -input fasttext-embedding-train.txt -output embedding-model > /dev/null 2>&1","metadata":{"_cell_guid":"31ef2d6d-d384-468f-89de-e069573cecc5","_uuid":"aa2cbc2d80f0157277844fe52b46a6399ca8c20e"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4c5e1525-84b5-4a11-b6d4-1ff55f7e8fcf","_uuid":"e96de2cbde3012e2ce2a3d3220650ecfcad5a08c"},"source":"!fasttext skipgram -input fasttext-embedding-train.txt -output embedding-model >nul 2>&1","outputs":[]},{"cell_type":"markdown","source":"Now I need to:\n- prepare list of words from train/validation/test sets\n- calculate vectors for each word\n- load vectors in mine model","metadata":{"_cell_guid":"ca96b197-63e3-4a8d-9220-e3662e30342b","_uuid":"4fb58a047cc86184d4ca7067f85f780badbf73b7"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"50792345-d852-476e-b675-5a58c6071422","_uuid":"122fff20980ae778e45b5c2c1ac649fd93aae169"},"source":"train_texts_tokenized = map(wordpunct_tokenize, train['comment_text'])\ntest_texts_tokenized = map(wordpunct_tokenize, train['comment_text'])\ntrain_text_tokens = set(chain(*train_texts_tokenized))\ntest_text_tokens = set(chain(*test_texts_tokenized))\ntext_tokens = sorted(train_text_tokens | test_text_tokens)\nwith open(\"fasttext-words.txt\", \"w\", encoding=\"utf-8\") as target:\n    for word in text_tokens:\n        target.write(\"{0}\\n\".format(word.strip()))","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"8129aa3f-3420-4883-a367-179d7c73b522","_uuid":"23241e099d3b8a8569873ed39f4916ffcf1f8cfe"},"source":"!fasttext print-word-vectors embedding-model.bin < fasttext-words.txt > fasttext-vectors.txt","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"2dd0b692-077a-46dd-99b6-29975d9f6627","_uuid":"a54d009e967551b9ba5ab42c32985783772ef1ce"},"source":"embedding_matrix = np.zeros([len(text_tokens) + 1, 100])\nword2index = {}\nwith open(\"fasttext-vectors.txt\", \"r\", encoding=\"utf-8\") as src:\n    for i, line in enumerate(src):\n        parts = line.strip().split(' ')\n        word = parts[0]\n        vector = map(float, parts[1:])\n        word2index[word] = len(word2index)\n        embedding_matrix[i] = np.fromiter(vector, dtype=np.float)","outputs":[]},{"cell_type":"markdown","source":"And finally I'll replace words in text with embedding vector indices:","metadata":{"_cell_guid":"0dcfc4f3-72f4-44e5-a589-4b45c1de1819","_uuid":"bc4dba8de20ba2e29bedd31b2bd3636ac0a38937"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"74ee53e7-30a1-4209-a085-673325b3e2d3","_uuid":"653bd7d40e4afbff9f885e76146ce29f8d12ec8e"},"source":"def text2sequence(text):\n    return list(map(lambda token: word2index.get(token, len(word2index) - 1), wordpunct_tokenize(str(text))))\n\n\nX_train = sequence.pad_sequences(list(map(text2sequence, texts_train)), maxlen=100)\nX_val = sequence.pad_sequences(list(map(text2sequence, texts_val)), maxlen=100)\nX_test = sequence.pad_sequences(list(map(text2sequence, texts_test)), maxlen=100)","outputs":[]},{"cell_type":"markdown","source":"# Model\n\nLet's build and train model:","metadata":{"_cell_guid":"e3abd0e4-0d18-4880-853a-4076fe9091c5","_uuid":"75aacd572adb78d187a8bbd1edf25f862f57fa0c"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"3d9250fb-7539-48d2-93cc-a2dfea43f671","_uuid":"bfe146cbad9cf8c32a4965762a4ae134c65db0a0"},"source":"embed_size = 100\nmodel = Sequential([\n    InputLayer(input_shape=(100,), dtype='int32'),\n    Embedding(len(embedding_matrix), embed_size),\n    Bidirectional(LSTM(50, return_sequences=True)),\n    GlobalMaxPool1D(),\n    Dropout(0.3),\n    Dense(50, activation='relu'),\n    Dropout(0.3),\n    Dense(6, activation='sigmoid')\n])\nembedding = model.layers[1]\nembedding.set_weights([embedding_matrix])\nembedding.trainable = False\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.summary()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"262aa817-0b1a-4092-807e-694b2b872a3d","_uuid":"9074989c6340e88f097203bbb2d618784c145b2b"},"source":"model.fit(X_train, y_train, \n          batch_size=64, \n          epochs=10, \n          validation_data=(X_val, y_val), \n          verbose=True, \n          callbacks=[\n              ModelCheckpoint('model.h5', save_best_only=True),\n              EarlyStopping(patience=3)\n          ])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"f20233b8-5373-48b3-bf20-6fbd3a193d60","_uuid":"5c34d1b7ddf3888ef2839605bd844ee9d770f9f0"},"source":"model.load_weights('model.h5')","outputs":[]},{"cell_type":"markdown","source":"# Test prediction","metadata":{"_cell_guid":"46f0afba-2fbe-4528-8bdd-bb1cb4171ef9","_uuid":"ded2fbf486f2229329083eeef2713bdb9b657621"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"cafcb869-dc2e-4607-a07f-9bd2bcc1300b","_uuid":"cbe29ef4194cc8e9c2f8a17a98351aa6fedf09c1"},"source":"test_prediction = model.predict(X_test, verbose=True)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"9856adc1-d0cd-43d0-a940-bf9e1dea00ef","_uuid":"6aab85f2e7ee36cd036ad9e77ddb2b8b49be3190"},"source":"for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n    submission[label] = test_prediction[:, i]","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"07eecc24-a4e9-40f0-85ae-ec662f7776a5","_uuid":"e93bd5aacf22cf4df1ca7099a022b44a6ddaa7b9"},"source":"submission.head()","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"3acf264a-6088-490a-8ee9-1edd5fa9eb16","_uuid":"a4101fd96b5de4c0c7efdb74d3e7624bc44acd74"},"source":"submission.to_csv('output.csv', index=None)","outputs":[]},{"cell_type":"markdown","source":"# Validation error analysis\n\nLet's make prediction on validation set - and see what kind of errors we making with different classes:","metadata":{"_cell_guid":"833c80a5-c47e-40b7-b64d-42d125ff0f40","_uuid":"018bfb05a32e8602e757f36e0febafff97844b78"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"dafdce80-022d-401a-afe6-9453549a4615","_uuid":"66c0a3fa72b974fd6edf885dcf8a61ade1f6993c"},"source":"val_prediction = model.predict(X_val, verbose=True)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"7944aacb-a1ea-4d6e-82a8-75c28961de61","_uuid":"20b3ba0784698d6e2fab4a9baa464f862cd6d4ce"},"source":"def show_confustion_matrix(y_true, y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    df = pd.DataFrame(OrderedDict([\n        (\"true-class\", [\"negative\", \"positive\"]),\n        (\"negative-classified\", [tn, fn]),\n        (\"positive-classified\", [fp, tp]),\n    ]))\n    return df.set_index(\"true-class\")","outputs":[]},{"cell_type":"markdown","source":"## Toxic","metadata":{"_cell_guid":"1fc33ef6-05be-4c99-86d6-6fec97880c19","_uuid":"9cdc24342847209e2d6493e862fc53f61e01ad92"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"0e340e59-e66f-48af-a263-e240346b8153","_uuid":"633cde6f47564d0447d9de4fd24dc8a6c6933fdc"},"source":"log_loss(y_val[:, 0], val_prediction[:, 0])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"f671b4ac-d846-495f-8366-6fbdf2613b8c","_uuid":"14231514a2e8f54a76c33585bbfbf2d660dca798"},"source":"show_confustion_matrix(y_val[:, 0], val_prediction[:, 0] > 0.5)","outputs":[]},{"cell_type":"markdown","source":"## Severe toxic","metadata":{"_cell_guid":"7fc15747-1bd8-48d8-955c-6e9ffab5f59d","_uuid":"d1a13de9157d6e6df19fb4fbda0faab7d7aa6e50"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"f9a22b0d-898d-4836-af22-14020dceac3a","_uuid":"74a2409d8f3a1a345d694d4d255cf8fd26841221"},"source":"log_loss(y_val[:, 1], val_prediction[:, 1])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"d74fb1c3-51eb-413d-9ad4-20a2a8726368","_uuid":"76a4f826cc9891c6bb0f87e58e5f86b5b15a0d47"},"source":"show_confustion_matrix(y_val[:, 1], val_prediction[:, 1] > 0.5)","outputs":[]},{"cell_type":"markdown","source":"## Obscene","metadata":{"_cell_guid":"63619573-e008-404f-af72-926ee98e8d26","_uuid":"15d7c0e8538d2433ead178e0dfbd07dc93bc267f"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"9d81efdb-7501-4459-a540-80b23233cd25","_uuid":"5d874d017026a0829d5597b8837d643d4b7ad7fa"},"source":"log_loss(y_val[:, 2], val_prediction[:, 2])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"cc838025-45d3-4779-8ee5-8b3c233ab21e","_uuid":"9ba314ce76ae333dfaf725ae552e11da2e2b2520"},"source":"show_confustion_matrix(y_val[:, 2], val_prediction[:, 2] > 0.5)","outputs":[]},{"cell_type":"markdown","source":"## Threat","metadata":{"_cell_guid":"fe8ce8d6-f146-4b19-abe7-0d7f2166e5a7","_uuid":"747889f380c116e82252f479b7d22e322c23dcb7"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"6750911e-7c1c-40ae-9414-a45496d90d54","_uuid":"30eadb532d448bb9047c17b72ec91a191735fda5"},"source":"log_loss(y_val[:, 3], val_prediction[:, 3])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"31b4970f-bd36-4c6a-b11c-45d30368bc59","_uuid":"24b18bb136fd1c3e418ce6752a0f6733ec64a914"},"source":"show_confustion_matrix(y_val[:, 3], val_prediction[:, 3] > 0.5)","outputs":[]},{"cell_type":"markdown","source":"## Insult","metadata":{"_cell_guid":"ae7c7836-03d6-4481-8afc-68a5eaf2d498","_uuid":"dfee179cf599f6fe0e492c364d2f54da470d3a87"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"f917cfc2-1a77-4736-aacd-9af6525dc3e8","_uuid":"07fc59488018ecebb2b36770fbebf85dd56d9b2d"},"source":"log_loss(y_val[:, 4], val_prediction[:, 4])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"79be8fda-04ed-482b-ac1b-f4cf796a4d94","_uuid":"a41ef273426733250ce158f1fce5a967d3e25a7d"},"source":"show_confustion_matrix(y_val[:, 4], val_prediction[:, 4] > 0.5)","outputs":[]},{"cell_type":"markdown","source":"## Identity hate","metadata":{"_cell_guid":"92640725-a370-41b9-99a6-ed79cdddc2f8","_uuid":"da268f03da2f5d6d85933840d95c4c99792416d0"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"1d195f26-af09-4f2f-b88f-21b1773af9e4","_uuid":"172c6cf6fc654320c5fb248987ca5a7b8715e876"},"source":"log_loss(y_val[:, 5], val_prediction[:, 5])","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"8d9d700a-997a-4965-94d5-828b94eb2dfd","_uuid":"e58615d421a3bb61242191283b2894e7846b2d42"},"source":"show_confustion_matrix(y_val[:, 5], val_prediction[:, 5] > 0.5)","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"7de1cc68-c0bb-455c-856d-5869c14725c1","_uuid":"374ce534e43ba5cee3d608aaa778a265a2167ac7"},"source":"","outputs":[]}],"metadata":{"language_info":{"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1,"nbformat":4}