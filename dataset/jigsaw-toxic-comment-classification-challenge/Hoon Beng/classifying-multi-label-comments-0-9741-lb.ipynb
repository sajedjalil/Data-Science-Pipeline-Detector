{"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"nbconvert_exporter":"python","name":"python","version":"3.6.3","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"9dacb7a2a5a9e582d7e597be4ea479cedd79489a","_cell_guid":"d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"},"source":"# Classifying multi-label comments with Logistic Regression\n#### Rhodium Beng\nStarted on 20 December 2017\n\nThis kernel is inspired by:\n- kernel by Jeremy Howard : _NB-SVM strong linear baseline + EDA (0.052 lb)_\n- kernel by Issac : _logistic regression (0.055 lb)_\n- _Solving Multi-Label Classification problems_, https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","collapsed":true},"source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b","_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b"},"source":"## Load training and test data","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","_cell_guid":"094fff47-db10-447c-965e-08056f718bde","collapsed":true},"source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6","_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa"},"source":"## Examine the data (EDA)","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"9e53b7599d707a9420a75c37c7ac6d05bed9df7b","_cell_guid":"c4c7137d-6bc7-4b41-b50a-e511883155e9","_kg_hide-output":true},"source":"train_df.sample(5)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"6c824d91ae1e801e1489e93e1a9932c8c0cb0e0a","_cell_guid":"40597119-1274-4d5b-a054-b4b17dbcbb36"},"source":"In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"7f7f2581edb2f42a64812dec31622a011dceff80","_cell_guid":"7e29bebb-d9b7-44ab-a6ba-9f98e6507d5e","collapsed":true},"source":"cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"e4a226272e319458391e117e8fb7f16b17c4884f","_cell_guid":"ce00e980-da07-4412-ae4c-5152cc2036e0"},"source":"# check missing values in numeric columns\ntrain_df.describe()","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"474f4cf26190a2f2011ca0908052973fec1b1520","_cell_guid":"b2be6443-eb4e-4e12-81b3-faf2ea692ca4"},"source":"There are no missing numeric values. Based on the mean values, it also looks like there are many comments which are not labelled in any of the six categories.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"99f1db4864c5c538f2a925d7b6733bb4b1c68707","_cell_guid":"997cc605-71a0-4ceb-a64b-e3e44c172aea"},"source":"# check for any 'null' comment\nno_comment = train_df[train_df['comment_text'].isnull()]\nlen(no_comment)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"964e57595bb6e87a47aa82557c9d63aae3c24bd0","_cell_guid":"af5ba625-bd5d-4ad3-948b-5641b10d62fb","_kg_hide-output":true},"source":"test_df.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"c616bcb0dcf611679dcb5009def9d71d6727cd0e","_cell_guid":"6bd65c22-9c8a-4756-a7bf-16187a6044d1"},"source":"no_comment = test_df[test_df['comment_text'].isnull()]\nno_comment","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"ad7f31ff8035dc60d37c8070f8bbaa9e7afac32f","_cell_guid":"ecce8833-b5b4-40b4-a164-015b7380b8b1"},"source":"There is a row in the test data which does not contain any comment, so let's put 'unknown' in its place.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"ffeacaea28588d64762dc2a67aedddb4b04088b5","_cell_guid":"ec4d0216-9bae-4d23-bbb6-c4afddf34815","collapsed":true},"source":"# fill NaN with string \"unknown\"\ntest_df.fillna('unknown',inplace=True)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"e497fc6688e16602e3f49a11939f32324d295a8d","_cell_guid":"b3dcbd9f-dbb8-4a5a-96b2-b93882516e27"},"source":"# let's see the total rows in train, test data and the numbers for the various categories\nprint('Total rows in train is {}'.format(len(train_df)))\nprint('Total rows in test is {}'.format(len(test_df)))","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"c5aea4341bf7da46627704b28f162776fc2a8c59","_cell_guid":"26e7c4a8-bf9d-4105-9e84-b48f9f056015"},"source":"print(train_df[cols_target].sum())","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654","collapsed":true},"source":"Majority of the comments are not labelled in one or more of these categories.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"1e97432af65b6b75b436daabc83bdf57775a59c1","_cell_guid":"b26588a7-9a7f-4183-98b2-fb94a70bedaa","collapsed":true},"source":"# Let's look at the character length for the rows and record these\ntrain_df['char_length'] = train_df['comment_text'].str.len()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"scrolled":true,"_uuid":"448c3492fc2fe24f30bd7b97047d69f16b58ca2f","_cell_guid":"d5ac5111-5bba-44c9-a039-7bb01a5bfd59"},"source":"# look at the histogram plot for text length\nsns.set()\ntrain_df['char_length'].hist()\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"d28a801b4057518f87e20c46a77f9dc8757ab944","_cell_guid":"5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"},"source":"Most of the text length are within 500 characters, with some up to 5,000 characters long.","cell_type":"markdown"},{"metadata":{"_uuid":"e1e0ecc1df6989b0ee73874f273f0dbbfc4c9d5e","_cell_guid":"f16a287f-27d4-4afa-82a1-dd441c2fd36c"},"source":"Next, let's examine the correlations among the target variables.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"fab22b3f850c80a10d665ae43ee09b5107a79887","_cell_guid":"64164a2c-770f-469d-9020-e91714a9b2a8","collapsed":true},"source":"data = train_df[cols_target]","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"scrolled":true,"_uuid":"58968a44d8fdb3b93ac57f1ca20f81ffb71d164f","_cell_guid":"7fc7803b-a7f1-414d-84fa-d1d2201c8bb7","_kg_hide-output":false},"source":"colormap = plt.cm.magma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"4f9da9651b4e007088b19a06165d0727fb4f4e07","_cell_guid":"095fd031-32a0-400b-9357-af0c8f50dd6b"},"source":"Indeed, it looks like the various labels are correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult.","cell_type":"markdown"},{"metadata":{"_uuid":"b677d6a32b7b72e08ba3b46bce572a223db964de","_cell_guid":"caa00f5d-7e26-48cb-92b3-acc1f1de0aca"},"source":"What about the character length & distribution of the comment text in the test data?","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"36e4d00a5afc15e6b04ffa1e79421396d051f614","_cell_guid":"0993d06b-495e-428a-933a-6ffec6bdcef3","collapsed":true},"source":"test_df['char_length'] = test_df['comment_text'].str.len()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"1cc05875b88e54b5a1079eafc088207536dea2f3","_cell_guid":"828b9990-d78e-46c7-b011-1c66e6e6be79"},"source":"plt.figure(figsize=(20,5))\nplt.hist(test_df['char_length'])\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"e429fb37db620abf3de274ccdd274c4f1517c5fe","_cell_guid":"495c87cf-6764-4d87-bb32-0e48cd799e63"},"source":"Looks like there are several very long comments in the test data. Let's see what they are.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b7403f24f1632e296c2d46190b46f82fdb926b4b","_cell_guid":"5398026f-ac8d-43ce-bc55-e78ad89294eb","_kg_hide-output":true},"source":"test_df[test_df['char_length']>5000]","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"edf30fd8b2192e4e67a277b5d76c20b6e82d6283","_cell_guid":"77f3d54a-efd3-4251-b74b-3eea98edaed0"},"source":"Let's truncate char length in test_df to 5,000 characters and see if the distribution would be similar to train_df.","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"6696494aacc851e303a8d724428688e2a2ac7751","_cell_guid":"5102345d-3d98-4759-876c-a76eb687a82c","collapsed":true},"source":"test_comment = test_df['comment_text'].apply(lambda x: x[:5000])\nchar_length = test_comment.str.len()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"deba592d146256b223dc32143ad5f2be269e8279","_cell_guid":"600871e5-ceae-4689-b1b3-b5298558ddea"},"source":"plt.figure()\nplt.hist(char_length)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"db69eb685cd1be44de2224c399cbdeabb5ceaa06","_cell_guid":"b2bbf246-f098-425a-99e3-2eb2126b975c"},"source":"Now, the shape of character length distribution looks similar to the train data. I guess the train data were clipped to 5,000 characters to facilitate the folks who did the labelling of comment categories.","cell_type":"markdown"},{"metadata":{"_uuid":"d88d9cea99dbd77e81a5b3c4b9309df88b04550b","_cell_guid":"fdf9d2f6-d248-452f-8f94-562755e3a3f3"},"source":"## Clean up the comment text","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b42586552d4cdc79793b0de8e630f863f2b2c456","_cell_guid":"24392feb-0adc-4e27-bd20-41ed8cadce37","collapsed":true},"source":"def clean_text(text):\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    return text","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true,"_uuid":"e67944653b23b6267fdb0634c03a5b8702ae6d26","_cell_guid":"ec11fdb5-22a8-4889-9a8d-c83d4179a0cf","_kg_hide-output":false},"source":"# clean the comment_text in train_df\ncleaned_train_comment = []\nfor i in range(0,len(train_df)):\n    cleaned_comment = clean_text(train_df['comment_text'][i])\n    cleaned_train_comment.append(cleaned_comment)\ntrain_df['comment_text'] = pd.Series(cleaned_train_comment).astype(str)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"3ed1fff4601eb381b0c9f2da0dddef0453b248d3","_cell_guid":"5292f0f4-cdaf-4e9e-88f4-28efc3aa224d","collapsed":true},"source":"# clean the comment_text in test_df\ncleaned_test_comment = []\nfor i in range(0,len(test_df)):\n    cleaned_comment = clean_text(test_df['comment_text'][i])\n    cleaned_test_comment.append(cleaned_comment)\ntest_df['comment_text'] = pd.Series(cleaned_test_comment).astype(str)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"cae81f6b1d9bb475fc486d5fbb81981025cc3672","_cell_guid":"f5abe72a-13a7-41f6-ae8e-1c34dca97110"},"source":"## Define X from entire train & test data for use in tokenization by Vectorizer","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b15dba3906f67e764af0a627e46d7b7e486888c5","_cell_guid":"30bf94de-36aa-4e8d-8d12-64172d8dc446","collapsed":true},"source":"train_df = train_df.drop('char_length',axis=1)","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b","collapsed":true},"source":"X = train_df.comment_text\ntest_X = test_df.comment_text","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"6b1e91df6163a437c5f55e9c4de88dc11a89e5ba","_cell_guid":"083686b8-483a-4fbd-8584-cb9da8357c57"},"source":"print(X.shape, test_X.shape)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351","_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67"},"source":"## Vectorize the data","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb"},"source":"# import and instantiate CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=20000,min_df=2)\nvect","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c"},"source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_dtm = vect.fit_transform(X)\n# examine the document-term matrix created from X_train\nX_dtm","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4"},"source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_X_dtm = vect.transform(test_X)\n# examine the document-term matrix from X_test\ntest_X_dtm","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"98a540b42db71f1639d47f31d2a4ea851aa1b9e5","_cell_guid":"c84eb9f4-4fa5-418c-883c-9d9316092db0"},"source":"## Solving a multi-label classification problem\nOne way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n* _**Binary Relevance.**_ This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n* _**Classifier Chains.**_ In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n* _**Label Powerset.**_ This method transforms the problem into a multi-class problem  where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a six-factorial or 720-class problem!","cell_type":"markdown"},{"metadata":{"_uuid":"389245398dce9573dfa0f7c2facd2849d2174f1f","_cell_guid":"7abd771a-b0f3-47bc-b4ff-13a78aff48e7"},"source":"## Binary Relevance - build a multi-label classifier using Logistic Regression","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"e7e5707b19c35c8371a0cff7351bc8aadc33acd1","_cell_guid":"e30be87f-e0a6-4fd7-bff2-b3c37737cbfe"},"source":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlogreg = LogisticRegression(C=6.0,random_state=123)\n\n# create submission file\nsubmission_binary = pd.read_csv('../input/sample_submission.csv')\n\nfor label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm, y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n    # compute the predicted probabilities for X_test_dtm\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_binary[label] = test_y_prob","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"ee6d942b861235107e0d94174a8d21c2bad0e9ea","_cell_guid":"5d0970eb-bc44-4fad-91b7-e2a23c2f986d"},"source":"### Create submission file","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"b89e22016a7b4f282940dc23253cbf343c1fbf83","_cell_guid":"a2816552-c314-4584-ad49-8464e80b1b29"},"source":"submission_binary.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"75d9571cc77eb1805d897af6ca0d86fd28405c6d","_cell_guid":"fb1bef7d-586e-437d-a1d3-b0868e7ac321","collapsed":true},"source":"# generate submission file\nsubmission_binary.to_csv('submission_binary.csv',index=False)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"18644d484121f3bab7a06207bd7fa739b15feff5","_cell_guid":"be8c8e90-f798-46d8-b77c-cd0668292646"},"source":"#### Binary Relevance with Logistic Regression classifier scored 0.062 on the public leaderboard.","cell_type":"markdown"},{"metadata":{"_uuid":"0393cd387f508609c0d068c68fb7dbb0be659383","_cell_guid":"5018c7ea-dd27-4d4c-b9ad-ef2140c773e5"},"source":"## Classifier Chains - build a multi-label classifier using Logistic Regression","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"f1b4fea94c83661d7bbad5c6bc7a5994643128e2","_cell_guid":"b2172222-42b8-4f0a-8d20-160d52af6f62","collapsed":true},"source":"# create submission file\nsubmission_chains = pd.read_csv('../input/sample_submission.csv')\n\n# create a function to add features\ndef add_feature(X, feature_to_add):\n    '''\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    '''\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"3cdc6de563643c7a86f0c54ecff2f720695fe81d","_cell_guid":"20c3ff4a-8925-4c78-a8f1-74f080f0b890"},"source":"for label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm,y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n    # make predictions from test_X\n    test_y = logreg.predict(test_X_dtm)\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_chains[label] = test_y_prob\n    # chain current label to X_dtm\n    X_dtm = add_feature(X_dtm, y)\n    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n    # chain current label predictions to test_X_dtm\n    test_X_dtm = add_feature(test_X_dtm, test_y)\n    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"a0be9c78f3facd8215c0d83c446dcef61af3fc43","_cell_guid":"d917c9c5-5140-4b08-86b3-64e0b566bc1b"},"source":"### Create submission file","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"f1d85021f64b145d28a55f65e0d3e806a6c91625","_cell_guid":"4b7db16c-ca27-4fa8-933c-369711f60ea3"},"source":"submission_chains.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"_uuid":"da35f9fd53f310366d0b760eb7573ae1b4e122c4","_cell_guid":"3e1dfebe-ecb4-4a0e-958c-56a2d27e6a7d","collapsed":true},"source":"# generate submission file\nsubmission_chains.to_csv('submission_chains.csv', index=False)","cell_type":"code","outputs":[]},{"metadata":{"_uuid":"cbbce014f73f54e9bf3e88e096ad66b89b88fcf4","_cell_guid":"7a012382-ad72-4c76-b53c-1f756544e23e"},"source":"### That's all for now. Would like to work on the last problem transformation method Label Powerset next, but right now, I can't think of how I could generate the prediction probability numbers in the format required for submission.\n### Tips and comments are most welcomed & appreciated.\n### Please upvote if you find it useful. Happy Holidays!","cell_type":"markdown"},{"execution_count":null,"metadata":{"_uuid":"21a21a9ff3c174753c8d48495e058a1782bf7a76","_cell_guid":"2b370cff-d22e-4393-94d5-98b4883a32d8","collapsed":true},"source":"","cell_type":"code","outputs":[]}]}