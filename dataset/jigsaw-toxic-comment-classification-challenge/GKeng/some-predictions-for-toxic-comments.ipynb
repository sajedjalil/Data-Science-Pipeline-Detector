{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"name":"python","mimetype":"text/x-python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"### Welcome to my 2nd kernel for Toxic Comments Classification Challenge\n\nHere I will test and benchmark some algorithm used in Natural Language Processing. Namely :\n1. Logistic Regression\n2. Naive Bayes Algorithm\n3. Long Short Term Memory Neural networks\n\nMy approach will be \" KIS\" : Keep It Simple, as long as I can. I will make no assumption when testing algorithm in the beginning and then analyze the results before new test\n\n![](http://www.elpoderdelasideas.com/wp-content/uploads/google-jigsaw-2016.png)"},{"metadata":{"_cell_guid":"de3c58cd-2257-4e2f-a906-0da85bc23f68","_uuid":"8e236cbebab43e8d6f1dd868afb385b57f6dc93f"},"cell_type":"code","outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#NLP tools\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\n# plot tools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\npath=\"../input/\"\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":101},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"train = pd.read_csv(path+\"train.csv\")\ntest = pd.read_csv(path+\"test.csv\")","execution_count":84},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"replacement_patterns = [\n (r'won\\'t', 'will not'),\n (r'can\\'t', 'cannot'),\n (r'i\\'m', 'i am'),\n (r'ain\\'t', 'is not'),\n (r'(\\w+)\\'ll', '\\g<1> will'),\n (r'(\\w+)n\\'t', '\\g<1> not'),\n (r'(\\w+)\\'ve', '\\g<1> have'),\n (r'(\\w+)\\'s', '\\g<1> is'),\n (r'(\\w+)\\'re', '\\g<1> are'),\n (r'(\\w+)\\'d', '\\g<1> would')\n]\nclass RegexpReplacer(object):\n    def __init__(self, patterns=replacement_patterns):\n         self.patterns = [(re.compile(regex), repl) for (regex, repl) in\n         patterns]\n     \n    def replace(self, text):\n        s = text\n        for (pattern, repl) in self.patterns:\n             s = re.sub(pattern, repl, s)\n        return s","execution_count":85},{"metadata":{},"cell_type":"code","outputs":[],"source":"from nltk.stem import WordNetLemmatizer\nlemmer = WordNetLemmatizer()\nstopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.tokenize import TweetTokenizer\n#from replacers import RegexpReplacer\nreplacer = RegexpReplacer()\ntokenizer=TweetTokenizer()\n\n\ndef comment_process(comment):\n        comment=tokenizer.tokenize(replacer.replace(comment))\n        comment= [word for word in comment if ( word.lower() not in stopwords \n                              and word.lower() not in list(string.punctuation) )]\n        comment=[lemmer.lemmatize(word, 'v') for word in comment]\n        comment.extend(list(comment))\n        comment=\" \".join(comment)\n        return comment\n    \n\ncleaned_train=train.comment_text.apply(comment_process)\n#cleaned_test=test.comment_text.apply(comment_process)\n\n\n","execution_count":86},{"metadata":{"collapsed":true},"cell_type":"code","outputs":[],"source":"tf = TfidfVectorizer( strip_accents='unicode',analyzer='word', max_features= 50000, ngram_range=(4,4),\n            use_idf=True,smooth_idf=True,sublinear_tf=True,\n            stop_words = 'english')","execution_count":87},{"metadata":{},"cell_type":"code","outputs":[],"source":"cols=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ny=train[cols]\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(cleaned_train, y, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)\n\nxtraintf=tf.fit_transform(xtrain)\nxvalidtf=tf.fit_transform(xvalid)\n\n#xtest=tf.transform(cleaned_test)\n\n\n\n\n","execution_count":88},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{},"cell_type":"code","outputs":[],"source":"prd_valid = np.zeros((xvalidtf.shape[0],yvalid.shape[1]))\nprd_train = np.zeros((xtraintf.shape[0],ytrain.shape[1]))\ntrain_loss = []\nvalid_loss = []\nbnb = LogisticRegression(penalty='l2')\nfor i,col in enumerate(cols):\n    print('Building {} model for column:{''}'.format(i,col)) \n    bnb.fit(xtraintf,ytrain[col])\n    prd_valid[:,i] = bnb.predict_proba(xvalidtf)[:,1]\n    prd_train[:,i] = bnb.predict_proba(xtraintf)[:,1]\n    train_loss_class=log_loss(ytrain[col],prd_train[:,i])\n    valid_loss_class=log_loss(yvalid[col],prd_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))","execution_count":89},{"metadata":{},"cell_type":"markdown","source":".143 mean column-wise log loss... No so bad for this simple model.\n## Let's try Naive Bayes Algorithm and compare"},{"metadata":{},"cell_type":"code","outputs":[],"source":"prd_valid = np.zeros((xvalidtf.shape[0],yvalid.shape[1]))\nprd_train = np.zeros((xtraintf.shape[0],ytrain.shape[1]))\ntrain_loss = []\nvalid_loss = []\nbnb = BernoulliNB()\nfor i,col in enumerate(cols):\n    print('Building {} model for column:{''}'.format(i,col)) \n    bnb.fit(xtraintf,ytrain[col])\n    prd_valid[:,i] = bnb.predict_proba(xvalidtf)[:,1]\n    prd_train[:,i] = bnb.predict_proba(xtraintf)[:,1]\n    train_loss_class=log_loss(ytrain[col],prd_train[:,i])\n    valid_loss_class=log_loss(yvalid[col],prd_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))","execution_count":90},{"metadata":{},"cell_type":"markdown","source":"10.23 mean column-wise log loss! Much worse than logistic regression. \n### My guess:\nMy guess is that the Naive assumption of the Naive Bayes Algorithm is not valid. This assumption is that the features ( the words) are independent with each other. And obviously it's not valid here. But it would be \"less unvalid\" if the algorithm 2 or 3 words ( or more) instead of just one.  \nThis picture illustrates the concept of n_grams. Instead of considering only one word we can consider every pair of words or every tree words. \n![](https://i.stack.imgur.com/8ARA1.png)\n\nOf course it will change the TF-IDF scores. The score will now be calculated for 2grams, 3grams... Ngrams."},{"metadata":{},"cell_type":"code","outputs":[],"source":"def test_model(model,xtraintf, xvalidtf, ytrain, yvalid ):    \n    prd_valid = np.zeros((xvalidtf.shape[0],yvalid.shape[1]))\n    prd_train = np.zeros((xtraintf.shape[0],ytrain.shape[1]))\n    train_loss = []\n    valid_loss = []\n    \n    if model==\"lr\":\n        model= LogisticRegression(penalty=\"l2\")\n    if model==\"nb\":\n        model=BernoulliNB()\n    for i,col in enumerate(cols):\n        model.fit(xtraintf,ytrain[col])\n        \n        prd_valid[:,i] = model.predict_proba(xvalidtf)[:,1]\n        prd_train[:,i] = model.predict_proba(xtraintf)[:,1]\n        \n        train_loss_class=log_loss(ytrain[col],prd_train[:,i])\n        valid_loss_class=log_loss(yvalid[col],prd_valid[:,i])\n        \n        train_loss.append(train_loss_class)\n        valid_loss.append(valid_loss_class)\n    return(np.mean(train_loss), np.mean(valid_loss))\n\n\n    ","execution_count":91},{"metadata":{},"cell_type":"code","outputs":[],"source":"train_lr, valid_lr=[],[]\ntrain_nb, valid_nb=[],[]\nngram_list=[x for x in range (1,6)]\n\nfor ngram in ngram_list:\n    tf = TfidfVectorizer( strip_accents='unicode',analyzer='word', \n                         max_features= 50000, ngram_range=(ngram,ngram),\n            use_idf=True,smooth_idf=True,sublinear_tf=True,\n            stop_words = 'english')\n    xtraintf=tf.fit_transform(xtrain)\n    xvalidtf=tf.fit_transform(xvalid)\n    print(\"testing logistic regression with \"+ str(ngram)+\"grams\")\n    score_lr=test_model('lr', xtraintf, xvalidtf, ytrain, yvalid )\n    train_lr.append(score_lr[0])\n    valid_lr.append(score_lr[1])\n    \n    print(\"testing naive bayes with \"+ str(ngram)+\"grams\")\n    score_nb=test_model('nb', xtraintf, xvalidtf, ytrain, yvalid )\n    train_nb.append(score_nb[0])\n    valid_nb.append(score_nb[1])\n    \n\n","execution_count":93},{"metadata":{},"cell_type":"code","outputs":[],"source":"plt.figure(figsize=(16,12))\nplt.suptitle(\"Ngrams comparison\",fontsize=20)\n\nplt.subplot2grid((2,1),(0,0))\nplt.title(\"Logistic Regression\")\nplt.plot(ngram_list, train_lr,'xkcd:crimson', label='train', linewidth=3 )\nplt.plot(ngram_list, valid_lr, 'xkcd:azure',label='validation', linewidth=3)\nplt.legend(fontsize=14)\nplt.ylabel('mean column-wise log loss', fontsize=20)\nplt.xlabel('Ngrams', fontsize=20)\n\nplt.subplot2grid((2,1),(1,0))\nplt.title(\"Naive Bayes\")\nplt.plot(ngram_list, train_nb, 'xkcd:crimson',label='train', linewidth=3 )\nplt.plot(ngram_list, valid_nb,'xkcd:azure',label='validation', linewidth=3)\nplt.legend(fontsize=14)\nplt.ylabel('mean column-wise log loss', fontsize=20)\nplt.xlabel('Ngrams', fontsize=20)\n\n","execution_count":104},{"metadata":{},"cell_type":"markdown","source":"## As we can see, the 2 models react differently to Ngrams size\n\n* The Naive Bayes Algorithm seems to give better results with bigger Ngrams. It validate smy intuition about the non-valid independency hypothesis of this algorithm. Again, the bigger the Ngrams, the less \"unvalid\" this hypothesis is.\n* Logistic regression's validation score does not seem to depend on the ngram size\n\n* The increasing training loss show that bigger ngrams makes the model underfit. Indeed, bigger ngrams makes the model less complex. The bigger the ngrams, the lesser ngrams the model will learn from ==> the lesser complex our model will be.\n\n## Next step :\n* keep on analyzing these results\n* trying long short term memory neural network.\nI know I could try SVM or Decision trees before LSTM because LSTM  are much more complex. But I have more knowledge on LSTM and I can't wait to try them ahah !"}],"nbformat_minor":1,"nbformat":4}