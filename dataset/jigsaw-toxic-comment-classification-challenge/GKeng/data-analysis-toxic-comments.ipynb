{"cells":[{"metadata":{"_uuid":"d12eac05072aec737ba8b76829c58b984da06d54","_cell_guid":"88628b4f-47d7-4884-98a6-b1e8cecc2326"},"source":"# It can be easy to hurt people when one get anonymous on social media or forums. \n**Jigsaw**, formerly Google Ideas, tries to protect people's against harassment on the internet. They aim at doing this with the use of technologies. That's why they launch this Kaggle competition. They want us to find the best algorithm to detect and classify toxic comments.\n\nThe** goal **of this Notebook is :\n* to draw insight from the dataset provided by Jigsaw \n* and then benchmarking the most commonly used algorithm for Natural Language Processing\n\nI have chosen this competition because I am interested in Natural Language Processing for its application to Social analytics and  Finance. I am amazed by what deep learning can do in computer vision. I want to test it on texts by myself. \n\nThese are the algorithm I will benchmark :\n* **Naive Bayes**\n* **SVM**\n* **Tree Based Method**\n* **Long Short Term Memory Neural Networks.**\n\nMy goal is not really to win the competition but I want to gain a better intuition about these algorithm.\n\n### This is my first kernel, any constructive comments or critics is welcomed =)\n\nLet's go !\n\n![ ](http://www.elpoderdelasideas.com/wp-content/uploads/google-jigsaw-2016.png)","cell_type":"markdown"},{"metadata":{"_uuid":"40bdc788b7e645fe6cb7084b2764635c23297a92","_cell_guid":"814afceb-a759-4dd3-a67d-f9c975bd6d82"},"source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\nfrom sklearn.metrics import log_loss\n\n\n#NLP tools\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nstopwords = nltk.corpus.stopwords.words('english')\n\n\n#Plot and image tools\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom matplotlib import gridspec\nimport seaborn as sns\nsns.set_style(\"dark\")\n\ntrain_data_path='../input/jigsaw-toxic-comment-classification-challenge/train.csv'\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"d09a06d3929d2316e436ac1dbd2fd01e7bb09179","collapsed":true,"_cell_guid":"a2219f63-3040-4dd2-b241-28be003b522a"},"source":"#Loading the Data\ntrain = pd.read_csv(train_data_path)\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"714bb44f07706b8aefca6c306141b812ec74fa22","_cell_guid":"387bc239-380e-432a-bbaa-33abe33d7343"},"source":"# We have got 6 categories of undesirable comments :\n1. Toxic\n2. Severe Toxic\n3. Obscene\n4. Threat\n5. Insult\n6 Identity hate","cell_type":"markdown"},{"metadata":{"_uuid":"a38bca9ba253a33e776de1d29ff0fc4cc4bc657c","_cell_guid":"a8613c82-f8cd-4e54-8161-c9a1627d54b3"},"source":"#a quick look at our training dataset\ntrain.head()","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"a5b9c01e28b7cd6b08d72fd9133ebb6e5451db33","_cell_guid":"556d14fb-1296-47f0-8ff0-a45a48d1b85c"},"source":"# the size of our training dataset\ntrain.shape","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"b6f3998936534522670e45e1370d6dd1293b1661","_cell_guid":"12df754e-9534-4945-845c-a416352b18a8"},"source":"### We create a new cell for \"clean\" comments. Comments that correspond to none of the 6 categories.","cell_type":"markdown"},{"metadata":{"_uuid":"313d6af650b4f48d00524581e36796c01803ae05","_cell_guid":"5d45d4b9-e0b2-4f44-a0b5-d8b0a942ab31"},"source":"rowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)\ntrain['clean'].sum()\n","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"05873a8434cb3b2bb522d88ba7144249a5e6780d","_cell_guid":"c7acdd81-7b6d-4e32-bc3a-3703e71ecd0f"},"source":"colors_list = [\"brownish green\", \"pine green\", \"ugly purple\",\n               \"blood\", \"deep blue\", \"brown\", \"azure\"]\n\npalette= sns.xkcd_palette(colors_list)\n\nx=train.iloc[:,2:].sum()\n\nplt.figure(figsize=(9,6))\nax= sns.barplot(x.index, x.values,palette=palette)\nplt.title(\"Class\")\nplt.ylabel('Occurrences', fontsize=12)\nplt.xlabel('Type ')\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 10, label, \n            ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"129b5b35019e8b7c4b7ed0e2ffe1d11b43fadc29","_cell_guid":"2d6be1cf-1c8b-4a5b-8b1b-4c7dd4ae889b"},"source":"[](http://)####  We have more than 86k clean comments... We have a very unbalanced dataset. Even non-clean comments are not equally reparted. This is an issue not to forget when we will train our learning algorithms. This also means that if we predict 'clean' for each comment, our result will not be so bad in term of accuracy.","cell_type":"markdown"},{"metadata":{"_uuid":"f26dc0c244741c020df6d26438ded674131dc40b","collapsed":true,"_cell_guid":"b81f2fec-a2b5-4b4d-858c-2b758be9a7a7"},"source":"# Just a list that contains all the text data. For me not to load the whole dataset everytime\ncomment_text_list = train.apply(lambda row : nltk.word_tokenize( row['comment_text']),axis=1)","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"8d96e0b9da1e7818d9df81f23481bcd578631cf8","_cell_guid":"ec54b9f7-b27b-4143-a6c5-f8d66cc03393"},"source":"### I thought about aggressive comments in french forums. And I remarked that we could detect some aggressive comments without even read them. Comments with many capital letters and many punctuation symbols like \"?????\" or \"!!!!!!\" are very likely to be toxic. Let's tag them ","cell_type":"markdown"},{"metadata":{"_uuid":"2e641d653e4f9a31de8b5d319987e8a85c631b31","collapsed":true,"_cell_guid":"2d4c1f18-2fb3-4b7d-8c3c-66a07489b107"},"source":"#An odd comment contains a high rate of punctuation symbols or capital letters\nrate_punctuation=0.7\nrate_capital=0.7\ndef odd_comment(comment):\n    punctuation_count=0\n    capital_letter_count=0\n    total_letter_count=0\n    for token in comment:\n        if token in list(string.punctuation):\n            punctuation_count+=1\n        capital_letter_count+=sum(1 for c in token if c.isupper())\n        total_letter_count+=len(token)\n    return((punctuation_count/len(comment))>=rate_punctuation or \n           (capital_letter_count/total_letter_count)>rate_capital)\n\nodd=comment_text_list.apply(odd_comment)\n","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"bb6d6c558c8aed0b7088a280ec694d4e25dffc25","_cell_guid":"9256fb2c-0de6-4ffd-a991-5d829bacc075"},"source":"### I was right :) More than 65% of my so called odd comments are not clean. It seems to be an interesting feature to add to the dataset. I will train a model for these specific odd comments that cannot be treated the same way as the normal ones. I will have to try other capital and punctuation rates though.","cell_type":"markdown"},{"metadata":{"_uuid":"1f44f5d7fcc0963d07160360c4712b3e0331b67f","_cell_guid":"335845ed-b701-465c-a586-38da706d9f86"},"source":"odd_ones=odd[odd==True]\n#list(ponctuation_polluted.index)\nodd_comments=train.loc[list(odd_ones.index)]\nodd_comments[odd_comments.clean==False].count()/len(odd_comments)","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ea1ecc8e5048f592580ce877dfc8c229ffaa1e39","_cell_guid":"512efbbd-6ec5-47f8-80da-37719ae84d04"},"source":"colors_list = [\"brownish green\", \"pine green\", \"ugly purple\",\n               \"blood\", \"deep blue\", \"brown\", \"azure\"]\n\npalette= sns.xkcd_palette(colors_list)\n\nx=odd_comments.iloc[:,2:].sum()\n\n\nplt.figure(figsize=(9,6))\nax= sns.barplot(x.index, x.values, alpha=0.8, palette=palette)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"cdbe02b6923551faf60ec77743dbaa27e2d8baea","_cell_guid":"17c99dac-e612-47ac-96eb-7b4835a63dde"},"source":"# quick check for empty comments\nempty_com=train[train.comment_text==\"\"]\nempty_com","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"c6041d8fbedadc5deff3c79a02ffbe6ade91e5fa","_cell_guid":"cfa69f5c-e0df-440c-8298-659afdef6e2b"},"source":"#quick check for duplicated comments\nduplicate=train.comment_text.duplicated()\nduplicate[duplicate==True]","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"adaa6b272e098f5ac3db8e94920041a6d6212fb8","collapsed":true,"_cell_guid":"a9bc451f-c04e-41fd-a6e0-8154ded5d73d"},"source":"#Just storing each categories of non clean comments in specific arrays\ntoxic=train[train.toxic==1]['comment_text'].values\nsevere_toxic=train[train.severe_toxic==1]['comment_text'].values\nobscene=train[train.obscene==1]['comment_text'].values\nthreat=train[train.threat==1]['comment_text'].values\ninsult=train[train.insult==1]['comment_text'].values\nidentity_hate=train[train.identity_hate==1]['comment_text'].values","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"4406d77f5b103c121c7548d1dc7be5b48c2dcc7d","_cell_guid":"5dab29db-d465-483b-bf24-ce18074412a7"},"source":"Wordclouds are a quick way to see which words are dominant in a text. We can even draw specific forms of cloud. Let's see which word are the most represented in toxic labeled comments","cell_type":"markdown"},{"metadata":{"_uuid":"89429e23ed1e6a0eb8302524567b30c8c56ae1eb","_cell_guid":"a4740976-2857-403b-ac8b-82e9042c73d3"},"source":"mask=np.array(Image.open('../input/imagetc/twitter.png'))\nmask=mask[:,:,1]\nfrom wordcloud import WordCloud, STOPWORDS\n# The wordcloud of Toxic Comments\nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=500,mask=mask \n             , stopwords=stopwords, max_font_size= 60)\nwc.generate(\" \".join(toxic))\nplt.title(\"Twitter Wordlcloud Toxic Comments\", fontsize=30)\n# plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\nplt.imshow(wc.recolor( colormap= 'Set1' , random_state=1), alpha=0.98)\nplt.axis('off')\nplt.savefig('twitter_wc.png')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"b687822f2599666fbe0db073ac4bebe44c41e103","_cell_guid":"48ed6015-604a-4c0d-a3ab-bc95ce96dbe0"},"source":"### Till now we have worked with raw text. Let's do some text processing now\n### We are gonna Lemmatize every word we have.\nLemmatization uses context and part of speech to determine the inflected form of the word and applies different normalization rules for each part of speech to get the root word (lemma). \nFor instance : Lemmatization(drank)=drink\n\n### We also replace some apostrophe words like \"won't\" ==> will not, \"can't\"==> cannot..\n### Let's begin with the toxic category","cell_type":"markdown"},{"metadata":{"_uuid":"0fec1f42266ffec4312b0571d03c5851744dcb51","collapsed":true,"_cell_guid":"8c5cbc21-d3ba-4a66-b497-79ce2bd61b13"},"source":"\n\nreplacement_patterns = [\n (r'won\\'t', 'will not'),\n (r'can\\'t', 'cannot'),\n (r'i\\'m', 'i am'),\n (r'ain\\'t', 'is not'),\n (r'(\\w+)\\'ll', '\\g<1> will'),\n (r'(\\w+)n\\'t', '\\g<1> not'),\n (r'(\\w+)\\'ve', '\\g<1> have'),\n (r'(\\w+)\\'s', '\\g<1> is'),\n (r'(\\w+)\\'re', '\\g<1> are'),\n (r'(\\w+)\\'d', '\\g<1> would')\n]\nclass RegexpReplacer(object):\n    def __init__(self, patterns=replacement_patterns):\n         self.patterns = [(re.compile(regex), repl) for (regex, repl) in\n         patterns]\n     \n    def replace(self, text):\n        s = text\n        for (pattern, repl) in self.patterns:\n             s = re.sub(pattern, repl, s)\n        return s","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"51a31891c13a5d6e6272fb58a5a589d3e41547e2","collapsed":true,"_cell_guid":"119beee5-d677-43a3-a92b-e8ce5baffd8d"},"source":"from nltk.stem import WordNetLemmatizer\nlemmer = WordNetLemmatizer()\nstopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.tokenize import TweetTokenizer\n#from replacers import RegexpReplacer\nreplacer = RegexpReplacer()\ntokenizer=TweetTokenizer()\n\ndef comment_process(category):\n    category_processed=[]\n    for i in range(category.shape[0]):\n        comment_list=tokenizer.tokenize(replacer.replace(category[i]))\n        comment_list_cleaned= [word for word in comment_list if ( word.lower() not in stopwords \n                              and word.lower() not in list(string.punctuation) )]\n        comment_list_lemmed=[lemmer.lemmatize(word, 'v') for word in comment_list_cleaned]\n        category_processed.extend(list(comment_list_lemmed))\n    return category_processed\n","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"150c8c4f4dee43d043cc1f2e788a50bcfc384ccf","collapsed":true,"_cell_guid":"98dd213a-ca1c-452a-9a1c-b0c7ccd20d06"},"source":"toxic1=comment_process(toxic)","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"44cf602e329096c6db9072367e1b899edb2047f1","_cell_guid":"f13fa99c-cb0d-4fec-a6de-e44cf73407be"},"source":"fd=nltk.FreqDist(word for word in toxic1)\n\nx=[fd.most_common(150)[i][0] for i in range(99)]\ny=[fd.most_common(150)[i][1] for i in range(99)]\n#palette=sns.color_palette(\"PuBuGn_d\",100)\npalette= sns.light_palette(\"crimson\",100,reverse=True)\nplt.figure(figsize=(45,15))\nax= sns.barplot(x, y, alpha=0.8,palette=palette)\nplt.title(\"Occurences per word in Toxic comments 1\")\nplt.ylabel('Occurrences', fontsize=30)\nplt.xlabel(' Word ', fontsize=30)\n#adding the text labels\nrects = ax.patches\nlabels = y\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n    plt.xticks(rotation=60, fontsize=18)\n#plt.savefig('Toxic_Word_count1.png')    \nplt.show()\n\n\n","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"afdfa659d0f063a34c9210af0d47abcbe0293e91","_cell_guid":"99acdc88-0462-41ab-b94d-cdd36b369592"},"source":" You can open that plot in a new window to see it bigger ;)\n### We see that some words have lots of occurences whereas we  did not even see them on the first wordcloud. Using the word cloud library on unprocessed data may be misleading. Let's do draw the same barplot without lemmatization and stemming","cell_type":"markdown"},{"metadata":{"_uuid":"9b65a0d3ca78d6e8bc29032f71920dc670939c9e","collapsed":true,"_cell_guid":"6ce4c2dd-b600-402e-ab7a-fb581e55f428"},"source":"toxic2=[]\nfor i in range(toxic.shape[0]):\n    comment_list=nltk.word_tokenize(toxic[i])\n    comment_list_cleaned= [word for word in comment_list if ( word.lower() not in stopwords \n                          and word.lower() not in list(string.punctuation) )]\n    toxic2.extend(list(set(comment_list_cleaned)))","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"67a1d2fd810da782826dc1cf18edc5a75a64c4a5","_cell_guid":"c1ec8724-18d2-4e13-9923-a667dd6da066"},"source":"fd2=nltk.FreqDist(word for word in toxic2)\nx=[fd2.most_common(100)[i][0] for i in range(99)]\ny=[fd2.most_common(100)[i][1] for i in range(99)]\npalette= sns.light_palette(\"crimson\",100,reverse=True)\nplt.figure(figsize=(45,15))\nax= sns.barplot(x, y, alpha=0.8,palette=palette)\nplt.title(\"Occurence per word in Toxic comments\")\nplt.ylabel('Occurrences', fontsize=30)\nplt.xlabel(' Word ', fontsize=30)\n\nrects = ax.patches\nlabels = y\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n    plt.xticks(rotation=60, fontsize=18)\n#plt.savefig('Toxic_Word_count2.png')    \nplt.show()\n#Check the bigger picture in imagetc files ","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"2090a5baf38f8401966731477d8a655e1449d730","_cell_guid":"85e440bf-e9a9-4f79-b57d-a3357db5bf49"},"source":"### Without Lemmatization, the roots of words can be separated in different bars. So we won't use this processed data","cell_type":"markdown"},{"metadata":{"_uuid":"8f45af49a3d43a4bd23efe29818b79edaa40bb34","_cell_guid":"5e30db49-e84f-4fba-9b34-cba16cf4a4cc"},"source":"def wordcloud_plot(category, name) : \n    plt.figure(figsize=(20,15))\n    wc = WordCloud(background_color=\"black\", max_words=500,mask=mask, min_font_size=6 \n                 , stopwords=stopwords, max_font_size= 60)\n    wc.generate(\" \".join(category))\n    plt.title(\"Twitter Wordlcloud \" + name +  \" Comments\", fontsize=30)\n    # plt.imshow(wc.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n    plt.imshow(wc.recolor( colormap= 'Set1' , random_state=21), alpha=0.98)\n    plt.axis('off')\n    plt.savefig(name+'_wc.png')\n    return(True)\n\nwordcloud_plot(toxic1,'Toxic')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"59a6efe00f01dfb581f9b393f22fd3eac3be23e9","_cell_guid":"6e45a2f6-664b-4b3f-a514-0b3d7d3fb68a"},"source":"### This final Word Cloud is very different to the first one. And this one is more coherent compared to the barplot. Now we are going to draw the wordclouds for each categories.","cell_type":"markdown"},{"metadata":{"_uuid":"df3b12f5f08956b4d6caa8817ef88838f6de520d","collapsed":true,"_cell_guid":"1abfc23d-19c6-456a-b2e1-52f8c9a4e8c1"},"source":"severe_toxic1=comment_process(severe_toxic)\nobscene1=comment_process(obscene)\nthreat1=comment_process(threat)\ninsult1=comment_process(insult)\nidentity_hate1=comment_process(identity_hate)","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"f117aa9046f5b75971751b859805704c0382f81c","_cell_guid":"8b3dcb01-1ecb-48fe-870e-d385b1ba2231"},"source":"wordcloud_plot(severe_toxic1,'Severe_toxic')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"57ecdb5fe285a9f78b446ba623f37e604d178179","_cell_guid":"ed3fb0f2-0533-4526-a5c0-43973df068a0"},"source":"wordcloud_plot(obscene1,'Obscene')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"59168d7a05d78b505c9041a357c7de0660c3a3a0","_cell_guid":"7eae5ecf-d680-4d86-a39c-a829820aa7e9"},"source":"wordcloud_plot(threat1,'Threat')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"scrolled":true,"_uuid":"a86f8741944764e56636f6caa588735797d94b08","_cell_guid":"f892ade6-565b-4dd4-9927-3b7f73291f84"},"source":"wordcloud_plot(insult1,'Insult')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"e7ada6844fe59487440199bf16b0a944c1313c47","_cell_guid":"f6f95fd2-72c8-45e9-ab78-f69de9d246da"},"source":"wordcloud_plot(identity_hate1,'Identity_Hate')","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"fe572ead0083ef33d26c83c2e1d4007848f764df","_cell_guid":"fbd3c70a-2cdf-4108-873c-ef10b7920e94"},"source":"### From all those plots we conclude that :\nSome categories share the same vocabulary in term of richness and word frequency, like Insult and Toxic categories. And some others, like Identity Hate category, use specific words more frequently. I let you check that for yourself. \nWe could try to find the words that are specific to each category to create a new feature but the TF-IDF trick will do that for us.","cell_type":"markdown"},{"metadata":{"_uuid":"9f0578fd1c08e8236cf94730edc7dab7bbb0e79d","_cell_guid":"fd39970c-f654-478c-b10d-ff55d34b1a6a"},"source":"### Let's Tackle the problem of length\n","cell_type":"markdown"},{"metadata":{"_uuid":"311c2374d87d94bd67f2d195accbf1f0df03b6f6","_cell_guid":"1ee38155-d166-4643-be52-a0adcfa9f64c"},"source":"toxic3=train[train.clean==False]['comment_text']\ntoxic3\ntoxic_count_word=toxic3.apply(lambda x: len(str(x).split()))\nprint(toxic_count_word.describe(),'\\n \\n',toxic_count_word.quantile(q=0.9))","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"64936da61297eceb19135777a445348c51c84bf5","_cell_guid":"27204cc1-95a1-43b9-be39-aaa2b796164a"},"source":"clean3=train[train.clean==True]['comment_text']\nclean_count_word=clean3.apply(lambda x: len(str(x).split()))\nprint(clean_count_word.describe(), clean_count_word.quantile(q=0.9))","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"44ced4ba6cb0ef39a3ea30290fc0077a65f8db6b","_cell_guid":"1800a588-a4be-4659-8071-9a75e31c25c0"},"source":"It seems like we will not use the number of word in comments as a features. The variance for clean and non clean comment is too big and their distribution overlap each other. :(","cell_type":"markdown"},{"metadata":{"_uuid":"372cfe14ae234084c08d38a5e5c4f0cd33290115","_cell_guid":"184939db-973f-4e9c-9380-0bcf8d8c1e99"},"source":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"321b33bc72d7c29fff646837bd92dd2caa3110cc","_cell_guid":"23da3adb-e245-4538-9287-2b67c22b663a"},"source":"One could think that the comments with more label may be longer. But it's not even the case...","cell_type":"markdown"},{"metadata":{"_uuid":"74d06402cbc4bf211d201a0190e247e02997fa1c","_cell_guid":"ebf6bb16-0e3f-450d-8dcf-c2a979e644a0"},"source":"Another Kaggler, Jagan, proposed to identify spam comments by their percentage of unique word. Great idea !","cell_type":"markdown"},{"metadata":{"_uuid":"8175a3957b540541e239df773ad3bb6ca0ac519c","_cell_guid":"4e0dbaeb-45cd-4b52-af53-ec45bb55a13a"},"source":"#Credit Jagan\n\ntrain['count_unique_word']=train[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\ntrain['count_word']=train[\"comment_text\"].apply(lambda x: len(str(x).split()))\ntrain['word_unique_percent']=train['count_unique_word']*100/train['count_word']\nspammers=train[train['word_unique_percent']<30]\n\nplt.figure(figsize=(16,12))\nplt.suptitle(\"What's so unique ?\",fontsize=20)\n#gridspec.GridSpec(2,1)\n\n\nplt.subplot2grid((2,1),(0,0))\nplt.title(\"Percentage of unique words of total words in comment\")\n#sns.boxplot(x='clean', y='word_unique_percent', data=train_feats)\nax=sns.kdeplot(train[train.clean == 0].word_unique_percent, label=\"Bad\",shade=True,color='r')\nax=sns.kdeplot(train[train.clean == 1].word_unique_percent, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('Percent unique words', fontsize=12)\n\nx=spammers.iloc[:,2:9].sum()\nplt.subplot2grid((2,1),(1,0),colspan=2)\nplt.title(\"Count of comments with low(<30%) unique words\",fontsize=15)\nax=sns.barplot(x=x.index, y=x.values,color='crimson')\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.xlabel('Threat class', fontsize=12)\nplt.ylabel('# of comments', fontsize=12)\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"dc85189983799eb073cf8ec2cc14effea16bd121","collapsed":true,"_cell_guid":"be55da17-3dd0-453b-9c3d-9e16e1876330"},"source":"# And as espected, spams are more toxic\nWe will train another specific model for spams.","cell_type":"markdown"},{"metadata":{"_uuid":"84c7e782569be958b85958bd7c919ce05d96c9b4","_cell_guid":"cd366778-799f-48a7-8386-05a39a471ae0"},"source":"### Bag of Words and TF-IDF\nAs I wrote earlier, TF-IDF is a trick that detects the most discriminatory words. A sentence with many discriminatory word is likely to be more important. TF-IDF calculate a score of each and every word and then look for an average score normalized by the words that are important\nThis [Article](http://datameetsmedia.com/bag-of-words-tf-idf-explained/) gives a good explanation of Bag of Words and the TF-IDF trick","cell_type":"markdown"},{"metadata":{"_uuid":"72fd2b61ca042d3d0c6278ef2e0058ecce27c8da","_cell_guid":"beeecde3-5675-4c84-8e4b-286a1ce28d41"},"source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf = TfidfVectorizer( strip_accents='unicode',analyzer='word',ngram_range=(1,1),\n            use_idf=True,smooth_idf=True,sublinear_tf=True,\n            stop_words = 'english')\n\"\"\"tf = TfidfVectorizer(min_df=100,  max_features=100000, \n            strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n            use_idf=1,smooth_idf=1,sublinear_tf=1)\"\"\"","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"496987cf6b8289df5534f84e68ff81351e0c6c7b","collapsed":true,"_cell_guid":"2fa17d44-9b65-45bb-aeac-76bbef88c513"},"source":"def category_to_tfidf(category):\n    tvec_weights = tf.fit_transform(category)\n    weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n    weights_df = pd.DataFrame({'term': tf.get_feature_names(), 'weight': weights})\n    return(weights_df.sort_values(by='weight', ascending=False).head(10))\n\n","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"89f132e0ac372ab75f876a62171c7081bcf7076c","_cell_guid":"e6906d18-f10c-484b-aa36-5612c1609327"},"source":"toxic_idf=category_to_tfidf(toxic1)\nsevere_toxic_idf=category_to_tfidf(severe_toxic1)\nthreat_idf=category_to_tfidf(threat1)\ninsult_idf=category_to_tfidf(insult1)\nobscene_idf=category_to_tfidf(obscene1)\nidentity_hate_idf=category_to_tfidf(identity_hate1)\ntoxic_idf\n","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"14e398b70207d485853ccb2459c9cd1b72bc45ab","_cell_guid":"ae49ae65-a994-419a-b136-451e1b8d90e3"},"source":"color_list = [\"xkcd:brownish green\", \"xkcd:pine green\", \"xkcd:ugly purple\",\n               \"xkcd:blood\", \"xkcd:deep blue\", \"xkcd:brown\"]\nplt.figure(figsize=(20,22))\nplt.suptitle(\"TF-IDF ranking \",fontsize=20)\ngridspec.GridSpec(3,2)\nplt.subplot2grid((3,2),(0,0))\nsns.barplot(toxic_idf.term,\n            toxic_idf.weight,color=color_list[0])\nplt.title(\"Toxic\",fontsize=15)\nplt.xlabel('Term', fontsize=12)\nplt.ylabel('Score', fontsize=12)\n\nplt.subplot2grid((3,2),(0,1))\nsns.barplot(severe_toxic_idf.term,\n            severe_toxic_idf.weight,color=color_list[1])\nplt.title(\" Severe toxic\",fontsize=15)\nplt.xlabel('Term', fontsize=12)\nplt.ylabel('Score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(1,0))\nsns.barplot(obscene_idf.term,\n            obscene_idf.weight,color=color_list[2])\nplt.title(\"Obscene\",fontsize=15)\nplt.xlabel('Term', fontsize=12)\nplt.ylabel('Score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(1,1))\nsns.barplot(threat_idf.term,\n            threat_idf.weight,color=color_list[3])\nplt.title(\"Threat\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('Score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,0))\nsns.barplot(insult_idf.term,\n            insult_idf.weight,color=color_list[4])\nplt.title(\"Insult\",fontsize=15)\nplt.xlabel('Term', fontsize=12)\nplt.ylabel('Score', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,1))\nsns.barplot(identity_hate_idf.term,\n            identity_hate_idf.weight,color=color_list[5])\nplt.title(\"Identity hate\",fontsize=15)\nplt.xlabel('Term', fontsize=12)\nplt.ylabel('Score', fontsize=12)\n\n\nplt.show()","execution_count":null,"outputs":[],"cell_type":"code"},{"metadata":{"_uuid":"ab2ebcd6473135fcdbfb9f11a11bb9abb2ad3289","_cell_guid":"a4d62583-938b-4976-8820-a5fd6d65fea5"},"source":"### I think this TF-IDF ranking is perfectible. We still have words like \" Wikipedia\" or \"like\"  that do not give much informtion about the specificity of each category.","cell_type":"markdown"},{"metadata":{"_uuid":"b9a06dd377b7913d0a8ddaa56b6648f607c75a70","_cell_guid":"7933144c-dd22-49f7-b562-65a4ab22d8dc"},"source":"# This is the end of my first EDA. Let's summarize what we have learned from that :\n* The dataset is unbalanced with far more clean comments than toxic ones\n*  There are odd comments and spams that need to be traited separately.\n* We have made a good use of wordcloud to see some words that are specific to each category\n* We have used the TF-IDF quite succesfuly but it seems to be perfectible\n\n### Now let's  test some algorithms on this data.\n\nYou can read the rest of this notebook but I will write another one for the prediction part. \nYou can find it [there](https://www.kaggle.com/gakngm/some-predictions-for-toxic-comments/)\n\n# Enjoy =)\n","cell_type":"markdown"},{"metadata":{"_uuid":"874b0a8b358badb73048056ffc30ea79c891b625","collapsed":true,"_cell_guid":"8ce4eb4e-54a5-4f24-8513-f54404a19569"},"source":"","execution_count":null,"outputs":[],"cell_type":"code"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","name":"python","pygments_lexer":"ipython3","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3}}},"nbformat_minor":1,"nbformat":4}