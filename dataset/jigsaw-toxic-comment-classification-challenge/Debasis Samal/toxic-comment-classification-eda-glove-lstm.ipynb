{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction:\nBeing anonymous over the internet can sometimes make people say nasty things that they normally would not in real life. Let's filter out the hate from our platforms one comment at a time.\n\n## Objective:\nTo do exploratory data analysis for toxic comment classification and build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. We will be using a dataset of comments from Wikipedia’s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.\n\nDisclaimer: the dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n## Data Overview:\nThe dataset here is from wiki corpus dataset which was rated by human raters for toxicity. The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015.\n\nDifferent platforms/sites can have different standards for their toxic screening process. Hence the comments are tagged in the following five categories\n\ntoxic\nsevere_toxic\nobscene\nthreat\ninsult\nidentity_hate","metadata":{}},{"cell_type":"code","source":"#import required packages\nimport pandas as pd\nimport numpy as np\n\nimport re\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom sklearn.utils import resample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the training dataset\ntrain_data_or = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain_data_or.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the testing dataset\ntest_data = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping the unecessary features\ntrain_data_or.drop(['id'],axis=1,inplace=True)\n# test_data.drop(['id'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the shape of the dataset\ntrain_data_or.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=train_data_or.iloc[:,2:].sum()\n\n#marking comments without any tags as \"clean\"\nrowsums=train_data_or.iloc[:,2:].sum(axis=1)\ntrain_data_or['clean']=(rowsums==0)\n\n#count number of clean entries\ntrain_data_or['clean'].sum()\nprint(\"Total comments = \",len(train_data_or))\nprint(\"Total clean comments = \",train_data_or['clean'].sum())\nprint(\"Total tags =\",x.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*  We can see the the class is imbalance and the dataset is huge. So inorder to faster training and balance the dataset, we have downsampled the majority class.","metadata":{}},{"cell_type":"code","source":"df_majority = train_data_or[train_data_or.clean==True]\ndf_minority = train_data_or[train_data_or.clean==False]\n\ndf_majority_downsampled = resample(df_majority,\n                                  replace=False,\n                                  n_samples=10000,\n                                  random_state=123)\n\ntrain_data = pd.concat([df_majority_downsampled,df_minority])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the distribution of tag over the dataset\ntrain_data.iloc[:,1:].sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for the null or missing value in the dataset\ntrain_data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_comment_type(row):\n     for c in train_data.iloc[:,1:]:\n        if row[c]==1:\n            return c\n\ncomment_type = train_data.apply(get_comment_type, axis=1)\ntrain_data['comment_type'] = comment_type","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='comment_type', data=train_data)\nplt.xlabel(\"Type of comment\")\nplt.ylabel(\"Number of comment\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.fillna(value=np.nan)\ntrain_data = train_data.fillna(value='safe')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\n\nax = (train_data.comment_type.value_counts()/len(train_data)*100).sort_index().plot(kind=\"bar\", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n    \nplt.xlabel(\"Type of comment\")\nplt.ylabel(\"Number of comment\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#displaying the example from each type of toxic comment\n\nprint('\\033[1m' + 'TOXIC' + '\\033[0m')\nprint(train_data['comment_text'][train_data['toxic']==1].iloc[1])\nprint(' ')\n\nprint('******************************************************************')\n\nprint('\\033[1m' + 'SEVERE TOXIC' + '\\033[0m')\nprint(train_data['comment_text'][train_data['severe_toxic']==1].iloc[1])\nprint(' ')\n\nprint('******************************************************************')\n\nprint('\\033[1m' + 'OBSCENE' + '\\033[0m')\nprint(train_data['comment_text'][train_data['obscene']==1].iloc[1])\nprint(' ')\n\nprint('******************************************************************')\n\nprint('\\033[1m' + 'THREAT' + '\\033[0m')\nprint(train_data['comment_text'][train_data['threat']==1].iloc[0])\nprint(' ')\n\nprint('******************************************************************')\n\nprint('\\033[1m' + 'INSULT' + '\\033[0m')\nprint(train_data['comment_text'][train_data['insult']==1].iloc[1])\nprint(' ')\n\nprint('******************************************************************')\n\nprint('\\033[1m' + 'IDENTITY HATE' + '\\033[0m')\nprint(train_data['comment_text'][train_data['identity_hate']==1].iloc[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordclouds - Frequent words:\nNow, let's take a look at words that are associated with these classes.\n\nChart Desc: The visuals here are word clouds (ie) more frequent words appear bigger","metadata":{}},{"cell_type":"code","source":"stopword=set(STOPWORDS)\n\n\n#wordcloud for clean comments\nsubset=train_data[train_data.clean==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Clean Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#wordcloud for clean comments\nsubset=train_data[train_data.toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=4000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.subplot(221)\nplt.axis(\"off\")\nplt.title(\"Words frequented in Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244), alpha=0.98)\n\n#Severely toxic comments\nplt.subplot(222)\nsubset=train_data[train_data.severe_toxic==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Severe Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Reds' , random_state=244), alpha=0.98)\n\n#Threat comments\nplt.subplot(223)\nsubset=train_data[train_data.threat==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Threatening Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'summer' , random_state=2534), alpha=0.98)\n\n#insult\nplt.subplot(224)\nsubset=train_data[train_data.insult==1]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.axis(\"off\")\nplt.title(\"Words frequented in insult Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'Paired_r' , random_state=244), alpha=0.98)\n\nplt.suptitle(\"WordCloud of Toxic Words\", fontsize=30)   \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"APPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining a function to clean the data\ndef clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', text) # clean url\n    text = re.sub(r'#(\\w+)', '', text)   # clean hashes\n    text = re.sub(r'@(\\w+)', '', text)   # clean @\n    text = re.sub(r'<[^>]+>', '', text)  # clean tags\n    text = re.sub(r'\\d+', '', text)      # clean digits\n    text = re.sub(r'[,!@\\'\\\"?\\.$%_&#*+-:;]', '', text)   # clean punctuation\n    text = [APPO[word] if word in APPO else word for word in text.split()]  #\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying the cleantext function to bothe train and test data\ntrain_data['comment_text'] = train_data['comment_text'].apply(clean_text)\ntest_data['comment_text'] = test_data['comment_text'].apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train_data.iloc[:,0]\ntrain_y = train_data.iloc[:,1:7]\n\ntrain_y = np.array(train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the data into train and validation\ntrain_x, val_x, train_y, val_y = train_test_split(train_x,train_y, test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = 100000, oov_token='<oov>')\ntokenizer.fit_on_texts(train_data.comment_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traning_sequences = tokenizer.texts_to_sequences(train_x)\nmaxlen = max([len(x) for x in np.array(traning_sequences)])\ntraining_padded = pad_sequences(traning_sequences, maxlen = maxlen,\n                                padding = 'pre',\n                                truncating='pre')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_sequences = tokenizer.texts_to_sequences(val_x)\nvalidation_padded = pad_sequences(validation_sequences, maxlen = maxlen,\n                                padding = 'pre',\n                                truncating='pre')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.word_index) + 1\nvocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings_index = {}\nglovefile = open('../input/glove6b200d/glove.6B.200d.txt','r',encoding='utf-8')\nfor line in tqdm(glovefile):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n#     coefs.shape\n    embeddings_index[word] = coefs\nglovefile.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(tokenizer.word_index)+1, 200))\nfor words, index in tqdm(tokenizer.word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building a bidirectional LSTM model\nmodel = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = 200,weights = [embedding_matrix],input_length = maxlen),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\n    tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(6, activation='sigmoid')])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Summary of the model\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compiling and running the model\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.fit(training_padded,train_y, epochs = 2, validation_data=(validation_padded, val_y), batch_size = 32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting on the validation data\npredicted = model.predict(validation_padded)\nlabels = (predicted > 0.5).astype(np.int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_toxictype(i,act_or_pred):\n    \n    l=[]\n    count=0\n    \n    if sum(act_or_pred[:10][i])==0:\n        l.append('safe comment')\n        \n    else:\n        for j in range(len(act_or_pred[:10][i])):\n            if act_or_pred[:10][i][j]==1 and count == 0:\n                l.append('toxic')\n            elif act_or_pred[:10][i][j]==1 and count == 1:\n                l.append('severe_toxic')\n            elif act_or_pred[:10][i][j]==1 and count == 2:\n                l.append('obscene')\n            elif act_or_pred[:10][i][j]==1 and count == 3:\n                l.append('threat')\n            elif act_or_pred[:10][i][j]==1 and count == 4:\n                l.append('insult')\n            elif act_or_pred[:10][i][j]==1 and count == 5:\n                l.append('identity_hate')\n            \n            count=count+1\n            \n    return l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Displaying the actual  predicted type of comment","metadata":{}},{"cell_type":"code","source":"for i,j in enumerate(val_x.index[:10]):\n    print('')\n    print('\\033[1m' + 'Predicted type of comment:' + '\\033[0m', get_toxictype(i,labels))\n    print('\\033[1m' + 'Actual type of comment:' + '\\033[0m', get_toxictype(i,val_y))\n    print('\\033[1m' + 'Comment: ' + '\\033[0m',train_data_or.iloc[j,0])\n    print('')\n    print('****************************************************************************')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predicting the result on the Test dataset","metadata":{}},{"cell_type":"code","source":"testing_sequences = tokenizer.texts_to_sequences(test_data.comment_text)\ntest_padded = pad_sequences(testing_sequences, maxlen = maxlen,\n                                padding = 'pre',\n                                truncating='pre')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted = model.predict(test_padded, batch_size = 200)\npredict = np.hstack((test_data.id[:, np.newaxis], predicted))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = pd.DataFrame(predict, columns = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\nsubm.to_csv('subm.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}