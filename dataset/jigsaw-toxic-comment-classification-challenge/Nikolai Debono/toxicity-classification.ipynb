{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Warning\nBecause of the nature of the project, some of the contents of this project might be unpleasant."},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nOnline communication has increased drastically in the last two decades. Nowadays, people use this medium to chat with family and friends, re-kindle old friendships, plan events, discuss their favourite topics, and much more. Despite all of this positive potentional, digital communication is unfortunately plagued with widespread hate speech and discrimination. In fact, a [Pew Research Center study](https://www.pewresearch.org/internet/2021/01/13/the-state-of-online-harassment/) found that 41% of US adults have experienced some form of online toxicity. Moreover, a majority of them also experienced severe harassment.\n\n![https://www.pewresearch.org/internet/wp-content/uploads/sites/9/2021/01/PI_2021.01.13_online-harrasment_0-02a-1.png?w=640](https://www.pewresearch.org/internet/wp-content/uploads/sites/9/2021/01/PI_2021.01.13_online-harrasment_0-02a-1.png?w=640)\n\nFor the purposes of that study, researchers defined six forms of toxic behaviour: \n* Offensive name-calling\n* Purposeful embarrassment\n* Stalking\n* Physical threats\n* Harassment over a sustained period of time\n* Sexual harassment\n\nMany online platforms struggle to moderate conversations due to the large commitment required. Effective moderation requires a dedicated team of people whose sole job is to find and remove toxic users and their comments. This leads many online platforms to either limit, or completely shut down user communication.\n\n# Task Description\nThe aim of this project is to create a tool that can help in the fight against online harassment. We will create a model that can not only decide whether a comment is toxic, but also describe the kind of toxicity that is present. This would allow online platforms to selectively remove the particular kind of toxicity they do not like. For example, an online community could be fine with mild swearing, but be completely opposed to obscenities.\n\nIn this project we will be working with a large dataset of Wikipedia comments that have been carefully labeled by human annotators. The possible labels for every comment are: toxic, severe_toxic, obscene, threat, insult, and identity hate. The task is to create a model that can correctly assign 0 or more of these labels to a piece of text. This means that essentially we are dealing with a multi-label binary classifcation problem, such that the output of the model is a vector of binary classification probabilities. "},{"metadata":{},"cell_type":"markdown","source":"# Install and Import Libraries\nWe start by intalling the transformers library. Huggingface provides this very useful library of pre-trained transformer models that can be fine-tuned on any task."},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom typing import Tuple, List\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom transformers import BertTokenizer, BertTokenizerFast, BertModel, AdamW, get_cosine_schedule_with_warmup, BertPreTrainedModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nimport tqdm\n\nimport torch.backends.cudnn as cudnn\n \nimport math\n\nimport time\nimport random\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip -o ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n!unzip -o ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n!unzip -o ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n!unzip -o ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"CONFIG = {\n    'train_path': './train.csv',\n    'test_path': './test.csv',\n    'test_label_path': './test_labels.csv',\n    'sample_path': './sample_submission.csv',\n    'epochs': 2,\n    'batch_size': 16,\n    'lr': 2e-5,\n    'model_code': 'bert-base-uncased'\n}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read CSV Files\nIn this section we start by reading the dataset csv files, and cleaning the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(CONFIG['train_path'])\ntest_csv = pd.read_csv(CONFIG['test_path'])\ntest_label = pd.read_csv(CONFIG['test_label_path'])\ntest_full_csv = pd.merge(test_csv, test_label, on=\"id\")\ntest_full_csv = test_full_csv.loc[test_full_csv[\"toxic\"] != -1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We visualise the training csv below to get an idea of its structure. This will help us understand how to extract the comments and process them into the form expected by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisation\nIn this section we will be visualising the training dataset in various ways. \n\n**Please beware that you could find these examples unpleasant. If so, please skip this section.**\n\n## Class Word Clouds"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS\n\ndef generate_wordcloud_from_df(df, column):\n    texts = []\n    comments = train_csv.loc[df[column] == 1]['comment_text']\n    for c in comments:\n        texts.append(c)\n    commonWord = ' '.join(texts)\n    return WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=1500,\n                      height=1500\n                     ).generate(commonWord)\n\ntrain_toxic = generate_wordcloud_from_df(train_csv, 'toxic')\ntrain_sev_toxic = generate_wordcloud_from_df(train_csv, 'severe_toxic')\ntrain_obscene = generate_wordcloud_from_df(train_csv, 'obscene')\ntrain_threat = generate_wordcloud_from_df(train_csv, 'threat')\ntrain_insult = generate_wordcloud_from_df(train_csv, 'insult')\ntrain_id_hate = generate_wordcloud_from_df(train_csv, 'identity_hate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10), gridspec_kw = {'wspace':0.01, 'hspace':0.1})\naxes[0][0].imshow(train_toxic)\naxes[0][0].axis('off')\naxes[0][0].set_title('Toxic Word Cloud')\naxes[0][0].set_aspect('equal')\n\naxes[0][1].imshow(train_sev_toxic)\naxes[0][1].axis('off')\naxes[0][1].set_title('Severely Toxic Word Cloud')\naxes[0][1].set_aspect('equal')\n\naxes[0][2].imshow(train_obscene)\naxes[0][2].axis('off')\naxes[0][2].set_title('Obscene Word Cloud')\naxes[0][2].set_aspect('equal')\n\naxes[1][0].imshow(train_threat)\naxes[1][0].axis('off')\naxes[1][0].set_title('Threat Word Cloud')\naxes[1][0].set_aspect('equal')\n\naxes[1][1].imshow(train_insult)\naxes[1][1].axis('off')\naxes[1][1].set_title('Insult Word Cloud')\naxes[1][1].set_aspect('equal')\n\naxes[1][2].imshow(train_id_hate)\naxes[1][2].axis('off')\naxes[1][2].set_title('Identity Hate Word Cloud')\naxes[1][2].set_aspect('equal')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Text Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"def output_random_df_samples(df):\n    length = len(df)\n    rnd_indx = np.random.randint(0, length, 10)\n    texts = df.loc[rnd_indx]['comment_text']\n    for i, text in enumerate(texts.array):\n        print(i+1, text, '\\n')\n    \noutput_random_df_samples(train_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class Counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig , axes = plt.subplots(2,3,figsize = (10,10), constrained_layout = True)\nsns.countplot(ax=axes[0,0],x='toxic',data=train_csv )\nsns.countplot(ax=axes[0,1],x='severe_toxic',data=train_csv)\nsns.countplot(ax=axes[0,2],x='obscene',data=train_csv)\nsns.countplot(ax = axes[1,0],x='threat',data=train_csv)\nsns.countplot(ax=axes[1,1],x='insult',data=train_csv)\nsns.countplot(ax=axes[1,2],x='identity_hate',data=train_csv)\nplt.suptitle('No Of Classes Of Each Category')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset\nIn this section we present the dataset class we use during training. To reduce overhead costs whilst loading data, we take advantage of abundant system RAM and pre-process the datasets during initialisation. We use a method packaged with the transformers library to tokenize the full dataset using WordPiece, add special tokens ([CLS], [SEP], etc..), and convert the resulting tokens to vocabulary IDs. We also truncate the input text to a maximum length of 120 tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicDataset(Dataset):\n    \n    def __init__(self, tokenizer, dataframe):\n        self.tokenizer = tokenizer\n        self.pad_idx = tokenizer.pad_token_id\n        self.df = dataframe \n        \n        self.X = []\n        self.Y = []\n        self.X_attn_msk = []\n\n        tokenized_comments = tokenizer(list(self.df['comment_text'].array), \n                                       add_special_tokens=True,\n                                       return_token_type_ids=False, \n                                       padding=False, \n                                       truncation=True, \n                                       verbose=True,\n                                       max_length=120)\n        self.X = tokenized_comments['input_ids']\n        self.X_attn_msk = tokenized_comments['attention_mask']\n        self.Y = self.df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values.tolist()\n        \n    \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        return  torch.LongTensor(self.X[index]), torch.FloatTensor(self.Y[index]),torch.LongTensor(self.X_attn_msk[index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To take advantage of batch processing during training, dataset pre-processing normally includes adding [PAD] tokens to create fixed size matrices. Crucially, this means that our transformer will be forced to process maximum length batches, wasting precious resources. This is because the self-attention mechanism within most transformers scales quadratically to the batch input length, both in processing and memory.\n\nWe overcome this issue by creating a custom data collating function that pads each individual batch dynamically. In this function we dynamically pad the input sequences with 0s ([PAD] token ID), and generate the respective attention masking sequences. Attention masking is crucial in this scenario, as we want the self-attention mechanism to exclude the [PAD] tokens from its calculations. We thus significantly cut down on training time by transferring, storing, and processing smaller batches. Another common method used in practice is to group similar sized input sequences for even greater efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"def collate_batch(batch):\n    pad_val = 0\n    x, y, attn_mask = zip(*batch)\n    x = pad_sequence(x, padding_value=pad_val, batch_first=True)\n    attn_mask = pad_sequence(attn_mask, padding_value=pad_val, batch_first=True)\n    y = torch.stack(y)\n    return x, y, attn_mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Traditionally, NLP methods like tfidf, SVD, LDA etc.. perform significantly worse when the data is not clean. However, text normalisation might not necessarily be beneficial in this project since we are dealing with online comments. These are texts that have a high variance owing to their informality. We might encounter spelling mistakes, forgotten words, and unknown symbols. This means that it is possible for us to clean the training data using a specific rule-based technique, train the model on that curated and heavily altered textual distribution, then encounter a completely different distribution in test set. For example, we could have a rule to map 'byo' and 'oby' to 'boy' in the training set, resulting in a clean training dataset. However, we could then encounter 'bo' in the test set, a drastic change from the expected 'boy', resulting in worse generalisation performance. \n\nPopular ways to overcome this in deep learning are through random textual data augmentation that makes the model more robust to outliers. This includes 'simple' methods like word order swapping, word insertions (random, wordnet synonym), and intentional grammatical mistakes like random character switching in a word. Other methods include using contextual word embeddings to insert and replace words, using translation models to generate new training data through back-translation, as well as adversarial training techniques.  \n\nIn the future we plan on experimenting with some of the aforementioned techniques to see if they help us improve model performance."},{"metadata":{},"cell_type":"markdown","source":"# Model\nIn this project we fine-tune a pre-trained BERT model to classify the toxicity in comments. Specifically, we follow the work of [Devlin et al.](https://arxiv.org/abs/1810.04805) and concatenate the [CLS] token hidden vectors of the last 4 transformer layers. We then propagate the resulting vector through a two layer classification MLP."},{"metadata":{"trusted":true},"cell_type":"code","source":"class ToxicityClassifier(nn.Module):\n    \n    def __init__(self, model_code, num_classes):\n        super().__init__()\n        self.model_code = model_code\n        self.num_classes = num_classes\n        self.transformer = BertModel.from_pretrained(self.model_code)\n        self.output_layer0 = nn.Linear(self.transformer.config.hidden_size*4, self.transformer.config.hidden_size)\n        self.output_layer = nn.Linear(self.transformer.config.hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, labels=None):\n        outputs = self.transformer(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids, output_hidden_states=True)\n\n        outputs = outputs[2][-4:]\n        outhid0 = outputs[0][:,0,:]\n        outhid1 = outputs[1][:,0,:]\n        outhid2 = outputs[2][:,0,:]\n        outhid3 = outputs[3][:,0,:]\n        outputs = None\n        cls_output = torch.hstack((outhid0, outhid1, outhid2, outhid3))\n        \n        cls_output = self.output_layer0(cls_output)\n        cls_output = torch.tanh(cls_output)\n        cls_output = self.output_layer(cls_output)\n        return cls_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experiment Builder\nWe fine-tune the BERT model using an AdamW optimiser, paired with a warm-up cosine LR scheduler for two epochs. For the loss function we use the expectation of the binary cross-entropy loss across every label. To score each prediction, we use the expected ROC-AUC score across labels.\n\n### ROC-AUC Metric\nThe [Receiver Operating Characteristic](https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/#:~:text=The%20Area%20Under%20the%20Curve,the%20positive%20and%20negative%20classes.) curve, or ROC curve, visualises the performance of a binary classifier as its classification threshold is varied. It is a probability curve that plots the true-positive rate (sensitivity) against false-positive rate (1-specificity) at various threshold values. The Area Under the Curve (AUC) is a summary of the ROC curve, scoring the ability of a binary classifier to accurately predict classes. When AUC=1, the model can perfectly classify data, and when AUC=0, it always predicts the wrong class. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExperimentBuilder(nn.Module):\n    def __init__(self, network_model, num_epochs, train_data, val_data,\n                 test_data, weight_decay_coefficient, lr, use_gpu):\n        \"\"\"\n        Initializes an ExperimentBuilder object. Such an object takes care of running training and evaluation of a deep net\n        on a given dataset. It also takes care of saving per epoch models and automatically inferring the best val model\n        to be used for evaluating the test set metrics.\n        :param network_model: A pytorch nn.Module which implements a network architecture.\n        :param experiment_name: The name of the experiment. This is used mainly for keeping track of the experiment and creating and directory structure that will be used to save logs, model parameters and other.\n        :param num_epochs: Total number of epochs to run the experiment\n        :param train_data: An object of the DataProvider type. Contains the training set.\n        :param val_data: An object of the DataProvider type. Contains the val set.\n        :param test_data: An object of the DataProvider type. Contains the test set.\n        :param weight_decay_coefficient: A float indicating the weight decay to use with the adam optimizer.\n        :param use_gpu: A boolean indicating whether to use a GPU or not.\n        \"\"\"\n        super(ExperimentBuilder, self).__init__()\n    \n        self.model = network_model\n        self.criterion = nn.BCEWithLogitsLoss()\n        \n        if torch.cuda.device_count() > 1 and use_gpu:\n            self.device = torch.cuda.current_device()\n            self.model.to(self.device)\n            self.model = nn.DataParallel(module=self.model)\n            print('Use Multi GPU', self.device)\n        elif torch.cuda.device_count() == 1 and use_gpu:\n            self.device =  torch.cuda.current_device()\n            self.model.to(self.device)  # sends the model from the cpu to the gpu\n            print('Use GPU', self.device)\n        else:\n            print(\"use CPU\")\n            self.device = torch.device('cpu')  # sets the device to be CPU\n            print(self.device)\n        \n\n        self.train_data = train_data\n        self.val_data = val_data\n        self.test_data = test_data\n\n        self.num_epochs = num_epochs\n        self.warmup_steps = 10 ** 3\n        self.total_steps = math.ceil((len(train_data)/CONFIG['batch_size'])) * (num_epochs+1)\n        \n        self.optimizer = AdamW(self.parameters(), lr=lr, weight_decay=weight_decay_coefficient, eps=1e-8)\n        self.learning_rate_scheduler = get_cosine_schedule_with_warmup(self.optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.total_steps)\n        \n\n        self.state = dict()\n        self.starting_epoch = 0\n\n    def get_num_parameters(self):\n        total_num_params = 0\n        for param in self.parameters():\n            total_num_params += np.prod(param.shape)\n\n        return total_num_params\n\n    def get_optimiser(self):\n        return self.optimizer  \n    \n    \n    def run_train_iter(self, x, y, attn_mask):\n        \"\"\"\n        Receives the inputs and targets for the model and runs a training iteration. Returns loss and accuracy metrics.\n        :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n        :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n        :return: the loss and accuracy for this batch\n        \"\"\"\n        self.train()\n        x, y = x.to(device=self.device), y.to(device=self.device)  # send data to device as torch tensors\n        attn_mask = attn_mask.to(device=self.device)\n        out = self.model.forward(x, attention_mask=attn_mask)  # forward the data in the model\n\n        loss = self.criterion(out, y)\n\n        self.optimizer.zero_grad()  # set all weight grads from previous training iters to 0\n        loss.backward()  # backpropagate to compute gradients for current iter loss\n        self.optimizer.step()  # update network parameters\n        self.learning_rate_scheduler.step()\n\n        with torch.no_grad():\n            out = torch.sigmoid(out)\n        out = out.cpu().data.numpy()\n\n        return loss.cpu().data.numpy(), out, y.cpu().data.numpy()\n        \n    def run_evaluation_iter(self, x, y, attn_mask):\n        \"\"\"\n        Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n        :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n        :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n        :return: the loss and accuracy for this batch\n        \"\"\"\n        self.eval()  # sets the system to validation mode\n        x, y = x.to(device=self.device), y.to(device=self.device)  # send data to device as torch tensors\n        attn_mask = attn_mask.to(device=self.device)\n        out = self.model.forward(x, attention_mask=attn_mask)  # forward the data in the model\n\n        loss = self.criterion(out, y)\n        \n        with torch.no_grad():\n            out = torch.sigmoid(out)\n        out = out.cpu().data.numpy()\n        \n        return loss.cpu().data.numpy(), out, y.cpu().data.numpy()\n                                                                            \n    def run_experiment(self):\n        \"\"\"\n        Runs experiment train and evaluation iterations, saving the model and best val model and val model accuracy after each epoch\n        :return: The summary current_epoch_losses from starting epoch to total_epochs.\n        \"\"\"\n        print('Starting Training')\n        print('Model Parameters: {}'.format(self.get_num_parameters()))\n        total_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [],\n                        \"val_loss\": []}  # initialize a dict to keep the per-epoch metrics\n        best_test_metrics = {'f1':0, 'acc':0, 'loss':0}\n        best_val_loss = math.inf\n        epochs_since_improv = 0\n        for i, epoch_idx in enumerate(range(self.starting_epoch, self.num_epochs)):\n            epoch_start_time = time.time()\n            current_epoch_losses = {\"train_acc\": [], \"train_loss\": [], \"train_f1\": [], \n                                    \"val_acc\": [], \"val_loss\": [], \"val_f1\": [],\n                                    \"test_acc\": [], \"test_loss\": [], \"test_f1\": []}\n            train_preds = list([])\n            train_labels = list([])\n            self.current_epoch = epoch_idx\n            with tqdm.tqdm(total=len(self.train_data)) as pbar_train:  # create a progress bar for training\n                for idx, (x, y, attn_mask) in enumerate(self.train_data):  # get data batches\n                    loss, preds, labels = self.run_train_iter(x=x, y=y, attn_mask=attn_mask) # take a training iter step\n                    train_preds += preds.tolist()\n                    train_labels += labels.tolist()\n                    current_epoch_losses[\"train_loss\"].append(loss)  # add current iter loss to the train loss list\n                    pbar_train.update(1)\n                    pbar_train.set_description(\"Epoch {} - loss: {:.4f}\".format(self.current_epoch, loss))\n            train_preds = np.array(train_preds)\n            train_labels = np.array(train_labels)\n            print('\\nTrain ROC')\n            for i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n                print(f\"{name} roc_auc {roc_auc_score(train_labels[:, i], train_preds[:, i])}\")\n            avg_train_loss = np.mean(np.array(current_epoch_losses[\"train_loss\"]))  \n            print(\"Train Loss: {:.4f}\".format(avg_train_loss))\n        \n        test_preds = list([])\n        test_labels = list([])\n        with tqdm.tqdm(total=len(self.test_data)) as pbar_test:  # ini a progress bar\n            for (x, y, attn_mask) in self.test_data:  # sample batch\n                loss, preds, labels = self.run_evaluation_iter(x=x, y=y, attn_mask=attn_mask)  # compute loss and accuracy by running an evaluation step\n                test_preds += preds.tolist()\n                test_labels += labels.tolist()\n                current_epoch_losses[\"test_loss\"].append(loss)  # save test loss\n                pbar_test.update(1)  # update progress bar status\n                pbar_test.set_description(\"loss: {:.4f}\".format(loss))  # update progress bar string output\n\n        test_preds = np.array(test_preds)\n        test_labels = np.array(test_labels)\n        print('\\nTest ROC')\n        avg_auc = 0\n        for i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n            auc = roc_auc_score(test_labels[:, i], test_preds[:, i])\n            avg_auc += auc\n            print(f\"{name} roc_auc {auc}\")\n        avg_auc /= 6\n        print('\\nFinal Test AUC:', avg_auc)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Experiment\nIn this section we run the actual experiment. We fine-tune the BERT model using an AdamW optimiser, paired with a warm-up cosine LR scheduler for two epochs. For the loss function we use the expectation of the binary cross-entropy loss across every label."},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 100\nos.environ['PYTHONHASHSEED'] = str(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\ncudnn.deterministic = True\ncudnn.benchmark = False\n\ntokenizer = BertTokenizerFast.from_pretrained(CONFIG['model_code'])\n\ntrain_set = ToxicDataset(tokenizer, train_csv)\ntrain_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, drop_last=False, num_workers=4, collate_fn=collate_batch)\n\ntest_set = ToxicDataset(tokenizer, test_full_csv)\ntest_loader = DataLoader(test_set, batch_size=CONFIG['batch_size']*2, shuffle=True, drop_last=False, num_workers=4, collate_fn=collate_batch)\n\nmodel = ToxicityClassifier(model_code=CONFIG['model_code'], num_classes=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exp = ExperimentBuilder(network_model=model, num_epochs=CONFIG['epochs'], train_data=train_loader, val_data=None,\n                 test_data=test_loader, weight_decay_coefficient=0.0, lr=CONFIG['lr'], use_gpu=True)\nexp.run_experiment()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\nWe obtain an average AUC score of 0.985"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion and Future Work\nIn this project we fine-tuned a pre-trained BERT model to classify the toxicity in online comments. We used the outputs of the last 4 transformer layers as a feature vector for classifcation. Through this method, we achieved an AUC score of 0.985! In the future, we plan on improving the generalisation of our model by making it more robust to the various outliers that are present in online comments."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}