{"metadata":{"language_info":{"version":"3.6.4","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"7b486ed1856a061e75f27c240c22886c565efa63","_cell_guid":"d2b7f058-8cf5-4599-b099-f58a17b27f91"},"source":"## Keras implementation of Yoon Kim's model for sentence classification","cell_type":"markdown"},{"metadata":{"_uuid":"71345317674baa7f0ab5e029cb2a07df9262fe6b","_cell_guid":"c2b62e7b-a49b-4501-b303-ef7328fa208d"},"source":"##### The following is a keras implementation of Yoon Kim's convolutional neural network model for sentence classification from the paper: https://arxiv.org/abs/1408.5882","cell_type":"markdown"},{"metadata":{"_uuid":"673c19d49285841534961478fec2dce4d1394cfd","_cell_guid":"ff691dcb-4d7e-481d-8a63-244571012e9c"},"source":"##### The basic idea is to use a convolutional neural network where different convolutions are used to produce different n-gram-like filters to determine the sentiment of a given text. Using the \"glove\" pre-trained embeddings and manually setting different kernels sizes, the hope is to capture strong single phrases in sentences that results in a particular outcome.","cell_type":"markdown"},{"metadata":{"_uuid":"5d4c457e9946e88f79fe3efd29cf601ebe9987db","collapsed":true,"_cell_guid":"44eb8bb2-979b-4dc9-8f36-0c6a7a74823a","scrolled":true},"source":"import pandas as pd\nimport re\nimport numpy as np\nfrom keras.preprocessing import sequence\nfrom keras.regularizers import l2\nfrom keras.models import Model\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom keras.layers import Dense, GlobalMaxPooling1D, Activation, Dropout, GaussianNoise\nfrom keras.layers import Embedding, Input, BatchNormalization, SpatialDropout1D, Conv1D\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom pandas_summary import DataFrameSummary \nfrom IPython.display import display\nimport itertools\nfrom nltk.corpus import words\n%matplotlib inline\nimport matplotlib.pyplot as plt","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"7394571847983f631cf44419e010532f1e7a525c","_cell_guid":"a37ed914-3787-4bbe-847c-3ba7a3c93b39"},"source":"##### Determine dimension of embedding vector, max size of vocabulary and max length of sentence (crop the rest)","cell_type":"markdown"},{"metadata":{"_uuid":"c21b046383b3dcdcdd8319a3ba4f51896a5ac385","collapsed":true,"_cell_guid":"76e9a099-026b-4632-b0ff-29329df05499"},"source":"# Set parameters\nembed_size   = 50    # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen       = 100   # max number of words in a comment to use ","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"0ccdb2895aa8e190f9f427290912282f6cb6baea","_cell_guid":"cbda94d6-ebdf-46fa-824d-22c23dbbaf36"},"source":"##### Load data...","cell_type":"markdown"},{"metadata":{"_uuid":"dca42dbb8cad2faa1b4527dedaa031c8939aa523","_cell_guid":"f66337db-af9a-4604-b675-f8c229084e85"},"source":"# Load data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"_NaN_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_NaN_\").values","outputs":[],"cell_type":"code","execution_count":1},{"metadata":{"_uuid":"214b0c3803ce23163787c559bf8c103271cacc9b","_cell_guid":"f7e84b2b-0b6b-4d26-96de-93714744992b"},"source":"##### Tokenize sentences, convert to integers and pad sentences < 100 words","cell_type":"markdown"},{"metadata":{"_uuid":"06ec3d6800e124b935178c6b981cfc5e27a3c7ad","collapsed":true,"_cell_guid":"09b529ed-38f0-44db-a97d-ca725d1c1f4f"},"source":"# Pad sentences and convert to integers\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post')\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post')","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"699edb3012216da4bf9754b706da58fc4fd10912","_cell_guid":"901cbce3-4e2d-48a5-80de-48366ced45a4"},"source":"##### Load \"glove\" pre-trained embeddings and construct vocabulary dictionary","cell_type":"markdown"},{"metadata":{"_uuid":"0ba1b991235f978f3648532f2a4a93af2ec0bfd3","collapsed":true,"_cell_guid":"8a8fccae-6501-432f-8db4-2ae4e7847c40"},"source":"# Read the glove word vectors (space delimited strings) into a dictionary from word->vector\ndef get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open('./data/glove.6B.50d.txt'))","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"febf2cd76cf3a6a6895a331c8e900f9df382bbb2","_cell_guid":"ce1655d7-b5dd-4f48-a3ef-e4b02bb5159f"},"source":"##### Create embedding matrix and initialize space for new words not present in \"glove\"","cell_type":"markdown"},{"metadata":{"_uuid":"8d96bbe653615bca5b5e0239e7a0adc337b69085","collapsed":true,"_cell_guid":"c4c3a7b3-6ad3-4cc3-acc4-8aeb18b30856"},"source":"# Create embeddings matrix\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\n\n# Create embedding matrix using our vocabulary\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n\n# Initialize embedding matrix\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\n# Loop through each word and get its embedding vector\nfor word, i in word_index.items():\n    if i >= max_features: \n        continue # Skip words appearing less than the minimum allowed\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix[i] = embedding_vector","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"e9999f863832b0790840cdc5cc1296eca67c41bd","_cell_guid":"71d0d9ae-0024-418d-92aa-8d91f10c78e3"},"source":"##### Set no. of convolution filters and weigh the outcome variable in order to balance.\n- 128 filters are used for each convolution. I.e. with a kernel size of 3, 128 tri grams are constructed each representing a specific feature. With a kernel size of 4, 128 4-grams are constructed and so on..","cell_type":"markdown"},{"metadata":{"_uuid":"53ed470e95c215e0ecc5be9df9f66402fb08726b","collapsed":true,"_cell_guid":"38d9b786-e556-4e92-ba9c-7b47bab9bc58"},"source":"# Initialize parameters\nconv_filters = 128 # No. filters to use for each convolution\nweight_vec = list(np.max(np.sum(y, axis=0))/np.sum(y, axis=0))\nclass_weight = {i: weight_vec[i] for i in range(6)}","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"570facca18c7ca89ef615c47ce562811b620c431","_cell_guid":"910d419d-7767-40fd-af9f-085e7e687b12"},"source":"##### Construct Convolutional Neural Network","cell_type":"markdown"},{"metadata":{"_uuid":"bc4b69a2db4bf1e4262d6aec4e7671d9353f084c","collapsed":true,"_cell_guid":"eaf137d0-229e-4130-a938-e30b7c8f8f66"},"source":"inp = Input(shape=(X_train.shape[1],), dtype='int64')\nemb = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n\n# Specify each convolution layer and their kernel siz i.e. n-grams \nconv1_1 = Conv1D(filters=conv_filters, kernel_size=3)(emb)\nbtch1_1 = BatchNormalization()(conv1_1)\ndrp1_1  = Dropout(0.2)(btch1_1)\nactv1_1 = Activation('relu')(drp1_1)\nglmp1_1 = GlobalMaxPooling1D()(actv1_1)\n\nconv1_2 = Conv1D(filters=conv_filters, kernel_size=4)(emb)\nbtch1_2 = BatchNormalization()(conv1_2)\ndrp1_2  = Dropout(0.2)(btch1_2)\nactv1_2 = Activation('relu')(drp1_2)\nglmp1_2 = GlobalMaxPooling1D()(actv1_2)\n\nconv1_3 = Conv1D(filters=conv_filters, kernel_size=5)(emb)\nbtch1_3 = BatchNormalization()(conv1_3)\ndrp1_3  = Dropout(0.2)(btch1_3)\nactv1_3 = Activation('relu')(drp1_3)\nglmp1_3 = GlobalMaxPooling1D()(actv1_3)\n\nconv1_4 = Conv1D(filters=conv_filters, kernel_size=6)(emb)\nbtch1_4 = BatchNormalization()(conv1_4)\ndrp1_4  = Dropout(0.2)(btch1_4)\nactv1_4 = Activation('relu')(drp1_4)\nglmp1_4 = GlobalMaxPooling1D()(actv1_4)\n\n# Gather all convolution layers\ncnct = concatenate([glmp1_1, glmp1_2, glmp1_3, glmp1_4], axis=1)\ndrp1 = Dropout(0.2)(cnct)\n\ndns1  = Dense(32, activation='relu')(drp1)\nbtch1 = BatchNormalization()(dns1)\ndrp2  = Dropout(0.2)(btch1)\n\nout = Dense(y.shape[1], activation='sigmoid')(drp2)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"1bfa4501e43e8b0784b7bafcac9e2d47a5043d9c","collapsed":true,"_cell_guid":"dd176424-156f-4e7e-8160-b3cae7a1a342","scrolled":true},"source":"# Compile\nmodel = Model(inputs=inp, outputs=out)\nadam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"7d8a9ad0b0b2b5aafb6d697a78380a53e3efe77e","collapsed":true,"_cell_guid":"18960cdb-ab02-4052-8f05-0c794047c8c6","scrolled":true},"source":"# Estimate model\nmodel.fit(X_train, y, validation_split=0.1, epochs=2, batch_size=32, shuffle=True, class_weight=class_weight)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"f744a0429fd2706808016b917f2194ac5bbe6f84","_cell_guid":"6e1d85ed-c594-4134-a5e8-00f046dbb1ab"},"source":"##### Predict and finally submit","cell_type":"markdown"},{"metadata":{"_uuid":"a445e8f05353f391ab56ab5f50248e6417751190","collapsed":true,"_cell_guid":"82c412ed-e24d-46bf-9912-99c255eb1ca9"},"source":"# Predict\npreds = model.predict(X_test)","outputs":[],"cell_type":"code","execution_count":null},{"metadata":{"_uuid":"b6e0af2175dccc76623c108305592ac209780e51","collapsed":true,"_cell_guid":"628ae9f9-5dee-4b74-bdcd-b1e2a16c2fbb"},"source":"# Create submission\nsubmid = pd.DataFrame({'id': test[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = list_classes)], axis=1)\nsubmission.to_csv('conv_glove_simple_sub.csv', index=False)","outputs":[],"cell_type":"code","execution_count":null}],"nbformat_minor":1}