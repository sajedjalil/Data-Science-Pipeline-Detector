{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.6.4","file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python"}},"cells":[{"metadata":{"_cell_guid":"677912b7-2ec7-4a7d-bab4-5568de233e47","_uuid":"03644a330d3da78a3b89fb58b2115e456a5420a1"},"cell_type":"markdown","source":"# Keras cnn + GloVe + Dropout + Early Stopping = [0.048 LB]\n\n#### 2 Jan 2018\n***"},{"metadata":{"_cell_guid":"582fe746-5914-4b94-8b0b-ba603fc604cd","_uuid":"3e03184d38dfa0b4d3f39de3f4bfe54b44e05dc6"},"cell_type":"markdown","source":"## **Summary**"},{"metadata":{"_cell_guid":"67537a49-3915-4151-b01a-ef36b79192d8","_uuid":"8508bac4219f877514c72c0a6599e4533eb3ba44"},"cell_type":"markdown","source":"This model, nicknamed the_Detoxinator, uses pretrained Glove 840B with Adam optimizer and a learning rate of 0.0001. The model implements early stopping using a stratified validation set (25% of training set). Training was done on a GPU (Crestle). If I remember correctly, training did not take more than 30 minutes.\n\nPlease note that this script is written to run locally, not in the Kaggle kernel environment. \n\n\n#### **The CNN architecture is as follows:**"},{"metadata":{"_cell_guid":"f0eca2e0-d55b-4cc2-84cc-51c6c2254ebd","collapsed":true,"_uuid":"4ce391d1c940e7542dea352f3c3a72638e6cd875"},"cell_type":"markdown","source":"model = Sequential()<br>\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=500,trainable=False)<br>\nmodel.add(e)<br>\nmodel.add(Conv1D(128, 3, activation='relu'))<br>\nmodel.add(MaxPooling1D(3))<br>\nmodel.add(Dropout(0.2))<br>\nmodel.add(Conv1D(64, 3, activation='relu'))<br>\nmodel.add(MaxPooling1D(3))<br>\nmodel.add(Dropout(0.2))<br>\nmodel.add(Conv1D(64, 3, activation='relu'))<br>\nmodel.add(Dropout(0.2))<br>\nmodel.add(Flatten())<br>\nmodel.add(Dense(1, activation='sigmoid'))<br>"},{"metadata":{"_cell_guid":"588893d5-e47b-4bde-a835-b171ecc0542e","_uuid":"79a7469c0b3d6daea3fc856a9c9d331930ec205d"},"cell_type":"markdown","source":"![](http://)The public LB score was 0.048. I found that this score could be improved to 0.046 by averaging the predictions of this cnn model with the predictions generated by another logistic regression model that I created. The following kernel explains how to easily do this:\n\nhttps://www.kaggle.com/jhoward/minimal-lstm-nb-svm-baseline-ensemble-lb-0-044\n\nUsing a weighted average (0.6 x cnn_model + 0.4 x logistic_regression_model) further improved the public LB score to 0.045.\n***"},{"metadata":{"_cell_guid":"2c09a73b-225c-4cb3-b6e5-8360cfe27bef","_uuid":"461c683283c62f10be20c61a56dfdadc4e38c392"},"execution_count":null,"outputs":[],"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.optimizers import Adam\nfrom keras.layers import BatchNormalization, Flatten, Conv1D, MaxPooling1D\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')"},{"metadata":{"_cell_guid":"8ff1b6f1-2e72-4eec-954e-017ced59dce5","_uuid":"2baebccd3836f4bda9b03ac3ea85b2fd511e73c8"},"execution_count":null,"outputs":[],"cell_type":"code","source":"# read in the data\n\n#df_train = pd.read_csv('train.csv.zip')\n#df_test = pd.read_csv('test.csv.zip')\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\nprint(df_train.shape)\nprint(df_test.shape)"},{"metadata":{"_cell_guid":"20b9b3aa-110b-4c59-9970-1732e87017fa","collapsed":true,"_uuid":"c4392f8fc9f88c5daef65e66d233b72093de8a9c"},"execution_count":null,"outputs":[],"cell_type":"code","source":"# combine the train and test sets for encoding and padding\n\ntrain_len = len(df_train)\ndf_combined =  pd.concat(objs=[df_train, df_test], axis=0).reset_index(drop=True)\n\nprint(df_combined.shape)"},{"metadata":{"_cell_guid":"0b23211a-fb43-4fe3-a855-86317a7937ce","collapsed":true,"_uuid":"f5b22283efd3faef27316ea3c65867f688427817"},"execution_count":null,"outputs":[],"cell_type":"code","source":"# define text data\ndocs_combined = df_combined['comment_text'].astype(str)\n\n# initialize the tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs_combined)\nvocab_size = len(t.word_index) + 1\n\n# integer encode the text data\nencoded_docs = t.texts_to_sequences(docs_combined)\n\n# pad the vectors to create uniform length\npadded_docs_combined = pad_sequences(encoded_docs, maxlen=500, padding='post')"},{"metadata":{"_cell_guid":"210f64cb-5e3c-4e08-90ad-dbbb23b90df8","collapsed":true,"_uuid":"410ce48a5a51e8ecb2e75bf9ecf9c2473e0b4cdc"},"execution_count":null,"outputs":[],"cell_type":"code","source":"# seperate the train and test sets\n\ndf_train_padded = padded_docs_combined[:train_len]\ndf_test_padded = padded_docs_combined[train_len:]\n\nprint(df_train_padded.shape)\nprint(df_test_padded.shape)"},{"metadata":{"_cell_guid":"27d6f8a3-415d-460c-8f6a-d21ec5219d21","_uuid":"9ff0edb62e63e13339031f4f9b17c9558bb9cf2d"},"cell_type":"markdown","source":"### **Load the GloVe embeddings**"},{"metadata":{"_cell_guid":"8239f3a1-3e27-44b6-8e92-ca8e8a43ecb3","_uuid":"a4182a93db8d378f0b214f62735b2f013e8114e3"},"cell_type":"markdown","source":"The code for processing the Glove embeddings is taken from this excellent blog post:\n\nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n\nI made some minor changes to suit glove840B and to fix a \"could not convert string to float\" error."},{"metadata":{"_cell_guid":"c1ddb595-c6dd-4a10-812e-9528f0489fab","collapsed":true,"_uuid":"3c7fca0da64ff99701ff9d30223e598046c1ce05"},"execution_count":null,"outputs":[],"cell_type":"code","source":"# load the glove840B embedding into memory after downloading and unzippping\n\nembeddings_index = dict()\nf = open('glove.840B.300d.txt')\n\nfor line in f:\n    # Note: use split(' ') instead of split() if you get an error.\n\tvalues = line.split(' ')\n\tword = values[0]\n\tcoefs = np.asarray(values[1:], dtype='float32')\n\tembeddings_index[word] = coefs\nf.close()\n\nprint('Loaded %s word vectors.' % len(embeddings_index))\n\n# create a weight matrix\nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in t.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector"},{"metadata":{"_cell_guid":"58dfc4cc-01ee-489a-808b-3ff3120125b8","_uuid":"855ccac09de4f92e853cae879089e209e5b57201"},"cell_type":"markdown","source":"### **Define X and y**"},{"metadata":{"_cell_guid":"9d242e26-e9a0-4ae3-a47c-97a41e638343","collapsed":true,"_uuid":"086e9e087dc425b4ff79564680bb47ac55569776"},"execution_count":null,"outputs":[],"cell_type":"code","source":"X = df_train_padded\nX_test = df_test_padded\n\n# target columns\ny_toxic = df_train['toxic']\ny_severe_toxic = df_train['severe_toxic']\ny_obscene = df_train['obscene']\ny_threat = df_train['threat']\ny_insult = df_train['insult']\ny_identity_hate = df_train['identity_hate']"},{"metadata":{"_cell_guid":"b50dbcce-d6a2-4561-932d-8220b18e89f4","_uuid":"774dd49341c23f367e3d4c1e08894bd129fd852b"},"cell_type":"markdown","source":"### **Train and generate predictions for each of the 6 target columns:**"},{"metadata":{"_cell_guid":"6d9218a9-de6e-4df9-b667-c394c2bd8495","collapsed":true,"_uuid":"8d94984e6a3508f9f781007c17be6ad950fa501e"},"execution_count":null,"outputs":[],"cell_type":"code","source":"# create a list of the target columns\ntarget_cols = [y_toxic,y_severe_toxic,y_obscene,y_threat,y_insult,y_identity_hate]\n\npreds = []\n\nfor col in target_cols:\n    \n    print('\\n')\n    \n    # set the value of y\n    y = col\n    \n    # create a stratified split\n    X_train, X_eval, y_train ,y_eval = train_test_split(X, y,test_size=0.25,shuffle=True,\n                                                    random_state=5,stratify=y)\n\n    # cnn model\n    model = Sequential()\n    e = Embedding(vocab_size, 300, weights=[embedding_matrix], \n                  input_length=500, trainable=False)\n    model.add(e)\n    model.add(Conv1D(128, 3, activation='relu'))\n    model.add(MaxPooling1D(3))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64, 3, activation='relu'))\n    model.add(MaxPooling1D(3))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64, 3, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n\n    # compile the model\n    Adam_opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    model.compile(optimizer=Adam_opt, loss='binary_crossentropy', metrics=['acc'])\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n    save_best = ModelCheckpoint('toxic.hdf', save_best_only=True, \n                               monitor='val_loss', mode='min')\n\n    history = model.fit(X_train, y_train, validation_data=(X_eval, y_eval),\n                        epochs=100, verbose=1,callbacks=[early_stopping,save_best])\n\n    \n    # make a prediction on y (target column)\n    model.load_weights(filepath = 'toxic.hdf')\n    predictions = model.predict(X_test)\n    y_preds = predictions[:,0]\n    \n    # append the prediction to a python list\n    preds.append(y_preds)\n"},{"metadata":{"_cell_guid":"ce2b8fc7-dc8c-438b-ab91-6723abad0007","_uuid":"60ed543d945f6ba0ea9f4db52f5aa68d6769d69a"},"cell_type":"markdown","source":"### **Create a submission file**"},{"metadata":{"_cell_guid":"1cce236f-c2a8-4a49-af46-e3f757d2a96b","collapsed":true,"_uuid":"34d25b09fe313b0d94aabb0849929512e5c7f803"},"execution_count":null,"outputs":[],"cell_type":"code","source":"df_results = pd.DataFrame({'id':df_test.id,\n                            'toxic':preds[0],\n                           'severe_toxic':preds[1],\n                           'obscene':preds[2],\n                           'threat':preds[3],\n                           'insult':preds[4],\n                           'identity_hate':preds[5]}).set_index('id')\n\n# Pandas automatically sorts the columns alphabetically by column name.\n# Therefore, we need to re-order the columns to match the sample submission file.\ndf_results = df_results[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n\n# create a submission csv file\ndf_results.to_csv('kaggle_submission.csv', \n                  columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']) "},{"metadata":{"_cell_guid":"400a186c-1b13-4680-8416-a061e4672258","_uuid":"f7a3b6cf0c539d64d92ea2741c9ed069b6cc73cb"},"cell_type":"markdown","source":"***\n### **Resources**\n"},{"metadata":{"_cell_guid":"21843050-dee8-46f5-8410-40fd99f1404c","_uuid":"b187960941b6c7be32720261a987fcc489953d1b"},"cell_type":"markdown","source":"These are a few cnn and nlp resources I found helpful:\n\n- What are word embeddings?<br>\nhttps://www.youtube.com/watch?v=Eku_pbZ3-Mw\n\n\n- Blog post with a simple example explaining how to use pre trained embeddings:<br>https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n\n\n- Online cnn course:<br>\nhttps://www.coursera.org/learn/convolutional-neural-networks<br>\nThis course can be taken for free. \n\n\n- Lesson 5 notes from the fast.ai course:<br>\nhttp://wiki.fast.ai/index.php/Lesson_5_Notes\n\n\n- GloVe: Global Vectors for Word Representation<br>\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014.<br>\nhttps://nlp.stanford.edu/projects/glove/\n\n\n- Machine learning with text<br>\nhttps://www.youtube.com/watch?v=ZiKMIuYidY0\n\n\n- NLTK Tutorial series<br>\nhttps://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/"},{"metadata":{"_cell_guid":"490b224a-22db-4135-bf9f-3f65886c2994","_uuid":"9939b4da27ecece2944fb217e19a8520c65c2ddc"},"cell_type":"markdown","source":"***\nThis competition is a great learning experience. Thank you to all who have been commenting and publishing.\n\nHappy new year!"},{"metadata":{"_cell_guid":"98c3d980-6862-4e7f-99a8-134a48fcc99f","collapsed":true,"_uuid":"562e4ee4a08ea5f3b47d157cd6443e4a51b0b05c"},"execution_count":null,"outputs":[],"cell_type":"code","source":""}],"nbformat_minor":1,"nbformat":4}