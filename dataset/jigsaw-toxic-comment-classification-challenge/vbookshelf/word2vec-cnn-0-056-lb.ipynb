{"cells":[{"metadata":{"_uuid":"5545c16338519fee08965c886d098843e6cc5fbc","_cell_guid":"079edb7d-6545-4088-821e-1e2eecb4544e"},"cell_type":"markdown","source":"### Using Word2Vec Embeddings with a CNN\n\n10 Jan 2018\n\n***\n### Summary\n\nThis model scored 0.056 on the leaderboard. This is far from fantastic, however the main purpose of this kernel is to describe a process for creating word2vec embeddings and then using those embeddings to train a Keras cnn.\n\nLessons like this are easy to forget so I'm publishing this here mainly for my own future reference. Other beginners may also find this kernel helpful.\n\nThe training time for this model was approx. 90 minutes using a GPU.\n\n***\n\n#### References:\n\n- Kaggle Word2Vec tutorial by Angela Chapman:<br>\nThis tutorial is excellent.<br>\nhttps://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors <br>\n\n- Blog post by Dr Jason Brownlee:<br>\nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/ <br>\n\n- My previous kernel that used pre trained GloVe embeddings:<br>\nhttps://www.kaggle.com/vbookshelf/keras-cnn-glove-early-stopping-0-048-lb <br>\n\n- Other helpful info:<br>\nhttps://radimrehurek.com/gensim/models/word2vec.html<br>\nhttps://radimrehurek.com/gensim/models/keyedvectors.html<br>\n\n\n***\n"},{"outputs":[],"metadata":{"_uuid":"0c4fcc311a168983671f8193a8dc5d37c9fff44a","_cell_guid":"7084eff7-873d-49e6-b420-cfeb0a19c79a"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport math\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\n\nfrom gensim.models import word2vec\n\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding\nfrom keras.optimizers import Adam\nfrom keras.layers import BatchNormalization, Flatten, Conv1D, MaxPooling1D\nfrom keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":1},{"outputs":[],"metadata":{"_uuid":"6a83175bcfd1124dc7d565238045c394f2ab9913","_cell_guid":"d5ff48e3-a965-484f-82a1-bd84013c4787"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\ndf_train.fillna(value='none',inplace=True)\ndf_test.fillna(value='none',inplace=True)\n\nprint(df_train.shape)\nprint(df_test.shape)","execution_count":2},{"outputs":[],"metadata":{"_uuid":"00eb004e8399e772657406b68958181be137d9d6","_cell_guid":"38647cac-3b1e-4a02-9865-84df781509a0"},"cell_type":"code","source":"# combine the train and test sets for encoding and padding\ntrain_len = len(df_train)\ndf_combined =  pd.concat(objs=[df_train, df_test], axis=0).reset_index(drop=True)\n\nprint(df_combined.shape)\n\n# make a copy of df_combined\ndf_combined_copy = df_combined","execution_count":3},{"metadata":{"_uuid":"2d525a503bd0d5cce03a01a13a43ca0f0fb4055e","_cell_guid":"cb5abd6a-4774-4909-952d-625eb3a5bde8"},"cell_type":"markdown","source":"### Format word2vec input\n\nThis is the input format that Word2vec wants:\n\n[ [Hello, how, are, you.], [I, am, fine, thanks.] ]\n\nWord2Vec expects single sentences. Each sentence is a list of words. In other words, the input format is a list of lists."},{"metadata":{"_uuid":"23af4f78fa66adb9ea6823e7c7481332007391b1","_cell_guid":"1d3c4d20-6ff1-442e-86f1-ab7639ca7586"},"cell_type":"markdown","source":"### 1. Extract the sentences from each comment"},{"outputs":[],"metadata":{"_uuid":"545150a244106a8562b2471b21ef3a316d656380","_cell_guid":"ef69102f-4a57-4168-aa4d-e4fa5b446c41"},"cell_type":"code","source":"\n# initialize the tokenizer for extracting sentences\ntok = nltk.data.load('tokenizers/punkt/english.pickle')\n\noutput_list = []\n\ndef sentence_to_list(x):\n    \"\"\"\n    1. Input: All text in the corpus - i.e. every comment\n    2. Output: List of sentences where each sentence is a list of words e.g.\n    output = [[hello,how,are,you],[i,am,fine,thanks]]\n    3. The output python list contains all sentences from every train and test comment.\n    \n    \"\"\"\n    sentence_list= tok.tokenize(x)\n    \n    for sentence in sentence_list:\n        # convert the sentence into a list of words\n        word_list = sentence.split()\n        # add the sentence to the list of sentences\n        output_list.append(word_list)\n        \n    return output_list\n\n\n# Run the function\n# note that df_combined_copy['comment_text'] is not usable after this step.\n# After running this line, a variable called output_list is created in memory...\n# Okay, this is not the most pythonic way of doing things but apply() runs fast.\ndf_combined_copy['comment_text'].apply(sentence_to_list)\n\nprint(len(output_list))","execution_count":4},{"metadata":{"_uuid":"933417e1a101b9b7717d1455630ee81b9a186ae0","_cell_guid":"a4d883e9-009b-4681-af37-1ffeca075010"},"cell_type":"markdown","source":"### 2. Create the word2vec embedding"},{"outputs":[],"metadata":{"_uuid":"d91f2d8f3b02ae67245491db57c854daa6c6d486","collapsed":true,"_cell_guid":"4c4b4801-b17d-4cdf-97ba-3597dbd1af5f"},"cell_type":"code","source":"\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 4    # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model\n\nw2v_model = word2vec.Word2Vec(output_list, workers=num_workers, \n            size=num_features, min_count = min_word_count, \n            window = context, sample = downsampling)\n\nw2v_model.init_sims(replace=True)\n\n# save the model\n# model_name = \"300features_4minwords_10context\"\n# w2v_model.save(model_name)\n\nprint('Training completed.')","execution_count":null},{"outputs":[],"metadata":{"_uuid":"a6ad14b47d542cc6a822ca3a3425583bf741bdfe","collapsed":true,"_cell_guid":"34312436-3db8-4bd5-8f3f-93c394ee525c"},"cell_type":"code","source":"# save the word vectors\n\n#word_vectors = w2v_model.wv\n#word_vectors.save('word2vec_toxic_vectors.csv')\n\n# load the saved word vectors\n#word_vectors = KeyedVectors.load('word2vec_toxic_vectors.csv')\n","execution_count":null},{"outputs":[],"metadata":{"_uuid":"941c9d69fccfedbea7cd78f4ea88cdd694b64b60","collapsed":true,"_cell_guid":"6d3d1fe9-6c9a-4edd-b5d9-60ee6dcde90c"},"cell_type":"code","source":"# get the shape of the word2vec embedding matrix\nw2v_model.syn1neg.shape","execution_count":null},{"outputs":[],"metadata":{"_uuid":"7961dfdd442c46b507606e7a02c80e5c8a24c68e","collapsed":true,"_cell_guid":"4de58051-3e8e-4d74-a339-af04570ee64f"},"cell_type":"code","source":"# Tell me what words are most similar to the word 'man'?\nw2v_model.most_similar(\"man\")","execution_count":null},{"outputs":[],"metadata":{"_uuid":"49670c55c3cc86e4a9db5ad4fd71bac803a26c16","collapsed":true,"_cell_guid":"35678c5c-cec4-4e66-b109-9eec9dcbe2fc"},"cell_type":"code","source":"# This is how to access the embedding vector for a given word\nw2v_model.wv['hello']","execution_count":null},{"metadata":{"_uuid":"db6ab4f7a96e64f595c87dafcaa2730f6f84c351","_cell_guid":"c72188d9-aa6d-481f-8a73-5d60911591a9"},"cell_type":"markdown","source":"### Now that we have converted our corpus into a word2vec embedding,  how do we actually use it in a machine learning model?\n\nGood question. One way to do this is to add an embedding matrix to the embedding layer of a neural network. Here's how I did it."},{"metadata":{"_uuid":"b0058cc11b693e8bdc13d34bd5c78363ee47a719","_cell_guid":"fd1a9373-8035-4e92-b39e-e179e611fb9a"},"cell_type":"markdown","source":"### 1. Process the train and test comments again:\n1. Each word is assigned a unique integer.\n2. Each training example (comment) is transformed into a sequence of these unique integers. This is a vector. (A vector is simply a list of numbers.)\n3. Make all vectors the same length by padding the vector with zeros (if too short). Here we set the vector length (max_length) as 500.\n4. These padded vectors will be our model inputs: X and X_test"},{"outputs":[],"metadata":{"_uuid":"a622b7645927afca07b6c065facbedc82eaf7561","collapsed":true,"_cell_guid":"914d759b-29e6-4b08-bad8-0e2abe761c91"},"cell_type":"code","source":"# create the padded vectors\n\ndocs_combined = df_combined['comment_text'].astype(str)\n\n# This tokenizer creates a python list of words\nt = Tokenizer()\nt.fit_on_texts(docs_combined)\nvocab_size = len(t.word_index) + 1\n\n# integer encode the documents\n# assign each word a unique integer\nencoded_docs = t.texts_to_sequences(docs_combined)\n\n# pad documents to a max length of 500 words\nmax_length = 500 ###\npadded_docs_combined = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","execution_count":null},{"outputs":[],"metadata":{"_uuid":"a8577e386a0073bf5211669dff39ddc0116a702e","collapsed":true,"_cell_guid":"878594f4-9487-48d1-b933-d9b05d07feb4"},"cell_type":"code","source":"# seperate the train and test sets\n\ndf_train_padded = padded_docs_combined[:train_len]\ndf_test_padded = padded_docs_combined[train_len:]\n\nprint(df_train_padded.shape)\nprint(df_test_padded.shape)","execution_count":null},{"outputs":[],"metadata":{"_uuid":"39917cea5776a80c44ee1dfdac021c706e158c05","collapsed":true,"_cell_guid":"f708c926-4703-423a-8ed3-a525753d9877"},"cell_type":"code","source":"# create a embedding matrix for words that are in our combined train and test dataframes\n\nembedding_matrix = zeros((vocab_size, 300))\n\nfor word, i in t.word_index.items():\n    # check if the word is in the word2vec vocab\n    if word in w2v_model.wv:\n        embedding_vector = w2v_model.wv[word]\n        \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","execution_count":null},{"metadata":{"_uuid":"6c6d85307f10aacefcbe554f799e4b965ca7e7a5","_cell_guid":"2871e84b-59d3-499c-bd69-aa6b54f61549"},"cell_type":"markdown","source":"#### What is the above code doing?\n\nFirst, recall that we access a word's embedding like this:<br>\n\nw2v_model.wv['some_word']\n\nIf a word is in the train or test comments and that word does have a word2vec embedding, then we insert that embedding into our new embedding_matrix. Later, this embedding_matrix will be input into the cnn embedding layer."},{"outputs":[],"metadata":{"_uuid":"0c84764d94b73686e16760bf44d5c8fc29b891b5","collapsed":true,"_cell_guid":"2af1e14f-9a68-472a-9b4d-abb54b0401c4"},"cell_type":"code","source":"# check the shape of the new embedding matrix\nembedding_matrix.shape","execution_count":null},{"metadata":{"_uuid":"a8a8a9fc86478ed74c9125342a7163c866e89b0c","_cell_guid":"0f05077b-c56f-47c1-b12c-ba7a89514fbc"},"cell_type":"markdown","source":"### 2. CNN Model\nFinally, we arrive at the cnn model...\n\nWe run the model seperately for each of the 6 targets - toxic, severe_toxic, obscene, threat, insult, identity_hate."},{"outputs":[],"metadata":{"_uuid":"9623c01bdb118b845243f8389e92f834f8311efc","collapsed":true,"_cell_guid":"dd5f0b28-2e6a-41c3-8160-a37437196b2a"},"cell_type":"code","source":"X = df_train_padded\nX_test = df_test_padded\n\n# target columns\ny_toxic = df_train['toxic']\ny_severe_toxic = df_train['severe_toxic']\ny_obscene = df_train['obscene']\ny_threat = df_train['threat']\ny_insult = df_train['insult']\ny_identity_hate = df_train['identity_hate']","execution_count":null},{"outputs":[],"metadata":{"_uuid":"8729f2131930e150c35aac1d2343f499518c3378","collapsed":true,"_cell_guid":"5c4548a6-9ac5-49c5-aa45-25201b6144c2"},"cell_type":"code","source":"# target columns for each of the 6 models\ntarget_cols = [y_toxic,y_severe_toxic,y_obscene,y_threat,y_insult,y_identity_hate]\n\npreds = []\n\nfor col in target_cols:\n    \n    # set the value of y_train\n    y = col\n    \n    X_train, X_eval, y_train ,y_eval = train_test_split(X, y,test_size=0.25,shuffle=True,\n                                                    random_state=5,stratify=y)\n\n    # define model\n    model = Sequential()\n    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=500, trainable=False)\n    model.add(e)\n    model.add(Conv1D(128, 3, activation='relu'))\n    model.add(MaxPooling1D(pool_size=3, strides=2))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64, 3, activation='relu'))\n    model.add(MaxPooling1D(pool_size=3, strides=2))\n    model.add(Dropout(0.2))\n    model.add(Conv1D(64, 3, activation='relu'))\n    model.add(MaxPooling1D(pool_size=3, strides=2))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n\n\n    # compile the model\n    Adam_new = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n    model.compile(optimizer=Adam_new, loss='binary_crossentropy', metrics=['acc'])\n\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')\n   \n    save_best = ModelCheckpoint('toxic.hdf', save_best_only=True, monitor='val_loss', \n                               mode='min')\n\n    history = model.fit(X_train, y_train, validation_data=(X_eval, y_eval),epochs=100, verbose=1,\n                   callbacks=[early_stopping,save_best])\n\n\n    model.load_weights(filepath = 'toxic.hdf')\n    \n    # make a prediction\n    predictions = model.predict(X_test)\n\n    y_preds = predictions[:,0]\n    \n    preds.append(y_preds)\n","execution_count":null},{"outputs":[],"metadata":{"_uuid":"6fe1516c06a60602fa55dc7ddfebb0c240af092e","collapsed":true,"_cell_guid":"634dd99d-62e0-4ad1-8c0e-fa7026e1293c"},"cell_type":"code","source":"# put the results into a dataframe\n\ndf_results = pd.DataFrame({'id':df_test.id,\n                            'toxic':preds[0],\n                           'severe_toxic':preds[1],\n                           'obscene':preds[2],\n                           'threat':preds[3],\n                           'insult':preds[4],\n                           'identity_hate':preds[5]}).set_index('id')\n\n# Pandas automatically sorts the columns alphabetically by column name.\n# Therefore we need to re-order the columns to match the sample submission file.\ndf_results = df_results[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n\n# create a submission csv file\n#df_results.to_csv('word2vec_with_cnn.csv', columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']) ","execution_count":null},{"metadata":{"_uuid":"e839462a466a0e3879daf1d17d48e6ecc2418781","_cell_guid":"3452e234-b778-4bfd-b902-5bd2cbb1a735"},"cell_type":"markdown","source":"\n***\n\nThank you for reading."},{"outputs":[],"metadata":{"_uuid":"82e60bb38c799b631aea3048f7b8700a70fac83a","collapsed":true,"_cell_guid":"3aa94e73-786b-48a9-b44b-d7808abe37eb"},"cell_type":"code","source":"","execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","name":"python","file_extension":".py","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}