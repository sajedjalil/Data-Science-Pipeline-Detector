{"cells":[{"metadata":{},"cell_type":"markdown","source":"![photo](https://img.freepik.com/free-photo/night-view-neon-sign-with-text-words-have-power_78790-1119.jpg?size=626&ext=jpg)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib                  # 2D Plotting Library\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfig, plots = plt.subplots(2,3,figsize=(18,12))\nplot1, plot2, plot3, plot4, plot5, plot6 = plots.flatten()\nsns.countplot(df['obscene'], palette= 'deep', ax = plot1)\nsns.countplot(df['threat'], palette= 'muted', ax = plot2)\nsns.countplot(df['insult'], palette = 'pastel', ax = plot3)\nsns.countplot(df['identity_hate'], palette = 'dark', ax = plot4)\nsns.countplot(df['toxic'], palette= 'colorblind', ax = plot5)\nsns.countplot(df['severe_toxic'], palette= 'bright', ax = plot6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nrslt_df = df[(df['toxic'] == 0) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nrslt_df2 = df[(df['toxic'] == 1) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nnew1 = rslt_df[['id', 'comment_text', 'toxic']].iloc[:23891].copy() \nnew2 = rslt_df2[['id', 'comment_text', 'toxic']].iloc[:946].copy()\nnew = pd.concat([new1, new2], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What does tf-idf mean?\nTf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### How to Compute:\n\nTypically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n\n**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n\n*TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).*\n\n**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n\n*IDF(t) = log_e(Total number of documents / Number of documents with term t in it).*\n\nSee below for a simple example.\n\n**Example:**\n\nConsider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=5)\nXv = vectorizer.fit(new['comment_text'])\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test train split\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(new[\"comment_text\"], new['toxic'], test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.95, min_df=5)\nX1 = vectorizer.transform(X_train)\nX_test1= vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SMOTE\nSMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![photo](https://miro.medium.com/max/2246/1*o_KfyMzF7LITK2DlYm_wHw.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original dataset shape %s' % Counter(y_train))\nsm = SMOTE(random_state=12)\nx_train_res, y_train_res = sm.fit_sample(X1, y_train)\nprint('Resampled dataset shape %s' % Counter(y_train_res))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LOGISTIC REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nclf2 = LogisticRegression(C=0.1, solver='sag')\nscores = cross_val_score(clf2, x_train_res,y_train_res, cv=5,scoring='f1_weighted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_p1 = clf2.fit(x_train_res, y_train_res).predict(X_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, y_p1)\nprint('Accuracy: %f' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.908137 * (1 - 0.908137)) / y_test.shape[0])\ninterval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Confidence Interval - [88.97  90.21]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn import svm\nclf = svm.SVC(kernel='linear', C=1)\nscores = cross_val_score(clf,x_train_res,y_train_res, cv=5)\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\ny_p2 = clf.fit(x_train_res, y_train_res).predict(X_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n# accuracy: (tp + tn) / (p + n)\naccuracy = accuracy_score(y_test, y_p2)\nprint('Accuracy: %f' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.963279 * (1 - 0.963279)) / y_test.shape[0])\ninterval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Confidence Interval - [93.41  94.21]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"RANDOM FOREST","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nclf3 = RandomForestClassifier() #Initialize with whatever parameters you want to\n\n# 10-Fold Cross validation\nscores = cross_val_score(clf3,x_train_res,y_train_res, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_p3 = clf3.fit(x_train_res, y_train_res).predict(X_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_p3)\nprint('Accuracy: %f' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.9629 * (1 - 0.9629)) / y_test.shape[0])\ninterval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Confidence Interval -  [95.94 96.74] ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"MULTINOMIAL NB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclf4 = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\nscores = cross_val_score(clf4,x_train_res,y_train_res, cv=5)\ny_pred4 = clf4.fit(x_train_res, y_train_res).predict(X_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred4)\nprint('Accuracy: %f' % accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.893376 * (1 - 0.893376)) / y_test.shape[0])\ninterval","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Confidence Interval [89.06  90.38]","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}