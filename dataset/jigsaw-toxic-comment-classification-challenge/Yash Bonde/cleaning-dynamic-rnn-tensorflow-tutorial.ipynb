{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{},"source":"### WARNING: Contains Obscene Words\n\nTo go along with this tutorial, read the code comments.\n\n## PART 0: Loading the data"},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"147489fd-2711-4e81-b683-88a01cde3e32","_uuid":"696faf52fc5a128d984938fa90dbef10234c22da"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # graphs\n%matplotlib inline\nfrom tqdm import tqdm # progress bar\nimport copy # deepcopy not refrencing\nimport re # Regex\nfrom collections import Counter # counter\n\n# Machine learning\nimport tensorflow as tf\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1bebfdeb-0e2d-4b0f-bb80-29d65bc0f1b4","_uuid":"e74866732d82c5cb96e4c3f9b6ccd2f0170ab23d","collapsed":true},"source":"data_train = pd.read_csv('../input/train.csv')\ndata_test = pd.read_csv('../input/test.csv')","execution_count":2},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"797670fb-8004-4e53-9ff5-bb1be146d9b8","_uuid":"89c5fd5bad2dae6d1d718d28015ce3829df7d34c"},"source":"data_train.head(10)","execution_count":3},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"1fb5d121-c234-4abf-b185-4f3577bb5c23","_uuid":"4e1b90a841ed78bdc7b85e6d066738802413882d"},"source":"data_test.head(10)","execution_count":4},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"b9cd558d-806a-4f51-a150-bc23de2dbf45","_uuid":"18076a71dc2b0e004b717d68d31ec86c663245c4","collapsed":true},"source":"# data\ntrain_sentences = data_train['comment_text'].fillna(\"_na_\").values.tolist()\ntest_sentences = data_test['comment_text'].fillna(\"_na_\").values.tolist()\n\n# labels\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = data_train[list_classes].values\n\nlabels_ = np.zeros([len(data_train), 6])\nfor i in range(len(y)):\n    for j in range(6):\n        labels_[i][j] = y[i][j]","execution_count":5},{"cell_type":"code","outputs":[],"metadata":{"_cell_guid":"a276b944-ff4e-473b-8dfa-665f8acce1d9","_uuid":"5a9ff9fec074570fef0aea942a12b008eaa83422"},"source":"print(len(train_sentences))\nprint(len(test_sentences))","execution_count":6},{"cell_type":"markdown","metadata":{},"source":"## PART 1: Cleaning the text"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"# some parameters\nmaxlen_sent = 200 # maximum length of the sentence, explained below","execution_count":7},{"cell_type":"code","outputs":[],"metadata":{},"source":"# I will be using the previous method I used in my notebook\n# https://www.kaggle.com/ybonde/cleaning-word2vec-lstm-working\n\n# making a list of total sentences\ntotal_ = copy.deepcopy(train_sentences)\ntotal_.extend(test_sentences)\nprint('[*]Training Sentences:', len(train_sentences))\nprint('[*]Test Sentences:', len(test_sentences))\nprint('[*]Total Sentences:', len(total_))\n\n# converting the text to lower\nfor i in range(len(total_)):\n    total_[i] = str(total_[i]).lower()\n\n# convert into list of words remove unecessary characters, split into words,\n# no hyphens and other special characters, split into words\ndef sentence_to_wordlist(raw):\n    clean = re.sub(\"[^a-zA-Z0-9]\",\" \", raw)\n    words = clean.split()\n    return words\n\n# tokenising the lowered corpus\nclean_ = []\nfor i in tqdm(range(len(total_))):\n    clean_.append(sentence_to_wordlist(total_[i]))","execution_count":8},{"cell_type":"code","outputs":[],"metadata":{},"source":"# Getting the tokens\ntokens_ = []\nfor s in clean_:\n    tokens_.extend(s)\nprint(\"[*]total number of tokens:\",len(tokens_))\ntokens_ = sorted(list(set(tokens_)))\nprint(\"[*]total number of unique tokens:\",len(tokens_))","execution_count":9},{"cell_type":"code","outputs":[],"metadata":{},"source":"# making the word2id dictionary\nword2id = dict((c,i) for i,c in enumerate(tokens_))\nid2word = dict((i,c) for c,i in word2id.items())","execution_count":10},{"cell_type":"code","outputs":[],"metadata":{},"source":"# now we need to make a graph to understand the length distribution\n# number of sentence whose lengths is less than 100\nless_than_100 = 0\nless_than_200 = 0\nfor s in clean_:\n    if len(s) <= 100:\n        less_than_100 += 1\n    if len(s) <= 200:\n        less_than_200 += 1\n# thus 82% is <= 100\nprint(less_than_100/len(clean_) * 100)\n# thus 93% is <= 200\nprint(less_than_200/len(clean_) * 100)\n# equal to 1\nequal_to_1 = 0\nfor s in clean_:\n    if len(s) == 1:\n        equal_to_1 += 1\nprint(equal_to_1)","execution_count":11},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"# Thus we select 200 as our length of data, that is all the data will be clipped\n# to last 200 words. Note that we previously defined maxlen_sent\nclean_last_ = [s[-maxlen_sent:] for s in clean_]\n\n# converting the text to numbers\ndata_ = np.array([[word2id[i] for i in s] for s in clean_last_])","execution_count":12},{"cell_type":"code","outputs":[],"metadata":{},"source":"print(total_[12])\nprint()\nprint(clean_[12])\nprint()\nprint(clean_last_[12])\nprint()\nprint(data_[12])","execution_count":13},{"cell_type":"code","outputs":[],"metadata":{},"source":"# we now calculate the sequences lengths of each sentence as we will pass those through\n# the tf.nn.dynamic_rnn() function\nseqlens = np.array([len(s) for s in data_])\n\n# removing the sentences with length less than 1\nzero_length_indices = [i for i,c in enumerate(data_) if len(c) == 0]\n# we will save this for later use\n\n# removing those indices from the data\ndata_ = np.array([data_[i] for i in range(len(data_)) if i not in zero_length_indices])\nseqlens_ = np.array([seqlens[i] for i in range(len(seqlens)) if i not in zero_length_indices])\nlabels_ = np.array([labels_[i] for i in range(len(labels_)) if i not in zero_length_indices])","execution_count":14},{"cell_type":"code","outputs":[],"metadata":{},"source":"# all the sentences with 0 length are in testing set, so we don't need to worry when we slice he \nprint(len(train_sentences) < zero_length_indices[0])","execution_count":15},{"cell_type":"code","outputs":[],"metadata":{},"source":"# now we pad the sequences to proper length, other wise the tensorflow won't be able\n# to input it via placeholder, though these zeros won't be processed.\nfor i in tqdm(range(len(data_))):\n    data_[i] = np.hstack([data_[i], np.zeros(maxlen_sent - len(data_[i]), dtype = np.int32)])","execution_count":16},{"cell_type":"code","outputs":[],"metadata":{},"source":"# thus the padding was succesfull\nprint(data_[0].shape)","execution_count":17},{"cell_type":"code","outputs":[],"metadata":{},"source":"# now splitting the data into training, validation and testing data\n# training\ntrain_data_ = data_[:int(len(train_sentences) * 0.9)]\ntrain_labels_ = labels_[:int(len(train_sentences) * 0.9)]\ntrain_seqlens_ = seqlens_[:int(len(train_sentences) * 0.9)]\n\n# validation\ntrain_data_v = data_[int(len(train_sentences) * 0.9): len(train_sentences)]\ntrain_labels_v = labels_[int(len(train_sentences) * 0.9): len(train_sentences)]\ntrain_seqlens_v = seqlens_[int(len(train_sentences) * 0.9): len(train_sentences)]\n\n# testing\ntest_data_ = data_[len(train_sentences):]\ntest_seqlens_ = seqlens_[len(train_sentences):]\n\n# reconfig the zero indices list\nzero_length_indices_corrected = np.array(zero_length_indices) - len(train_sentences)\n\n'''\nNow this is a pretty good time to save these arrays as they are, so we can load them later and\nuse as per requirement. You can train your own word2vec model, and then use it. Or as shown\nhere, directly feed them into tensorflow, using supervised embeddings.\n'''","execution_count":18},{"cell_type":"markdown","metadata":{},"source":"## PART 2: Build the model\nWe are using tensorflow as our machine learning API library. For learning how to make a dynamic_rnn graph go [here](https://github.com/yashbonde/basic-utils/blob/master/Dynamic%20LSTM%20in%20Tensorflow.ipynb)."},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"# parameters\nbatch_size = 256 # batch size\nn_epochs = 1000 # number of training epochs\nnum_classes = 6 # number of output classes\ne_dim = 128 # dimension of embeddings\nvocab_size = len(tokens_) # total number of words\nn_hidden_lstm = 256 # hidden size of LSTM network\ndisp_step = 10 # disply status after every disp_step epochs","execution_count":29},{"cell_type":"code","outputs":[],"metadata":{},"source":"# Defining the placeholders for IO\n_x = tf.placeholder(tf.int32, [batch_size, maxlen_sent])\n_y = tf.placeholder(tf.float32, [batch_size, num_classes])\n_seqlens = tf.placeholder(tf.int32, [batch_size])","execution_count":20},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"with tf.name_scope(\"embeddings\"):\n    embeddings = tf.Variable(tf.random_uniform([vocab_size, e_dim]), name = 'embedding_matrix')\n    embed = tf.nn.embedding_lookup(embeddings, _x)","execution_count":21},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"with tf.variable_scope(\"lstm\"):\n    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_lstm, forget_bias = 1.0)\n    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed,\n                                        sequence_length = _seqlens,\n                                        dtype = tf.float32)","execution_count":22},{"cell_type":"code","outputs":[],"metadata":{},"source":"# defining the dense layers weights and biases\n# first layer\nW1 = tf.truncated_normal([n_hidden_lstm, 128], name = 'W1')\nb1 = tf.truncated_normal([128], name = 'b1')\n\n# second layer\nW2 = tf.truncated_normal([128, num_classes], name = 'W2')\nb2 = tf.truncated_normal([num_classes], name = 'b2')","execution_count":23},{"cell_type":"code","outputs":[],"metadata":{},"source":"op_1 = tf.nn.relu(tf.matmul(states[1], W1) + b1) # output dense layer 1\ny_op = tf.matmul(op_1, W2) + b2 # output dense layer 2","execution_count":24},{"cell_type":"code","outputs":[],"metadata":{},"source":"# defining the cross_entropy, the competition uses log loss so that is what we are\n# going to implememt the same\nsigmoid_diff = tf.nn.sigmoid_cross_entropy_with_logits(labels = _y, logits = y_op)\ncross_entropy = tf.reduce_mean(sigmoid_diff)","execution_count":25},{"cell_type":"markdown","metadata":{},"source":"## PART 3: Training the model"},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"train_step = tf.train.AdamOptimizer().minimize(cross_entropy)","execution_count":26},{"cell_type":"code","outputs":[],"metadata":{"collapsed":true},"source":"def get_batch(batch_size, data_x, data_y, data_seqlen):\n    instance_indices = list(range(len(data_x)))\n    np.random.shuffle(instance_indices)\n    batch = instance_indices[:batch_size]\n    x = [data_x[i] for i in batch]\n    y = [data_y[i] for i in batch]\n    seqlens = [data_seqlen[i] for i in batch]\n    return x,y,seqlens","execution_count":27},{"cell_type":"code","outputs":[],"metadata":{},"source":"# tensorflow session\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n# loss_list\nloss_list = []\nval_loss_list = []\n\nfor e in range(n_epochs):\n    # get batch for training\n    x_batch, y_batch, seqlen_batch = get_batch(batch_size, train_data_,\n                                               train_labels_, train_seqlens_)\n    # make feed dictionary\n    feed_dict = {_x:x_batch, _y:y_batch, _seqlens:seqlen_batch}\n    # find cross_entropy\n    ce = sess.run(cross_entropy, feed_dict = feed_dict)\n    loss_list.append(ce)\n    # run one step\n    sess.run(train_step, feed_dict = feed_dict)\n    if e%disp_step == 0:\n        # getting a batch for validation task\n        val_x_batch, val_y_batch, val_seqlen_batch = get_batch(batch_size, train_data_v,\n                                                               train_labels_v,\n                                                               train_seqlens_v)\n        # making feed dictionary\n        feed_dict_val = {_x: val_x_batch, _y:val_y_batch, _seqlens:val_seqlen_batch}\n        # getting loss\n        val_loss = sess.run(cross_entropy, feed_dict = feed_dict_val)\n        val_loss_list.append(val_loss)\n        # printing the result\n        print(\"Epoch: {0}, Validation Loss: {1}\".format(e, val_loss))\n        # training on validation data\n        sess.run(train_step, feed_dict = feed_dict_val)","execution_count":33},{"cell_type":"code","outputs":[],"metadata":{},"source":"plt.figure(figsize = (10,10))\nplt.plot(loss_list)","execution_count":34},{"cell_type":"code","outputs":[],"metadata":{},"source":"plt.figure(figsize = (10,10))\nplt.plot(val_loss_list)","execution_count":35},{"cell_type":"markdown","metadata":{},"source":"Clearly we need to optimize the model, but the results are sufficient as a tutorial on how to use Dynamic LSTM for analysis of text."}],"metadata":{"language_info":{"name":"python","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.4","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat_minor":1}