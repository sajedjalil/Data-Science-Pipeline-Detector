{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<centre><title><h1>Toxic Comment Classification</h1></title></centre>","metadata":{}},{"cell_type":"markdown","source":"<center><img src='https://www.blogtyrant.com/wp-content/uploads/2013/01/how-to-get-more-blog-comments.jpg'></center>","metadata":{}},{"cell_type":"markdown","source":"<br>\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em;line-height: 1.7em; font-family: Verdana;\"><b style=\"font-size: 18px;\">ðŸ›‘ &nbsp; WARNING:</b><br><br><b>The dataset for this competition contains text that may be considered profane, vulgar, or offensive.</b><br></div></center>","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport random\nimport re\nimport json\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom path import Path\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sp\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\n\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly import graph_objs as go\n","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:38:54.6758Z","iopub.execute_input":"2022-05-18T10:38:54.676589Z","iopub.status.idle":"2022-05-18T10:38:59.327761Z","shell.execute_reply.started":"2022-05-18T10:38:54.676468Z","shell.execute_reply":"2022-05-18T10:38:59.326964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About Competition","metadata":{}},{"cell_type":"markdown","source":"<p><h3>Description</h3></p>\n<p>Discussing things you care about can be difficult. The <b><mark>threat of abuse and harassment</mark></b> online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.</p>\n<p>The <b><mark>Conversation AI</mark></b> team, a research initiative founded by <b><mark>Jigsaw</mark></b> and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far theyâ€™ve built a range of publicly available models served through the <b><mark>Perspective API</mark></b>, including toxicity. But the current models still make errors, and they donâ€™t allow users to select which types of toxicity theyâ€™re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content)</p>\n<p>In this competition, weâ€™re challenged to build a multi-headed model thatâ€™s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspectiveâ€™s current models. Weâ€™ll be using a dataset of comments from Wikipediaâ€™s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful.</p>\n<br>\n<p><h3>Evaluation</h3></p>\n<p>The score is the average of the individual AUCs of each predicted column.</p>","metadata":{}},{"cell_type":"markdown","source":"# About Dataset","metadata":{}},{"cell_type":"markdown","source":"\n**`train.csv`**\n- The training set, contains comments with their binary labels.\n\n**`test.csv`**\n- The test set, contains some comments which are not included in scoring.\n","metadata":{}},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:38:59.329733Z","iopub.execute_input":"2022-05-18T10:38:59.330414Z","iopub.status.idle":"2022-05-18T10:38:59.336473Z","shell.execute_reply.started":"2022-05-18T10:38:59.330376Z","shell.execute_reply":"2022-05-18T10:38:59.335664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"../input/jigsaw-toxic-comment-classification-challenge/\"\ntrain_data_path = 'train.csv.zip'\ntest_data_path = 'test.csv.zip'\n\ntrain_df = pd.read_csv(path + train_data_path)\nprint(f'shape of train_dataset: {train_df.shape}')\nprint('\\n\\n')\nprint('========== Train Dataset ==========')\nprint('\\n')\ndisplay(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:38:59.337751Z","iopub.execute_input":"2022-05-18T10:38:59.338142Z","iopub.status.idle":"2022-05-18T10:39:01.442659Z","shell.execute_reply.started":"2022-05-18T10:38:59.338074Z","shell.execute_reply":"2022-05-18T10:39:01.441697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<div class='alert alert-info' role='alert'>\n    <p><h3><b>Highlights</b></h3></p>\n    <p><b>1.</b>Train Dataset contains <b>159571 unique comments</b>.</p>\n    <p><b>2.</b>It contains <b>16225 toxic</b> and <b>143346 non-toxic</b>  comments.</p>\n    <p class='paragraph'><b>3.</b>Types of toxicity are: <b>toxic</b> , <b>severe_toxic</b> , <b>obscene</b> , <b>threat</b> , <b>insult</b> , <b>identity_hate</b>.</p>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"temp_df = pd.DataFrame()\ntoxicity = []\ncomment_type = []\ncount = []\nfor col in train_df.columns:\n    if col not in ('id','comment_text'):\n        toxicity.extend([col,col])\n        comment_type.extend(train_df[col].value_counts().keys().tolist())\n        count.extend(train_df[col].value_counts().values)\n\ntemp_df['toxicity'] = toxicity\ntemp_df['comment_type'] = np.array(comment_type,dtype=np.str)\ntemp_df['count'] = count\n\nfig = px.bar(temp_df, x='toxicity', y='count', color='comment_type', title='Value Counts',\n             color_discrete_sequence=['#1616A7','#FB0D0D'],width=500)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:39:01.444806Z","iopub.execute_input":"2022-05-18T10:39:01.445164Z","iopub.status.idle":"2022-05-18T10:39:02.521173Z","shell.execute_reply.started":"2022-05-18T10:39:01.445116Z","shell.execute_reply":"2022-05-18T10:39:02.520534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(text):\n    text = re.sub(r\" -\", \"\", text)\n    text = re.sub(r\"\\d+:\\d+, \\w+ \\d+, \\d+ \\(\\w+\\)\",\"\",text) # to remove 21:51, January 11, 2016 (UTC)\n    text = re.sub(r\"\\d+.+\\d\",\"\",text)\n    text = re.sub(r\"\\n\",\" \",text)\n    text = re.sub(r\"\\\\\",\"\",text)\n    text = re.sub(r\"\\.\\.\\.|\\.\\.\",\" \",text)\n    text = re.sub(r\":|_|#\",\" \",text)\n    text = re.sub(r\"@|\\||\\(|\\)|!|:|;|\\\"\",\"\",text)\n    text = re.sub(r\"\\.|\\?|,\",\" \", text)\n    return text\n\n\n\ntemp_df = train_df.copy()\n\ntemp_df['comment_text'] = temp_df['comment_text'].apply(preprocessing)\ntemp_df['tokenized_text'] = temp_df['comment_text'].apply(word_tokenize)\n#temp_df['comment_text'] = temp_df['comment_text'].apply(lambda x: ' '.join(x.split()))\n\ntemp_df['len'] = temp_df['comment_text'].apply(len)\n\ntemp_df2 = pd.DataFrame()\n\ntemp_df2['len'] = temp_df['len'].value_counts().keys()\ntemp_df2['count'] = temp_df['len'].value_counts().values\n\ntemp_df2 = temp_df2.sort_values('len')\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=temp_df2['len'].values,y=temp_df2['count'].values,\n                         mode='lines', name='len'))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:39:02.522176Z","iopub.execute_input":"2022-05-18T10:39:02.522793Z","iopub.status.idle":"2022-05-18T10:40:57.833484Z","shell.execute_reply.started":"2022-05-18T10:39:02.522759Z","shell.execute_reply":"2022-05-18T10:40:57.832944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic = temp_df[temp_df[\"toxic\"] == 1].reset_index(drop=True)\nsevere_toxic = temp_df[temp_df[\"severe_toxic\"] == 1].reset_index(drop=True)\nobscene = temp_df[temp_df[\"obscene\"] == 1].reset_index(drop=True)\nthreat = temp_df[temp_df[\"threat\"] == 1].reset_index(drop=True)\ninsult = temp_df[temp_df[\"insult\"] == 1].reset_index(drop=True)\nidentity_hate = temp_df[temp_df[\"identity_hate\"] == 1].reset_index(drop=True)\n\nlemma = WordNetLemmatizer()\nstem = SnowballStemmer(\"english\")\ndef most_common(data,n,col):\n    top = Counter([stem.stem(lemma.lemmatize(item)) for sublist in data[col] for item in sublist if item.lower() not in STOPWORDS])\n    temp = pd.DataFrame(top.most_common(20))\n    temp.columns = [\"common_word\",\"count\"]\n    return temp.style.background_gradient(cmap=\"Blues\")\nprint(\"top 20 common words in each toxicity\")\nprint('\\n')\nprint('========== toxic ==========')\ndisplay(most_common(toxic, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== severe_toxic ==========')\ndisplay(most_common(severe_toxic, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== obscene ==========')\ndisplay(most_common(obscene, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== threat ==========')\ndisplay(most_common(threat, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== insult ==========')\ndisplay(most_common(insult, 20, 'tokenized_text'))\n\nprint('\\n')\nprint('========== identity_hate ==========')\ndisplay(most_common(identity_hate, 20, 'tokenized_text'))\nprint('\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:40:57.834725Z","iopub.execute_input":"2022-05-18T10:40:57.835179Z","iopub.status.idle":"2022-05-18T10:41:23.177656Z","shell.execute_reply.started":"2022-05-18T10:40:57.835117Z","shell.execute_reply":"2022-05-18T10:41:23.176679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in toxic['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/butterfly-shadow-animal-icon-silhouettes-isolated-dark-black-graphical-white-background-184947266.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False, \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Toxic -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:31:14.401029Z","iopub.execute_input":"2022-05-18T11:31:14.401344Z","iopub.status.idle":"2022-05-18T11:31:30.527325Z","shell.execute_reply.started":"2022-05-18T11:31:14.401311Z","shell.execute_reply":"2022-05-18T11:31:30.526499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in severe_toxic['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/20854.Jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Severe toxic -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:34:39.972841Z","iopub.execute_input":"2022-05-18T11:34:39.973523Z","iopub.status.idle":"2022-05-18T11:34:45.871845Z","shell.execute_reply.started":"2022-05-18T11:34:39.973488Z","shell.execute_reply":"2022-05-18T11:34:45.871044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(60,100)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in obscene['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/istockphoto-858216614-612x612.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Obscene -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:33:23.123468Z","iopub.execute_input":"2022-05-18T11:33:23.123872Z","iopub.status.idle":"2022-05-18T11:33:31.705681Z","shell.execute_reply.started":"2022-05-18T11:33:23.123841Z","shell.execute_reply":"2022-05-18T11:33:31.702433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in threat['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/Picsart_22-05-18_16-33-42-111.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Threat -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:35:40.716018Z","iopub.execute_input":"2022-05-18T11:35:40.716346Z","iopub.status.idle":"2022-05-18T11:35:51.235968Z","shell.execute_reply.started":"2022-05-18T11:35:40.716314Z","shell.execute_reply":"2022-05-18T11:35:51.235185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in insult['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/red-kite-rapture-black-silhouette-cut-out-and-isolated-on-a-white-background-2C4B3E9.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=2000,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Insult -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:21:41.353729Z","iopub.execute_input":"2022-05-18T12:21:41.354046Z","iopub.status.idle":"2022-05-18T12:22:23.612362Z","shell.execute_reply.started":"2022-05-18T12:21:41.35401Z","shell.execute_reply":"2022-05-18T12:22:23.611572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def similer_color_func(word=None, font_size=None,\n                       position=None, orientation=None,\n                       font_path=None, random_state=None):\n    h=24\n    s=100\n    l=random_state.randint(30,70)\n    return f'hsl({h}, {s}%, {l}%)'\n\ntext = [stem.stem(lemma.lemmatize(item.lower())) for sublist in identity_hate['tokenized_text'] for item in sublist if item.lower() not in STOPWORDS]\ntext = ' '.join(text)\n\nmask = np.array(Image.open('../input/images/2d27acd5d288284587e13b0411e6e48a.jpg'))\nwc = WordCloud(mask=mask,\n              background_color='black',\n              max_words=500,\n              stopwords=STOPWORDS,\n              max_font_size=256,\n              width=mask.shape[1],\n              height=mask.shape[0],\n              collocations=False,  \n              font_path=\"../input/font-style/SalmaAlfasans-Light.otf\",\n              color_func=similer_color_func)\n\nwc.generate(text)\nfig = plt.figure(figsize=(15, 15))\nplt.title(\"- Most Common Words within Identity hate -\",\n           size=22, weight=\"bold\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:45:19.994438Z","iopub.execute_input":"2022-05-18T12:45:19.994729Z","iopub.status.idle":"2022-05-18T12:45:32.942531Z","shell.execute_reply.started":"2022-05-18T12:45:19.9947Z","shell.execute_reply":"2022-05-18T12:45:32.941591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}