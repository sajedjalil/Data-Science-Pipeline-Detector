{"cells":[{"cell_type":"markdown","source":"In this notebook we will do two simple things:\n1. Train a linear model (logistic regression) for each class a make a submission\n2. Generate the most neutral and the most toxic comment with the help of trained model.\n\nLet's go!","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import re\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nCLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","metadata":{"collapsed":true,"_uuid":"0f40772311354c4ef3184ae2545e9bc95eaefd97","_cell_guid":"7b7f00ca-a3a7-4bcb-91b5-003d89be86c6"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Tets data contains one NaN, so we have to reaplce it with something\ntest.fillna(' ', inplace=True)","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"To work with the text data we first need to normalize it. Here is very simple normalizer. That's how it works:\n1. Convert everything to the lowercase\n2. Replace all new lines and tabs with whitespaces\n3. Replace all non-alphanumerical characters with space (leave only letters, numbers and underscores)\n4. Replace subsequent whitespaces with single space\n5. Strip the line (remove whitespace from the begginign and end)\n\nThe normalized text will be put to he column named `normalized`","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"def normalize(text):\n    text = text.lower()\n    text = text.replace('\\n', ' ').replace('\\t', ' ')\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    return text","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"train['normalized'] = train.comment_text.map(normalize)\ntest['normalized'] = test.comment_text.map(normalize)","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"We will use two different text vectorizer. One will works on the word level and one on the char level. This will allow us to find interesting interactions and generalize on the grammar level.","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"vect_words = TfidfVectorizer(max_features=5000)\nvect_chars = TfidfVectorizer(max_features=1000, analyzer='char', ngram_range=(1, 3))","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Creating features\nXtrain_words = vect_words.fit_transform(train.normalized)\nXtrain_chars = vect_chars.fit_transform(train.normalized)\n\n# Combine two different types of features into single sparse matrix\nXtrain = sparse.hstack([Xtrain_words, Xtrain_chars])","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Train the models and save them to the dict to use later","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"models = {}\nfor toxicity in CLASSES:\n    # I encourage you to change this C=5.0 and try different regularization\n    lm = LogisticRegression(C=5.0, random_state=42)  \n    lm.fit(Xtrain, train[toxicity])\n    models[toxicity] = lm\n    print(\"Model for %s trained\" % toxicity, flush=True)","metadata":{}},{"cell_type":"markdown","source":"Create predictions for the test set","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"Xtest_words = vect_words.transform(test.normalized)\nXtest_chars = vect_chars.transform(test.normalized)\n\nXtest = sparse.hstack([Xtest_words, Xtest_chars])\n\npredictions = pd.DataFrame(test.id)\nfor toxicity in CLASSES:\n    predictions[toxicity] = models[toxicity].predict_proba(Xtest)[:, 1]\n    \npredictions.to_csv('./submission.csv', index=False)","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Now, when the submission is created and the job is done, let's have some fun. We can extract coefficients for every word and character n-gram and see how the contribute to the toxicity of the comment. Thus, we can just sum coefficients for all classes of toxicity to get the most neutral and the most toxic words. And we will use them to generate **the worst comment ever**. What can go wrong?","metadata":{"collapsed":true}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"coefficients = pd.DataFrame(index=vect_words.get_feature_names() + vect_chars.get_feature_names())\nfor toxicity in CLASSES:\n    coefficients[toxicity] = models[toxicity].coef_[0]\n    \ncoefficients['total'] = coefficients.sum(1)\ncoefficients.sort_values('total', inplace=True)","metadata":{"collapsed":true}},{"cell_type":"markdown","source":"Ready for the most neutral and non-toxic comment? Here it is:","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Let's randomly permutate top-10 words to amke it more interesting\nnp.random.seed(1)\n' '.join(np.random.permutation(coefficients.head(10).index))","metadata":{}},{"cell_type":"markdown","source":"The worst one? Let's go!","metadata":{}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(1)\n' '.join(np.random.permutation(coefficients.tail(10).index))","metadata":{}},{"cell_type":"markdown","source":"It's like watching the *Blood & Concrete: A Love Story* opening scene. I hope I'll not be banned on Kaggle for this notebook :) In my defence I can only say: *\"Hey! It's in the data!\"*","metadata":{}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}