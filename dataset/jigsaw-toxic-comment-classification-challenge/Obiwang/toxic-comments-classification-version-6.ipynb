{"nbformat":4,"metadata":{"language_info":{"version":"3.6.3","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"**In this notebook, three models (NB-SVM, LSTM, LR) are trained. The final submission are the weighted average of the results of three models.**","metadata":{"_cell_guid":"214f8950-536b-4dd6-82d7-7aaaa86263c7","_uuid":"108f96aa9bd403e5e980222493c255f4f64c115c"}},{"execution_count":null,"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"dd9cf18c-3a50-4f22-96f6-999f7d66d37e","_uuid":"23de149637d198065ed3e2842195aa779c6dead2"}},{"cell_type":"markdown","source":"Now let's have a look at the three data files to get a sense what they look like.\n","metadata":{"_cell_guid":"6b9d54be-45e1-4335-a383-c9e4060748f7","_uuid":"62e6a26c7536d50b61f74f4dbde876e3aa237221"}},{"execution_count":null,"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntrain.head(10)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"9dc72003-92b3-4975-8036-204f2e5f7677","_uuid":"f7b086c527b49c883d120418b7c49d1b3ba11755"}},{"cell_type":"markdown","source":"Have a look at the lengths of the comments. I replace empty comments with \"unknown\" to avoid errors.","metadata":{"_cell_guid":"920b06c5-5acc-4db1-8f3e-fcef8704a7f3","_uuid":"1ea8696779713ab9a2750a7e39ac36cd3422bf1c"}},{"execution_count":null,"cell_type":"code","source":"lens = train.comment_text.str.len()\nlens.hist()\nsorted(lens.tolist())[:10]\nCOMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"bc54eb52-e200-455c-a6be-4a0be7813b76","_uuid":"8d85a0a27045fe48bf9dd85cc175dd1e397170b7"}},{"execution_count":null,"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"5b85c35f-3910-49bb-9d79-cf67f2df68d1","_uuid":"89c99351b07c5c3f2adb2d4c4e0e6d7d53006dc7"}},{"execution_count":null,"cell_type":"code","source":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[\"comment_text\"])\ntest_term_doc = vec.transform(test[\"comment_text\"])","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"2f8c6fc2-a632-4245-9ab7-9da78da7dfb3","_uuid":"bee54c73c3ab278bd56cbed12e2e02b0c867ff09"}},{"execution_count":null,"cell_type":"code","source":"trn_term_doc, test_term_doc","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"6c48c71f-fad2-4b00-93cf-1f4dd712d245","_uuid":"1799b3d013fb0f285b06f0ec88eb777f627febed"}},{"execution_count":null,"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"4c0411e4-bfdf-46a4-b837-352e0cb7e1e3","_uuid":"85ddae523b4642b810576164a47dda2659dfbab3"}},{"execution_count":null,"cell_type":"code","source":"x = trn_term_doc\ntest_x = test_term_doc","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"9e463252-12df-4d50-bb7c-fd2fbce48bd9","_uuid":"0e28f1809c993cf5ddcc032a1b70c9a163b0fd3a"}},{"execution_count":null,"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"2bac6c17-5926-4505-9997-7ec7a1c4f6b6","_uuid":"2fb9d9555a63dca864c8b2a245e6bfc334b832da"}},{"execution_count":null,"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\npreds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"44296916-1779-4616-88a3-b0f31c66c681","_uuid":"43e06d3b632544775cae3365537ca156d5cd7ccc"}},{"execution_count":null,"cell_type":"code","source":"subm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\nsubmid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission_NBSVM.csv', index=False)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"9b01a6d1-c546-4ae1-9cca-b56b43d8c29e","_uuid":"c143112445a7d37dc3df12eb9b51a845089a579c"}},{"cell_type":"markdown","source":"Now try LSTM","metadata":{"_cell_guid":"2ede9442-dd03-4b32-8972-b2a7181eaf9c","_uuid":"e4c6e73311e885a90e10d718c74df332cf04dcb7"}},{"execution_count":null,"cell_type":"code","source":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"108feabc-4dde-4018-9ac5-714f2ab55665","_uuid":"f15e82c8037716343e8eeb309beff54481959b16"}},{"execution_count":null,"cell_type":"code","source":"path = '../input/'\ncomp = 'jigsaw-toxic-comment-classification-challenge/'\nEMBEDDING_FILE=f'{path}glove6b50d/glove.6B.50d.txt'\nTRAIN_DATA_FILE=f'{path}{comp}train.csv'\nTEST_DATA_FILE=f'{path}{comp}test.csv'","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"f068a911-fd0e-4e62-93c1-1db8e6bb2306","_uuid":"0d4002ca89deabca80f39652962d3135c0acf292"}},{"execution_count":null,"cell_type":"code","source":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"dd64834b-4d98-4864-aa11-eeed0af60276","_uuid":"40b27e99a5e9a026d5f5ac13501952517384602a"}},{"execution_count":null,"cell_type":"code","source":"train = pd.read_csv(TRAIN_DATA_FILE)\ntest = pd.read_csv(TEST_DATA_FILE)\n\nlist_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\nlist_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"13bc01b0-3665-4d42-9edf-ab8354f656cf","_uuid":"c300ff1357da8eadc619becfd951855d550f28e3"}},{"execution_count":null,"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"3ae6503d-bd0d-4572-8443-fc46f55a1555","_uuid":"5d6af43bf8138c05480ac21bb25241f8cb679a0d"}},{"execution_count":null,"cell_type":"code","source":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"f9b956a5-c942-40cc-849a-6d751e877e37","_uuid":"6c6a7eba73e2492651f27e7f8890f9ee640e0dbb"}},{"execution_count":null,"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"818695aa-a94e-4e83-8fe9-b1f7d4db0da1","_uuid":"272b8a4017458ba9988c3afa59d60fe315a27673"}},{"execution_count":null,"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(50, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(6, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"14232c10-1d25-4336-8742-289de5fbffc9","_uuid":"b507634f01ee6832ae3c7c8e2e63dc5a84ade472"}},{"execution_count":null,"cell_type":"code","source":"model.fit(X_t, y, batch_size=32, epochs=2) # validation_split=0.1);","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"d1733d1f-1f85-47c1-bcb6-c517891a9771","_uuid":"506ecd84dabc843a45a3dc5f4470289feb5841d5"}},{"execution_count":null,"cell_type":"code","source":"y_test = model.predict([X_te], batch_size=1024, verbose=1)\nsubmission2 = pd.read_csv(f'{path}{comp}sample_submission.csv')\nsubmission2[list_classes] = y_test\nsubmission2.to_csv('submission_LSTM.csv', index=False)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"5565e070-5ca7-4ece-8e4c-9b0110f918b9","_uuid":"4c839a288da2602f7f0857bfb697249cd07db41f"}},{"cell_type":"markdown","source":"Now try linear regression model. The data used are from https://www.kaggle.com/eoveson/convai-datasets-baseline-models","metadata":{"_cell_guid":"1c19821b-2ca0-4e9f-88c6-33de30876aec","_uuid":"f80a6207abc5532641d7761cb56cfff457caa660"}},{"execution_count":null,"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom scipy import sparse\nfrom subprocess import check_output\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"f8adc8f4-3af4-4ce5-a1df-afa259784285","_uuid":"94aa709499ba681ca3681224721305428c039f12"}},{"execution_count":null,"cell_type":"code","source":"train = pd.read_csv('../input/convai-datasets-baseline-models/train_with_convai.csv')\ntest = pd.read_csv('../input/convai-datasets-baseline-models/test_with_convai.csv')","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"6e4b8e35-0a17-4d46-8c66-78b8296fe8bd","_uuid":"a4ab142130bc34322ab79fb61b9e49b78febf40b"}},{"execution_count":null,"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\np_res = submission2.copy()\np_res[label_cols] = (submission[label_cols] + submission2[label_cols]) / 2\np_res.to_csv('submission.csv', index=False)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"5bc1de73-0a6a-4709-a114-d36e3cf33251","_uuid":"7ba3829fd9b1f832da7cb3f4e8e5ba473f8eaf4d"}},{"execution_count":null,"cell_type":"code","source":"feats_to_concat = ['comment_text', 'toxic_level', 'attack', 'aggression']\n# combining test and train\nalldata = pd.concat([train[feats_to_concat], test[feats_to_concat]], axis=0)\nalldata.comment_text.fillna('unknown', inplace=True)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"47f5af13-2dce-4f59-bb18-54145b0b2381","_uuid":"d5263e981979581fb942fb566340f756ce6d0074"}},{"execution_count":null,"cell_type":"code","source":"vect_words = TfidfVectorizer(max_features=50000, analyzer='word', ngram_range=(1, 1))\nvect_chars = TfidfVectorizer(max_features=20000, analyzer='char', ngram_range=(1, 3))\nall_words = vect_words.fit_transform(alldata.comment_text)\nall_chars = vect_chars.fit_transform(alldata.comment_text)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"74a824c8-412e-442a-9261-f3ddf66b624d","_uuid":"101f5cc31fe65c531f4ebdc9aaaeadc2cd58b625"}},{"execution_count":null,"cell_type":"code","source":"train_new = train\ntest_new = test\n\ntrain_words = all_words[:len(train_new)]\ntest_words = all_words[len(train_new):]\n\ntrain_chars = all_chars[:len(train_new)]\ntest_chars = all_chars[len(train_new):]","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"4f12cfc0-57b0-48ab-8988-734e2e9167e0","_uuid":"61bf4bbf9293280a086308ad1d730840fd7888f1"}},{"execution_count":null,"cell_type":"code","source":"feats = ['toxic_level', 'attack']\n# make sparse matrix with needed data for train and test\ntrain_feats = sparse.hstack([train_words, train_chars, alldata[feats][:len(train_new)]])\ntest_feats = sparse.hstack([test_words, test_chars, alldata[feats][len(train_new):]])","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"9cc59e76-b9de-4044-832c-1913264ab0b1","_uuid":"6ffa9b8087c84746d4e89fd59fe403821193547c"}},{"execution_count":null,"cell_type":"code","source":"col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nonly_col = ['toxic']\n\npreds = np.zeros((test_new.shape[0], len(col)))\n\nfor i, j in enumerate(col):\n    print('===Fit '+j)\n    \n    model = LogisticRegression(C=4.0, solver='sag')\n    print('Fitting model')\n    model.fit(train_feats, train_new[j])\n      \n    print('Predicting on test')\n    preds[:,i] = model.predict_proba(test_feats)[:,1]","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"e8e94d9a-42b0-4a45-8dc5-96c3928066e2","_uuid":"7076d260d2c25ef180b639d791063c66680c4b45"}},{"execution_count":null,"cell_type":"code","source":"subm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n\nsubmid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission3 = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\nsubmission3.to_csv('submission_LR.csv', index=False)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"67195fdb-b58b-4f26-9f28-caf8c1f5189e","_uuid":"926479cddd3bf5b7c6bac5064d48c915d335e588"}},{"execution_count":null,"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\np_res = submission4.copy()\np_res[label_cols] = (2* submission[label_cols] + 3 * submission2[label_cols] + 4 * submission3[label_cols]) / 9\np_res.to_csv('submission.csv', index=False)","outputs":[],"metadata":{"collapsed":true,"_cell_guid":"862db44b-5f41-4339-8968-cd075fd5396f","_uuid":"a10435806d731484cc1be9b8a31cb74fd8e59e7e"}}]}