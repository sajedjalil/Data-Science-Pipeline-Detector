{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install tensorflow==2.0.0-beta1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd, numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\n##test_labels = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv') }Can not be loaded \nsubm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = train['comment_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for train\nlens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test\nlens = test.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lens.hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1) ## each colum may have the value of one ( Labled ) . 1- calc the max # if has no lable max = 0 then col = 1 -0 = 0\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train),len(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## deal with nulls \nCOMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building the model¶\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re, string\nre_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\ndef clean(s): return re_tok.sub(r' \\1 ', s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## decide vocab size \nwords = []\nfor t in text:\n    words.extend(tokenize(t))\nprint(words[:100])\nvocab = list(set(words))\nprint(len(words), len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean(train['comment_text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_word_embedding(vtrain_data,vtest_data):\n    # switch data back to text \n    train_labels = vtrain_data[label_cols]\n    txt_train_data = [clean(txt) for txt in train['comment_text']]\n    txt_test_data = [clean(txt) for txt in test['comment_text']]\n    \n    # integer encode the documents\n    vocab_size = 10000\n    encoded_txt_train_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_train_data]\n    encoded_txt_test_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_test_data]\n    #print(encoded_txt_train_data)\n\n    ptxt_train_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_train_data,\n                                                            padding='post',\n                                                            maxlen=5000)\n\n    ptxt_test_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_test_data,\n                                                           padding='post',\n                                                           maxlen=5000)\n    x_val = ptxt_train_data[:100000] \n    partial_x_train = ptxt_train_data[100000:]\n\n    y_val = train_labels[:100000]\n    partial_y_train = train_labels[100000:]\n    return (x_val,partial_x_train,y_val,partial_y_train,ptxt_test_data)\n\ndef full_one_hot_word_embedding(vtrain_data,vtest_data):\n    # switch data back to text \n    train_labels = vtrain_data[label_cols]\n    txt_train_data = [clean(txt) for txt in train['comment_text']]\n    txt_test_data = [clean(txt) for txt in test['comment_text']]\n    \n    # integer encode the documents\n    vocab_size = 10000\n    encoded_txt_train_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_train_data]\n    encoded_txt_test_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_test_data]\n    #print(encoded_txt_train_data)\n\n    ptxt_train_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_train_data,\n                                                            padding='post',\n                                                            maxlen=5000)\n\n    ptxt_test_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_test_data,\n                                                           padding='post',\n                                                           maxlen=5000)\n    partial_x_train = ptxt_train_data\n    partial_y_train = train_labels\n    return (partial_x_train,partial_y_train,ptxt_test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_with_emb_acc(vtrain_data,vtest_data,vocab_size = 10000):\n    model1 = keras.Sequential()\n    model1.add(keras.layers.Embedding(vocab_size, 16))\n    model1.add(keras.layers.GlobalAveragePooling1D())\n    model1.add(keras.layers.Dense(512, activation=tf.nn.relu))\n    #model.add(keras.layers.Dense(16, activation=tf.nn.relu,activity_regularizer=keras.regularizers.l1(0.001)))\n    #model.add(keras.layers.Dropout(0.2))\n    model1.add(keras.layers.Dense(6, activation=tf.nn.sigmoid))\n    model1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    x_val,partial_x_train,y_val,partial_y_train,test_data = one_hot_word_embedding(vtrain_data,vtest_data)\n    earlystopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    history = model1.fit(x_val,\n                     y_val,\n                     epochs=20,\n                     callbacks=[earlystopper],\n                     batch_size=512,\n                     validation_data=(x_val, y_val),\n                     verbose=1)\n#     results1 = model1.evaluate(x_val, y_val)\n    return (model1,test_data,history)\n\ndef full_model_with_emb_acc(vtrain_data,vtest_data,vocab_size = 10000):\n    model1 = keras.Sequential()\n    model1.add(keras.layers.Embedding(vocab_size, 16))\n    model1.add(keras.layers.GlobalAveragePooling1D())\n    model1.add(keras.layers.Dense(512, activation=tf.nn.relu))\n    #model.add(keras.layers.Dense(16, activation=tf.nn.relu,activity_regularizer=keras.regularizers.l1(0.001)))\n    #model.add(keras.layers.Dropout(0.2))\n    model1.add(keras.layers.Dense(6, activation=tf.nn.sigmoid))\n    model1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\n    partial_x_train,partial_y_train,test_data = full_one_hot_word_embedding(vtrain_data,vtest_data)\n    earlystopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    history = model1.fit(partial_x_train,\n                     partial_y_train,\n                     epochs=20,\n                     callbacks=[earlystopper],\n                     batch_size=1024,\n                     verbose=1)\n#     results1 = model1.evaluate(x_val, y_val)\n    return (model1,test_data,history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1,test_data,his1 = full_model_with_emb_acc(train,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# serialize model to JSON\nmodel_json = model1.to_json()\nwith open(\"my_model1.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \nmodel1.save_weights('my_model1_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### load the model 1 \n# load json and create model\njson_file = open('my_model1.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel1 = tf.keras.models.model_from_json(loaded_model_json)\n# load weights into new model\nmodel1.load_weights(\"my_model1_weights.h5\")\nprint(\"Loaded model from disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def column(matrix, i):\n    return [row[i] for row in matrix]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model1.predict(test_data, batch_size=1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['toxic'] = column(y_pred, 0)\nsubmission['severe_toxic'] = column(y_pred, 1)\nsubmission['obscene'] = column(y_pred, 2)\nsubmission['threat'] = column(y_pred, 3)\nsubmission['insult'] = column(y_pred, 4)\nsubmission['identity_hate'] = column(y_pred, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}