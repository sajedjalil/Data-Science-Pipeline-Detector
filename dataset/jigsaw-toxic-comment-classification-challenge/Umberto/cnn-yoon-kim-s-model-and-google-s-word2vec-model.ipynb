{"cells":[{"metadata":{"_cell_guid":"cdae024e-9d51-46dd-bd28-70c7b4f91425","_uuid":"84b70525e08f2a7d2ea6894583cebc8eb28d5972"},"cell_type":"markdown","source":"In this Kernel I’ll show you a way to use **CNNs** in **NLP**. \nInstead of an image pixels, the input is sentences represented as a matrix. Each row of the matrix is a vector that represents a sentence. This vector is the average of  **word2vec** (Google’s Word2Vec pre-trained model) scores of all words in our sentence.\nFor 10 sentences using a 300-dimensional embedding we would have a 10×300 matrix as our input. \nThat’s our “image”.\n","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1e35fec1-f9a6-4d0d-9f56-a2abcc35d223","collapsed":true,"_uuid":"beac69605322f4a98a30bd6cb7e33111e2a21242","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Flatten, Dropout, Merge\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nimport gensim\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nimport codecs\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nstop_words = set(stopwords.words('english'))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26d1feea-73fc-4438-b042-cbb58dcb6b3b","collapsed":true,"_uuid":"d0ceacafb5f397e76044154734730acd93c68f15","trusted":false},"cell_type":"code","source":"EMBEDDING_DIM = 300 # how big is each word vector\nMAX_VOCAB_SIZE = 175303 # how many unique words to use (i.e num rows in embedding vector)\nMAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n\n#training params\nbatch_size = 256 \nnum_epochs = 2 ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4edd7ec-7ddf-4c83-b3c5-f6dc751c82d4","collapsed":true,"_uuid":"53086d9c9e086a9f66a4dc245dec759787b7bd7a","trusted":false},"cell_type":"code","source":"train_comments = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\", sep=',', header=0)\ntrain_comments.columns=['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nprint(\"num train: \", train_comments.shape[0])\ntrain_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"962c528c-a6a8-4b39-bc09-f83c3dc9a846","collapsed":true,"_uuid":"f1e228fc842d11a46da231b337099d3756f43a1f","trusted":false},"cell_type":"code","source":"label_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny_train = train_comments[label_names].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89a34bbd-3df4-4843-9356-3ef48d5ecaba","collapsed":true,"_uuid":"17ecdd78e8c098cd61e7ad19a5a1d39280983b17","trusted":false},"cell_type":"code","source":"test_comments = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\", sep=',', header=0)\ntest_comments.columns=['id', 'comment_text']\nprint(\"num test: \", test_comments.shape[0])\ntest_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72d3b1e6-9406-4961-b7b0-89c21cf37787","_uuid":"d5dd53b853972acf59c5da3d8fee8812b415b263"},"cell_type":"markdown","source":"**Cleaning Text**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3241b3ff-dfcf-4361-965d-6f7c71c8bfee","collapsed":true,"_uuid":"d2d970eb5d9a4042ffa67b86d6d9732f45a31e68","trusted":false},"cell_type":"code","source":"def standardize_text(df, text_field):\n    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n    df[text_field] = df[text_field].str.lower()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47da37eb-cc1d-40f5-8655-e2fdf6f36c97","collapsed":true,"_uuid":"37157fd7ace420f22c708a616b0245740e8e499d","trusted":false},"cell_type":"code","source":"train_comments.fillna('_NA_')\ntrain_comments = standardize_text(train_comments, \"comment_text\")\ntrain_comments.to_csv(\"train_clean_data.csv\")\ntrain_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df5b86cf-a38a-4bb1-9a7c-95ef9d11f564","collapsed":true,"_uuid":"992f605af050c9d395d42d76cdc6c89ddbd18f14","trusted":false},"cell_type":"code","source":"test_comments.fillna('_NA_')\ntest_comments = standardize_text(test_comments, \"comment_text\")\ntest_comments.to_csv(\"test_clean_data.csv\")\ntest_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41b86444-27b9-476a-a767-b8d5cc3104de","_uuid":"18749b4d4dac9e47a0f2c97e2ea7934889c50b02"},"cell_type":"markdown","source":"**Tokenizing Text**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c2a2dea0-3008-4227-9032-d1d3442a1461","collapsed":true,"_uuid":"5eb646e0533f485c39744bfe380b8ca605c3d843","trusted":false},"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'\\w+')\nclean_train_comments = pd.read_csv(\"train_clean_data.csv\")\nclean_train_comments['comment_text'] = clean_train_comments['comment_text'].astype('str') \nclean_train_comments.dtypes\nclean_train_comments[\"tokens\"] = clean_train_comments[\"comment_text\"].apply(tokenizer.tokenize)\n# delete Stop Words\nclean_train_comments[\"tokens\"] = clean_train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n   \nclean_train_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"230ca18f-3a1e-4077-87ca-b77869f78acc","collapsed":true,"_uuid":"64495855c6f06ecedfa4ea7537cbb80fe5671ce9","trusted":false},"cell_type":"code","source":"clean_test_comments = pd.read_csv(\"test_clean_data.csv\")\nclean_test_comments['comment_text'] = clean_test_comments['comment_text'].astype('str') \nclean_test_comments.dtypes\nclean_test_comments[\"tokens\"] = clean_test_comments[\"comment_text\"].apply(tokenizer.tokenize)\nclean_test_comments[\"tokens\"] = clean_test_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n\nclean_test_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8443ff4f-2bfc-43ab-8051-6d4543909e93","collapsed":true,"_uuid":"5b788b67473bbfb913e35ed7ca5568559331e350","trusted":false},"cell_type":"code","source":"all_training_words = [word for tokens in clean_train_comments[\"tokens\"] for word in tokens]\ntraining_sentence_lengths = [len(tokens) for tokens in clean_train_comments[\"tokens\"]]\nTRAINING_VOCAB = sorted(list(set(all_training_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\nprint(\"Max sentence length is %s\" % max(training_sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"21828cf4-cd50-4045-b779-e46fcd0c7d45","collapsed":true,"_uuid":"5ea92372c340bfc6c4efd7086c026fe96290d5b9","trusted":false},"cell_type":"code","source":"all_test_words = [word for tokens in clean_test_comments[\"tokens\"] for word in tokens]\ntest_sentence_lengths = [len(tokens) for tokens in clean_test_comments[\"tokens\"]]\nTEST_VOCAB = sorted(list(set(all_test_words)))\nprint(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\nprint(\"Max sentence length is %s\" % max(test_sentence_lengths))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b053648-0414-4ba9-82c5-81342903806a","_uuid":"c65f795647a86c1415839d6ed42654eb8f22ba9d"},"cell_type":"markdown","source":"Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence. In this way we lose the syntax of our sentence, while keeping some semantic information.\n![](https://cdn-images-1.medium.com/max/1400/1*THo9NKchWkCAOILvs1eHuQ.png)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a871e0f7-2f49-4f9c-8d32-f0a183d7b439","collapsed":true,"_uuid":"039e8adccac39202f6ee185407e39fec587ae1f5","trusted":false},"cell_type":"code","source":"word2vec_path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n\ndef get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n                                                                                generate_missing=generate_missing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3fb56b5-6045-4995-bf5d-038cb67f467d","collapsed":true,"_uuid":"f199f3c8fa270aab90400934f783712c1e6d2c55","trusted":false},"cell_type":"code","source":"training_embeddings = get_word2vec_embeddings(word2vec, clean_train_comments, generate_missing=True)\n# test_embeddings = get_word2vec_embeddings(word2vec, clean_test_comments, generate_missing=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83fdccfb-9549-40d4-b929-787652a66062","collapsed":true,"_uuid":"71eb13c4238975062db0a5d6f37f3273a3f43d14","trusted":false},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\ntokenizer.fit_on_texts(clean_train_comments[\"comment_text\"].tolist())\ntraining_sequences = tokenizer.texts_to_sequences(clean_train_comments[\"comment_text\"].tolist())\n\ntrain_word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(train_word_index))\n\ntrain_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\nfor word,index in train_word_index.items():\n    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\nprint(train_embedding_weights.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a477520a-f8d2-4ab1-866c-a3a2cebbc7a7","collapsed":true,"_uuid":"190b1764e182887b402bee3af641bbb6adb0c8b5","trusted":false},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(clean_test_comments[\"comment_text\"].tolist())\ntest_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d256e6f-aab4-44ac-953b-be50b586be55","_uuid":"45f5ba479f261f3a05e06f69b78183a46819b953"},"cell_type":"markdown","source":"Define a Convolutional Neural Network following Yoon Kim model [2]","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"54b65983-a6ed-4509-921e-c459840a1040","collapsed":true,"_uuid":"8f89ca5fc4fa41bc002cbac0738dc56cb6d7aca1","trusted":false},"cell_type":"code","source":"def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index, trainable=False, extra_conv=True):\n    \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            weights=[embeddings],\n                            input_length=max_sequence_length,\n                            trainable=trainable)\n\n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n\n    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n    convs = []\n    filter_sizes = [3,4,5]\n\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n        convs.append(l_pool)\n\n    l_merge = Merge(mode='concat', concat_axis=1)(convs)\n\n    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n    pool = MaxPooling1D(pool_size=3)(conv)\n\n    if extra_conv==True:\n        x = Dropout(0.5)(l_merge)  \n    else:\n        # Original Yoon Kim model\n        x = Dropout(0.5)(pool)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    # Finally, we feed the output into a Sigmoid layer.\n    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n    preds = Dense(labels_index, activation='sigmoid')(x)\n\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f32f31b8-eda6-4f59-90ec-14eb9d4a9712","collapsed":true,"_uuid":"1bbb0e66fc5ad6ef4521a9d400db3127969070c6","trusted":false},"cell_type":"code","source":"x_train = train_cnn_data\ny_tr = y_train","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"23c15b32-ef1f-4c6e-8977-161f948470fe","collapsed":true,"_uuid":"65613319e466f72cda5a1d27a35aae620fb040ad","trusted":false},"cell_type":"code","source":"model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n                len(list(label_names)), False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f3b7c3c-cecd-41bc-a62a-cb22ddbebce6","collapsed":true,"_uuid":"843d512895d45ced0a2f9463f4fbabdccacd6d7b","trusted":false},"cell_type":"code","source":"#define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d8225c5-4cc6-4c52-824b-5217e62c7c29","_uuid":"fc16b6d9009948108f5227b0ccf0296edd074bfc"},"cell_type":"markdown","source":"Now let's train our Neural Network","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3d6d73bd-d9e8-494a-88d7-04295a030e3c","collapsed":true,"_uuid":"813df22e089125425c34e1e73ed039c0471ef9c6","trusted":false},"cell_type":"code","source":"hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"984173d7-b21d-4a4d-b613-35f609f13853","collapsed":true,"_uuid":"562cbcd921cdf6c3d575eb6350e5918e83cca81d","trusted":false},"cell_type":"code","source":"y_test = model.predict(test_cnn_data, batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"65022913-00b5-4bb8-a4f6-71fc30406afb","collapsed":true,"_uuid":"0d1849e93ae0ff6acdc7744c1de2814c6c702f6c","trusted":false},"cell_type":"code","source":"#create a submission\nsubmission_df = pd.DataFrame(columns=['id'] + label_names)\nsubmission_df['id'] = test_comments['id'].values \nsubmission_df[label_names] = y_test \nsubmission_df.to_csv(\"./cnn_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ef1bc6c-6d46-4d7f-8900-b833a8165e09","collapsed":true,"_uuid":"8e092255d6d67075be858bfe1f3b3dc731cf1653","trusted":false},"cell_type":"code","source":"#generate plots\nplt.figure()\nplt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c88d722c-1921-41ce-8a8c-ab8efcc4e7a2","collapsed":true,"_uuid":"edf14e826df5db0f231459b9a3a60624b7f31463","trusted":false},"cell_type":"code","source":"plt.figure()\nplt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\nplt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbca83d0-aa29-457f-afff-a7e13b19ddef","_uuid":"ed1776d31f77de38e76530bf190102e271c4117f"},"cell_type":"markdown","source":"**References**:   \n* [1] How to solve 90% of NLP problems: a step-by-step guide\n * https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\n* [2] Yoon Kim model\n * https://arxiv.org/abs/1408.5882\n* [3] Understanding Convolutional Neural Networks for NLP:\n * http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}