{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","file_extension":".py"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"2be48ddb-1f53-4bfd-a5e1-9380f7a6754e","_uuid":"293ac51fbc6c09f8d3b380ad30351d3f9cf2198b"},"cell_type":"markdown","source":"Hi guys,\n\nThis will be a very short example of how we can utilize TFIDF in combination with Chi2 test to find predictive features (and by that I mean filthy words). If you dare, read on...\n# Data Import\nWe'll start by importing the data:"},{"metadata":{"_cell_guid":"080820d8-71b0-46e0-b36c-c3004bec227f","collapsed":true,"_uuid":"c0f57ccd91197d2f43b4299a7b08270699af8e04"},"execution_count":null,"source":"import pandas as pd\n\ntrain = pd.read_csv('../input/train.csv', header = 0)\ntrain.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"406e8070-b5db-4e0d-8255-49a3a706c146","_uuid":"45a07670b1a4d89316de8e83185141f0d2c20a1b"},"cell_type":"markdown","source":"We'll just check if there are any empty fields:"},{"metadata":{"_cell_guid":"ad4e0c2b-4d8a-4260-be60-9f129cca5c8d","collapsed":true,"_uuid":"31d90feabfd409fd52385f4823673823e31fda83"},"execution_count":null,"source":"train.info()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"5b30d4f7-ddc4-4e90-bc45-8d82ea454797","_uuid":"0d2234d546f7f434855e747f6b38fbee39905700"},"cell_type":"markdown","source":"Let's see if we can get some insights into the data by checking some standard metrics on the target fields:"},{"metadata":{"_cell_guid":"3294c951-61aa-4cec-9ac5-368d3de2c965","collapsed":true,"_uuid":"c0884fffda624a9ce5ec8cb50378b698923227ac"},"execution_count":null,"source":"train.describe()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2e8f4edf-0cc3-4675-8c5a-d997b0c3a6bc","_uuid":"60e90c1f1cbcd8d1142a4548a3fb0d451486fb7a"},"cell_type":"markdown","source":"Looks like the mean value for the 'toxic' column is the highest. This means that more comments are labeled as 'toxic' than as 'severe toxic' or any other category. With the limited resources that the kernels provide, it would be best to focus only on predicting for that column.\n\nTo do that, we'll further split our training set into 'train' and 'test' set. This will help us at least partially evaluate our hypothesis."},{"metadata":{"_cell_guid":"9323c61c-51ac-48c6-b7fa-20c1d2a7cd14","collapsed":true,"_uuid":"e7fb7f93509d34f387b4deafdd93c44b08e3e62f"},"execution_count":null,"source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import words\nfrom sklearn.model_selection import train_test_split\n\nX, y = train[['comment_text']], train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.10, random_state = 42)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"4f72a56a-6208-45c3-821f-43170bf0126d","_uuid":"b7482c19219918b3a99f20c1ed2d6ffee9e264ba"},"cell_type":"markdown","source":"# The Vectorizer\nWe'll then instantiate a count vectorizer and create a matrix of all the tokens contained in each comment. The matrix will exclude all English stop words and vectorize only valid English words. This will have some consequences:\n\n* Our algorithm will be optimized for English (other languages will be ignored)\n* Our algorithm will not take into account purposefully misspelled obscenities"},{"metadata":{"_cell_guid":"85a52c7d-5988-4edd-b053-6235bc114891","collapsed":true,"_uuid":"d847fa3b10d91dd84ad1e765c21ce421dbdbfa63"},"execution_count":null,"source":"vectorizer = CountVectorizer(stop_words = 'english',\\\n                             lowercase = True,\\\n                             max_df = 0.95,\\\n                             min_df = 0.05,\\\n                             vocabulary = set(words.words()))\n\nvectorized_text = vectorizer.fit_transform(X_train.comment_text)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cffb6caf-0df8-4d30-91f3-774601756d27","_uuid":"3a3b07f2e8db6ff94f1fd7983f7782a6fc68b9c4"},"cell_type":"markdown","source":"We'll now use our vectorized matrix and run TFIDF on it:"},{"metadata":{"_cell_guid":"8deef4b3-67b7-4b00-b4de-5863f21af2f3","collapsed":true,"_uuid":"30d4310c2f68267df32ae5cc14309821f8cbf8db"},"execution_count":null,"source":"transformer = TfidfTransformer(smooth_idf = False)\ntfidf = transformer.fit_transform(vectorized_text)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"44a3a9b2-6245-48ec-92ae-d865979df70d","_uuid":"6d06d6f8da0854d3e0daa2753ecfe3541f2d090f"},"cell_type":"markdown","source":"Here comes the interesting part, we'll use the weighted matrix terms to select the 200 best predictors of toxic comments. We can expect that those would be quite obscene terms."},{"metadata":{"_cell_guid":"b058239a-5708-4355-89be-17a8811995da","collapsed":true,"_uuid":"4a4587cdad59a5337806836a340cb56aa32bd8b8"},"execution_count":null,"source":"from sklearn.feature_selection import SelectKBest, chi2\n\nch2 = SelectKBest(chi2, k = 200)\nbest_features = ch2.fit_transform(tfidf, y_train.toxic)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"6e68bf98-d791-41f9-89a2-b7ff76e0e8bc","_uuid":"92ce083756499e250b19a1f81999e721e03273a5"},"cell_type":"markdown","source":"Fair warning, the next code snippet wil display the distilled essence of online hatred. Scroll further only if you can stomach it... [Otherwise, jump directly to the next section.](#The-Analyzer)"},{"metadata":{"_cell_guid":"5a538198-bd42-4580-93aa-a78b6107e911","collapsed":true,"_uuid":"61adaf0e854dd5c15df107a97a052dc252056620"},"execution_count":null,"source":"filth = [feature for feature, mask in\\\n         zip(vectorizer.get_feature_names(), ch2.get_support())\\\n         if mask == True]\n\nprint(filth)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"ca213432-de35-48e8-b15b-99c0aa1a9b56","_uuid":"20b33a44430fe23b85c3b37e80ef771b8c45b4d7"},"cell_type":"markdown","source":"# The Analyzer\nWe'll now build a new count vectorizer. We'll call it analyzer (analogous to 2 polarizing glasses) and it will vectorize again our input by only counting the predictive obscenities from above. This will give us a new matrix of n features, where n is the number of predictive words.\n"},{"metadata":{"_cell_guid":"b50bcd4c-ac70-40f1-933a-5227b2731d16","collapsed":true,"_uuid":"f68d027909a2e3b8c55f6793cb209741e56d7c73"},"execution_count":null,"source":"analyzer = CountVectorizer(lowercase = True,\\\n                             vocabulary = filth)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"8f85fc2a-8f03-44c7-8b67-de3b2b0edfb0","_uuid":"048d0ccde5c39f5f1677aa1e70e43865bfbc647f"},"cell_type":"markdown","source":"Now, let's define a function that vectorizes comment texts and weighs the vectors using the already trained TFIDF transformer:"},{"metadata":{"_cell_guid":"1783839d-acfd-4bb9-a535-34b179891d16","collapsed":true,"_uuid":"40a14b799377790f059eab576ae6bab9c0241186"},"execution_count":null,"source":"def get_features(frame):\n    result = pd.DataFrame(\\\n                transformer.fit_transform(\\\n                analyzer.fit_transform(\\\n                frame.comment_text)\\\n                                         ).todense(),\\\n                                            index = frame.index)\n    return result","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"24923534-620c-449d-aa17-42e5dd112414","_uuid":"39f705ac0d91ca14fd1b50a9702c7300d69de202"},"cell_type":"markdown","source":"We'll also define a dictionary which will contain our input train and test data:"},{"metadata":{"_cell_guid":"1a6197c2-a193-414e-954a-a0bc5cad3014","collapsed":true,"_uuid":"4e974fc8abdef35359536a122028efdaec83b2a2"},"execution_count":null,"source":"feature_frames = {}\n\nfor frame in ('train', 'test'):\n    feature_frames[frame] = get_features(eval('X_%s' % frame))\n\nfeature_frames['train'].info()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"158f462d-f5cc-4976-952f-775f6a897709","_uuid":"433a6379306a8c6f89258804f9dbb41e0f986c8b"},"cell_type":"markdown","source":"# Training\nWe can now train our algorithm of choice using the feature frames:"},{"metadata":{"_cell_guid":"7bece7e7-ed06-4d34-9183-69dd44557ab5","collapsed":true,"_uuid":"b2a19f43dcfbf75c9fa1c9d2bd2dffca624a5468"},"execution_count":null,"source":"from sklearn.neighbors import KNeighborsClassifier\n\nknc = KNeighborsClassifier(n_neighbors = 10)\nknc.fit(feature_frames['train'], y_train.toxic)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"aacd3ee7-bb94-41c9-8a17-784196858bc0","_uuid":"089ad6b76bbc7faf24d0dabc347b58de1254cc14"},"cell_type":"markdown","source":"# Log Loss and Conclusion\nFinally, we assess our log loss:"},{"metadata":{"_cell_guid":"139ef0e9-e328-4461-87dd-7448faa7d74b","collapsed":true,"_uuid":"08800dd660c75d6d9cd5ba768310cf5430d809ad"},"execution_count":null,"source":"from sklearn.metrics import log_loss\n\nresult = pd.DataFrame(knc.predict_proba(feature_frames['test']), index = feature_frames['test'].index)\n\nresult['actual'] = y_test.toxic\nresult['text'] = X_test.comment_text\n\nprint(log_loss(y_test.toxic, result[1]))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"33c40ec7-0fe0-4107-8b63-3ab0db89a541","_uuid":"10815c95b5e49e6c61ba922a3dfc8d1186abf414"},"cell_type":"markdown","source":"And here are some examples of predictions and their corresponding comments (again, viewer discretion is advised):"},{"metadata":{"_cell_guid":"a8259dfb-5a79-4f34-b431-9da616c67e79","collapsed":true,"_uuid":"81e175ddd986729fb9ec2aa7be11729a415206f6"},"execution_count":null,"source":"pd.set_option('max_colwidth', 100)\nresult[[1, 'actual', 'text']][(result.actual == 1) & (result[1] > 0.5)][:10]","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b422850a-5387-4092-8f11-ab6f70cd4b30","_uuid":"c5a1609800b7c0a014bce7fae35da6f6384375ab"},"cell_type":"markdown","source":"Afterword:\n\n* In a live system such a model should use additional matching criteria for pursposefully misspelled obscenities (e.g. 'id10t' instead of 'idiot')\n* The model could be improved by using ngrams \n* The model could be improved by using an ensemble of models"}],"nbformat_minor":1}