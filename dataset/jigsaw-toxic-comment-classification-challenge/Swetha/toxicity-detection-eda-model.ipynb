{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Acknowledgements**\n\nI thank the following kernels for inspiration :[](http://)\n\n1. https://www.kaggle.com/jagangupta/stop-the-s-toxic-comments-eda\n2. https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline-eda-0-052-lb\n3. https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nBeing anonymous over the internet can sometimes make people say nasty things that they normally would not in real life. Let's filter out the hate from our platforms one comment at a time."},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\n#from scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\n#settings\nstart_time=time.time()\ncolor = sns.color_palette()\nsns.set_style(\"dark\")\neng_stopwords = set(stopwords.words(\"english\"))\nwarnings.filterwarnings(\"ignore\")\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"!du -l ../input/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\n\nzf = zipfile.ZipFile('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip') \ntrain = pd.read_csv(zf.open('train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_zf = zipfile.ZipFile('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip') \ntest = pd.read_csv(test_zf.open('test.csv'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Review data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#take a peak\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Example comments"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"toxic:\")\nprint(train[train.toxic==1].iloc[3,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"severe toxic:\")\nprint(train[train.severe_toxic==1].iloc[3,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for Class imbalance"},{"metadata":{"trusted":true},"cell_type":"code","source":"columnsums=train.iloc[:,2:].sum()\n#marking comments without any tags as \"clean\"\nrowsums=train.iloc[:,2:].sum(axis=1)\ntrain['clean']=(rowsums==0)\n#count number of clean entries\ntrain['clean'].sum()\nprint(\"Total comments = \",len(train))\nprint(\"Total clean comments = \",train['clean'].sum())\nprint(\"Total tags =\",columnsums.sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_check=train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['comment_text'].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_check=test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_check","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize | Toxicity spread"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train.iloc[:,2:].sum()\n#plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"# per class\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize | Multi tagging - how many comments have multiple tags?"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=rowsums.value_counts()\n\n#plot\nplt.figure(figsize=(8,4))\nax = sns.barplot(x.index, x.values, alpha=0.8,color=color[2])\nplt.title(\"Multiple tags per comment\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of tags ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize | Word Cloud"},{"metadata":{},"cell_type":"markdown","source":"Chart Desc: The visuals here are word clouds (ie) more frequent words appear bigger. A cool way to create word clouds with funky pics is given here. It involves the following steps.\n\n* Search for an image and its base 64 encoding\n* Paste encoding in a cell and convert it using codecs package to image\n* Create word cloud with the new image as a mask\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean comments\nclean_mask=np.array(Image.open(\"../input/imagesforkernal/safe-zone.png\"))\nclean_mask=clean_mask[:,:,1]\n#wordcloud for clean comments\nsubset=train[train.clean==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=set(STOPWORDS))\nwc.generate(\" \".join(text))\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Clean Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n\n**Direct features:**\n\nFeatures which are a directly due to words/content.We would be exploring the following techniques\n\n* Word frequency features\n* Count features\n* Bigrams\n* Trigrams\n* Vector distance mapping of words (Eg: Word2Vec)\n* Sentiment scores\n"},{"metadata":{},"cell_type":"markdown","source":"**Indirect features**\n\nSome more experimental features.\n\n* count of sentences\n* count of words\n* count of unique words\n* count of letters\n* count of punctuations\n* count of uppercase words/letters\n* count of stop words\n* Avg length of each word\n* Word count percent in each comment\n* Punct percent in each comment"},{"metadata":{"trusted":true},"cell_type":"code","source":"merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])\ndf=merge.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['count_sent']=df[\"comment_text\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n#Word count in each comment:\ndf['count_word']=df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n#Unique word count\ndf['count_unique_word']=df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n#Letter count\ndf['count_letters']=df[\"comment_text\"].apply(lambda x: len(str(x)))\n#punctuation count\ndf[\"count_punctuations\"] =df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n#upper case words count\ndf[\"count_words_upper\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n#title case words count\ndf[\"count_words_title\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n#Number of stopwords\ndf[\"count_stopwords\"] = df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n#Average length of the words\ndf[\"mean_word_len\"] = df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n#Word count percent in each comment:\ndf['word_unique_percent']=df['count_unique_word']*100/df['count_word']\n#derived features\n#Punct percent in each comment:\ndf['punct_percent']=df['count_punctuations']*100/df['count_word']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seperate train and test features\ntrain_feats=df.iloc[0:len(train),]\ntest_feats=df.iloc[len(train):,]\n#join the tags\ntrain_tags=train.iloc[:,2:]\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA on engineered features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feats['count_sent'].loc[train_feats['count_sent']>10] = 10 \nplt.figure(figsize=(12,6))\n## sentenses\nplt.subplot(121)\nplt.suptitle(\"Are longer comments more toxic?\",fontsize=20)\nsns.violinplot(y='count_sent',x='clean', data=train_feats,split=True)\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of sentences', fontsize=12)\nplt.title(\"Number of sentences in each comment\", fontsize=15)\n# words\ntrain_feats['count_word'].loc[train_feats['count_word']>200] = 200\nplt.subplot(122)\nsns.violinplot(y='count_word',x='clean', data=train_feats,split=True,inner=\"quart\")\nplt.xlabel('Clean?', fontsize=12)\nplt.ylabel('# of words', fontsize=12)\nplt.title(\"Number of words in each comment\", fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Long sentences or more words do not seem to be a significant indicator of toxicity.\n\nChart desc: Violin plot is an alternative to the traditional box plot. The inner markings show the percentiles while the width of the \"violin\" shows the volume of comments at that level/instance."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feats['count_unique_word'].loc[train_feats['count_unique_word']>200] = 200\n#prep for split violin plots\n#For the desired plots , the data must be in long format\ntemp_df = pd.melt(train_feats, value_vars=['count_word', 'count_unique_word'], id_vars='clean')\n#spammers - comments with less than 40% unique words\nspammers=train_feats[train_feats['word_unique_percent']<30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nplt.suptitle(\"What's so unique ?\",fontsize=20)\ngridspec.GridSpec(2,2)\nplt.subplot2grid((2,2),(0,0))\nsns.violinplot(x='variable', y='value', hue='clean', data=temp_df,split=True,inner='quartile')\nplt.title(\"Absolute wordcount and unique words count\")\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Count', fontsize=12)\n\nplt.subplot2grid((2,2),(0,1))\nplt.title(\"Percentage of unique words of total words in comment\")\n#sns.boxplot(x='clean', y='word_unique_percent', data=train_feats)\nax=sns.kdeplot(train_feats[train_feats.clean == 0].word_unique_percent, label=\"Bad\",shade=True,color='r')\nax=sns.kdeplot(train_feats[train_feats.clean == 1].word_unique_percent, label=\"Clean\")\nplt.legend()\nplt.ylabel('Number of occurances', fontsize=12)\nplt.xlabel('Percent unique words', fontsize=12)\n\nx=spammers.iloc[:,-7:].sum()\nplt.subplot2grid((2,2),(1,0),colspan=2)\nplt.title(\"Count of comments with low(<30%) unique words\",fontsize=15)\nax=sns.barplot(x=x.index, y=x.values,color=color[3])\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n\nplt.xlabel('Threat class', fontsize=12)\nplt.ylabel('# of comments', fontsize=12)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word count VS unique word count:\nThere are noticeable shifts in the mean of both word count and unique word count across clean and toxic comments.\n\nChart desc: The first chart is a split violin chart. It is a variation of the traditional box chart/violin chart which allows us to split the violin in the middle based on a categorical variable.\nUnique word count percent:\nThere is a bulge near the 0-10% mark which indicates a large number of toxic comments which contain very little variety of words.\n\nChart desc: The second chart is an overlay of two kernel density estimation plots of percentage of unique words out of all the words in the comment, done for both clean and toxic comments\nEven though the number of clean comments dominates the dataset(~90%), there are only 75 clean comments that are spam, which makes it a powerful indicator of a toxic comment.\n\n## Finding : Spammers are more toxic!\nNo surprises here. Let's take a look at some clean and toxic spam messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Clean Spam example:\")\nprint(spammers[spammers.clean==1].comment_text.iloc[1])\nprint(\"Toxic Spam example:\")\nprint(spammers[spammers.toxic==1].comment_text.iloc[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding : Spam is toxic to the model too!\nThese spam entries are bad if we design our model to contain normal word counts features. Imagine the scenario in which our model picked up the words \"mitt romney\" from any comment and classified it as toxic :("},{"metadata":{},"cell_type":"markdown","source":"## Clean Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n# Aphost lookup dict\nAPPO = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)\n    # remove leaky elements like ip,user\n    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n    #removing usernames\n    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n    \n    #Split the sentences into words\n    words=tokenizer.tokenize(comment)\n    \n    # (')aphostophe  replacement (ie)   you're --> you are  \n    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n    words=[APPO[word] if word in APPO else word for word in words]\n    words=[lem.lemmatize(word, \"v\") for word in words]\n    words = [w for w in words if not w in eng_stopwords]\n    \n    clean_sent=\" \".join(words)\n    # remove any non alphanum,digit character\n    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n    return(clean_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=merge.comment_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean(corpus.iloc[12235])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_corpus=corpus.apply(lambda x :clean(x))\n\nend_time=time.time()\nprint(\"total time till Cleaning\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFIDF : Unigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Unigrams -- TF-IDF \n# using settings recommended here for TF-IDF -- https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n\n#some detailed description of the parameters\n# min_df=10 --- ignore terms that appear lesser than 10 times \n# max_features=None  --- Create as many words as present in the text corpus\n    # changing max_features to 10k for memmory issues\n# analyzer='word'  --- Create features from words (alternatively char can also be used)\n# ngram_range=(1,1)  --- Use only one word at a time (unigrams)\n# strip_accents='unicode' -- removes accents\n# use_idf=1,smooth_idf=1 --- enable IDF\n# sublinear_tf=1   --- Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf)\n\n\n#temp settings to min=200 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\nstart_unigrams=time.time()\ntfv = TfidfVectorizer(min_df=200,  max_features=10000, \n            strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\n\ntrain_unigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_unigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://buhrmann.github.io/tfidf-analysis.html\ndef top_tfidf_feats(row, features, top_n=25):\n    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats)\n    df.columns = ['feature', 'tfidf']\n    return df\n\ndef top_feats_in_doc(Xtr, features, row_id, top_n=25):\n    ''' Top tfidf features in specific document (matrix row) '''\n    row = np.squeeze(Xtr[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndef top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n    ''' Return the top n features that on average are most important amongst documents in rows\n        indentified by indices in grp_ids. '''\n    \n    D = Xtr[grp_ids].toarray()\n\n    D[D < min_tfidf] = 0\n    tfidf_means = np.mean(D, axis=0)\n    return top_tfidf_feats(tfidf_means, features, top_n)\n\n# modified for multilabel milticlass\ndef top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n        calculated across documents with the same class label. '''\n    dfs = []\n    cols=train_tags.columns\n    for col in cols:\n        ids = train_tags.index[train_tags[col]==1]\n        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n        feats_df.label = label\n        dfs.append(feats_df)\n    return dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tags.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_top_n_per_lass=top_feats_by_class(train_unigrams,features)\n\nend_unigrams=time.time()\n\nprint(\"total time in unigrams\",end_unigrams-start_unigrams)\nprint(\"total time till unigrams\",end_unigrams-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\ngridspec.GridSpec(4,2)\nplt.subplot2grid((4,2),(0,0))\nsns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:9],tfidf_top_n_per_lass[0].tfidf.iloc[0:9],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((4,2),(0,1))\nsns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFIDF Bigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"#temp settings to min=150 to facilitate top features section to run in kernals\n#change back to min=10 to get better results\ntfv = TfidfVectorizer(min_df=150,  max_features=30000, \n            strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n            use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\ntfv.fit(clean_corpus)\nfeatures = np.array(tfv.get_feature_names())\ntrain_bigrams =  tfv.transform(clean_corpus.iloc[:train.shape[0]])\ntest_bigrams = tfv.transform(clean_corpus.iloc[train.shape[0]:])\n#get top n for bigrams\ntfidf_top_n_per_lass=top_feats_by_class(train_bigrams,features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,22))\nplt.suptitle(\"TF_IDF Top words per class(Bigrams)\",fontsize=20)\ngridspec.GridSpec(4,2)\nplt.subplot2grid((4,2),(0,0))\nsns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:5],tfidf_top_n_per_lass[0].tfidf.iloc[0:5],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n\nplt.subplot2grid((4,2),(0,1))\nsns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:5],tfidf_top_n_per_lass[1].tfidf.iloc[0:5],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nplt.xlabel('Word', fontsize=12)\nplt.ylabel('TF-IDF score', fontsize=12)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Model using Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n    \n# model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SELECTED_COLS=['count_sent', 'count_word', 'count_unique_word',\n       'count_letters', 'count_punctuations', 'count_words_upper',\n       'count_words_title', 'count_stopwords', 'mean_word_len',\n       'word_unique_percent', 'punct_percent']\ntarget_x=train_feats[SELECTED_COLS]\n# target_x\n\nTARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntarget_y=train_tags[TARGET_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding indirect features + direct text features using sparse hstack"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Using only Indirect features\")\nmodel = LogisticRegression(C=3) #Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\nX_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2019)\ntrain_loss = []\nvalid_loss = []\nimportance=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_train = np.zeros((y_train.shape[0],y_train.shape[1]))\npreds_valid = np.zeros((y_valid.shape[0],y_valid.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    importance.append(model.coef_)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\nend_time=time.time()\nprint(\"total time till Indirect feat model\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,22))\nplt.suptitle(\"Feature importance for indirect features\",fontsize=20)\ngridspec.GridSpec(3,2)\nplt.subplots_adjust(hspace=0.4)\nplt.subplot2grid((3,2),(0,0))\nsns.barplot(SELECTED_COLS,importance[0][0],color=color[0])\nplt.title(\"class : Toxic\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((3,2),(0,1))\nsns.barplot(SELECTED_COLS,importance[1][0],color=color[1])\nplt.title(\"class : Severe toxic\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((3,2),(1,0))\nsns.barplot(SELECTED_COLS,importance[2][0],color=color[2])\nplt.title(\"class : Obscene\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\n\nplt.subplot2grid((3,2),(1,1))\nsns.barplot(SELECTED_COLS,importance[3][0],color=color[3])\nplt.title(\"class : Threat\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,0))\nsns.barplot(SELECTED_COLS,importance[4][0],color=color[4])\nplt.title(\"class : Insult\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\n\nplt.subplot2grid((3,2),(2,1))\nsns.barplot(SELECTED_COLS,importance[5][0],color=color[5])\nplt.title(\"class : Identity hate\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=45)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.subplot2grid((4,2),(3,0),colspan=2)\nsns.barplot(SELECTED_COLS,importance[6][0],color=color[0])\nplt.title(\"class : Clean\",fontsize=15)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=90)\nplt.xlabel('Feature', fontsize=12)\nplt.ylabel('Importance', fontsize=12)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import csr_matrix, hstack\n\n#Using all direct features\nprint(\"Using all features\")\ntarget_x = hstack((train_bigrams,train_unigrams,train_feats[SELECTED_COLS])).tocsr()\n\n\nend_time=time.time()\nprint(\"total time till Sparse mat creation\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Classifier(C=4, dual=True, n_jobs=-1)\nX_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\ntrain_loss = []\nvalid_loss = []\npreds_train = np.zeros((y_train.shape[0],y_train.shape[1]))\npreds_valid = np.zeros((y_valid.shape[0],y_valid.shape[1]))\nfor i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n    print(metrics.classification_report(y_train[j], (preds_train[:,i] > 0.5).astype(int), digits=3)) \n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n\n\nend_time=time.time()\nprint(\"total time till NB base model creation\",end_time-start_time)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Result : Precision is ok, Recall is low all across the target classes, we need to improve it"},{"metadata":{},"cell_type":"markdown","source":"## First Submission "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_feats = test_feats.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_feats.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = hstack((test_bigrams,test_unigrams,test_feats[SELECTED_COLS])).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_zf = zipfile.ZipFile('/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip') \nsubmission = pd.read_csv(submission_zf.open('sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_df = pd.DataFrame(X_test.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test = np.zeros((X_test.shape[0],6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    preds_test[:,i] = model.predict_proba(X_test)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submid = pd.DataFrame({'id': submission[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds_test, columns = TARGET_COLS)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}