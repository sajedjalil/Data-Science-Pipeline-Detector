{"cells":[{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnp.random.seed(42)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport sys\nfrom keras import backend as K\nfrom keras.layers import Dense,Input, LSTM, Bidirectional, Embedding, TimeDistributed, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras import losses\nfrom keras import initializers as initializers, regularizers, constraints\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nimport nltk\nimport re\nfrom keras.engine.topology import Layer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e8bd3575-f711-4ca6-a653-8ec1c74c0204","_uuid":"cf43ac37cbd14d8baa088648c2275123550135d6","trusted":false},"cell_type":"code","source":"train[\"comment_text\"].fillna(\"fillna\")\ntest[\"comment_text\"].fillna(\"fillna\")\nX_train = train[\"comment_text\"].str.lower()\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\nX_test = test[\"comment_text\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"109e2eaa-3e0a-4946-8d5b-f2e280b6cf82","_uuid":"52d244d1cc11be42e0fafcebf67efae787dc0839","trusted":false},"cell_type":"code","source":"X_train = list(X_train)\nX_test = list(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"79aff1ae-b342-47d7-a912-24418b96e0e7","_uuid":"f5076a2f3496e0fa1baf7e095724132e45f9ed11","trusted":false},"cell_type":"code","source":"def remove_noise(input_text):\n    text = re.sub('\\(talk\\)(.*)\\(utc\\)','',input_text)\n    text = text.split()\n    text = [re.sub('[\\d]+','',x) for x in text]\n    return ' '.join(text)\n\n\nfor i in range(len(X_train)):\n    X_train[i] = remove_noise(X_train[i])\nfor i in range(len(X_test)):\n    X_test[i] = remove_noise(X_test[i])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"43173dca-e503-4bc9-8a82-b4b6a6fb806c","_uuid":"d7a87d9b68c9e2a2a12d7046936bd850b64e55f3","trusted":false},"cell_type":"code","source":"def replace_word(X):\n    repl = {\n        \"&lt;3\": \" good \",\":d\": \" good \",\":dd\": \" good \",\":p\": \" good \",\"8)\": \" good \",\":-)\": \" good \", \":)\": \" good \",\";)\": \" good \",\n        \"(-:\": \" good \",\"(:\": \" good \",\"yay!\": \" good \",\"yay\": \" good \",\"yaay\": \" good \",\"yaaay\": \" good \",\"yaaaay\": \" good \",\n        \"yaaaaay\": \" good \",\":/\": \" bad \",\":&gt;\": \" sad \",\":')\": \" sad \",\":-(\": \" bad \",\":(\": \" bad \", \":s\": \" bad \",\":-s\": \" bad \",\n        \"&lt;3\": \" heart \",\":d\": \" smile \",\":p\": \" smile \",\":dd\": \" smile \",\"8)\": \" smile \", \":-)\": \" smile \", \":)\": \" smile \",\n        \";)\": \" smile \",\"(-:\": \" smile \",\"(:\": \" smile \",\":/\": \" worry \",\":&gt;\": \" angry \", \":')\": \" sad \",\":-(\": \" sad \",\":(\": \" sad \",\n        \":s\": \" sad \", \":-s\": \" sad \",r\"\\br\\b\": \"are\",r\"\\bu\\b\": \"you\",r\"\\bhaha\\b\": \"ha\",r\"\\bhahaha\\b\": \"ha\",r\"\\bdon't\\b\": \"do not\",\n        r\"\\bdoesn't\\b\": \"does not\",r\"\\bdidn't\\b\": \"did not\",r\"\\bhasn't\\b\": \"has not\",r\"\\bhaven't\\b\": \"have not\",r\"\\bhadn't\\b\": \"had not\",\n        r\"\\bwon't\\b\": \"will not\",r\"\\bwouldn't\\b\": \"would not\",r\"\\bcan't\\b\": \"can not\",r\"\\bcannot\\b\": \"can not\",r\"\\bi'm\\b\": \"i am\",\n        \"m\": \"am\",\"r\": \"are\",\"u\": \"you\",\"haha\": \"ha\",\"hahaha\": \"ha\",\"don't\": \"do not\",\"doesn't\": \"does not\",\"didn't\": \"did not\",\n        \"hasn't\": \"has not\",\"haven't\": \"have not\",\"hadn't\": \"had not\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"can't\": \"can not\",\n        \"cannot\": \"can not\",\"i'm\": \"i am\",\"m\": \"am\",\"i'll\" : \"i will\",\"its\" : \"it is\",\"it's\" : \"it is\",\"'s\" : \" is\",\"that's\" : \"that is\",\n        \"weren't\" : \"were not\"\n    }\n    keys = repl.keys()\n    new_X = []\n    for i in X:\n        arr = str(i).split()\n        xx = \"\"\n        for j in arr:\n            j = str(j).lower()\n            if j[:4] == 'http' or j[:3] == 'www':\n                continue\n            if j in keys:\n                j = repl[j]\n            xx += j + \" \"\n        new_X.append(xx)\n    return new_X\n\nX_train = replace_word(X_train)\nX_test = replace_word(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"da409613-3688-4d2e-a072-f67dee02617b","_uuid":"efad6a0ecd758a759f14287a69bfd9cafa8c8fb2","trusted":false},"cell_type":"code","source":"max_features=200000\nmax_senten_len=30\nmax_senten_num=10\nembed_size=300","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6f560b8e-fa4f-400f-b415-337ffc04f8cb","_uuid":"799fd6b51a1415a4714ddaebf7d6435380c7c628","trusted":false},"cell_type":"code","source":"def filt_sent(X,max_senten_num):\n    X_sent = []\n    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n    for paragraph in X:\n        raw = sent_tokenizer.tokenize(paragraph)\n        filt = []\n        min_sent_len = 5 if len(raw) <= 10 else 10\n        for sentence in raw:\n            if len(sentence.split()) >= min_sent_len and len(filt) < max_senten_num:\n                filt.append(sentence)\n        while len(filt) < max_senten_num:\n            filt.append('nosentence')\n        X_sent.append(filt)\n    return X_sent","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"eed8323c-9d4e-4d39-b07a-dd91d915c2dc","_uuid":"6d479f5e37f5990ea13f8cfbf69f4be9d3ff2f90","trusted":false},"cell_type":"code","source":"X_train_sent = filt_sent(X_train ,max_senten_num)\nX_test_sent = filt_sent(X_test, max_senten_num)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7a665c08-b4a9-4792-b40b-481b3da907e5","_uuid":"b07e998ccedaf3aaaf4b4e67b207ad5490eb24f7","trusted":false},"cell_type":"code","source":"tok=text.Tokenizer(num_words=max_features,lower=True)\ntok.fit_on_texts(list(X_train)+list(X_test))\nfor i in range(len(X_train_sent)):\n        X_train_sent[i] = tok.texts_to_sequences(X_train_sent[i])\n        X_train_sent[i] = sequence.pad_sequences(X_train_sent[i],maxlen=max_senten_len)\nfor i in range(len(X_test_sent)):\n        X_test_sent[i] = tok.texts_to_sequences(X_test_sent[i])\n        X_test_sent[i] = sequence.pad_sequences(X_test_sent[i],maxlen=max_senten_len)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9e57a7cb-c061-4361-bbe2-05c0486a3f18","_uuid":"9488bc9d68dfd1fde1f99d23a9f1ed7b30ceb87f","trusted":false},"cell_type":"code","source":"embeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e2490100-fc9c-4e46-ae84-7dfa65fcddba","_uuid":"d56ad119931a971b2588355deb726a045764c9ad","trusted":false},"cell_type":"code","source":"word_index = tok.word_index\n#prepare embedding matrix\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"43944725-1842-4093-a2fd-8701d09c9200","_uuid":"269daf8e81503a014262929fbe984b4a15984df5","trusted":false},"cell_type":"code","source":"def dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f27e0264-d863-450a-902a-395f265242f3","_uuid":"78f033922530bb7687ca2a65179c9c17802753a3","trusted":false},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"fc02de57-3ce5-4f78-b66d-15538b57f90e","_uuid":"19ffa659d304c2a406e566ca0abfb083dfad75a6","trusted":false},"cell_type":"code","source":"embedding_layer = Embedding(max_features,\n                            embed_size,\n                            input_length=max_senten_len,\n                            weights=[embedding_matrix])\n\nword_input = Input(shape=(max_senten_len,), dtype='int32')\nword = embedding_layer(word_input)\nword = SpatialDropout1D(0.2)(word)\nword = Bidirectional(LSTM(128, return_sequences=True))(word)\nword_out = AttentionWithContext()(word)\nwordEncoder = Model(word_input, word_out)\n\nsente_input = Input(shape=(max_senten_num, max_senten_len), dtype='int32')\nsente = TimeDistributed(wordEncoder)(sente_input)\nsente = SpatialDropout1D(0.2)(sente)\nsente = Bidirectional(LSTM(128, return_sequences=True))(sente)\nsente = AttentionWithContext()(sente)\npreds = Dense(6, activation='sigmoid')(sente)\nmodel = Model(sente_input, preds)\nopt = Adam(clipnorm=5.0)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=opt,\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"90727db5-d597-4100-9e65-aba6e5d75791","_uuid":"4861f4e238ebb34e1b1a09822ae52ed8081a5d91","trusted":false},"cell_type":"code","source":"X_train_sent = np.asarray(X_train_sent)\nX_test_sent = np.asarray(X_test_sent)\nprint('Shape of data tensor:', X_train_sent.shape)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"58d507b5-2741-4f49-a547-253504370475","_uuid":"4a8a9f1a9b4a16e7d4b0d6867758ffec0d9e2371","trusted":false},"cell_type":"code","source":"batch_size = 256\nepochs = 3\n\nX_tra, X_val, y_tra, y_val = train_test_split(X_train_sent, y_train, train_size=0.95, random_state=233)\nRocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\nfilepath=\"weights_base.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),callbacks = callbacks_list, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":false,"_cell_guid":"265115a8-296e-4b67-a6fc-02d0a584b501","_uuid":"f7377e50952ffb6cc14bab3b34442788864eed68","trusted":false},"cell_type":"code","source":"# y_pred = model.predict(x_test,batch_size=1024,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5c703574-cf76-495f-b800-1fc6c9384ded","_uuid":"ddc5afff4b22841fb184e32cc4b19a2225dec451","trusted":false},"cell_type":"code","source":"# submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n# submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"78885dfc-dcbc-45f8-8be9-4ca9249f7986","_uuid":"0f0aa330f8e577d28e1561be894156b4502dc06e","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}