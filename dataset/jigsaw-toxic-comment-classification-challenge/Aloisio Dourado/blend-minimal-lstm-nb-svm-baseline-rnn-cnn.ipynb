{"cells":[{"source":"## Introduction","metadata":{"_cell_guid":"d1aed5c1-2284-4aa0-9bdd-8b3a87f10455","_uuid":"6c82c5ade1ea2ebe3115b7acdbcb5eb01b8dc604"},"cell_type":"markdown"},{"source":"There are two very different strong baselines currently in the kernels for this competition:\n    \n- An *LSTM* model, which uses a recurrent neural network to model state across each text, with no feature engineering\n- An *NB-SVM* inspired model, which uses a simple linear approach on top of naive bayes features\n\nIn theory, an ensemble works best when the individual models are as different as possible. Therefore, we should see that even a simple average of these two models gets a good result. Let's try it! First, we'll load the outputs of the models (in the Kaggle Kernels environment you can add these as input files directly from the UI; otherwise you'll need to download them first).","metadata":{"_cell_guid":"5a738023-69cc-4acc-a92b-53c338dd1ab3","_uuid":"c8b9bb50ea3e069843370be618e835cd53a2dc5e"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"4f49e4ea-a226-4d61-921b-db33c41453a8","_uuid":"d3acc5945691f3ac94b41a99efd00d2f794b28d9"},"cell_type":"code","source":"import numpy as np, pandas as pd\n\nf_lstm = '../input/improved-lstm-baseline-glove-dropout-lb-0-048/submission.csv'\nf_nbsvm = '../input/nb-svm-strong-linear-baseline-eda-0-052-lb/submission.csv'\nf_cnnrnn = '../input/keras-cnn-rnn-0-051-lb/baseline.csv'"},{"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"55a8085b-7a52-4c20-975d-4f6fe0dc7202","_uuid":"12511977319d644ed8a07af913b0f8e00b14f22f"},"cell_type":"code","source":"p_lstm = pd.read_csv(f_lstm)\np_nbsvm = pd.read_csv(f_nbsvm)\np_cnnrnn = pd.read_csv(f_nbsvm)"},{"source":"Now we can take the average of the label columns.","metadata":{"_cell_guid":"74113048-4055-4ead-98b8-a003b6b075bd","_uuid":"756837877eacd2ab5312623640c66e156a75c1fe"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"c736d49f-de3b-46f5-b4c3-04a26ac98d31","_uuid":"d3a16c6bb660eb600e68c11c03bd2d841b7515f4"},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\np_res = p_lstm.copy()\np_res[label_cols] = (p_cnnrnn[label_cols]*1 + p_nbsvm[label_cols]*3 + p_lstm[label_cols]*6) / 10"},{"source":"And finally, create our CSV.","metadata":{"_cell_guid":"d4359384-5657-4660-8fd9-6b395bb7c2a6","_uuid":"807d5f936cc49ccff2e4edc28287be06b008beb1"},"cell_type":"markdown"},{"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"_cell_guid":"5f73aafe-81a9-4149-b879-717fee5f1814","_uuid":"30905be21b812c74c8bb1e3469134b226cbcfc78"},"cell_type":"code","source":"p_res.to_csv('submission.csv', index=False)"},{"source":"As we hoped, when we submit this to Kaggle, we get a great result - a 0.44 score, compared to the individual scores of 0.48 and 0.52. This is currently the best Kaggle kernel submission that runs within the kernels sandbox!","metadata":{"_cell_guid":"d66ecdcc-6339-48d5-94ad-519187078972","_uuid":"648c18c7829885667bbb79a4ed95b82cb5cc620d"},"cell_type":"markdown"}],"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.3","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4}