{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom pymagnitude import *\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import RNN, GRU, LSTM, Dense, Input, Embedding, Dropout, Activation, concatenate\nfrom keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nfrom keras.callbacks import Callback\nfrom keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, SimpleRNN\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport os\nimport nltk\nimport re\nfrom bs4 import BeautifulSoup\nimport urllib3\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport itertools\nfrom sklearn import preprocessing\nfrom scipy import sparse\nfrom keras import backend as K # Importing Keras backend (by default it is Tensorflow)\nfrom keras.layers import Input, Dense # Layers to be used for building our model\nfrom keras.models import Model # The class used to create a model\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils # Utilities to manipulate numpy arrays\nfrom tensorflow import set_random_seed # Used for reproducible experiments\nfrom tensorflow import keras\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.layers import InputLayer, Input, Embedding, Dense, Dropout, Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, SpatialDropout1D, Conv1D, CuDNNLSTM, CuDNNGRU, TimeDistributed, Reshape, Permute, LocallyConnected1D, concatenate, ELU, Activation, add, Lambda, BatchNormalization, PReLU, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.optimizers import Adam\nfrom keras import regularizers\n#from kgutil.models.keras.base import DefaultTrainSequence, DefaultTestSequence\n#from kgutil.models.keras.rnn import KerasRNN, load_emb_matrix\nfrom copy import deepcopy\nimport inspect\n\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read/Transformat data\n- Read dataset\n- Split comments(x) and categories(y)\n- Tokenize all the comment (take the max_features most frequent words of all the comments)\n- Pad each comment to max_len"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train_data[classes].values\n\ntrain_sentences = train_data[\"comment_text\"].fillna(\"fillna\").str.lower()\ntest_sentences = test_data[\"comment_text\"].fillna(\"fillna\").str.lower()\n\nmax_features = 150000\nmax_len = 150\nembed_size = 300\n\ntokenizer = Tokenizer(max_features)\ntokenizer.fit_on_texts(list(train_sentences))\n\ntokenized_train_sentences = tokenizer.texts_to_sequences(train_sentences)\ntokenized_test_sentences = tokenizer.texts_to_sequences(test_sentences)\n\ntrain_padding = pad_sequences(tokenized_train_sentences, max_len)\ntest_padding = pad_sequences(tokenized_test_sentences, max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create embeddings matrix\n- Download embeddings with Magnitude libray\n- Create an embedding_matrix dims: number_of_words x embeddings.dim with zero values\n- Fill the embedding_matrix with the embeddings with .query() Magnitude's function"},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_len = 150\n#https://github.com/plasticityai/magnitude\n#!curl -s http://magnitude.plasticity.ai/glove+subword/glove.6B.300d.magnitude --output vectors.magnitude\n\n#vecs_word2vec = Magnitude('http://magnitude.plasticity.ai/word2vec/heavy/GoogleNews-vectors-negative300.magnitude', stream=True, pad_to_length=max_len) \nvecs_glove = Magnitude('http://magnitude.plasticity.ai/glove+subword/glove.6B.300d.magnitude')\nvecs_fasttext = Magnitude('http://magnitude.plasticity.ai/fasttext+subword/wiki-news-300d-1M.magnitude', pad_to_length=max_len)\n#vecs_elmo = Magnitude('http://magnitude.plasticity.ai/elmo/medium/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.magnitude', stream=True, pad_to_length=max_len)\n\n#vectors = Magnitude(vecs_fasttext, vecs_glove) # concatenate word2vec with glove","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, vecs_glove.dim))\n\nfrom tqdm import tqdm_notebook as tqdm\nfor word, i in tqdm(word_index.items()):\n    if i >= max_features:\n        continue\n    embedding_vector = vecs_glove.query(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        embedding_matrix[i] = np.random.uniform(-0.25, 0.25, embed_size)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Helpers\nInitialize the custome classes/functions that we'll need for our models\n\n- RocAuc metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/yekenot/pooled-gru-fasttext\n\n#Define a class for model evaluation\nclass RocAucEvaluation(Callback):\n    def __init__(self, training_data=(),validation_data=()):\n        super(Callback, self).__init__()\n       \n        self.X_tra, self.y_tra = training_data\n        self.X_val, self.y_val = validation_data\n        self.aucs_val = []\n        self.aucs_tra = []\n        \n    def on_epoch_end(self, epoch, logs={}):                   \n        y_pred_val = self.model.predict(self.X_val, verbose=0)\n        score_val = roc_auc_score(self.y_val, y_pred_val)\n\n        y_pred_tra = self.model.predict(self.X_tra, verbose=0)\n        score_tra = roc_auc_score(self.y_tra, y_pred_tra)\n\n        self.aucs_tra.append(score_tra)\n        self.aucs_val.append(score_val)\n        print(\"\\n ROC-AUC - epoch: %d - score_tra: %.6f - score_val: %.6f \\n\" % (epoch+1, score_tra, score_val))\n\ndef recall(y_true, y_pred):    \n    \"\"\"\n    Recall metric.\n    Only computes a batch-wise average of recall.\n    Computes the recall, a metric for multi-label classification of\n    how many relevant items are selected.\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n\ndef precision(y_true, y_pred):    \n    \"\"\"\n    Precision metric.\n    Only computes a batch-wise average of precision.\n    Computes the precision, a metric for multi-label classification of\n    how many selected items are relevant.\n    Source\n    ------\n    https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n    \"\"\"\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n\ndef f1(y_true, y_pred):\n    \n    \"\"\"Calculate the F1 score.\"\"\"\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * ((p * r) / (p + r))\n\n\ndef accuracy(y_true, y_pred):\n    return K.mean(K.equal(y_true, K.round(y_pred)), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Plots:\n    def plot_history(history):\n        loss = history.history['loss']\n        val_loss = history.history['val_loss']\n        x = range(1, len(val_loss) + 1)\n\n        plt.plot(x, loss, 'b', label='Training loss')\n        plt.plot(x, val_loss, 'r', label='Validation loss')\n        plt.title('Training and validation loss')\n        plt.legend()\n\n    def plot_roc_auc(train_roc, val_roc):\n        x = range(1, len(val_roc) + 1)\n\n        plt.plot(x, train_roc, 'b', label='Training RocAuc')\n        plt.plot(x, val_roc, 'r', label='Validation RocAuc')\n        plt.title('Training and validation RocAuc')\n        plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create models\n- BaseLine models https://realpython.com/python-keras-text-classification/\n- Single mode 98.18: https://github.com/ipcplusplus/toxic-comments-classification/blob/master/toxic_comment_analysis.ipynb\n- Attention Display : https://github.com/conversationai/conversationai-models/blob/master/attention-tutorial/Attention_Model_Tutorial.ipynb\n- Attention Models: https://github.com/thinline72/toxic/tree/master/skolbachev/toxic\n- Many models: https://github.com/neptune-ml/open-solution-toxic-comments\n- More modes alno: https://github.com/alno/kaggle-jigsaw-toxic-comment-classification-challenge"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tra, X_val, y_tra, y_val = train_test_split(train_padding, y, train_size=0.90, random_state=233)\nRocAuc = RocAucEvaluation(training_data=(X_tra, y_tra) ,validation_data=(X_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple RNN\n\nText data have a sequence. Thus, the meaning of a word is dependant on the previous words. Thus, we will try to use RNN that uses the previous state of the sequence."},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"input_layer = Input(shape=(max_len, ))\nX = Embedding(max_features, embed_size, weights=[embedding_matrix])(input_layer)\nX = SimpleRNN(units=128, activation=\"relu\")(X)\nX = Dense(6, activation=\"sigmoid\")(X)\nmodel = Model(inputs=input_layer, outputs=X)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['accuracy', 'binary_crossentropy'])\n\nsaved_model = \"weights_base.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(saved_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nbatch_sz = 64\nepoch = 20\n\nmodel.fit(X_tra,\n          y_tra,\n          validation_data=(X_val, y_val),\n          batch_size=batch_sz,\n          epochs=epoch,\n          callbacks=callbacks_list,\n          shuffle=True,\n          verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bidirectional RNN\nFrom the simple RNN we saw that using the information from the previous state of the sequence helps, hence we will try to use a biRNN in order to use information that is not only before that token but also after"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"input_layer = Input(shape=(max_len, ))\nX = Embedding(max_features, embed_size, weights=[embedding_matrix])(input_layer)\nX = Bidirectional(CuDNNGRU(128))(X)\nX = Dense(6, activation=\"sigmoid\")(X)\nmodel = Model(inputs=input_layer, outputs=X)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['accuracy', 'binary_crossentropy'])\n\nsaved_model = \"weights_base.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(saved_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nbatch_sz = 128\nepoch = 20\n\nmodel.fit(X_tra,\n          y_tra,\n          validation_data=(X_val, y_val),\n          batch_size=batch_sz,\n          epochs=epoch,\n          callbacks=callbacks_list,\n          shuffle=True,\n          verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BiGRU with ebedding projection layer"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"input_layer = Input(shape=(max_len, ))\nX = Embedding(max_features, embed_size, weights=[embedding_matrix])(input_layer)\nX = Dense(units=max_len, activation='relu')(X)\nX = BatchNormalization()(X)\nX = Bidirectional(CuDNNGRU(128))(X)\nX = Dense(6, activation=\"sigmoid\")(X)\nmodel = Model(inputs=input_layer, outputs=X)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['accuracy', 'binary_crossentropy'])\n\nsaved_model = \"weights_base.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(saved_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nbatch_sz = 128\nepoch = 20\n\nmodel.fit(X_tra,\n          y_tra,\n          validation_data=(X_val, y_val),\n          batch_size=batch_sz,\n          epochs=epoch,\n          callbacks=callbacks_list,\n          shuffle=True,\n          verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BiGRU with MLP on top and embeddings projection layer"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"input_layer = Input(shape=(max_len, ))\nX = Embedding(max_features, embed_size, weights=[embedding_matrix])(input_layer)\n# Embedding projection Layer before the RNN\nX = Dense(units=max_len, activation='relu')(X)\n# X = Dropout(0.2)(X)\nX = BatchNormalization()(X)\nX = Bidirectional(CuDNNGRU(128))(X)\n# MLP on top of BiGRU\nX = Dense(256, activation='relu' )(X)\n# X = Dense(100, activation='relu' )(X)\nX = Dense(6, activation=\"sigmoid\")(X)\nmodel = Model(inputs=input_layer, outputs=X)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['accuracy', 'binary_crossentropy'])\n\nsaved_model = \"weights_base.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(saved_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nbatch_sz = 128\nepoch = 20\n\nmodel.fit(X_tra,\n          y_tra,\n          validation_data=(X_val, y_val),\n          batch_size=batch_sz,\n          epochs=epoch,\n          callbacks=callbacks_list,\n          shuffle=True,\n          verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"input_layer = Input(shape=(max_len, ))\nX = Embedding(max_features, embed_size, weights=[embedding_matrix])(input_layer)\n# Embedding projection Layer before the RNN\nX = Dense(units=max_len, activation='relu')(X)\n# X = Dropout(0.2)(X)\nX = BatchNormalization()(X)\nx_state, x_fwd, x_bwd = Bidirectional(CuDNNGRU(128, return_sequences=True, return_state=True))(X)\nX = concatenate([x_fwd, x_bwd])\n# MLP on top of BiGRU\n# X = Reshape((2 * max_len,128, 1))(X)\nX = Dense(units=256, activation='relu')(X)\nX = Dense(6, activation=\"sigmoid\")(X)\nmodel = Model(inputs=input_layer, outputs=X)\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam',  metrics=['accuracy', 'binary_crossentropy'])\n\nsaved_model = \"weights_base.best.hdf5\"\n\ncheckpoint = ModelCheckpoint(saved_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nbatch_sz = 128\nepoch = 20\n\nmodel.fit(X_tra,\n          y_tra,\n          validation_data=(X_val, y_val),\n          batch_size=batch_sz,\n          epochs=epoch,\n          callbacks=callbacks_list,\n          shuffle=True,\n          verbose=1)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacked BiGRU"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"target_shape = 6\nlr=0.0003\nrnn_dropout=None\nrnn_layers=[128, 64]\nmlp_layers=[70]\nmlp_dropout=0.1\ntext_emb_dropout=0.0\ntext_emb_size=300\n\nmodel = Sequential()\nmodel.add(InputLayer(name='comment_text', input_shape=[max_len]))\nmodel.add(Embedding(max_features, text_emb_size, weights=[embedding_matrix], trainable=False))\nmodel.add(Dropout(text_emb_dropout))\n\nfor layer_size in rnn_layers:\n    #Fast LSTM implementation backed by CuDNN. Can only be run on GPU, with the TensorFlow backend.\n    model.add(Bidirectional(CuDNNLSTM(layer_size, return_sequences=True)))\n    if rnn_dropout is not None:\n        model.add(SpatialDropout1D(rnn_dropout))\n\nmodel.add(GlobalMaxPool1D())\nfor layer_size in mlp_layers:\n    model.add(Dense(layer_size, activation=\"relu\"))\n    model.add(Dropout(mlp_dropout))\nmodel.add(Dense(6, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.000015))\n\nsaved_model = \"weights_base.best.hdf5\"\ncheckpoint = ModelCheckpoint(saved_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\ncallbacks_list = [checkpoint, early, RocAuc]\n\nmodel.fit(x=X_tra,\n          y=y_tra,\n          validation_data=(X_val, y_val),\n          batch_size=128,\n          epochs=80,         \n          callbacks=callbacks_list, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\ndel model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.summary() # Print a description of the model.\n# Plots.plot_roc_auc(RocAuc.aucs_tra, RocAuc.aucs_val)\nPlots.plot_history(model.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_values = model.predict([test_padding], batch_size=1024, verbose=1)\nsample_submission = pd.read_csv('../input/sample_submission.csv')\nsample_submission[classes] = test_values\nsample_submission.to_csv('/submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\nsample_submission.to_csv('submission.csv', index=False)\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.vis_utils import plot_model\n\nplot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\nfrom IPython.display import Image\nImage(retina=True, filename='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install ann_visualizer\n!pip install graphviz\n!pip install h5py\nfrom ann_visualizer.visualize import ann_viz\n# create model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nann_viz(model, title=\"Artificial Neural network - Model Visualization\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('model.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}