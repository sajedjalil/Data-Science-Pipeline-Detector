{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Forked from **Jeremy Howard**'s NB-LR kernel: https://www.kaggle.com/jhoward/nb-svm-baseline-0-06-lb\n\n## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic ClassiÔ¨Åcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n","metadata":{"_cell_guid":"d3b04218-0413-4e6c-8751-5d8a404d73a9","_uuid":"0bca9739b82d5d51e1229243e03ea1b6db35c17e"}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import cross_val_predict\nimport csv","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ef06cd19-66b6-46bc-bf45-184e12d3f7d4","_uuid":"cca038ca9424a3f66e10262fc9129de807b5f855","collapsed":true}},{"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubm = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"a494f561-0c2f-4a38-8973-6b60c22da357","_uuid":"f70ebe669fcf6b434c595cf6fb7a76120bf7809c","collapsed":true}},{"cell_type":"markdown","source":"## Looking at the data\n\nThe training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.","metadata":{"_cell_guid":"3996a226-e1ca-4aa8-b39f-6524d4dadb07","_uuid":"2c18461316f17d1d323b1959c8eb4e5448e8a44e"}},{"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"5ddb337b-c9b2-4fec-9652-cb26769dc3c6","scrolled":true,"_uuid":"5f5269c56ea6ded273881b0d4dcdb6af83a3e089"}},{"cell_type":"markdown","source":"ddHere's a couple of examples of comments, one toxic, and one with no labels.","metadata":{"_cell_guid":"b3b071fb-7a2c-4195-9817-b01983d11c0e","_uuid":"004d2e823056e98afc5adaac433b7afbfe93b82d"}},{"cell_type":"markdown","source":"The length of the comments varies a lot.","metadata":{"_cell_guid":"2ea37597-02f7-43cf-ad16-a3d50aac1aba","_uuid":"5c4c716de98a4b1c2ecc0e516e67813b4fc1473e"}},{"cell_type":"code","source":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"fd3fe158-4d7f-4b30-ac15-42605240ea4f","_uuid":"9c1a3f81397199fa250a2b642edc7fbc5f9f504e","collapsed":true}},{"cell_type":"code","source":"lens.hist();","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"d2e55012-4736-425f-84f3-c148ac1f4852","_uuid":"eb68f1c83a5ad11e652ca5f2150993a06d43edb4","collapsed":true}},{"cell_type":"markdown","source":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset.","metadata":{"_cell_guid":"b8515824-b2dd-4c95-bbf9-dc74c80355db","_uuid":"0151ab55887071aed82d297acb2c6545ed964c2b"}},{"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c66f79d1-1d9f-4d94-82c1-8026af198f2a","_uuid":"4ba6ef86c82f073bf411785d971a694348c3efa9","collapsed":true}},{"cell_type":"code","source":"# add label col \n# https://stackoverflow.com/questions/44464280/mapping-one-hot-encoded-target-values-to-proper-label-names\nnew_label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','none']\nf, u = pd.factorize(new_label_cols)\ny_test  = np.array(\n    train[new_label_cols]\n)\ntrain[\"target\"]=[', '.join(u[y.astype(bool)]) for y in y_test]\n\n# train[\"target\"]=\n# labels = [', '.join(u[y.astype(int)]) for y in y_test]","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"459f047f-3375-4da7-8ec0-0cc290a2d15d","_uuid":"d0374a4b7fd4dbac31ea2ce1355029898b0913ec","collapsed":true}},{"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e416fd59-1588-49f4-88a2-557ab08745d8","_uuid":"097dde9259217dba58cb0c2abf92cf6885090916","collapsed":true}},{"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"6309d174-c22b-4d9d-8280-c0acd9aa6219","_uuid":"70a693111edb1979959351ff05275f0a8d58bf0e","collapsed":true}},{"cell_type":"code","source":"train[label_cols].max(axis=1).describe()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"68962905-15fc-483e-909e-eb1e58e0098c","scrolled":true,"_uuid":"8d9f66a506ea3ea1ba643dec16ec750cebc0191a","collapsed":true}},{"cell_type":"code","source":"len(train),len(test)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9f6316e3-7e29-431b-abef-73acf4a08637","_uuid":"b7b0d391248f929a026b16fc38936b7fc0176351","collapsed":true}},{"cell_type":"markdown","source":"There are a few empty comments that we need to get rid of, otherwise sklearn will complain.","metadata":{"_cell_guid":"1b221e62-e23f-422a-939d-6747edf2d613","_uuid":"bfdcf59624717b37ca4ffc0c99d2c28a2d419b06"}},{"cell_type":"code","source":"# COMMENT = 'comment_text'\n# train[COMMENT].fillna(\"unknown\", inplace=True)\n# test[COMMENT].fillna(\"unknown\", inplace=True)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"fdba531c-7ef2-4967-88e2-fc2b04f6f2ef","_uuid":"1e1229f403225f1889c7a7b4fc9be90fda818af5","collapsed":true}},{"cell_type":"code","source":"df = pd.concat([train['comment_text'], test['comment_text']], axis=0)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"dc15b813-6f92-46ee-b775-b681d9f5e832","_uuid":"6f5dccdadc19b491c7b77737e1cfd885e4b93905","collapsed":true}},{"cell_type":"code","source":"pd.concat([train, test], axis=0).drop_duplicates(subset='comment_text').drop(\"id\",axis=1).to_csv('toxic_raw_text.csv.gz', index=False,compression=\"gzip\",quoting=csv.QUOTE_ALL)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"fa847b8f-11e5-42d4-b3f6-d27e31f6cb0d","_uuid":"19bd1b88e2bb88bc16bac5f4dbfbf270cfa69153","collapsed":true}},{"cell_type":"code","source":"train[[\"id\",'comment_text',\"target\"]].to_csv('train_toxic_raw_v0.csv.gz', index=False,compression=\"gzip\",quoting=csv.QUOTE_ALL)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"0f40bfd5-d6b1-4424-8149-8e03690a9858","_uuid":"c4906080347e915ae0e8b919aa888f31fde33dea","collapsed":true}},{"cell_type":"markdown","source":"## Building the model\n\nWe'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper.","metadata":{"_cell_guid":"480780f1-00c0-4f9a-81e5-fc1932516a80","_uuid":"f2e77e8e6df5e29b620c7a2a0add1438c35af932"}},{"cell_type":"code","source":"n = train.shape[0]\nvec = CountVectorizer(ngram_range=(1,2),min_df=3, max_df=0.97,max_features = 60000) # could also try adding stop word removals, stemming, not lowercasing!\n\nvec.fit(df.values)\ntrn_term_doc = vec.transform(train[COMMENT])\ntest_term_doc = vec.transform(test[COMMENT])\n\n# trn_term_doc = vec.fit_transform(train[COMMENT])\n# test_term_doc = vec.transform(test[COMMENT])","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"26952eec-a7fb-4469-af5b-78bf35de5b0e","_uuid":"6bc08d6ac10871f09ac81adaa92930007d30fcf8","collapsed":true}},{"cell_type":"markdown","source":"Here's the basic naive bayes feature equation:","metadata":{"_cell_guid":"b9eb802a-160c-4781-9782-ab67dcdcb17c","_uuid":"85c13f3ba3d86a42cb9640016efce7ffd13ea3cc"}},{"cell_type":"code","source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"45fc6070-ba13-455b-9274-5c2611e2809c","_uuid":"8b277f01cecd575ed4fcae2e630c0dd8ce979793","collapsed":true}},{"cell_type":"markdown","source":"We *binarize* the features as discussed in the NBSVM paper.","metadata":{"_cell_guid":"12341e9e-ab19-44a1-9f10-20718238f85e","_uuid":"8e165fbbcf3e4b22b4ee23dc4f9b3366613f6546"}},{"cell_type":"code","source":"x=trn_term_doc.sign()\ntest_x = test_term_doc.sign()","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"2299d24b-5515-4d37-92d9-e7f6b16a290a","_uuid":"926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4","collapsed":true}},{"cell_type":"markdown","source":"Fit a model for one dependent at a time:","metadata":{"_cell_guid":"92760538-b2df-4568-9b92-f0e7e27114d1","_uuid":"d8b56c17c4a5fba190395acbe1f024d20f2382b5"}},{"cell_type":"code","source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=0.1, dual=True) # ORIG\n#     m = LogisticRegressionCV(Cs=12)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b756c889-a383-4952-9ee9-eca79fd3454f","_uuid":"8652ab2f5f84e77fa395252be9b60be1e44fd583","collapsed":true}},{"cell_type":"code","source":"preds = np.zeros((len(test), len(label_cols)))","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"33fd5f8c-adfc-45a1-9fde-1769a0993e76","_uuid":"0fa103b5406aabdc36ea9ef21612d343e4982fc4","collapsed":true}},{"cell_type":"code","source":"# for i, j in enumerate(label_cols):\n#     print('fit', j)\n#     m,r = get_mdl(train[j])\n#     preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9f2dd982-9542-4d37-be77-087c293e3c99","_uuid":"dfff4b395bad89158d49d6da3878607f284257eb","collapsed":true}},{"cell_type":"markdown","source":"And finally, create the submission file.","metadata":{"_cell_guid":"c3f79dcf-a1db-4f81-9429-2c772d10ac76","_uuid":"6f2b640da9c9cf88a08526ce8ce9a33c6bdc3bd5"}},{"cell_type":"code","source":"# submid = pd.DataFrame({'id': subm[\"id\"]})\n# submission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n# submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"bc6a4575-fbbb-47ea-81ac-91fa702dc194","_uuid":"5dd033a93e6cf32cdbdaa0a8b05cd8d27de2b21d","collapsed":true}},{"cell_type":"code","source":"","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"33427980-5316-4b66-bfd5-fcac6d391b8a","_uuid":"cf33817f680c7c83a94697e7434a34558eb75a1c","collapsed":true}}],"metadata":{"language_info":{"nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.3","mimetype":"text/x-python"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}