{"metadata":{"language_info":{"version":"3.6.3","pygments_lexer":"ipython3","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"5e2f73c7-b9c2-4361-8c12-a3743aacc992","_uuid":"56e5448cf75bce64f2a5c6bca1469323d30d4503"},"cell_type":"markdown","source":"This is a simple stacking notebook to get you started with stacking keras's take on fastText + a classic BOW sklearn model.\n\nBased on: https://www.kaggle.com/sterby/fasttext-like-baseline-with-keras-lb-0-257 , https://www.kaggle.com/yekenot/toxic-regression"},{"metadata":{"_cell_guid":"08af5ead-6614-4f92-a4e3-093697cd4630","collapsed":true,"_uuid":"9f8d6f83925faf36ff75acefedbb423e6fe76918"},"cell_type":"code","outputs":[],"execution_count":null,"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.feature_extraction.text import CountVectorizer"},{"metadata":{"_cell_guid":"e78abc52-c2fb-48e5-80e2-a5538f9ef421","_uuid":"622dd3eda5f1a4e20ca0a11ddc88e6cf1823bbf8"},"cell_type":"code","outputs":[],"execution_count":null,"source":"from keras.preprocessing import sequence\nfrom keras.models import Model, Input\nfrom keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, SpatialDropout1D\nfrom keras.preprocessing.text import Tokenizer"},{"metadata":{"_cell_guid":"fb04a7a2-f1e9-44ad-a320-c8feda8a2a61","_uuid":"1aee7e6435f922592df409c705751cc4a716ebc8"},"cell_type":"markdown","source":"# Load the data"},{"metadata":{"_cell_guid":"09bf6043-2919-4aac-9b85-aa2dc71554d0","collapsed":true,"_uuid":"849d6b0c78be1391c6148869118c87c3d3712544"},"cell_type":"code","outputs":[],"execution_count":null,"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")"},{"metadata":{"_cell_guid":"78f87293-8016-49dd-a63c-151779956a06","collapsed":true,"_uuid":"5b0b36db084f6cd8d08f4b8135ea4b43ea36c38e"},"cell_type":"code","outputs":[],"execution_count":null,"source":"df = pd.concat([train_df['comment_text'], test_df['comment_text']], axis=0).fillna(\"BLANK\")  # concat data for \"cheating\" in vectorizing"},{"metadata":{"_cell_guid":"6d95839a-7b1a-49db-89c2-37fe29d6bdfa","collapsed":true,"_uuid":"4406ce81bc1da62436aa8d849426bde899ff1cfc","scrolled":true},"cell_type":"code","outputs":[],"execution_count":null,"source":"train_df.head()"},{"metadata":{"_cell_guid":"e0767151-80de-4109-8775-f986e685a505","_uuid":"68b69fedacc0b693853926df2e42b71bd816b72f"},"cell_type":"markdown","source":"## A little EDA: Is this multiclass or multilabel? \n* Looks like it can be multilabel :(\n* Might be reversable with : https://stackoverflow.com/questions/44464280/mapping-one-hot-encoded-target-values-to-proper-label-names"},{"metadata":{"_cell_guid":"2ebf324c-9be5-4f13-955b-66be73b3d38d","collapsed":true,"_uuid":"5bb32bce59fc3f1888379d4b1c9f83f296134ef4"},"cell_type":"code","outputs":[],"execution_count":null,"source":"(train_df.iloc[:,2:].apply(sum,axis=1)>1).sum()"},{"metadata":{"_cell_guid":"aea018f1-c215-468d-98c3-e0a68e4a3078","collapsed":true,"_uuid":"856455e4d300957ee15ad757d06faf4ba8e5eb51"},"cell_type":"code","outputs":[],"execution_count":null,"source":"print(train_df.comment_text.str.len().describe())"},{"metadata":{"_cell_guid":"6c5916e5-78c9-402b-b5bb-7c1d77380440","collapsed":true,"_uuid":"f6caafa577523b187af482ee58b7681edd2a33d5","scrolled":false},"cell_type":"code","outputs":[],"execution_count":null,"source":"print(train_df.comment_text.str.split().str.len().describe())"},{"metadata":{"_cell_guid":"31bd24ee-4af6-458e-98f0-89de251908f1","collapsed":true,"_uuid":"231cd713a15baddc391eecce4ad0923ae9ec1baa"},"cell_type":"code","outputs":[],"execution_count":null,"source":"print(test_df.comment_text.str.split().str.len().describe())"},{"metadata":{"_cell_guid":"7cde6c71-6ff7-4af6-9615-ba45171df768","_uuid":"bd46cf991dcb8f9236682180dcfcb994c79aac5f"},"cell_type":"markdown","source":"* It looks like we have less than a hundred words, and a few hundred chars per sentence. \n* This will help us design our max len, as well as giving us insight into there being many short words/characters"},{"metadata":{"_cell_guid":"b3575b93-1b4b-4dd5-a624-816d87fa6724","collapsed":true,"_uuid":"837178d74233a88f98b48b7e66e55771f0093fb5"},"cell_type":"code","outputs":[],"execution_count":null,"source":"X_train = train_df[\"comment_text\"].fillna(\"BLANK\").values\ny_train = train_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\nX_test = test_df[\"comment_text\"].fillna(\"BLANK\").values"},{"metadata":{"_cell_guid":"bc678041-4cc1-4ac3-8725-b82e64df2306","collapsed":true,"_uuid":"4732035b6722f3edb52eb6b7d793018186eefb00"},"cell_type":"code","outputs":[],"execution_count":null,"source":"i = 0\nprint(\"Comment: {}\".format(X_train[i]))\nprint(\"Label: {}\".format(y_train[i]))"},{"metadata":{"_cell_guid":"d779dd97-8872-4142-b4a0-ec567d22c1db","_uuid":"53f203285fae275a29a91f7ef41ef164f867ab66"},"cell_type":"markdown","source":"# Use simple fasttext-like model"},{"metadata":{"_cell_guid":"b291a985-4b50-4a1f-b1d8-80772a4f7992","collapsed":true,"_uuid":"f14a84b803adc74b14b11e871ef87d9361f29d8a"},"cell_type":"code","outputs":[],"execution_count":null,"source":"# Set parameters:\nmax_features = 95000\nmaxlen = 84\nbatch_size = 32\nembedding_dims = 60 #50\nepochs = 3"},{"metadata":{"_cell_guid":"6c66a34e-f161-43c6-9516-da1c4529df2d","collapsed":true,"_uuid":"dd03a24e101354dd3e42a7c56ffa4299e300e3d0"},"cell_type":"code","outputs":[],"execution_count":null,"source":"print('Tokenizing data...')\ntok = Tokenizer(num_words=max_features)\ntok.fit_on_texts(list(X_train) + list(X_test))\nx_train = tok.texts_to_sequences(X_train)\nx_test = tok.texts_to_sequences(X_test)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))"},{"metadata":{"_cell_guid":"0cf4fbaa-0127-4074-8294-b9c735f39680","collapsed":true,"_uuid":"6ec8bdd68059159a542cab8c9ff32d31d26dc373"},"cell_type":"code","outputs":[],"execution_count":null,"source":"print('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)"},{"metadata":{"_cell_guid":"38454ada-8e95-4add-a039-345bc6856666","collapsed":true,"_uuid":"2fdfd068dd38b910f0b3605b863bf65d4e2d34c7"},"cell_type":"code","outputs":[],"execution_count":null,"source":"print('Build model...')\ncomment_input = Input((maxlen,))\n\n# we start off with an  embedding layer\ncomment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n# We see that we overfit straight away, so dropout may be useful\ndrp = SpatialDropout1D(0.1)(comment_emb)\n# we add a GlobalAveragePooling1D, which will average the embeddings\n# of all words in the document\nmain = GlobalAveragePooling1D()(drp)\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\noutput = Dense(6, activation='softmax')(main)\n\nmodel = Model(inputs=comment_input, outputs=output)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])"},{"metadata":{"_cell_guid":"6e64b88a-81aa-455d-af55-c7b54625ddd0","collapsed":true,"_uuid":"78d7759e7b282f535198186cb864c5deece6ee73"},"cell_type":"code","outputs":[],"execution_count":null,"source":"hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"},{"metadata":{"_cell_guid":"f9201487-a13d-4b63-b0da-fb2773507358","_uuid":"907ff8f48c6785640f433c3978b44c43f2f8720d"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"ef4c41c8-22ad-40fe-8c75-11a10622e5ea","collapsed":true,"_uuid":"ab8dfc297cc8a7e451498662eb576de8b39a262f"},"cell_type":"code","outputs":[],"execution_count":null,"source":"# print('Build model...')\n# comment_input = Input((maxlen,))\n\n# # we start off with an  embedding layer\n# comment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n# # We see that we overfit straight away, so dropout may be useful\n# drp = Dropout(0.15)(comment_emb)\n# # we add a GlobalAveragePooling1D, which will average the embeddings\n# # of all words in the document\n# main = GlobalAveragePooling1D()(drp)\n\n# drp2 =  Dropout(0.25)(main)\n# # We project onto a single unit output layer, and squash it with a sigmoid:\n# output = Dense(6, activation='softmax')(drp2)\n\n# model2 = Model(inputs=comment_input, outputs=output)\n\n# model2.compile(loss='categorical_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\n# hist2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"},{"metadata":{"_cell_guid":"735185a5-81fd-4e03-bac6-1f76ddc85e21","_uuid":"1495d6641a66944a593598739803e3ff72646c5c"},"cell_type":"markdown","source":"#### Playing with dropout doesn't move the FastText needle (results are the same with/without dropout(s)). Not very surprising as it's just a linear embedding.\n* Final output is still \"loss: 0.2863 - acc: 0.9890 - val_loss: 0.3014 - val_acc: 0.9892\"\n\n* Note that spatial dropout has a much bigger effect! "},{"metadata":{"_cell_guid":"9d6db9c8-3e58-4cf9-a826-4e1754c90447","_uuid":"91bc07c33b1bcd8a9baea93c79be0e501373edf8"},"cell_type":"markdown","source":"## Ensemble!\n* Let's add the output from another model"},{"metadata":{"_cell_guid":"b9370bee-ded0-40ab-b6e0-9138c0312e3c","collapsed":true,"_uuid":"58328c6812b6211bbe6ee5e4bf7ad4a751ec70ec","scrolled":true},"cell_type":"code","outputs":[],"execution_count":null,"source":"nrow_train = train_df.shape[0]\n\nvectorizer = CountVectorizer(stop_words='english',min_df=3, max_df=0.97,max_features = 40000)\ndata = vectorizer.fit_transform(df)\n\ncol = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nlr_preds = np.zeros((test_df.shape[0], len(col)))\n\nX_train = data[:nrow_train]\nX_test = data[nrow_train:]\n\nfor i, j in enumerate(col):\n    print('fit '+j)\n    lr_model = LogisticRegression(C=0.1, dual=True)\n    lr_model.fit(X_train, train_df[j])\n    lr_preds[:,i] = lr_model.predict_proba(X_test)[:,1]\nprint(\"done\")"},{"metadata":{"_cell_guid":"170ff5e4-9371-4ec3-bba1-37e7b3490a04","_uuid":"f84bdf717fb9cf6c63060b49868999ccb8afde00"},"cell_type":"markdown","source":"## Quick sanity check. compare out predicted outputs"},{"metadata":{"_cell_guid":"59d227dd-d38c-4b60-a156-331ca1f1c665","collapsed":true,"_uuid":"86fe1c1262d97862000669d2f3129e393141c473"},"cell_type":"code","outputs":[],"execution_count":null,"source":"for i, j in enumerate(col):\n    print(j,lr_preds[:,i].mean())"},{"metadata":{"_cell_guid":"f0399a20-e8a8-466e-a768-3ee22245a661","collapsed":true,"_uuid":"be8514eefbd5a3fe95600d710cbb2d24580b10b4"},"cell_type":"code","outputs":[],"execution_count":null,"source":"# Get predictions from our keras/fasttext model\nft_pred = model.predict(x_test)"},{"metadata":{"_cell_guid":"e00bf5fc-ac8a-491f-b1f1-25b3e153f846","_uuid":"61aba7e0f1a356b78fbd8db340c7560fe35bbe59"},"cell_type":"markdown","source":"# submit"},{"metadata":{"_cell_guid":"4472f2e8-8c4a-4a36-97ec-050e6f952839","collapsed":true,"_uuid":"787560f67d2f6be8ee2acd997171fa3e0b72dc89"},"cell_type":"code","outputs":[],"execution_count":null,"source":"# get mean of both submissions\n\ny_pred = lr_preds+ft_pred\ny_pred = y_pred/2.0\n"},{"metadata":{"_cell_guid":"20edfc41-2e40-4dcb-8c93-a628878d21d7","collapsed":true,"_uuid":"bcd379c47dc1770cb595720cf47a21d0bb289a18"},"cell_type":"code","outputs":[],"execution_count":null,"source":""},{"metadata":{"_cell_guid":"e4dabe11-4cf0-4feb-af9b-e5074233511f","collapsed":true,"_uuid":"7d98f168948c9240b680808ccf306220b045a876"},"cell_type":"code","outputs":[],"execution_count":null,"source":"submission = pd.read_csv(\"../input/sample_submission.csv\")"},{"metadata":{"_cell_guid":"5491387a-4e8e-4080-b4cb-d549c3372f6b","collapsed":true,"_uuid":"c5aaed3d9c600e4a674959902f9552a3112d47e8"},"cell_type":"code","outputs":[],"execution_count":null,"source":"submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred"},{"metadata":{"_cell_guid":"3342bf21-3402-44f1-b601-51b6a0684515","collapsed":true,"_uuid":"e335a3a0af28935e41aa08a140129033ef298658"},"cell_type":"code","outputs":[],"execution_count":null,"source":"submission.head()"},{"metadata":{"_cell_guid":"50495ec2-e5d8-41cd-8b3a-c2779ef7b61c","collapsed":true,"_uuid":"46dd6762d02e4de6c0fbef22f3257bc740f306b5"},"cell_type":"code","outputs":[],"execution_count":null,"source":"submission.to_csv(\"submission_fasttext_1.csv\", index=False)"},{"metadata":{"_cell_guid":"e8549abb-9cda-44be-9d0f-abe49050474a","collapsed":true,"_uuid":"c33847d50412c864b7ab099fea086405d9cbe193"},"cell_type":"code","outputs":[],"execution_count":null,"source":""}]}