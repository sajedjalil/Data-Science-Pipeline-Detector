{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"version":"3.6.3","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","name":"python"}},"cells":[{"metadata":{"_uuid":"0bca9739b82d5d51e1229243e03ea1b6db35c17e","_cell_guid":"d3b04218-0413-4e6c-8751-5d8a404d73a9"},"source":"* Forked from **Jeremy Howard**'s NB-LR kernel: https://www.kaggle.com/jhoward/nb-svm-baseline-0-06-lb\n\n## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic ClassiÔ¨Åcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n","cell_type":"markdown"},{"metadata":{"_uuid":"cca038ca9424a3f66e10262fc9129de807b5f855","collapsed":true,"_cell_guid":"ef06cd19-66b6-46bc-bf45-184e12d3f7d4"},"source":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.feature_extraction.text import CountVectorizer","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f70ebe669fcf6b434c595cf6fb7a76120bf7809c","collapsed":true,"_cell_guid":"a494f561-0c2f-4a38-8973-6b60c22da357"},"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubm = pd.read_csv('../input/sample_submission.csv')","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"2c18461316f17d1d323b1959c8eb4e5448e8a44e","_cell_guid":"3996a226-e1ca-4aa8-b39f-6524d4dadb07"},"source":"## Looking at the data\n\nThe training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.","cell_type":"markdown"},{"metadata":{"_uuid":"5f5269c56ea6ded273881b0d4dcdb6af83a3e089","collapsed":true,"scrolled":true,"_cell_guid":"5ddb337b-c9b2-4fec-9652-cb26769dc3c6"},"source":"train.head()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"004d2e823056e98afc5adaac433b7afbfe93b82d","_cell_guid":"b3b071fb-7a2c-4195-9817-b01983d11c0e"},"source":"Here's a couple of examples of comments, one toxic, and one with no labels.","cell_type":"markdown"},{"metadata":{"_uuid":"1ba9522a65227881a3a55aefaee9de93c4cfd792","collapsed":true,"_cell_guid":"d57f0b31-c09b-4305-a0b0-0b864e944fd1"},"source":"train['comment_text'][0]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b0d70e9d745411ea6228c95c5f19bd3a2ca6dd55","collapsed":true,"scrolled":true,"_cell_guid":"9caf5da3-33bb-422d-81c4-fef20fbda1a8"},"source":"train['comment_text'][2]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"5c4c716de98a4b1c2ecc0e516e67813b4fc1473e","_cell_guid":"2ea37597-02f7-43cf-ad16-a3d50aac1aba"},"source":"The length of the comments varies a lot.","cell_type":"markdown"},{"metadata":{"_uuid":"9c1a3f81397199fa250a2b642edc7fbc5f9f504e","collapsed":true,"_cell_guid":"fd3fe158-4d7f-4b30-ac15-42605240ea4f"},"source":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"eb68f1c83a5ad11e652ca5f2150993a06d43edb4","collapsed":true,"_cell_guid":"d2e55012-4736-425f-84f3-c148ac1f4852"},"source":"lens.hist();","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0151ab55887071aed82d297acb2c6545ed964c2b","_cell_guid":"b8515824-b2dd-4c95-bbf9-dc74c80355db"},"source":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset.","cell_type":"markdown"},{"metadata":{"_uuid":"4ba6ef86c82f073bf411785d971a694348c3efa9","collapsed":true,"_cell_guid":"c66f79d1-1d9f-4d94-82c1-8026af198f2a"},"source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8d9f66a506ea3ea1ba643dec16ec750cebc0191a","collapsed":true,"scrolled":true,"_cell_guid":"68962905-15fc-483e-909e-eb1e58e0098c"},"source":"train[label_cols].max(axis=1).describe()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"b7b0d391248f929a026b16fc38936b7fc0176351","collapsed":true,"_cell_guid":"9f6316e3-7e29-431b-abef-73acf4a08637"},"source":"len(train),len(test)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"bfdcf59624717b37ca4ffc0c99d2c28a2d419b06","_cell_guid":"1b221e62-e23f-422a-939d-6747edf2d613"},"source":"There are a few empty comments that we need to get rid of, otherwise sklearn will complain.","cell_type":"markdown"},{"metadata":{"_uuid":"1e1229f403225f1889c7a7b4fc9be90fda818af5","collapsed":true,"_cell_guid":"fdba531c-7ef2-4967-88e2-fc2b04f6f2ef"},"source":"COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6f5dccdadc19b491c7b77737e1cfd885e4b93905","collapsed":true,"_cell_guid":"dc15b813-6f92-46ee-b775-b681d9f5e832"},"source":"df = pd.concat([train['comment_text'], test['comment_text']], axis=0)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"f2e77e8e6df5e29b620c7a2a0add1438c35af932","_cell_guid":"480780f1-00c0-4f9a-81e5-fc1932516a80"},"source":"## Building the model\n\nWe'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper.","cell_type":"markdown"},{"metadata":{"_uuid":"6bc08d6ac10871f09ac81adaa92930007d30fcf8","collapsed":true,"_cell_guid":"26952eec-a7fb-4469-af5b-78bf35de5b0e"},"source":"n = train.shape[0]\nvec = CountVectorizer(ngram_range=(1,2),min_df=3, max_df=0.97,max_features = 60000) # could also try adding stop word removals, stemming, not lowercasing!\n\nvec.fit(df.values)\ntrn_term_doc = vec.transform(train[COMMENT])\ntest_term_doc = vec.transform(test[COMMENT])\n\n# trn_term_doc = vec.fit_transform(train[COMMENT])\n# test_term_doc = vec.transform(test[COMMENT])","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"85c13f3ba3d86a42cb9640016efce7ffd13ea3cc","_cell_guid":"b9eb802a-160c-4781-9782-ab67dcdcb17c"},"source":"Here's the basic naive bayes feature equation:","cell_type":"markdown"},{"metadata":{"_uuid":"8b277f01cecd575ed4fcae2e630c0dd8ce979793","collapsed":true,"_cell_guid":"45fc6070-ba13-455b-9274-5c2611e2809c"},"source":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"8e165fbbcf3e4b22b4ee23dc4f9b3366613f6546","_cell_guid":"12341e9e-ab19-44a1-9f10-20718238f85e"},"source":"We *binarize* the features as discussed in the NBSVM paper.","cell_type":"markdown"},{"metadata":{"_uuid":"926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4","collapsed":true,"_cell_guid":"2299d24b-5515-4d37-92d9-e7f6b16a290a"},"source":"x=trn_term_doc.sign()\ntest_x = test_term_doc.sign()","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"d8b56c17c4a5fba190395acbe1f024d20f2382b5","_cell_guid":"92760538-b2df-4568-9b92-f0e7e27114d1"},"source":"Fit a model for one dependent at a time:","cell_type":"markdown"},{"metadata":{"_uuid":"8652ab2f5f84e77fa395252be9b60be1e44fd583","collapsed":true,"_cell_guid":"b756c889-a383-4952-9ee9-eca79fd3454f"},"source":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n#     m = LogisticRegression(C=0.1, dual=True) # ORIG\n    m = LogisticRegressionCV(Cs=5)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"0fa103b5406aabdc36ea9ef21612d343e4982fc4","collapsed":true,"_cell_guid":"33fd5f8c-adfc-45a1-9fde-1769a0993e76"},"source":"preds = np.zeros((len(test), len(label_cols)))","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"dfff4b395bad89158d49d6da3878607f284257eb","collapsed":true,"_cell_guid":"9f2dd982-9542-4d37-be77-087c293e3c99"},"source":"for i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","outputs":[],"execution_count":null,"cell_type":"code"},{"metadata":{"_uuid":"6f2b640da9c9cf88a08526ce8ce9a33c6bdc3bd5","_cell_guid":"c3f79dcf-a1db-4f81-9429-2c772d10ac76"},"source":"And finally, create the submission file.","cell_type":"markdown"},{"metadata":{"_uuid":"5dd033a93e6cf32cdbdaa0a8b05cd8d27de2b21d","collapsed":true,"_cell_guid":"bc6a4575-fbbb-47ea-81ac-91fa702dc194"},"source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","outputs":[],"execution_count":null,"cell_type":"code"}],"nbformat":4,"nbformat_minor":1}