{"nbformat":4,"cells":[{"source":"# Exploring Interrelations between Classes\nWe can assume that given classes toxic and severe_toxic, having one class implies having the other. What about other target variables - does one variable affect another and can we use it to improve the final score? This notebook is an attempt to glimpse into the decision-making process of the annotators and also in the nature of the data itself. We will calculate one standard and one not-so-standard statistic and visualize them using plotly heatmaps.","cell_type":"markdown","metadata":{}},{"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\ninit_notebook_mode(connected=True)","cell_type":"code","metadata":{"_uuid":"17d5dbef0f8bf58110e49633a82eebfd13c5d270","_cell_guid":"5160c093-8d61-4c91-986f-c323da566f74"},"outputs":[],"execution_count":24},{"source":"First, we load the dataset and separate text data from the target variables.","cell_type":"markdown","metadata":{}},{"source":"train = pd.read_csv('../input/train.csv')\ntrain.index = train['id']\nx_train = train['comment_text']\ny_train = train.iloc[:, 2:]","cell_type":"code","metadata":{"collapsed":true},"outputs":[],"execution_count":2},{"source":"Then, let's calculate Spearman correlation between the target variables.","cell_type":"markdown","metadata":{}},{"source":"correlation_matrix = y_train.corr()","cell_type":"code","metadata":{},"outputs":[],"execution_count":8},{"source":"..and plot it using a heatmap.","cell_type":"markdown","metadata":{}},{"source":"heatmap = go.Heatmap(\n    z=np.flip(correlation_matrix.values, axis=1),  # try it without flipping - looks unusual\n    x=y_train.columns[::-1],\n    y=y_train.columns,\n    showscale=False,\n    colorscale=\"viridis\"\n)\n\nlayout = go.Layout(\n    title=\"Correlation between target variables\",\n    showlegend=False,\n    width=700, height=700,\n    autosize=False,\n    margin=go.Margin(l=100, r=100, b=100, t=100, pad=4)\n)\n\nfig = go.Figure(data=[heatmap], layout=layout)\niplot(fig, filename='hmap')","cell_type":"code","metadata":{},"outputs":[],"execution_count":36},{"source":"We see that, for instance, obscene and insult really correlate with each other, wich is kind of logical - most insults are obscene. But this doesn't give us the full picture. What is really important is, according to annotators, do we always assign \"toxic\" for \"sever_toxic\"? And what about \"obscene\"?\n\nLet's calculate a following matrix: given classes *i* and *j*, what proportion of objects belonging to class *i* also belong to class *j*?","cell_type":"markdown","metadata":{}},{"source":"dim = y_train.shape[1]\ncooccurence_matrix = np.zeros((dim, dim))\n\nfor i in range(dim):\n    for j in range(dim):\n        res = sum(y_train.iloc[:, i] & y_train.iloc[:, j]) / sum(y_train.iloc[:, i])\n        cooccurence_matrix[i, j] = res","cell_type":"code","metadata":{"collapsed":true},"outputs":[],"execution_count":54},{"source":"And another heatmap:","cell_type":"markdown","metadata":{}},{"source":"heatmap = go.Heatmap(\n    z=np.flip(cooccurence_matrix, axis=1),\n    x=y_train.columns[::-1],\n    y=y_train.columns,\n    showscale=False,\n    colorscale=\"viridis\"\n)\n\nlayout = go.Layout(\n    title=\"Coocurence of target variables\",\n    showlegend=False,\n    width=700, height=700,\n    autosize=False,\n    margin=go.Margin(l=100, r=100, b=100, t=100),\n    yaxis=dict(\n        title='If...'\n    ),\n    xaxis=dict(\n        title='then...'\n    )\n)\n\nfig = go.Figure(data=[heatmap], layout=layout)\niplot(fig, filename='hmap')","cell_type":"code","metadata":{},"outputs":[],"execution_count":55},{"source":"What this means is *if we have severe_toxic, we have toxic 100% of the time* (hover over the plot to see the figures). This means it is a good sanity check to see if your sever_toxic comments also have toxic class. Also threats are toxic in 95% and obscenities are toxic in 94% of the cases.\n\nIf a comment is an insult, it is obscene in 78% of the cases, which is, as already said, quite logical. More interestingly, 86% of sever_toxic comments also contains insults, as well as 82% of identity_hate instances. But if a comment is obscene, it is a threat only 3% of the time, but if it is a threat, it more probably than not contains an insult (65%).","cell_type":"markdown","metadata":{}}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py","version":"3.6.3"}},"nbformat_minor":1}