{"cells":[{"metadata":{"_cell_guid":"edd0d03b-f11d-4578-a9e1-1036129d97dd","_uuid":"e66399c15e2dc1d0015694fa6df02c64b0f56a2e","collapsed":true,"trusted":false},"cell_type":"code","source":"# Hide deprecation warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Common imports\nimport numpy as np\nimport pandas as pd\nimport re, string\nfrom itertools import product\n\n# Imports for models\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n# To format floats\nfrom IPython.display import display\npd.set_option('display.float_format', lambda x: '%.5f' % x)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb25564d-30b2-4fe1-aadc-ee3a3d4a5a90","_uuid":"a4f02f2305c2d7af71e240b76c50ce41f34836b3","trusted":false,"collapsed":true},"cell_type":"code","source":"#Load all the csv files into Pandas dataframes, properly parsing dates\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c91f15af-2602-4056-ad2c-7c4d8eddfe09","_uuid":"0ec8fc7e31fb02de9daed7b30fbbf3e6940b7da9","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Train DF:\\n\\n{}\\n\".format(train.head()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"85afedc9-07f7-4269-82a6-9efea45c6316","_uuid":"a03d70604397a2d7f802f43208c74a9c024b923e","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Test DF:\\n\\n{}\\n\".format(test.head()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"274209c6-ea99-4ad0-8ecd-44b8203e1a46","_uuid":"dcae6c3d38e22c1f406b1ac720793d8be501a2c1","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Train DF:\\n\")\nprint(train.info(),\"\\n\\n\")\nprint(train.drop('id',axis=1).describe(),\"\\n\\n\")\nprint(\"\\n\\nNulls:\\n\\n{}\\n\\n\".format(train.isnull().sum()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77b4786a-880d-4de3-9e8a-75e1318fb13a","_uuid":"08612c8a3958530449b45aee2f265d8e36557726","trusted":false,"collapsed":true},"cell_type":"code","source":"d = train\nsplit = 0.7\nd_train = d[:int(split*len(d))]\nd_test = d[int((split)*len(d)):]\n\nvectorizer = CountVectorizer()\nfeatures = vectorizer.fit_transform(d_train.comment_text)\ntest_features = vectorizer.transform(d_test.comment_text)\n\ni = 15000\nj = 15000\n\n#An example of what features have inside\n\nwords = vectorizer.get_feature_names()[i:i+10]\npd.DataFrame(features[j:j+7,i:i+10].todense(), columns=words)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cd54464-2ea2-4631-8bfd-1f3bd7b8ac9e","_uuid":"222123009b56a9b794c7fb2ed525026b56af3396","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Different words:\",len(vectorizer.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"184064a3-f493-4046-bd45-b30badc02d4b","collapsed":true,"_uuid":"7ffeb7d61f9e5a2ca9bf3d9dd5f338d578548621","trusted":false},"cell_type":"code","source":"def performance(y_true, pred, ann=True):\n    acc = accuracy_score(y_true, pred[:,1]>0.5)\n    auc = roc_auc_score(y_true, pred[:,1])\n    fpr, tpr, thr = roc_curve(y_true, pred[:,1])\n    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 7))\n    plt.plot(fpr, tpr, color='royalblue', linewidth=\"3\")\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    if ann:\n        ax.annotate(\"Acc: %0.2f\" % acc, (0.2,0.7), size=14)\n        ax.annotate(\"AUC: %0.2f\" % auc, (0.2,0.6), size=14)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6802635-9a02-4fd2-9455-274443cdc409","_uuid":"da5aa31720a6489973d1a0d92b07242c05f43dc5"},"cell_type":"markdown","source":"## NAÃVE BAYES CLASSIFIER"},{"metadata":{"_cell_guid":"0dc2842a-36f7-49d3-93c1-1287d800ed4f","_uuid":"f2a3ca5be3197bdcc3450f24aafb6747e7722bb7","trusted":false,"collapsed":true},"cell_type":"code","source":"model1 = MultinomialNB()\nmodel1.fit(features, d_train.toxic)\npred1 = model1.predict_proba(test_features)\n\nperformance(d_test.toxic, pred1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c8144b13-95fb-4021-8e81-547b6a7595e4","_uuid":"a1bcc130680f00893f2bd0ab0fcb6550b6dcd746","trusted":false,"collapsed":true},"cell_type":"code","source":"comment = \"What a stupid comment is the one you made you dumbass\"\n>>> print(model1.predict(vectorizer.transform([comment]))[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef025597-ce84-4d22-aa32-34aa6efaba9c","_uuid":"01eb25ce290c94bd6facbcf58082edc6ea27dee8","trusted":false,"collapsed":true},"cell_type":"code","source":"comment = \"Great comment, thanks for contributing\"\n>>> print(model1.predict(vectorizer.transform([comment]))[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a38ebd40-e129-4ec3-862b-eff5eb7d43d7","_uuid":"51273641af63ed7f3d04509949ba9de9921b65e1"},"cell_type":"markdown","source":"Seems to be working fine!"},{"metadata":{"_cell_guid":"65828d5e-df11-4670-9c23-d770041b97c2","_uuid":"21f6012773cdf92bf55c62e148b8fe4d0059b7e4"},"cell_type":"markdown","source":"## Bag-of-words features with the tf-idf algorithm"},{"metadata":{"_cell_guid":"f20e5ebb-7f2f-4fd9-8c6b-d353f8435465","_uuid":"146d0519e71aa1b7cfc0694aaa2258272a9d326b","trusted":false,"collapsed":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nfeatures = vectorizer.fit_transform(d_train.comment_text)\n\nmodel2 = MultinomialNB()\nmodel2.fit(features, d_train.toxic)\n\npred2 = model2.predict_proba(vectorizer.transform(d_test.comment_text))\nperformance(d_test.toxic, pred2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f17af0a9-7fae-4a86-a11d-5eb9d767a1ee","_uuid":"a5ab76c4c6442e0ad9454d0cd7c1d7cc01ca49b0"},"cell_type":"markdown","source":"## Optimizing model parameters"},{"metadata":{"_cell_guid":"fa528c76-9b48-48d1-a03e-ad3632b1f0d9","collapsed":true,"_uuid":"2ce3817e01a69f95fb8b3a74e6b8521c52b6517b","trusted":false},"cell_type":"code","source":"def build_model(Tfid = True, max_features=None, min_df=1, nb_alpha=1.0):\n    if Tfid:\n        vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df)\n    else:\n        vectorizer = CountVectorizer(max_features=max_features, min_df=min_df)\n    features = vectorizer.fit_transform(d_train.comment_text)\n    model = MultinomialNB(alpha=nb_alpha)\n    model.fit(features, d_train.toxic)\n    pred = model.predict_proba(vectorizer.transform(d_test.comment_text))\n    return {\n        \"Tfid\": Tfid,\n        \"max_features\": max_features,\n        \"min_df\": min_df,\n        \"nb_alpha\": nb_alpha,\n        \"auc\": roc_auc_score(d_test.toxic, pred[:,1])\n    }","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72439d96-60c0-408a-b504-c78e6790efd0","_uuid":"c3125de3484eceabb68a890969aa785a03d80b79","trusted":false,"collapsed":true},"cell_type":"code","source":"param_values = {\n  \"Tfid\": [True,False],\n  \"max_features\": [10000, 30000, 50000, None],\n  \"min_df\": [1,2,3],\n  \"nb_alpha\": [0.01, 0.1, 1.0]\n}\n\nresults = []\nmax_auc = 0\n\nfor p in product(*param_values.values()):\n    res = build_model(**dict(zip(param_values.keys(), p)))\n    results.append(res)\n    if res.get('auc')>max_auc:\n        max_auc = res.get('auc')\n        Tfid_opt = res.get('Tfid')\n        max_features_opt = res.get('max_features')\n        min_df_opt = res.get('min_df')\n        nb_alpha_opt = res.get('nb_alpha')\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3516e0f9-1062-44f4-9375-57de770251a6","_uuid":"a164012ac22088c05ac4104588d9a91f6bb65d65"},"cell_type":"markdown","source":"Let's see how the optimized model works"},{"metadata":{"_cell_guid":"070ef5d5-8543-491a-bc86-43d4f6851a49","_uuid":"58e9c518c83bc826e69ef2aa6a4de50df4a7665e","trusted":false,"collapsed":true},"cell_type":"code","source":"if Tfid_opt:\n    vectorizer = TfidfVectorizer(max_features=max_features_opt, min_df=min_df_opt)\nelse:\n    vectorizer = CountVectorizer(max_features=max_features, min_df=min_df)\n    \nfeatures = vectorizer.fit_transform(d_train.comment_text)\n\nmodel2_opt = MultinomialNB(alpha=nb_alpha_opt)\nmodel2_opt.fit(features, d_train.toxic)\n\npred2_opt = model2_opt.predict_proba(vectorizer.transform(d_test.comment_text))\nperformance(d_test.toxic, pred2_opt)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a07e22ac-6ff1-4f11-9521-be0b082c6a00","_uuid":"205e1377fc0a0d05be2f39ecd5def6e060f0862f","trusted":false,"collapsed":true},"cell_type":"code","source":"roc_auc_score(d_test.toxic, pred2_opt[:,1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f14c03a-ca9f-4a8e-8cdb-6ea16b0920c0","_uuid":"1eaa49e5ef8e253c0eebf3db2b41e6e6f1f3f1d5"},"cell_type":"markdown","source":"## Word2vec"},{"metadata":{"_cell_guid":"6d01edd6-b032-4eb8-865b-212a51f89d91","collapsed":true,"_uuid":"058041852ccfe8b9d7ca4e3bc4f6ab981c392cc4","trusted":false},"cell_type":"code","source":"stop_words = set(['all', \"she'll\", \"don't\", 'being', 'over', 'through', \n'yourselves', 'its', 'before', \"he's\", \"when's\", \"we've\", 'had', 'should',\n\"he'd\", 'to', 'only', \"there's\", 'those', 'under', 'ours', 'has', \n\"haven't\", 'do', 'them', 'his', \"they'll\", 'very', \"who's\", \"they'd\", \n'cannot', \"you've\", 'they', 'not', 'during', 'yourself', 'him', 'nor', \n\"we'll\", 'did', \"they've\", 'this', 'she', 'each', \"won't\", 'where', \n\"mustn't\", \"isn't\", \"i'll\", \"why's\", 'because', \"you'd\", 'doing', 'some', \n'up', 'are', 'further', 'ourselves', 'out', 'what', 'for', 'while', \n\"wasn't\", 'does', \"shouldn't\", 'above', 'between', 'be', 'we', 'who', \n\"you're\", 'were', 'here', 'hers', \"aren't\", 'by', 'both', 'about', 'would', \n'of', 'could', 'against', \"i'd\", \"weren't\", \"i'm\", 'or', \"can't\", 'own', \n'into', 'whom', 'down', \"hadn't\", \"couldn't\", 'your', \"doesn't\", 'from', \n\"how's\", 'her', 'their', \"it's\", 'there', 'been', 'why', 'few', 'too', \n'themselves', 'was', 'until', 'more', 'himself', \"where's\", \"i've\", 'with', \n\"didn't\", \"what's\", 'but', 'herself', 'than', \"here's\", 'he', 'me', \n\"they're\", 'myself', 'these', \"hasn't\", 'below', 'ought', 'theirs', 'my', \n\"wouldn't\", \"we'd\", 'and', 'then', 'is', 'am', 'it', 'an', 'as', 'itself', \n'at', 'have', 'in', 'any', 'if', 'again', 'no', 'that', 'when', 'same', \n'how', 'other', 'which', 'you', \"shan't\", 'our', 'after', \"let's\", 'most', \n'such', 'on', \"he'll\", 'a', 'off', 'i', \"she'd\", 'yours', \"you'll\", 'so', \n\"we're\", \"she's\", 'the', \"that's\", 'having', 'once'])\n\ndef tokenize(docs):\n    pattern = re.compile('[\\W_]+', re.UNICODE)\n    sentences = []\n    for d in docs:\n        sentence = d.lower().split(\" \")\n        sentence = [pattern.sub('', w) for w in sentence]\n        sentences.append( [w for w in sentence if w not in stop_words] )\n    return sentences\n\ndef featurize_w2v(model, sentences):\n    f = np.zeros((len(sentences), model.vector_size))\n    for i,s in enumerate(sentences):\n        for w in s:\n            try:\n                vec = model[w]\n            except KeyError:\n                continue\n            f[i,:] = f[i,:] + vec\n        f[i,:] = f[i,:] / len(s)\n    return f\n\ndef delete_nans(features):\n    rows_to_delete = []\n    for i in range(len(features)):\n        if np.isnan(features[i].sum()):\n            rows_to_delete.append(i)\n    return rows_to_delete","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e560f089-620e-49e8-8981-2ef7629515ce","_uuid":"c30090593dba0ce8db7ab8133df0858171a75751","trusted":false,"collapsed":true},"cell_type":"code","source":"sentences = tokenize(d_train.comment_text)\nmodel = Word2Vec(sentences, size=500, window=5, min_count=6, sample=1e-3, workers=2)\nmodel.init_sims(replace=True)\n\nfeatures_w2v = featurize_w2v(model, sentences)\n\nrows_to_delete = delete_nans(features_w2v)\nfeatures_w2v = np.delete(features_w2v, rows_to_delete, 0)\n\nmodel3 = RandomForestClassifier(n_estimators=600, n_jobs=-1, max_features=\"log2\")\nmodel3.fit(features_w2v, d_train.toxic.drop(d_train.index[rows_to_delete]))\n\ntest_sentences = tokenize(d_test.comment_text)\ntest_features_w2v = featurize_w2v(model, test_sentences)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c9867ec-a97e-4d57-ada9-25743172f0a1","collapsed":true,"_uuid":"610e4c07e9cd363bdc11752d05612565d07c8c4b","trusted":false},"cell_type":"code","source":"rows_to_delete = delete_nans(test_features_w2v)\ntest_features_w2v = np.delete(test_features_w2v, rows_to_delete, 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81983e67-9682-43c0-80f1-ecd0abfc6c4e","_uuid":"1a4da611790085948bddce9592ea47329bbacad9","trusted":false,"collapsed":true},"cell_type":"code","source":"pred3 = model3.predict_proba(test_features_w2v)\nperformance(d_test.toxic.drop(d_test.index[rows_to_delete]), pred3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1ee16fd-20f0-47eb-97f9-4a602519b9f3","_uuid":"84b8087581bf048c445cd7055a95c79a3cd75cc8","trusted":false,"collapsed":true},"cell_type":"code","source":"\nroc_auc_score(d_test.toxic.drop(d_test.index[rows_to_delete]), pred3[:,1])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31830c9a-d45c-407f-a95a-4898c42ce954","_uuid":"b23b1ea78bf4b3def01b7359f8bce27706b1dfbd"},"cell_type":"markdown","source":"This is definitely the best model so far"},{"metadata":{"_cell_guid":"ce62ca27-ba67-48b0-a87d-df7956e7d079","_uuid":"53a7d0c7bc59ae17b39b06eb01e7d30f977f06f1"},"cell_type":"markdown","source":"## Final prediction"},{"metadata":{"_cell_guid":"67004445-70fb-46b0-aa35-636d8d583421","collapsed":true,"_uuid":"5a61b91316614a9323332cde38c493551d32b2bc","trusted":false},"cell_type":"code","source":"labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\npreds = np.zeros((len(test), len(labels)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b30b58c4-0e21-4e7c-94d7-550162479329","_uuid":"e3261d94f196a1e72844329acdae60920279e993","trusted":false,"collapsed":true},"cell_type":"code","source":"sentences = tokenize(train.comment_text)\nmodel_w2v = Word2Vec(sentences, size=500, window=5, min_count=6, sample=1e-3, workers=2)\nmodel_w2v.init_sims(replace=True)\nfeatures_w2v = featurize_w2v(model_w2v, sentences)\ntrain_rows_to_delete = delete_nans(features_w2v)\nfeatures_w2v = np.delete(features_w2v, train_rows_to_delete, 0)\n\nfor i, label in enumerate(labels):\n    model_rfc = RandomForestClassifier(n_estimators=600, n_jobs=-1, max_features=\"log2\")\n    model_rfc.fit(features_w2v, train[label].drop(train.index[train_rows_to_delete]))\n\n    test_sentences = tokenize(test.comment_text.astype('str'))\n    test_features_w2v = featurize_w2v(model_w2v, test_sentences)\n    np.nan_to_num(test_features_w2v,copy=False)\n    \n    preds[:,i] = model_rfc.predict_proba(test_features_w2v)[:,1]\n    \n    print('{}/{} labels fitted'.format(i+1,len(labels)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d2fbf6c-8cf0-47f2-9496-3edca7c72c40","_uuid":"dd720d8676c721025e6d5e3e03c96c5245750059","trusted":false,"collapsed":true},"cell_type":"code","source":"subm_id = pd.DataFrame({'id': sample_submission[\"id\"]})\nsubmission = pd.concat([subm_id, pd.DataFrame(preds, columns = labels)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}