{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tensorflow_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport re\nimport os\nimport json\nimport shutil\nimport string\nimport joblib\nfrom io import StringIO\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nimport tensorflow_text as text\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nfrom sklearn import model_selection \nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.models import Model\nfrom keras.layers import *\nfrom tensorflow.python.keras.utils.vis_utils import plot_model\n\n\nEMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/subashgandyer/toxiccomments\nwith open('../input/toxiccomments/train.csv', 'r', encoding='utf-8') as f:\n    train = pd.read_csv(f, sep=',', engine='python').dropna(how='all', axis=1)\nwith open('../input/toxiccomments/test.csv', 'r', encoding='utf-8') as f:\n    test = pd.read_csv(f, sep=',', engine='python').dropna(how='all', axis=1)\nwith open('../input/toxiccomments/test_labels.csv', 'r', encoding='utf-8') as f:\n    test_labels = pd.read_csv(f, sep=',', engine='python').dropna(how='all', axis=1)\nwith open('../input/toxiccomments/sample_submission.csv', 'r', encoding='utf-8') as f:\n    sub = pd.read_csv(f, sep=',', engine='python').dropna(how='all', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dashboard_training(history):\n    plt.figure(figsize=(15,5))\n    epochs = range(len(history.history['AUC']))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epochs,history.history['AUC'],'-o',label='Train AUC',color='#ff7f0e')\n    plt.plot(epochs,history.history['val_AUC'],'-o',label='Val AUC',color='#1f77b4')\n    x = np.argmax(history.history['val_AUC'] ); y = np.max( history.history['val_AUC'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n    plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=15)\n    plt.legend(loc=4)\n    \n    \n    plt.subplot(1, 2, 2)\n    plt2 = plt.gca().twinx()\n    plt2.plot(epochs,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(epochs,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    x = np.argmin(history.history['val_loss'] ); y = np.min(history.history['val_loss'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0];ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n    plt.ylabel('Loss',size=14);plt.xlabel('Epoch',size=15)\n    plt.legend(loc=2)\n    \n    plt.show()\n    \ndef plot_lr_history(history):\n    epochs = range(len(history.history['accuracy']))\n    plt.figure(figsize=(10,5))\n    \n    plt.rc('grid', linestyle=\"--\", color='black')\n    plt.semilogx(epochs,history.history[\"lr\"],'-o',label='Learning Rate',color='#d62728')\n    x = np.argmin(history.history['lr'] ); y = np.min(history.history['lr'] )\n    xdist = plt.xlim()[1] - plt.xlim()[0];ydist = plt.ylim()[1] - plt.ylim()[0]\n    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'Min lr\\n%.2E'%y,size=14)\n    plt.ylabel('Learning Rate',size=15);plt.xlabel('Epoch',size=15)\n    plt.xlim(0,len(epochs)+15)\n    plt.legend(loc=2)\n    plt.grid(True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.copy()\nN_splits = 20\ndf = df.drop(columns = [\"id\"])\ndf[\"kfold\"] = -1 # create a new column\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.toxic.values\nkf = model_selection.StratifiedKFold(n_splits=N_splits)\nfor f,(t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,\"kfold\"] = f\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = 100000\nembedding_dim = 300\nmax_length = 150\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n\nDEVICE = \"GPU\" #or \"TPU\"\nBUFFER_SIZE = 10000\nBATCH_SIZE = 1024\nEPOCHS = 10\nLR = 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEVICE == \"GPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model with the minimum validation loss\n# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\nEarlyStopping_cb = EarlyStopping(monitor='val_loss', mode='min', patience=5,restore_best_weights=True, verbose=1)\n\n# Save best model\nCheckpoint_cb_AUC = ModelCheckpoint(\"best_AUC_model.h5\",save_best_only=True,monitor='val_AUC',mode='max')\nCheckpoint_cb_loss = ModelCheckpoint(\"best_loss_model.h5\",save_best_only=True,monitor='val_loss',mode='min')\n\n# Reduce learning rate once learning stagnates\n# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\nReduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.2,patience=5,min_lr=1e-15,mode='min',verbose=1)\n\nCsv_logger = tf.keras.callbacks.CSVLogger('training.log')\n\nAdamOptimizer = tf.keras.optimizers.Adam(learning_rate = LR)\nRMSpropOptimizer = tf.keras.optimizers.RMSprop(learning_rate = LR)\nSGDOptimizer = tf.keras.optimizers.SGD(learning_rate = LR)\nAdagradOptimizer = tf.keras.optimizers.Adagrad(learning_rate = LR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def My_model_1():\n    METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),tf.keras.metrics.AUC(name='AUC')]\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(12, activation='relu'),\n    tf.keras.layers.Dense(6, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy', optimizer=AdamOptimizer, metrics=METRICS)\n    return model\n\ndef My_model_2():\n    METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),tf.keras.metrics.AUC(name='AUC')]\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(6, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n    return model\n\ndef My_model_3(embedding_matrix):\n    METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),tf.keras.metrics.AUC(name='AUC')]\n    sequence_input = tf.keras.layers.Input(shape=(max_length, ))\n    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix],trainable = False)(sequence_input)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n    x = tf.keras.layers.Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(x)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(x)\n    x = tf.keras.layers.concatenate([avg_pool, max_pool]) \n    # x = Dense(128, activation='relu')(x)\n    # x = Dropout(0.1)(x)\n    preds = tf.keras.layers.Dense(6, activation=\"sigmoid\")(x)\n    model = tf.keras.models.Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = My_model_1()\n\ntrain_df = df[df.kfold !=0].reset_index(drop=True)\ntest_df = df[df.kfold == 0].reset_index(drop=True)\n\ntraining_sentences = train_df.comment_text.values\ntesting_sentences = test_df.comment_text.values\ntraining_labels = train_df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values\ntesting_labels = test_df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values\n\ntokenizer = Tokenizer(num_words=vocab_size,filters='\"#$%&+-/:;<=>@[\\\\]^_`{|}~\\n', oov_token=oov_tok,lower=True)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# Need this block to get it to work with TensorFlow 2.x\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((training_padded,training_labels))\ntest_dataset = tf.data.Dataset.from_tensor_slices((testing_padded,testing_labels))\n\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))\n\nhistory = model.fit(train_dataset,\n                #steps_per_epoch=int(len(train_df)/BATCH_SIZE)//REPLICAS,  # Nv images = batch_size * steps\n                epochs=EPOCHS,\n                validation_data=test_dataset,\n                #validation_steps=int(len(test_df)/BATCH_SIZE)//REPLICAS,  # Nb images = batch_size * steps\n                verbose=1,callbacks=[Checkpoint_cb_AUC,Checkpoint_cb_loss])\ndashboard_training(history)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_sentences = test.comment_text.values\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\ntesting_padded = np.array(testing_padded)\ntest_dataset = tf.data.Dataset.from_tensor_slices((testing_padded))\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))\nXtest = test_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = tf.keras.models.load_model(\"./best_loss_model.h5\")\ny_test = model.predict(Xtest, batch_size=BATCH_SIZE, verbose=1)\nsub[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = y_test\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe "},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df[df.kfold !=0].reset_index(drop=True)\ntest_df = df[df.kfold == 0].reset_index(drop=True)\n\ntraining_sentences = train_df.comment_text.values\ntesting_sentences = test_df.comment_text.values\ntraining_labels = train_df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values\ntesting_labels = test_df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values\n\ntokenizer = Tokenizer(num_words=vocab_size,filters='\"#$%&+-/:;<=>@[\\\\]^_`{|}~\\n', oov_token=oov_tok,lower=True)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# Need this block to get it to work with TensorFlow 2.x\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((training_padded,training_labels))\ntest_dataset = tf.data.Dataset.from_tensor_slices((testing_padded,testing_labels))\n\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))\ntest_dataset = test_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\n#prepare embedding matrix\nnum_words = min(vocab_size, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in word_index.items():\n    if i >= num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = My_model_3(embedding_matrix)\nhistory = model.fit(train_dataset,\n                #steps_per_epoch=int(len(train_df)/BATCH_SIZE)//REPLICAS,  # Nv images = batch_size * steps\n                epochs=EPOCHS,\n                validation_data=test_dataset,\n                #validation_steps=int(len(test_df)/BATCH_SIZE)//REPLICAS,  # Nb images = batch_size * steps\n                verbose=1,callbacks=[Checkpoint_cb_AUC,Checkpoint_cb_loss])\ndashboard_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = tf.keras.models.load_model(\"./best_loss_model.h5\")\ny_test = model.predict(Xtest, batch_size=BATCH_SIZE, verbose=1)\nsub[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = y_test\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE_FASTTEXT = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index_2 = {}\nwith open(EMBEDDING_FILE_FASTTEXT,encoding='utf8') as f:\n    for line in f:\n        values_2 = line.rstrip().rsplit(' ')\n        word_2 = values_2[0]\n        coefs_2 = np.asarray(values_2[1:], dtype='float32')\n        embeddings_index_2[word_2] = coefs_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\n#prepare embedding matrix\nnum_words = min(vocab_size, len(word_index) + 1)\nembedding_matrix_2 = np.zeros((num_words, embedding_dim))\nfor word, i in word_index.items():\n    if i >= num_words:\n        continue\n    embedding_vector = embeddings_index_2.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix_2[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = My_model_3(embedding_matrix_2)\nhistory = model.fit(train_dataset,\n                #steps_per_epoch=int(len(train_df)/BATCH_SIZE)//REPLICAS,  # Nv images = batch_size * steps\n                epochs=EPOCHS,\n                validation_data=test_dataset,\n                #validation_steps=int(len(test_df)/BATCH_SIZE)//REPLICAS,  # Nb images = batch_size * steps\n                verbose=1,callbacks=[Checkpoint_cb_AUC,Checkpoint_cb_loss])\ndashboard_training(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe + fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"def My_model_4(embedding_matrix1,embedding_matrix2):\n\n    METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),tf.keras.metrics.AUC(name='AUC')]\n    sequence_input = tf.keras.layers.Input(shape=(max_length, ))\n    x1 = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix1],trainable = False)(sequence_input)\n    x1 = tf.keras.layers.Dropout(0.2)(x1)\n    x1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x1)\n    x1 = tf.keras.layers.Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x1)\n    avg_pool1 = tf.keras.layers.GlobalAveragePooling1D()(x1)\n    max_pool1 = tf.keras.layers.GlobalMaxPooling1D()(x1)\n    x1 = tf.keras.layers.concatenate([avg_pool1, max_pool1])\n    \n    x2 = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix2],trainable = False)(sequence_input)\n    x2 = tf.keras.layers.Dropout(0.2)(x2)\n    x2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x2)\n    x2 = tf.keras.layers.Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x2)\n    avg_pool2 = tf.keras.layers.GlobalAveragePooling1D()(x2)\n    max_pool2 = tf.keras.layers.GlobalMaxPooling1D()(x2)\n    x2 = tf.keras.layers.concatenate([avg_pool2, max_pool2])\n    \n    x = tf.keras.layers.concatenate([x1, x2])\n    # x = Dense(128, activation='relu')(x)\n    # x = Dropout(0.1)(x)\n    preds = tf.keras.layers.Dense(6, activation=\"sigmoid\")(x)\n    model = tf.keras.models.Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_1 = embedding_matrix\n\nmodel =  My_model_4(embedding_matrix_1,embedding_matrix_2)\n#model = build_model_with_sequential()\n# Plot model graph\nplot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nhistory = model.fit(train_dataset,\n                #steps_per_epoch=int(len(train_df)/BATCH_SIZE)//REPLICAS,  # Nv images = batch_size * steps\n                epochs=EPOCHS,\n                validation_data=test_dataset,\n                #validation_steps=int(len(test_df)/BATCH_SIZE)//REPLICAS,  # Nb images = batch_size * steps\n                verbose=1,callbacks=[Checkpoint_cb_AUC,Checkpoint_cb_loss])\ndashboard_training(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = tf.keras.models.load_model(\"./best_loss_model.h5\")\ny_test = model.predict(Xtest, batch_size=BATCH_SIZE, verbose=1)\nsub[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = y_test\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}