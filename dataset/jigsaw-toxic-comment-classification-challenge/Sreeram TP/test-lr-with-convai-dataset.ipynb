{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","source":"This is a basic LogisticRegression model trained using the data from https://www.kaggle.com/eoveson/convai-datasets-baseline-models\n\nThe baseline model in that kernal is tuned a little to get the data for this kernal\nThis kernal scored 0.045 in the LB","metadata":{}},{"cell_type":"code","outputs":[],"source":"# loading libraries\nimport pandas as pd, numpy as np","execution_count":null,"metadata":{"collapsed":true,"_uuid":"4bec1f3e60c75b2f5694d85851008926c770a1ee","_cell_guid":"12b0e062-9372-43eb-b26c-2c3bb069d63f"}},{"cell_type":"code","outputs":[],"source":"# fixing seed.!!\nseed = 7\nnp.random.seed(seed)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"377f028f429bba39a20bfd674d80f0a0a3fcaeab","_cell_guid":"7381b67f-ad06-4e01-b2ed-19fa15b00b23"}},{"cell_type":"code","outputs":[],"source":"# output of the kernal https://www.kaggle.com/eoveson/convai-datasets-baseline-models with some tunings\ntest_new = pd.read_csv('../input/convai-datasets-baseline-models/test_with_convai.csv')\ntrain_new = pd.read_csv('../input/convai-datasets-baseline-models/train_with_convai.csv')","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a0c99b65b855c65427c768fac481da88e377d815","_cell_guid":"bcf50cb6-0514-4f3b-a13c-88bdf5d57b67"}},{"cell_type":"code","outputs":[],"source":"# features we are interesed on\nfeats_to_concat = ['comment_text', 'toxic_level', 'attack', 'aggression']","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a781c415eef3c4178ed9e0ec83eb2532fd70b851","_cell_guid":"c2addf90-27fe-4ebe-9b4d-d97bb5af7517"}},{"cell_type":"code","outputs":[],"source":"# combining test and train\nalldata = pd.concat([train_new[feats_to_concat], test_new[feats_to_concat]], axis=0)\nalldata.comment_text.fillna('unknown', inplace=True)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"81056be77a391b390e23a562d606a68ac8e71187","_cell_guid":"dc0ca431-70fe-40fa-b94e-85776cb55b65"}},{"cell_type":"code","outputs":[],"source":"# loading libraries\nimport nltk\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport re","execution_count":null,"metadata":{"_uuid":"ae77a08bdf798e19edf38c8874741b9d60dd0c6d","_cell_guid":"5e562d1a-20f3-44a7-b23e-7cb58b844fda"}},{"cell_type":"code","outputs":[],"source":"# define function for cleaning..!!\n\ndef cleanData(text, stemming = False, lemmatize=False):\n    \n    text = text.lower().split()\n    text = \" \".join(text)\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n   \n    if stemming:\n        st = PorterStemmer()\n        txt = \" \".join([st.stem(w) for w in text.split()])\n        \n    if lemmatize:\n        wordnet_lemmatizer = WordNetLemmatizer()\n        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])\n\n\n    return text","execution_count":null,"metadata":{"collapsed":true,"_uuid":"78d45e24dfd66206a5383fdae10012a7e1da6b15","_cell_guid":"0dd15f0d-025c-4c42-9292-b8b9739daaad"}},{"cell_type":"code","outputs":[],"source":"# cleaning data - stemm and lemm are done later\nalldata['comment_text'] = alldata['comment_text'].map(lambda x: cleanData(x,  stemming = False, lemmatize=False))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"3826014d622ac8621a9778ad5a993fda33628091","_cell_guid":"054f6e37-7eab-4dc6-8923-ee2a26845f0c"}},{"cell_type":"code","outputs":[],"source":"# again libraries.!!\nfrom matplotlib import pyplot as plt\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom scipy import sparse","execution_count":null,"metadata":{"collapsed":true,"_uuid":"e4d2ea1d8ca7311f8659fe3d1f8eaa89786bc65d","_cell_guid":"c26fe15d-025f-4669-a8e3-4f3a0a7d2884"}},{"cell_type":"code","outputs":[],"source":"# set stopwords\nfrom nltk.corpus import stopwords\n\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"370ed58fb2e93e545b58b482914c8a00e0b5e0b0","_cell_guid":"818af801-81f1-4a28-89e3-cdefcab0c33e"}},{"cell_type":"code","outputs":[],"source":"# stemming and lemmatizing\n# adapted from the kernal \nstemmer = EnglishStemmer()\n\n@lru_cache(30000)\ndef stem_word(text):\n    return stemmer.stem(text)\n\n\nlemmatizer = WordNetLemmatizer()\n\n@lru_cache(30000)\ndef lemmatize_word(text):\n    return lemmatizer.lemmatize(text)\n\n\ndef reduce_text(conversion, text):\n    return \" \".join(map(conversion, wordpunct_tokenize(text.lower())))\n\n\ndef reduce_texts(conversion, texts):\n    return [reduce_text(conversion, str(text))\n            for text in tqdm(texts)]","execution_count":null,"metadata":{"collapsed":true,"_uuid":"91930ebb920a4584ceb23e97a4a08b873b43fe54","_cell_guid":"75d8a64e-59af-4528-aef7-f050afbec4ee"}},{"cell_type":"code","outputs":[],"source":"# lemmatizing and stemming\nalldata['comment_text'] = reduce_texts(stem_word, alldata['comment_text'])\nalldata['comment_text'] = reduce_texts(lemmatize_word, alldata['comment_text'])","execution_count":null,"metadata":{"_uuid":"8638487f82962c96ee7fe6cbe5f64e90e19b05f7","_cell_guid":"b26a1073-e0da-461f-9a68-00a0f7728e98"}},{"cell_type":"code","outputs":[],"source":"# making placeholder for prediction\ncol = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nonly_col = ['toxic']\n\npreds = np.zeros((test_new.shape[0], len(col)))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"479a47cfb10222633d154a4702b671a5e282c4b4","_cell_guid":"934ffb62-8c22-4da2-90e5-61aae99c81f5"}},{"cell_type":"code","outputs":[],"source":"# TfidfVectorizer for words and chars\nvect_words = TfidfVectorizer(max_features=40000, analyzer='word', ngram_range=(1, 1))\nvect_chars = TfidfVectorizer(max_features=10000, analyzer='char', ngram_range=(1, 3))","execution_count":null,"metadata":{"collapsed":true,"_uuid":"63f59a9e8880cfa429b3dc6bbc4d80b5d2752a22","_cell_guid":"dc07560b-d84f-4460-a712-cbc2a92ebc82"}},{"cell_type":"code","outputs":[],"source":"# Creating features\nall_words = vect_words.fit_transform(alldata.comment_text)\nall_chars = vect_chars.fit_transform(alldata.comment_text)","execution_count":null,"metadata":{"collapsed":true,"_uuid":"13ef5bc73e40832e03df02804cc2140553046822","_cell_guid":"65253ee8-b182-42e6-9567-1d4b9a1ee3db"}},{"cell_type":"code","outputs":[],"source":"# splitting to train and test\ntrain_words = all_words[:len(train_new)]\ntest_words = all_words[len(train_new):]\n\ntrain_chars = all_chars[:len(train_new)]\ntest_chars = all_chars[len(train_new):]","execution_count":null,"metadata":{"collapsed":true,"_uuid":"09f2f806538b375bc32fbb0a43001444c36996ca","_cell_guid":"50d9a084-21cc-4ca6-8fe3-a2a139f59c2d"}},{"cell_type":"markdown","source":"It can be seen from the dataset that the features attack and aggression is very much same. So we will only take one.\nHere I take attack leaving aggression","metadata":{}},{"cell_type":"code","outputs":[],"source":"# needed feats.!!\nfeats = ['toxic_level', 'attack']","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a3aadec5446294b7562bbf1ac199ef8a0267edc6","_cell_guid":"f30e63b0-3c19-4feb-b9bb-7ed817be5a29"}},{"cell_type":"code","outputs":[],"source":"# make sparse matrix with needed data for train and test\ntrain_feats = sparse.hstack([train_words, train_chars, alldata[feats][:len(train_new)]])\ntest_feats = sparse.hstack([test_words, test_chars, alldata[feats][len(train_new):]])","execution_count":null,"metadata":{"collapsed":true,"_uuid":"4725cf76651697d80da9d3bef90d41a833df3edc","_cell_guid":"606265d2-a25e-4181-b2bd-acc4bc2fa1f4"}},{"cell_type":"code","outputs":[],"source":"# libraries.!\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0f4cae7efd9bb89b940d6c7ec316a9a596980164","_cell_guid":"07088448-6278-4cea-9f70-d46316c823c0"}},{"cell_type":"code","outputs":[],"source":"# fit a LogisticRegression model on full train data and make prediction\nfor i, j in enumerate(col):\n    print('===Fit '+j)\n    \n    model = LogisticRegression(C=4.0, solver='sag')\n    print('Fitting model')\n    model.fit(train_feats, train_new[j])\n      \n    print('Predicting on test')\n    preds[:,i] = model.predict_proba(test_feats)[:,1]","execution_count":null,"metadata":{"_uuid":"1f451754ac9680abdb7cc2441fed85db02fc8e58","_cell_guid":"3b256937-6512-48b9-b07c-e959e5602377"}},{"cell_type":"code","outputs":[],"source":"# make submission..!!\nsubm = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n\nsubmid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\nsubmission.to_csv('feat_lr_2cols.csv', index=False) # 0.045 in the LB","execution_count":null,"metadata":{"collapsed":true,"_uuid":"8f7fbf044a6cd80279ad839def5e67976d882e4e","_cell_guid":"14f3a10f-ab93-42f3-82e8-239e44a79516"}},{"cell_type":"code","outputs":[],"source":"","execution_count":null,"metadata":{"collapsed":true,"_uuid":"66577e4b1500b0f6fb1411aa54966267bcfa7875","_cell_guid":"4444de1c-5160-4e5d-a8dd-6bf6fbcb45e9"}}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"nbconvert_exporter":"python","version":"3.6.3","name":"python","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","pygments_lexer":"ipython3"}}}