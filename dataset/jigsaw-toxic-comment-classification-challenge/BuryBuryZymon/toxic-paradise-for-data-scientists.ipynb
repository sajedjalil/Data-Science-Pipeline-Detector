{"cells":[{"cell_type":"markdown","source":"# Preamble \nIn this problem, I think we have to fit figure out the category type of a given sentence. We are given sentences and there are different categories, I suspect that a same sentence can be of 2 categories ( We will check it in analysis). we will be doing feature engineering and will fitting a seperate guassian for each class of sentence (analogous to soft clustering) and will give probability for given sentence.\n\nIn this kernel I will be starting features extraction for this competition, I will be extracting various features related to POS, tf_idf etc and then build a model to classify them. Lets start with following features - \n\n- **Grams features ** - grams features are nothing but n-grams features and represents \n- **Sentiment Analysis features ** - sentiment analysis features detect sentiments of given sentence \n- **POS features ** - Part of speech features for a sentense given \n\n**Note** - upvote if you like my kernel and all suggestions are welcome. \n","metadata":{"_cell_guid":"5782d729-bd55-4de5-975a-42011e17a9da","_uuid":"62f9d064285ee9a9a888aa815b3f88543cbd7c51"}},{"source":"import pandas as pd\nimport numpy as np \nimport numpy as np\nfrom textblob import TextBlob\nimport nltk\nimport string\nimport random\nimport tensorflow as tf\nimport os\nimport io\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport gensim\nimport csv, collections\nfrom textblob import TextBlob\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn import ensemble, metrics, model_selection, naive_bayes\nfrom nltk.corpus import stopwords","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6721792f-3a1b-44b3-a2d7-bb5b8b1fb739","_uuid":"680a69e51219899546ff8abf70e253e86b060f08"}},{"source":"train = pd.read_csv(\"../input/train.csv\")\ntrain.head()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"86ae4e28-028f-40c2-852e-e2142a87ecf9","_uuid":"6422bb959fb4b35508a8c44c51e8b599ea12ec84"}},{"source":"test = pd.read_csv(\"../input/test.csv\")\nsubm = pd.read_csv('../input/sample_submission.csv')\ntest.head()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6a26e8b3-8a7b-496e-855b-66396a5260c2","_uuid":"9d42155152c25d9d20941eea23218b87cf6689cf"}},{"cell_type":"markdown","source":"## Sanity check","metadata":{"_cell_guid":"07b4a33a-b267-4fa1-84c0-fc4965175784","_uuid":"5f940a1516bacd26697875ceb0a811fc4d8cad20"}},{"source":"print(\"Number of sentences in train data is {}\".format(train.shape[0]))\nprint(\"Number of NAs in train_data {}\".format(train.isnull().sum()))\ncategories = ['toxic','severe_toxic','obscene','threat','insult', 'identity_hate']\nsanity = pd.DataFrame(train.groupby(categories)['id'].count())\nsanity_copy = sanity.copy()\nsanity.reset_index(inplace = True)\nif sanity.shape[0] == 5:\n    print(\"One sentence falls into one category\")\nelse:\n    print(\"They want us to train multiple models NN or GB, OR Gaussian miture models\")   ","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"8bd18e03-ca24-46ed-9d6e-79707da2dd37","_uuid":"c9c9c5a411f304aea2526fbdd61d9c4222abf796"}},{"cell_type":"markdown","source":"# Features' Extraction \n- **Grams features ** - grams features are nothing but n-grams features and represents \n- **Sentiment Analysis features ** - sentiment analysis features detect sentiments of given sentence \n- **POS features ** - Part of speech features for a sentense given ","metadata":{"_cell_guid":"8a6afa1a-77cf-4b7d-a2c9-66a521e41ccf","_uuid":"a8cc6ac460f890084b5e841ea9c3cc303a98e9e7"}},{"source":"train_df = train.copy() # just saving copy of train data \ntest_df = test.copy()\neng_stopwords = set(stopwords.words(\"english\"))\nimport time\nstart = time.time()\ndef remove_noise(row):\n    \"\"\"function to remove unnecessary noise from the data - sentences\"\"\"\n    try:\n        text = row['comment_text']\n        text_splited = text.split(' ')\n        text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n        noise_words = ['\\n', '\\n\\n']\n        text_splited = [''.join(c for c in s if c not in noise_words) for s in text_splited]\n        text_splited = [s for s in text_splited if s]\n        return(text_splited)\n    except:\n        return(row['comment_text'])\n    \n    \n    \ndef grams_features(train_df, test_df):\n    \"\"\"function to extract grams features for a given sentence\"\"\"\n    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n    full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n    train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n    test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())\n    return(train_tfidf, test_tfidf)\n        \nend = time.time()    \nprint(\"Time taken in tf-idf is {}.\".format(end-start))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1ab4441d-08ed-4ec3-8e10-1aed3c5fc858","_uuid":"46797514425b7e9fc11731ff671d533d82609388"}},{"source":"train_df['processed_text'] = train_df.apply(lambda row: remove_noise(row), axis = 1)\ntest_df['processed_text'] = test_df.apply(lambda row: remove_noise(row), axis = 1)\ntrain_df.head()","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"234b1f55-e403-40fa-bc90-5107482e192c","_uuid":"e2c6a7483a162b9c7f07354ee8b631602f664c15"}},{"source":"train_df.dropna(inplace = True)\ntest_df.dropna(inplace = True)\nstart = time.time()\ntrain_tfidf, test_tfidf = grams_features(train_df, test_df)\nend = time.time()\nprint(\"Time taken in tf-idf is {}.\".format(end-start))","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5fc47fec-4b93-4d93-a4a8-2b0e1e4899ea","_uuid":"7e5c8b654508853c41a5d3cc26b558487bede1da"}},{"cell_type":"markdown","source":"# NB-SVM \nI thought of developing model later but after looking at [Jeremy Howard's NB-SVM baseline (0.06 lb)](http://https://www.kaggle.com/jhoward/nb-svm-baseline-0-06-lb). I would also like to try this on whatever features I have. Lets make a basic model. All credits goes to Jeremy for next cell of code.\n\n","metadata":{"_cell_guid":"4a2307eb-7d37-4ba5-8cf2-55e8d7c6a068","_uuid":"8ab42f6078a97f3d3c084014c59fadc554c4ef08"}},{"source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ndef pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) / ((y==y_i).sum()+1)\n\nx=train_tfidf.sign()\ntest_x = test_tfidf.sign()\nfrom sklearn.linear_model import LogisticRegression\ndef get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) / pr(0,y))\n    m = LogisticRegression(C=0.1, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r\n\npreds = np.zeros((len(test_df), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train_df[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c27472ec-34ae-4830-8587-d3b3c388aec9","_uuid":"1c49f9df36e13bbf2b9921013d38f9b7de8162ea"}},{"source":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","cell_type":"code","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"**Gaussian discriminant analysis is to be implemented soon ...**","metadata":{}},{"source":"","cell_type":"code","outputs":[],"execution_count":null,"metadata":{"collapsed":true}}],"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","version":"3.6.3","mimetype":"text/x-python","file_extension":".py"}}}