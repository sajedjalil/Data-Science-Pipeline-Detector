{"cells":[{"cell_type":"markdown","source":"Motivation:\n\nCloned from kaggle kernel (details in fork)\nhttps://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n\n\n\n## Word embeddings with Gensim\n\nThe importance of encoding text data is crucial for Deep Learning models. A model that encodes the similarity and proximity between words in the representation itself intuitively should work better for many tasks and it has been proved to be so - it is not always the best choice though: it's no silver bullet.\n\nTwo of the most important models for word representation in the n-dimensional space are [word2vec](https://arxiv.org/abs/1310.4546) and [GloVe](https://nlp.stanford.edu/projects/glove/). \n\nIn this tutorial, I will show how to use [Gensim](https://radimrehurek.com/gensim/index.html) in order to use both word2vec and GloVe encodings for text data.\n\nI assume you already know how to setup an environment for machine learning development with Python. If you don't, take a look at [this tutorial](https://medium.com/cocoaacademymag/basic-tools-for-machine-learning-85e887224ee4) on the basic tools for Machine Learning, which has everything you will need to follow this one.\n\n### Summary\n\n* ✅ Installing and importing Gensim\n* ✅ Creating a word2vec model from text data\n* Creating a GloVe model from text data\n* Intrinsic evaluation for both models\n* Extrinsic evaluation for both models","metadata":{"_cell_guid":"4f572eef-47be-40a0-a01c-14aa40891e25","_uuid":"c178b3ad1ef833e83fe290c5a6e7a1a64e938435"}},{"cell_type":"markdown","source":"# word2vec\n\n## Preparing the text to train the model\n\nIn this example, I will open a csv file, get the text from it, split it into the different lines, then I split each line into \"words\" - actually, I should use a more sophisticated method to separate the words, but since this is just an example, I will use the space as a boundary between words, *which is absolutely naive and should not be done in production* - stripping the ponctuation in order to clean the corpus a little. In _real life_ you should use a tokenizer in order to separate the tokens to be vectorized and also in order to handle ponctuation properly. Depending on the task, it might also be helpful to lemmatize the tokens.\n\nMy `sentences` variable will store a list of lists of strings, where each string ~roughly~ represents a word.","metadata":{"_cell_guid":"a8dfd008-9782-44d7-8849-a43553534aa1","_uuid":"e137fcc35264c842f8072a69a4747d58695b5e41"}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/train.csv')\ncorpus_text = '\\n'.join(df[:50000]['comment_text'])\nsentences = corpus_text.split('\\n')\nsentences = [line.lower().split(' ') for line in sentences]","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b83a560a-4ead-4139-bed9-20fea0806427","collapsed":true,"_uuid":"68f9c07ff274c9fc01ab5a2b9bcaebddfc008a1a"}},{"cell_type":"code","source":"def clean(s):\n    return [w.strip(',.\"!?:;()\\'') for w in s]\nsentences = [clean(s) for s in sentences if len(s) > 0]","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"18a09cc6-424a-44e3-9698-27ebdbca55b2","collapsed":true,"_uuid":"8e9e5a08ee23a1dc9ebbb4db002090b59fae712a"}},{"cell_type":"markdown","source":"## Training the model\n\nOnce we have the sentences, we can use `Gensim` to create a model for us.\nHere's a simple way to do it:","metadata":{"_cell_guid":"dd4c9adf-5c76-46f0-95ac-fe36f9230b93","_uuid":"1224cca9290ff75fd8c023732ab57951aa883a8e"}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nmodel = Word2Vec(sentences, size=100, window=5, min_count=3, workers=4)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a5e1c300-03eb-4dfe-8e24-4f20d75e097f","collapsed":true,"_uuid":"f36be6330aca74250bc8bbda63e479abcfd7c4ae"}},{"cell_type":"markdown","source":"modeOf course, you can change the hyperparameters such as window size or the dimensions of the resulting vectors to get better results.\nIf our model is too big, and we're done training it we can delete it keeping only the vectors.","metadata":{"_cell_guid":"0738e214-7043-4a6d-b577-276ceae0a97e","_uuid":"7bf1943de47ca5baa9a03b852d2aa87c70abcdf0"}},{"cell_type":"code","source":"vectors = model.wv\n# del model","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"11bc5378-b827-4c28-83b0-a8560d6ef861","_uuid":"8a5e9d85374d8b5910b7a2244f099791a7422c7f"}},{"cell_type":"markdown","source":"## Using the vectors\n\nNow, for each word (as represented in a string), we can get its appropriate vector.","metadata":{"_cell_guid":"ba772f7f-b101-4d17-872b-15f2b78e895b","_uuid":"4cd0fb16ecb2eee34e1f4000f3c0a77274affd07"}},{"cell_type":"code","source":"vectors['good']","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a77486d5-3c8b-4a5d-b8b9-edb4e6e65117","_uuid":"efa09a4bbdd7f3d28428a139f2a61f3c1b4a58e5"}},{"cell_type":"markdown","source":"We can also compare words in order to assess their similarity, \ncheck which word is the most similar to a given word - i.e. the \none with the least distant vector.","metadata":{"_cell_guid":"7e4799c8-6b23-4ebd-8d30-d07210ddda0a","_uuid":"55f8fb1cb4dae1ef89e42186ff07e6bef6ffecd9"}},{"cell_type":"code","source":"print(vectors.similarity('you', 'your'))\nprint(vectors.similarity('you', 'internet'))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d57f9fda-93ff-4f50-a8b1-3e15227a8012","_uuid":"dbb4627996fafd6e713af366adf6ecca9f523ad0"}},{"cell_type":"code","source":"vectors.most_similar('kill')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"74d97c78-1c93-4290-b3d7-8e0abf4eda8d","_uuid":"a477366ca8b818f8037a04edd1b510cfd2b6b859"}},{"cell_type":"code","source":"len(model.wv.vocab)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"0eae119f-2241-4b4e-be60-b45fee7b73e4","_uuid":"b5289ee31ee4d5d040599ee86f20477b307a55e6"}},{"cell_type":"code","source":"# build a list of the terms, integer indices,\n# and term counts from the food2vec model vocabulary\nordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]\n\n# sort by the term counts, so the most common terms appear first\nordered_vocab = sorted(ordered_vocab, key=lambda k: -k[2])\n\n# unzip the terms, integer indices, and counts into separate lists\nordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n# print(ordered_terms)\n# create a DataFrame with the food2vec vectors as data,\n# and the terms as row labels\nword_vectors = pd.DataFrame(model.wv.syn0norm[term_indices, :], index=ordered_terms)\n\nword_vectors","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"0d72a978-1bdf-4b34-87e3-221666fbf2e9","_uuid":"3699297959e52f72d08fd5a3be27dba328b04c1b"}},{"cell_type":"code","source":"def get_related_terms(token, topn=10):\n    \"\"\"\n    look up the topn most similar terms to token\n    and print them as a formatted list\n    \"\"\"\n\n    for word, similarity in model.most_similar(positive=[token], topn=topn):\n        print (word, round(similarity, 3))","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2d69bbb1-a764-45ea-b018-34079a39e8ef","collapsed":true,"_uuid":"dfc0173de4192b05780a65b97d0ab941f5fb5599"}},{"cell_type":"code","source":"get_related_terms(u'killed')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"43212a76-8e0a-4dd3-85df-fcd5123c3984","_uuid":"0882058774f1cd0b63dfda2502d0327372357a0d"}},{"cell_type":"code","source":"get_related_terms(u'japanese')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"ef475ddd-2742-42ab-bea2-ba99f1a417c9","_uuid":"d8301d223b51b1b2814def81a9e8306d44b58f26"}},{"cell_type":"code","source":"get_related_terms(u'asshole')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4f2cd030-70e9-4dfa-b945-62b2109ecdb8","_uuid":"84e063fa47aa7c4a4c2e9d3bf5d19c9aed8e4837"}},{"cell_type":"code","source":"get_related_terms(u'discussion')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3fcbb8c9-cee6-41b8-8c0b-09ca5928f8c8","_uuid":"bbbe3cf6a0e88ad8ee4d129dd528aca8f3d94624"}},{"cell_type":"code","source":"get_related_terms(u'wikipedia')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e80675c4-f328-4219-8fdd-bf9621c95c13","_uuid":"b703412f21012196867aa5beab9039d86b553aa0"}},{"cell_type":"code","source":"get_related_terms(u'please')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"406a385f-93f4-438b-8292-8086933837e0","_uuid":"d23d5e9ad94d82a70ded5945d187cc5676761ce7"}},{"cell_type":"code","source":"get_related_terms(u'vandalism')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"5c1ececf-7990-4fcd-b7ba-5e0f94fd622f","_uuid":"f07a75ee9dd33f7c7b248023d0d3c523ee18c5a2"}},{"cell_type":"code","source":"get_related_terms(u'media')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"974a3b61-d4f2-4400-aee4-291a1d2d274c","_uuid":"2f725b2e7df6aa5bf0b08ee6e8df49e2c6c56360"}},{"cell_type":"code","source":"get_related_terms(u'language')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"583cbc06-e6e3-47a7-a1c5-7b06acc47347","_uuid":"5c10b27848ee186aa42bc217468e41644d7f6e1c"}},{"cell_type":"code","source":"get_related_terms(u'perhaps')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"773f0885-a0e1-4ca9-b58c-2ef22ba22f19","_uuid":"0e1539f458dd5f4637cea02d51f049278953fa81"}},{"cell_type":"code","source":"get_related_terms(u'sex')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"c3702fde-1230-4a1b-baa9-00fb6c0cba35","_uuid":"07e94428680a2a05d561247986f5a287333dab05"}},{"cell_type":"code","source":"get_related_terms(u'conflict')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3e1576eb-0347-4a80-a9c5-4ca35e786d35","_uuid":"9b3703a4aca5a1216f7d8e37bbb712a849ea8a6d"}},{"cell_type":"code","source":"get_related_terms(u'bastard')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"cf804d3c-5d69-4942-910d-9a0f662dc9d0","_uuid":"b5ed64b144820a5e8cd56821384563a1b80452d5"}},{"cell_type":"code","source":"get_related_terms(u'jewish')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"f1787c7b-2c3c-43a4-80e9-841690d4d305","_uuid":"27bf1e4e179dd952ed7bc5c7fca0b0a442ad2250"}},{"cell_type":"code","source":"get_related_terms(u'introduction')","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d9424d88-ee17-4c9e-b40d-137248117abc","_uuid":"e1b2ae754a1e6119b1322126969e02c07b9dea3f"}},{"cell_type":"code","source":"def word_algebra(add=[], subtract=[], topn=1):\n    \"\"\"\n    combine the vectors associated with the words provided\n    in add= and subtract=, look up the topn most similar\n    terms to the combined vector, and print the result(s)\n    \"\"\"\n    answers = model.most_similar(positive=add, negative=subtract, topn=topn)\n    \n    for term, similarity in answers:\n        print(term)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1eac9776-2c94-4d6a-9b80-83786481f44b","collapsed":true,"_uuid":"f31e5ff159a10b7d3b1a629834659f6ca7653333"}},{"cell_type":"code","source":"word_algebra(add=[u'i', u'will'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"7d7bba36-9c72-4bbf-b9ce-0409a0d65ff3","_uuid":"b91b02eccfa01fc9517fd3cccf5330cc5cd0ddb0"}},{"cell_type":"code","source":"word_algebra(add=[u'you', u'will'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"fe9ba6ce-62e4-491b-9d37-77b7cbe4371e","_uuid":"ddc800b10cebd109e9ac0601b1439d6c5e80d0e0"}},{"cell_type":"code","source":"word_algebra(add=[u'i', u'am'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2834c3ab-6c82-4def-9edc-705a40e3eff6","_uuid":"dd3cd9ed1709c232a1d852591fd4b3d365d49ccc"}},{"cell_type":"code","source":"word_algebra(add=[u'mother', u'fuck'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a8b6f82d-8af3-48c7-b8b8-f7452e16da02","_uuid":"8942401fd922a8f710a9f21e8e5f342b7aab1f15"}},{"cell_type":"code","source":"word_algebra(add=[ u'fuck', 'you'])","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"6f97b02b-af69-49db-b80a-f4eed35f9d29","_uuid":"d31785994f57499066f372606e9c407601fe9c21"}},{"cell_type":"code","source":"from sklearn.manifold import TSNE","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"2551db3c-708c-46ab-9785-3808814d72ac","collapsed":true,"_uuid":"a6816e49a69b7282b763108f6653b14d27e08e66"}},{"cell_type":"code","source":"tsne_input = word_vectors\ntsne_input = tsne_input.head(5000)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"7c2d4189-f32a-4726-ba2b-aa4ac6217dbd","collapsed":true,"_uuid":"86aa7fd3e5638674bdc2e91d0236922b7eb755ca"}},{"cell_type":"code","source":"tsne_input","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"1c495888-2c4a-4710-8e94-dfb585fa64f7","_uuid":"fd708fb861c78fb95553a27492d84c16930ab233"}},{"cell_type":"code","source":"tsne = TSNE()\ntsne_vectors = tsne.fit_transform(tsne_input.values)","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"a6320e5b-7e76-4e04-a6fe-6fa45862329d","collapsed":true,"_uuid":"6541457d6042f25a928f6c85935e46ca3abe6a2e"}},{"cell_type":"code","source":"tsne_vectors = pd.DataFrame(tsne_vectors,\n                            index=pd.Index(tsne_input.index),\n                            columns=[u'x_coord', u'y_coord'])\n\ntsne_vectors.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"30891d00-89b6-481b-8f4d-b43800085a72","_uuid":"b05efda7e343f2a87dbc674bcca3145897d03a55"}},{"cell_type":"code","source":"tsne_vectors[u'word'] = tsne_vectors.index","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"d9690674-ec0d-4423-8439-5ddde36d42bd","collapsed":true,"_uuid":"28285878829bdc3b4fcc2f8733267b0fd1489873"}},{"cell_type":"code","source":"tsne_vectors.head()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"b26cb3fc-3012-4b8c-afa7-de619033daac","_uuid":"ec27d615f8488d74aa9d732632b605d4f02721f2"}},{"cell_type":"code","source":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, value\n\noutput_notebook()","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"e65a227f-4eb3-443a-95e3-5a0167c98fda","_uuid":"14db1233fd8299942b6c247ea4ee80dc34f2747c"}},{"cell_type":"code","source":"# add our DataFrame as a ColumnDataSource for Bokeh\nplot_data = ColumnDataSource(tsne_vectors)\n\n# create the plot and configure the\n# title, dimensions, and tools\ntsne_plot = figure(title=u't-SNE Word Embeddings',\n                   plot_width = 800,\n                   plot_height = 800,\n                   tools= (u'pan, wheel_zoom, box_zoom,'\n                           u'box_select, reset'),\n                   active_scroll=u'wheel_zoom')\n\n# add a hover tool to display words on roll-over\ntsne_plot.add_tools( HoverTool(tooltips = u'@word') )\n\n# draw the words as circles on the plot\ntsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n                 color=u'blue', line_alpha=0.2, fill_alpha=0.1,\n                 size=10, hover_line_color=u'black')\n\n# configure visual elements of the plot\ntsne_plot.title.text_font_size = value(u'16pt')\ntsne_plot.xaxis.visible = False\ntsne_plot.yaxis.visible = False\ntsne_plot.grid.grid_line_color = None\ntsne_plot.outline_line_color = None\n\n# engage!\nshow(tsne_plot);","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"3c75755a-8bc1-46a0-879c-71f32f943ce5","_uuid":"2c1b00e567d71b6dd8924105cbff8539870d2e9b"}},{"cell_type":"code","source":"","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"4bd8edc4-6e11-43b5-8c47-9eca12e55c72","collapsed":true,"_uuid":"1a863e58c35eec9f6554ce8d466eeb0a302bc166"}}],"nbformat":4,"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}}