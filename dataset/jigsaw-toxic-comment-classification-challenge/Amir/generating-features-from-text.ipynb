{"nbformat":4,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py","name":"python","version":"3.6.4","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1,"cells":[{"source":"## Hello!\n\nIn this simple baseline notebook I will generate simple features from text to try predict the labels. I will use a combination of TfIdfVectorizer, SVD and XGBoost.","cell_type":"markdown","metadata":{"_cell_guid":"b398f7d2-a0f6-4dd2-ab1b-0e49c29ea496","_uuid":"cbf4397e8f23371c3c0f234e2d9d4856de8d6ed2"}},{"source":"import numpy as np\nimport pandas as pd \nfrom subprocess import check_output\nfrom gensim.models import Word2Vec\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold, train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score\n\nimport xgboost as xgb\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n\nalpha_tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\nlemmatizer = WordNetLemmatizer()\nstop = stopwords.words('english')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_cell_guid":"3ebd5d8f-a710-42f7-8cac-b7f6561e996c","_uuid":"e51dc9c2f054c7b6f0422bd5831f255a17047c5d","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsubmissions = pd.read_csv('../input/sample_submission.csv')\n\ntrain.comment_text.fillna('', inplace=True)\ntest.comment_text.fillna('', inplace=True)","metadata":{"_cell_guid":"be0df950-015a-43b3-904d-9c7248913ecd","_uuid":"dfc122975a7c7905f842d75387020e2675497944","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"How does the data look?","cell_type":"markdown","metadata":{"_cell_guid":"77356f64-0cfe-46b9-b74a-3ca1b2601840","_uuid":"8168f91219cbce511abde4d1e428560f2f298d4d"}},{"source":"train.head()","metadata":{"_cell_guid":"84161fd2-a950-4064-a4ba-91bcf6d14965","_uuid":"6cf335242b797a2a48ce0c3b227d384ce784d57a","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"plt.figure(figsize=(12,6))\nsns.barplot(train.toxic.value_counts().index, train.toxic.value_counts().values, alpha=0.8)\nplt.ylabel('Amount of class instances', fontsize=16)\nplt.xlabel('Class', fontsize=16)\nplt.show();","metadata":{"_cell_guid":"7e4a3a62-15fd-4e77-b7f9-8a248764403f","_uuid":"2fbb3b48f0c968f3010499769d713ee9fa56dfad","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"plt.figure(figsize=(12,6))\nsns.barplot(train.obscene.value_counts().index, train.obscene.value_counts().values, alpha=0.8)\nplt.ylabel('Amount of class instances', fontsize=16)\nplt.xlabel('Class', fontsize=16)\nplt.show();","metadata":{"_cell_guid":"f24a272d-940a-4b39-964c-92fcaca24e59","_uuid":"82c489e6bb33b3583080fe7d42e59be76ad1a648","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"There are a lot of different ways to generate features through different vectorizers based on count-matrices and co-ocurrence-matrices.","cell_type":"markdown","metadata":{"_cell_guid":"5d924eb4-55e5-4098-9df4-a2b1a8b4c54e","_uuid":"87f74a58c3af0f4ce7cc5a6161f10931b526730f"}},{"source":"vectorizers = [ ('3-gram TF-IDF Vectorizer on words', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n               ('3-gram Count Vectorizer on words', CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n               ('3-gram Hashing Vectorizer on words', HashingVectorizer(ngram_range=(1, 5), analyzer='word', binary=False)),\n                ('TF-IDF + SVD', Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n                                ('svd', TruncatedSVD(n_components=150)),\n                               ])),\n               ('TF-IDF + SVD + Normalizer', Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='word', binary=False)),\n                                ('svd', TruncatedSVD(n_components=150)),\n                                ('norm', Normalizer()),\n                               ]))\n              ]","metadata":{"_cell_guid":"184ce1f4-d100-497f-9b60-90b88243166a","_uuid":"23c413d42c1c01a1382ed45d250c5baaf7e383b1","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"estimators = [\n              (KNeighborsClassifier(n_neighbors=3), 'K-Nearest Neighbors', 'yellow'),\n              (SVC(C=1, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True,tol=0.001, verbose=False), 'Support Vector Machine', 'red'),\n              (LogisticRegression(tol=1e-8, penalty='l2', C=0.1), 'Logistic Regression', 'green'),\n              (MultinomialNB(), 'Naive Bayes', 'magenta'),\n              (RandomForestClassifier(n_estimators=10, criterion='gini'), 'Random Forest', 'gray'),\n              (None, 'XGBoost', 'pink')\n]","metadata":{"_cell_guid":"5b5175e9-061f-4952-b3a1-43fe186c9ea9","_uuid":"8ddf69b73342cf8570eec6b78e6c27cec25da503","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"params = {}\nparams['objective'] = 'multi:softprob'\nparams['eta'] = 0.1\nparams['max_depth'] = 3\nparams['silent'] = 1\nparams['num_class'] = 3\nparams['eval_metric'] = 'mlogloss'\nparams['min_child_weight'] = 1\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.3\nparams['seed'] = 0","metadata":{"_cell_guid":"6640e1e7-41be-4c95-8d2e-84f7f1bcf00a","_uuid":"5b87eede05110bf8ba94226bf4a5f5bd71474f2c","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"I will try to compare all of them splitting training set to the train chunk and to the test chunk.","cell_type":"markdown","metadata":{"_cell_guid":"f6edf577-e57d-43c9-94df-ebe3d710d799","_uuid":"5c0b1a4667c39aa4b8c6cf85da98f8d8e7f98f8b"}},{"source":"def vectorize():\n\n    test_size = 0.3\n\n    train_split, test_split = train_test_split(train[:10], test_size=test_size)\n\n    for column in range(2, len(train.axes[1])):\n        for vectorizer in vectorizers:\n            print(vectorizer[0] + '\\n')\n            X = vectorizer[1].fit_transform(train.comment_text.values)\n            X_train, X_test = train_test_split(X, test_size=test_size)\n            for estimator in estimators:\n                if estimator[1] == 'XGBoost': \n                    xgtrain = xgb.DMatrix(X_train, train_split.iloc[:,column].values)\n                    xgtest = xgb.DMatrix(X_test)\n                    model = xgb.train(params=list(params.items()), dtrain=xgtrain,  num_boost_round=40)\n                    predictions = model.predict(xgtest, ntree_limit=model.best_ntree_limit).argmax(axis=1)\n                else:\n                    estimator[0].fit(X_train, train_split.iloc[:,column].values)\n                    predictions = estimator[0].predict(X_test)\n                print(accuracy_score(predictions, test_split.iloc[:,column].values), estimator[1])","metadata":{"_cell_guid":"60f9b52f-6eea-4040-bce5-4e78cfef890e","_uuid":"38a490080c3318930d05e43465c0c20f9d3dc94a","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"To make a quick submission I will just make a simple pre-processing on the given text data and use a TfIDfVectorizer on words uni-grams.","cell_type":"markdown","metadata":{"_cell_guid":"a271b8d3-b860-4a34-98a4-169b544b8d72","_uuid":"cf01e586e19b1f6cb98b62e618c9f1b9f498df1b"}},{"source":"train_text = [' '.join([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop]) for sent in train.comment_text.values]\ntest_text = [' '.join([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(sent) if word.lower() not in stop]) for sent in test.comment_text.values]","metadata":{"_cell_guid":"990c52ac-5c44-4723-bb32-6a29854da207","_uuid":"3ebe0b2f1a94b59e830f0920cb325d2ff87a0e01","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"vectorizer = TfidfVectorizer(ngram_range=(1,1), analyzer='word')\n\nfull = vectorizer.fit_transform(train_text + test_text)\nX_train = vectorizer.transform(train_text)\nX_test = vectorizer.transform(test_text)\n\n# NUM_FEATURES = 100\n\n# model = Word2Vec(train_text + test_text, min_count=2, size=NUM_FEATURES, window=4, sg=1, alpha=1e-4, workers=4)\n\n# def get_feature_vec(tokens, num_features, model):\n#     featureVec = np.zeros(shape=(1, num_features), dtype='float32')\n#     missed = 0\n#     for word in tokens:\n#         try:\n#             featureVec = np.add(featureVec, model[word])\n#         except KeyError:\n#             missed += 1\n#             pass\n#     if len(tokens) - missed == 0:\n#         return np.zeros(shape=(num_features), dtype='float32')\n#     return np.divide(featureVec, len(tokens) - missed).squeeze()\n\n# train_vectors = []\n# for i in train_text:\n#     train_vectors.append(get_feature_vec([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(i) if word.lower() not in stop], NUM_FEATURES, model))\n    \n# test_vectors = []\n# for i in test_text:\n#     test_vectors.append(get_feature_vec([lemmatizer.lemmatize(word.lower()) for word in alpha_tokenizer.tokenize(i) if word.lower() not in stop], NUM_FEATURES, model))","metadata":{"_cell_guid":"9dbbba0d-0e8d-4c7f-9bfb-9fc7cebcce31","_uuid":"94338e873ffb1c23de4159cbaca75d9d8e044d64","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"X_train = train_vectors\nX_test = test_vectors","metadata":{"_cell_guid":"aae4915a-8338-4a63-98b0-57e5fd0628ab","_uuid":"2e01ff4008cc3f3a9cc6e608b11e96fba0b4a9a8","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"Then I will be able to give predictions using gradient boosting:","cell_type":"markdown","metadata":{"_cell_guid":"d970b464-3305-469f-b48f-8d7d3f078c65","_uuid":"5ba7c44abc7508a948648fef365c43022b0b160b"}},{"source":"start = 2\npredictions = np.zeros((len(test), len(train.axes[1]) - start))\n\nfor column in range(start, len(train.axes[1])):\n    y_train = train.iloc[:,column].values\n    # estimator = LogisticRegression(C = 0.01)\n    # estimator.fit(X_train, y_train)\n    # predictions[:,column - start] = estimator.predict_proba(X_test)[:,1]\n    xgtrain = xgb.DMatrix(X_train, y_train)\n    xgtest = xgb.DMatrix(X_test)\n    model = xgb.train(params=list(params.items()), dtrain=xgtrain, num_boost_round=500)\n    predictions[:,column - start]  = model.predict(xgtest, ntree_limit=model.best_ntree_limit)[:,1]","metadata":{"_cell_guid":"247147a3-e704-4aea-8540-1c58bd7aeef0","_uuid":"bb8358e7b3d2cbde09c51806811c52a65ea36b48","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"result = pd.concat([pd.DataFrame({'id': submissions['id']}), pd.DataFrame(predictions, columns = train.columns.values[2:])], axis=1)\nresult.to_csv('submission.csv', index=False)","metadata":{"_cell_guid":"280bc746-ad91-476a-ba49-6e252c61ace0","_uuid":"ba99d8c5e37cafa727514bc7343a06bb74bdfd25","collapsed":true},"cell_type":"code","outputs":[],"execution_count":null},{"source":"Of course, this is not all, and this kernel is just a simple baseline. I am working on a more interesting model right now, and I will start with more powerful methods of vectorizing the text data like Doc2Vec or FastText -- stay tuned! I will be glad to hear your comments and suggestions!","cell_type":"markdown","metadata":{"_cell_guid":"74750127-85a2-4bd5-99fe-e5d5f92421a0","_uuid":"48c985e6ae08814ca5369e5644be6153e2cdefe1"}}]}