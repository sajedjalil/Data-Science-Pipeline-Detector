{"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python","version":"3.6.3"}},"nbformat":4,"cells":[{"source":"# Goals\n\nThe main aim of this notebook is understanding the toxic comments dataset and get the gist of what the dataset looks like. At the implementation level, the notebook does the following, \n\n1. [Training data distribution](#train_data_dist)\n  1. [Visualization by fingerprinting the whole training dataset](#fingerprint_dataset)\n  2. [Visualization of dataset distribution by individual comment types](#viz_dataset_by_individ_cmnts)\n  3. [Visualization of dataset distribution by multilabel comment types](#viz_dataset_by_multilable_cmnts)\n  4. [Visualization of dataset distribution by co-occurrence of comment types](#viz_dataset_by_cooccur)\n2. Looking at the data\n\n  1. [Top 30 words per comment type](#top30_words)\n\n\n<div class=\"alert alert-block alert-success\">\nAll the visualizations are implemented in <code>Plotly</code>. I am trying to master <code>Plotly</code>, if you have some nice tips on improving the plots, styling the plots, etc., please do not hesitate to drop me a comment. \n</div>","cell_type":"markdown","metadata":{"_cell_guid":"6a346dda-ce33-4136-b9ac-ea43951e055c","_uuid":"b3a5aca86386694e2c17d6cad89792b3cfac584a"}},{"source":"# Imports & initializations & helper functions","cell_type":"markdown","metadata":{"_cell_guid":"2971bdc7-bc43-42e6-ae0b-7086a55bb130","_uuid":"9ea532ec98b8a6eb423ba660a19c099c086d0bd9"}},{"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# ------------------------ Standard Kaggle statements END --------------------------------------\n\nimport spacy\nfrom spacy import __version__\n#print(\"Going to use Spacy version - \", __version__)\n\nfrom plotly import tools\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n#print(\"Going to use Plotly version - \", __version__)\ninit_notebook_mode(connected=True)\n\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom nltk.corpus import stopwords\n\n# Let's load Spacy\nnlp = spacy.load(\"en\")\n\n# Plotly definitions \n# ------------------\n\n# Plot background color\npaper_bgcolor = \"rgb(240, 240, 240)\"\nplot_bgcolor = \"rgb(240, 240, 240)\"\n\n# Red, blue, green (used by plotly by default)\nrgb_def = ['rgb(228,26,28)', 'rgb(77,175,74)', 'rgb(55,126,184)']\n\n# Contrasting 2 qualities, highlighting one\ncontra_2_cols = [\"rgb(150,150,150)\", \"rgb(55,126,184)\"]\n\n# Barchart axis templates\n# template 1\nbchart_xaxis_temp1 = dict(\n    zeroline=False,\n    showline=False, \n    showgrid=False, \n    showticklabels=False,    \n    tickfont=dict(\n        size=9,\n        color=\"grey\"\n    )      \n)\n\nbchart_yaxis_temp1=dict(\n    tickfont=dict(\n        size=9,\n        color=\"grey\"\n    )        \n)\n\n# template 2\nbchart_xaxis_temp2 = dict(\n    zeroline=False,\n    showline=False, \n    showgrid=False, \n    showticklabels=False,    \n    tickfont=dict(\n        size=10,\n        color=\"grey\"\n    )      \n)\n\nbchart_yaxis_temp2=dict(\n    tickfont=dict(\n        size=10,\n        color=\"grey\"\n    )        \n)\n\n# Heatmap templates\nheatmap_axis_temp1 = dict(\n    zeroline=False,\n    showline=False,\n    showgrid=False, \n    showticklabels=False,  \n    ticks=''                \n)   ","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"084366dc-9b94-494e-ad56-4b6aafa13c9a","_uuid":"543cd697cdb314425837f5912b122409f67830c0","_kg_hide-input":false}},{"execution_count":null,"source":"def concat_label_columns(row):\n    multiheads = []\n    for col in list(train_orig.columns)[2:]:\n        if row[col]:\n            multiheads.append(col) \n    if len(multiheads) == 0:\n        return \"non_toxic\"\n    else:\n        return \":\".join(multiheads)\n    \ndef get_reshaped_array(one_d_nparray, fc):\n    \"\"\"Given an 1d array of an arbitrary size, make the shape of the \n    1d array divisible by factor \"fc\" by appending np.nan values when it is not divisible evenly. \n    For ex: If the array \n    dimension is (114, ), and the factor \"fc\" is 100,  \n    then the np.nan 1d array of shape 86 will \n    be appended to the original 1d array to become array size 200. \n    Array shape 200 evenly divides by 100.\n    \"\"\"    \n    remainder = one_d_nparray.shape[0] % fc\n    if remainder > 0:\n        cells_to_fill = fc - remainder\n        nan_array = np.full(cells_to_fill, np.nan)\n        one_d_nparray = np.append(one_d_nparray, nan_array)\n    num_cols_heatmap = int(one_d_nparray.shape[0] / fc)\n    num_rows_heatmap = fc\n    return one_d_nparray.reshape((num_rows_heatmap, num_cols_heatmap))","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6af2985b-3d04-4e7d-a5bd-f539b5551efb","collapsed":true,"_uuid":"ce6d56fb2dbb885978209b1b5e7d6016617455a4","_kg_hide-input":false}},{"source":"# Load datasets","cell_type":"markdown","metadata":{"_cell_guid":"d28b112a-397a-4fbb-a9fd-2d03555c62fa","_uuid":"8c68ff0b80bf85e19a1f355b9d941e23d747ba0e"}},{"execution_count":null,"source":"train_orig = pd.read_csv(\"../input/train.csv\")\ntest_orig = pd.read_csv(\"../input/test.csv\")\n\n# copy of the datasets \ntrain = train_orig.copy()\ntest = test_orig.copy()\n\n# concatenate the labels into comma separated one label and save it in a new column \ntrain[\"concatenated_label\"] = train.apply(concat_label_columns, axis=1)\n\ncomment_types = list(train_orig.columns)[2:]\ncomment_types_incl = [\"non_toxic\"] + comment_types\n\nmulti_comment_types = list(train[\"concatenated_label\"].unique())","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"166bbd33-4339-4c46-9d86-a8c38eb3f8e3","collapsed":true,"_uuid":"58389935554e9811a85cdd974f48c0a2f16ccd9f"}},{"source":"<a id='train_data_dist'></a>","cell_type":"markdown","metadata":{"_cell_guid":"bade80ad-8330-499e-9068-ba394daa929e","_uuid":"0cd1cd837c5ebc9ddeb86859d1ea60f8cdd260bf"}},{"source":"# Training data distribution","cell_type":"markdown","metadata":{"_cell_guid":"9a4955cd-27bf-4b37-8ac3-f1ac7bc0a0be","_uuid":"6631e470a919ec59ffb4d3bffe8792cf092424ab"}},{"execution_count":null,"source":"print(train_orig.shape)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"b3352f22-ed3a-4246-a0af-cbb94a32c48d","_uuid":"7a5b9a4b438d7927c25acb64bbda17433fa4f9d5"}},{"execution_count":null,"source":"print(comment_types)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"cd84c8c7-e563-474d-97de-25089a775c86","_uuid":"5f67a91f5a6bc9a36c1a198444bba7d968707d22"}},{"execution_count":null,"source":"train_orig[0:5]","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6268403f-729b-4330-80ba-e42d69344bc7","_uuid":"5572e33072063ca41a4c9f77836e18a32f1e73ac"}},{"source":"<a id='fingerprint_dataset'></a>","cell_type":"markdown","metadata":{"_cell_guid":"339c491b-8079-45af-8d8f-40bd57e0b6e2","_uuid":"61a75f3f1a8f9d95359a1e3a3a951e483e87de3a"}},{"source":"## Visualization by fingerprinting the whole training dataset\n\nFingerprinting helps eyeballing the dataset without actually looking into the numbers. Just by looking at the plots, we can get some basic information about the dataset, such as which type of comment types are most dominant, least dominant, and fuzzy and so on. \n\nFingerprinting of the toxic comments dataset works this way: \n\n* Fingerprint of the dataset is plotted as a set of heatmaps, one per comment type.\n* Number of grid points in heatmap equals to number of training data points. \n* For each location in the grid, we fill the label information as <code>Z</code> of the heatmap. In our case, the label is between 0 and 1.","cell_type":"markdown","metadata":{"_cell_guid":"51209f5d-634f-4083-a451-df7dbc6780b7","_uuid":"945cc3ae6810154259967e791dbd0a4c1a901f92"}},{"execution_count":null,"source":"# Fingerprint of training data per comment type\nfig_coords = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3)]\naxes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\"), (\"x4\", \"y4\"), (\"x5\", \"y5\"), (\"x6\", \"y6\")]\naxes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\"), (\"xaxis4\", \"yaxis4\"), (\"xaxis5\", \"yaxis5\"), (\"xaxis6\", \"yaxis6\")]\nfig = tools.make_subplots(\n    rows=2, \n    cols=3, \n    horizontal_spacing=0.1,\n    vertical_spacing=0.1,\n    subplot_titles=(comment_types[0], comment_types[1], comment_types[2], comment_types[3], comment_types[4], comment_types[5])\n)\nfor c_type, fig_coord, ax in zip(comment_types, fig_coords, axes_names):\n    reshaped_labels = get_reshaped_array(train_orig[c_type].as_matrix(), 75)\n    trace = go.Heatmap(\n        z=reshaped_labels, \n        colorscale = 'YlGnBu', \n        zmin=0, \n        zmax=1, \n        xaxis=ax[0], \n        yaxis=ax[1],\n        name=c_type\n    )\n    fig.append_trace(trace, fig_coord[0], fig_coord[1])\n\nfig[\"layout\"].update(\n    title = \"<b>Fingerprint of training data by comment types</b>\",\n    xaxis1=heatmap_axis_temp1,\n    yaxis1=heatmap_axis_temp1,   \n    xaxis2=heatmap_axis_temp1,\n    yaxis2=heatmap_axis_temp1, \n    xaxis3=heatmap_axis_temp1,\n    yaxis3=heatmap_axis_temp1, \n    xaxis4=heatmap_axis_temp1,\n    yaxis4=heatmap_axis_temp1,  \n    xaxis5=heatmap_axis_temp1,\n    yaxis5=heatmap_axis_temp1, \n    xaxis6=heatmap_axis_temp1,\n    yaxis6=heatmap_axis_temp1,  \n    margin=go.Margin(\n        l=100,\n        r=150,\n        t=150,\n        b=25\n    ),\n    autosize=False,\n    width=900,\n    height=600,\n)\niplot(fig)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"15817ac0-b8a8-4638-9536-8e57087deed4","_uuid":"b14d0b289d961463e00c9692e26ebbf6242a6628","_kg_hide-input":true}},{"source":"From the above fingerprint plot, we can see that, \n\n* <code>\"toxic\"</code> comment type is most common in the training data (if we exclude normal comments from the dataset)\n* <code>\"threat\"</code> is among the least common type of comments in the training dataset.","cell_type":"markdown","metadata":{}},{"source":"<a id=\"viz_dataset_by_individ_cmnts\"></a>","cell_type":"markdown","metadata":{"_cell_guid":"68f1b9d3-bf70-458a-8191-4e5a93f32129","_uuid":"32ce3b62c549a9e04d60879a9480cbe9a2e6041b"}},{"source":"## Visualization of dataset distribution by individual comment types\n\nEach data point is labeled for multiple comment categories. There are six types of labels/comments: \n\n1. <code>\"toxic\"</code>\n2. <code>\"sever_toxic\"</code>\n3. <code>\"obscene\"</code>\n4. <code>\"threat\"</code>\n5. <code>\"insult\"</code>\n6. <code>\"identity_hate\"</code>\n\nFor the sake of looking into the proportion of data that belong to none of the above cateories, i.e., the data that has labels <code>[0, 0, 0, 0, 0, 0]</code> for the 6 categories, we can assign them to <code>\"non_toxic\"</code>, so that the entire dataset has some sort of labels.","cell_type":"markdown","metadata":{"_cell_guid":"6c5a763f-8e70-4da2-bd44-dd66fcea0f04","_uuid":"68b90b1a71fed3e9a9b3513b992dfa2ddd8361b7"}},{"execution_count":null,"source":"individual_cmnt_type_counts = []\nfor cmnt_type in comment_types_incl:\n    if cmnt_type == \"non_toxic\":\n        individual_cmnt_type_counts.append(train[train[\"concatenated_label\"] == cmnt_type].shape[0])\n    else:\n        individual_cmnt_type_counts.append(train[cmnt_type].sum())\nindividual_cmnt_type_counts = pd.Series(individual_cmnt_type_counts, index=comment_types_incl)\nindividual_cmnt_type_counts = individual_cmnt_type_counts.sort_values()\nbar_colors = [contra_2_cols[1]] * len(comment_types_incl)\n# set color for \"non_toxic\" category\nbar_colors[list(individual_cmnt_type_counts.index).index(\"non_toxic\")] = contra_2_cols[0]\n\ndata = []\ntrace1 = go.Bar(\n    x=individual_cmnt_type_counts, \n    y=list(individual_cmnt_type_counts.index), \n    orientation=\"h\",\n    marker=dict(\n        color=bar_colors\n    ),\n    name=\"Comment type\"\n)\ntrace2 = go.Scatter(\n    x=np.full(len(individual_cmnt_type_counts), train.shape[0]), \n    y=list(individual_cmnt_type_counts.index), \n    mode=\"lines\",\n    line = dict(\n        color=(rgb_def[0]),\n        width = 7,\n        dash = 'dashdot',\n    ),\n    name=\"training data size\"\n)\ndata.append(trace1)\ndata.append(trace2)\nlayout=go.Layout(\n    title=\"<b>Distribution of individual comment types in training data</b>\",\n    xaxis=dict(\n        title=\"Count of comment types in training data\",\n        type='log',\n        autorange=True,\n        tickfont=dict(\n            color=\"grey\"\n        )        \n    ),\n    yaxis=dict(\n        title=\"Comment type\",\n        tickfont=dict(\n            color=\"grey\"\n        )\n    ),\n    showlegend=False,\n    annotations=[\n        dict(\n            x=5.1,\n            y=5.75,\n            xref='x',\n            yref='y',\n            text='Training data size',\n            showarrow=True,\n            arrowhead=4,\n            ax=20,\n            ay=-50            \n        )\n    ],   \n    autosize=False,    \n    width=900,\n    height=600,    \n    margin=go.Margin(\n        l=150,\n        r=150,\n        b=25,\n        t=100,\n    ),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"83d04616-164a-4815-83b6-508ce10d0da1","scrolled":false,"_uuid":"b185fc4e19e89fbd90061c7c87f15578bea2bb5a","_kg_hide-input":true}},{"source":"We can observe that the majority of the dataset contains normal comments, and the rest of the dataset is labeled with one or more of the six categories. The above plot assumes that the labels are independent, i.e., the overlap is counted as two separate data point with two labels. To get more insight into the distribtion of the dataset we should also look at the multilabel data as single data point with single label. The following plot does exactly that. ","cell_type":"markdown","metadata":{}},{"source":"<a id='viz_dataset_by_multilable_cmnts'></a>","cell_type":"markdown","metadata":{"_cell_guid":"bd709292-f043-471a-90cd-21edddf07e7e","_uuid":"f88e8ce20ea64b34b86337e406d9f8dc2e0429f6"}},{"source":"## Visualization of dataset distribution by multilabel comment types\n\nLet's now look into multilabel issue. \n\n1. How many times the data point tagged with <code>\"toxic\"</code> also tagged with <code>\"sever_toxic\"</code>?\n2. How many times the comments are tagged with only <code>\"threat\"</code> comment types?\n\nTo answer the above questions, we should consider multi-label assignments as belonging to a single category. We cannot assign the comment to either this or that caegory, we will lose the information. The solution is making new labels out of the multi-label comments. For example, if a comment is tagged with <code>\"toxic\"</code> and <code>\"obscene\"</code>, the new label for that comment would be <code>\"toxic_obscene\"</code>. The following code makes such modifications by adding another column in the training data - <code>\"concatenated_label\"</code>. Now the frequency distribution can show the training data distribution for the <code>fine-grained</code> labels. ","cell_type":"markdown","metadata":{"_cell_guid":"0e503f40-933a-420c-8505-24f9e891e967","_uuid":"d2a2d0b7588ba52a5d77f68041e2979d209b1a58"}},{"execution_count":null,"source":"multi_cmnt_type_counts = []\nfor cmnt_type in multi_comment_types:\n    multi_cmnt_type_counts.append(train[train[\"concatenated_label\"] == cmnt_type].shape[0])    \nmulti_cmnt_type_counts = pd.Series(multi_cmnt_type_counts, index=multi_comment_types)\nmulti_cmnt_type_counts = multi_cmnt_type_counts.sort_values()\ndata = []\nbar_colors = [contra_2_cols[1]] * len(multi_comment_types)\n# set color for \"non_toxic\" category\nbar_colors[list(multi_cmnt_type_counts.index).index(\"non_toxic\")] = contra_2_cols[0]\n\ntrace1 = go.Bar(\n    x=multi_cmnt_type_counts, \n    y=list(multi_cmnt_type_counts.index), \n    orientation=\"h\",\n    marker=dict(\n        color=bar_colors\n    ),\n    name=\"Comment type\"\n)\ntrace2 = go.Scatter(\n    x=np.full(len(multi_cmnt_type_counts), train.shape[0]), \n    y=list(multi_cmnt_type_counts.index), \n    mode=\"lines\",\n    line = dict(\n        color=(rgb_def[0]),\n        width = 7,\n        dash = 'dashdot',\n    ),\n    name=\"training data size\"\n)\ndata.append(trace1)\ndata.append(trace2)\nlayout=go.Layout(\n    title=\"<b>Distribution of multilabel comment types in the training data</b>\",\n    xaxis=dict(\n        title=\"Count of comment types in training data\",\n        type='log',\n        autorange=True,\n        tickfont=dict(\n            color=\"grey\",\n        )        \n    ),\n    yaxis=dict(\n        title=\"Comment type\",\n        tickfont=dict(\n            color=\"grey\",\n            size=8            \n        )\n    ),\n    showlegend=False,\n    annotations=[\n        dict(\n            x=4.9,\n            y=5,\n            xref='x',\n            yref='y',\n            text='Training data size',\n            showarrow=True,\n            arrowhead=4,\n            ax=-70,\n            ay=-40\n        )\n    ],   \n    autosize=False,    \n    width=900,\n    height=900,    \n    margin=go.Margin(\n        l=200,\n        r=100,\n        b=25,\n        t=100,\n    ),\n)\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"24cf4de9-968a-4c50-8bc0-9c71d9331ec7","_uuid":"0e4e2c1fa9468e5044187d31ca82bad33e4b2b0f","_kg_hide-input":true}},{"source":"<a id='viz_dataset_by_cooccur'></a>","cell_type":"markdown","metadata":{"_cell_guid":"631c7fa8-ae85-4f3f-86e3-e7b9b5404e44","_uuid":"ee305cfd2a59bad3bd72516c7960a78fcff99fe5"}},{"source":"## Visualization of dataset distribution by co-occurrence of comment types\n\nEach training data has multi-label assignment, meaning, a single training data can be classified into multiple comment types. Let's look at how each comment type is cooccurring in the training dataset. The following <code>Plotly</code> visualizations show the same information in,\n\n* Barcharts \n* A heatmap\n\nThe plots here show co-occurrence of comment types per major comment category. So, it makes it easier to look at the co-occurrence information.","cell_type":"markdown","metadata":{"_cell_guid":"ddec62eb-ef6b-49b4-9d2e-1662fc69bd45","_uuid":"4cd3bdef3e92cd8db65e7121e1dff8e1b709c601"}},{"execution_count":null,"source":"cmnt_count_matrix = []\nfor cmnt_type1 in comment_types:\n    cmnt_type_frame = train[train[cmnt_type1] == 1]\n    cmnt_type2_count = []\n    for cmnt_type2 in comment_types:\n        cmnt_type2_count.append(cmnt_type_frame[cmnt_type2].sum())\n    cmnt_count_matrix.append(cmnt_type2_count)\ncmnt_count_matrix = np.array(cmnt_count_matrix)\n\nfig_coords = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3)]\naxes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\"), (\"x4\", \"y4\"), (\"x5\", \"y5\"), (\"x6\", \"y6\")]\naxes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\"), (\"xaxis4\", \"yaxis4\"), (\"xaxis5\", \"yaxis5\"), (\"xaxis6\", \"yaxis6\")]\nfig = tools.make_subplots(\n    rows=2, \n    cols=3, \n    horizontal_spacing=0.15, \n    vertical_spacing=0.25,\n    subplot_titles=(comment_types[0], comment_types[1], comment_types[2], comment_types[3], comment_types[4], comment_types[5])\n)\nfor i, c_type, fig_coord, ax in zip(range(len(comment_types)),comment_types, fig_coords, axes_names):\n    inner_count = pd.Series(cmnt_count_matrix[i, :], index=comment_types)\n    inner_count = inner_count.sort_values()\n    trace = go.Bar(x=inner_count, y=list(inner_count.index), orientation = 'h')\n    fig.append_trace(trace, fig_coord[0], fig_coord[1])\n\nfig[\"layout\"].update(\n    showlegend=False,\n    title=\"<b>Co-occurrence of comment types</b>\",\n    xaxis1=bchart_xaxis_temp2,\n    yaxis1=bchart_yaxis_temp2,\n    xaxis2=bchart_xaxis_temp2,\n    yaxis2=bchart_yaxis_temp2,\n    xaxis3=bchart_xaxis_temp2,\n    yaxis3=bchart_yaxis_temp2,    \n    xaxis4=bchart_xaxis_temp2,\n    yaxis4=bchart_yaxis_temp2,\n    xaxis5=bchart_xaxis_temp2,\n    yaxis5=bchart_yaxis_temp2,\n    xaxis6=bchart_xaxis_temp2,\n    yaxis6=bchart_yaxis_temp2,\n\n    margin=go.Margin(\n        l=100,\n        r=100,\n        t=100,\n        b=25,\n    ),\n    autosize=False,\n    width=900,\n    height=500,\n)\niplot(fig)\n\n# As a heatmap\nfig = ff.create_annotated_heatmap(\n    z=cmnt_count_matrix, \n    x=comment_types, \n    y=comment_types, \n    colorscale='YlGnBu', \n    zmin=1, \n    zmax=cmnt_count_matrix.max()\n)\nfig[\"layout\"][\"xaxis\"].update(side=\"bottom\")\nfig[\"layout\"].update(\n    title=\"<b>Co-occurrence of comment types</b>\",    \n    xaxis=dict(\n        title=\"Major comment category\",\n        tickfont=dict(\n            color=\"grey\"\n        )        \n    ),   \n    yaxis=dict(\n        title=\"Co-occurring comment category\",\n        tickfont=dict(\n            color=\"grey\"\n        )        \n    ),   \n    \n    margin=go.Margin(\n        l=150,\n        r=150,\n        t=150,\n        b=75\n    ),\n    autosize=False,\n    width=900,\n    height=450,\n)\niplot(fig)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"8c9446f5-0151-4d38-8b7f-318a5c44e81c","scrolled":false,"_uuid":"36d8519573be6f5892cc68b868594b6251f8ac41","_kg_hide-input":true}},{"source":"# Let's look at the data itself\n\nThe following subsections go deep into the training dataset by looking at the content of the toxic comments. ","cell_type":"markdown","metadata":{"_cell_guid":"cc1c83ac-fad0-4d5d-867d-7ab61e74936e","_uuid":"3b88ad957b22aaf3f4ea30ef0029b084299e26cd"}},{"source":"<a id='top30_words'></a>","cell_type":"markdown","metadata":{"_cell_guid":"abd488e4-d2ef-4a70-ac68-bf168ebc2f5f","_uuid":"f362bec907aafe270a737da5acdd06f8aa7d6f1e"}},{"source":"## Top 30 words per comment type\n Now comes the meaty part. What kind of vocabulary is used in different types of comments? We are especially interested in bad comments in general. Let's find top 30 words for each comment type from the training data. The way we are going to look at is by taking the TF-IDF of the training data set and find most important words for each comment category.","cell_type":"markdown","metadata":{"_cell_guid":"dcb6b506-dead-4e66-810c-39cd19792732","collapsed":true,"_uuid":"99adfd4e9c92fc3a3202929a9463efa80a049876"}},{"execution_count":null,"source":"stop_words_new = list(sklearn.feature_extraction.text.ENGLISH_STOP_WORDS.union(stopwords.words(\"english\")))\ncount_vect = CountVectorizer(min_df=2, stop_words=stop_words_new)\ntrain_counts = count_vect.fit_transform(train[\"comment_text\"])\ntfidf_transformer = TfidfTransformer()\ntrain_tfidf = tfidf_transformer.fit_transform(train_counts)\nfeatures_array = np.array(count_vect.get_feature_names())","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"6b462462-b909-486c-b852-027bbe4853d0","collapsed":true,"_uuid":"e5de40c73b01644537a45824db9ac2af53121a28","_kg_hide-input":true}},{"execution_count":null,"source":"fig_coords = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3)]\naxes_names = [(\"x1\", \"y1\"), (\"x2\", \"y2\"), (\"x3\", \"y3\"), (\"x4\", \"y4\"), (\"x5\", \"y5\"), (\"x6\", \"y6\")]\naxes_lo_names = [(\"xaxis1\", \"yaxis1\"), (\"xaxis2\", \"yaxis2\"), (\"xaxis3\", \"yaxis3\"), (\"xaxis4\", \"yaxis4\"), (\"xaxis5\", \"yaxis5\"), (\"xaxis6\", \"yaxis6\")]\nfig = tools.make_subplots(\n    rows=2, \n    cols=3, \n    horizontal_spacing=0.01, \n    vertical_spacing=0.05,\n    subplot_titles=(comment_types[0], comment_types[1], comment_types[2], comment_types[3], comment_types[4], comment_types[5])\n)\n\nnum_top_words = 30\n\nfor i, cmnt_type, fig_coord, ax in zip(range(len(comment_types)),comment_types, fig_coords, axes_names):\n    instances_of_cmnt_type_ind = list(train[train[cmnt_type] == 1].index)\n    tfidf_cmnt_type = train_tfidf[instances_of_cmnt_type_ind].toarray()\n    mean_tfidf_cmnt_type = tfidf_cmnt_type.mean(axis=0)\n    top_words_vals = np.sort(mean_tfidf_cmnt_type)[::-1][0:num_top_words]\n    top_words_ind = mean_tfidf_cmnt_type.argsort()[::-1][0:num_top_words]\n    top_words = features_array[top_words_ind]\n    trace = go.Bar(\n        x=top_words_vals[::-1], \n        y=top_words[::-1], \n        orientation = 'h',\n        name=cmnt_type\n    )\n    fig.append_trace(trace, fig_coord[0], fig_coord[1])\n    \nfig[\"layout\"].update(\n    showlegend=False,\n    title=\"<b>Top 30 words for each comment type</b>\",\n    xaxis1=bchart_xaxis_temp1,\n    yaxis1=bchart_yaxis_temp1,\n    xaxis2=bchart_xaxis_temp1,\n    yaxis2=bchart_yaxis_temp1,\n    xaxis3=bchart_xaxis_temp1,\n    yaxis3=bchart_yaxis_temp1,    \n    xaxis4=bchart_xaxis_temp1,\n    yaxis4=bchart_yaxis_temp1,\n    xaxis5=bchart_xaxis_temp1,\n    yaxis5=bchart_yaxis_temp1,\n    xaxis6=bchart_xaxis_temp1,\n    yaxis6=bchart_yaxis_temp1,\n    margin=go.Margin(\n        l=75,\n        r=75,\n        t=100,\n        b=100,\n    ),\n    autosize=False,\n    width=900,\n    height=900,\n)\niplot(fig)","outputs":[],"cell_type":"code","metadata":{"_cell_guid":"f8c4bf8f-40f3-4502-b479-14d693b9b9e4","scrolled":false,"_uuid":"2b3d63791bf24c9ca3d3a4c52ad0880f5cafca11","_kg_hide-input":true}}]}