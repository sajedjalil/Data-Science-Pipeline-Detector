{"cells":[{"source":"#changes:except LR all other models taking so much time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","cell_type":"code","outputs":[],"metadata":{"_uuid":"326f783807d6854603906d3a8e31a4f73ad06eea","collapsed":true,"_cell_guid":"d6fdcd34-eab4-4be5-9f17-bea806317501"},"execution_count":null},{"source":"#import libraries\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')","cell_type":"code","outputs":[],"metadata":{"_uuid":"0797c43943489427b3c0aa0daafc67109aafd73b","collapsed":true,"_cell_guid":"180cf7af-7894-48d6-9fcd-baf60d49f37d"},"execution_count":null},{"source":"#shape of training dataset\ntrain.shape","cell_type":"code","outputs":[],"metadata":{"_uuid":"9362612452737e88397f100886b591349e7214f0","collapsed":true,"_cell_guid":"943e3812-1d8b-4305-99b3-570c3f96c689"},"execution_count":null},{"source":"#shape of testing dataset\ntest.shape","cell_type":"code","outputs":[],"metadata":{"_uuid":"a0bf1fd43f5f5f72a71f48841d0b9ef29f82b164","collapsed":true,"_cell_guid":"34f89493-9c82-4ff1-ba71-d20c0c9bb9de"},"execution_count":null},{"source":"#peek of the dataset\ntrain.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"123654486f4b95c83ea31906fb125352586e53f1","collapsed":true,"_cell_guid":"f5e17251-086a-4a79-b87c-a37202a0fa2e"},"execution_count":null},{"source":"#peek of the dataset\ntest.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"444b0ca2424ae665565437230b4ee67ed7475288","collapsed":true,"_cell_guid":"9540db2c-9195-44cb-b73f-1ff87befa802"},"execution_count":null},{"source":"#peek of the submission file\nsub.head()","cell_type":"code","outputs":[],"metadata":{"_uuid":"ad379894b58c4a839f71d7924b918386bdfc93ae","collapsed":true,"_cell_guid":"05894067-0601-4e65-b716-27e83c6528a1"},"execution_count":null},{"source":"#check datatypes\ntrain.dtypes","cell_type":"code","outputs":[],"metadata":{"_uuid":"3c3c72ad5ac98674eb56978df50baa14b6ea0363","collapsed":true,"_cell_guid":"f6071478-067b-48a1-acef-cfc97fe1c9d1"},"execution_count":null},{"source":"test.dtypes","cell_type":"code","outputs":[],"metadata":{"_uuid":"8a7e2fc8ee00e41625c41febad098935311b5597","collapsed":true,"_cell_guid":"71a70385-64fa-47a9-ac70-b7767e036a1e"},"execution_count":null},{"source":"import nltk.stem\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nenglish_stemmer = nltk.stem.SnowballStemmer('english')\nclass StemmedTfidfVectorizer(TfidfVectorizer):\n    def build_analyzer(self):\n        analyzer = super(TfidfVectorizer, self).build_analyzer()\n        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))","cell_type":"code","outputs":[],"metadata":{"_uuid":"6733e8e6d673329f5a1bcea1abb9cf5c43ecca70","collapsed":true,"_cell_guid":"fc2fefae-2a2c-4d1e-a78d-4c5ecc6d7263"},"execution_count":null},{"source":"import re\ndef clean_text( text ):\n    # Function to convert a document to a sequence of words\n    text = re.sub(\"[^A-za-z0-9^,?!.\\/'+-=]\",\" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" _exclamationmark_ \", text)\n    text = re.sub(r\"\\?\", \" _questionmark_ \", text)\n    return text","cell_type":"code","outputs":[],"metadata":{"_uuid":"b7f91f4889450f648c50e024830ee75646d94e76","collapsed":true,"_cell_guid":"022d1358-9f46-4493-b86c-6877d3116571"},"execution_count":null},{"source":"def build_data_set(ngram=3,stem=False,max_features=2000,min_df=2,remove_stopwords=True):\n    train = pd.read_csv(\"../input/train.csv\")\n    test = pd.read_csv(\"../input/test.csv\")\n    test.fillna('missing',inplace=True)\n    clean_train_comments = []\n    \n    for i in range(train.shape[0]):\n        clean_train_comments.append( clean_text(train[\"comment_text\"][i]) )\n\n    for i in range(test.shape[0]):\n        clean_train_comments.append( clean_text(test[\"comment_text\"][i]) )\n        \n    qs = pd.Series(clean_train_comments).astype(str)\n    \n    if not stem:\n        # 1-gram / no-stem\n        vect = TfidfVectorizer(analyzer=u'word',stop_words='english',\n                               min_df=min_df,ngram_range=(1, ngram),max_features=max_features)\n        ifidf_vect = vect.fit_transform(qs) \n        #print(\"ifidf_vect:\", ifidf_vect.shape)\n        X = ifidf_vect.toarray()\n        X_train = X[:train.shape[0]]\n        X_test = X[train.shape[0]:]\n    else:\n        vect_stem = StemmedTfidfVectorizer(analyzer=u'word',stop_words='english',\n                                           min_df=min_df,ngram_range=(1, ngram),max_features=max_features)\n        ifidf_vect_stem = vect_stem.fit_transform(qs)\n        #print(\"ifidf_vect_stem:\", ifidf_vect_stem.shape)\n        X = ifidf_vect_stem.toarray()\n        X_train = X[:train.shape[0]]\n        X_test = X[train.shape[0]:]\n    Y_train = train[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n    assert Y_train.shape[0] == X_train.shape[0]\n    del train, test\n    return X_train,X_test,Y_train","cell_type":"code","outputs":[],"metadata":{"_uuid":"90a55e44741d1570c362c85d148532db9a5b772d","collapsed":true,"_cell_guid":"5d6adeb7-7c4e-47c0-969c-f86c103cb63a"},"execution_count":null},{"source":"labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\nparams = {\n    'toxic': {'ngrams': 1, 'stem': True, 'max_features': 1000, 'C': 10 } , \n    'threat': {'ngrams': 1, 'stem': False, 'max_features': 1000, 'C': 10 } , \n    'severe_toxic': {'ngrams': 1, 'stem': True, 'max_features': 1000, 'C': 1.2 } , \n    'obscene': {'ngrams': 1, 'stem': True, 'max_features': 1000, 'C': 10 } , \n    'insult': {'ngrams': 1, 'stem': True, 'max_features': 1000, 'C': 1.2 } , \n    'identity_hate': {'ngrams': 1, 'stem': True, 'max_features': 1000, 'C': 10 } \n}","cell_type":"code","outputs":[],"metadata":{"_uuid":"1b9e0d8d820845a4cf509057af42e4cc8b632582","collapsed":true,"_cell_guid":"975d016f-5841-47f3-a2e2-fd89fc7d630a"},"execution_count":null},{"source":"import time\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","cell_type":"code","outputs":[],"metadata":{"_uuid":"2df8c6f059ecf4db0c3d5cdfeb3d24d228978e12","collapsed":true,"_cell_guid":"cb9b7de3-6dad-43ce-836d-9a6553b5a34a"},"execution_count":null},{"source":"start_time = time.time()\n\nfor label in labels:\n    print(\">>> processing \",label)\n    \n    X_train,X_test,Y_train = build_data_set(ngram=params[label]['ngrams'],\n                                            stem=params[label]['stem'],\n                                            max_features=params[label]['max_features'],\n                                            min_df=2,remove_stopwords=True)\n    Y_train_lab = Y_train[label]\n    seed = 7\n    scoring = 'accuracy'\n    # Spot Check Algorithms\n    models = []\n    models.append(('LR', LogisticRegression()))\n    #models.append(('LDA', LinearDiscriminantAnalysis()))\n    #models.append(('KNN', KNeighborsClassifier()))\n    #models.append(('CART', DecisionTreeClassifier()))\n    #models.append(('NB', GaussianNB()))\n    #models.append(('SVM', SVC()))\n    # evaluate each model in turn\n    results = []\n    names = []\n    for name, model in models:\n        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n        cv_results = model_selection.cross_val_score(model, X_train, Y_train_lab, cv=kfold, scoring=scoring)\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","cell_type":"code","outputs":[],"metadata":{"_uuid":"b54df39ef67d27b2c4c4b48e48a19a08bfc64fc9","collapsed":true,"_cell_guid":"19d9fdb6-eaf0-4e08-b06f-a6a0002db45b"},"execution_count":null},{"source":"#sub[label] = output\n#sub.to_csv(\"output_.csv\", index=False)","cell_type":"code","outputs":[],"metadata":{"_uuid":"a0bf40440b4212f50d09058f58aac23046a34fa5","collapsed":true,"_cell_guid":"282ae7ac-e1b2-4ce5-b9ec-c4072af9e886"},"execution_count":null}],"metadata":{"language_info":{"file_extension":".py","pygments_lexer":"ipython3","name":"python","nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"}},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"nbformat_minor":1}