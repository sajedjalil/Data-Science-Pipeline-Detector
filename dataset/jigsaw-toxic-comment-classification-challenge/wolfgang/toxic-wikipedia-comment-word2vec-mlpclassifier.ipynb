{"cells":[{"cell_type":"markdown","source":"# Toxic Wikipedia comment analysis\n\nSome first impressions about the given toxic comments data set that was taken from Wikipedia. \n\n### Contents: \n1. Import libraries\n2. Read the data\n3. Data analysis\n5. Fill missing data\n6. Split comments into words\n7. Train a word2vec model\n8. Choose a classifier\n9. Creating submission file\n\nFeedback and hints for improvement is greatly appreciated!\n","metadata":{"_cell_guid":"9a2b4532-f663-45f1-8662-49b12014e427","_uuid":"ee1597248380caa30f6a8b84ff3c598b681cb82f"}},{"cell_type":"markdown","source":"## 1. Import libraries\nWe will need numpy for linear algebra matrix handling and pandas for convenient data import and cleaning purposes. ","metadata":{"_cell_guid":"479a5530-a94e-4dfa-8535-ec3ff6472098","_uuid":"955051436aa21367b9119cb53cefdd6ec478d3e8"}},{"source":"import time\n\n# import necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#text libraries\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom gensim.models import word2vec\n\n# classifier imports\nfrom sklearn.neural_network import MLPClassifier","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"01725041-76de-43bc-9dee-19f834b9a13e","_uuid":"bb5babb160cba46f2cae7506601dc6767de6ba01","collapsed":true}},{"cell_type":"markdown","source":"## 2. Read the data\nUse pandas to read the complete training and test data set. ","metadata":{"_cell_guid":"43a086b8-fb13-4181-a551-1d6e2118b242","_uuid":"e8bea99ed48619c883e906469bfee5747bf3edc3"}},{"source":"# Read data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n# copy test id column for later submission\nresult = test[['id']].copy() \n# show first 3 rows of the training set to get a first impression about the data\nprint(train.head(3))","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"25e597c5-0332-40e6-9e10-e1778a29ace1","_uuid":"bff633fa4139b21c5e59d7369e3d39bed7842e01","collapsed":true}},{"cell_type":"markdown","source":"## 3. Data analysis\nTake a closer look at the given data. \n\nThere are no additional feature columns beside the **comment_text** column. The training data comes with six labels defining the incremental severity of toxity of each comment row. ","metadata":{"_cell_guid":"46b2aee3-72d8-4f5d-b5bb-4536a7c6ca53","_uuid":"a73dd93c3c8157f710eac25a935f40e65bd0558d"}},{"source":"# count each severity \nprint('toxic: %d' % train[train['toxic'] > 0]['toxic'].count())\nprint('severe_toxic: %d' % train[train['severe_toxic'] > 0]['severe_toxic'].count())\nprint('obscene: %d' % train[train['obscene'] > 0]['obscene'].count())\nprint('threat: %d' % train[train['threat'] > 0]['threat'].count())\nprint('insult: %d' % train[train['insult'] > 0]['insult'].count())\nprint('identity_hate: %d' % train[train['identity_hate'] > 0]['identity_hate'].count())","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"a35acf8f-3088-4211-99dc-2d5e0a791143","_uuid":"7045fa17d84bb66cfa0a10db14ae78bd05f354ca","collapsed":true}},{"cell_type":"markdown","source":"I am curious if the label 'toxic' is the precondition for the label 'severe_toxic'?","metadata":{"_cell_guid":"b564684a-a769-4b94-b542-1eeab6586296","_uuid":"86aafbf596d83b0ef3e60579f6d6ade702dcc531"}},{"source":"print('Severe toxic but NOT toxic?: %d' % train[(train['severe_toxic'] > 0) & (train['toxic'] == 0)]['id'].count())\nprint('Insult but NOT toxic?: %d' % train[(train['insult'] > 0) & (train['toxic'] == 0)]['id'].count())\nprint('Obscene but NOT toxic?: %d' % train[(train['obscene'] > 0) & (train['toxic'] == 0)]['id'].count())\nprint('Threat but NOT insult?: %d' % train[(train['threat'] > 0) & (train['insult'] == 0)]['id'].count())","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"f16e2bdf-8105-444a-b1dd-522fec6225be","_uuid":"f67d349e756ae7beba9d031d053a8086ac76311a","collapsed":true,"scrolled":true,"_kg_hide-input":true}},{"cell_type":"markdown","source":"Check the typical length of a comment.","metadata":{"_cell_guid":"a5546b36-dacc-425b-890c-008f52f44d16","_uuid":"09552bc7f82d55ede4e178d948f1c532d01a9095"}},{"source":"train['len'] = train['comment_text'].str.len()\nprint('Average comment length: %d' % train['len'].mean())\nprint('Median comment length: %d' % train['len'].quantile(.5))\nprint('90th percentile comment length: %d' % train['len'].quantile(.9))","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"9ae88b5d-80be-46f0-b7b4-f71da2958ac8","_uuid":"3cb52de743e1ef3a9addbec7c27ba75df2cb10fb","collapsed":true,"scrolled":true}},{"source":"print(train[train['comment_text'].isnull()])\nprint(test[test['comment_text'].isnull()])","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"2fcfce4c-3bd4-4887-a8c4-bd239452d3df","_uuid":"8e2974f014f982e4879ab88a14211452f779b9b2","collapsed":true}},{"cell_type":"markdown","source":"### Some observations\n\n1. Most frequent labels are toxic, obscene and insult. They seem to be the major categories.\n2. Toxic and severe_toxic label are related while all other labels seem to be independent ","metadata":{"_cell_guid":"9819d081-e8f7-4ea2-bb1b-230f9e7092c1","_uuid":"1ab6ea7c3f7e57514b0f24b25cac8c267d0c16ce"}},{"cell_type":"markdown","source":"## 4. Fill missing data\nWe have to make sure that there are no null values within the training and test data sets, otherwise our algorithms might fail.","metadata":{"_cell_guid":"45bfeb28-c122-4469-b6c9-a613f12e24cc","_uuid":"a0f5c483256815dae78f78a81f77061183974e37"}},{"source":"test['comment_text'].fillna(value='none', inplace=True) # there is one \ntrain['comment_text'].fillna(value='none', inplace=True) ","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b129bbfa-508a-45b0-8f99-453a6ddc71ca","_uuid":"00978273239c254c9bfe713f50c0b1022c783876","collapsed":true}},{"cell_type":"markdown","source":"## 5. Split comments into array of words\nWe will split the given comments into arrays of single words. We will also remove non letter/number characters.  ","metadata":{"_cell_guid":"5f54e6be-35ca-4f99-be74-79b70d41a910","_uuid":"28a68eba3749dadb64d55ae67f00bbc31dccb3c9"}},{"source":"def text_to_words(raw_text, remove_stopwords=False):\n    # 1. Remove non-letters, but including numbers\n    letters_only = re.sub(\"[^0-9a-zA-Z]\", \" \", raw_text)\n    # 2. Convert to lower case, split into individual words\n    words = letters_only.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\")) # In Python, searching a set is much faster than searching\n        meaningful_words = [w for w in words if not w in stops] # Remove stop words\n        words = meaningful_words\n    return words \n\nsentences_train = train['comment_text'].apply(text_to_words, remove_stopwords=False)\nsentences_test = test['comment_text'].apply(text_to_words, remove_stopwords=False)\n# show first three arrays as sample\nprint(sentences_train[:3])","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"7b05dc35-b0e4-489c-85c4-9a415c0a1826","_uuid":"a29ff3dc0fef06e988e6f54b86fdd3a7dadd1fab","collapsed":true}},{"cell_type":"markdown","source":"## 6. Train a word2vec model\nTrain a word2vec model with the given training sentences. ","metadata":{"_cell_guid":"37919469-2780-427b-b9e2-114fbc2de777","_uuid":"426af7816cd3b429677c4545e69a4145c01d1dcc"}},{"source":"# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n# Initialize and train the model (this will take some time)\nmodel = word2vec.Word2Vec(sentences_train, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\nmodel.init_sims(replace=True) # marks the end of training to speed up the use of the model","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"f459fa67-e33a-4c8e-a3c6-0a3248a660bc","_uuid":"6c503a77f930e3f35ebf8dbf8e448e792b419053","collapsed":true}},{"cell_type":"markdown","source":"## 7. Choose and train a classifier\nIn this example we will use our trained word2vec model to get avarage word vectors from each sentence and train a neural network.","metadata":{"_cell_guid":"e39bfd9e-c9a6-4b9e-b394-0172544134f4","_uuid":"6a868d434c2888650079ea659fe1e98e54b14b7f"}},{"source":"def makeFeatureVec(words, model, num_features):\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    #\n    nwords = 0\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1\n            featureVec = np.add(featureVec, model[word])\n    # Divide the result by the number of words to get the average\n    if nwords == 0:\n        nwords = 1\n    featureVec = np.divide(featureVec, nwords)\n    return featureVec\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n    counter = 0\n    # Loop through the reviews\n    for review in reviews:\n        # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n        counter = counter + 1\n    return reviewFeatureVecs\n\nf_matrix_train = getAvgFeatureVecs(sentences_train, model, num_features)\nf_matrix_test = getAvgFeatureVecs(sentences_test, model, num_features)\n# we have to train 6 different models with 6 different Y labels\ny = [train['toxic'], train['severe_toxic'], train['obscene'], train['threat'], train['insult'], train['identity_hate']]\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c28384f6-7536-4299-8627-bb48a1e2944c","_uuid":"5e7d762b95fc896e90b582010629e88285cea0ce","collapsed":true}},{"cell_type":"markdown","source":"We create 6 models, one for each toxic level each.","metadata":{"_cell_guid":"93309f79-5eaf-4413-84ff-de139f0152f2","_uuid":"19d631489bd05081e531bfb6b09e49f6fd831a0c"}},{"source":"# create 6 MLP models\nmodel = []\nfor i in range(0, 6):\n    m = MLPClassifier(solver='adam', hidden_layer_sizes=(30,30,30), random_state=1)\n    model.append(m)\nprint(model)\n","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"0ec61191-0271-4ef9-863f-db19477780a4","_uuid":"fbff5122e065a2f93838df68e8c4c8e9186e060b","collapsed":true}},{"cell_type":"markdown","source":"Now train the models with a partial fit approach","metadata":{"_cell_guid":"dcea7134-bb2b-4cc5-b513-0630e4a307e7","_uuid":"aa8970521313b043256e48d001178f71fff4f1f0"}},{"source":"batch_size = 10000\ntotal_rows = f_matrix_train.shape[0]\nduration = 0\nstart_train = time.time()\npos = 0\nclasses = [0,1]\n# we use a partial fit approach\nwhile duration < 2500 and pos < total_rows:\n    for i in range(0, 6):\n        if pos+batch_size > total_rows:\n            batch_size = total_rows-pos\n        X_p = f_matrix_train[pos:pos+batch_size]\n        y_p = y[i][pos:pos+batch_size]\n        model[i].partial_fit(X_p, y_p, classes)\n    pos = pos + batch_size\n    duration = time.time() - start_train # how long did we train so far?\n    print(\"Pos %d/%d duration %d\" % (pos, total_rows, duration))\n    # end test partial fit  ","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"334f80f9-e073-4cd5-acac-eec2a224ba23","_uuid":"3acf97a8e33feec66af9a70ca1446b933bbfc944","collapsed":true}},{"cell_type":"markdown","source":"Now predict the result for each toxic level","metadata":{"_cell_guid":"81f3cc04-2cd0-4704-bb8a-a9c97beb32cb","_uuid":"206326029ddae3637fd4265deec94a750c24edf0"}},{"source":"result['toxic'] = model[0].predict_proba(f_matrix_test)[:,1]\nresult['severe_toxic'] = model[1].predict_proba(f_matrix_test)[:,1]\nresult['obscene'] = model[2].predict_proba(f_matrix_test)[:,1]\nresult['threat'] = model[3].predict_proba(f_matrix_test)[:,1]\nresult['insult'] = model[4].predict_proba(f_matrix_test)[:,1]\nresult['identity_hate'] = model[5].predict_proba(f_matrix_test)[:,1]","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b986d3f0-bf81-4f58-810c-06fae9a532fa","_uuid":"54713cf6986129d050924e88f162a86772961888","collapsed":true}},{"cell_type":"markdown","source":"## 8. Write prediction into submission file\nSave the predicted values of all our 6 models into a submission csv file.","metadata":{"_cell_guid":"3683d90b-ebe8-4dcc-9f7f-6515a84ba396","_uuid":"e0472bd53dd5b0f8baadd668d58588f6d633a071"}},{"source":"result.to_csv('submission.csv', encoding='utf-8', index=False)","cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b7be5e1c-e7b3-4138-acde-cc42628adc0b","_uuid":"88b32565becb2c37660fdb5833631af2fcc78ec9","collapsed":true,"_kg_hide-input":false,"_kg_hide-output":false}},{"cell_type":"markdown","source":"Wow, you really read my notebook till the last line, congratulation :)\n\nMy prediction is by far not a top ranking one but if you use any part of this notebook in a published kernel, credit (you can simply link back here) would be greatly appreciated.\n\nSources:\n[Word2Vec introduction](https://www.kaggle.com/c/word2vec-nlp-tutorial), Great word2vec tutorial by [Angela Chapman](http://www.linkedin.com/pub/angela-chapman/5/330/b97)\n","metadata":{"_cell_guid":"a887bdfb-2412-44ac-b38d-db902667c590","_uuid":"767602cca818bfd6ea1fa25bea7e24a3d7faafb6"}}],"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"}},"nbformat":4}