{"cells":[{"metadata":{"_uuid":"37f13b65d17cd5afea513607f0181ddae2d280a6","_cell_guid":"83a4355c-1634-47f2-b05d-872900e20799","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport numpy as np\nimport re\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet as wn\nfrom keras.layers import Bidirectional\nimport codecs\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa319c3c6260a43541fb42b15cc7d98cd054e80f","_cell_guid":"a5e8f789-deed-4aa1-a8e6-8ae759beba9c"},"cell_type":"markdown","source":"TLDR: Inspired by the Jeremy's lstm, i tried using the convolutional layer on the top of that.\nCollectivley these type of codes are Long-term Recurrent Convolutional Networks or CNN-LSTM networks\nHere are few links explaining its effectiveness \nhttps://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n\nhttps://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n\nhttps://yerevann.github.io/2016/06/26/combining-cnn-and-rnn-for-spoken-language-identification/\n\nAdding the CNN on top of LSTM helps in the sense that CNN combined with pooling layers can bring out important temporal features devoid of any noise which the LSTM can use more effectively.\nIn the end, bidirectional LSTM will help in classifying the data.\n\nLet's go to the code directly.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"bd5d04f9d0b5033601ff5bc9ee8d880af98d606f","_cell_guid":"689d4aba-0ec7-4c86-9822-20e8a221de02","trusted":false},"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"cb418c4ba6f65b0b13d6ccbf847e8d6522caa83a","_cell_guid":"48400efc-a312-44ca-b3a7-76ca0b2deec4","trusted":false},"cell_type":"code","source":"#Some data cleaning\ntrain.comment_text = train.comment_text.apply(lambda x : re.sub(' u ', 'you', x))\ntrain.comment_text = train.comment_text.apply(lambda x : re.sub('\\nu ', 'you', x))\ntrain.comment_text = train.comment_text.apply(lambda x : re.sub(' u\\n', 'you', x))\ntrain.comment_text = train.comment_text.apply(lambda x : re.sub(\"i'm\", 'i am', x))\ntrain.comment_text = train.comment_text.apply(lambda x : re.sub(\"fucksex\", 'fuck sex', x))\n\ntest.comment_text = test.comment_text.apply(lambda x : re.sub(' u ', 'you', x))\ntest.comment_text = test.comment_text.apply(lambda x : re.sub('\\nu ', 'you', x))\ntest.comment_text = test.comment_text.apply(lambda x : re.sub(' u\\n', 'you', x))\ntest.comment_text = test.comment_text.apply(lambda x : re.sub(\"i'm\", 'i am', x))\ntest.comment_text = test.comment_text.apply(lambda x : re.sub(\"fucksex\", 'fuck sex', x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"242f22632cfbb6f94c8d5da396e36915da9c8081","_cell_guid":"36daad23-6f78-47ff-8bcd-276dd76fe061","trusted":false,"collapsed":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3f8cb02e393df02b0930a78e247cc6e6cf524d86","_cell_guid":"e1fb7b85-10d8-43d0-86a8-8142e245c3a9","trusted":false},"cell_type":"code","source":"#Let's see the words which constitutes the toxic comments\ndef getwordcountdf(data, key):\n    filtered=data[data[key]==1]\n    sequence=[]\n    tr_words=set(stopwords.words('english'))\n    for x in filtered.comment_text:\n        sequence+=text_to_word_sequence(x, \n                                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                        lower=True,\n                                        split=\" \")\n    filtered_words = [word for word in sequence if word not in tr_words]\n    df= pd.DataFrame({'words': filtered_words})\n    z= df.groupby('words').size().reset_index(name='counts')\n    return z.sort_values('counts',ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1219f116407e7063b0218dadb85c3ce9433a8cc9","_cell_guid":"fa90e952-9ecc-4341-bb6c-08c08119a7e3","trusted":false},"cell_type":"code","source":"obscene=getwordcountdf(train, 'obscene')\ntoxic=getwordcountdf(train, 'toxic')\nsevere_toxic=getwordcountdf(train, 'severe_toxic')\nthreat=getwordcountdf(train, 'threat')\ninsult=getwordcountdf(train, 'insult')\nidentity_hate=getwordcountdf(train, 'identity_hate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4d02b8b7e4c757f17bb37b7c746d684b88465e9","_cell_guid":"e75f1a26-c53b-497c-a093-822087db19cc"},"cell_type":"markdown","source":"#### Lets look at the various keywords which are most frequent in each category of toxic comments","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a836cf9f683b5b6c65affbaa26aa2b99cc4d63d6","_cell_guid":"693834d5-c258-4a45-9338-6dba8ee930cb","trusted":false,"collapsed":true},"cell_type":"code","source":"obscene.head(50).plot.bar(x='words', y='counts', figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93333c6ac0db662806933c8ce61a452af872c818","_cell_guid":"f9d08cbd-16f1-4a0a-9ec7-ee583ee97e00","trusted":false,"collapsed":true},"cell_type":"code","source":"severe_toxic.head(50).plot.bar(x='words', y='counts', figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32737fa7ed4210b8305b4ebf029d0b70de499493","_cell_guid":"1556b6d1-0a02-4e8e-bab0-04f6e8ae1b81","trusted":false,"collapsed":true},"cell_type":"code","source":"threat.head(50).plot.bar(x='words', y='counts', figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5500b11a41403f15ee70b5393d0fe877ef6a62af","_cell_guid":"155e2b55-5b89-489b-82a2-ca47d6406f9b","trusted":false,"collapsed":true},"cell_type":"code","source":"insult.head(50).plot.bar(x='words', y='counts', figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f39ed2b58c4dfcab80df6325e2390e1f65a16e5a","_cell_guid":"9df808e1-ee23-4fdf-83e9-457823b40c23","trusted":false,"collapsed":true},"cell_type":"code","source":"identity_hate.head(50).plot.bar(x='words', y='counts', figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c2a806613405f0d344e563abaaab9af2790dd30","_cell_guid":"262faf05-611e-4065-bfdd-a8536bf1998b"},"cell_type":"markdown","source":"We can see that most frequent words do corresponds to the class they have been assigned to. \nThese are really toxic words and we should get rid of these comments anyhow.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"56b03814aca1f15b28d54a65f3a834c945724f5f","_cell_guid":"1d3e0b3d-b100-4b88-bbb7-3827f62ae982"},"cell_type":"markdown","source":"### Let's now remove the stopwords from the data.\n#### I know its not good idea to remove the stopwords from a LSTM but it seems to be working better this way.\n","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"651857b7c71909873a347574d6ec5f03d1de5609","_cell_guid":"12215cc0-3426-4408-b1a2-fd0aa280eb4b","trusted":false},"cell_type":"code","source":"def cleanupDoc(s):\n    stopset = set(stopwords.words('english'))\n    stopset.add('wikipedia')\n    tokens =sequence=text_to_word_sequence(s, \n                                        filters=\"\\\"!'#$%&()*+,-˚˙./:;‘“<=·>?@[]^_`{|}~\\t\\n\",\n                                        lower=True,\n                                        split=\" \")\n    cleanup = \" \".join(filter(lambda word: word not in stopset, tokens))\n    return cleanup\n\ntest.comment_text=test.comment_text.apply(cleanupDoc)\ntrain.comment_text=train.comment_text.apply(cleanupDoc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9199a0751028fc991b63e2e02be7daef78507ed","_cell_guid":"f73a8140-41e7-4a20-b78a-120b17980862"},"cell_type":"markdown","source":"##### Standard tokenization of data to get the indexes of embedding matrix","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"6b177e756899943e5ec05b08995e7aeaf595576a","_cell_guid":"bede56fa-7331-4d17-9e83-711188356426","trusted":false},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.comment_text)\nsequences = tokenizer.texts_to_sequences(train.comment_text)\ntest_sequence=tokenizer.texts_to_sequences(test.comment_text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a22736fe82db04beb33cb65e48d8f384ecaac6dc","_cell_guid":"9a25a673-6e0f-4a5c-891b-f2d1fb8f3d3d"},"cell_type":"markdown","source":"###### Pad the sequence so that we have the same length input, since keras supports same length input only","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"e103e368c0082948ad1a4e02fce6237a74553763","_cell_guid":"b2d409c7-0f1c-4dab-8a7e-a55a9b4d2c74","trusted":false},"cell_type":"code","source":"data = pad_sequences(sequences, maxlen=150)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3f636568baa8d661d457814b9e93db8c767601ab","_cell_guid":"1a7bf316-97ed-45fd-9057-caedc060fc8e","trusted":false},"cell_type":"code","source":"t_data = pad_sequences(test_sequence, maxlen=150)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14b6925ebcc5a1648c80b4e20182cc9dad27481e","_cell_guid":"1dc4998a-aac5-4a8a-a7fc-e59a27359f58","trusted":false,"collapsed":true},"cell_type":"code","source":"### Load the pretrained glove vectors\nprint('Indexing word vectors.')\nembeddings_index = {}\nf = codecs.open('../input/glove6b300dtxt/glove.6B.300d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2eeb292be304613c4939e8887aac0efae94084c7","_cell_guid":"b780254c-531a-4472-b7c2-15b296869315","trusted":false},"cell_type":"code","source":"vector_length=300\nlength=150\nnum_classes=6","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1775cb72ebf04cfb24fc4208ee2a1abd57232dbe","_cell_guid":"7092e7cb-3707-40c5-9734-2e22c0e0c162","trusted":false,"collapsed":true},"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Preparing embedding matrix.')\n# prepare embedding matrix\nnb_words = min(200000, len(word_index))\nnotfound=[]\nembedding_matrix = np.zeros((nb_words, vector_length))\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        notfound.append(word)\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b8e69e74850018efdad39e31893aabf8cffe679","_cell_guid":"05452c4a-67b4-4aac-a029-c3ab2d9b70f4","trusted":false,"collapsed":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35a59ea7289d1f6c80cc81f19b70daeec99c5bb2","_cell_guid":"28992d9e-7eb9-447c-b7a5-19d369b18469"},"cell_type":"markdown","source":"### Here is the keras model with CNN + LSTM","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"329848b9f4c53091d3eaa2074c434ac0453ea76e","_cell_guid":"1956a0d3-e710-4419-8dc3-b438acd67592","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7dcc704d2c76e2b4585e84d39414150f5236b760","_cell_guid":"2bae0bae-f0aa-44a8-b1e4-b3b89a87f113","trusted":false},"cell_type":"code","source":"#keras stuff now\nimport os\nimport csv\nimport codecs\nfrom keras.layers import LSTM, Convolution1D,Convolution2D, Flatten, Dropout, Dense, Input, Conv1D, GRU, GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Sequential\nfrom keras.layers import Merge, merge, concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend as K\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\n#Using Pretrained Embedding Matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18e529792b3cd33b60269ff6f27d14254e16c3b1","_cell_guid":"b090e775-d600-4fb6-80f3-47b4c60fef8a"},"cell_type":"markdown","source":"#### Since it is a multi label classification, we need to have 6 sigmoid units in the last layer rather than having a softmax layer.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"363fd9a10c70ba9df90ee35d5697c38abed38bec","_cell_guid":"feb67a35-d8d1-4a94-ab0b-b732182ccbfb","trusted":false},"cell_type":"code","source":"def getjmodel():\n    inp = Input(shape=(length,))\n    x = Embedding(nb_words, vector_length, weights=[embedding_matrix])(inp)\n    x = Conv1D(256, 3, activation='relu')(x)\n    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n    x = GlobalMaxPooling1D()(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"cacd9365851fe8a6d699ed8e24da29c49811108a","_cell_guid":"799015d2-71e9-47cd-b1f8-62d9fd4a7892","trusted":false},"cell_type":"code","source":"labels=train[['toxic','severe_toxic', 'obscene','threat','insult', 'identity_hate' ]]\ndata_train, data_test, y_train, y_test, comm_train, comm_trst =train_test_split(data, np.array(labels),train.comment_text, test_size=0.20, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbbffb628bf2ce4e123ec04f97f22809ead08ec8","_cell_guid":"c38dcc1f-5d2c-4206-bd82-fe2430cfa1bb","trusted":false,"collapsed":true},"cell_type":"code","source":"galaxyModel=getjmodel()\ngalaxyModel.fit(data_train, y_train, 1024, epochs=3, validation_data=(data_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad89006a766276055a9572cf7b819a4b2b473119","_cell_guid":"bc9566dc-fc5f-4592-ac48-2c64c0faf093"},"cell_type":"markdown","source":"### submission script","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"a492497720350c36511e1e4aa7ce9856a3633b19","_cell_guid":"1e3cc6bd-f1ed-4dbb-ad85-369dfccd8240","trusted":false},"cell_type":"code","source":"preds=galaxyModel.predict(t_data)\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\nsample_submission = pd.read_csv('sample_submission.csv')\nsample_submission[list_classes] = preds\nsample_submission.to_csv('mysubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fef63f0a9c1ed9098ae2825c0325fbb7704ce0b","_cell_guid":"de43d097-7882-4b44-bfcc-d29771341c87"},"cell_type":"markdown","source":"# Future tasks:\n1. Add data augmentation\n2. Improve model hyperparameters\n3. Correct misspelled words. \n\nThanks for looking at the kernel, hit the like button if you found it useful.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"094e323e1ffbdb797ac34c5ebcca2d903fda45b3","_cell_guid":"d3a8d693-45ce-4dab-9d47-06887a3d3748","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","file_extension":".py","version":"3.6.4","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","nbconvert_exporter":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1}