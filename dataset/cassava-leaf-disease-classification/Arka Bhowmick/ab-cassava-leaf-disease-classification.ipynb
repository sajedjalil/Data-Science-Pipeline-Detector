{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm import tqdm\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nimport tensorflow as tf\nimport random\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tensorflow.keras.experimental import CosineDecay\nimport glob\n\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visaulizing some images from dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def visualizing_train_set(num_of_images, train_csv):\n    rows = 4\n    columns = num_of_images // rows\n    #fig, axs = plt.subplots(rows, columns)\n    images = []\n    images_name = []\n    count = 0\n    fig = plt.figure(figsize=(20,20))\n    for image in tqdm(glob.glob(\"../input/cassava-leaf-disease-classification/train_images/*.jpg\")):\n        if count == num_of_images:\n            break\n        else:\n            img = cv2.imread(image)\n            images.append(img)\n            images_name.append(image.split('/')[-1])\n            count = count + 1\n    \"\"\"\n    count_1 = 0\n    for row in range(rows):\n        for col in range(columns):\n            axs[row, col].imshow(images[count_1])\n            title_image = find_the_label(images_name[count_1], train_csv)\n            axs[row, col].set_title(title_image)\n            count_1 = count_1 + 1\n    \"\"\"        \n    axs = []\n    for i in range(columns*rows):\n        axs.append(fig.add_subplot(rows, columns, i+1) )\n        subplot_title = (find_the_label(images_name[i], train_csv))\n        axs[-1].set_title(subplot_title)  \n        plt.imshow(images[i])\n    fig.tight_layout()\n    plt.show()\n    \ndef find_the_label(image_name, train_csv):\n    i = train_csv.index[train_csv['image_id'] == image_name]\n    return train_csv['class_name'].values[i]\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the data by Plotting Graphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def exploring_the_data(train_csv, _return):\n    # gettin the number of positive samples\n    cbb = 0\n    cbsd = 0\n    cgm = 0\n    cmd = 0\n    healthy = 0\n    \n    cbb = train_csv.query('label == 0').label.count()\n    cbsd = train_csv.query('label == 1').label.count()\n    cgm = train_csv.query('label == 2').label.count()\n    cmd = train_csv.query('label == 3').label.count()\n    healthy = train_csv.query('label == 4').label.count()\n    \n    # creating a plot with all the data\n    label = ['CBB [0]', 'CBSD [1]', 'CGM [2]', 'CMD [3]', 'Healthy [4]']\n    fig = plt.figure(figsize = (25,10))\n    plt.bar(label, [cbb, cbsd, cgm, cmd, healthy])\n    plt.xticks(label, rotation = 45)\n    plt.show()\n    \n    if _return == 1:\n        return cbb, cbsd, cgm, cmd, healthy\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performing data oversampling and UnderSampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function is performing oversampling of the classes whose significantly less than the dominant class and it \n# also performs undersampling of the dominant class\n# oversampling the dataset\ndef oversampling_the_data(cbb, cbsd, cgm, cmd, healthy, train_csv, percent):\n    print(\"[*] Old length = {}\".format(len(train_csv)))\n    train_csv_new = train_csv\n    label_dict = {'cbb' : 0, 'cbsd' : 1, 'cgm' : 2, 'cmd' : 3, 'healthy' : 4}\n    # finding the percentage of each category and finding which label has a lower percenatage\n    cbb_percent = cbb/ (cbb + cbsd + cgm + cmd + healthy)\n    cbsd_percent = cbsd/ (cbb + cbsd + cgm + cmd + healthy)\n    cgm_percent = cgm/ (cbb + cbsd + cgm + cmd + healthy)\n    cmd_percent = cmd/ (cbb + cbsd + cgm + cmd + healthy)\n    healthy_percent = healthy/ (cbb + cbsd + cgm + cmd + healthy)\n    \n    # create a dictionary with the percentage\n    percent_dict = {'cbb' : cbb_percent, 'cbsd' : cbsd_percent, 'cgm' : cgm_percent, 'cmd' : cmd_percent, 'healthy' : healthy_percent}\n    \n    # identifying the highest percent of label\n    max_class_value = max(percent_dict, key = percent_dict.get)\n    print(\"[*] Category of class present most is {} with label {}\".format(max_class_value, label_dict[max_class_value]))\n    \n    # increasing the dataset\n    for key, value in percent_dict.items():\n        print(\"[*] Dictionary key {}\".format(key))\n        if key != max_class_value:\n            # identifying the label\n            label = np.int64(label_dict[key])\n            \n            for i in range (len(train_csv)):\n                if train_csv.iloc[i,1] == label:\n                    # using probability to decide whether to add that sample at the end of the dataframe\n                    prob = random.uniform(0, 1)\n                    if prob >= percent:\n                        df = {\"image_id\" :train_csv.iloc[i,0], \"label\" : train_csv.iloc[i,1]}\n                        train_csv_new = train_csv_new.append(df, ignore_index = True)\n    row_index_list = []\n    print(\"Train CSV NEW : \\n\")\n    print(train_csv_new.iloc[5,1])\n    for i in range (len(train_csv_new)):\n        if train_csv_new.iloc[i,1] == label_dict[max_class_value]:\n            prob = random.uniform(0, 1)\n            if prob >= percent:\n                row_index_list.append(i)\n    print(\"[*]Length of Row Index List is {}\".format(len(row_index_list)))\n    train_csv_new = train_csv_new.drop(row_index_list)\n            \n            \n        \n    print(\"[*] New length after oversampling = {}\".format(len(train_csv_new)))       \n    return train_csv_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Stratified K-Fold method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# implementing stratified K-fold\nskf = StratifiedKFold(n_splits = 3, random_state = 7, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Data generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating data generators \ndef creating_data_generators(train_dataframe, train_directory_path, validation_dataframe, y_col_list, Image_Size, Batch_Size):\n    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale = 1./255,\n            shear_range=25,\n            zoom_range = 0.3,\n            horizontal_flip = True,\n            rotation_range = 270,\n            vertical_flip = True,\n            samplewise_std_normalization = True,\n            brightness_range=[0.1,0.9],\n            width_shift_range=0.2,\n            height_shift_range=0.2)\n    \n    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n                                                                  samplewise_std_normalization = True)\n    \n    train_generator = train_datagen.flow_from_dataframe(\n            dataframe = train_dataframe,\n            directory = train_directory_path,\n            x_col = 'image_id',\n            y_col = 'label',\n            target_size = (Image_Size, Image_Size),\n            batch_size = Batch_Size,\n            class_mode = \"categorical\",\n            shuffle = True)\n    \n    validation_generator = test_datagen.flow_from_dataframe(\n            dataframe = validation_dataframe,\n            directory = train_directory_path,\n            x_col = 'image_id',\n            y_col = 'label',\n            target_size = (Image_Size, Image_Size),\n            batch_size = Batch_Size,\n            class_mode = \"categorical\",\n            shuffle = True)\n    \n    return train_generator, validation_generator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating data generators for Stratified KFold"},{"metadata":{"trusted":true},"cell_type":"code","source":"def creating_data_generators_with_kFoldCV(train_data, image_dir, validation_data, Image_Size, Batch_Size):\n    \n    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale = 1./255,\n            shear_range=25,\n            zoom_range = 0.3,\n            horizontal_flip = True,\n            rotation_range = 270,\n            vertical_flip = True,\n            samplewise_std_normalization = True,\n            brightness_range=[0.1,0.9],\n            width_shift_range=0.2,\n            height_shift_range=0.2)\n    \n    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,  samplewise_std_normalization = True)\n    train_generator = train_datagen.flow_from_dataframe(\n            dataframe = train_data,\n            directory = image_dir,\n            x_col = 'image_id',\n            y_col = 'label',\n            target_size = (Image_Size, Image_Size),\n            batch_size = Batch_Size,\n            class_mode = \"categorical\")\n    validation_generator = test_datagen.flow_from_dataframe(\n            dataframe = validation_data,\n            directory = image_dir,\n            x_col = 'image_id',\n            y_col = 'label',\n            target_size = (Image_Size, Image_Size),\n            batch_size = Batch_Size,\n            class_mode = \"categorical\")    \n    \n    return train_generator, validation_generator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def creating_the_model(model_name, classes, LEARNING_RATE, IMG_Size):\n    model = tf.keras.models.Sequential()\n    # initialize the model with input shape as (224,224,3)\n    if model_name == \"InceptionResNet\":\n        base = tf.keras.applications.InceptionResNetV2(input_shape = (IMG_Size, IMG_Size, 3), include_top = False, weights = 'imagenet')\n    if model_name == \"ENet\":\n        base = tf.keras.applications.EfficientNetB4(input_shape = (IMG_Size, IMG_Size, 3), include_top = False, weights = 'imagenet')\n    model.add(base)\n    model.add(tf.keras.layers.BatchNormalization(axis=-1))\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dense(256, activation = 'relu'))\n    #model.add(Dropout(0.7))\n    #model.add(tf.keras.layers.Dense(32, activation = 'relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(classes, activation = 'softmax'))\n    #for layer in base.layers:\n        #layer.trainable = False\n    \n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits = False,\n                                                   label_smoothing=0.4,\n                                                   name='categorical_crossentropy')\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n                        loss = loss,\n                        metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# callbacks function\ndef lr_scheduler(epoch,lr):\n    if epoch > 3 and epoch % 2 == 0:\n        return lr/1.125\n    else :\n        return lr\n\ndef leraning_rate_scheduler():\n    _lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n    return _lr_scheduler\ndef cassava_callback(checkpoint_path):\n    _model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath = checkpoint_path,\n    save_weights_only = False,\n    monitor = 'val_accuracy',\n    mode = 'max',\n    save_best_only = True)\n    return _model_checkpoint_callback \n\"\"\"\ndef reduce_learning_rate(FACTOR, PATIENCE):\n    _reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=FACTOR, patience=PATIENCE, verbose=0,\n    mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.0001)\n    \n    return _reduce_learning_rate\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def training_the_model_with_generator(cassava_model, train_generator, valid_generator, _model_checkpoint_callback, _lr_scheduler, batch_size, LEARNING_RATE, EPOCH):\n    history = cassava_model.fit(\n        train_generator,\n        steps_per_epoch = len(train_generator)/batch_size,\n        epochs = EPOCH,\n        validation_data = valid_generator,\n        validation_steps = len(valid_generator)/batch_size,\n        shuffle=True, callbacks=[_model_checkpoint_callback, _lr_scheduler])\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting Loss and Accuracy Graphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotting_the_graphs(history):\n    #  \"Accuracy\"\n    plt.figure()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    # \"Loss\"\n    plt.figure()\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main function from where all the functions are called"},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    #boolean variable to check whether to use sratified k fold or not\n    if_stratified_kold = True # check to see if we want to use Cross Validation\n    perform_oversampling = False # check to see if we want to perform oversampling or undersampling\n    use_merged_data = True # check to see if we want to use (2019 + 2020) data\n    load_model = True\n    \n    # name of the model to use\n    model_name = \"ENet\"  # Either use \"ENet\" : EfficientNetB4 or use \"InceptionResNet\" : InceptionResNetV2\n    #######################################################\n    \n    # some general parameters\n    EPOCH = 10 #number of epochs\n    batch_size = 8 #batch size \n    Image_Size = 512 #Size of image\n    classes = 5 #target classes\n    LEARNING_RATE = 0.01 #learning rate\n    \n    if use_merged_data == False:\n        train_csv_original = pd.read_csv(\"../input/cassava-leaf-disease-classification/train.csv\")\n    else:\n        train_csv_original = pd.read_csv(\"../input/cassava-leaf-disease-merged/merged.csv\")\n    \n    with open('../input/cassava-leaf-disease-classification/label_num_to_disease_map.json') as json_file:\n        data = json.load(json_file)\n    # exploring data using graphs\n    cbb, cbsd, cgm, cmd, healthy = exploring_the_data(train_csv_original, _return = 1)\n    print(\"[*] CBB = {}, CBSD = {}, CGM = {}, CMD = {}, HEALTHY = {}\".format(cbb, cbsd, cgm, cmd, healthy))\n    \n    \n    # oversampling the data and visualizing \n    train_csv = None\n    if perform_oversampling == True:\n        train_csv = oversampling_the_data(cbb, cbsd, cgm, cmd, healthy, train_csv_original, percent = 0.5)\n        print(\"[*] Plotting data after performing oversampling\")\n        cbb, cbsd, cgm, cmd, healthy = exploring_the_data(train_csv, _return = 1)\n        print(\"[+] After oversampling the numbers are :\\n\")\n        print(\"[*] CBB = {}, CBSD = {}, CGM = {}, CMD = {}, HEALTHY = {}\".format(cbb, cbsd, cgm, cmd, healthy))\n    \n        # saving the new dataframe\n        #train_csv.to_csv(\"/kaggle/working/train_csv.csv\", index = False)\n    else:\n        train_csv = train_csv_original\n\n    \n    # creating a new column called \"class_name\" for the dataframe\n    with open(\"../input/cassava-leaf-disease-classification/label_num_to_disease_map.json\") as f:\n        real_labels = json.load(f)\n        real_labels = {int(k):v for k,v in real_labels.items()}\n    # Defining the working dataset\n    train_csv['class_name'] = train_csv['label'].map(real_labels)\n    print(\"[*] New train CSV\\n\")\n    print(train_csv.head(5))\n    #train_csv.to_csv(\"/kaggle/working/train_csv.csv\", index = False)\n    \n        \n    # visulaize training data\n    visualizing_train_set(20, train_csv)\n    \n    # path to the train images that will be passed to the data generators\n    if use_merged_data == False:\n        image_path = '../input/cassava-leaf-disease-classification/train_images'\n    else:\n        image_path = '../input/cassava-leaf-disease-merged/train'\n    \n    # this part of if-else is considered if we do not want to perform KFold Cross Validation\n    if if_stratified_kold == False:\n        # creating the dataset\n        # using datagenerator to create dataset\n        creating_data_generator = True\n        if creating_data_generator:\n            print(\"[*] Creating the data generator\")\n            #train_csv = pd.read_csv(\"/kaggle/working/train_csv.csv\")\n            train_csv['label'] = train_csv.label.astype('str')\n            y_cols = train_csv.columns.to_list()[1:]\n            print(\"[*] Y_COLS : {}\".format(type(y_cols)))\n\n            # creating data generators\n            train_dataframe, test_dataframe = train_test_split(train_csv, test_size = 0.05, random_state = 42, shuffle=True, stratify = train_csv['class_name'])\n            train_generator, validation_generator = creating_data_generators(train_dataframe, image_path, test_dataframe, y_cols, Image_Size, batch_size)\n        else:\n            # creating the dataset using traditional method\n            print(\"[*] Creating the dataset\")\n            X_train, y_train_encoded, X_val, y_val_encoded, X_test, y_test_encoded = creating_the_dataset(train_csv, save_image_path)\n        \n        \n        ####### creating the model\n        \n        if load_model == False:\n            cassava_model = creating_the_model(model_name, classes, LEARNING_RATE, Image_Size)\n        else:\n            cassava_model = tf.keras.models.load_model(\"../input/cassav-model/cassava_model.h5\")\n        print(\"[*] The model is created\")\n        cassava_model.summary()\n        tf.keras.utils.plot_model(cassava_model)\n        \n        \n        \n        ###### creating the mkdir\n        save_path = '/kaggle/working/model_checkpoint'\n        if (os.path.exists(save_path)):\n            pass\n        else:\n            os.mkdir(save_path)\n        print(\"[*] The _model_checkpoint_callback save_path is created\")\n\n        ###### creating a callback checkpoint\n        checkpoint_path = \"/kaggle/working/model_checkpoint/cassava_model.h5\"\n        _model_checkpoint_callback = cassava_callback(checkpoint_path)\n\n        ## controlling learning rate parameters\n        FACTOR = 0.1\n        PATIENCE = 5\n        #reduce_learning_rate = reduce_learning_rate(FACTOR, PATIENCE)\n        _lr_scheduler = leraning_rate_scheduler()\n        ###### training the model\n        ## learning parameters\n        if creating_data_generator: \n            print(\"[*] Training the model with generator\")\n            history = training_the_model_with_generator(cassava_model, train_generator, validation_generator, _model_checkpoint_callback, _lr_scheduler, batch_size, LEARNING_RATE, EPOCH)\n        else:\n            print(\"[*] Training the model without generator\")\n            history = training_the_model(cassava_model, X_train, y_train_encoded, X_val, y_val_encoded, _model_checkpoint_callback)\n\n\n        ###### plotting the accuracy graphs\n        print(\"[*] Plotting the model accuracy and loss\")\n        plotting_the_graphs(history)\n        \n        ## Evaluating the Best model\n        # loading the best model\n        cassava_model = tf.keras.models.load_model(\"/kaggle/working/model_checkpoint/cassava_model.h5\")\n        # loading the test image and resizing and predicting\n        predictions = []\n        sample_submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n\n        for image in sample_submission.image_id:\n            test_image = tf.keras.preprocessing.image.load_img('../input/cassava-leaf-disease-classification/test_images/' + image)\n            test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n            test_image = tf.keras.preprocessing.image.smart_resize(test_image, (Image_Size, Image_Size))\n            test_image = tf.reshape(test_image, (-1, Image_Size, Image_Size, 3))\n            test_prediction = cassava_model.predict(test_image/255)\n            predictions.append(np.argmax(test_prediction))\n\n        submission = pd.DataFrame({'image_id': sample_submission.image_id, 'label': predictions})\n        submission.to_csv('submission.csv', index=False)\n        \n        print(\"Submission File:\")\n        print(submission.head()) # output\n        \n    ## this part of handles if we want to perform Cross Validation using Stratified KFold\n    else:\n        print (\"[*] Training with Stratified KFold\")\n        # implementing if \"if_stratified_k_fold\" is  True\n        \n        train_csv['label'] = train_csv.label.astype('str')\n        Y = train_csv['label']\n        fold_num = 0 # counter for fold number\n        for train_index, val_index in skf.split(np.zeros(len(Y)),Y):\n            # creating training and validation data using folds \n            training_data = train_csv.iloc[train_index]\n            validation_data = train_csv.iloc[val_index]\n            # creating data gneerators using the above created folds\n            train_generator, validation_generator = creating_data_generators_with_kFoldCV(training_data, image_path, validation_data, Image_Size, batch_size)\n                \n            ####### creating the model\n            if load_model == False:\n                cassava_model = creating_the_model(model_name, classes, LEARNING_RATE, Image_Size)\n            else:\n                cassava_model = tf.keras.models.load_model(\"../input/cassav-model/cassava_model.h5\")\n            print(\"[*] The model is created\")\n            cassava_model.summary()\n            tf.keras.utils.plot_model(cassava_model)\n        \n            tf.keras.utils.plot_model(cassava_model)\n\n            ###### creating the mkdir\n            save_path = '/kaggle/working/model_checkpoint'\n            if (os.path.exists(save_path)):\n                pass\n            else:\n                os.mkdir(save_path)\n            print(\"[*] The _model_checkpoint_callback save_path is created\")\n\n            ###### creating a callback checkpoint\n            checkpoint_path = \"/kaggle/working/model_checkpoint/cassava_model_{}.h5\".format(fold_num)\n            _model_checkpoint_callback = cassava_callback(checkpoint_path)\n\n            ## controlling learning rate parameters\n            FACTOR = 0.1\n            PATIENCE = 5\n            #_reduce_learning_rate = reduce_learning_rate(FACTOR, PATIENCE)\n            _lr_scheduler = leraning_rate_scheduler()\n            \n            ###### training the model\n            print(\"[*] Training the model for fold : {}\".format(fold_num))\n            history = training_the_model_with_generator(cassava_model, train_generator, validation_generator, _model_checkpoint_callback, _lr_scheduler, batch_size, LEARNING_RATE, EPOCH)\n        \n\n            ###### plotting the accuracy graphs\n            print(\"[*] Plotting the model accuracy and loss\")\n            plotting_the_graphs(history)\n            fold_num = fold_num + 1\n            \n        # Predicting using all the models and taking the best prediction\n        final_prediction = [] # list of all the predictions for each model from which we choose the best model\n        sample_submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n        for image in sample_submission.image_id:\n            test_image = tf.keras.preprocessing.image.load_img('../input/cassava-leaf-disease-classification/test_images/' + image)\n            test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n            test_image = tf.keras.preprocessing.image.smart_resize(test_image, (Image_Size, Image_Size))\n            test_image = tf.reshape(test_image, (-1, Image_Size, Image_Size, 3))\n            predictions = []\n            for files in glob.glob(\"/kaggle/working/model_checkpoint/*.h5\"):\n                print(\"[*] Model loaded for prediction : {}\".format(files.split('/')[-1]))\n                cassava_model = tf.keras.models.load_model(files)\n                # loading the test image and resizing and predicting\n                test_prediction = cassava_model.predict(test_image/255)\n                predictions.append(np.argmax(test_prediction))\n            final_prediction.append(max(predictions,key=predictions.count))\n            \n        submission = pd.DataFrame({'image_id': sample_submission.image_id, 'label': final_prediction})\n        submission.to_csv('submission.csv', index=False)\n        \n        print(\"Submission File:\")\n        print(submission.head()) # output\n            \n\n            \n\n                \n            \n                ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}