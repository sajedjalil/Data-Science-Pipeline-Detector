{"cells":[{"metadata":{},"cell_type":"markdown","source":"- I have prepared both datasets jpeg and tfredords.\n- There is a prepared dataset at the following link.\n- JPEG ([128x128](https://www.kaggle.com/spidermandance/cassava-jpeg-128x128), [196x196](https://www.kaggle.com/spidermandance/cassava-jpeg-196x196), [256x256](https://www.kaggle.com/spidermandance/cassava-jpeg-256x256), [384x384](https://www.kaggle.com/spidermandance/cassava-jpeg-384x384), [512x512](https://www.kaggle.com/spidermandance/cassava-jpeg-512x512))\n- TFRecords ([128x128](https://www.kaggle.com/spidermandance/cassava-tfrecords-128x128), [196x196](https://www.kaggle.com/spidermandance/cassava-tfrecords-196x196), [256x256](https://www.kaggle.com/spidermandance/cassava-tfrecords-256x256), [384x384](https://www.kaggle.com/spidermandance/cassava-tfrecords-384x384), [512x512](https://www.kaggle.com/spidermandance/cassava-tfrecords-512x512))\n\n(11/21/2020 ver3)\n- Fix: Removing the `img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)` TFRecords cell.\n- Add: Image check."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n\nDATA_PATH = '/kaggle/input/cassava-leaf-disease-classification'\n\nJPEG_PATH = os.path.join(DATA_PATH, 'train_images')\nJPEG_SAVE_PATH = '/kaggle/train_images_jpeg'\n\nCSV_PATH = os.path.join(DATA_PATH, 'train.csv')\n\nRESIZE = 128\nNUM_TFREDORDS = 1338\nIMG_QUALITY = 95\nDEBUG = False\n\n\nos.makedirs(JPEG_SAVE_PATH, exist_ok=True)\ntrain_df = pd.read_csv(CSV_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Size checking"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"img_size = {}\nfiles = sorted(os.listdir(JPEG_PATH))\ntargets = train_df['label']\n\n# https://www.kaggle.com/nakajima/duplicate-train-images\n# drop_img_id = ['3551135685.jpg', '911861181.jpg', '1562043567.jpg', '3551135685.jpg']\n# flies = [f for f in files if f not in drop_img_id]\n\nif DEBUG:\n    files = files[:25]\n    targets = targets[:25]\n\nfor img_id in tqdm(files):\n    img = cv2.imread(os.path.join(JPEG_PATH, img_id))\n    if img.shape in img_size:\n        img_size[img.shape] += 1\n    else:\n        img_size[img.shape] = 1\n        \nprint(f'Size of each the image: {img_size}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## JPEG"},{"metadata":{"trusted":true},"cell_type":"code","source":"for img_id in tqdm(files):\n    load_path = os.path.join(JPEG_PATH, img_id)\n    save_path = os.path.join(JPEG_SAVE_PATH, img_id)\n    img = cv2.imread(load_path)\n    img = cv2.resize(img, (RESIZE, RESIZE))\n    cv2.imwrite(save_path, img)\n    \n!tar -czf 'train_images_{RESIZE}x{RESIZE}.tar.gz' /kaggle/train_images_jpeg/*.jpg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### image checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef jpeg_display(directory_path):\n    fig, axes = plt.subplots(5, 5, figsize=(16, 16))\n    for i in range(25):\n        img = cv2.imread(os.path.join(directory_path, files[i]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes[i//5][i%5].imshow(img)\n        axes[i//5][i%5].set_title(f'{files[i]}: {targets[i]}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jpeg_display(JPEG_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jpeg_display(JPEG_SAVE_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TFRecords"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nnum_iter = math.ceil(len(files) / NUM_TFREDORDS)\n\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef serialize_example(feature0, feature1, feature2):\n  feature = {\n      'image': _bytes_feature(feature0),\n      'image_name': _bytes_feature(feature1),\n      'target': _int64_feature(feature2)\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()\n\n\n\nfor i in range(num_iter):\n    print(f'Writing TFRecord: {i}')\n    cnt = min(NUM_TFREDORDS, len(files) - i*NUM_TFREDORDS)\n    tf_filename = f'ld_train{str(i).zfill(2)}-{cnt}.tfrec'\n    \n    with tf.io.TFRecordWriter(tf_filename) as wf:\n        for j in range(cnt):\n            img_id = files[NUM_TFREDORDS*i + j]\n            img = cv2.imread(os.path.join(JPEG_PATH, img_id))\n            img = cv2.resize(img, (RESIZE, RESIZE))\n            # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  -> Fix:20201121\n            \n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, IMG_QUALITY))[1].tostring()\n            img_id = str.encode(img_id)\n            target = train_df['label'][NUM_TFREDORDS*i + j]\n            \n            example = serialize_example(img, img_id, target)\n            \n            wf.write(example)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### image checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\n\ndef parse_example(example):\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = example['image_name']\n    target = example['target']\n    return image, label, target\n\n\ndef display_one(image, title, target, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(f'{title}: {target}')\n    return (subplot[0], subplot[1], subplot[2]+1)\n\n\ndef display_batch_of_images(databatch):\n    images, labels, targets = databatch\n    images = images.numpy()\n    labels = labels.numpy()\n    targets = targets.numpy()\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n    if targets is None:\n        targets = [None for _ in enumerate(targets)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.2\n    subplot=(rows, cols, 1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label, target) in enumerate(zip(images[:rows*cols], labels[:rows*cols], targets[:rows*cols])):\n        title = label\n        title = title.decode('utf-8')\n        correct = True\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one(image, title, target, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0.2, hspace=0.2)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resize_file = f'ld_train00-{min(len(files), NUM_TFREDORDS)}.tfrec'\ndataset = tf.data.TFRecordDataset([resize_file]).map(parse_example).batch(25)\ndata = iter(dataset)\ndisplay_batch_of_images(next(data))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}