{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport re\nimport glob\nimport tensorflow as tf\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TPU/GPU Configuration"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"basepath = \"../input/cassava-leaf-disease-classification/\"\ntrain_images = basepath + 'train_images/'\ntrain_label = pd.read_csv(basepath + 'train.csv')\n\nDATASET = '../input/cassava-leaf-disease-tfrecords-384x384/' \ntrain_filenames_name = [file for file in os.listdir(DATASET) if file.endswith(\".tfrec\")]  \ntrain_filenames = [DATASET + tfrec for tfrec in train_filenames_name]\n\ntest_filenames_name = [file for file in os.listdir(basepath + 'test_tfrecords/') if file.endswith(\".tfrec\")]  \ntest_filenames = [basepath + 'test_tfrecords/' + tfrec for tfrec in test_filenames_name]\n\nmodel_path_list = glob.glob('../input/cassava-leaf-disease-classon-inceptionv3/*.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(basepath)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nSEED = 42\nSIZE = [384,384] \nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nTTA_STEPS = 5\nN_CLASSES = 5\n\ndef seed_everything(SEED):\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED) \n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize  Image samples"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize': (12, 5)})\n\ntrain_count_label = train_label.groupby('label').aggregate({'label': 'count'}).rename(\n    columns={'label': 'label_count'}).reset_index()\n\ntrain_count_label['label'] = train_count_label['label'].map(\n    {0: '0: Cassava Bacterial Blight (CBB)',\n     1: '1: Cassava Brown Streak Disease (CBSD)', \n     2: '2: Cassava Green Mottle (CGM)',     \n     3: '3: Cassava Mosaic Disease (CMD)',\n     4: '4: Healthy'}).astype(str)\n\n\ntrain_count_label_l = sns.barplot('label', 'label_count', data = train_count_label)\nfor item in train_count_label_l.get_xticklabels():\n    item.set_rotation(65)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_img(img_list, cassava):\n    plt.figure(figsize=(20, 7))\n    pos = 1\n    for i in img_list:\n        image = cv2.imread(i)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        plt.subplot(2, 3, pos)\n        plt.title(cassava)\n        plt.imshow(image)\n        plt.axis(\"off\")\n        pos += 1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for num_label, cassava in zip(list(range(5)), list(train_count_label.label)):\n    random_label_number = random.sample(list(train_label[train_label['label'] == num_label].index), 6)\n    im = [train_images + jpg for jpg in train_label.iloc[random_label_number].image_id]\n    show_img(im, cassava)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def decode_image(image, labeled = True):\n    image = tf.image.decode_jpeg(image, channels=3) \n    image = tf.cast(image, tf.float32)/255.0 \n    if labeled:\n        image = tf.reshape(image, [*SIZE, 3]) \n    else: \n        image = tf.reshape(image, [512,512,3]) \n    return image\n\ndef data_augment(image, label=None, seed=SEED):\n    #image = tf.image.rot90(image,k=np.random.randint(4))\n    #image = tf.image.random_flip_left_right(image, seed=seed) \n    #image = tf.image.random_flip_up_down(image, seed=seed) \n    \n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    p_flip = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    if p_flip >= .8:\n        image = tf.image.random_flip_left_right(image) \n    elif p_flip >= .5:   \n        image = tf.image.random_flip_up_down(image) \n    else:\n        image = tf.image.rot90(image, k=4) \n\n    if p_pixel_1 >= .4: \n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4: \n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4: \n        image = tf.image.random_brightness(image, max_delta=.1)\n    \n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1)\n    elif p_rotate > .15:\n        image = tf.image.rot90(image, k=4) \n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef read_labeled_tfrecord(example):\n    \n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"target\": tf.io.FixedLenFeature([], tf.int64),  } \n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'], labeled = True)\n    label = tf.cast(example['target'], tf.int32) \n    return image, label \n\ndef read_unlabeled_tfrecord(example):\n    \n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string), }\n    \n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT) \n    image = decode_image(example['image'], labeled = False)\n    image_name = example['image_name']\n    return image, image_name\n\ndef load_dataset(filenames, labeled=True, ordered=False): \n    \n    ignore_order = tf.data.Options() \n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    dataset = (tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) \n              .with_options(ignore_order) \n              .map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO))\n            \n    return dataset\n\ndef count_data_items(filenames): \n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_filenames, valid_filenames = train_test_split(train_filenames, test_size = 0.2,random_state = SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = (load_dataset(train_filenames, labeled=True)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE,drop_remainder=True)\n    .repeat()\n    .prefetch(AUTO))\n\n\nvalid_dataset = (load_dataset(valid_filenames, labeled=True)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_dataset)\nprint(valid_dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = (load_dataset(test_filenames, labeled=False,ordered=True).batch(BATCH_SIZE))  \ntest_dataset_image_name = test_dataset.map(lambda image, image_name: image_name).unbatch()\nnum_test_images = count_data_items(test_filenames)\ntest_ids = next(iter(test_dataset_image_name.batch(num_test_images))).numpy().astype('U')\n\npredictions = np.zeros((num_test_images, N_CLASSES))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Inception_model(unitsdense):\n\n    with strategy.scope(): \n\n        model = tf.keras.Sequential([\n            tf.keras.applications.InceptionV3(input_shape=(*SIZE, 3), weights=None,pooling='avg',include_top=False),\n            tf.keras.layers.Dropout(0.15),\n            tf.keras.layers.Dense(150, activation = unitsdense[0]),\n            tf.keras.layers.Dense(50, activation = unitsdense[1]),\n            tf.keras.layers.Dense(5, activation = 'softmax')\n        ])\n        \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Inception_model(['relu', 'relu'])\nmodel2 = Inception_model(['elu', 'elu'])\nmodel3 = Inception_model(['relu', 'elu'])\nmodel4 = Inception_model(['elu', 'relu'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for pat in range(len(model_path_list)):\n    print(model_path_list[pat])\n    K.clear_session()\n    model1.load_weights(model_path_list[pat])\n    model2.load_weights(model_path_list[pat])\n    model3.load_weights(model_path_list[pat])\n    model4.load_weights(model_path_list[pat])\n    \n    for step in range(TTA_STEPS):\n        \n        test_dataset_images = test_dataset.map(lambda image, image_name: image)\n        \n        predictions += model1.predict(test_dataset_images) / (TTA_STEPS * len(model_path_list))\n        predictions += model2.predict(test_dataset_images) / (TTA_STEPS * len(model_path_list))\n        predictions += model3.predict(test_dataset_images) / (TTA_STEPS * len(model_path_list))\n        predictions += model4.predict(test_dataset_images) / (TTA_STEPS * len(model_path_list))\n        \n    print(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.argmax(predictions, axis=-1)\nsubmission = pd.DataFrame({'image_id': test_ids, 'label': predictions})  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}