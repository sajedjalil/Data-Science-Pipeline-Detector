{"cells":[{"metadata":{"_uuid":"a177ef48-0045-4dae-ab23-46a9aff908ad","_cell_guid":"8f7f298e-d74e-412d-a665-450c9d07cd1c","trusted":true},"cell_type":"markdown","source":"_Hello and welcome to my notebook!\\\nFeel free to leave any feedback or observation in the comment section_"},{"metadata":{"_uuid":"f0a98a97-068a-48a2-b34b-318e6da3bc3d","_cell_guid":"96d46ce7-8498-4504-8ed6-9b2a3a8751f5","trusted":true},"cell_type":"markdown","source":"![](https://jgi.doe.gov/wp-content/uploads/2016/04/w1-IMG_1213_jessenmottled.jpg)\n\nThe **Cassava** plant is the second biggest source of carbohydrates for the African population. The diseases affecting it are a distressing issue that is ought to be solved through modern techniques, such as Machine Learning. Make sure to watch this inspiring [video](https://www.youtube.com/watch?v=NlpS-DhayQA) from the TensorFlow team that tackles this specific problem."},{"metadata":{"_uuid":"fe946223-16ec-4bbe-861d-6d46940f9085","_cell_guid":"99a134d1-9610-4ebd-a353-736cff370a83","trusted":true},"cell_type":"markdown","source":"## What can you find in this notebook\n- a Deep Neural Network classifier using TensorFlow\n- for some simple **EDA**, make sure to check out [my other notebook](https://www.kaggle.com/grigorelucian/simple-eda-cassava-leaf-disease)"},{"metadata":{},"cell_type":"markdown","source":"## Why use EfficientNets\n### and not juse another classic Convolutional Neural Network?\n\n![](https://1.bp.blogspot.com/-oNSfIOzO8ko/XO3BtHnUx0I/AAAAAAAAEKk/rJ2tHovGkzsyZnCbwVad-Q3ZBnwQmCFsgCEwYBhgL/s1600/image3.png)\n\nWell, the answer is simple and consists of two parts.\\\n**Firstly**, in the most common researches in the field, CNNs are overtaken by EfficientNets in every category possible. Thus, this new type of networks:\n* have less parameters\n* achieve better accuracy on 5 out of 8 widely used datasets\n* use less hardware (and have better latency)\n\n_Make sure to refer to [this article](https://arxiv.org/pdf/1905.11946.pdf)_\n\n**Secondly**, I have personally observed that classic CNNs will just bottleneck at some point on large datasets. Of course, without using 12 GPUs and 64-core CPUs.\nA previous model of mine achieved on this particular dataset no more than ~61% accuracy on the train set, which has been actually achieved before first epoch ended. The model was not able to improve this accuracy by the end of the training.\n\n_You can find that model [here](https://www.kaggle.com/grigorelucian/classic-cnn-cassava-leaf-disease)_"},{"metadata":{"_uuid":"b38a7696-9be5-4e2a-97d8-f693abd28209","_cell_guid":"f3e34678-fa80-4c42-82bd-4eb6a190c31a","trusted":true},"cell_type":"markdown","source":"## Let's get to work!"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip show tensorflow","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a97d2cb6-18a6-4d41-8dd7-3c3f684e53d1","_cell_guid":"82177838-5543-4914-998a-cc767fc047e0","trusted":true},"cell_type":"code","source":"# libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.data import TFRecordDataset\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.activations import swish\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# constants\nIMAGE_WIDTH = IMAGE_HEIGHT = 300\nno_batches = 64\nno_classes = 5\ndecay = 0.9\nmomentum = 0.9\nbatch_norm_momentum = 0.99\nweight_decay = 1e-5\ninitial_lr = 0.256\nlr_decay = 0.97\nlr_decay_freq = 2.4 # epochs\ndropout_rate = 0.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# storing the tfrecords filenames\ntfrecs_path = '../input/cassava-leaf-disease-classification/train_tfrecords'\nrecords = os.listdir(tfrecs_path)\ndef _fmap(elem):\n    return tfrecs_path + '/' + elem\n\n# personally avoided using map() because it returns a <map object> that cannot be used later\nfor i in range(0, len(records)):\n    records[i] = _fmap(records[i])\nfor record in records:\n    print(record)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # creating raw train dataset\n# raw_dataset = TFRecordDataset(records)\n# raw_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how one entry looks like (I will do it here with some lines from the first image)\n> for entry in raw_dataset.take(1):\\\n>     print(repr(entry))\n\n> <tf.Tensor: shape=(), dtype=string, numpy=b'\\n\\xc5\\xef\\x06\\n\\x1f\\n\\nimage_name\\x12\\x11\\n\\x0f\\n\\r499934842.jpg\\n\\x0f\\n\\x06target\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x8f\\xef\\x06\\n\\x05image\\x12\\x84\\xef\\x06\\n\\x80\\xef\\x06\\n\\xfc\\xee\\x06\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x01\\x01\\x01\\x01\\x02\\x01\\x01\\x01\\x02\\x02\\x02\\x02\\x02\\x04\\x03\\x02\\x02\\x02\\x02\\x05\\x04\\x04\\x03\\x04\\x06\\x05\\x06\\x06\\x06\\x05\\x06\\x06\\x06\\x07\\t\\x08\\x06\\x07\\t\\x07\\x06\\x06\\x08\\x0b\\x08\\t\\n\\n\\n\\n\\n\\x06\\x08\\x0b\\x0c\\x0b\\n\\x0c\\t\\n\\n\\n\\xff\\xdb\\x00C\\x01\\x02\\x02\\x02\\x02\\x02\\x02\\x05\\x03\\x03\\x05\\n\\x07\\x06\\x07\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xff\\xc0\\x00\\x11\\x08\\x02\\x00\\x02\\x00\\x03\\x01\\x11\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1e\\x00\\x00\\x02\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x05\\x03\\x06\\x02\\x07\\x08\\x01\\x00\\t\\n\\xff\\xc4\\x00O\\x10\\x00\\x01\\x03\\x02\\x04\\x04\\x04\\x04\\x04\\x03\\x06\\x04\\x04\\x04\\x02\\x0b\\x01\\x02\\x03\\x04\\x05\\x11\\x00\\x06\\x12!\\x07\\x131A\\x08\"Qa\\x142q\\x81\\x15#B\\x91R\\xa1\\xb1\\t\\x16$3b\\xc1Cr\\x82\\xd14\\xe1\\xf0\\xf1\\x17%S\\x92\\xa25s\\x18&Dc\\xb2\\'\\x83\\xa3\\xc2\\xd2\\xff\\xc4\\x00\\x1c\\x01\\x00\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x04\\x01\\x02\\x05\\x00\\x06\\x07\\x08\\xff\\xc4\\x009\\x11\\x00\\x02\\x02\\x02\\x02\\x02\\x02\\x01\\x04\\x00\\x05\\x03\\x02\\x06\\x02\\x03\\x01\\x02\\x00\\x03\\x04\\x11\\x12!\\x051\\x13\"A\\x06\\x142Q#3Baq\\x07\\x15\\x81$R\\x16\\x174\\xa1\\xc1\\xf0%\\x91C\\xb1\\xe1\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\xef\\xd4\\xf8|r:\\xd0\\xec\\xfa\\xfcB\\xa4u\\xfc\\xe4\\xd8\\x7f<|\\xc5\\xef\\x13\\xc3\\xff\\x00\\xd8\\xf0\\x7f\\xb8MK\\x87T8\\xd1\\xd3\\x19\\x9c\\xcb\\r\\xa1n\\xef$\\x8b\\xf7\\xef\\x8c\\xdb\\xef\\xb46\\xd7\\xd4\\xb2\\xf8\\xcf\\x1bWD\\xc54\\xac\\x97\\x95\\xe2\\xcaP\\x9b\\x9ei\\xae%F\\xe0%\\xd1\\xb7\\xf3\\xc2/\\xe4\\xb2\\x14\\xea2\\x98>/\\xff\\x00tx\\xceE\\xcb\\x08\\x90\\x87\\xe2q\\x02\\x9c\\x95\\x84$\\x84\\x19(\\xb0 \\xfdp\\x0e\\x18\\xc8\\xdb\\xdcg\\xf6\\x9e7\\xfb\\x93\\xd48=\\x94\\xb3J\\xd0\\x99\\\\H\\x80\\xa0\\x8d\\xd4\\x94\\xbe\\x92\\t\\xfd\\xf1\\xb7\\x8d\\x9fEi\\xf52\\x1b\\x07\\xc6\\xd85\\xb8\\xd6\\x99\\xe1\\xc3#<\\xd8z6y\\x82\\xee\\x825$<\\x91\\xb5\\xfe\\xb8c\\xfe\\xe4%\\xab\\xf0\\x9e1\\xbd\\x99t\\xa0p\\x92\\x87L\\xa6=O\\xa6\\xd7\\xe2\\xa3\\x9c\\x82\\x95\\x10\\xe87\\xf7\\xc4\\x8c\\xca-;x\\xdax\\n5\\xaa\\x1c\\x01\\x11O\\xf0\\xa1^\\x9b\\xf9\\xf034r\\x95\\x1d\\x88#\\x0fU\\x99\\x88\\xbf\\x99C\\xfaJ\\xcb\\xbb\\xf9\\x04\\xc1\\xbf\\x0c\\x9cK\\xa1\\xb6]\\x89Si\\xe2>Ts6#\\xd7\\x14\\xbb\"\\xab\\x07F$\\x7fJ\\xdb[\\x1f\\xcc\\x90\\xf0\\x97\\x89pc\\xf2\\x1a\\xa1\\x95\\xb8\\xb4\\xeaR\\xf5\\x02\\x95c\\x1e\\xfc5\\xb3\\xd4\\xb3x,\\x85B$QrF}\\xa58\\x99K\\xcbn4\\xe2Up\\xa4\\xa4\\xd8\\x9cgY\\x81r\\xfa\\x81\\xab\\xc6\\xe4\\xd3\\xf8\\x96\\xac\\xbfU\\x"},{"metadata":{},"cell_type":"markdown","source":"As we can see, one entry in these tfrecords is described through:\n* target\n* image_name\n* image\n\nWe now need to parse these entries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # let's parse these records\n# features = {\n#     'target': tf.io.FixedLenFeature([],\n#                                     tf.int64,\n#                                     default_value = 0),\n#     'image_name': tf.io.FixedLenFeature([],\n#                                         tf.string,\n#                                         default_value = ''),\n#     'image': tf.io.FixedLenFeature([],\n#                                    tf.string,\n#                                    default_value = '')}\n\n# def _parse_function(proto):    \n#     return tf.io.parse_single_example(proto, features)\n\n# parsed_dataset = raw_dataset.map(_parse_function,\n#                                  num_parallel_calls = 4)\n# # parsed_dataset.repeat()\n# # parsed_dataset.batch(no_batches)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how a parsed entry looks like\n> for entry in parsed_dataset.take(1):\\\n>     print(repr(entry))\n\n> {'image': <tf.Tensor: shape=(), dtype=string, numpy=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x01,\\x01,\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x01\\x01\\x01\\x01\\x02\\x01\\x01\\x01\\x02\\x02\\x02\\x02\\x02\\x04\\x03\\x02\\x02\\x02\\x02\\x05\\x04\\x04\\x03\\x04\\x06\\x05\\x06\\x06\\x06\\x05\\x06\\x06\\x06\\x07\\t\\x08\\x06\\x07\\t\\x07\\x06\\x06\\x08\\x0b\\x08\\t\\n\\n\\n\\n\\n\\x06\\x08\\x0b\\x0c\\x0b\\n\\x0c\\t\\n\\n\\n\\xff\\xdb\\x00C\\x01\\x02\\x02\\x02\\x02\\x02\\x02\\x05\\x03\\x03\\x05\\n\\x07\\x06\\x07\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xff\\xc0\\x00\\x11\\x08\\x02\\x00\\x02\\x00\\x03\\x01\\x11\\x00\\x02\\x11\\x01\\x03\\x11\\x01\\xff\\xc4\\x00\\x1e\\x00\\x00\\x02\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x05\\x03\\x06\\x02\\x07\\x08\\x01\\x00\\t\\n\\xff\\xc4\\x00O\\x10\\x00\\x01\\x03\\x02\\x04\\x04\\x04\\x04\\x04\\x03\\x06\\x04\\x04\\x04\\x02\\x0b\\x01\\x02\\x03\\x04\\x05\\x11\\x00\\x06\\x12!\\x07\\x131A\\x08\"Qa\\x142q\\x81\\x15#B\\x91R\\xa1\\xb1\\t\\x16$3b\\xc1Cr\\x82\\xd14\\xe1\\xf0\\xf1\\x17%S\\x92\\xa25s\\x18&Dc\\xb2\\'\\x83\\xa3\\xc2\\xd2\\xff\\xc4\\x00\\x1c\\x01\\x00\\x02\\x03\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x04\\x01\\x02\\x05\\x00\\x06\\x07\\x08\\xff\\xc4\\x009\\x11\\x00\\x02\\x02\\x02\\x02\\x02\\x02\\x01\\x04\\x00\\x05\\x03\\x02\\x06\\x02\\x03\\x01\\x02\\x00\\x03\\x04\\x11\\x12!\\x051\\x13\"A\\x06\\x142Q#3Baq\\x07\\x15\\x81$R\\x16\\x174\\xa1\\xc1\\xf0%\\x91C\\xb1\\xe1\\xff\\xda\\x00\\x0c\\x03\\x01\\x00\\x02\\x11\\x03\\x11\\x00?\\x00\\xef\\xd4\\xf8|r:\\xd0\\xec\\xfa\\xfcB\\xa4u\\xfc\\xe4\\xd8\\x7f<|\\xc5\\xef\\x13\\xc3\\xff\\x00\\xd8\\xf0\\x7f\\xb8MK\\x87T8\\xd1\\xd3\\x19\\x9c\\xcb\\r\\xa1n\\xef$\\x8b\\xf7\\xef\\x8c\\xdb\\xef\\xb46\\xd7\\xd4\\xb2\\xf8\\xcf\\x1bWD\\xc54\\xac\\x97\\x95\\xe2\\xcaP\\x9b\\x9ei\\xae%F\\xe0%\\xd1\\xb7\\xf3\\xc2/\\xe4\\xb2\\x14\\xea2\\x98>/\\xff\\x00tx\\xceE\\xcb\\x08\\x90\\x87\\xe2q\\x02\\x9c\\x95\\x84$\\x84\\x19(\\xb0 \\xfdp\\x0e\\x18\\xc8\\xdb\\xdcg\\xf6\\x9e7\\xfb\\x93\\xd48=\\x94\\xb3J\\xd0\\x99\\\\H\\x80\\xa0\\x8d\\xd4\\x94\\xbe\\x92\\t\\xfd\\xf1\\xb7\\x8d\\x9fEi\\xf52\\x1b\\x07\\xc6\\xd85\\xb8\\xd6\\x99\\xe1\\xc3#<\\xd8z6y\\x82\\xee\\x825$<\\x91\\xb5\\xfe\\xb8c\\xfe\\xe4%\\xab\\xf0\\x9e1\\xbd\\x99t\\xa0p\\x92\\x87L\\xa6=O\\xa6\\xd7\\xe2\\xa3\\x9c\\x82\\x95\\x10\\xe87\\xf7\\xc4\\x8c\\xca-;x\\xdax\\n5\\xaa\\x1c\\x01\\x11O\\xf0\\xa1^\\x9b\\xf9\\xf034r\\x95\\x1d\\x88#\\x0fU\\x99\\x88\\xbf\\x99C\\xfaJ\\xcb\\xbb\\xf9\\x04\\xc1\\xbf\\x0c\\x9cK\\xa1\\xb6]\\x89Si\\xe2>Ts6#\\xd7\\x14\\xbb\"\\xab\\x07F$\\x7fJ\\xdb[\\x1f\\xcc\\x90\\xf0\\x97\\x89pc\\xf2\\x1a\\xa1\\x95\\xb8\\xb4\\xeaR\\xf5\\x02\\x95c\\x1e\\xfc5\\xb3\\xd4\\xb3x,\\x85B$QrF}\\xa58\\x99K\\xcbn4\\xe2Up\\xa4\\xa4\\xd8\\x9cgY\\x81r\\xfa\\x81\\xab\\xc6\\xe4\\xd3\\xf8\\x96\\xac\\xbfU\\xcf\\xd1\\x0f&\\xa7\\x05\\xc7\\x12~f\\xe47\\xb7\\xf3\\xc2\\x8e\\xdeC\\x10\\xee\\xa1\\xeeiQ~v7J=\\xc7i\\xe1\\xfeI\\xce\\xcb13\\x16Xm\\x97Kw\\x1f\\x93a{\\xfa\\xfa\\xe1\\xca<\\x8eH\\x1f\\xe2\\x89\\xa0q\\xd3\\xc"},{"metadata":{},"cell_type":"markdown","source":"Now we have, for each image, a dictionary with key:value pairs for each of the features! Awesome!\\\nEvery value is a **Tensor**, which, unfortunately, cannot be fed into EfficientNets, as these are made to deal with images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# in_image_tensor = []\n# in_label_tensor = []\n# for elem in parsed_dataset.take(-1):\n# #     img = tf.io.decode_and_crop_jpeg(elem['image'],\n# #                                      crop_window = \n# #                                      channels = 3)\n#     #img = tf.image.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n# #     img = elem['image']\n#     img = tf.io.decode_jpeg(elem['image'], channels = 3)\n#     img = tf.image.resize_with_pad(img,\n#                                    target_width = IMAGE_WIDTH,\n#                                    target_height = IMAGE_HEIGHT,\n#                                    method = 'bicubic')\n#     in_image_tensor.append(img)\n#     lbl = elem['target']\n#     in_label_tensor.append(lbl)\n    \n# print(len(in_label_tensor))\n# print(len(in_image_tensor))\n# print(in_label_tensor[0])\n# print(in_image_tensor[0])\n\n# # with tf.compat.v1.Session():\n# #     print(in_image_tensor[0].numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # function that parses one single \n# def _fparse(proto):\n#     features = {\n#         'target': tf.io.FixedLenFeature([],\n#                                         tf.int64,\n#                                         default_value = 0),\n#         'image_name': tf.io.FixedLenFeature([],\n#                                             tf.string,\n#                                             default_value = ''),\n#         'image': tf.io.FixedLenFeature([],\n#                                        tf.string,\n#                                        default_value = '')}\n    \n#     parsed_entry = tf.io.parse_single_example(proto, features)\n#     image = tf.io.decode_jpeg(parsed_entry['image'], channels = 3)\n#     image = tf.image.resize_with_pad(image, target_width = IMAGE_WIDTH, target_height = IMAGE_HEIGHT)\n#     return image, parsed_entry['target']\n\n# def gen_dataset(filenames):\n#     dataset = TFRecordDataset(filenames)\n#     dataset = dataset.map(_fparse, num_parallel_calls = 4)\n#     iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n#     image, label = iterator.get_next()\n#     #image = tf.reshape(image, [1, IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n#     #label = tf.one_hot(label, no_classes)\n#     return image, label\n\n# in_image_tensor, in_label_tensor = gen_dataset(records)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing baseline network\nbase_model = EfficientNetB3(include_top = False,\n                            weights = 'imagenet',\n                            pooling = 'avg',\n                            input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nfor layer in base_model.layers:\n    layer.trainable = True\n\n# building model\nclassifier = Sequential()\nclassifier.add(base_model)\nclassifier.add(Dense(units = 128, activation = 'swish'))\nclassifier.add(Dropout(dropout_rate))\nclassifier.add(Dense(no_classes, activation = 'softmax'))\n\n# let's compile the model\nclassifier.compile(optimizer = RMSprop(lr = initial_lr,\n                                      momentum = momentum,\n                                      rho = decay,\n                                      centered = True,\n                                      epsilon = weight_decay),\n                   loss = 'categorical_crossentropy',\n                   metrics = ['accuracy'])\n\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _schedule(epoch, lr):\n    '''\n    We reduce learning rate by 0.97 every 2.4 epochs, as suggested in the article.\n    '''\n#     if epoch % lr_decay_freq == 0.0 and epoch > 2.4:\n#         lr -= lr_decay\n    return lr\n\nlearning_rate_scheduler = LearningRateScheduler(_schedule,\n                                                verbose = 1)\n\ncallbacks_used = [\n#     learning_rate_scheduler,\n    EarlyStopping(monitor = 'accuracy',\n                  patience = 3),\n    ModelCheckpoint(filepath = 'cassava_model.h5',\n                    monitor = 'accuracy',\n                    save_best_only = True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training our model\nclassifier.fit(x = in_image_tensor,\n               y = in_label_tensor,\n               epochs = 10,\n               verbose = 1,\n               callbacks = callbacks_used)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*References:*\\\nhttp://digital-thinking.de/tensorflow-vs-keras-or-how-to-speed-up-your-training-for-image-data-sets-by-factor-10/\nhttps://arxiv.org/pdf/1905.11946.pdf"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}