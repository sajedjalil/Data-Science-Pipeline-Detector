{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, json, cv2, math, re\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nimport random\n\n#model imports (keras/tensorflow)\nimport tensorflow as tf\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras import layers, models\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport tensorflow.keras.backend as K\n\nfrom kaggle_datasets import KaggleDatasets\nfrom functools import partial\n\nprint(\"Tensorflow version \" + tf.__version__)\n\nos.system('pip install /kaggle/input/kerasapplications -q')\nos.system('pip install /kaggle/input/efficientnet-keras-source-code/ -q --no-deps')\n\nimport efficientnet.tfkeras as efn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Notes\n \n- Everyone here is in the top 100, might be worth checking out [https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/212411#1162195](link)\n\n- vit models? [vit-keras](https://github.com/faustomorales/vit-keras)\n    - #! pip install vit-keras\n    - #from vit_keras import vit, utils\n\n- Oof cross val [M.Innat's Notebook](https://www.kaggle.com/ipythonx/tf-keras-cassava-leaf-advanced-augmentation)\n    \n- Try training on a EfficientNetB6 or EfficientNetB7 because their input sizes are closer to 512x512? [link](https://github.com/brendanartley/keras-applications)\n\n- Going to see if I can try using a ResNet50 / SeresneXt50 model. I am able to implement build that model, but it seems I need some sort of pipeline to get validation accuracy\n\n- Train without mixup and cutmix on first two or three epochs [Custom Training Loop](https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/212347)\n\n#### Denoising\n\n- See distributions of the imgs that were wrongly predicted. If the wrongly predicted images have a low prob threshold we could change something on submission?\n- denoise strat 1st place solution [link](https://www.kaggle.com/c/prostate-cancer-grade-assessment/discussion/169143)\n- awesome discussion [link](https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/202673)\n\n### Updates\n\n- V4: Normalized test images on submission (prior to this all TPU submission were <.60) \n- V9: New Transform option + effnetb3 - LB: .870\n- V14: Increase train size - LB: .870\n- V27: Manually executing notebook via run all as TPU Commit is not working\n- V28: add dropout layer + cosine decay LR - LB: .877\n- V30: Lowered learning rate / lowest val_loss score (val_loss - 0.34114) - LB: 0.883\n- V31: EffNetB4 - LB: 0.881\n- V35: Course Dropout/My TFrecord Files - LB: 0.881\n- V36: Effnetb3 new seed (val_loss 0.30586) - LB: 0.876 \n- V41: Fixed TFrecords + Normalized by channel mean-std - LB: 0.882\n- V46: simple_data_augmentor w/ cutmix and mixup + one-hot-encoded labels (val_loss 0.29366) - LB: 0.884\n- V48: seed 14 - LB: 0.887\n- V58: 2019 + 2020 data, Custom Loss LB: 0.887\n- V61: Label Smoothing + no cutmix LB: 0.888\n- V65: EffnetB7 + CCE w/ Label Smoothing LB: 0.891\n- V78: five-fild EffNetB3 LB: 0.891"},{"metadata":{},"cell_type":"markdown","source":"### Notebooks that I found very useful"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"- [dimitreoliveira's notebook](https://www.kaggle.com/dimitreoliveira/flower-classification-with-tpus-eda-and-baseline)\n\n- [Xhlulu's flower competition notebook](https://www.kaggle.com/xhlulu/flowers-tpu-concise-efficientnet-b7)\n\n- [Getting Started with TPU's](https://www.kaggle.com/jessemostipak/getting-started-tpus-cassava-leaf-disease) -- [TPU Docs](https://www.kaggle.com/docs/tpu) -- [TFRecords Basics](https://www.kaggle.com/ryanholbrook/tfrecords-basics)\n\nDATA Aug\n- [TF Data Augmentation Docs](tensorflow.org/tutorials/images/data_augmentation)\n- [TF Image Docs](https://www.tensorflow.org/api_docs/python/tf/image)"},{"metadata":{},"cell_type":"markdown","source":"### Setting a Seed\n\nDoing this for reproduciblity."},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 15\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    random.seed(seed)    \n\nseed_everything(SEED)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Detecting TPU\n\nThe following cell isnt necessary but is nice to double check that we are going to be using the TPU. If we have everything set up correctly the number of replicas should be 8. If we do not have the TPU turned on we will see a value of 1.\n\nADD Google Cloud Software Development Kit (SDK) to Notebook if using a private dataset (add-ons tab). Pretty much always using public data though."},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Variables\n\n- note: actual file path for data is 'cassava-leaf-disease-tfrecords-center-512x512'"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path('cldtfrecords512x512') \n#GCS_PATH = KaggleDatasets().get_gcs_path('v2cldtfrecordswnoisyremoved') #dataset with some removed\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [512, 512]\nTARGET_SIZE = 512\nCLASSES = ['0', '1', '2', '3', '4']\nNUM_OF_CLASSES = len(CLASSES)\nEPOCHS = 15\nDROPOUT_RATE = 0.4 #would like to implement this \nAUG_BATCH = BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting TFRecords\n\n\nThe number on the end of the tfrecord file corresponds to the number of images in that tfrecord.\n\nExample: 'gs://kds-100c2bc3bab7e1f77f19378980a417f43e62119932994bd622dc7cb4/Id_train01-1427.tfrec' (1427 imgs)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#this function counts number of images in all TFRecords\ndef count_data_items(filenames):\n    n = [int(re.compile(r'-([0-9]*)\\.').search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nALL_TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/*.tfrec')\nNUM_ALL_TRAINING_IMAGES = count_data_items(ALL_TRAINING_FILENAMES)\n\n#reading train metadata\ntrain = pd.read_csv('../input/cldtfrecords512x512/train.csv')\n#print(f'Train samples: {len(train)}')\n\nprint(f'GCS: train images: {NUM_ALL_TRAINING_IMAGES}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train_test_split\n\nWe only use this train_test_split if we plan to train a single model. If we are doing a five-fold split I have included that in the training loop at the bottom of the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINING_FILENAMES, VALIDATION_FILENAMES = train_test_split(\n    ALL_TRAINING_FILENAMES,\n    train_size= 0.90, test_size=0.10,\n    random_state=SEED,\n)\n\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n\nprint(\"Training Images: {}  Validation Image: {}\".format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES))\nprint(\"Training Percent: {:.2f}  Validation Percent: {:.2f}\".format((NUM_TRAINING_IMAGES/NUM_ALL_TRAINING_IMAGES),\n                                                           (NUM_VALIDATION_IMAGES/NUM_ALL_TRAINING_IMAGES)))\n\nSTEPS_PER_EPOCH =  NUM_TRAINING_IMAGES // BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES // BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am printing out the validation TFrecs so that when I use cross-validation model strategies I can ensure no data leakage between train and validation datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"for num, filename in enumerate(VALIDATION_FILENAMES):\n    print(\"File: {}\".format(int(re.compile(r'train([0-9]*)').search(VALIDATION_FILENAMES[num]).group(1))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Functions\n\nThe following functions are how I am reading the data from the TF records. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3) #decoding jpeg-encoded img to uint8 tensor\n    image = tf.cast(image, tf.float32) / 255.0 #cast int val to float so we can normalize pixels\n    ch_1 = (tf.cast(image[:, :, 0], tf.float32)  - 0.4303)/ 0.2142 #TEST\n    ch_2 = (tf.cast(image[:, :, 1], tf.float32)  - 0.4967)/ 0.2191 #TEST\n    ch_3 = (tf.cast(image[:, :, 2], tf.float32)  - 0.3134)/ 0.1954 #TEST\n    image = tf.stack([ch_1, ch_2, ch_3], axis = 2) #TEST\n    image = tf.image.resize(image, [*IMAGE_SIZE]) #precautionary as all imgs should be 512x512\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) #resizing to proper shape\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_tfrecord(example, labeled=True):\n    \"\"\"\n        1. Parse data based on the 'TFREC_FORMAT' map.\n        2. Decode image.\n        3. If 'labeled' returns (image, label) if not (image, name).\n    \"\"\"\n    if labeled:\n        TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string), \n            'target': tf.io.FixedLenFeature([], tf.int64), \n        }\n    else:\n        TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string), \n            'image_name': tf.io.FixedLenFeature([], tf.string), \n        }\n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    image = decode_image(example['image'])\n    if labeled:\n        label_or_name = tf.cast(example['target'], tf.int32)\n        #label_or_name = tf.one_hot(indices, depth, dtype=tf.int32)\n    else:\n        label_or_name =  example['image_name']\n    return image, label_or_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_dataset(filenames, labeled=True, ordered=False):\n    \"\"\"\n        Create a Tensorflow dataset from TFRecords.\n    \"\"\"\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data augmentation\n\nI have a few options for data augmentation here.\n\n* The simple_data_augmenter provides some simple rotation and flipping augmentation.\n\n* The dropout function provides random dropout areas in the image of a specified size and number of dropout areas. \n\n* The transform_one function is similar to the simple_data_augmentor, but provides more variation in terms of structural manipulation. Random rotations/zooms/shears. \n\n* The transform_two function provides cut_mix and mix_up transformations on the images. \n\nNOTE: I will not use all the augmentation options when training the model, but I have included them all here for testing."},{"metadata":{},"cell_type":"markdown","source":"#### simple_data_augmenter"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_data_augmenter(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    \n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32) #random int and rotating img based on result\n    \n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) # rotate 270ยบ\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) # rotate 180ยบ\n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) # rotate 90ยบ\n    \n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_flip_left_right(image)\n    \n    \n    return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropout\n\nTrying out another augmentation technique here called Coarse Dropout which randomly cuts out parts of the image. This is another technique to prevent overfitting.\n\nTo perform this dropout function just call it on the return statement of the transform function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dropout(image, DIM=512, PROBABILITY = 0.75, CT = 5, SZ = 0.1):\n    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image with CT squares of side size SZ*DIM removed\n    \n    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n    if (P==0)|(CT==0)|(SZ==0): return image\n    \n    for k in range(CT):\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        # COMPUTE SQUARE \n        WIDTH = tf.cast( SZ*DIM,tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # DROPOUT IMAGE\n        one = image[ya:yb,0:xa,:]\n        two = tf.zeros([yb-ya,xb-xa,3]) \n        three = image[ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM,:,:]],axis=0)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR \n    image = tf.reshape(image,[DIM,DIM,3])\n    return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Transform_one\n\nThanks to Chris Deotte for sharing this next data Augmentation Function. [link](https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96/comments)\n\n- alter the parameters in the transform function in order to tune the data augmentation parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_one(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 45. * tf.random.normal([1],dtype='float32')\n    shr = 0. * tf.random.normal([1],dtype='float32') #need to test with and without shear\n    h_zoom = 1.1 #+ tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.1 #+ tf.random.normal([1],dtype='float32')/10.\n    h_shift = 20. * tf.random.normal([1],dtype='float32') \n    w_shift = 20. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    \n    #PERFORMING DROPOUT ON RETURN STATEMENT\n        \n    return tf.reshape(d,[DIM,DIM,3]),label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### transform_two (Cutmix + Mixup)\n\nCutmix basically adds pieces of other images to an image in a similar way to the dropout augmentation technique. \n\nMix-up basically adds a different translucent image ontop of an image. \n\nNote there is also a one-hot encode function that encodes the labels "},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot(image,label):\n    CLASSES = NUM_OF_CLASSES\n    return image,tf.one_hot(label,CLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cutmix(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = NUM_OF_CLASSES\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.int32)\n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        # CHOOSE RANDOM LOCATION\n        x = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        y = tf.cast( tf.random.uniform([],0,DIM),tf.int32)\n        b = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n        WIDTH = tf.cast( DIM * tf.math.sqrt(1-b),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]\n        middle = tf.concat([one,two,three],axis=1)\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mixup(image, label, PROBABILITY = 1.0):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = IMAGE_SIZE[0]\n    CLASSES = NUM_OF_CLASSES\n    \n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        # DO MIXUP WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast( tf.random.uniform([],0,1)<=PROBABILITY, tf.float32)\n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([],0,AUG_BATCH),tf.int32)\n        a = tf.random.uniform([],0,1)*P # this is beta dist with alpha=1.0\n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        # MAKE CUTMIX LABEL\n        if len(label.shape)==1:\n            lab1 = tf.one_hot(label[j],CLASSES)\n            lab2 = tf.one_hot(label[k],CLASSES)\n        else:\n            lab1 = label[j,]\n            lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image2,label2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_two(image,label):\n    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n    DIM = IMAGE_SIZE[0]\n    CLASSES = NUM_OF_CLASSES\n    SWITCH = 0.5\n    CUTMIX_PROB = 0\n    MIXUP_PROB = 0.666\n    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n    image3, label3 = mixup(image, label, MIXUP_PROB)\n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n        labs.append(P*label2[j,]+(1-P)*label3[j,])\n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n    return image4,label4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Choose Augmentation Function\n\nHere I just define the final augmentation technique that I selected. This enables me to make make variations of the augmentation function without having to change the order in which the functions are defined. "},{"metadata":{},"cell_type":"markdown","source":"### Loading Data\n\nMake sure to have the correct augmenter selected in the next function. \n\n- simple_data_augmenter - quicker but more simple augmenter\n- transform_one - Chris Deottes Augmenter\n- transform_two - Cutmix and Mixup augmentation option"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_dataset(dataset, do_aug=True, do_onehot=False):\n    dataset = dataset.map(simple_data_augmenter, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.batch(AUG_BATCH)\n    if do_onehot: dataset = dataset.map(onehot, num_parallel_calls=AUTOTUNE) #onehot happens in do_aug as well \n    if do_aug: dataset = dataset.map(transform_two, num_parallel_calls=AUTOTUNE) # note we put AFTER batching\n    dataset = dataset.unbatch()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(dataset, do_onehot=True):\n    dataset = dataset.batch(BATCH_SIZE)\n    if do_onehot: dataset = dataset.map(onehot, num_parallel_calls=AUTOTUNE) # we must use one hot like augmented train data\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Transformations\n\nVisualizing transformations function, comment out during actual model trainig to save RAM.\n\nvisualize_transformations_one is for all augmentations except cutmix and mixup which need to be visualized with visualize_transformations_two."},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_transformations_one():\n    row = 2; col = 4;\n    all_elements = get_training_dataset().unbatch()\n    one_element = tf.data.Dataset.from_tensors(next(iter(all_elements)) )\n    augmented_element = one_element.repeat().map(transform_one).batch(row*col)\n    \n\n    for (img,label) in augmented_element:\n        plt.figure(figsize=(15,int(15*row/col)))\n        for j in range(row*col):\n            plt.subplot(row,col,j+1)\n            plt.axis('off')\n            plt.imshow(img[j,])\n        plt.show()\n        break\n        \n#visualize_transformations()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_transformations_two():\n    row = 6; col = 4;\n    row = min(row,AUG_BATCH//col)\n    all_elements = get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=False).unbatch()\n    augmented_element = all_elements.repeat().batch(AUG_BATCH).map(transform_two)\n\n    for (img,label) in augmented_element:\n        plt.figure(figsize=(15,int(15*row/col)))\n        for j in range(row*col):\n            plt.subplot(row,col,j+1)\n            plt.axis('off')\n            plt.imshow(img[j,])\n        plt.show()\n        break\n\n#visualize_transformations_two()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Model\n\nAlso note that we're using sparse_categorical_crossentropy as our loss function, because we did not one-hot encode our labels.\n\n--\n\nThere is a very useful repository on github where I am trying the 'noisy-student' starting weights for the EfficientNet models. Check old notebook version for direct download, but I used the same thing from a dataset so I can still load this in when internet is turned off. \n\nGithub Repo -> [Link](https://github.com/qubvel/efficientnet)"},{"metadata":{},"cell_type":"markdown","source":"#### Custom Loss Function? \n\n- Tried Symmetrical Categorical Crossentropy but CC with label smoothing gave me better results."},{"metadata":{"trusted":true},"cell_type":"code","source":"CatCross_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, \n                                               label_smoothing=0.1, \n                                               name='categorical_crossentropy' ) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    model = models.Sequential()\n    \n    model.add(efn.EfficientNetB3(include_top = False, weights = 'noisy-student', \n                              input_shape = (TARGET_SIZE, TARGET_SIZE, 3)))\n    \n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(DROPOUT_RATE))\n    model.add(layers.Dense(5, activation = \"softmax\"))# 5 is the dimensionality of the output space \"5 options\"\n\n    model.compile(optimizer = 'adam',\n                  loss = CatCross_loss, #use sparse_catgeorical_crossentropy of if not one_hot_encoding\n                  metrics = [\"acc\"]) #try sparse_categorical_accuracy here\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to ensure that our model is trained on the TPU, we build it using strategy.scope()"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = create_model()\n\nmodel.save('./EffNet_untrained_TPU_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Callbacks\n\n\n#### Custom Learning Rate Schedulers\n\n1. The first option updates the Learning Rate at the end of every epoch.\n2. The second custom LR scheduler updates the learning rate every step."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Learning rate schedule for TPU, GPU and CPU.\n# # Using an LR ramp up because fine-tuning a pre-trained model.\n# # Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.00001\nLR_MAX = 0.0001\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 3\nLR_SUSTAIN_EPOCHS = 0\nWARMUP_STEPS = LR_RAMPUP_EPOCHS * (NUM_TRAINING_IMAGES//BATCH_SIZE)\nTOTAL_STEPS = EPOCHS * (NUM_TRAINING_IMAGES//BATCH_SIZE)\nLR_EXP_DECAY = 0.85\n\n# def lrfn(epoch):\n#     if epoch < LR_RAMPUP_EPOCHS:\n#         lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n#     elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n#         lr = LR_MAX\n#     else:\n#         #cosine decay\n#         progress = (epoch - LR_RAMPUP_EPOCHS) / (EPOCHS - LR_RAMPUP_EPOCHS)\n#         lr = LR_MAX * (0.5 * (1.0 + tf.math.cos(np.pi * ((1.0 * progress) % 1.0))))\n        \n#         #exponential decay\n#         #lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n#     return lr\n\n    \n# #setting verbose=True allows us to see LR in model training\n# lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\n# #visualizing the learning rate schedule\n# rng = [i for i in range(EPOCHS)]\n# y = [lrfn(x) for x in rng]\n\n# sns.set(style='whitegrid')\n# plt.figure(figsize=(13, 5))\n# plt.xlabel('Epoch')\n# plt.ylabel('Learning Rate')\n# plt.plot(rng, y)\n# print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This second learning rate function updates every step, and should be a much better alternative. \n\nYou can include print statements in the on_train_batch_beging function to ensure the training rate is updating in the way you want to.\n\nFor more info on custom callbacks, check out Tensorflow's guide here --> [Link](https://www.tensorflow.org/guide/keras/custom_callback)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lrfn_step(step):\n    if step < WARMUP_STEPS:\n        lr = (LR_MAX - LR_START) / WARMUP_STEPS * step + LR_START\n    else:\n        progress = (step - WARMUP_STEPS) / (TOTAL_STEPS - WARMUP_STEPS)\n        lr = LR_MAX * (0.5 * (1.0 + tf.math.cos(np.pi * ((1.0 * progress) % 1.0))))\n    return lr\n\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, schedule):\n        super(CustomCallback, self).__init__()\n        self.schedule = schedule\n        self.epoch = 0\n        \n    def on_train_batch_begin(self, batch, logs=None):\n        actual_step = (self.epoch*STEPS_PER_EPOCH) + batch\n        # Call schedule function to get the scheduled learning rate.\n        scheduled_lr = self.schedule(actual_step)\n        # Set the value back to the optimizer before this epoch starts\n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        if batch == 0:\n            print(\"--Learning Rate: {:.6f} --\".format(scheduled_lr))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        self.epoch+=1\n        \n    \n\n#visualizing Learning Rate Schedule\nrng = [i for i in range(TOTAL_STEPS)]\ny = [lrfn_step(tf.cast(x, tf.float32)) for x in rng]\n\nsns.set(style='whitegrid')\nfig, ax = plt.subplots(figsize=(20, 6))\nplt.plot(rng, y)\n\nprint(f'{TOTAL_STEPS} total steps and {NUM_TRAINING_IMAGES//BATCH_SIZE} steps per epoch')\nprint(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Early_Stopping + ModelCheckpoint\n\nSince the data is quite noisy, I am monitoring the validation accuracy rather than the validation loss for saving the best model weights. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#note if you monitor=val_loss then mode=min, if monitor=val_sparse_categorical_accuracy then mode=max\nmodel_save = ModelCheckpoint('./Effnet_TPU_Model_best_weights.h5', \n                             save_best_only = True, \n                             save_weights_only = True,\n                             monitor = 'val_acc', \n                             mode = 'max',\n                             verbose = 1)\n\nmy_early_stopper = EarlyStopping(monitor = 'val_acc', min_delta = 0.001, \n                           patience = 3, mode = 'max', verbose = 1,\n                           restore_best_weights = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the Model (single-fold)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# history = model.fit(x=get_training_dataset(load_dataset(TRAINING_FILENAMES)),\n#                     epochs=EPOCHS,\n#                     steps_per_epoch = STEPS_PER_EPOCH,\n#                     validation_steps=VALID_STEPS,\n#                     validation_data=get_validation_dataset(load_dataset(VALIDATION_FILENAMES)),\n#                     callbacks = [CustomCallback(lrfn_step), model_save, my_early_stopper],\n#                     verbose=1,\n#                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.save_weights('./Effnet_TPU_Model_final_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Model History"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(13, 5))\n# plt.plot(history.history['loss'])\n# plt.plot(history.history['val_loss'])\n# plt.title(\"Model Loss\")\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend(['Train', 'Test'])\n# plt.ylim(ymax = 2, ymin = 0)\n# plt.grid()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(13, 5))\n# plt.plot(history.history['acc'])\n# plt.plot(history.history['val_acc'])\n# plt.title('Model Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend(['Train','Test'])\n# plt.grid()\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the models (5-fold)\n\nHere I am tryng to ensemble five models and find the optimal weight to give each model prediction. I started off by defining a new early stopping callback which restores the best model weights at the end of training."},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining a second early_stopper that restores model best weights\nfold_early_stopper = EarlyStopping(monitor = 'val_acc', min_delta = 0.001, \n                           patience = 5, mode = 'max', verbose = 1,\n                           restore_best_weights = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions in a seperate notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_ROUND = 0\nMODELS = []\nMODELS_HISTORY = []\nMODELS_PREDICTIONS = []\nkf = KFold(n_splits=5)\nmodel = 0\n\nrandom.shuffle(TRAINING_FILENAMES) #seeded at top of notebook\n\ndef lrfn_step(step):\n    if step < WARMUP_STEPS:\n        lr = (LR_MAX - LR_START) / WARMUP_STEPS * step + LR_START\n    else:\n        progress = (step - WARMUP_STEPS) / (TOTAL_STEPS - WARMUP_STEPS)\n        lr = LR_MAX * (0.5 * (1.0 + tf.math.cos(np.pi * ((1.0 * progress) % 1.0))))\n    return lr\n\nclass CustomCallback(keras.callbacks.Callback):\n    def __init__(self, schedule):\n        super(CustomCallback, self).__init__()\n        self.schedule = schedule\n        self.epoch = 0\n\n    def on_train_batch_begin(self, batch, logs=None):\n        actual_step = (self.epoch*STEPS_PER_EPOCH) + batch\n        # Call schedule function to get the scheduled learning rate.\n        scheduled_lr = self.schedule(actual_step)\n        # Set the value back to the optimizer before this epoch starts\n        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n        if batch == 0:\n            print(\"--Learning Rate: {:.6f} --\".format(scheduled_lr))\n\n    def on_epoch_end(self, epoch, logs=None):\n        self.epoch+=1\n\nwith strategy.scope():\n    for train_index, test_index in kf.split(ALL_TRAINING_FILENAMES):\n\n        TRAINING_FILENAMES_SPLIT = []\n        VALIDATION_FILENAMES_SPLIT = []\n\n        for i in train_index:\n            TRAINING_FILENAMES_SPLIT.append(ALL_TRAINING_FILENAMES[i])\n        for i in test_index:\n            VALIDATION_FILENAMES_SPLIT.append(ALL_TRAINING_FILENAMES[i])\n        \n        #counting number of imgs in each tfrecord + setting fit parameters\n        NUM_VALIDATION_IMAGES_SPLIT = count_data_items(VALIDATION_FILENAMES_SPLIT)\n        NUM_TRAINING_IMAGES_SPLIT = count_data_items(TRAINING_FILENAMES_SPLIT)\n\n        STEPS_PER_EPOCH =  NUM_TRAINING_IMAGES_SPLIT // BATCH_SIZE\n        VALID_STEPS = NUM_VALIDATION_IMAGES_SPLIT // BATCH_SIZE\n        WARMUP_STEPS = LR_RAMPUP_EPOCHS * (STEPS_PER_EPOCH)\n        TOTAL_STEPS = EPOCHS * (STEPS_PER_EPOCH)\n        \n        #fitting each model fold\n        print(\"TRAINING MODEL: {}\".format(TRAIN_ROUND))\n        \n        MODELS.append(create_model())\n\n        MODELS[TRAIN_ROUND].fit(x=get_training_dataset(load_dataset(TRAINING_FILENAMES_SPLIT)),\n                                    epochs = EPOCHS,\n                                    steps_per_epoch = STEPS_PER_EPOCH,\n                                    validation_steps = VALID_STEPS,\n                                    validation_data=get_validation_dataset(load_dataset(VALIDATION_FILENAMES_SPLIT)),\n                                    callbacks = [CustomCallback(lrfn_step), fold_early_stopper],\n                                    verbose=1,\n                                   )\n        MODELS[TRAIN_ROUND].save_weights('Model_{}_best_weights.h5'.format(TRAIN_ROUND))\n        TRAIN_ROUND+=1\n        model+=1","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}