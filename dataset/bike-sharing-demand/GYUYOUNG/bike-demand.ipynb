{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike demand predict"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = pd.read_csv(\"../input/bike-sharing-demand/train.csv\")\ntest = pd.read_csv(\"../input/bike-sharing-demand/test.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Range of variable in train and test is similar. So for now, i don't remove outlier\n\n<br><br>\n\n## Create variables and visualization\ncreate columns from datetime"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"for df in [train, test]:\n    df[\"datetime\"] =  pd.DatetimeIndex(df[\"datetime\"])\n    df[\"hour\"] = [x.hour for x in df[\"datetime\"]]\n    df[\"weekday\"] = [x.dayofweek for x in df[\"datetime\"]]\n    df[\"month\"] = [x.month for x in df[\"datetime\"]]\n    df[\"year\"] = [x.year for x in df[\"datetime\"]]\n    df['year_season'] = df['year'].astype(str) + \"_\" +  df['season'].astype(str) \n    df[\"year\"] = df[\"year\"].map({2011:1, 2012:0})\n    df.drop('datetime',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See variables's distribution by distplot and countplot"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(15,10))\nplt.suptitle('variables distribution')\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\nfor i, col in enumerate(train.columns[:11]):\n    plt.subplot(3,4,i+1)\n    if str(train[col].dtypes)[:3]=='int':\n        if len(train[col].unique()) > 5:\n            sns.distplot(train[col])\n        else:\n            sns.countplot(train[col])\n    else:\n        sns.distplot(train[col])\n    plt.ylabel(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"see relation of categorical predictors and outcomes by countplot"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(13,20))\nplt.suptitle('casual vs registered vs count')\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\ncol_list = [\"season\",\"holiday\",\"workingday\",\"weather\",\"year\",\"year_season\",\"month\",\"weekday\",\"hour\"]\ncount_list = [\"casual\",\"registered\",\"count\"]\n\nfor i, col in enumerate(col_list):\n    for j, con in enumerate(count_list):\n        plt.subplot(9,3,3*i+j+1)\n        sns.barplot(train[col],train[con])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In count of holiday, workingday and weekday, there is no difference depending on categories.\nbut in registered and casual, it depend of the categories. So need to look at this part differently."},{"metadata":{},"cell_type":"markdown","source":"see relationship between weekday and each count by workingday and holiday"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.subplot(121)\nsns.barplot(x=\"weekday\", y=\"casual\", hue=\"workingday\", data=train)\nplt.subplot(122)\nsns.barplot(x=\"weekday\", y=\"registered\", hue=\"workingday\", data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no holiday in Tuesday and Thursday.\nAnd there is differences when Monday, Wednesday, and Friday."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"see relationship between hour and each count by workingday and holiday"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(18,11))\nplt.subplot(221)\nsns.pointplot(x=\"hour\", y=\"casual\", hue=\"workingday\", data=train)\nplt.subplot(222)\nsns.pointplot(x=\"hour\", y=\"casual\", hue=\"holiday\", data=train)\nplt.subplot(223)\nsns.pointplot(x=\"hour\", y=\"registered\", hue=\"workingday\", data=train)\nplt.subplot(224)\nsns.pointplot(x=\"hour\", y=\"registered\", hue=\"holiday\", data=train)\n# train.pivot_table(index=\"hour\", columns=\"workingday\", aggfunc=\"size\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The number of registered and casual according to workingday and holiday show the opposite pattern.\nAnd there are differences in the number of registered according to workingday at the closing hour and the office-going hour.\nSo many registered is can be expected to workers."},{"metadata":{},"cell_type":"markdown","source":"## correlation"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(11,11))\nsns.heatmap(train.corr(),annot=True,cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"temp and atemp have high correlation and register and have too.\nAnd windspeed and outcomes have low correlation(<=0.1)\nSee scatterplot of temp and atemp."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"for i, df in enumerate([train,test]):\n    plt.subplot(1,2,i+1)\n    sns.scatterplot(x = 'temp', y = 'atemp',data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In train data, there is strange pattern, but not in test.\nIt seems to be haved wrong value in atemp.\nSo based on correlation and scatterplot, judged to remove atemp"},{"metadata":{},"cell_type":"markdown","source":"Based on the above results, make new variable."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df_list = {\"train\":None, \"test\" : None}\nfor name, df in zip(df_list.keys(),[train, test]):\n    df['windspeed'] = np.log(df['windspeed']+1)\n    df[\"weekday_working\"] = df[\"weekday\"]*df[\"workingday\"]\n    df[\"weekday_holiday\"] = df[\"weekday\"]*df[\"holiday\"]\n    df['casual_workhour'] = df[['hour', 'workingday']].apply(lambda x: int(x['workingday'] == 0 and 10 <= x['hour'] <= 19), axis=1)\n    df['casual_holi_hour'] = df[['hour', 'holiday']].apply(lambda x: int(x['holiday'] == 1 and 9 <= x['hour'] <= 22), axis=1)\n    df['register_workhour'] = df[['hour', 'workingday']].apply(\n      lambda x:int((x['workingday'] == 1 and (6 <= x['hour'] <= 8 or 17 <= x['hour'] <= 20))\n        or (x['workingday'] == 0 and 10 <= x['hour'] <= 15)), axis=1)\n    df['register_holi_hour'] = df[['hour', 'holiday']].apply(\n      lambda x:int(x['holiday'] == 0 and (7 <= x['hour'] <= 8 or 17 <= x['hour'] <= 18)), axis=1)\n    df.drop('atemp',axis=1,inplace=True)\nby_season = train.groupby('year_season')[['count']].median()\nby_season.columns = ['count_season']\ntrain1 = train.join(by_season, on='year_season').drop('year_season',axis=1)\ntest1 = test.join(by_season, on='year_season').drop('year_season',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Divide predictors and outcomes. And take logging outcomes to normalize."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny_list = [\"casual\",\"registered\",\"count\"]\ntrain_x = train1[[col for col in train1.columns if col not in ['casual','registered', 'count']]]\ntrain_y = np.log(train1[y_list]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n\n#### - 1. lightgbm + cross validation\n\nUse lightgbm model, and use cross-validation to prevent overfitting"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 123)\nrms1,rms2 = [],[]\nmodels1,models2 = [], []\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train1)) :\n    x_train, y_train = train_x.ix[trn_idx], train_y.ix[trn_idx] \n    x_val, y_val = train_x.ix[val_idx], train_y.ix[val_idx]\n    \n    lgb_param = {'boosting_type':'gbdt',\n             'num_leaves': 45,\n             'max_depth': 30,\n            'learning_rate': 0.01, \n            'bagging_fraction' : 0.9,\n            'bagging_freq': 20,\n            'colsample_bytree': 0.9,\n             'metric': 'rmse',\n            'min_child_weight': 1,\n            'min_child_samples': 10,\n             'zero_as_missing': True,\n            'objective': 'regression',\n            }\n    train_set1 = lgb.Dataset(x_train, y_train[\"registered\"], silent=False)\n    valid_set1 = lgb.Dataset(x_val, y_val[\"registered\"], silent=False)\n    lgb_model1 = lgb.train(params = lgb_param, train_set = train_set1 , num_boost_round=5000, early_stopping_rounds=100,verbose_eval=500, valid_sets=valid_set1)\n    train_set2 = lgb.Dataset(x_train, y_train[\"casual\"], silent=False)\n    valid_set2 = lgb.Dataset(x_val, y_val[\"casual\"], silent=False)\n    lgb_model2 = lgb.train(params = lgb_param, train_set = train_set2 , num_boost_round=5000, early_stopping_rounds=100,verbose_eval=500, valid_sets=valid_set2)\n    models1.append(lgb_model1)\n    models2.append(lgb_model2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"see feature importance"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': lgb_model1.feature_importance()})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (15,15))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"preds = []\nfor model in models1:\n    regi_pred = model.predict(test1)\n    preds.append(regi_pred)\nfin_casual = np.mean(preds, axis=0)\n\npreds = []\nfor model in models2:\n    casual_pred = model.predict(test1)\n    preds.append(casual_pred)\nfin_regi = np.mean(preds, axis=0)\ncount_pred1 = np.exp(fin_casual) + np.exp(fin_regi) - 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### - 2. lgbmRegressor + crossvalidation + Bayesian optimization"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.model_selection import KFold \nfrom sklearn.metrics import mean_squared_error\ndef lgb_cv(num_leaves, learning_rate, n_estimators, reg_alpha, reg_lambda, min_split_gain, min_child_weight,min_child_samples, colsample_bytree, x_data=None, y_data=None, n_splits=5, output='score'):\n    score = 0\n    kf = KFold(n_splits=n_splits)\n    models = []\n    for train_index, valid_index in kf.split(x_data):\n        x_train, y_train = x_data.iloc[train_index], y_data[train_index]\n        x_valid, y_valid = x_data.iloc[valid_index], y_data[valid_index]\n        \n        model = lgb.LGBMRegressor(\n            num_leaves = int(num_leaves), \n            learning_rate = learning_rate, \n            n_estimators = int(n_estimators), \n            reg_alpha = reg_alpha, \n            reg_lambda = reg_lambda,\n            min_split_gain= min_split_gain,\n            min_child_weight = min_child_weight,\n            min_child_samples = int(min_child_samples),\n            colsample_bytree = np.clip(colsample_bytree, 0, 1), \n        )\n        \n        model.fit(x_train, y_train)\n        models.append(model)\n        \n        pred = model.predict(x_valid)\n        true = y_valid\n        score -= mean_squared_error(true, pred)/n_splits\n    \n    if output == 'score':\n        return score\n    if output == 'model':\n        return models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from functools import partial \nfrom bayes_opt import BayesianOptimization\nfunc_fixed1 = partial(lgb_cv, x_data=train_x, y_data=train_y[\"casual\"], n_splits=5, output='score')\nfunc_fixed2 = partial(lgb_cv, x_data=train_x, y_data=train_y[\"registered\"], n_splits=5, output='score')\nlgbBO = BayesianOptimization(\n    func_fixed1, \n    {\n        'num_leaves': (30, 100),    \n        'learning_rate': (0.001, 0.015),  \n        'n_estimators': (1000, 3000),                        \n        'reg_alpha': (0.0001, 1),       \n        'reg_lambda': (0.0001, 1), \n        'min_split_gain' : (0.001, 0.1),\n        'min_child_weight' : (0.001, 0.1),\n        'min_child_samples' : (10,25),\n        'colsample_bytree': (0.85, 1.0),\n    }, \n    random_state=4321            \n)\nlgbBO.maximize(init_points=5, n_iter=20)\nlgbB1 = BayesianOptimization(\n    func_fixed2, \n    {\n        'num_leaves': (30, 100),    \n        'learning_rate': (0.001, 0.015),  \n        'n_estimators': (1000, 3000),                        \n        'reg_alpha': (0.0001, 1),       \n        'reg_lambda': (0.0001, 1), \n        'min_split_gain' : (0.001, 0.1),\n        'min_child_weight' : (0.001, 0.1),\n        'min_child_samples' : (10,25),\n        'colsample_bytree': (0.85, 1.0),\n    }, \n    random_state=4321            \n)\nlgbB1.maximize(init_points=5, n_iter=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"params1 = lgbBO.max['params']\nparams2 = lgbB1.max['params']\nlgb_models1 = lgb_cv(\n    params1['num_leaves'], \n    params1['learning_rate'], \n    params1['n_estimators'], \n    params1['reg_alpha'], \n    params1['reg_lambda'], \n    params1['min_split_gain'], \n    params1['min_child_weight'],\n    params1['min_child_samples'],\n    params1['colsample_bytree'],\n    x_data=train_x, y_data=train_y[\"casual\"], n_splits=5, output='model')\nlgb_models2 = lgb_cv(\n    params2['num_leaves'], \n    params2['learning_rate'], \n    params2['n_estimators'], \n    params2['reg_alpha'], \n    params2['reg_lambda'], \n    params2['min_split_gain'], \n    params2['min_child_weight'],\n    params2['min_child_samples'],\n    params2['colsample_bytree'],\n    x_data=train_x, y_data=train_y[\"registered\"], n_splits=5, output='model')\npreds = []\nfor model in lgb_models1:\n    pred = model.predict(test1)\n    preds.append(pred)\ncasual_pred = np.mean(preds, axis=0)\npreds = []\nfor model in lgb_models2:\n    pred = model.predict(test1)\n    preds.append(pred)\nregistered_pred = np.mean(preds, axis=0)\ncount_pred2 = np.exp(casual_pred) + np.exp(registered_pred) - 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### - 3. randomforest and gradientboostingregressor"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\npreds = {}\nregs = {\"gbdt\": GradientBoostingRegressor(random_state=0),\n        \"rf\": RandomForestRegressor(random_state=0, n_jobs=-1)}\nfor name, reg in regs.items():\n    if name == 'gbdt':\n        reg.set_params(n_estimators=1500, min_samples_leaf=6)\n    elif name == 'rf':\n        reg.set_params(n_estimators=1500, min_samples_leaf=2)\n    reg.fit(train_x, train_y['casual'])\n    pred_casual = reg.predict(test1)\n    pred_casual = np.exp(pred_casual) - 1\n    pred_casual[pred_casual < 0] = 0\n    if name == 'gbdt':\n        reg.set_params(n_estimators=1500, min_samples_leaf=6)\n    elif name == 'rf':\n        reg.set_params(n_estimators=1500, min_samples_leaf=2)\n    reg.fit(train_x, train_y['registered'])\n    pred_registered = reg.predict(test1)\n    pred_registered = np.exp(pred_registered) - 1\n    pred_registered[pred_registered < 0] = 0\n    preds[name] = pred_casual + pred_registered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': reg.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (15,15))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"pred_mean = (count_pred1 + count_pred2 + preds['gbdt'] + preds['rf'])/4\nsample = pd.read_csv(\"../input/bike-sharing-demand/sampleSubmission.csv\")\nsample[\"count\"] = pred_mean\nsample.to_csv(\"sample.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Result rmsle is 0.38081"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}