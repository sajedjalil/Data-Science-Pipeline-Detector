{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Basics\nimport numpy as np \nimport pandas as pd \nimport calendar\nfrom datetime import datetime\n\n#Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n#Models\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n#Misc\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV,KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score,make_scorer\nfrom sklearn.pipeline import make_pipeline\n\n#Stats\nfrom scipy.stats import skew,norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\n\n#Location path\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Reading data set as train & test\ntrain=pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\ntest=pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Essentials","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"****Overview****\n\nBike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. Currently, there are over 500 bike-sharing programs around the world.\n\n****Data Fields****\n* datetime - hourly date + timestamp\n* season - 1 = spring, 2 = summer, 3 = fall, 4 = winter\n* holiday - whether the day is considered a holiday\n* workingday - whether the day is neither a weekend nor holiday\n* weather -\n    1: Clear, Few clouds, Partly cloudy, Partly cloudy\n    2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n    3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n* temp - temperature in Celsius\n* atemp - \"feels like\" temperature in Celsius\n* humidity - relative humidity\n* windspeed - wind speed\n* casual - number of non-registered user rentals initiated\n* registered - number of registered user rentals initiated\n* count - number of total rentals (Dependent Variable)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **EDA**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The goal is try to predict dependent variable is that 'count'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's look closer to our target, 'count'\nsns.set_style(\"white\")\nsns.set_color_codes(palette=\"deep\")\nf, ax=plt.subplots(figsize=(4,5))\n\n#Lets check the distribution of it\nsns.distplot(train['count'],color='b');\nax.xaxis.grid(False)\nax.set(ylabel='Frequency')\nax.set(xlabel='count')\nax.set(title='Bike Count Distribution')\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets look at Skewness and Kurtosis\nprint(\"Skewness: %f\" % train['count'].skew())\nprint(\"Kurtosis: %f\" % train['count'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that positive (right) skewness and it has kind of has Kurtosis. In later stages, we will try to solve this issue.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Detailed investigation on features**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding first numerical features - p1\nnumeric_dtypes=train.dtypes[train.dtypes != \"object\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding numerical features-2\nnumeric_dtypes=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric=[]\n\nfor i in train.columns:\n    if train[i].dtype in numeric_dtypes:\n        if i in [ 'count']:\n            pass\n        else:\n            numeric.append(i)\n\n#Visualization more features\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 30))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(train[numeric]), 1):\n    if(feature=='count'):\n        break\n    plt.subplot(len(list(numeric)), 2, i)\n    sns.scatterplot(x=feature, y='count', hue='count', palette='Blues', data=train)\n\n    plt.xlabel('{}'.format(feature), size=12,labelpad=12.5)\n    plt.ylabel('count', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation matrix with features\ncorr=train.corr()\nplt.subplots(figsize=(10,12))\nsns.heatmap(corr,vmax=0.8,cmap=\"BuPu\",square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_vis2=pd.concat([train['count'],train['atemp']],axis=1)\nf, ax=plt.subplots(figsize=(18,6))\nfig=sns.boxplot(x=train['atemp'],y=train['count'],data=df_vis2)\nfig.axis(ymin=0);\nplt.xticks(rotation=60);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems that riders love hot temperature like 30-40 Celcius Range","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_vis=pd.concat([train['count'],train['season']],axis=1)\nf, ax=plt.subplots(figsize=(18,6))\nfig=sns.boxplot(x=train['season'],y=train['count'],data=df_vis)\nfig.axis(ymin=0);\nplt.xticks(rotation=60);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summer & Fall Cycling Times..","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_vis3=pd.concat([train['count'],train['windspeed']],axis=1)\nf, ax=plt.subplots(figsize=(18,6))\nfig=sns.boxplot(x=train['windspeed'],y=train['count'],data=df_vis3)\nfig.axis(ymin=0);\nplt.xticks(rotation=60);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool windspeed is okey for them like 11-19 range","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets split datetime into meaningful date parts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split features and labels\ntrain_labels = train['count'].reset_index(drop=True)\ntrain_features = train.drop(['count'], axis=1)\ntest_features = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unnecessary variables omitted\nall_features[\"year\"] = [t.year for t in pd.DatetimeIndex(all_features.datetime)]\n\nall_features[\"date\"] = all_features.datetime.apply(lambda x : x.split()[0])\nall_features[\"hour\"] = all_features.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\nall_features[\"weekday\"] = all_features.date.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\nall_features[\"month\"] = all_features.date.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.drop([\"datetime\",\"date\",\"hour\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you remember, our target 'count' was skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# log(1+x) transform\ntrain[\"count\"] = np.log1p(train[\"count\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['count'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['count'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution & you can see new-black version\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"count\")\nax.set(title=\"Count distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers - ?\nfor col in all_features.dtypes[all_features.dtypes != \"object\"].index:\n    percentiles = all_features[col].quantile([0.01,0.99]).values\n    all_features[col] = np.clip(all_features[col], percentiles[0], percentiles[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data - why count still come out? - NO :)\nTotal = all_features.isnull().sum().sort_values(ascending=False)\nPercent = (all_features.isnull().sum()/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, Percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Skewed Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetch all numeric features\nnumeric_dtypes=['int64','float64']\nnumeric=[]\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's normalize skewed features\nfor i in skew_index:\n    all_features[i]=boxcox1p(all_features[i],boxcox_normmax(all_features[i]+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_features[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find new version of skewed numerical features- 2 variables stayed same but why ? \nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's create some squared/log features between numeric features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fetch all numeric features\nnumeric_dtypes=['int64','float64']\nlog_features=[]\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        log_features.append(i)\nprint(log_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\nlog_features=['atemp']\nall_features = logs(all_features, log_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['weather', 'atemp', ]\nall_features = squares(all_features, squared_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Â Encode numerical features**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to convert categorical variables to numeric ones because lots of ML model can not recognize categorical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Be careful about row numbers should be larger than column numbers - log & squared transformations can cause this situation\nall_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Karma turns to Order**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is here is a shape difference problem? \n\nX = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Investigation starts..**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Key features of the model training process:\n\n* Cross Validation: Using 10-fold cross-validation\n* Models: On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n* Results: We will see it in a chart","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Cross Validation & Error Metrics**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setup cross validation folds\nkf = KFold(n_splits=10, random_state=42, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" **Setup Models**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=200,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=200,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=200,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Cross Validation Scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Fit the Models**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(train_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('svr')\nsvr_model_full_data = svr.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ridge')\nridge_model_full_data = ridge.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('rf')\nrf_model_full_data = rf.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('gbr')\ngbr_model_full_data = gbr.fit(X, train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blend the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# RANDOM SELECTION - Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.15 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get rf rmsle\nblended_score_rf = rmsle(train_labels, rf_model_full_data.predict(X))\nscores['blended'] = (blended_score_rf, 0)\nprint('RMSLE score on train data:')\nprint(blended_score_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get ridge rmsle\nblended_score_ridge = rmsle(train_labels, ridge_model_full_data.predict(X))\nscores['blended'] = (blended_score_ridge, 0)\nprint('RMSLE score on train data:')\nprint(blended_score_ridge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get final precitions from the blended model\nblended_score_rf = rmsle(train_labels, rf_model_full_data.predict(X))\nscores_rf['blended'] = (blended_score_rf, 0)\nprint('RMSLE score on train data:')\nprint(blended_score_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the best ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n \nplt.title('Scores of Models', size=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find the best - part2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# X & train_labels came from:\n    #X = all_features.iloc[:len(train_labels), :]\n    #X_test = all_features.iloc[len(train_labels):, :]\n    #X.shape, train_labels.shape, X_test.shape\nx_train, x_test, y_train, y_test=train_test_split(X,train_labels,test_size=0.25,random_state=42)\nridge_model=Ridge(alpha=0.1).fit(x_train,y_train)\ny_pred=ridge_model.predict(x_train)\nrmse=np.sqrt(mean_squared_error(y_train,y_pred))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(h, y): \n    \"\"\"\n    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n    \n    Args:\n        h - numpy array containing predictions with shape (n_samples, n_targets)\n        y - numpy array containing targets with shape (n_samples, n_targets)\n    \"\"\"\n    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmsle(y_train,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Credits\n\n* This project established for the future - solid basic for my future projects.\n* Thanks a lot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}