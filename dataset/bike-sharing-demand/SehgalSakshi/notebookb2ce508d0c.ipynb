{"cells":[{"metadata":{"id":"rveFMj66PfL_"},"cell_type":"markdown","source":"# <center>Bike Rental Problem</center>\n\n"},{"metadata":{"id":"roDYy-YMPykI"},"cell_type":"markdown","source":"<b>Overview</b><br>\nWe’re given with a <b>bike sharing problem</b>. <br>\nBike sharing platform is used to <b>automate bike rentals</b> from one station to another.<br>\nUnlike the regular rental problems, our goal is not to <strike>identify the income generated</strike> but to use <b>bike sharing data as mobility data within city</b> to identify what are days and weather conditions bike riders prefer bike riding on and to predict number of bikers.<br>\nAlso if we can figure out interesting patterns, we might also <b>identify important city events the bike riders are going for!</b><br>This problem is a <b>Regression Problem</b> since we've to identify count of bike riders given various conditions!"},{"metadata":{"id":"j9V4dJEqP9Kn"},"cell_type":"markdown","source":"<b>About DataSet </b>\n<br>Both hour.csv and day.csv have the following fields, except hr which is not available in day.csv\n\n<br>- instant: record index\n<br>- dteday : date\n<br>- season : season (1:winter, 2:spring, 3:summer, 4:fall)\n<br>- yr : year (0: 2011, 1:2012)\n<br>- mnth : month ( 1 to 12)\n<br>- hr : hour (0 to 23)\n<br>- holiday : weather day is holiday or not (extracted from [Web Link])\n<br>- weekday : day of the week\n<br>- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n<br>+ weathersit :\n<br>- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n<br>- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n<br>- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n<br>- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n<br>- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n<br>- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n<br>- hum: Normalized humidity. The values are divided to 100 (max)\n<br>- windspeed: Normalized wind speed. The values are divided to 67 (max)\n<br>- casual: count of casual users\n<br>- registered: count of registered users\n<br>- cnt: count of total rental bikes including both casual and registered\n\n<br>Since hour and day files contains same data, just the hour file contains more data in detail. Hence we'll use this for our analysis"},{"metadata":{"id":"tkw_5F_nR7CI"},"cell_type":"markdown","source":"# Setting up the Environment"},{"metadata":{"id":"1jcyoB5qR_Oa","outputId":"5ba04b2f-6cca-4fbe-d38d-bc2525686079","trusted":true},"cell_type":"code","source":"#Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import spearmanr\n%matplotlib inline\nimport itertools\nimport os\nimport calendar\nfrom datetime import datetime\nfrom scipy import stats\nfrom scipy.special import inv_boxcox\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split as split\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"S_hOqcPoiJKP"},"cell_type":"markdown","source":"# Helper Functions"},{"metadata":{"id":"ys5S6azfWFSm","trusted":true},"cell_type":"code","source":"def get_numerical_and_categorical_col(df):\n    numerical_col = []\n    cat_col = []\n    for col in df.columns:\n        if str(df[col].dtype).startswith(('int','float')):\n            numerical_col.append(col)\n        elif str(df[col].dtype) == 'category':\n            cat_col.append(col)\n    return numerical_col, cat_col","execution_count":null,"outputs":[]},{"metadata":{"id":"WrAEOB3RUtiA","trusted":true},"cell_type":"code","source":"def change_to_categorical(data, max_cat=10):\n  df = data.copy()\n  for col in df.columns:\n        if df[col].dtype == object or str(df[col].dtype).startswith(('int','float')):\n            count = len(df[col].unique())\n            if count <= max_cat:\n                df[col] = df[col].astype('category')\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"JoUx-KzOXQ1z","trusted":true},"cell_type":"code","source":"# Helper method for printing percentage on count plot\ndef print_percent_count_plot(value_counts, ax):\n    total = sum(value_counts)\n    for idx, count in value_counts.iteritems():            \n      percent_val = (count*100)/total\n      add_to_idx = 0\n      if min(value_counts.index) > 0:\n        add_to_idx = 1\n      plt.text(idx - add_to_idx-.1,count/2,str(round(percent_val))+'%')","execution_count":null,"outputs":[]},{"metadata":{"id":"XYxL3ZAhYapq","trusted":true},"cell_type":"code","source":"def get_count_plot(x, df, ax, y=None, value_counts = None, print_percent = False):\n  if value_counts is None:\n    counts = df[x].value_counts().sort_index()\n  else:\n    counts = df[value_counts]\n  #counts.plot.bar()\n  #sns bars are just more colorful :P\n  if y is None:\n    sns.countplot(x, data=df, ax=ax)\n  else:\n    sns.barplot(x, y, data=df, ax=ax)\n  if print_percent:\n    print_percent_count_plot(counts, ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"De28t-uEaXHy","trusted":true},"cell_type":"code","source":"def get_count_plot_for_categorical(df, n_cols = 2, y='cnt', list_cat=None, value_counts=None, print_percent=False):\n  if list_cat is None:\n    num_col, cat_col = get_numerical_and_categorical_col(df)\n  else:\n    cat_col = list_cat\n  f, axs, n_rows = get_fig_and_axis_for_subplots(len(cat_col), n_cols)\n  for i, col in enumerate(cat_col):\n    ax = plt.subplot(n_rows, n_cols, i+1)\n    get_count_plot(col, df, ax, y, value_counts, print_percent)","execution_count":null,"outputs":[]},{"metadata":{"id":"MK7ZcKf7rrzh","trusted":true},"cell_type":"code","source":"def get_target_dist_with_categorical(df, n_cols = 2, y='cnt', list_cat=None, plot_type = 'box'):\n  if list_cat is None:\n    num_col, cat_col = get_numerical_and_categorical_col(df)\n  else:\n    cat_col = list_cat\n  f, axs, n_rows = get_fig_and_axis_for_subplots(len(cat_col), n_cols)\n  for i, col in enumerate(cat_col):\n    ax = plt.subplot(n_rows, n_cols, i+1)\n    if plot_type == 'box':\n      sns.boxplot(x=col, data=df,y=y,orient=\"v\",ax=ax)\n    else:\n      sns.violinplot(col, data=df,y=y,orient=\"v\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"eyS6piXbBuWx","trusted":true},"cell_type":"code","source":"def get_fig_and_axis_for_subplots(total, n_cols = 2):\n  rows = total/ n_cols\n  n_rows = int(np.ceil(rows))\n  f, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 3))\n  plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=.25, wspace=0.4)\n  if rows* n_cols < n_rows * n_cols:\n    diff = n_rows - rows\n    for i in range(n_cols-int(np.ceil(diff))):\n      if n_rows == 1:\n        f.delaxes(axs[n_cols-1-i])\n      else:\n        f.delaxes(axs[n_rows-1, n_cols-1-i])     \n  return f, axs, n_rows","execution_count":null,"outputs":[]},{"metadata":{"id":"WhKwYhGmBWVi","trusted":true},"cell_type":"code","source":"def get_plot_for_numerical(df, n_cols = 2, plot_type='probability',list_col=None, hist=True, kde=True):\n  if list_col is None:\n      num_col, cat_col = get_numerical_and_categorical_col(df)\n  else:\n      num_col = list_col\n  f, axs, n_rows = get_fig_and_axis_for_subplots(len(num_col), n_cols)\n  for i, col in enumerate(num_col):\n    ax = plt.subplot(n_rows, n_cols, i+1)\n    if plot_type == 'probability':\n      sns.distplot(df[col], hist=hist, kde=hist, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    elif plot_type == 'box':\n      sns.boxplot(data=df,y=col,orient=\"v\",ax=ax)\n    else:\n      sns.violinplot(data=df,y=col,orient=\"v\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"M_kuHUtIFxa3","trusted":true},"cell_type":"code","source":"# visualize correlation matrix\ndef visualize_corr_matrix(data):\n    numerical_col, cat_col = get_numerical_and_categorical_col(data)\n    df = data[numerical_col]\n    corr = df.corr()# plot the heatmap\n    #generating masks for upper triangle so that values are not repeated\n    mask_ut=np.triu(np.ones(corr.shape)).astype(np.bool)\n    sns.heatmap(corr, mask=mask_ut, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","execution_count":null,"outputs":[]},{"metadata":{"id":"hoO8UlsZL6V3","trusted":true},"cell_type":"code","source":"def remove_outliers_for_variable_by_quantiles(data, col, q1=.25, q2=.75):\n  df = data.copy()\n  median = df[col].median()\n  q25, q75 = df[col].quantile([q1,q2])\n  iqr = q75-q25\n  upper_wh = q75 +1.5*iqr\n  lower_wh = q25 - 1.5*iqr\n  whiskers = int(np.floor(lower_wh)), int(np.ceil(upper_wh))\n  df.drop(df[~df[col].between(whiskers[0], whiskers[1]) & (~np.isnan(df[col]))].index, inplace=True)\n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"CMFCMb6T1Uvn","trusted":true},"cell_type":"code","source":"def remove_outliers_for_variable_by_std(data, col):\n  df = data.copy()\n  df = df[np.abs(df[col]-df[col].mean())<=(3*df[col].std())] \n  return df","execution_count":null,"outputs":[]},{"metadata":{"id":"GwgFLGJimQ72","trusted":true},"cell_type":"code","source":"#loop for chi square values\ndef calculate_chi_square_values(df, alpha=.05):\n    chi2_dict = {}\n    numerical_col, cat_col  = get_numerical_and_categorical_col(df)\n    for i in cat_col:\n        for j in cat_col:\n            if i!=j and (j+' '+i) not in chi2_dict.keys():\n                chi2, p, dof, ex = chi2_contingency(pd.crosstab(df[i], df[j]))\n                chi2_dict[i+' '+j] = 'Independent? '+ str(p>alpha)\n    return chi2_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_,convertExp=True):\n    \n    if convertExp:\n        y = inv_boxcox(y, fitted_lambda),\n        y_ = inv_boxcox(y_, fitted_lambda)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_prediction(test, test_pred, train, train_pred, convert_to_original_form = False):\n    if convert_to_original_form:\n        test = inv_boxcox(test, fitted_lambda),\n        test_pred = inv_boxcox(test_pred, fitted_lambda)\n        train = inv_boxcox(train, fitted_lambda),\n        train_pred = inv_boxcox(train_pred, fitted_lambda)\n    f, ax = plt.subplots(1,2, figsize=(10, 5))\n    ax1 = plt.subplot(1,2,1)\n    sns.distplot(test, hist=True, kde=True, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4}, ax = ax1)\n    sns.distplot(test_pred, hist=True, kde=True, \n             color = 'red', \n             hist_kws={'edgecolor':'red'},\n             kde_kws={'linewidth': 4}, ax = ax1)\n    ax1.set_title(\"Actual vs Predicted (Test)\")\n    \n    ax2 = plt.subplot(1,2,2)\n    sns.distplot(train, hist=True, kde=True, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4}, ax = ax2)\n    sns.distplot(train_pred, hist=True, kde=True, \n             color = 'red', \n             hist_kws={'edgecolor':'red'},\n             kde_kws={'linewidth': 4}, ax = ax2)\n    ax2.set_title(\"Actual vs Predicted (Train)\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ecX-r3RdQkdI"},"cell_type":"markdown","source":"# Basic Info About Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"id":"Un-hPfCaRBwv","trusted":true},"cell_type":"code","source":"df_hour = pd.read_csv(\"/kaggle/input/bike-sharing-demand/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/bike-sharing-demand/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"KVF-mojvTEG_","outputId":"e5394a2b-c829-4ed5-9549-8df295720214","trusted":true},"cell_type":"code","source":"df_hour.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_hour[\"dteday\"] = df_hour.datetime.apply(lambda x : x.split()[0])\ndf_hour[\"yr\"] = df_hour.datetime.apply(lambda x : x.split()[0][:4])\ndf_hour['yr'] = df_hour.yr.map({'2011': 0, '2012':1})\ndf_hour[\"hr\"] = df_hour.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\ndf_hour[\"weekday\"] = df_hour.dteday.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\ndf_hour.weekday = df_hour.weekday.map({'Saturday':6, 'Sunday':0, 'Monday':1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4, 'Friday':5})\ndf_hour[\"mnth\"] = df_hour.dteday.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])\ndf_hour.mnth = df_hour.mnth.map({'January':0, 'February':1, 'March':2, 'April':3, 'May':4, 'June':5, 'July':6,\n       'August':7, 'September':8, 'October':9, 'November':10, 'December':11})\ndf_hour[\"weathersit\"] = df_hour.weather\ndf_hour['dteday'] = pd.to_datetime(df_hour['dteday'])\ndel df_hour['weather']\ndel df_hour['datetime']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#performing same on test\n#performing same on test\ndf_test[\"dteday\"] = df_test.datetime.apply(lambda x : x.split()[0])\ndf_test[\"yr\"] = df_test.datetime.apply(lambda x : x.split()[0][:4])\ndf_test['yr'] = df_test.yr.map({'2011': 0, '2012':1})\ndf_test[\"hr\"] = df_test.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\ndf_test[\"weekday\"] = df_test.dteday.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\ndf_test.weekday = df_test.weekday.map({'Saturday':6, 'Sunday':0, 'Monday':1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4, 'Friday':5})\ndf_test[\"mnth\"] = df_test.dteday.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])\ndf_test.mnth = df_test.mnth.map({'January':0, 'February':1, 'March':2, 'April':3, 'May':4, 'June':5, 'July':6,\n       'August':7, 'September':8, 'October':9, 'November':10, 'December':11})\ndf_test[\"weathersit\"] = df_test.weather\ndf_test['dteday'] = pd.to_datetime(df_test['dteday'])\ndel df_test['weather']","execution_count":null,"outputs":[]},{"metadata":{"id":"Rs5CFj6FTi84","outputId":"6af82e86-3858-4545-ad62-f06944f3b5eb","trusted":true},"cell_type":"code","source":"df_hour['cnt'] = df_hour['count']\ndel df_hour['count']\ndf_hour.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"A2KmVB1GTJcJ","outputId":"6c29c4c2-f923-4194-bead-00e97222dff9","trusted":true},"cell_type":"code","source":"df_hour.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"3eoRiP-NTRhM"},"cell_type":"markdown","source":"No null values and all categorical values have already been encoded"},{"metadata":{"id":"hvW0vSZ0TvAw","outputId":"d55cef51-727a-4db3-e19a-15d78862efcf","trusted":true},"cell_type":"code","source":"df_hour.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"n33lmO_XUKCy","trusted":true},"cell_type":"code","source":"#Creating a copy of df to preserve original df\ndf = df_hour.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"lvweb2NVPowR","outputId":"cc946b28-a103-44df-e1df-530af5afbfa0","trusted":true},"cell_type":"code","source":"#Analysing which of these are categorical variables\nfor col in df.columns:\n  print('Count of unique values for ', col, ': ', len(df[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{"id":"TuI25VikV3Xy","trusted":true},"cell_type":"code","source":"'''We know maximum categories for any categorical col is 24 (for month)\nHence we can use this to def function to convert variable to categorical type'''\ndf = change_to_categorical(df, max_cat=24)","execution_count":null,"outputs":[]},{"metadata":{"id":"MMSc7REHV_vw","outputId":"278b50b0-a40c-42d2-9593-6f8c51e65699","trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"j5y9TMadWbfz","outputId":"243fc415-b6d0-4c90-aeaa-328ff72eba37","trusted":true},"cell_type":"code","source":"get_numerical_and_categorical_col(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"PJ2_eEO-XWRQ"},"cell_type":"markdown","source":"Analysing type of data"},{"metadata":{"id":"-uI7xtVzhfiJ"},"cell_type":"markdown","source":"Checking for count type of variables in dataframe"},{"metadata":{"id":"A3Qex4hBZgVt","outputId":"f32e34c7-980a-4759-f135-c3a28b74226a","trusted":true},"cell_type":"code","source":"dataTypeDf = (df.dtypes.astype(str).value_counts()).reset_index().rename(columns={\"index\":\"variableType\",0:\"count\"})\nfig,ax = plt.subplots()\nfig.set_size_inches(12,5)\nget_count_plot('variableType',dataTypeDf, ax, 'count', value_counts='count', print_percent=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"p7rUf-9Eg_Fg"},"cell_type":"markdown","source":"So we have 50% of data as categorical data"},{"metadata":{"id":"LaYgMDVuQdA1"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"id":"asdAUb-2h3Vz"},"cell_type":"markdown","source":"### Descriptive stats for numerical and categorical values"},{"metadata":{"id":"cR4cG4CwhwzP","outputId":"7d91c69c-4173-4e1c-f878-1c8a3028859f","trusted":true},"cell_type":"code","source":"df[get_numerical_and_categorical_col(df)[0]].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"hB_aKRWEjFGd","outputId":"fa7e023b-eb60-46e6-cecb-58db1ec037fd","trusted":true},"cell_type":"code","source":"df[get_numerical_and_categorical_col(df)[1]].describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"id":"8HaJKeu0kpf8","outputId":"eb1d7c70-b376-47da-97a5-3b664d2eb276","trusted":true},"cell_type":"code","source":"get_plot_for_numerical(df,3)","execution_count":null,"outputs":[]},{"metadata":{"id":"iv9SzhuJEZPI"},"cell_type":"markdown","source":"Cnt, casual and registered have log normal distribution, can be converted to near normal taking log of these variables.\n<br>Also, did you see uncanny similarity between these 3 variables? <br>Either we're too lucky to get such a data set or is it too good to be true?<br>\nLet's dig deeper!!"},{"metadata":{"id":"OZUOxQHsFhCj"},"cell_type":"markdown","source":"### Visualizing correlation Matrix"},{"metadata":{"id":"GBJn5wybDUTi","outputId":"d3f94685-3874-4aff-8eab-1745b2b507eb","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nvisualize_corr_matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"id":"oLoonigOJHU4"},"cell_type":"markdown","source":"<b>The Analysis</b><br>\nLet's set a threshold, if two input variables have correlation greater than +- 0.4, they're good candidates for adding <b>Multicollinearity</b> hence one of them should be dropped.<br>\n<b>Multicollinearity</b> is dangerous as model won't be able to accurately which of the two variables is predicting it. Thus coefficients/ weights won't be assigned properly<br>\natemp and temp have .99 correlation<br> Nothing surprising as they're essentially same values in different units. Let's drop one to them.\n<br>windspeed has very low predicting power, should be dropped too.\ncasual seems to have g\nRegistered and casual are highly correlated with each other, hence can drop one of these.<br>But wait for discussion part 2 on this, coming right after!"},{"metadata":{"id":"YVkZEuqyGjyG"},"cell_type":"markdown","source":"<b>Analysis Part 2</b>\nHmm registered variable is predicting count by .97!<br>\nThis dataset is almost like winning a lottery in data science!<br>No now we're not just sniffing but sure there is something fishy about registered and casual!"},{"metadata":{"id":"TrzYAafpHCtu"},"cell_type":"markdown","source":"Embrace yourself!<br> We've found... (Drum rolls in the background :P) <b>Leakage Variables</b> in our data set!<br>"},{"metadata":{"id":"XWstvipDHmXW"},"cell_type":"markdown","source":"### <b>Leakage Variables</b>: Variables that expose information about the target variable\nWhen data contains a certain feature that already predicts the target has already occurred.\n\nIn our problem registered and nonregistered variables are the leakage variables because for any given row total count of these two variables tells us that this was the number of cyclists for the row hence it is directly predicting the target.\n<br><a href=\"https://link.medium.com/Rk1IJWxeK9\">More on this here</a>\n"},{"metadata":{"id":"mAbw5JN3GSnp","trusted":true},"cell_type":"code","source":"'''Let's start creating a list which contains all the variables to be deleted.\nWe can delete them once we're done with our exploratory analysis'''\ncols_to_remove = ['registered','casual','windspeed']\n#besides atemp should be deleted immediately for obvious reasons!\ndel df['atemp']\ndel df_test['atemp']","execution_count":null,"outputs":[]},{"metadata":{"id":"h82wSqTgMfxJ"},"cell_type":"markdown","source":"### Outlier Analysis"},{"metadata":{"id":"1HQeXbqvtHfP","outputId":"83588f26-d863-4f84-ea08-2c18d7c2d877","trusted":true},"cell_type":"code","source":"get_plot_for_numerical(df, 3, plot_type='box')","execution_count":null,"outputs":[]},{"metadata":{"id":"cDhJHB-7tuB7","outputId":"f3d9a24f-add8-4864-d684-b48bda612765","trusted":true},"cell_type":"code","source":"get_target_dist_with_categorical(df,n_cols=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"c5NZ3BpeLaOh","outputId":"933137d9-79d3-40f6-ee97-bf8c5f377d94","trusted":true},"cell_type":"code","source":"get_target_dist_with_categorical(df, n_cols=2, plot_type='violin')","execution_count":null,"outputs":[]},{"metadata":{"id":"-sCb-MxuQHWp"},"cell_type":"markdown","source":"It’s pretty evident outliers have been contributed the most by season 3. Also average numbers are highest in this season.\n<br>Working day or not, mean bikers count is almost the same telling us that biking is not just being used for leisure but for daily activities too.\n<br>Same is supported by the fact there is high rise in number of bikers from 7 to 9 AM and 5 to 7 and gradually decreasing on both sides, suggesting these as school or office commute hours.\n<br>Outliers in the evening time and on holidays or working or the seasons least favourable for bike rides may suggest an event in the city!\n\nMonth, hr and season seem to be good predictors"},{"metadata":{"id":"VSclb0ldQrVZ"},"cell_type":"markdown","source":"Distribution across all parameters is almost log normal, right skewed.\n<br>"},{"metadata":{"id":"SGeeg32jl_lu","outputId":"6e095719-6fd5-4fb5-e74f-6860ab4ffe53","trusted":true},"cell_type":"code","source":"#Let's perform categorical test chi2 to decide which categorical columns to delete\nchi2_dict = calculate_chi_square_values(df)\nchi2_dict","execution_count":null,"outputs":[]},{"metadata":{"id":"iyKvdmyCnwwt"},"cell_type":"markdown","source":"From the visual analysis, month, hr, season seems to be a good predictor.<br>\nOut of these 3, month and season describe the most!<br>\nHence we'll remove all other categorical values dependent on hr\n<br>Weathersit is dependent on all\n<br>holiday, weekday, workingday, month, season all dependent!\n"},{"metadata":{"id":"caRO5SuvbKjG","trusted":true},"cell_type":"code","source":"cols_to_remove.append('season')\ncols_to_remove.append('holiday')\ncols_to_remove.append('weekday')\ncols_to_remove.append('weathersit')","execution_count":null,"outputs":[]},{"metadata":{"id":"xzo7OFhVizwg","outputId":"748b6931-5235-4b31-e71d-061c28427ded","trusted":true},"cell_type":"code","source":"sns.pointplot(x='hr',y='cnt',data=df, hue='season', markers = 'x')","execution_count":null,"outputs":[]},{"metadata":{"id":"FBj5G1vSjYxg","outputId":"d69c9eb4-e884-4b18-ae75-37bf92f4a601","trusted":true},"cell_type":"code","source":"sns.pointplot(x='hr',y='cnt',data=df, hue='weekday', markers = 'x')","execution_count":null,"outputs":[]},{"metadata":{"id":"1O6JUKwyjxXO","outputId":"cc69b6a6-16d7-4f9a-b81c-74093c12c812","trusted":true},"cell_type":"code","source":"#to visualize similar plot for type of user, we would need to use melt\n#what melt would do, take each hour and generate rows for value variables. Next we'll use this to find mean for each hour and for each type of users\nhr_users_type = pd.melt(df[[\"hr\",\"casual\",\"registered\"]], id_vars=['hr'], value_vars=['casual', 'registered']).sort_values(by='hr')\nhr_users_type.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"f-L4iPwNkrM1","outputId":"fd8c5975-1952-4f27-e57a-45145da7e01d","trusted":true},"cell_type":"code","source":"hr_users_type_mean = pd.DataFrame(hr_users_type.groupby([\"hr\",\"variable\"],sort=True)[\"value\"].mean()).reset_index()\nhr_users_type_mean.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"F21BSinckuPi","outputId":"fff83086-2303-453c-b0b2-645d9735fc40","trusted":true},"cell_type":"code","source":"sns.pointplot(x=hr_users_type_mean[\"hr\"], y=hr_users_type_mean[\"value\"],hue=hr_users_type_mean[\"variable\"],hue_order=[\"casual\",\"registered\"], data=hr_users_type_mean, join=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"ni3zHbI3Xab6","outputId":"d0adf536-5676-40c6-d70c-7a442f1c2fe8","trusted":true},"cell_type":"code","source":"#We done with visually exploring data, let's just see how many variables we decided to drop\n#Also we should be dropping dteday too\ncols_to_remove.append('dteday')\ncols_to_remove","execution_count":null,"outputs":[]},{"metadata":{"id":"RPNPav47lUgE","outputId":"bb29959c-f69d-40ae-f533-34f12d22fee3","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"hC8ysqoxlXU5","trusted":true},"cell_type":"code","source":"#deleting outliers from all numerical variables\nfor col in get_numerical_and_categorical_col(df)[0]:\n df = remove_outliers_for_variable_by_std(df, col)","execution_count":null,"outputs":[]},{"metadata":{"id":"jkLLLlsV19Iy"},"cell_type":"markdown","source":"# Using Random Forest for feature importance<br>\nUsing Random Forest to get feature importance values and we'll be comparing it with our list of columns to remove<br>\n... Moment of truth... Hang on"},{"metadata":{"id":"ddoHA_KoxCzd","outputId":"d9346d35-c36f-4133-f456-575cf8c2594f","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nnp.random.seed(42)\n# drop target columns\ndf_original = df.copy()\ndrop_cols=['cnt', 'dteday','registered','casual']\nX = df.drop(drop_cols, axis = 1) # X = independent columns (potential predictors)\ny = df['cnt'] # y = target column (what we want to predict)\n# instantiate RandomForestClassifier\nrf_model = RandomForestRegressor()\nrf_model.fit(X,y)\nfeat_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n# determine 20 most important features\ndf_imp_feat = feat_importances.nlargest(20)\ndf_imp_feat.plot(kind='bar')\nplt.show()\nprint(df_imp_feat)\nprint('Comparing with our columns')\nprint(cols_to_remove)","execution_count":null,"outputs":[]},{"metadata":{"id":"7ZczyP6G4t9p"},"cell_type":"markdown","source":"Phew!! So far so good!<br>\n<b>One last important step</b> is left... <br>\nYes, you guessed it right! Let's go and clean the dataset by dropping the identified variables!<br>Woah, it was fun getting our hands dirty here!"},{"metadata":{"id":"RiO-zLSQxXig","outputId":"efd919dd-b033-45f3-ecd2-25a65979bc95","trusted":true},"cell_type":"code","source":"df_cleaned = df.copy()\ndf_cleaned.drop(cols_to_remove, axis=1, inplace=True)\ndf_test.drop(cols_to_remove, axis=1, inplace=True, errors='ignore')\ndf_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling\nDo we need to? Let's find out..!!"},{"metadata":{"id":"a4PJBLhc5l9X","trusted":true},"cell_type":"code","source":"df_cleaned.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes!! Feature scaling is definitely required because we've observed there's linear relationship<br>\nHence to try on linear models, we need features to be scaled.<br>\nThere wouldn't have been a need if features would have been in similar ranges but guess God has different plans for us! :("},{"metadata":{},"cell_type":"markdown","source":"## Log transform for target variable\nSince it's a lognormal distribution, it's important to transform to near gaussian for linear regression models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform training data & save lambda value \nfitted_data, fitted_lambda = stats.boxcox(df_cleaned.cnt)\ndf_cleaned['cnt_box_cox'] = fitted_data\ndf_cleaned['cnt_log'] = np.log(df_cleaned.cnt)\n#to be used for last step\ndf_cleaned['box_cox_reverse'] = inv_boxcox(fitted_data, fitted_lambda)\n\nget_plot_for_numerical(df_cleaned, 3, list_col=['cnt','cnt_box_cox','cnt_log','box_cox_reverse'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see box cox has produced better distribution than simply taking log, due to hyper parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cleaned.drop(['cnt','cnt_log','box_cox_reverse'], axis=1, inplace=True)\ndf_cleaned.rename(columns={'cnt_box_cox':'count_transformed'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moving on to feature Scaling!<br>\nWe're spoilt by choices here too!<br>\n### The big question!! <b>Normalize or Standardize??</b><br>\n\n* Normalization when distribution does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n* Standardization, helpful where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n<br>Woah! Ironical!! Normalization is actually for non normal distribution\n<br> Our distribution is not perfectly normal and it still does contain some outlier. Let's go with standardization."},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hold on!! <b>First Split, then Normalize!!!</b>\n<br> Here's why!<br>\n    You first need to split the data into training and test set (validation set could be useful too).\n<br>\nDon't forget that testing data points represent real-world data. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance).\n<br>\nTherefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points."},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'count_transformed'\nX = df_cleaned.drop(target, axis=1)\ny = df_cleaned[target]\nseed=23\nX_train, X_test, y_train, y_test = split(X, y, test_size=.3, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the Models"},{"metadata":{},"cell_type":"markdown","source":"Starting with the basics!\n## Linear Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression,Ridge,Lasso\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn import metrics\n\n# Initialize logistic regression model\nlr = LinearRegression()\n\n# Train the model\nlr.fit(X_train,y = y_train)\n\n# Make predictions\ny_pred = lr.predict(X_test)\n\ny_pred_train = lr.predict(X_train)\n\nprint('RMSLE for test: ',rmsle(y_test, y_pred, True))\nprint('RMSLE for train: ',rmsle(y_train, y_pred_train, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_df = pd.DataFrame(lr.coef_, X.columns, columns=['Coefficient'])  \ncoeff_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prediction(y_test, y_pred, y_train, y_pred_train, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regularizing Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nridge_param = {'max_iter':[3000], 'alpha':[.1,.03,.3,1,3,10, 30, 100,300]}\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)\ngrid_ridge = GridSearchCV(ridge,\n                           ridge_param,\n                           scoring = rmsle_scorer,\n                           cv = 10)\ngrid_ridge.fit(X_train, y_train)\ny_pred_ridge = grid_ridge.predict(X_test)\ny_pred_ridge_train = grid_ridge.predict(X_train)\nprint('Grid Ridge Best Params: ', grid_ridge.best_params_)\nprint('RMSLE for test: ',rmsle(y_test, y_pred_ridge, True))\nprint('RMSLE for train: ',rmsle(y_train, y_pred_ridge_train, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax= plt.subplots()\nfig.set_size_inches(12,5)\ndf = pd.DataFrame(grid_ridge.cv_results_)\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"])\ndf[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\nsns.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prediction(y_test, y_pred_ridge, y_train, y_pred_ridge_train, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso()\nalpha = 1/np.array([.1,.03,.3,1,3,10, 30, 100,300,1000])\nlasso_param = {'max_iter':[3000], 'alpha':alpha}\nrmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)\nrandom_lasso = RandomizedSearchCV(lasso,\n                           lasso_param,\n                           scoring = rmsle_scorer,\n                           cv = 10)\nrandom_lasso.fit(X_train, y_train)\ny_pred_lasso = random_lasso.predict(X_test)\ny_pred_lasso_train = random_lasso.predict(X_train)\nprint('Random Lasso Best Params: ', random_lasso.best_params_)\nprint('RMSLE for test: ',rmsle(y_test, y_pred_lasso, True))\nprint('RMSLE for train: ',rmsle(y_train, y_pred_lasso_train, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax= plt.subplots()\nfig.set_size_inches(12,5)\ndf = pd.DataFrame(random_lasso.cv_results_)\ndf[\"alpha\"] = df[\"params\"].apply(lambda x:x[\"alpha\"])\ndf[\"rmsle\"] = df[\"mean_test_score\"].apply(lambda x:-x)\nsns.pointplot(data=df,x=\"alpha\",y=\"rmsle\",ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prediction(y_test, y_pred_lasso, y_train, y_pred_lasso_train, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor as dt\ndt_m = dt(random_state=0)\ndt_m.fit(X_train,y_train)\ny_pred_dt=dt_m.predict(X_test)\ny_pred_dt_train=dt_m.predict(X_train)\nprint('RMSLE for test: ',rmsle(y_test, y_pred_dt, True))\nprint('RMSLE for train: ',rmsle(y_train, y_pred_dt_train, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prediction(y_test, y_pred_dt, y_train, y_pred_dt_train, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RMSLE value for train and the plot above suggests it has overfit."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor as rfr\nrf = rfr(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\ny_pred_rf_train = rf.predict(X_train)\nprint('RMSLE for test: ',rmsle(y_test, y_pred_rf, True))\nprint('RMSLE for train: ',rmsle(y_train, y_pred_rf_train, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prediction(y_test, y_pred_rf, y_train, y_pred_rf_train, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbm = GradientBoostingRegressor(n_estimators=3000,alpha=.03)\ngbm.fit(X_train,y_train)\ny_pred_gbm = gbm.predict(X_test)\ny_pred_gbm_train = gbm.predict(X_train)\nprint('RMSLE for test: ',rmsle(y_test, y_pred_gbm, True))\nprint('RMSLE for train: ',rmsle(y_train, y_pred_gbm_train, True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_prediction(y_test, y_pred_gbm, y_train, y_pred_gbm_train, True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The final Verdict!!\n1. Linear Regression and even the type 1 and type 2 regularizations are not pretty good at predicting. \n2. Decision Tree performed moderately but since there was no pruning, hence it was overfitting the data.\n\n### The 2 finalists are....\nRandom Forest and XGBoost.. well the competition was really close!<br>\nBoth performed equally well on test (validation data)\nBut the tie break was the train score!!\n## And the winner is... (drumrolls..) \"XGBoost\"\nWell <strike>not because it scored better for test</strike> but because it performed relatively poorer on training set because that is what we want for model to over fit!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.sort_values(by='datetime')\ndatetime_series = df_test.datetime\ndf_test_for_model = df_test.copy()\ndf_test_for_model.drop(['datetime'], inplace=True, axis=1)\nX_test_ndarry = df_test_for_model.to_numpy()\nfinal_X_test = sc.fit_transform(X_test_ndarry)\nfinal_y_pred = inv_boxcox(gbm.predict(final_X_test), fitted_lambda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_y_pred_rf = inv_boxcox(rf.predict(final_X_test), fitted_lambda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_y_pred.shape, datetime_series.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'datetime':datetime_series, 'count':np.round(final_y_pred)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_rf = pd.DataFrame({'datetime':datetime_series, 'count':final_y_pred_rf})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(final_y_pred, hist=True, kde=True, \n             color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems pretty similar to original distribution!!\n** Fingers Crossed! Let's go ahead and submit!! "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Just to avoid any missing values\nsubmission[submission['count'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see values around it to fill these\n#intuition knn\nsubmission.iloc[721],submission.iloc[720],submission.iloc[719]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By this and also by our initial analysis, afternoon is not a preferable time to ride bike\nsubmission.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.iloc[725],submission.iloc[726],submission.iloc[727]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('bike_predictions_rounded.csv', index=False)\nsubmission.to_csv('bike_predictions_random_forest.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"P.S., it was my 1st ever ML project! So thank you for staying till here!<br>\nP.P.S, if you too found the journey fun, please upvote! Cheers!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}