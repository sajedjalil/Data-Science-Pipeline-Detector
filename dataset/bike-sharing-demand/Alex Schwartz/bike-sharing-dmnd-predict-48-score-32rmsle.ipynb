{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Bike Sharing Demand Analysis"},{"metadata":{},"cell_type":"markdown","source":"Data in this case is about a bike sharing system, it includes the first 19 days of each month for the period of 2 yrs, and we have to predict the demand for the next +- 10 days."},{"metadata":{},"cell_type":"markdown","source":"Let's see whats inside the data, but as a first apporach, i would say that peak hours during week should be for people commuting work, and weekends would depend a lot of weather."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style=\"dark\")\nsns.set(style=\"whitegrid\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lmport Datasets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/bike-sharing-demand/train.csv')\ntest=pd.read_csv('/kaggle/input/bike-sharing-demand/test.csv')\nprint('train shape:',train.shape)\nprint('test shape:',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# About Data:"},{"metadata":{},"cell_type":"markdown","source":"Data provided is about hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. The goal is to predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.\n**Data Fields:\n\n**datetime:** hourly date + timestamp     \n**season:**  1 = spring, 2 = summer, 3 = fall, 4 = winter    \n**holiday:** whether the day is considered a holiday  \n**workingday:** whether the day is neither a weekend nor holiday   \n**weather:** 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog    \n**temp:** temperature in Celsius   \n**atemp:** \"feels like\" temperature in Celsius   \n**humidity:** relative humidity   \n**windspeed:** wind speed   \n**casual:** number of non-registered user rentals initiated   \n**registered:** number of registered user rentals initiated   \n**count:** number of total rentals   "},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for null data\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import missingno as msno\n\nfig,ax=plt.subplots(2,1,figsize=(10,5))\n\nmsno.matrix(train,ax=ax[0])\nax[0].set_title('Train Data')\nmsno.matrix(test,ax=ax[1])\nax[1].set_title('Test Data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#variable datatype:\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No null data!, so lets change some formats and start EDA!"},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom dateutil import parser\nimport calendar\n\n#parse string datetime into datetime format\ntrain['datetime2']=train.datetime.apply(lambda x: parser.parse(x))\n\n#Get some different time variables\ntrain['year']=train.datetime2.apply(lambda x: x.year)\ntrain['month']=train.datetime2.apply(lambda x: x.month)\ntrain['weekday']=train.datetime2.apply(lambda x: x.weekday())\ntrain['weekday_name']=train.datetime2.apply(lambda x: calendar.day_name[x.weekday()])\ntrain['hour']=train.datetime2.apply(lambda x: x.hour)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create categorical data\ntrain['season_decode']=train.season.map({1:'spring',2:'summer',3:'fall',4:'winter'})\ntrain['working_decode']=train.workingday.map({1:'work',0:'notwork'})\ntrain['weather_decode']=train.weather.map({1:'Clear',2:'Mist',3:'LightRain',4:'HeavyRain'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outliers Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2)\nsns.distplot(train['count'],bins=30,ax=ax[0])\nax[0].set_title('count distrib')\nsns.boxplot(data=train,y=train['count'],ax=ax[1])\nax[1].set_title('count boxplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution is right skewed. but lets see how many instances are out of the 3 bias range:"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_count=train['count'].mean()\nstd_count=train['count'].std()\nprint(mean_count-3*std_count)\nprint(mean_count+3*std_count)\noutliers1=train[train['count']>(mean_count+3*std_count)]\nlen(outliers1['count'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 147 cases where the count is out of the 99% probability given the count mean and std, so i will take them out for next steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"train2=train[train['count']<=(mean_count+3*std_count)]\ntrain2.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This outlier analysis is super basic, in order to get a better prediction, the analysis should be done by combining different features. "},{"metadata":{},"cell_type":"markdown","source":"# EDA: "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Season\nsns.boxplot(data=train2,y=train2['count'],x=train['season_decode']).set_title('Demand by season')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite strange but spring looks like the season with the less bikers."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Year\n\ntrain2.groupby(['year','month'])['count'].mean().plot().set_title('demand by year')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"demand is increasing, so year, month and season are important features. More if we take in count that we are going to predict the endo of each month."},{"metadata":{"trusted":true},"cell_type":"code","source":"#WeekDay & Hour:\nweek_hour=train2.groupby(['weekday_name','hour'])['count'].mean().unstack()\nweek_hour=week_hour.reindex(index=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n\n\nplt.figure(figsize=(15,6))\ncmap2 = sns.cubehelix_palette(start=2,light=1, as_cmap=True)\n\nsns.heatmap(week_hour,cmap=cmap2).set_title('Demand by Day-Hour')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most important time during Week: 8 & 17-18  -->Work Commuters!  \nMost important time during Weekend: 13-16  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Difference between casual and resgitered\ntrain2.groupby(['hour'])['casual','registered','count'].mean().plot().set_title('Demand by hour')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain2.groupby(['weekday_name'])['casual','registered','count'].mean().plot(kind='bar').set_title('demand by day of week')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Casual demand increases during weekend, while registered is for comuting work.\n* Registered demand has a high importance in the overall demand"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Weather\ntrain2.groupby(['weather_decode'])['casual','registered'].mean().plot(kind='bar').set_title('demand by weather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Temp\nseason_temp=train2.groupby(['season_decode','temp'])['count'].mean().unstack()\n\n\nplt.figure(figsize=(15,8))\ncmap3 = sns.cubehelix_palette(start=6,light=1, as_cmap=True)\n\nsns.heatmap(season_temp,cmap=cmap3).set_title('demand by season and temperature')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation & Choosing Variables:"},{"metadata":{"trusted":true},"cell_type":"code","source":"Correlation_Matrix=train2[['holiday','workingday','weather','temp','atemp','humidity','windspeed','casual','registered','count']].corr()\nmask = np.array(Correlation_Matrix)\nmask[np.tril_indices_from(mask)] = False\nfig,ax= plt.subplots()\nfig.set_size_inches(20,10)\nsns.heatmap(Correlation_Matrix,mask=mask,vmax=.8,annot=True,square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run a random forest for selectin features & understand importance of each"},{"metadata":{"trusted":true},"cell_type":"code","source":"#preparing data sets for random forest\nX=train2[['season','holiday','workingday','weather','temp','atemp','humidity','windspeed','year','month','weekday','hour']]\n\ny_count=train2['count']\ny_casual=train2['casual']\ny_reg=train2['registered']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n#Scaled all distributions\nX_Scaled=StandardScaler().fit_transform(X=X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#Split for train-test\nX_train, X_test, y_train, y_test = train_test_split(X_Scaled, y_count, test_size=0.25, random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_count=RandomForestRegressor()\nrf_count.fit(X_train,y_train)\n\nimportance_count=pd.DataFrame(rf_count.feature_importances_ , index=X.columns, columns=['count']).sort_values(by='count',ascending=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_count.plot(kind='bar',color='r').set_title('Importance of features for total demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeat for casual demand:\n\nX_train, X_test, y_train, y_test = train_test_split(X_Scaled, y_casual, test_size=0.25, random_state=42)\n\nrf_casual=RandomForestRegressor()\nrf_casual.fit(X_train,y_train)\n\nimportance_casual=pd.DataFrame(rf_casual.feature_importances_ , index=X.columns, columns=['casual']).sort_values(by='casual',ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_casual.plot(kind='bar').set_title('Importance of features for casual demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeat for registered demand:\n\nX_train, X_test, y_train, y_test = train_test_split(X_Scaled, y_reg, test_size=0.25, random_state=42)\n\nrf_reg=RandomForestRegressor()\nrf_reg.fit(X_train,y_train)\n\nimportance_reg=pd.DataFrame(rf_reg.feature_importances_ , index=X.columns, columns=['reg']).sort_values(by='reg',ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_reg.plot(kind='bar',color='g').set_title('Importance of features for registered demand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance_df=pd.concat([importance_count,importance_casual,importance_reg],axis=1)\nimportance_df.plot(kind='bar').set_title('Feature importance for each kind of demand')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prelliminar Conclussions:\n\n- **Hour** is the most important feature in order to predict the demand, either in casual and registered bikers.\n- resgitered bikers are driving the most of the bike demand.\n- for **registered bikers** the demand peaks are during pre&post work hours, so **commuters** are the importan users.\n- the overall bike users are increasing year by year, we can observe an tendency that is getting importance as we are going to predict the demand for the lasts days of each month.\n- for **casual bikers** the **atemp** ('how it feels weather') and the not working days are super important as well, and this has a lot of sense."},{"metadata":{},"cell_type":"markdown","source":"As resgitered bikers have a big importance on the total demand, and casual bikers demand bikes the not working days, i will proceed with a shared model for both behaviors, it looks like they are not fighting for the 'same' bikes."},{"metadata":{},"cell_type":"markdown","source":"# Preparing Train/Test set and final feature selection: "},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_selection=['workingday','weather','atemp','humidity','windspeed','year','month','weekday','hour']\nprint('features for model:',len(feature_selection))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare Training data\nX_train=train2[feature_selection]\nprint(X_train.shape)\n\ny_train=train2['count']\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare Test data\n\n#parse string datetime into datetime format\ntest['datetime2']=test.datetime.apply(lambda x: parser.parse(x))\n\n#Get some different time variables\ntest['year']=test.datetime2.apply(lambda x: x.year)\ntest['month']=test.datetime2.apply(lambda x: x.month)\ntest['weekday']=test.datetime2.apply(lambda x: x.weekday())\ntest['hour']=test.datetime2.apply(lambda x: x.hour)\n\nX_test=test[feature_selection]\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled=StandardScaler().fit_transform(X=X_train)\nX_test_scaled=StandardScaler().fit_transform(X=X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As wee seen before, there are some outliers, i have cleaned the base but not in a deep way, so as cost function i will use RMSLE, that is better to not penalize huge differences when actual predicted value are both huge numbers.   \n   \n\nFor more: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113064"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import make_scorer\n\n\ndef rmsle(y,y_pred):\n    return np.sqrt(mean_squared_log_error(y,y_pred))\n    \nrmsle_score=make_scorer(rmsle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nrfr=RandomForestRegressor(random_state=42)\n\nscore=cross_val_score(rfr,X_train_scaled,y_train,cv=15,scoring=rmsle_score)\n\nprint(f'Score rmsle mean: {np.round(score.mean(),4)}')\nprint(f'Score  rmsle std: {np.round(score.std(),4)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr.fit(X_train_scaled,y_train)\ny_pred=rfr.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv('/kaggle/input/bike-sharing-demand/sampleSubmission.csv')\nsubmission['count']=y_pred\nsubmission.to_csv('submissionI.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Score for SubnimissionI = 0.48816**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Without Scaling Data\n\nrfr.fit(X_train,y_train)\ny_pred=rfr.predict(X_test)\nsubmission2=pd.read_csv('/kaggle/input/bike-sharing-demand/sampleSubmission.csv')\nsubmission2['count']=y_pred\nsubmission2.to_csv('submissionII.csv',index=False)               ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Score for SubnimissionII = 0.48682**"},{"metadata":{},"cell_type":"markdown","source":"LetÂ´s tune the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, train_test_split\n\n\nx_train2,x_test2,y_train2,y_test2=train_test_split(X_train,y_train,test_size=0.25,random_state=42)\n\nparams={'n_estimators': [10,50,100,300,500],\n       'n_jobs':[-1],\n       'max_features':['auto','sqrt','log2'],\n       'random_state':[42]}\n\nrfr_tuned=GridSearchCV(estimator=RandomForestRegressor(),param_grid=params,scoring='neg_mean_squared_log_error',verbose=True)\n\nrfr_tuned.fit(x_train2,y_train2)\nprint(rfr_tuned.best_params_)\nprint(rfr_tuned.best_estimator_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_final=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features='auto', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n                      oob_score=False, random_state=42, verbose=0,\n                      warm_start=False)\n\nrfr_final.fit(x_train2,y_train2)\ny_pred2=rfr_final.predict(x_test2)\nprint('RMSLE:',np.round(rmsle(y_test2,y_pred2),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_final=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n                      max_features='auto', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n                      oob_score=False, random_state=42, verbose=0,\n                      warm_start=False)\n\nrfr_final.fit(X_train,y_train)\ny_pred=rfr.predict(X_test)\nsubmission3=pd.read_csv('/kaggle/input/bike-sharing-demand/sampleSubmission.csv')\nsubmission3['count']=y_pred\nsubmission3.to_csv('submissionIII.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a conclusion, there are some features that are highly important. We can predict an overall demand in an easy way, and without using a complex model, but if we want to improve the final result we must spend time working on outliers.\n\nBy other hand, i could have builded two different models (one for casual and the other for registered), in orden to be more precise.\n\nFor the last, taking in count rmsle helps us to not penalize higher the difference between real values and predicted on the peaks of demand. I think this is a good strategy for building an algorithm that predict demand."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}