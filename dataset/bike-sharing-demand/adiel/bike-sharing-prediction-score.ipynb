{"cells":[{"metadata":{"id":"6jRw8I3QczAI"},"cell_type":"markdown","source":"# Bike Sharing Prediction Score RMSLE: 0.47290\n\n# EDA\n- Analyze the bike charing test vs training data\n- Find intracting point to work on\n\n# Pre-process of data\n- Change Weather and Season to one-hot column\n- Normlized Values to better Predicition\n\n# Analyze data\n- Grid search over XG-BOOST to find best paramaters\n- Compare XGB vs Linare regression\n- Compare number of Regression algoritms\n\n- Select XGB with optimal paramter\n\n# Submit\n- get score of 0.47290\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom sklearn import preprocessing\n\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator\n\nos.system(\"pip install fbprophet\")\nimport fbprophet\n\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification","execution_count":null,"outputs":[]},{"metadata":{"id":"Oi3JR4p9czAI","trusted":true},"cell_type":"code","source":"import os\nstart_path = \"\"\nif os.path.exists('/kaggle/input/bike-sharing-demand'):\n    start_path = \"/kaggle/input/bike-sharing-demand/\"\n\n\ntrain_df=pd.read_csv(start_path+'train.csv')\ntest_df=pd.read_csv(start_path+'test.csv')\n\n\n\nseed=1\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4XVKTT-OczAI","trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\n\ndef prepare_prophet(df):\n    # prepare expected column names\n    tmp_df = pd.DataFrame()\n    tmp_df['ds']= pd.to_datetime(df['datetime'])\n    tmp_df['y'] = np.log(df['count'])\n    # define the model\n    model = Prophet(yearly_seasonality=True)\n    model.add_country_holidays(country_name='US')\n    # fit the model\n    model.fit(tmp_df)\n    return model\n\ndef predict_prophet(df,model):\n    future = pd.DataFrame()\n    future['ds']= pd.to_datetime(df['datetime'])\n    print(future.sample())\n    # use the model to make a forecast\n    forecast = model.predict(future)\n    # summarize the forecast\n    # print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].sample())\n    # print(\"====\")\n    print(forecast.sample())\n    forecast['exp_yhat']=np.exp(forecast['yhat'])\n    df = pd.concat([df,forecast['exp_yhat']],axis=1)\n    # plot forecast\n    return df\n\nmodel = prepare_prophet(train_df)\ntrain_df = predict_prophet(train_df,model)\ntest_df = predict_prophet(test_df,model)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4bYwPAqRczAI","trusted":true},"cell_type":"code","source":"df = pd.concat([train_df,test_df])","execution_count":null,"outputs":[]},{"metadata":{"id":"EsZL85dXczAI","outputId":"3f76dbd6-6033-473d-e83e-f42297e5eb35","trusted":true},"cell_type":"code","source":"df.columns.unique()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"myoDpr18czAJ","outputId":"1371e709-d3a5-48a5-ec7e-920dc9fb4702","trusted":true},"cell_type":"code","source":"df.corr()['count'].drop('count').sort_values()","execution_count":null,"outputs":[]},{"metadata":{"id":"xw0egxFJczAJ","outputId":"6f57860b-fe37-45da-9317-0011abb899f6","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"BFYUMiS2czAK","outputId":"8808a724-2903-4a11-c337-67cb25dc8adb","trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"zk2ACuaRczAK","outputId":"3f456892-fda9-4bb3-fa00-9190590de3bb","trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"kP-Zt5myczAK"},"cell_type":"markdown","source":"sesson - [1,2,3,4]\nholiday = 0,1 mostly 0\nworkingday = 0,1 mostly 1\nweather = 1,2,3,4 mostly 2-1\n\ntemp - [0.82..41] mostly bellow 27\natemp = [0..50] mostly bellow 31\n\nhumidity = 0..100 \nwindspeed = 0..56 mostly bellow 7..16\ncasual = 0..367 mostly bellow 49\n\nregistered = 0..886 mostly bellow 222\ncount = 1..977 msotly bellow 284\n"},{"metadata":{"id":"5ouFbgiTczAK","outputId":"8352ea1a-a2e3-4847-d580-efc8b13b49f8","trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"id":"9PV-KRssczAK","outputId":"1dc24b7f-fbca-4ecb-fc1e-bfe017ed0a34","trusted":true},"cell_type":"code","source":"df.describe()\n#the remove change of type to bool made holiday and workday to be removed from describe output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"IL_GQhuhczAK"},"cell_type":"markdown","source":"## Idea\n- most of the fields can be change into full numbers fields\nconvery to int instead of float\n\n\n"},{"metadata":{"id":"lc1dD1hwczAK","outputId":"85e3ef5e-2805-424e-f875-edd1ea82aabd","trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 250\nplt.hist(train_df[train_df['season'] == 1]['count'], alpha = 0.1, bins=bins, label='Season-0')\nplt.hist(train_df[train_df['season'] == 2]['count'], alpha = 0.2, bins=bins, label='Season-1')\nplt.hist(train_df[train_df['season'] == 3]['count'], alpha = 0.3, bins=bins, label='Season-2')\nplt.hist(train_df[train_df['season'] == 4]['count'], alpha = 0.4, bins=bins, label='Season-3')\n\nplt.xlabel('bins of days')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"n3g_QpYyczAK","outputId":"82b0a059-ddd4-4fce-f66e-ee4ae881d193","trusted":true},"cell_type":"code","source":"fig,axes=plt.subplots(4,7)\nfor i in range(4):\n    df = train_df[train_df['season'] == i+1]\n    df = df.expanding(min_periods=10).mean()\n    axes[i,0].hist(x=\"temp\",data=df,edgecolor=\"black\",linewidth=2,color='red')\n    axes[0,0].set_title(\"temp\")\n    axes[i,1].hist(x=\"atemp\",data=df,edgecolor=\"black\",linewidth=2,color='red')\n    axes[0,1].set_title(\"atemp\")\n    axes[i,2].hist(x=\"windspeed\",data=df,edgecolor=\"black\",linewidth=2,color='black')\n    axes[0,2].set_title(\"windspeed\")\n    axes[i,3].hist(x=\"humidity\",data=df,edgecolor=\"black\",linewidth=2,color='green')\n    axes[0,3].set_title(\"humidity\")\n    axes[i,4].hist(x=\"casual\",data=df,edgecolor=\"black\",linewidth=2,color='blue')\n    axes[0,4].set_title(\"casual\")\n    axes[i,5].hist(x=\"registered\",data=df,edgecolor=\"black\",linewidth=2,color='blue')\n    axes[0,5].set_title(\"registered\")\n    axes[i,6].hist(x=\"count\",data=df,edgecolor=\"black\",linewidth=2,color='blue')\n    axes[0,6].set_title(\"count\")\nfig.set_size_inches(10,10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"doMekECjczAK","outputId":"9ab5c12b-9cd1-4377-ae76-527357a2bfc1","trusted":true},"cell_type":"code","source":"train_df.plot.scatter(x=[\"temp\"], y=['count'], alpha=0.3, color='green')","execution_count":null,"outputs":[]},{"metadata":{"id":"kxcDjHPoczAK","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"neD-NTU_czAK","outputId":"99b2a117-1895-4eb4-a79d-b12f81a1d806","trusted":true},"cell_type":"code","source":"\n\ndef add_datepart(df, field_name, prefix=None, drop=False, time=False):\n    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n    df[prefix+field_name] = pd.to_datetime(df[field_name], infer_datetime_format=True)\n    field = df[prefix+field_name]\n    attr = ['Year', 'Month', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start',\n    'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr:\n        if (prefix+n not in df):\n            df[prefix + n] = getattr(field.dt, n.lower())\n    # Pandas removed `dt.week` in v1.1.10\n    if (prefix+\"Week\" not in df):\n        week = field.dt.isocalendar().week if hasattr(field.dt, 'isocalendar') else field.dt.week\n        df[prefix+'Week'] = week.astype(np.int32)\n    mask = ~field.isna()\n#     df[prefix + 'Elapsed'] = np.where(mask,field.values.astype(np.int64) // 10 ** 9,None)\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df\n\n# Convery dates to year,mount,day column\ndef add_year_day_to_df(test_df):\n    test_df[\"dt_hour\"] = [t.hour for t in pd.DatetimeIndex(test_df.datetime)]\n#     test_df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(test_df.datetime)]\n#     test_df[\"month\"] = [t.month for t in pd.DatetimeIndex(test_df.datetime)]\n    test_df['year'] = [t.year for t in pd.DatetimeIndex(test_df.datetime)]\n    test_df['year'] = test_df['year'].map({2011:0, 2012:1})\n    test_df = add_datepart(test_df, 'datetime', prefix=\"dt_\")\n    test_df['date_day_idx'] = test_df['year']*365+test_df['dt_Dayofyear']\n    test_df['date_day_idx'] -= min(test_df['date_day_idx'])\n    test_df['date_hour_idx'] = test_df['date_day_idx']*24+test_df['dt_hour']\n#     test_df.drop('datetime',axis=1,inplace=True)\n    return test_df\n\n\ntrain_df = add_year_day_to_df(train_df)\ntest_df = add_year_day_to_df(test_df)\ndf = pd.concat([train_df, test_df])\n\n\nremove_fields = []\n# remove_fields.append('dt_Year')\nremove_fields.append('datetime')\nremove_fields.append('dt_datetime')\nremove_fields.append('casual')\nremove_fields.append('registered')\nremove_fields.append('count')\n\n\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"gxzaQrobczAL","trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"id":"nvJBPWTUczAL","outputId":"518886a9-95c4-446b-8208-f0292cbaf899","trusted":true},"cell_type":"code","source":"# test_df['year']*365+test_df['dt_Dayofyear']\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"J0jX-Ul2czAL","outputId":"3a86cbbf-2e56-4119-e5ad-98506d54b5b5","trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2,2,figsize = (16, 10))\nax11 = plt.subplot(2,2,1)\nsns.pointplot(x=train_df['dt_hour'] , y=train_df['count'] , hue = train_df['season'] , ax = ax11)\nax11.set_title('The influence of hour(weekday)')\n\nax12 = plt.subplot(2,2,2)\nsns.pointplot(x=train_df['dt_hour'] , y=train_df['count'] , hue = train_df['weather'] , ax = ax12)\nax12.set_title('The influence of hour(weekday)')\n\nax2 = plt.subplot(2,2,3)\nsns.pointplot(x=train_df['dt_hour'] , y=train_df['count'] , hue = train_df['workingday'] , ax = ax2)\nax2.set_title('The influence of hour(workingday)')\n\nax3 = plt.subplot(2,2,4)\nsns.pointplot(x=train_df['dt_hour'] , y=train_df['count'] , hue = train_df['holiday'] , ax = ax3)\nax3.set_title('The influence of hour(holiday)')","execution_count":null,"outputs":[]},{"metadata":{"id":"JZlhmt5IczAL","outputId":"a8b1355f-6b46-48e4-f1ff-a95ddc6d1761","trusted":true},"cell_type":"code","source":"[df['datetime'].min(),df['datetime'].max()]","execution_count":null,"outputs":[]},{"metadata":{"id":"kuXbjx4yczAL","outputId":"b9426690-8203-4845-ec7d-2f4c892bb4f1","trusted":true},"cell_type":"code","source":"train_df.plot.scatter(x=[\"dt_Month\"], y=['count'], alpha=0.3, color='blue')","execution_count":null,"outputs":[]},{"metadata":{"id":"ycjwTNbEczAL","outputId":"1b43bdf6-dfcd-4e11-9288-289da37f0e69","trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(figsize=(12, 4))\ntrain_df.groupby(train_df[\"dt_hour\"])[\"count\"].mean().plot(kind='bar',rot=0,ax=axs)\nplt.xlabel(\"Hour of the day\");  # custom x label using matplotlib\nplt.ylabel(\"count\");","execution_count":null,"outputs":[]},{"metadata":{"id":"BYiA0Ds7czAL","outputId":"8cebd8a3-4bf5-4854-d0c5-6a3c47343625","trusted":true},"cell_type":"code","source":"#how many uniqe values i got per field\nprint(df.nunique())\n# do we have then a way to translate it into easier to use numbers - normilze \ntrain_df.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"s2E7rGntczAL"},"cell_type":"markdown","source":"## process\nremove field from each column and try to predict its values\n"},{"metadata":{"id":"c2Ege_LlczAL"},"cell_type":"markdown","source":"#process\n- use knn or other unsuprvised method to detect groups\n- translate those groups as feature\n\n"},{"metadata":{"id":"9djl8W7SczAL"},"cell_type":"markdown","source":"# data\n- per month we have max and min sold items\n- temp can be convery by *10 and -min values -> will create int\n"},{"metadata":{"id":"3NrF51TmczAL"},"cell_type":"markdown","source":"#normlize\n- should i normlize per all values or per hour of the or month since at night or day we got diffrent values\n- normlize to geosian or uniformal why the diffrent?\n- running avrage\n"},{"metadata":{"id":"3LnJjn-IczAL"},"cell_type":"markdown","source":"# test vs train\nwhat are the diffrenses between the two?\n"},{"metadata":{"id":"60PLKWCCczAL","outputId":"ff2bbb90-cdcb-4477-d552-064da4d348be","trusted":true},"cell_type":"code","source":"#change season to one hot\nimport sklearn.preprocessing\ndef update_feature_with_convert(df):\n    for x in ['season']:\n        if (x+'_1' not in df):\n            tmp_df = pd.get_dummies(df[x], prefix=x)\n            df = pd.concat([df,tmp_df], axis=1)\n            if (x in df):\n                df = df.drop(x,axis=1)\n    return df\n\ntrain_df = update_feature_with_convert(train_df)\ntest_df = update_feature_with_convert(test_df)\ndf = update_feature_with_convert(df)\n\ndf.info()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_redundent_fetures(df):\n    for field_name in ['atemp','date_hour_idx']:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ntrain_df = remove_redundent_fetures(train_df)\ntest_df = remove_redundent_fetures(test_df)\ndf = remove_redundent_fetures(df)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hINshilOczAL","outputId":"f448509e-7082-42a2-fe66-bb439414fca4","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\nfrom matplotlib.patches import Patch\n\n\n\nn_splits=5\n\n\n# x = train_df.sample(n=1000, weights='count')\nprint(remove_fields)\nX = train_df.drop(remove_fields,axis=1)\ny = train_df['count']\n\nprint(X.head())\nprint(X.shape)\nprint(y.head())\nprint(y.shape)\ntime_splits = TimeSeriesSplit(n_splits=n_splits)\nindex = 1\nfor train_index, test_index in time_splits.split(X):\n    print(\"train:\",[min(train_index),max(train_index)],\" test:\",[min(test_index),max(test_index)] )\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"U3KX5F_cczAL","trusted":true},"cell_type":"code","source":"\ndef get_rmsle(y_pred, y_actual):\n    diff = np.log(y_pred + 1) - np.log(y_actual + 1)\n    mean_error = np.square(diff).mean()\n    return np.sqrt(mean_error)\n\n#Make RLSME Scorer\ndef rmsle(predicted, actual):\n    return np.sqrt(np.square(np.log(predicted + 1) - np.log(actual + 1)).mean())\n\nfrom sklearn.metrics import make_scorer\nscorer= make_scorer(rmsle, greater_is_better=False)\n\n\n#Train some model on the data\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Score_mean\", scores.mean())\n    print(\"Score_std\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"id":"hrUkBBqP2eiU","outputId":"98c804ac-5c75-451d-fc12-623a6f246c77","trusted":true},"cell_type":"code","source":"#XGBoost hyper-parameter tuning\n#https://www.kaggle.com/felipefiorini/xgboost-hyper-parameter-tuning\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1, 1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'reg_alpha' : [0,0.1,1],\n        'reg_lambda' : [0,0.1,1],\n        'objective': ['reg:squarederror'],\n        # 'tree_method': ['gpu_hist']\n        'booster' : ['gbtree', 'gblinear', 'dart']\n    }\n\n    xgb_model = XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           scoring = scorer,\n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = time_splits,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch\n\n# grid_xgb_best = hyperParameterTuning(X, y)\n# xgb_best_est = grid_xgb_best.best_estimator_\n# grid_xgb_best.best_params_\n\n# best_xgb_param = {'colsample_bytree': 0.7,\n#  'learning_rate': 0.01,\n#  'max_depth': 7,\n#  'min_child_weight': 1,\n#  'n_estimators': 500,\n#  'objective': 'reg:squarederror',\n#  'reg_alpha': 1,\n#  'reg_lambda': 1,\n#  'subsample': 0.7};","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best_est = XGBRegressor(\n        colsample_bylevel=0.7,\n        learning_rate= 0.01,\n        max_depth = 7,\n        min_child_weight = 1,\n        n_estimators = 500,\n        objective = 'reg:squarederror',\n        reg_alpha = 1,\n        reg_lambda = 1,\n        subsample = 0.7\n    )","execution_count":null,"outputs":[]},{"metadata":{"id":"uZBshFODczAL","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"X0iOWiwcczAL","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"_5ZHgOJvczAL","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, cross_validate\n\n\nmodel_l = {\n    'lmodel' : LinearRegression(),\n    'xg_boost' : xgb.XGBRegressor(),\n    'xg_boost_best' : xgb_best_est,\n}\nresult={}\n\nyy = list(set(y.values))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=.2, random_state=seed)\nfor m in model_l:\n    print(f\"Start {m}\")\n    lmodel = model_l[m]\n    result[m]={}\n\n    \n    lmodel.fit(X_train, y_train)\n    cv_scores = cross_val_score(lmodel, X, y, cv=time_splits, scoring=scorer)\n    print(\"Cross val:\",cv_scores, cv_scores.mean())\n    result[m]['mean']=cv_scores.mean()\n    idx=0\n    for train_index, test_index in time_splits.split(X):\n        print(\"train:\",[min(train_index),max(train_index)],\" test:\",[min(test_index),max(test_index)] ,\" score:\",cv_scores[idx])\n        idx+=1\n\n    if (m in ['lmodel']):\n        # print the coefficients\n        print(\"itercept:\",lmodel.intercept_)\n        for x_idx, x_col_name in enumerate(X_train.columns):\n            print(f\"itercept {x_col_name:20} {round(lmodel.coef_[x_idx],5):10}\")\n    if (m in ['xg_boost', 'xg_best_est']):\n        for x_idx, feature_imp in zip(X_train.columns, lmodel.feature_importances_):\n            print(f\"itercept {x_idx:20} {round(feature_imp,5):10}\")\n        \n    # for train_index, test_index in splits.split(X):\n    #     print(\"train:\",[min(train_index),max(train_index)],\" test:\",[min(test_index),max(test_index)] )\n\n    lmodel.fit(X_train, y_train)\n    pre_train = lmodel.predict(X_train)\n    pre_test = lmodel.predict(X_test)\n    print(\"rmsle train:\",rmsle(y_train, pre_train))\n    result[m]['lrmse_train']=rmsle(y_train, pre_train)\n    print(\"rmsle test:\",rmsle(y_test, pre_test))\n    result[m]['lrmse']=rmsle(y_test, pre_test)\nprint(result)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dx0sWESHczAL","trusted":true},"cell_type":"code","source":"\n# Machine learning pipeline\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom xgboost.sklearn import XGBClassifier\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"gVHWmFupczAL","trusted":true},"cell_type":"code","source":"def create_baseline_classifiers(seed=1):\n    \"\"\"Create a list of baseline classifiers.\n    \n    Parameters\n    ----------\n    seed: (optional) An integer to set seed for reproducibility\n\n    Returns\n    -------\n    A list containing tuple of name, model object for each of these algortihms:\n    DummyClassifier, LogisticRegression, SGDClassifier, ExtraTreesClassifier, \n    GradientBoostingClassifier, RandomForestClassifier, MultinomialNB, SVC, \n    XGBClassifier.\n    \n    \"\"\"\n    models = []\n    models.append(('dum', DummyClassifier(random_state=seed, strategy='most_frequent')))\n    models.append(('log', LogisticRegression(random_state=seed)))\n    models.append(('sgd', SGDClassifier(random_state=seed)))\n    models.append(('etc', ExtraTreesClassifier(random_state=seed)))\n    models.append(('gbm', GradientBoostingClassifier(random_state=seed)))\n    models.append(('rfc', RandomForestClassifier(random_state=seed)))\n    models.append(('mnb', MultinomialNB()))\n    models.append(('svc', SVC(random_state=seed, probability=True)))\n    models.append(('xgb', XGBClassifier(seed=seed)))\n    return models\n\ndef assess_models(X, y, models, cv=5, metrics=['roc_auc', 'f1'],scorer={}, plot=0):\n    \"\"\"Provide summary of cross validation results for models.\n    \n    Parameters\n    ----------\n    X: A pandas DataFrame containing feature matrix\n    y: A pandas Series containing target vector\n    models: A list of models to train\n    cv: (optional) An integer to set number of folds in cross-validation\n    metrics: (optional) A list of scoring metrics or a string for a metric\n\n    Returns\n    -------\n    A pandas DataFrame containing summary of baseline models' performance.\n    \n    \"\"\"\n    summary = pd.DataFrame()\n    for name, model in models:\n        cross_validate_val = cross_validate(model, X, y, cv=cv, scoring=metrics)\n        # print(\"1 cross_validate_val:\\n\",cross_validate_val)\n        for scorer_name in scorer:\n            result1 = cross_val_score(model, X, y, cv=time_splits, scoring=scorer[scorer_name])\n            cross_validate_val[scorer_name] = result1\n            # print(\"result1:\",result1)\n        # print(\"2 cross_validate_val:\\n\",cross_validate_val)\n        result = pd.DataFrame(cross_validate_val)\n        mean = result.mean().rename('{}_mean'.format)\n        std = result.std().rename('{}_std'.format)\n        min1 = result.min().rename('{}_min'.format)\n        max1 = result.max().rename('{}_max'.format)\n\n        summary[name] = pd.concat([mean, std, max1, min1], axis=0)\n\n        #plot real vs predicted values if the plot boolean is true\n        if (plot):\n            plt.figure(figsize=(5, 5))\n            plt.scatter(y_test, y_predicted)\n            plt.ylabel(\"predicted values\")\n            plt.xlabel(\"real values\")\n            #find min and max in the test target values\n            minimum = math.ceil(y_test.min())\n            maximum = math.ceil(y_test.max())\n            #plot a diagonal line accross the scattered plot to better see the difference in values\n            #idealy all points should be on the diagonal line\n            plt.plot( [minimum,maximum],[minimum,maximum], color='red')\n\n\n    return summary.sort_index()\n\ndef extract_metric(summary, metric):\n    \"\"\"Provide summary of baseline models' performance for a metric.\n    \n    Parameters\n    ----------\n    summary: A pandas DataFrame containing the summary of baseline models\n    metric: A string specifying the name of the metric to extract info\n    \n    Returns\n    -------\n    A pandas DataFrame containing mean, standard deviation, lower and upper\n    bound of the baseline models' performance in cross validation according to\n    the metric specified.\n    \n    \"\"\"\n    output = summary[summary.index.str.contains(metric)].T\n    output.columns = output.columns.str.replace(f'test_{metric}_', '')\n    output.sort_values(by='mean', ascending=False, inplace=True)\n    output['lower'] = output['mean'] - 2*output['std']\n    output['upper'] = output['mean'] + 2*output['std']\n    return output\n","execution_count":null,"outputs":[]},{"metadata":{"id":"xOkMLXxr0D3c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"i5cI-XomczAM","trusted":true},"cell_type":"code","source":"# Set target\ntarget = 'count'\nfeatures = X.columns\n\n# Split data into train & test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=.2, random_state=seed)\n\n# Inspect data\nprint(f\"Training data ({X_train.shape[0]} rows): Target distribution\")\n# print(y_train.value_counts(normalize=True))\nprint(f\"\\nTest data ({X_test.shape[0]} rows): Target distribution\")\n# print(y_train.value_counts(normalize=True))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vazkptfcczAM","trusted":true},"cell_type":"code","source":"# models = create_baseline_classifiers(seed=seed)\n# summary = assess_models(X_train, y_train, models, cv=time_splits, metrics=['r2','neg_mean_absolute_error','neg_root_mean_squared_error','neg_mean_squared_log_error'])\n# summary","execution_count":null,"outputs":[]},{"metadata":{"id":"W9D-Mjb0kWEi","trusted":true},"cell_type":"code","source":"\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost.sklearn import XGBRegressor\n\ndef create_baseline_regressors(seed=8):\n    \"\"\"Create a list of of baseline regressors.\n    \n    Parameters\n    ----------\n    seed: (optional) An integer to set seed for reproducibility\n\n    Returns\n    -------\n    A list containing tuple of name, model object for each of these algortihms:\n    DummyRegressor, LinearRegression, SGDRegressor, ExtraTreesRegressor,\n    GradientBoostingRegressor, RandomForestRegressor, SVR, XGBRegressor.\n    \n    \"\"\"\n    models = []\n    models.append(('dum', DummyRegressor(strategy='mean')))\n    models.append(('ols', LinearRegression()))\n    models.append(('sgd', SGDRegressor(random_state=seed)))\n    models.append(('etr', ExtraTreesRegressor(random_state=seed)))\n    models.append(('gbm', GradientBoostingRegressor(random_state=seed)))\n    models.append(('rfr', RandomForestRegressor(random_state=seed)))\n    models.append(('svc', SVR()))\n    models.append(('xgb', XGBRegressor(seed=seed)))\n    models.append(('xgb_best', xgb_best_est))\n    return models\n\n\nmodels_reg = create_baseline_regressors(seed=seed)\nsummary_reg = assess_models(X_train, y_train, models_reg, cv=time_splits, metrics=['max_error','r2','neg_mean_absolute_error','neg_median_absolute_error'], scorer={'LRSME':scorer})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_reg","execution_count":null,"outputs":[]},{"metadata":{"id":"aPHZcItS_AWf","trusted":true},"cell_type":"code","source":"clf_rf = xgb_best_est\nclf_rf.fit(X,y)\npred=clf_rf.predict(test_df.drop(['datetime', 'dt_datetime'],axis=1))\n\nfor idx,p in enumerate(pred):\n    if (p < 0):\n        print(idx,p)\n        print(test_df.iloc[idx])\n        p=0\n\nd={'datetime':test_df['datetime'],'count':abs(pred)}\nans=pd.DataFrame(d)\nans.to_csv('submission.csv',index=False) # saving to a csv file for predictions.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}