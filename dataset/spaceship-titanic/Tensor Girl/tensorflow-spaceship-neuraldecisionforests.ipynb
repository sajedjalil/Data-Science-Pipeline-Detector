{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://drive.google.com/uc?id=1e5ZF3Jmap6S2EAM80GrftyBIWvsOqPcQ)\n\nWelcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n\nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destinationâ€”the torrid 55 Cancri Eâ€”the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\n# **<span style=\"color:#F7B2B0;\">Goal</span>**\n\nThe goal of this competition is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly.\n\n# **<span style=\"color:#F7B2B0;\">Data</span>**\n\n\n**Files**\n\n`train.csv` -  Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n\n`test.csv` - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set. \n\n`sample_submission.csv` - a sample submission file in the correct format\n\n**Columns**\n\n`PassengerId` - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n\n`HomePlanet` - The planet the passenger departed from, typically their planet of permanent residence.\n\n`CryoSleep` - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n\n`Cabin` - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n\n`Destination` - The planet the passenger will be debarking to.\n\n`Age` - The age of the passenger.\n\n`VIP` - Whether the passenger has paid for special VIP service during the voyage.\n\n`RoomService, FoodCourt, ShoppingMall, Spa, VRDeck` - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n\n`Name` - The first and last names of the passenger.\n\n`Transported` - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\n\n# **<span style=\"color:#F7B2B0;\">Evaluation Metric</span>**\n\nSubmissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct.\n\n<img src=\"https://camo.githubusercontent.com/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [Space Titanic project on W&B Dashboard](https://wandb.ai/usharengaraju/SpaceTitanic)\n> \n> - To get the API key, create an account in the [website](https://wandb.ai/site) .\n> - Use secrets to use API Keys more securely \n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\nimport missingno as msno\nimport shap\n\nimport wandb\n\nfrom tensorflow.keras.layers import StringLookup\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport math\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_curve,auc, confusion_matrix,precision_recall_curve,precision_recall_curve,plot_precision_recall_curve","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:24.449349Z","iopub.execute_input":"2022-02-25T01:27:24.449693Z","iopub.status.idle":"2022-02-25T01:27:33.015079Z","shell.execute_reply.started":"2022-02-25T01:27:24.449583Z","shell.execute_reply":"2022-02-25T01:27:33.014211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n    \nCONFIG = dict(competition = 'SpaceTitanic',_wandb_kernel = 'tensorgirl')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-25T01:27:33.016865Z","iopub.execute_input":"2022-02-25T01:27:33.017067Z","iopub.status.idle":"2022-02-25T01:27:33.70844Z","shell.execute_reply.started":"2022-02-25T01:27:33.017042Z","shell.execute_reply":"2022-02-25T01:27:33.707753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\ndf_test = pd.read_csv(\"../input/spaceship-titanic/test.csv\")\nNUMERIC_FEATURE_NAMES = [\n    \"Age\",\n    \"RoomService\",\n    \"FoodCourt\",\n    \"ShoppingMall\",\n    \"Spa\",\n    \"VRDeck\",\n]\n\nCATEGORICAL_FEATURE_NAMES1 = [\"HomePlanet\",\"CryoSleep\",\"Destination\",\"VIP\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:33.709583Z","iopub.execute_input":"2022-02-25T01:27:33.710344Z","iopub.status.idle":"2022-02-25T01:27:33.789419Z","shell.execute_reply.started":"2022-02-25T01:27:33.710294Z","shell.execute_reply":"2022-02-25T01:27:33.788802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head().style.background_gradient(cmap=\"Pastel1\")","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:33.790937Z","iopub.execute_input":"2022-02-25T01:27:33.791277Z","iopub.status.idle":"2022-02-25T01:27:33.877413Z","shell.execute_reply.started":"2022-02-25T01:27:33.79125Z","shell.execute_reply":"2022-02-25T01:27:33.876622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Missing Values</span>**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (25,11))\nsns.heatmap(df_train.isna().values, cmap = ['#fef9ef','#fe6d73'], xticklabels=df_train.columns)\nplt.title(\"Missing values in training Data\", size=20);","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:33.878408Z","iopub.execute_input":"2022-02-25T01:27:33.878611Z","iopub.status.idle":"2022-02-25T01:27:35.120261Z","shell.execute_reply.started":"2022-02-25T01:27:33.878584Z","shell.execute_reply":"2022-02-25T01:27:35.119473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_columns = [col for col in df_train.columns if df_train[col].isnull().any()]\nmissingvalues_count =df_train.isna().sum()\nmissingValues_df = pd.DataFrame(missingvalues_count.rename('Null Values Count')).loc[missingvalues_count.ne(0)]\nmissingValues_df .style.background_gradient(cmap=\"Pastel1\")","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:35.121635Z","iopub.execute_input":"2022-02-25T01:27:35.121984Z","iopub.status.idle":"2022-02-25T01:27:35.146198Z","shell.execute_reply.started":"2022-02-25T01:27:35.121956Z","shell.execute_reply":"2022-02-25T01:27:35.145392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic stats of features\ndf_train.describe().style.background_gradient(cmap=\"Pastel1\")","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:35.147295Z","iopub.execute_input":"2022-02-25T01:27:35.147583Z","iopub.status.idle":"2022-02-25T01:27:35.182911Z","shell.execute_reply.started":"2022-02-25T01:27:35.147552Z","shell.execute_reply":"2022-02-25T01:27:35.182103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">Target variable distribution</span>**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15, 7))    \nsns.countplot(y = df_train['Transported'], color = '#fe6d73')        \nplt.title(\"Target Distribution\")    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:35.184271Z","iopub.execute_input":"2022-02-25T01:27:35.184719Z","iopub.status.idle":"2022-02-25T01:27:35.301599Z","shell.execute_reply.started":"2022-02-25T01:27:35.184678Z","shell.execute_reply":"2022-02-25T01:27:35.300794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#e76f51;\">Numerical Feature Distribution</span>**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(2,3, figsize=(18, 18))\nfor i, feature in enumerate(NUMERIC_FEATURE_NAMES):\n    sns.distplot(df_train[feature], color = '#fe6d73', ax=ax[math.floor(i/3),i%3]).set_title(f'{feature} Distribution')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:35.303236Z","iopub.execute_input":"2022-02-25T01:27:35.304177Z","iopub.status.idle":"2022-02-25T01:27:37.007424Z","shell.execute_reply.started":"2022-02-25T01:27:35.304125Z","shell.execute_reply":"2022-02-25T01:27:37.006493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#e76f51;\">Categorical Feature Distribution</span>**","metadata":{}},{"cell_type":"code","source":"def countplot_features(df_train, feature, title):\n    '''Takes a column from the dataframe and plots the distribution (after count).'''    \n           \n    plt.figure(figsize = (15, 7))\n    \n    sns.countplot(df_train[feature], color = '#fe6d73')\n        \n    plt.title(title, fontsize=15)\n    plt.xticks(rotation=45)\n    plt.show();\n\n# plot distributions of categorical features\nfor feature in CATEGORICAL_FEATURE_NAMES1:\n    fig = countplot_features(df_train, feature=feature, title = \"Frequency of \"+ feature)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:37.009671Z","iopub.execute_input":"2022-02-25T01:27:37.009914Z","iopub.status.idle":"2022-02-25T01:27:37.723629Z","shell.execute_reply.started":"2022-02-25T01:27:37.009884Z","shell.execute_reply":"2022-02-25T01:27:37.722777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#e76f51;\">Correlation of Features</span>**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25, 9))\nsns.heatmap(df_train[[f'{feature}' for feature in df_train.columns]].corr(),annot=True ,cmap = \"Pastel1\")","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:37.724672Z","iopub.execute_input":"2022-02-25T01:27:37.724866Z","iopub.status.idle":"2022-02-25T01:27:38.257396Z","shell.execute_reply.started":"2022-02-25T01:27:37.724842Z","shell.execute_reply":"2022-02-25T01:27:38.25658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#e76f51;\">Logging Plots to Weights and Biases</span>**","metadata":{}},{"cell_type":"code","source":"def create_wandb_hist(x_data=None, x_name=None, title=None, log=None):\n    '''Create and save histogram in W&B Environment.\n    x_data: Pandas Series containing x values\n    x_name: strings containing axis name\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[x] for x in x_data]\n    table = wandb.Table(data=data, columns=[x_name])\n    wandb.log({log : wandb.plot.histogram(table, x_name, title=title)})\n\n# Log Plots to W&B environment\ntitle = \"Distribution of Numerical features\"\nrun = wandb.init(project='SpaceTitanic', name=title,anonymous=anony,config=CONFIG)\nfor feature in NUMERIC_FEATURE_NAMES:\n    if feature != \"Transported\":\n        title = \"Distribution of Numerical \"+feature    \n        create_wandb_hist(x_data=df_train[feature],x_name=feature , title=title,log=\"hist\")    \nwandb.finish()\n\ntitle = \"Countplot Distribution\"\nrun = wandb.init(project='SpaceTitanic', name=title,anonymous=anony,config=CONFIG)    \nfor feature in CATEGORICAL_FEATURE_NAMES1:\n    #fig = countplot_features(train, feature=feature, title = feature + \" countplot distribution\")\n    wandb.log({feature + \" countplot distribution\": fig})\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:28:43.504377Z","iopub.execute_input":"2022-02-25T01:28:43.504705Z","iopub.status.idle":"2022-02-25T01:29:16.563836Z","shell.execute_reply.started":"2022-02-25T01:28:43.504672Z","shell.execute_reply":"2022-02-25T01:29:16.562752Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#e76f51;\">Preprocessing</span>**","metadata":{}},{"cell_type":"code","source":"df_train.drop(['PassengerId', 'Cabin','Name','CryoSleep','VIP'],axis =1 , inplace=True)\ndf_test.drop(['PassengerId', 'Cabin','Name','CryoSleep','VIP'],axis = 1 ,inplace=True)\ndf_train.Transported.replace([True,False], [\"1\", \"0\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.157194Z","iopub.status.idle":"2022-02-25T01:27:45.157972Z","shell.execute_reply.started":"2022-02-25T01:27:45.157694Z","shell.execute_reply":"2022-02-25T01:27:45.157724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"Age\"].fillna(df_train[\"Age\"].mean(skipna=True), inplace=True)\ndf_test[\"Age\"].fillna(df_test[\"Age\"].mean(skipna=True), inplace=True)\n\ndf_train[\"RoomService\"].fillna(df_train[\"RoomService\"].mean(skipna=True), inplace=True)\ndf_test[\"RoomService\"].fillna(df_test[\"RoomService\"].mean(skipna=True), inplace=True)\n\ndf_train[\"FoodCourt\"].fillna(df_train[\"FoodCourt\"].mean(skipna=True), inplace=True)\ndf_test[\"FoodCourt\"].fillna(df_test[\"FoodCourt\"].mean(skipna=True), inplace=True)\n\ndf_train[\"ShoppingMall\"].fillna(df_train[\"ShoppingMall\"].mean(skipna=True), inplace=True)\ndf_test[\"ShoppingMall\"].fillna(df_test[\"ShoppingMall\"].mean(skipna=True), inplace=True)\n\ndf_train[\"Spa\"].fillna(df_train[\"Spa\"].mean(skipna=True), inplace=True)\ndf_test[\"Spa\"].fillna(df_test[\"Spa\"].mean(skipna=True), inplace=True)\n\ndf_train[\"VRDeck\"].fillna(df_train[\"VRDeck\"].mean(skipna=True), inplace=True)\ndf_test[\"VRDeck\"].fillna(df_test[\"VRDeck\"].mean(skipna=True), inplace=True)\n\n\ndf_train[\"HomePlanet\"].fillna('Kepler', inplace=True)\ndf_test[\"HomePlanet\"].fillna('Kepler', inplace=True)\n\ndf_train[\"Destination\"].fillna('TRAPPIST-1e', inplace=True)\ndf_test[\"Destination\"].fillna('TRAPPIST-1e', inplace=True)\n\ndf_train.dropna(inplace = True)\ndf_test.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.15934Z","iopub.status.idle":"2022-02-25T01:27:45.160261Z","shell.execute_reply.started":"2022-02-25T01:27:45.159972Z","shell.execute_reply":"2022-02-25T01:27:45.160001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CSV_HEADER = [\n    \"HomePlanet\",\n    \"Destination\",\n    \"Age\",\n    \"RoomService\",\n    \"FoodCourt\",\n    \"ShoppingMall\",\n    \"Spa\",\n    \"VRDeck\",\n    \"Transported\",\n    ]\n\ntrain_data_file = \"train_data.csv\"\ntest_data_file = \"test_data.csv\"\n\ndf_train.to_csv(train_data_file, index=False, header=False)\ndf_test.to_csv(test_data_file, index=False, header=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.161444Z","iopub.status.idle":"2022-02-25T01:27:45.162294Z","shell.execute_reply.started":"2022-02-25T01:27:45.16203Z","shell.execute_reply":"2022-02-25T01:27:45.162057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#e76f51;\">W & B Artifacts</span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https://docs.wandb.ai/guides/artifacts)\n\n![](https://drive.google.com/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","metadata":{}},{"cell_type":"code","source":"# Save train data to W&B Artifacts\nrun = wandb.init(project='SpaceTitanic', name='training_data', anonymous=anony,config=CONFIG) \nartifact = wandb.Artifact(name='training_data',type='dataset')\nartifact.add_file(\"./train_data.csv\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-25T01:27:45.163533Z","iopub.status.idle":"2022-02-25T01:27:45.16419Z","shell.execute_reply.started":"2022-02-25T01:27:45.163916Z","shell.execute_reply":"2022-02-25T01:27:45.163944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A list of the numerical feature names.\nNUMERIC_FEATURE_NAMES = [\n    \"Age\",\n    \"RoomService\",\n    \"FoodCourt\",\n    \"ShoppingMall\",\n    \"Spa\",\n    \"VRDeck\",\n]\n# A dictionary of the categorical features and their vocabulary.\nCATEGORICAL_FEATURES_WITH_VOCABULARY = {\n    \"HomePlanet\": sorted(list(df_train[\"HomePlanet\"].unique())),\n    \"Destination\": sorted(list(df_train[\"Destination\"].unique())),\n        \n}\n\n# A list of the categorical feature names.\nCATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n\n# A list of all the input features.\nFEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n\n# The name of the target feature.\nTARGET_FEATURE_NAME = \"Transported\"\n\n# A list of the labels of the target features.\nTARGET_LABELS = [\"0\", \"1\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.166014Z","iopub.status.idle":"2022-02-25T01:27:45.166599Z","shell.execute_reply.started":"2022-02-25T01:27:45.166358Z","shell.execute_reply":"2022-02-25T01:27:45.166382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## **<span style=\"color:#e76f51;\">ðŸŽ¯tf.data</span>**\n\ntf.data API is used for building efficient input pipelines which can handle large amounts of data and perform complex data transformations . tf.data API has provisions for handling different data formats .\n\n\n## **<span style=\"color:#e76f51;\">ðŸŽ¯tf.data.Dataset</span>**\n\ntf.data.Dataset is an abstraction introduced by tf.data API and consists of sequence of elements where each element has one or more components . For example , in a tabular data pipeline , an element might be a single training example , with a pair of tensor components representing the input features and its label\n\ntf.data.Dataset can be created using two distinct ways\n\nConstructing a dataset using data stored in memory by a data source\n\nConstructing a dataset from one or more tf.data.Dataset objects by a data transformation","metadata":{}},{"cell_type":"code","source":"def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n    dataset = tf.data.experimental.make_csv_dataset(\n        csv_file_path,\n        batch_size=batch_size,\n        column_names=CSV_HEADER,\n        label_name=TARGET_FEATURE_NAME,\n        num_epochs=1,\n        shuffle=shuffle\n    )\n    return dataset.cache()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.167581Z","iopub.status.idle":"2022-02-25T01:27:45.168452Z","shell.execute_reply.started":"2022-02-25T01:27:45.168181Z","shell.execute_reply":"2022-02-25T01:27:45.168209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#e76f51;\">ðŸŽ¯Model Inputs</span>**","metadata":{}},{"cell_type":"code","source":"def create_model_inputs():\n    inputs = {}\n    for feature_name in FEATURE_NAMES:\n        if feature_name in NUMERIC_FEATURE_NAMES:\n            inputs[feature_name] = layers.Input(\n                name=feature_name, shape=(), dtype=tf.float32\n            )\n        else:\n            inputs[feature_name] = layers.Input(\n                name=feature_name, shape=(), dtype=tf.string\n            )\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.169517Z","iopub.status.idle":"2022-02-25T01:27:45.170104Z","shell.execute_reply.started":"2022-02-25T01:27:45.169852Z","shell.execute_reply":"2022-02-25T01:27:45.169879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **<span style=\"color:#e76f51;\">Feature representation using Keras Preprocessing Layers</span>**\n\nFeature representations can be one of the crucial aspect in model developement workflows . It is a experimental process and there is no perfect solution . Keras preprocessing Layers helps us create more flexible preprocessing pipeline where new data transformations can be applied while changing the model architecture .\n\n![](https://drive.google.com/uc?id=1248y8JYTwjnxZnIEaTQHr1xV5jUZotLm)\n\n[ImageSource](https://blog.tensorflow.org/2021/11/an-introduction-to-keras-preprocessing.html)\n\n## **<span style=\"color:#e76f51;\">Keras Preprocessing Layers - Numerical Features</span>**\n\nThe Keras preprocessing layers available for numerical features are below \n\n`tf.keras.layers.Normalization`: performs feature-wise normalization of input features.\n  \n`tf.keras.layers.Discretization`: turns continuous numerical features into integer categorical features.\n\n`adapt():`\n\nAdapt is an optional utility function which helps in setting the internal state of layers from input data . adapt() is available on all stateful processing layerrs and it computes mean and variance for the layerrs and stores them as layers weights . adapt() is called before fit() , evaluate or predict()\n\n\n## **<span style=\"color:#e76f51;\">Keras Preprocessing Layers - Categorical Features</span>**\n\nThe various keras preprocessing layers available for categorical variables are below .\n\n`tf.keras.layers.CategoryEncoding:` turns integer categorical features into one-hot, multi-hot, or count dense representations.\n\n`tf.keras.layers.Hashing:` performs categorical feature hashing, also known as the \"hashing trick\".\n\n`tf.keras.layers.StringLookup:` turns string categorical values an encoded representation that can be read by an Embedding layer or Dense layer.\n\n`tf.keras.layers.IntegerLookup:` turns integer categorical values into an encoded representation that can be read by an Embedding layer or Dense layer.","metadata":{}},{"cell_type":"code","source":"def encode_inputs(inputs):\n    encoded_features = []\n    for feature_name in inputs:\n        if feature_name in CATEGORICAL_FEATURE_NAMES:\n            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n            # Create a lookup to convert a string values to an integer indices.\n            # Since we are not using a mask token, nor expecting any out of vocabulary\n            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n            lookup = StringLookup(\n                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n            )\n            # Convert the string input values into integer indices.\n            value_index = lookup(inputs[feature_name])\n            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n            # Create an embedding layer with the specified dimensions.\n            embedding = layers.Embedding(\n                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n            )\n            # Convert the index values to embedding representations.\n            encoded_feature = embedding(value_index)\n        else:\n            # Use the numerical features as-is.\n            encoded_feature = inputs[feature_name]\n            if inputs[feature_name].shape[-1] is None:\n                encoded_feature = tf.expand_dims(encoded_feature, -1)\n\n        encoded_features.append(encoded_feature)\n\n    encoded_features = layers.concatenate(encoded_features)\n    return encoded_features","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.171546Z","iopub.status.idle":"2022-02-25T01:27:45.172011Z","shell.execute_reply.started":"2022-02-25T01:27:45.171763Z","shell.execute_reply":"2022-02-25T01:27:45.171787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NeuralDecisionTree(keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(NeuralDecisionTree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2 ** depth\n        self.num_classes = num_classes\n\n        # Create a mask for the randomly selected features.\n        num_used_features = int(num_features * used_features_rate)\n        one_hot = np.eye(num_features)\n        sampled_feature_indicies = np.random.choice(\n            np.arange(num_features), num_used_features, replace=False\n        )\n        self.used_features_mask = one_hot[sampled_feature_indicies]\n\n        # Initialize the weights of the classes in leaves.\n        self.pi = tf.Variable(\n            initial_value=tf.random_normal_initializer()(\n                shape=[self.num_leaves, self.num_classes]\n            ),\n            dtype=\"float32\",\n            trainable=True,\n        )\n\n        # Initialize the stochastic routing layer.\n        self.decision_fn = layers.Dense(\n            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n        )\n\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n\n        # Apply the feature mask to the input features.\n        features = tf.matmul(\n            features, self.used_features_mask, transpose_b=True\n        )  # [batch_size, num_used_features]\n        # Compute the routing probabilities.\n        decisions = tf.expand_dims(\n            self.decision_fn(features), axis=2\n        )  # [batch_size, num_leaves, 1]\n        # Concatenate the routing probabilities with their complements.\n        decisions = layers.concatenate(\n            [decisions, 1 - decisions], axis=2\n        )  # [batch_size, num_leaves, 2]\n\n        mu = tf.ones([batch_size, 1, 1])\n\n        begin_idx = 1\n        end_idx = 2\n        # Traverse the tree in breadth-first order.\n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n            level_decisions = decisions[\n                :, begin_idx:end_idx, :\n            ]  # [batch_size, 2 ** level, 2]\n            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n            begin_idx = end_idx\n            end_idx = begin_idx + 2 ** (level + 1)\n\n        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.174039Z","iopub.status.idle":"2022-02-25T01:27:45.174756Z","shell.execute_reply.started":"2022-02-25T01:27:45.174479Z","shell.execute_reply":"2022-02-25T01:27:45.174505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NeuralDecisionForest(keras.Model):\n    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n        super(NeuralDecisionForest, self).__init__()\n        self.ensemble = []\n        # Initialize the ensemble by adding NeuralDecisionTree instances.\n        # Each tree will have its own randomly selected input features to use.\n        for _ in range(num_trees):\n            self.ensemble.append(\n                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n            )\n\n    def call(self, inputs):\n        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n        batch_size = tf.shape(inputs)[0]\n        outputs = tf.zeros([batch_size, num_classes])\n\n        # Aggregate the outputs of trees in the ensemble.\n        for tree in self.ensemble:\n            outputs += tree(inputs)\n        # Divide the outputs by the ensemble size to get the average.\n        outputs /= len(self.ensemble)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.175755Z","iopub.status.idle":"2022-02-25T01:27:45.176624Z","shell.execute_reply.started":"2022-02-25T01:27:45.176353Z","shell.execute_reply":"2022-02-25T01:27:45.176382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.01\nbatch_size = 265\nnum_epochs = 10\nhidden_units = [64, 64]\nnum_trees = 25\ndepth = 5\nused_features_rate = 0.5\nnum_classes = len(TARGET_LABELS)\n\n\ndef run_experiment(model):\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    )\n\n    print(\"Start training the model...\")\n    train_dataset = get_dataset_from_csv(\n        train_data_file, shuffle=True, batch_size=batch_size\n    )\n\n    model.fit(train_dataset, epochs=num_epochs)\n    print(\"Model training finished\")\n\n  ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.177969Z","iopub.status.idle":"2022-02-25T01:27:45.178848Z","shell.execute_reply.started":"2022-02-25T01:27:45.178556Z","shell.execute_reply":"2022-02-25T01:27:45.178586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef create_forest_model():\n    inputs = create_model_inputs()\n    features = encode_inputs(inputs)\n    features = layers.BatchNormalization()(features)\n    num_features = features.shape[1]\n\n    forest_model = NeuralDecisionForest(\n        num_trees, depth, num_features, used_features_rate, num_classes\n    )\n\n    outputs = forest_model(features)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\nforest_model = create_forest_model()\nrun_experiment(forest_model)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-25T01:27:45.179999Z","iopub.status.idle":"2022-02-25T01:27:45.180847Z","shell.execute_reply.started":"2022-02-25T01:27:45.18056Z","shell.execute_reply":"2022-02-25T01:27:45.180587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span style=\"color:#F7B2B0;\">References</span>**\n\n[Deep Neural Decision Forests](https://github.com/keras-team/keras-io/blob/master/examples/structured_data/deep_neural_decision_forests.py)","metadata":{}},{"cell_type":"markdown","source":"# Work in progress ðŸš§","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}