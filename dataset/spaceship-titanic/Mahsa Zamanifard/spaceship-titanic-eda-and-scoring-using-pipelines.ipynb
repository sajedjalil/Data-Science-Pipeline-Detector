{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In V8:\n\n**EDA:**\nMore in depth EDA and visualization\n\n**Outliers:**\n\n1-removing rows with 5 or more outliers\n\n**Missing values:**\n\n1-Using IterativeImputer to impute missing values\n\n2-Using Target Encoding for categorical features\n\n\n**Scaling:**\n\n1-PowerTransformer\n\n**Models:**\n\n1-A voting classifier with LGBM,XGBC,GBC","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-30T09:55:50.514711Z","iopub.execute_input":"2022-06-30T09:55:50.515114Z","iopub.status.idle":"2022-06-30T09:55:50.532157Z","shell.execute_reply.started":"2022-06-30T09:55:50.515084Z","shell.execute_reply":"2022-06-30T09:55:50.531163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Load The Data","metadata":{}},{"cell_type":"markdown","source":"## 1-1) Import Libraries","metadata":{}},{"cell_type":"code","source":"# Load libraries\nfrom pandas import read_csv\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport missingno as mno","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:50.748885Z","iopub.execute_input":"2022-06-30T09:55:50.749286Z","iopub.status.idle":"2022-06-30T09:55:50.757183Z","shell.execute_reply.started":"2022-06-30T09:55:50.749254Z","shell.execute_reply":"2022-06-30T09:55:50.755859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1-2) Load Dataset","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/spaceship-titanic/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/spaceship-titanic/test.csv\")\nsample=pd.read_csv(\"/kaggle/input/spaceship-titanic/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:50.968095Z","iopub.execute_input":"2022-06-30T09:55:50.968938Z","iopub.status.idle":"2022-06-30T09:55:51.015985Z","shell.execute_reply.started":"2022-06-30T09:55:50.968898Z","shell.execute_reply":"2022-06-30T09:55:51.014935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set=train.copy()\ntest_set=test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:51.363245Z","iopub.execute_input":"2022-06-30T09:55:51.363951Z","iopub.status.idle":"2022-06-30T09:55:51.369879Z","shell.execute_reply.started":"2022-06-30T09:55:51.36391Z","shell.execute_reply":"2022-06-30T09:55:51.36875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2) Summarize the Dataset","metadata":{}},{"cell_type":"markdown","source":"## 2-1) Dimensions of the Dataset","metadata":{}},{"cell_type":"code","source":"train_set.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:51.589066Z","iopub.execute_input":"2022-06-30T09:55:51.589973Z","iopub.status.idle":"2022-06-30T09:55:51.596813Z","shell.execute_reply.started":"2022-06-30T09:55:51.589925Z","shell.execute_reply":"2022-06-30T09:55:51.595679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:51.795493Z","iopub.execute_input":"2022-06-30T09:55:51.795925Z","iopub.status.idle":"2022-06-30T09:55:51.803818Z","shell.execute_reply.started":"2022-06-30T09:55:51.795892Z","shell.execute_reply":"2022-06-30T09:55:51.8029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-2) Peek at the Data","metadata":{}},{"cell_type":"code","source":"train_set.head(10) ","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:52.071214Z","iopub.execute_input":"2022-06-30T09:55:52.071925Z","iopub.status.idle":"2022-06-30T09:55:52.096432Z","shell.execute_reply.started":"2022-06-30T09:55:52.071886Z","shell.execute_reply":"2022-06-30T09:55:52.094771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:52.310382Z","iopub.execute_input":"2022-06-30T09:55:52.311066Z","iopub.status.idle":"2022-06-30T09:55:52.328236Z","shell.execute_reply.started":"2022-06-30T09:55:52.311027Z","shell.execute_reply":"2022-06-30T09:55:52.326697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2-3) Statistical Summary:","metadata":{}},{"cell_type":"code","source":"train_set.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:52.522466Z","iopub.execute_input":"2022-06-30T09:55:52.523108Z","iopub.status.idle":"2022-06-30T09:55:52.556028Z","shell.execute_reply.started":"2022-06-30T09:55:52.523076Z","shell.execute_reply":"2022-06-30T09:55:52.555132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that:\n* the numeric features are on different scales\n* there are some missing values","metadata":{}},{"cell_type":"code","source":"train_set.Transported.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:52.771946Z","iopub.execute_input":"2022-06-30T09:55:52.772382Z","iopub.status.idle":"2022-06-30T09:55:52.784154Z","shell.execute_reply.started":"2022-06-30T09:55:52.772345Z","shell.execute_reply":"2022-06-30T09:55:52.782902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see the distribution is almost equal","metadata":{}},{"cell_type":"markdown","source":"# 3) Data Visualization","metadata":{}},{"cell_type":"markdown","source":"We are going to look at two types of plots:\n*  Univariate plots to better understand each attribute.\n*  Multivariate plots to better understand the relationships between attributes.","metadata":{}},{"cell_type":"markdown","source":"## 3-1) Univariate Plots","metadata":{}},{"cell_type":"markdown","source":"### 3-1-1) Univariate Plots for Numerical Features","metadata":{}},{"cell_type":"code","source":"#this will create a box plot for numeric values\ntrain_set.plot(kind='box',subplots=True,layout=(2,3),sharex=False,sharey=False,figsize=(15,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:52.991723Z","iopub.execute_input":"2022-06-30T09:55:52.992446Z","iopub.status.idle":"2022-06-30T09:55:53.678854Z","shell.execute_reply.started":"2022-06-30T09:55:52.992394Z","shell.execute_reply":"2022-06-30T09:55:53.677937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this will create a historgram for numeric values\ntrain_set.hist(figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:53.680368Z","iopub.execute_input":"2022-06-30T09:55:53.680856Z","iopub.status.idle":"2022-06-30T09:55:54.651654Z","shell.execute_reply.started":"2022-06-30T09:55:53.680825Z","shell.execute_reply":"2022-06-30T09:55:54.650409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Only 'Age' has a Gaussian-like distribution, the rest are highly skewed!","metadata":{}},{"cell_type":"markdown","source":"### 3-1-2) Univariate Plot for Categorical Features","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,10)})\nfig, axes = plt.subplots(2, 2)\nnames=['HomePlanet','CryoSleep','Destination','VIP']\n\nfor name, ax in zip(names, axes.flatten()):\n    sns.countplot(x=name,data=train_set,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:55:54.653548Z","iopub.execute_input":"2022-06-30T09:55:54.653918Z","iopub.status.idle":"2022-06-30T09:55:55.174459Z","shell.execute_reply.started":"2022-06-30T09:55:54.653885Z","shell.execute_reply":"2022-06-30T09:55:55.173249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3-2) Multivariate Plots","metadata":{}},{"cell_type":"markdown","source":"### 3-2-1) Multivariate Plots for Numerical Data","metadata":{}},{"cell_type":"code","source":"#sns.heatmap(data=train_set.corr(),annot=True) \n#I won't create this heatmap because the default method for .corr() is pearson and it assums Gaussian Distribution and Outliers can heavily influence the outcomes\n#but the distribution for numerical features here are highly skewed and this heatmap can not give reliable answers!","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:52:17.472076Z","iopub.execute_input":"2022-06-30T09:52:17.472584Z","iopub.status.idle":"2022-06-30T09:52:17.477627Z","shell.execute_reply.started":"2022-06-30T09:52:17.472533Z","shell.execute_reply":"2022-06-30T09:52:17.476652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=train_set.select_dtypes(['number','bool']),hue=\"Transported\",palette='CMRmap')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T10:34:43.82713Z","iopub.execute_input":"2022-06-30T10:34:43.827569Z","iopub.status.idle":"2022-06-30T10:35:07.647628Z","shell.execute_reply.started":"2022-06-30T10:34:43.827531Z","shell.execute_reply":"2022-06-30T10:35:07.646469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apparantly there's not linear correlation between the numerical columns.\n\nLet's take a closer look at the diagonal plots...","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3,figsize=(15,15))\nnames=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n\nfor name, ax in zip(names, axes.flatten()):\n    sns.kdeplot(x=name,hue='Transported',data=train_set,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:04:45.020636Z","iopub.execute_input":"2022-06-30T12:04:45.021073Z","iopub.status.idle":"2022-06-30T12:04:46.742048Z","shell.execute_reply.started":"2022-06-30T12:04:45.021043Z","shell.execute_reply":"2022-06-30T12:04:46.740716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 3,figsize=(18,15))\nnames=['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']\n\nfor name, ax in zip(names, axes.flatten()):\n    sns.stripplot(y=name,x='Transported',data=train_set,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T11:48:33.848521Z","iopub.execute_input":"2022-06-30T11:48:33.849128Z","iopub.status.idle":"2022-06-30T11:48:34.962906Z","shell.execute_reply.started":"2022-06-30T11:48:33.849087Z","shell.execute_reply":"2022-06-30T11:48:34.961128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the last two plots, we can see:\n\n1-**Age:** \n* Children (up to 10-12) have a higher chance of being Transported\n* Adults (20-40) have a higher chance of **not** being Transported\n* Rest have almost an equal chance of being Transported\n\n2-**RoomService:**\n* People who spent no money on RoomService have a higher chance of being Transported; actually, as the amount of expenditure goes higher, fewer are among the Transported people and for the extreme expenditures, there are no Transported people.\n\n3-**FoodCourt:**\n* People who spent nothing on FoodCourt are less likely to be Transported, as the amount of expenditure goes higher, the chance of being Transported is almost equal, till we reach about 17000, from there on, we have a few people; but all of them are Transported.\n\n4-**ShoppingMall:**\n* There seems to be little to no distinction between Transported and not Transported people, based on expenditure in the ShoppingMall, this feature doesn't seem to be very informative\n\n5-**Spa:**\n* People who spent little to no money on Spa, have a higher chance of being Transported\n\n6-**VRDeck:**\n* Same as Spa","metadata":{}},{"cell_type":"markdown","source":"### 3-2-2) Multivariate Plots for Categorical Data","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 2)\nnames=['HomePlanet','CryoSleep','Destination','VIP']\n\nfor name, ax in zip(names, axes.flatten()):\n    sns.barplot(x=name,y='Transported',data=train_set,ax=ax)\n    ax.set( ylabel=\"Transportation Probability\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:43.259244Z","iopub.execute_input":"2022-06-30T08:23:43.25962Z","iopub.status.idle":"2022-06-30T08:23:44.369555Z","shell.execute_reply.started":"2022-06-30T08:23:43.259589Z","shell.execute_reply":"2022-06-30T08:23:44.368643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll explore the relevance of each categorical feature to the target, using chi2 test later, but what we can tell so far is:\n\n1- People in VIP section, have less chance of being Transported\n\n2- People in CryoSleep have much higher chance of survival\n\nI think this is because, VIP people were awake and probably scattered in different parts of the spaceship; shopping or eating or whatever, so they were less safe in case of collision, whereas people in cryosleep, they probably were in a strong container, which would keep them safe.\n\n3-For the destination and homeplanet, we can see how the Transportation Probability changes among them, in the plot.","metadata":{}},{"cell_type":"markdown","source":"# 4) Handling Missing Values and Outliers","metadata":{}},{"cell_type":"markdown","source":"## 4-1) Outliers","metadata":{}},{"cell_type":"markdown","source":"Using below function, I'll detect rows with n number of outliers, I'll drop rows with more than 5 outliers","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ndef outlier_detect(df,n,cols):\n    rows,to_drop=[],[]\n    for col in cols:\n        Q1=np.nanpercentile(df[col],25)\n        Q3=np.nanpercentile(df[col],75)\n        IQR=Q3-Q1\n        outlier_point=1.5*IQR\n        rows.extend(df[(df[col]<Q1-outlier_point)|(df[col]>Q3+outlier_point)].index)\n    for r,c in Counter(rows).items():\n        if c>=n: to_drop.append(r)\n    return to_drop","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.370892Z","iopub.execute_input":"2022-06-30T08:23:44.371225Z","iopub.status.idle":"2022-06-30T08:23:44.379136Z","shell.execute_reply.started":"2022-06-30T08:23:44.371196Z","shell.execute_reply":"2022-06-30T08:23:44.377714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop=outlier_detect(train_set,5,train_set.select_dtypes('float').columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.380728Z","iopub.execute_input":"2022-06-30T08:23:44.381986Z","iopub.status.idle":"2022-06-30T08:23:44.411766Z","shell.execute_reply.started":"2022-06-30T08:23:44.381945Z","shell.execute_reply":"2022-06-30T08:23:44.410715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.drop(to_drop,inplace=True,axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.414272Z","iopub.execute_input":"2022-06-30T08:23:44.414675Z","iopub.status.idle":"2022-06-30T08:23:44.421128Z","shell.execute_reply.started":"2022-06-30T08:23:44.414643Z","shell.execute_reply":"2022-06-30T08:23:44.420156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-2) Missing and Duplicates","metadata":{}},{"cell_type":"code","source":"#there are no rows with all null values\ntrain_set.isna().all(axis=1).unique()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.424729Z","iopub.execute_input":"2022-06-30T08:23:44.425086Z","iopub.status.idle":"2022-06-30T08:23:44.440679Z","shell.execute_reply.started":"2022-06-30T08:23:44.425054Z","shell.execute_reply":"2022-06-30T08:23:44.439379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#there are no duplicates\ntrain_set.duplicated().any()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.442644Z","iopub.execute_input":"2022-06-30T08:23:44.443392Z","iopub.status.idle":"2022-06-30T08:23:44.467637Z","shell.execute_reply.started":"2022-06-30T08:23:44.443343Z","shell.execute_reply":"2022-06-30T08:23:44.46639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_prcnt=[train_set[col].isna().sum()/train_set.shape[0] *100 for col in train_set.columns]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.469108Z","iopub.execute_input":"2022-06-30T08:23:44.469489Z","iopub.status.idle":"2022-06-30T08:23:44.482353Z","shell.execute_reply.started":"2022-06-30T08:23:44.469458Z","shell.execute_reply":"2022-06-30T08:23:44.480989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_tbl=pd.DataFrame(missing_prcnt,columns=['%missing'],index=train_set.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.483625Z","iopub.execute_input":"2022-06-30T08:23:44.484128Z","iopub.status.idle":"2022-06-30T08:23:44.496254Z","shell.execute_reply.started":"2022-06-30T08:23:44.484074Z","shell.execute_reply":"2022-06-30T08:23:44.495181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_tbl","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.498068Z","iopub.execute_input":"2022-06-30T08:23:44.498501Z","iopub.status.idle":"2022-06-30T08:23:44.517493Z","shell.execute_reply.started":"2022-06-30T08:23:44.498465Z","shell.execute_reply":"2022-06-30T08:23:44.516375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see if there's a pattern to missing values","metadata":{}},{"cell_type":"code","source":"mno.matrix(train_set, figsize = (20, 6))","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.519791Z","iopub.execute_input":"2022-06-30T08:23:44.520174Z","iopub.status.idle":"2022-06-30T08:23:44.963957Z","shell.execute_reply.started":"2022-06-30T08:23:44.520144Z","shell.execute_reply":"2022-06-30T08:23:44.962726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They seem pretty random...","metadata":{}},{"cell_type":"markdown","source":"We'll get back to the missing values and impute them.","metadata":{}},{"cell_type":"markdown","source":"# 5) Exploring Features","metadata":{}},{"cell_type":"markdown","source":"## 5-1) Name","metadata":{}},{"cell_type":"markdown","source":"I noticed people with same last names, I'm going to extract the last names and see if I can extract a feature named: \"Family\" from them!","metadata":{}},{"cell_type":"code","source":"train_set[['Name','Last']] = train_set.Name.str.split(\" \", expand=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.96576Z","iopub.execute_input":"2022-06-30T08:23:44.966102Z","iopub.status.idle":"2022-06-30T08:23:44.986432Z","shell.execute_reply.started":"2022-06-30T08:23:44.966071Z","shell.execute_reply":"2022-06-30T08:23:44.985415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.drop('Name',inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:44.987982Z","iopub.execute_input":"2022-06-30T08:23:44.989383Z","iopub.status.idle":"2022-06-30T08:23:45.006Z","shell.execute_reply.started":"2022-06-30T08:23:44.989341Z","shell.execute_reply":"2022-06-30T08:23:45.00484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fam_size=train_set.Last.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.007863Z","iopub.execute_input":"2022-06-30T08:23:45.008519Z","iopub.status.idle":"2022-06-30T08:23:45.019608Z","shell.execute_reply.started":"2022-06-30T08:23:45.008484Z","shell.execute_reply":"2022-06-30T08:23:45.018206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fam_size","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.021163Z","iopub.execute_input":"2022-06-30T08:23:45.021539Z","iopub.status.idle":"2022-06-30T08:23:45.041277Z","shell.execute_reply.started":"2022-06-30T08:23:45.021507Z","shell.execute_reply":"2022-06-30T08:23:45.040393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set['Last']=train_set['Last'].map(fam_size)\ntrain_set['Last']=train_set['Last'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.042992Z","iopub.execute_input":"2022-06-30T08:23:45.043619Z","iopub.status.idle":"2022-06-30T08:23:45.057161Z","shell.execute_reply.started":"2022-06-30T08:23:45.043584Z","shell.execute_reply":"2022-06-30T08:23:45.055514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g=sns.barplot(x='Last',y='Transported',data=train_set)\ng.set( ylabel=\"Transportation Probability\")\n#Generally, it looks like smaller families had a higher chance of survival.","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.058611Z","iopub.execute_input":"2022-06-30T08:23:45.059338Z","iopub.status.idle":"2022-06-30T08:23:45.872228Z","shell.execute_reply.started":"2022-06-30T08:23:45.059277Z","shell.execute_reply":"2022-06-30T08:23:45.870843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for the sake of better naming\ntrain_set=train_set.rename(columns={'Last':'Fsize'})","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.873717Z","iopub.execute_input":"2022-06-30T08:23:45.874063Z","iopub.status.idle":"2022-06-30T08:23:45.881669Z","shell.execute_reply.started":"2022-06-30T08:23:45.874032Z","shell.execute_reply":"2022-06-30T08:23:45.880403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5-2) Cabin","metadata":{}},{"cell_type":"code","source":"#This shows how many people were in each cabin\ncabin_cap= train_set.Cabin.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.883384Z","iopub.execute_input":"2022-06-30T08:23:45.88373Z","iopub.status.idle":"2022-06-30T08:23:45.897344Z","shell.execute_reply.started":"2022-06-30T08:23:45.883701Z","shell.execute_reply":"2022-06-30T08:23:45.896073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm going to use the number of people in each cabin and replace the cabin names with number of people in it.","metadata":{}},{"cell_type":"code","source":"train_set['Cabin']=train_set['Cabin'].map(cabin_cap)\ntrain_set['Cabin']=train_set['Cabin'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.898727Z","iopub.execute_input":"2022-06-30T08:23:45.899055Z","iopub.status.idle":"2022-06-30T08:23:45.917242Z","shell.execute_reply.started":"2022-06-30T08:23:45.899026Z","shell.execute_reply":"2022-06-30T08:23:45.916246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set.Cabin.unique()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.918512Z","iopub.execute_input":"2022-06-30T08:23:45.919658Z","iopub.status.idle":"2022-06-30T08:23:45.934102Z","shell.execute_reply.started":"2022-06-30T08:23:45.919538Z","shell.execute_reply":"2022-06-30T08:23:45.933046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g=sns.barplot(x='Cabin',y='Transported',data=train_set)\ng.set( ylabel=\"Transportation Probability\")\n#seems like, people in cabins with moderate capacity had more chance of survival","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:45.935726Z","iopub.execute_input":"2022-06-30T08:23:45.936878Z","iopub.status.idle":"2022-06-30T08:23:46.475754Z","shell.execute_reply.started":"2022-06-30T08:23:45.936829Z","shell.execute_reply":"2022-06-30T08:23:46.474408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5-3) Chi2 test","metadata":{}},{"cell_type":"markdown","source":"**Let's do a chi2 test to see if there are any features with a pvalue higher than 0.05**","metadata":{}},{"cell_type":"code","source":"from scipy.stats import chi2_contingency\ndef chi2_calc(df,target):\n    scores=[]\n    for col in df.columns:\n        ct=pd.crosstab(df[col],target)\n        stat,p,dof,expected=chi2_contingency(ct)\n        scores.append(p)\n    return pd.DataFrame(scores, index=df.columns, columns=['P value']).sort_values(by='P value')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.477428Z","iopub.execute_input":"2022-06-30T08:23:46.477779Z","iopub.status.idle":"2022-06-30T08:23:46.485425Z","shell.execute_reply.started":"2022-06-30T08:23:46.47775Z","shell.execute_reply":"2022-06-30T08:23:46.484174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi2_calc(train_set.select_dtypes(['object']),train_set.Transported)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.491947Z","iopub.execute_input":"2022-06-30T08:23:46.493306Z","iopub.status.idle":"2022-06-30T08:23:46.629368Z","shell.execute_reply.started":"2022-06-30T08:23:46.493258Z","shell.execute_reply":"2022-06-30T08:23:46.627999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**So all the categorical features have significant relevance to the target (pvalue <0.05), except passengerId which is ok, it's not really a feature.**","metadata":{}},{"cell_type":"code","source":"train_set.drop('PassengerId',inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.630792Z","iopub.execute_input":"2022-06-30T08:23:46.63118Z","iopub.status.idle":"2022-06-30T08:23:46.638237Z","shell.execute_reply.started":"2022-06-30T08:23:46.631146Z","shell.execute_reply":"2022-06-30T08:23:46.637122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=train_set.pop('Transported')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.639658Z","iopub.execute_input":"2022-06-30T08:23:46.640234Z","iopub.status.idle":"2022-06-30T08:23:46.654158Z","shell.execute_reply.started":"2022-06-30T08:23:46.640153Z","shell.execute_reply":"2022-06-30T08:23:46.6528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=y.map({False:0,True:1})","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.655577Z","iopub.execute_input":"2022-06-30T08:23:46.656376Z","iopub.status.idle":"2022-06-30T08:23:46.668934Z","shell.execute_reply.started":"2022-06-30T08:23:46.656326Z","shell.execute_reply":"2022-06-30T08:23:46.667787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.670502Z","iopub.execute_input":"2022-06-30T08:23:46.671307Z","iopub.status.idle":"2022-06-30T08:23:46.68276Z","shell.execute_reply.started":"2022-06-30T08:23:46.671266Z","shell.execute_reply":"2022-06-30T08:23:46.681818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6) PipeLine Implementation\nHere are the steps I'm going to take:\n\n* 1- Use 2 pipelines, one for categorical data and one for numerical data. in these pipelines, 2 things are going to happen:\n\n1-1 For numerical pipeline: imputing and scaling\n\n1-2 For categorical pipeline:imputing and encoding\n\n* 2-Use a ColumnTransformer to implement the functions in the pipeline on their respective data types\n\n\n* 3-Try a number of models to see which ones work better","metadata":{}},{"cell_type":"code","source":"train_set.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:46.683959Z","iopub.execute_input":"2022-06-30T08:23:46.684399Z","iopub.status.idle":"2022-06-30T08:23:47.222459Z","shell.execute_reply.started":"2022-06-30T08:23:46.684349Z","shell.execute_reply":"2022-06-30T08:23:47.22108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num=list(train_set.select_dtypes('float').columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:47.224269Z","iopub.execute_input":"2022-06-30T08:23:47.224825Z","iopub.status.idle":"2022-06-30T08:23:47.235427Z","shell.execute_reply.started":"2022-06-30T08:23:47.224775Z","shell.execute_reply":"2022-06-30T08:23:47.233961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat=list(train_set.select_dtypes(['object']).columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:47.237839Z","iopub.execute_input":"2022-06-30T08:23:47.238199Z","iopub.status.idle":"2022-06-30T08:23:47.252329Z","shell.execute_reply.started":"2022-06-30T08:23:47.238171Z","shell.execute_reply":"2022-06-30T08:23:47.250897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note: OrdinalEncoder (without defining an order) is like LabelEncoder, but we can apply it to multiple columns at once\nwhereas LabelEncoder, is for one column transformation. The order in which they encode, is ascending, meaning A will be 1 and B will be 2**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\nfrom sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer,KNNImputer,SimpleImputer\nfrom category_encoders import MEstimateEncoder,PolynomialEncoder,BackwardDifferenceEncoder,LeaveOneOutEncoder,QuantileEncoder\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import PowerTransformer\n\n  \nrandom_state=0\n\n#setting up the imputer for numerical features\nNImputer=IterativeImputer(random_state=random_state,tol=1e-5,max_iter=20)\n\n#setting up the imputer for categorical features (We can explore with this imputer too, I'll let it be here just in case)\nCImputer=IterativeImputer(estimator=LinearDiscriminantAnalysis(),random_state=random_state,tol=1e-5,max_iter=20)\n\n#target encoder: it's usually used for high cardinality features. \nt_encoder = MEstimateEncoder(m=10, random_state=random_state,handle_missing='return_nan')\n\n#PolynomialEncoder\np_ecnoder=PolynomialEncoder()\n\n#BackwardDifferenceCoding\nb_encoder=BackwardDifferenceEncoder()\n\n#QuantileEncoder\nq_encoder=QuantileEncoder(m=10)\n\n#creating log function that has fit and fit_transform methods because numerical columns are mostly skewed\ndef log_transform(x):\n    return np.log(x + 1)\nlog_pip=FunctionTransformer(log_transform)\n\n#Using power transformer class for standardizing data distirbution in numerical columns\npt=PowerTransformer()\n\n#creating two preprocessing pipelines for categorical and numerical data types\nnumeric_pip=Pipeline(steps=[('PowerTransformer',pt),('NImputer',NImputer)])\ncategory_pip=Pipeline(steps=[('MEstimateEncoder',t_encoder),('NImputer',NImputer)])\n\n\n#creating a column transformer to implement the transformation\nct=ColumnTransformer(transformers=[('num',numeric_pip,num),('cat',category_pip,cat)])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:47.253455Z","iopub.execute_input":"2022-06-30T08:23:47.253809Z","iopub.status.idle":"2022-06-30T08:23:47.267095Z","shell.execute_reply.started":"2022-06-30T08:23:47.253781Z","shell.execute_reply":"2022-06-30T08:23:47.265876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is for testing to see if the columntransformer works properly\ntst = Pipeline([\n('coltrns',ct), #COLUMN TRANSFORMER\n])\nxx=tst.fit_transform(train_set,y)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:47.268613Z","iopub.execute_input":"2022-06-30T08:23:47.268944Z","iopub.status.idle":"2022-06-30T08:23:48.048854Z","shell.execute_reply.started":"2022-06-30T08:23:47.268915Z","shell.execute_reply":"2022-06-30T08:23:48.047418Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xx=pd.DataFrame(xx)\nxx.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:48.050575Z","iopub.execute_input":"2022-06-30T08:23:48.051531Z","iopub.status.idle":"2022-06-30T08:23:48.081721Z","shell.execute_reply.started":"2022-06-30T08:23:48.051479Z","shell.execute_reply":"2022-06-30T08:23:48.080098Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6-3)Models and Scoring","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nclassifiers = []\nclassifiers.append(SVC(probability=True,random_state=random_state)) \nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state)) \nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state,C=0.5,solver='liblinear')) \nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(MLPClassifier(random_state=random_state, max_iter=500,tol=0.01))\nclassifiers.append(GaussianNB())\nclassifiers.append(XGBClassifier(random_state=random_state))\nclassifiers.append(LGBMClassifier(random_state=random_state))\n\n# CREATING A FOR LOOP FOR SCORING EACH MODEL\ncv_results = []\ncv = KFold(n_splits=2,shuffle=True,random_state=random_state)\nfor classifier in classifiers :\n    classif = Pipeline([\n('coltrns',ct), #COLUMN TRANSFORMER\n('classifier', classifier)])\n    cvs=cross_val_score(classif, train_set, y, scoring = \"accuracy\", cv = cv, n_jobs=-1)\n    cv_results.append(cvs)\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\n#CREATING A DATAFRAME OF MODEL SCORES\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValSDs\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoostClassifier\",\n                                                                                 \"RandomForestClassifier\",\n                                                                                 \"ExtraTreesClassifier\",\n                                                                                  \"GradientBoostingClassifier\",\n                                                                                  \"KNeighborsClassifier\",\n                                                                                  \"LogisticRegression\",\n                                                                                   \"MLPClassifier\",\n                                                                                  \"LinearDiscriminantAnalysis\",\n                                                                                  \"GaussianNB\",\n                                                                                  \"XGBClassifier\",\n                                                                                  \"LGBMClassifier\"]})\n#PLOTTING\ng = sns.barplot(x=\"CrossValMeans\",y=\"Algorithm\",data = cv_res.sort_values(by='CrossValMeans'), palette=\"twilight_shifted_r\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\ng=sns.set(rc={'figure.figsize':(5,5)})\n\ncv_res.sort_values(by='CrossValMeans')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:23:48.084196Z","iopub.execute_input":"2022-06-30T08:23:48.085243Z","iopub.status.idle":"2022-06-30T08:24:06.298823Z","shell.execute_reply.started":"2022-06-30T08:23:48.085175Z","shell.execute_reply":"2022-06-30T08:24:06.297514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7) Learning Curve\nHere we're going to take a look at the learning curve, using below function, before tuning the models.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(my_model, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=[np.linspace(.1, 1.0, 5)]):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        my_model, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes,scoring=\"accuracy\")\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    plt.grid()\n    return plt","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:24:06.300617Z","iopub.execute_input":"2022-06-30T08:24:06.301762Z","iopub.status.idle":"2022-06-30T08:24:06.313014Z","shell.execute_reply.started":"2022-06-30T08:24:06.301713Z","shell.execute_reply":"2022-06-30T08:24:06.311689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in [3,5,11,12]: #Index of the last 4 highest scores\n    model = Pipeline([\n        ('coltrns',ct), #COLUMN TRANSFORMER\n        ('classifier', classifiers[i])])\n    plot_learning_curve(model,model[1],train_set,y)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:24:06.315738Z","iopub.execute_input":"2022-06-30T08:24:06.316107Z","iopub.status.idle":"2022-06-30T08:24:45.420656Z","shell.execute_reply.started":"2022-06-30T08:24:06.316076Z","shell.execute_reply":"2022-06-30T08:24:45.419497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lesser the training curve changes, the more the model is overfit. because it's working well on the training data and learning it amazingly, but when it comes to the validation set, it can not generalize and performs poorly.","metadata":{}},{"cell_type":"markdown","source":"# 8) Model Tuning using GridSearchCV\n\nBecause of highly skewed data and the fact that if I were to delete all outliers, a big portion of the dataset would be deleted, I'm going to use tree-based models as final models for tuning. they have the highest scores in the chart and they are:\n\n* Random Forest Classifier\n* XGBClassifier\n* LGBMClassifier\n* Gradient Boosting Classifier\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:24:45.421919Z","iopub.execute_input":"2022-06-30T08:24:45.422232Z","iopub.status.idle":"2022-06-30T08:24:45.427542Z","shell.execute_reply.started":"2022-06-30T08:24:45.422204Z","shell.execute_reply":"2022-06-30T08:24:45.426395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7-1) Gradient Boost","metadata":{}},{"cell_type":"markdown","source":"[Reference](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) for tuning gradient boost","metadata":{}},{"cell_type":"code","source":"#GB\ngb_model = Pipeline([\n('coltrns',ct), #COLUMN TRANSFORMER\n('classifier', GradientBoostingClassifier(random_state=random_state))])\n\n#there are two types of parameter to be tuned here â€“ tree based and boosting parameters,\n#in general Lower the learning rate and increase the estimators proportionally to get more robust models.\ngb_grid =  {\n    \n    #Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems\n    \"classifier__learning_rate\":[0.1],\n    \n    #This should range around 40-70. Remember to choose a value on which your system can work fairly fast.\n    #This is because it will be used for testing various scenarios and determining the tree parameters.\n    \"classifier__n_estimators\":[60,70], \n    \n    #This should be ~0.5-1% of total values\n    \"classifier__min_samples_split\":[40,50],\n    \n    #Can be selected based on intuition. This is just used for preventing overfitting\n    \"classifier__min_samples_leaf\" : [120,150],\n    \n    #Should be chosen (5-8) based on the number of observations and predictors.\n    \"classifier__max_depth\" :[8,10],\n    \n    #.8 is a commonly used start value\n    \"classifier__subsample\":[.8,.5],\n   \n    \"classifier__max_features\":['log2']\n\n    }\n\ngsgb = GridSearchCV(gb_model,gb_grid , cv=cv, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsgb.fit(train_set,y)\n# gb_model.fit(train_set,y)\n\ngb_best = gsgb.best_estimator_\n\n# Best score\ndisplay(gsgb.best_score_)\ndisplay(gb_best)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:24:45.428744Z","iopub.execute_input":"2022-06-30T08:24:45.429054Z","iopub.status.idle":"2022-06-30T08:25:01.946476Z","shell.execute_reply.started":"2022-06-30T08:24:45.429016Z","shell.execute_reply":"2022-06-30T08:25:01.945183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7-2) LGBM","metadata":{}},{"cell_type":"code","source":"lgbm_model = Pipeline([\n('coltrns',ct), #COLUMN TRANSFORMER\n('classifier', LGBMClassifier(random_state=random_state))])\n\nlgbm_grid =  {\n    'classifier__num_leaves': [31, 127],\n    'classifier__reg_alpha': [0.1, 0.5],\n    'classifier__min_data_in_leaf': [30, 50, 100, 300, 400],\n    'classifier__reg_lambda': [0,.5, 1]\n    }\n\ngslgbm = GridSearchCV(lgbm_model,lgbm_grid , cv=cv, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngslgbm.fit(train_set,y)\n\nlgbm_best = gslgbm.best_estimator_\n\n# Best score\ndisplay(gslgbm.best_score_)\n\ndisplay(lgbm_best)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:25:01.947929Z","iopub.execute_input":"2022-06-30T08:25:01.948284Z","iopub.status.idle":"2022-06-30T08:25:29.998333Z","shell.execute_reply.started":"2022-06-30T08:25:01.948246Z","shell.execute_reply":"2022-06-30T08:25:29.997173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7-3) XGBClassifier","metadata":{}},{"cell_type":"code","source":"xgbc_model = Pipeline([\n('coltrns',ct), #COLUMN TRANSFORMER\n('classifier', XGBClassifier(random_state=random_state))])\n\nxgbc_grid =  {\n              \n              'classifier__learning_rate': [0.03,0.01], \n              'classifier__max_depth': [8,10],\n              'classifier__min_child_weight': [20,50],\n              'classifier__subsample': [.5,.8],\n              'classifier__colsample_bytree': [.5,.8],\n              'classifier__n_estimators': [100] \n              }\ngsxgbc = GridSearchCV(xgbc_model,xgbc_grid , cv=cv, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsxgbc.fit(train_set,y)\n\nxgbc_best = gsxgbc.best_estimator_\n\n# Best score\ndisplay(gsxgbc.best_score_)\n\ndisplay(xgbc_best)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:25:30.000032Z","iopub.execute_input":"2022-06-30T08:25:30.000501Z","iopub.status.idle":"2022-06-30T08:26:00.367508Z","shell.execute_reply.started":"2022-06-30T08:25:30.000467Z","shell.execute_reply":"2022-06-30T08:26:00.36627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8)Transforming and Predicting Test Set","metadata":{}},{"cell_type":"code","source":"#Creating Fsize Feature For Test Set\ntest_set[['Name','Last']] = test_set.Name.str.split(\" \", expand=True)\ntest_set.drop('Name',inplace=True,axis=1)\nfam_size=test_set.Last.value_counts()\ntest_set['Last']=test_set['Last'].map(fam_size)\ntest_set['Last']=test_set['Last'].astype('object')\ntest_set=test_set.rename(columns={'Last':'Fsize'})\n\n#Replacing Cabin names with their capacity using the number of people in it\ncabin_cap= test_set.Cabin.value_counts()\ntest_set['Cabin']=test_set['Cabin'].map(cabin_cap)\ntest_set['Cabin']=test_set['Cabin'].astype('object')\n\n#dropping PassengerId\ntest_set.drop('PassengerId',inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T08:33:33.228113Z","iopub.execute_input":"2022-06-30T08:33:33.228552Z","iopub.status.idle":"2022-06-30T08:33:33.266872Z","shell.execute_reply.started":"2022-06-30T08:33:33.228506Z","shell.execute_reply":"2022-06-30T08:33:33.265653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_xgbc = pd.Series(xgbc_best.predict(test_set), name=\"xgbc\")\ntest_lgbm = pd.Series(lgbm_best.predict(test_set), name=\"lgbm\")\ntest_gb = pd.Series(gb_best.predict(test_set), name=\"gb\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_xgbc,test_lgbm,test_gb],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)\n\n#The results mainly agree with eachother, but in general it's better if we have strong models that are not highly correlated so that they can\n#cover eachother's flaws in prediction to a degree!","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:41:48.842114Z","iopub.execute_input":"2022-06-30T09:41:48.842604Z","iopub.status.idle":"2022-06-30T09:41:49.290979Z","shell.execute_reply.started":"2022-06-30T09:41:48.842569Z","shell.execute_reply":"2022-06-30T09:41:49.28977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nvotingC = VotingClassifier(estimators=[ ('XGBC', xgbc_best),('LGBM',lgbm_best),('GB',gb_best)], voting='soft', n_jobs=-1)\n\nvotingC.fit(train_set,y)\npredictions = pd.DataFrame(votingC.predict(test_set)).values","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:29:23.858437Z","iopub.execute_input":"2022-06-30T09:29:23.859633Z","iopub.status.idle":"2022-06-30T09:29:26.165744Z","shell.execute_reply.started":"2022-06-30T09:29:23.859549Z","shell.execute_reply":"2022-06-30T09:29:26.164388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test['PassengerId'], 'Transported': predictions.flatten()})","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:31:01.827753Z","iopub.execute_input":"2022-06-30T09:31:01.828619Z","iopub.status.idle":"2022-06-30T09:31:01.835035Z","shell.execute_reply.started":"2022-06-30T09:31:01.828565Z","shell.execute_reply":"2022-06-30T09:31:01.834146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.hist()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T09:31:26.818666Z","iopub.execute_input":"2022-06-30T09:31:26.819072Z","iopub.status.idle":"2022-06-30T09:31:27.072796Z","shell.execute_reply.started":"2022-06-30T09:31:26.819036Z","shell.execute_reply":"2022-06-30T09:31:27.071408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9)Submission","metadata":{}},{"cell_type":"code","source":"# output.to_csv('submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank you for reading!","metadata":{}},{"cell_type":"markdown","source":"We can explore more with feature, for example, maybe the homeplanet-destination pairs have a statistically significant relation with the target or we can experiment with tuning and model selection; there are a lot of things we can do and I hope this notebook gives someone some ideas for their project!\n\n**I'd be happy to recieve your feedbacks and suggestions on how to improve my work :)**","metadata":{}}]}