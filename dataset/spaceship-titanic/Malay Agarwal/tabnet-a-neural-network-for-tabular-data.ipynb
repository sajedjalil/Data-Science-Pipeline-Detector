{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Neural networks have exploded in popularity in almost every data format, from images and videos to text. One data format where they are yet to make in-roads is tabular data. For tabular data, the most popular algorithms continue to be tree-based boosting algorithms such as [XGBoost](https://en.wikipedia.org/wiki/XGBoost), [LighGBM](https://en.wikipedia.org/wiki/LightGBM) and [CatBoost](https://en.wikipedia.org/wiki/Catboost), or even simple linear algorithms like [Linear](https://en.wikipedia.org/wiki/Linear_regression)/[Logistic](https://en.wikipedia.org/wiki/Logistic_regression) Regression.\n\nTabNet<sup>[\\[1\\]](#ref1)</sup> is a remedy to this. It is an attention-based neural network introduced in 2019 by a Google Cloud AI team. It beats XGBoost, LightGBM and CatBoost on multiple datasets such as [Forest Cover Type](https://archive.ics.uci.edu/ml/datasets/covertype) and [Poker Hand](https://archive.ics.uci.edu/ml/datasets/Poker+Hand). It also tends to be more explainable than these other algorithms.\n\nIn this notebook, we will use TabNet to solve the [Spaceship Titanic](https://www.kaggle.com/competitions/spaceship-titanic) competition. We will use the [TabNet implementation for PyTorch by Dreamquark](https://github.com/dreamquark-ai/tabnet) in order to do this. We will also use [Optuna](https://github.com/optuna/optuna) to tune the hyperparameters of the model.\n\n\n> Note: For more on TabNet, refer to this video by the co-author of the above mentioned library: [Talks # 4: Sebastien Fischman - Pytorch-TabNet: Beating XGBoost on Tabular Data Using Deep Learning](https://www.youtube.com/watch?v=ysBaZO8YmX8).","metadata":{}},{"cell_type":"code","source":"# Install TabNet\n!pip install pytorch-tabnet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-12T20:37:32.801232Z","iopub.execute_input":"2022-06-12T20:37:32.801726Z","iopub.status.idle":"2022-06-12T20:37:46.192421Z","shell.execute_reply.started":"2022-06-12T20:37:32.801628Z","shell.execute_reply":"2022-06-12T20:37:46.191378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TabNet Architecture","metadata":{}},{"cell_type":"markdown","source":"Architecturally, TabNet consists of multiple encoder steps, as shown below:\n\n![](https://drive.google.com/uc?export=view&id=1fM0jdeUB7pgj_Zg1z7jwmeM7ATZzWMF6)\n\nIntuitively, each step selects a subset of features from all the features available in the training data to use for its predictions. How it differs from most other neural networks is that this selection step occurs for each sample instead of the entire training data. Thus, the predictions for each sample are generated by different subset of features. This leads to better performance.\n\nThere are three main layers that make up a step.\n\n### Feature Transformer\n\n![TabNet Feature Transformer](https://drive.google.com/uc?export=view&id=1TSyJwEYjAN5CT5cdTLLaVbcmmXXaNpL6)\n\nThe feature transformer layer generates an internal representation of the features.\n\nEach layer consists of stacks of a fully-connected layer, a batch normalisation and a [GLU](https://paperswithcode.com/method/glu) activation function, with skip connections in between stacks.\n\nSome of the stacks are shared across all the steps and some stacks are local to a step. That is, the feature transformer in each step uses some weights that are common across every step and some weights that are learnt specifically for that step. This ensures that other steps have some information from every step while generating their internal representation.\n\nTake note of the split block in the overall architecture. This block is used to split the internal representation between the next step (red arrow) and the overall output (blue arrow). Clearly, no such split is required in step $0$ and hence, there is only a red arrow.\n\n### Attentive Transformer\n\n![TabNet Attentive Transformer](https://drive.google.com/uc?export=view&id=17SAFPueqARehjkxD0UmRmLAXAT8c-DXx)\n\nThe attentive transformer layer takes the learned representation of features as input and outputs a mask which is then used to select the features that should be used for this step and the current sample. The mask can be thought of as consisting of probabilities that sum up to $1$.\n\nEach layer consists of a fully-connected layer, a batch normalisation and a \"sparse\" softmax activation. The softmax activation is sparse since the generated mask has a lot of zeros in it, denoting that the features associated with those zeros are not used for generating predictions.\n\nThe prior scales shown in the diagram consists of information which denotes how much each feature has been used in the previous steps. This is taken into account while generating the mask. Mathematically, if the current step is $i$, $P[i]=\\prod_{j=1}^{i}(\\gamma-M[j])$, where $\\gamma$ is a relaxation parameter and $M[j]$ is the mask in step $j$. When $\\gamma=1$, a feature is enforced to be used only for one step (since then $\\gamma-M[j]\\approx0$) and as $\\gamma$ increases, the constraint relaxes so that the feature can be used for multiple steps.\n\n### Feature Masking\n\nThe feature masking layer uses the generated mask to select a subset of features. It is a element-wise product between the original features and the generated mask.","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"# Explainability","metadata":{}},{"cell_type":"markdown","source":"An important goal behind TabNet is to be explainable. This is achieved by using the generated masks in each step. Using the masks, it is possible to visualize which features are being used the most for each sample at each step.","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import functools\nimport os\nimport random\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom sklearn import metrics\n\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:46.194283Z","iopub.execute_input":"2022-06-12T20:37:46.194622Z","iopub.status.idle":"2022-06-12T20:37:49.998403Z","shell.execute_reply.started":"2022-06-12T20:37:46.19459Z","shell.execute_reply":"2022-06-12T20:37:49.997515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    torch.manual_seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.000244Z","iopub.execute_input":"2022-06-12T20:37:50.001003Z","iopub.status.idle":"2022-06-12T20:37:50.010435Z","shell.execute_reply.started":"2022-06-12T20:37:50.000958Z","shell.execute_reply":"2022-06-12T20:37:50.009479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"markdown","source":"We will define a configuration class that will store some basic configuration that is used throughout the notebook.","metadata":{}},{"cell_type":"code","source":"class Config:\n    DATA_DIR = \"../input/spaceship-titanic-prepared-datasets\"\n    MAX_EPOCHS = 30\n    N_TRIALS = 30\n    PATIENCE = 20\n    BATCH_SIZE = 1024\n    NUM_WORKERS = 2\n    \n    DEFAULTS = {\n        \"n_d\": 8,\n        \"n_a\": 8,\n        \"n_steps\": 3,\n        \"n_shared\":  2,\n        \"cat_emb_dim\": 1,\n        \"lr\": 2e-2,\n        \"mask_type\": \"entmax\",\n        \"lambda_sparse\": 1e-3,\n        \"max_epochs\": MAX_EPOCHS,\n        \"patience\": PATIENCE,\n    }\n    \n    @classmethod\n    def filepath(cls, filename):\n        return os.path.join(cls.DATA_DIR, filename)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:41:00.412506Z","iopub.execute_input":"2022-06-12T20:41:00.412969Z","iopub.status.idle":"2022-06-12T20:41:00.4206Z","shell.execute_reply.started":"2022-06-12T20:41:00.412926Z","shell.execute_reply":"2022-06-12T20:41:00.419745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Datasets\n\n> Note: This notebook uses the datasets as prepared in [Spaceship Titanic - Logistic Regression Baselines](https://www.kaggle.com/code/defcodeking/spaceship-titanic-logistic-regression-baselines). Link to dataset: [Spaceship Titanic Prepared Datasets](https://www.kaggle.com/datasets/defcodeking/spaceship-titanic-prepared-datasets).\n\nWe will load the datasets.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(Config.filepath(\"train_prepared_both_le.csv\"))\ntest_df = pd.read_csv(Config.filepath(\"test_prepared_both_le.csv\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.022667Z","iopub.execute_input":"2022-06-12T20:37:50.023071Z","iopub.status.idle":"2022-06-12T20:37:50.193171Z","shell.execute_reply.started":"2022-06-12T20:37:50.023041Z","shell.execute_reply":"2022-06-12T20:37:50.19213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.194269Z","iopub.execute_input":"2022-06-12T20:37:50.194561Z","iopub.status.idle":"2022-06-12T20:37:50.237685Z","shell.execute_reply.started":"2022-06-12T20:37:50.194535Z","shell.execute_reply":"2022-06-12T20:37:50.236605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.239115Z","iopub.execute_input":"2022-06-12T20:37:50.239657Z","iopub.status.idle":"2022-06-12T20:37:50.271688Z","shell.execute_reply.started":"2022-06-12T20:37:50.23961Z","shell.execute_reply":"2022-06-12T20:37:50.270839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nTabNet comes with suppoprt for categorical features out of the box. All we need to do is to make sure they are label encoded and have their datatype as integer. We will define a function which takes the training and test dataframes and label encodes all the one-hot encoded columns.","metadata":{}},{"cell_type":"code","source":"def preprocess_datasets(train_df, test_df):\n    # Make copies so that original datasets remain unchanged\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    \n    # Drop Transported and kfold\n    drop = [\"Transported\", \"kfold\"]\n    dropped = train_df[drop].values\n    train_df = train_df.drop(drop, axis=1)\n    \n    # Drop PassengerId\n    passenger_id = test_df[\"PassengerId\"].values\n    test_df = test_df.drop(\"PassengerId\", axis=1)\n    \n    # Add suffix to index and store indices\n    # So that the dataframes can be merged and split\n    train_df = train_df.rename(\"train_{}\".format)\n    test_df = test_df.rename(\"test_{}\".format)\n    \n    tr_idx = train_df.index\n    te_idx = test_df.index\n    \n    # Merge\n    df = pd.concat([train_df, test_df])\n    \n    oh_cols = [\"CabinDeck\", \"HomePlanet\", \"Destination\", \"GroupSize\"]\n    \n    for oh_col in oh_cols:\n        # Get all columns associated with the one-hot column\n        columns = [column for column in df.columns if column.startswith(f\"{oh_col}_\")]\n        \n        # .idxmax() returns that column name which has the maximum value in the row\n        values = df[columns].idxmax(axis=1)\n        \n        # Get all levels and make a mapping from level to index\n        levels = values.value_counts().index\n        mapping = {level: idx for idx, level in enumerate(levels)}\n        \n        # Add column with the mapping and specify type as int\n        df[oh_col] = values.map(mapping).astype(int)\n        \n        # Drop one-hot columns\n        df = df.drop(columns, axis=1)\n        \n    # Make sure other categorical features have the right type\n    missing = (col for col in df.columns if col.endswith(\"_missing\"))\n    others = [\"CryoSleep\", \"VIP\", \"Alone\", \"CabinNum\", \"GroupId\", *missing]\n    df[others] = df[others].astype(int)\n        \n    # Split and add dropped columns\n    train_df = df.loc[tr_idx, :]\n    train_df[drop] = dropped\n    \n    test_df = df.loc[te_idx, :]\n    test_df[\"PassengerId\"] = passenger_id\n    \n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.273088Z","iopub.execute_input":"2022-06-12T20:37:50.273538Z","iopub.status.idle":"2022-06-12T20:37:50.287555Z","shell.execute_reply.started":"2022-06-12T20:37:50.273504Z","shell.execute_reply":"2022-06-12T20:37:50.286485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Datasets\n\nWe will use the function above to prepare the datasets.","metadata":{}},{"cell_type":"code","source":"# Prepare datasets\ntrain_df, test_df = preprocess_datasets(train_df, test_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.288741Z","iopub.execute_input":"2022-06-12T20:37:50.289119Z","iopub.status.idle":"2022-06-12T20:37:50.468895Z","shell.execute_reply.started":"2022-06-12T20:37:50.289087Z","shell.execute_reply":"2022-06-12T20:37:50.467866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.4716Z","iopub.execute_input":"2022-06-12T20:37:50.472023Z","iopub.status.idle":"2022-06-12T20:37:50.499949Z","shell.execute_reply.started":"2022-06-12T20:37:50.47199Z","shell.execute_reply":"2022-06-12T20:37:50.498808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.501658Z","iopub.execute_input":"2022-06-12T20:37:50.502134Z","iopub.status.idle":"2022-06-12T20:37:50.529548Z","shell.execute_reply.started":"2022-06-12T20:37:50.502088Z","shell.execute_reply":"2022-06-12T20:37:50.528606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Some Parameters Required By TabNet\n\nIn order to correctly handle categorical features, TabNet requires two arguments:\n\n- `cat_idxs`: This should be a list of positions of categorical features in terms of indices in the training data.\n- `cat_dims`: This should be a list of the number of classes in each categorical features in the training data. It should have the same order as `cat_idxs`.\n\nThus, we will create these two lists.","metadata":{}},{"cell_type":"code","source":"cols = train_df.columns\n\n# Create a list of all categorical columns in the dataframe\nmissing = (col for col in cols if col.endswith(\"_missing\"))\ncategorical_features = [\n    \"CryoSleep\",\n    \"VIP\",\n    \"Alone\",\n    \"CabinNum\",\n    \"GroupId\",\n    \"HomePlanet\",\n    \"Destination\",\n    \"CabinDeck\",\n    \"GroupSize\",\n    \"CabinSide\",\n    *missing\n]","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.531144Z","iopub.execute_input":"2022-06-12T20:37:50.531595Z","iopub.status.idle":"2022-06-12T20:37:50.537556Z","shell.execute_reply.started":"2022-06-12T20:37:50.531551Z","shell.execute_reply":"2022-06-12T20:37:50.53641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"Transported\"\nunused = [\"kfold\"]\n\n# Create a list of all features\nfeatures = [col for col in cols if col not in unused + [target]]\n\n# Find the indices of all categorical features\ncategorical_idx = [idx for idx, feature in enumerate(features) if feature in categorical_features]\n\n# Get the number of classes of each categorical feature\n# The concatenation is required since there are some categorical features\n# With some levels present only in either train or test data\ncategorical_dims = [\n    pd.concat([train_df[col], test_df[col]]).nunique() for col in cols if col in categorical_features\n]\n\n\nfeatures, categorical_idx, categorical_dims","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.539052Z","iopub.execute_input":"2022-06-12T20:37:50.539482Z","iopub.status.idle":"2022-06-12T20:37:50.569466Z","shell.execute_reply.started":"2022-06-12T20:37:50.53944Z","shell.execute_reply":"2022-06-12T20:37:50.568834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters For TabNet\n\nThe function below will be used to get the the initialization parameters for TabNet. Using a function this way allows us to easily integrate Optuna into the pipeline. Take note of the number of hyperparameters TabNet has. Out of these, only a few need to be tuned. This is also another benefit TabNet has over XGBoost and LightGBM, which have a relatively large number of hyperparameters.\n\nAlso take note of `device_name`. When set to `auto`, TabNet automatically detects a GPU and uses it if available.","metadata":{}},{"cell_type":"code","source":"TabNetClassifier()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.570438Z","iopub.execute_input":"2022-06-12T20:37:50.57092Z","iopub.status.idle":"2022-06-12T20:37:50.578576Z","shell.execute_reply.started":"2022-06-12T20:37:50.570888Z","shell.execute_reply":"2022-06-12T20:37:50.577859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_init_params(params, verbose=1, cat_idxs=None, cat_dims=None):\n    return {\n        \"n_d\": params.get(\"n_d\", 8),\n        \"n_a\": params.get(\"n_d\", 8),\n        \"n_steps\": params.get(\"n_steps\", 3),\n        \"n_shared\": params.get(\"n_shared\", 2),\n        \"cat_emb_dim\": params.get(\"cat_emb_dim\", 1),\n        \"optimizer_params\": {\"lr\": params.get(\"lr\", 2e-2)},\n        \"mask_type\": params.get(\"mask_type\", \"sparsemax\"),\n        \"lambda_sparse\": params.get(\"lambda_sparse\", 1e-3),\n        \"optimizer_fn\": torch.optim.Adam,\n        \"cat_idxs\": cat_idxs or [],\n        \"cat_dims\": cat_dims or [],\n        \"verbose\": verbose,\n    }","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.579931Z","iopub.execute_input":"2022-06-12T20:37:50.580441Z","iopub.status.idle":"2022-06-12T20:37:50.588645Z","shell.execute_reply.started":"2022-06-12T20:37:50.58041Z","shell.execute_reply":"2022-06-12T20:37:50.587639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop\n\nWe will define the training loop as a function.\n\nIt takes the training and test dataframes, parameters for training, a list of features to be used, name of the target in the dataframe, a verbosity argument, and optional categorical indices and dimensions as required by TabNet. At the end, it returns the test predictions, training history, the average best score and data that can be used for explainability.\n\n> Note: TabNet automatically selects the epoch with the best score after `.fit()` for making predictions.","metadata":{}},{"cell_type":"code","source":"def train(\n    train_df,\n    test_df,\n    params,\n    features,\n    target,\n    verbose=1,\n    cat_idxs=None,\n    cat_dims=None\n):\n    # Variables to store test predictions, history and total best score\n    test_preds, history, total_best_score = [], {}, 0.0\n    \n    # Variable to store the data required for explainability\n    explains = {}\n    \n    for fold in range(5):\n        print(f\"Fold {fold + 1}:\")\n        \n        # Get the training and validation sets\n        train = train_df[train_df[\"kfold\"] != fold]\n        val = train_df[train_df[\"kfold\"] == fold]\n        \n        # Get the training features and labels\n        X_train = train[features].values\n        y_train = train[target].values\n        \n        # Get the validation features and labels\n        X_val = val[features].values\n        y_val = val[target].values\n        \n        # Get the init parameters\n        init_params = get_init_params(params, verbose=verbose, cat_idxs=cat_idxs, cat_dims=cat_dims)\n        \n        # Create model\n        clf = TabNetClassifier(**init_params)\n        \n        # Train model\n        clf.fit(\n            X_train=X_train,\n            y_train=y_train,\n            eval_set=[(X_train, y_train), (X_val, y_val)],\n            eval_name=[\"train\", \"valid\"],\n            eval_metric=[\"accuracy\"],\n            max_epochs=params.get(\"max_epochs\", Config.MAX_EPOCHS),\n            patience=params.get(\"patience\", Config.PATIENCE),\n            batch_size=Config.BATCH_SIZE,\n            virtual_batch_size=128,\n            num_workers=Config.NUM_WORKERS,\n            weights=1,\n            drop_last=False\n        )\n        \n        test = test_df[features].values\n        \n        # Get test predictions\n        test_pred = clf.predict_proba(test)[:, 1]\n        test_preds.append(test_pred)\n        \n        # Store data for explainability\n        explains[f\"fold_{fold}\"] = clf.explain(test)\n        \n        # Store fold history\n        history[f\"fold_{fold}\"] = clf.history\n        \n        # Get the best score and add it to total\n        # Note: The best cost is actually the accuracy here\n        # Therefore, Optuna should be set to maximize this\n        total_best_score += clf.best_cost\n        \n        print(\"\\n\\n\")\n    \n    # Get final test predictions and add to dataframe\n    test_preds = np.vstack(test_preds)\n    test_preds = np.mean(test_preds, axis=0)\n\n    result_df = test_df.copy()\n    result_df[\"preds\"] = test_preds\n    \n    # Calculate average best score\n    best_score = total_best_score / 5\n    \n    return result_df, history, best_score, explains","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.59057Z","iopub.execute_input":"2022-06-12T20:37:50.591102Z","iopub.status.idle":"2022-06-12T20:37:50.610492Z","shell.execute_reply.started":"2022-06-12T20:37:50.591056Z","shell.execute_reply":"2022-06-12T20:37:50.609472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna Objective\n\nWe will define the objective function that Optuna will optimize to find hyperparameters.","metadata":{}},{"cell_type":"code","source":"def objective(\n    trial,\n    train_df,\n    test_df,\n    features,\n    target,\n    cat_idxs=None,\n    cat_dims=None\n):\n    n_d = trial.suggest_int(\"n_d\", 8, 16, step=4)\n    \n    params = {\n        \"n_d\": n_d,\n        \"n_a\": n_d,\n        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 5),\n        \"n_shared\": trial.suggest_int(\"n_shared\", 2, 5),\n        \"cat_emb_dim\": trial.suggest_int(\"cat_emb_dim\", 1, 5),\n        \"lr\": trial.suggest_float(\"lr\", 2e-4, 2e-2),\n        \"mask_type\": trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"]),\n        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 1e-3, 3e-3, log=True),\n        \"patience\": trial.suggest_int(\"patience\", 5, 20, step=5),\n        \"max_epochs\": trial.suggest_int(\"max_epochs\", 5, 30, step=5),\n    }\n    \n    _, _, score, _ = train(\n        train_df=train_df,\n        test_df=test_df,\n        params=params,\n        features=features,\n        target=target,\n        verbose=0,\n        cat_idxs=cat_idxs,\n        cat_dims=cat_dims,\n    )\n    \n    return score","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.611918Z","iopub.execute_input":"2022-06-12T20:37:50.612906Z","iopub.status.idle":"2022-06-12T20:37:50.628077Z","shell.execute_reply.started":"2022-06-12T20:37:50.612858Z","shell.execute_reply":"2022-06-12T20:37:50.627053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Search\n\nWe will define a function that will use Optuna to find the best hyperparameters and return the found hyperparameters.","metadata":{}},{"cell_type":"code","source":"def hyperparameter_search(\n    train_df,\n    test_df,\n    features,\n    target,\n    cat_idxs=None,\n    cat_dims=None,\n    n_trials=Config.N_TRIALS\n):  \n    # Get the objective\n    objective_ = functools.partial(\n        objective,\n        train_df=train_df,\n        test_df=test_df,\n        features=features,\n        target=target,\n        cat_idxs=cat_idxs,\n        cat_dims=cat_dims,\n    )\n    \n    # Create study\n    sampler = optuna.samplers.TPESampler(seed=42)\n    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n    \n    # Enqueue a trial which uses the default values\n    study.enqueue_trial(Config.DEFAULTS)\n    \n    # Optimize\n    study.optimize(objective_, n_trials=n_trials)\n\n    return study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:37:50.62971Z","iopub.execute_input":"2022-06-12T20:37:50.630567Z","iopub.status.idle":"2022-06-12T20:37:50.640708Z","shell.execute_reply.started":"2022-06-12T20:37:50.630522Z","shell.execute_reply":"2022-06-12T20:37:50.639733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"We will first find the best hyperparameters.","metadata":{}},{"cell_type":"code","source":"print(\"Finding best hyperparameters...\")\nparams = hyperparameter_search(\n    train_df=train_df,\n    test_df=test_df,\n    features=features,\n    target=target,\n    cat_idxs=categorical_idx,\n    cat_dims=categorical_dims,\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-12T20:41:13.491219Z","iopub.execute_input":"2022-06-12T20:41:13.491805Z","iopub.status.idle":"2022-06-12T20:47:31.687649Z","shell.execute_reply.started":"2022-06-12T20:41:13.491755Z","shell.execute_reply":"2022-06-12T20:47:31.685818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best hyperparameters:\")\nparams","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:02:07.928705Z","iopub.execute_input":"2022-06-12T20:02:07.929114Z","iopub.status.idle":"2022-06-12T20:02:07.937249Z","shell.execute_reply.started":"2022-06-12T20:02:07.929075Z","shell.execute_reply":"2022-06-12T20:02:07.936366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will train the final model with the best parameters.","metadata":{}},{"cell_type":"code","source":"print(\"Training with best hyperparameters...\")\nresults, history, _, explains = train(\n    train_df=train_df,\n    test_df=test_df,\n    params=params,\n    features=features,\n    target=target,\n    cat_idxs=categorical_idx,\n    cat_dims=categorical_dims,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:02:16.591759Z","iopub.execute_input":"2022-06-12T20:02:16.592349Z","iopub.status.idle":"2022-06-12T20:05:19.917766Z","shell.execute_reply.started":"2022-06-12T20:02:16.592312Z","shell.execute_reply":"2022-06-12T20:05:19.915963Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plots","metadata":{}},{"cell_type":"markdown","source":"## Loss Curve\n\nWe will visualize the loss curve.","metadata":{}},{"cell_type":"code","source":"f, axs = plt.subplots(3, 2, figsize=(10, 8))\n\nfor fold, ax in zip(range(5), axs.flatten()):\n    fold_history = history[f\"fold_{fold}\"]\n    \n    ax.set_title(f\"Fold {fold + 1}\")\n    ax.plot(fold_history[\"loss\"])\n    ax.set_xlabel(\"Epochs\")\n    ax.set_ylabel(\"Training Loss\")\n    \nf.delaxes(axs.flatten()[-1])\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:06:36.82352Z","iopub.execute_input":"2022-06-12T20:06:36.823961Z","iopub.status.idle":"2022-06-12T20:06:37.500461Z","shell.execute_reply.started":"2022-06-12T20:06:36.823908Z","shell.execute_reply":"2022-06-12T20:06:37.499568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accuracy Curve\n\nWe will visualize the accuracy curve.","metadata":{}},{"cell_type":"code","source":"f, axs = plt.subplots(3, 2, figsize=(10, 8))\n\nfor fold, ax in zip(range(5), axs.flatten()):\n    fold_history = history[f\"fold_{fold}\"]\n    \n    ax.set_title(f\"Fold {fold + 1}\")\n    ax.plot(fold_history[\"train_accuracy\"], label=\"Train\")\n    ax.plot(fold_history[\"valid_accuracy\"], label=\"Valid\")\n    ax.set_xlabel(\"Epochs\")\n    ax.set_ylabel(\"Accuracy\")\n    ax.legend()\n    \nf.delaxes(axs.flatten()[-1])\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:06:41.876403Z","iopub.execute_input":"2022-06-12T20:06:41.876772Z","iopub.status.idle":"2022-06-12T20:06:42.62068Z","shell.execute_reply.started":"2022-06-12T20:06:41.87674Z","shell.execute_reply":"2022-06-12T20:06:42.618211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explainability\n\nBelow, we will visualize the explainability data. The y-axis is the sample number and the x-axis is the position of the feature in the dataset. A lighter color implies that that feature contributed more to the results for that sample.","metadata":{}},{"cell_type":"code","source":"n_steps = params[\"n_steps\"]\n# Change this number to see other folds\nfold = 0\n\nexplain_matrix, masks = explains[f\"fold_{0}\"]\n\nfig, axs = plt.subplots(1, n_steps, figsize=(20, 20))\n\nfor i in range(n_steps):\n    axs[i].imshow(masks[i][:50])\n    axs[i].set_xlabel(\"Features\")\n    axs[i].set_label(\"Samples\")\n    axs[i].set_title(f\"Mask {i} for fold {fold + 1}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:10:03.489248Z","iopub.execute_input":"2022-06-12T20:10:03.489712Z","iopub.status.idle":"2022-06-12T20:10:04.254382Z","shell.execute_reply.started":"2022-06-12T20:10:03.489672Z","shell.execute_reply":"2022-06-12T20:10:04.252788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = results[[\"PassengerId\", \"preds\"]]\nsubmission = submission.rename(columns={\"preds\": \"Transported\"})\nsubmission[\"Transported\"] = submission[\"Transported\"] >= 0.5\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:10:23.76933Z","iopub.execute_input":"2022-06-12T20:10:23.770239Z","iopub.status.idle":"2022-06-12T20:10:23.788212Z","shell.execute_reply.started":"2022-06-12T20:10:23.770198Z","shell.execute_reply":"2022-06-12T20:10:23.78676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"Transported\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-11T22:37:36.631991Z","iopub.execute_input":"2022-06-11T22:37:36.632497Z","iopub.status.idle":"2022-06-11T22:37:36.64323Z","shell.execute_reply.started":"2022-06-11T22:37:36.632449Z","shell.execute_reply":"2022-06-11T22:37:36.642434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-11T22:37:38.734964Z","iopub.execute_input":"2022-06-11T22:37:38.735323Z","iopub.status.idle":"2022-06-11T22:37:38.748166Z","shell.execute_reply.started":"2022-06-11T22:37:38.735293Z","shell.execute_reply":"2022-06-11T22:37:38.747445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nIn this notebook, we explored how TabNet coupled with Optuna can be used to achieve good performance on the Spaceship Titanic dataset.","metadata":{}},{"cell_type":"markdown","source":"# References\n\n\n<a id=\"#ref1\">[1]</a> Sercan O. ArÄ±k; and Tomas Pfister. 2019. [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/pdf/1908.07442v5.pdf). Google Cloud AI.","metadata":{}}]}