{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Run Light GBM Model\nhttps://www.kaggle.com/pietromarinelli/8th-place-at-day-1-with-lgb-with-few-features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Read in libraries\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom sklearn import preprocessing\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom random import random\n\nimport datetime\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Print Directories\")\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\nsub = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/submission.csv\")\nprint(\"read in files\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this part we append our testing data to the training data at the date where this is no overlap. Today the train set went up to March 30th."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.append(test[test['Date']>'2020-03-31'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fix dates\nimport datetime \ntrain['Date'] = pd.to_datetime(train['Date'], format='%Y-%m-%d')\ntrain['day_dist'] = train['Date']-train['Date'].min()\ntrain['day_dist'] = train['day_dist'].dt.days\nprint(train['Date'].max())\n#print(val['Date'].max())\nprint(test['Date'].min())\nprint(test['Date'].max())\n#print(test['Date'].max()-test['Date'].min())\ncat_cols = train.dtypes[train.dtypes=='object'].keys()\ncat_cols\n\n#fix na\nfor cat_col in cat_cols:\n    train[cat_col].fillna('no_value', inplace = True)\n\n#make a place variable \ntrain['place'] = train['Province_State']+'_'+train['Country_Region']\n#vcheck = train[(train['Date']>='2020-03-12')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the cat columns\ncat_cols = train.dtypes[train.dtypes=='object'].keys()\ncat_cols\n\n#label the columns\nfor cat_col in ['place']:\n    #train[cat_col].fillna('no_value', inplace = True) #train[cat_col].value_counts().idxmax()\n    le = preprocessing.LabelEncoder()\n    le.fit(train[cat_col])\n    train[cat_col]=le.transform(train[cat_col])\n    \n#check train keys \ntrain.keys()\n\n#set columns were going to drop during our training stage\ndrop_cols = ['Id', 'ConfirmedCases','Date', 'ForecastId','Fatalities','day_dist', 'Province_State', 'Country_Region']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point you want to set your validation set to be the section of time that is in the overlap period with train"},{"metadata":{"trusted":true},"cell_type":"code","source":"#val = train[(train['Id']).isnull()==True]\n#train = train[(train['Id']).isnull()==False]\nval = train[(train['Date']>='2020-03-19')&(train['Id'].isnull()==False)]\n#test = train[(train['Date']>='2020-03-12')&(train['Id'].isnull()==True)]\n#train = train[(train['Date']<'2020-03-22')&(train['Id'].isnull()==False)]\n#val = train\nval","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_ft = train[\"Fatalities\"]\ny_val_ft = val[\"Fatalities\"]\n\ny_cc = train[\"ConfirmedCases\"]\ny_val_cc = val[\"ConfirmedCases\"]\n\n#train.drop(drop_cols, axis=1, inplace=True)\n#test.drop(drop_cols, axis=1, inplace=True)\n#val.drop(drop_cols, axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define scoring functions\ndef rmsle (y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2)))\n\ndef mape (y_true, y_pred):\n    return np.mean(np.abs(y_pred -y_true)*100/(y_true+1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set params for lgbt\nparams = {\n    \"objective\": \"regression\",\n    \"boosting\": 'gbdt', #\"gbdt\",\n    \"num_leaves\": 1280,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.9, # 0.9,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\",\n    'min_data_in_leaf':20\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get dates for iterating over \ndates = test['Date'].unique()\ndates","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another tricky part, set this to be the same date as the stacked test data set from the beginning. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#subset them for the relevant dates\ndates = dates[dates>'2020-03-31']\ndates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(dates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfold_n = 0\nfor date in dates:\n\n    fold_n = fold_n +1 \n    i = i+1\n    if i==1:\n        nrounds = 200\n    else:\n        nrounds = 100\n    print(i)\n    print(nrounds)\n    train['shift_1_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i)\n    train['shift_2_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+1)\n    train['shift_3_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+2)\n    train['shift_4_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+3)\n    train['shift_5_cc'] = train.groupby(['place'])['ConfirmedCases'].shift(i+4)\n        \n    val2 = train[train['Date']==date]\n    train2 = train[(train['Date']<date)]\n    y_cc = train2[\"ConfirmedCases\"]\n    #y_val_cc = val2[\"ConfirmedCases\"]\n    \n    train2.drop(drop_cols, axis=1, inplace=True)\n    val2.drop(drop_cols, axis=1, inplace=True)\n    \n    #np.log1p(y)\n    #feature_importances = pd.DataFrame()\n    #feature_importances['feature'] = train.keys()\n    \n    #score = 0       \n    dtrain = lgb.Dataset(train2, label=y_cc)\n    dvalid = lgb.Dataset(val2, label=y_val_cc)\n\n    model = lgb.train(params, dtrain, nrounds, \n                            #valid_sets = [dtrain, dvalid],\n                            categorical_feature = ['place'], #'Province/State', 'Country/Region'\n                            verbose_eval=False)#, early_stopping_rounds=50)\n\n    y_pred = model.predict(val2,num_iteration=nrounds)  #model.best_iteration\n    #y_pred = np.expm1( y_pred)\n    #vcheck.loc[vcheck['Date']==date,'cc_predict'] = y_pred\n    test.loc[test['Date']==date,'ConfirmedCases'] = y_pred\n    train.loc[train['Date']==date,'ConfirmedCases'] = y_pred\n    #y_oof[valid_index] = y_pred\n\n    #rmsle_score = rmsle(y_val_cc, y_pred)\n    #mape_score = mape(y_val_cc, y_pred)\n    #score += rmsle_score\n    #print (f'fold: {date}, rmsle: {rmsle_score:.5f}' )\n    #print (f'fold: {date}, mape: {mape_score:.5f}' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = model.predict(val2,num_iteration=nrounds) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['Country_Region']=='Italy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\nfold_n = 0\nfor date in dates:\n\n    fold_n = fold_n +1 \n    i = i+1\n    if i==1:\n        nrounds = 200\n    else:\n        nrounds = 100\n    print(i)\n    print(nrounds)\n    \n    train['shift_1_cc'] = train.groupby(['place'])['Fatalities'].shift(i)\n    train['shift_2_cc'] = train.groupby(['place'])['Fatalities'].shift(i+1)\n    train['shift_3_cc'] = train.groupby(['place'])['Fatalities'].shift(i+2)\n    train['shift_4_cc'] = train.groupby(['place'])['Fatalities'].shift(i+3)\n    train['shift_5_cc'] = train.groupby(['place'])['Fatalities'].shift(i+4)\n        \n    val2 = train[train['Date']==date]\n    train2 = train[(train['Date']<date)]\n    y_ft = train2[\"Fatalities\"]\n    #y_val_cc = val2[\"ConfirmedCases\"]\n    \n    train2.drop(drop_cols, axis=1, inplace=True)\n    val2.drop(drop_cols, axis=1, inplace=True)\n    \n    #np.log1p(y)\n    #feature_importances = pd.DataFrame()\n    #feature_importances['feature'] = train.keys()\n    \n    #score = 0       \n    dtrain = lgb.Dataset(train2, label=y_ft)\n    dvalid = lgb.Dataset(val2, label=y_val_ft)\n\n    model = lgb.train(params, dtrain, nrounds, \n                            #valid_sets = [dtrain, dvalid],\n                            categorical_feature = ['place'], #'Province/State', 'Country/Region'\n                            verbose_eval=False)#, early_stopping_rounds=50)\n\n    y_pred = model.predict(val2,num_iteration=nrounds)  #model.best_iteration\n    #y_pred = np.expm1( y_pred)\n    #vcheck.loc[vcheck['Date']==date,'cc_predict'] = y_pred\n    test.loc[test['Date']==date,'Fatalities'] = y_pred\n    train.loc[train['Date']==date,'Fatalities'] = y_pred\n    #y_oof[valid_index] = y_pred\n\n    #rmsle_score = rmsle(y_val_cc, y_pred)\n    #mape_score = mape(y_val_cc, y_pred)\n    #score += rmsle_score\n    #print (f'fold: {date}, rmsle: {rmsle_score:.5f}' )\n    #print (f'fold: {date}, mape: {mape_score:.5f}' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['Country_Region']=='Italy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sub = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_sub.loc[(train_sub['Date']=='2020-03-24')&(train_sub['Country_Region']=='France')&(train_sub['Province_State']=='France'),'ConfirmedCases'] = 22654\n#train_sub.loc[(train_sub['Date']=='2020-03-24')&(train_sub['Country_Region']=='France')&(train_sub['Province_State']=='France'),'Fatalities'] = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.merge(test,train_sub[['Province_State','Country_Region','Date','ConfirmedCases','Fatalities']], on=['Province_State','Country_Region','Date'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['ConfirmedCases_x'].isnull()==True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['ConfirmedCases_x'].isnull()==True, 'ConfirmedCases_x'] = test.loc[test['ConfirmedCases_x'].isnull()==True, 'ConfirmedCases_y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.loc[test['Fatalities_x'].isnull()==True, 'Fatalities_x'] = test.loc[test['Fatalities_x'].isnull()==True, 'Fatalities_y']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#last_amount = test.loc[(test['Country_Region']=='Italy')&(test['Date']=='2020-03-24'),'ConfirmedCases_x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#last_fat = test.loc[(test['Country_Region']=='Italy')&(test['Date']=='2020-03-24'),'Fatalities_x']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#last_fat.values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(dates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#30/29\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#i = 0\n#k = 35","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#for date in dates:\n#    k = k-1\n#    i = i + 1\n#    test.loc[(test['Country_Region']=='Italy')&(test['Date']==date),'ConfirmedCases_x'] =  last_amount.values[0]+i*(5000-(100*i))\n#    test.loc[(test['Country_Region']=='Italy')&(test['Date']==date),'Fatalities_x'] =  last_fat.values[0]+i*(800-(10*i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test.loc[(test['Country_Region']=='Italy')] #&(test['Date']==date),'ConfirmedCases_x' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = test[['ForecastId', 'ConfirmedCases_x','Fatalities_x']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.columns = ['ForecastId', 'ConfirmedCases', 'Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[sub['ConfirmedCases']<0, 'ConfirmedCases'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[sub['Fatalities']<0, 'Fatalities'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Fatalities'].describe()\nsub.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename submission columns \n#sub = sub.rename(columns={'ForecastId': 'newName1', 'ConfirmedCases': 'Fatalities'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run ARIMA Influenza Baselines \nhttps://www.kaggle.com/skeller/arima-influenza-baselines"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Read in libraries\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom random import random\n\nprint(\"read in train file\")\ndf=pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\",\n               usecols=['Province_State','Country_Region','Date','ConfirmedCases','Fatalities'])\n\nprint(\"fill blanks and add region for counting\")\ndf.fillna(' ',inplace=True)\ndf['Lat']=df['Province_State']+df['Country_Region']\ndf.drop('Province_State',axis=1,inplace=True)\ndf.drop('Country_Region',axis=1,inplace=True)\n\ncountries_list=df.Lat.unique()\ndf1=[]\nfor i in countries_list:\n    df1.append(df[df['Lat']==i])\nprint(\"we have \"+ str(len(df1))+\" regions in our dataset\")\n\n#read in test file \ntest=pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\n\n#create the estimates assuming measurement error \nsubmit_confirmed=[]\nsubmit_fatal=[]\nfor i in df1:\n    # contrived dataset\n    data = i.ConfirmedCases.astype('int32').tolist()\n    # fit model\n    try:\n        #model = SARIMAX(data, order=(2,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        model = SARIMAX(data, order=(1,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        #model = SARIMAX(data, order=(1,1,0), seasonal_order=(0,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        #model = ARIMA(data, order=(3,1,2))\n        model_fit = model.fit(disp=False)\n        # make prediction\n        predicted = model_fit.predict(len(data), len(data)+34)\n        new=np.concatenate((np.array(data),np.array([int(num) for num in predicted])),axis=0)\n        submit_confirmed.extend(list(new[-43:]))\n    except:\n        submit_confirmed.extend(list(data[-10:-1]))\n        for j in range(34):\n            submit_confirmed.append(data[-1]*2)\n    \n    # contrived dataset\n    data = i.Fatalities.astype('int32').tolist()\n    # fit model\n    try:\n        #model = SARIMAX(data, order=(1,0,0), seasonal_order=(0,1,1,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        model = SARIMAX(data, order=(1,1,0), seasonal_order=(1,1,0,12),measurement_error=True)#seasonal_order=(1, 1, 1, 1))\n        #model = ARIMA(data, order=(3,1,2))\n        model_fit = model.fit(disp=False)\n        # make prediction\n        predicted = model_fit.predict(len(data), len(data)+34)\n        new=np.concatenate((np.array(data),np.array([int(num) for num in predicted])),axis=0)\n        submit_fatal.extend(list(new[-43:]))\n    except:\n        submit_fatal.extend(list(data[-10:-1]))\n        for j in range(34):\n            submit_fatal.append(data[-1]*2)\n            \n#make the submission file \ndf_submit=pd.concat([pd.Series(np.arange(1,1+len(submit_confirmed))),pd.Series(submit_confirmed),pd.Series(submit_fatal)],axis=1)\ndf_submit=df_submit.fillna(method='pad').astype(int)\n\n#view submission file \ndf_submit.head()\n#df_submit.dtypes\n\n#join the submission file info to the test data set \n#rename the columns \ndf_submit.rename(columns={0: 'ForecastId', 1: 'ConfirmedCases',2: 'Fatalities',}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine both into average"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rename \nsub.rename(columns={'ForecastId': 'ForecastId1', \"ConfirmedCases\": 'Confirmed_lgbt',\"Fatalities\": 'Fatalities_lgbt',}, inplace=True)\ndf_submit.rename(columns={'ForecastId': 'ForecastId2', \"ConfirmedCases\": 'Confirmed_arima',\"Fatalities\": 'Fatalities_arima',}, inplace=True)\ndf_submit.shape\n\n#combine \ndf_combine = pd.concat([sub, df_submit], axis=1, join='inner')\n\n#average \ncols = ['Confirmed_lgbt','Confirmed_arima']\ndf_combine['ConfirmedCases'] = df_combine[cols].astype(float).mean(axis=1)\n\ncols = ['Fatalities_lgbt','Fatalities_arima']\ndf_combine['Fatalities'] = df_combine[cols].astype(float).mean(axis=1)\n\n#drop\ndel df_combine['ForecastId2']\ndel df_combine['Confirmed_lgbt']\ndel df_combine['Fatalities_lgbt']\ndel df_combine['Confirmed_arima']\ndel df_combine['Fatalities_arima']\n\ndf_combine.head()\n\n#rename\ndf_combine.rename(columns={'ForecastId1': 'ForecastId', \"ConfirmedCases\": 'ConfirmedCases',\"Fatalities\": 'Fatalities',}, inplace=True)\n\n#make submission file\ndf_combine.to_csv('submission.csv',index=False)\n\n#make complete test file \ntest=pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\ncomplete_test= pd.merge(test, df_combine, how=\"left\", on=\"ForecastId\")\ncomplete_test.to_csv('complete_test.csv',index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}