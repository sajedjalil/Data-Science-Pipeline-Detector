{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting COVID-19 Infections with LSTM\n\n> *by Yohan Chung*\n\nThe pandemic of Coronavirus (COVID-19) became reality and many countries around the globe strive to contain further spread of the virus with social distancing as well as qurantining of those who contact the infected. The project aims to predict future of the infected by country and identify which country needs more attention. The prediction of the newly infected starts with reading data from the files train.csv and test.csv as below. \n\nThe datatsets are provided by Johns Hopkins University and include the COVID-19 confirmed cases & fatalities by country."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport tensorflow as tf\n\n# pd.set_option('display.float_format', lambda x: '%.20f' % x)\n\ntrain_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv') # historical data\ntest_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/test.csv') # predictions to be filled","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Cleansing & Transforming Data*\n\nI processed data before using them for modelling and details are below.\n* Filling blank cells with default value\n* Removing special char which can malfunction computation\n* Converting datetime to day count from the starting date of data\n* Scaling data to lower values"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = train_df.fillna({'Province_State': 'Unknown'})\ntest_df = test_df.fillna({'Province_State': 'Unknown'})\ntrain_df['Country_Region']= train_df['Country_Region'].str.replace(\"'\", \"\")\ntrain_df['Province_State']= train_df['Province_State'].str.replace(\"'\", \"\")\ntest_df['Country_Region']= test_df['Country_Region'].str.replace(\"'\", \"\")\ntest_df['Province_State']= test_df['Province_State'].str.replace(\"'\", \"\")\ntrain_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_datetime(dt):\n    return datetime.datetime.strptime(dt, '%Y-%m-%d')\n\ndef count_days(dt):\n    return (dt - datetime.datetime.strptime('2020-01-22', \"%Y-%m-%d\")).days\n\ntrain_df['Date_dt'] = train_df['Date'].map(to_datetime)\ntrain_df['Day'] = train_df['Date_dt'].map(count_days)\ntest_df['Date_dt'] = test_df['Date'].map(to_datetime)\ntest_df['Day'] = test_df['Date_dt'].map(count_days)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n# Min-Max Scaler\nscaler_c = MinMaxScaler(feature_range=(0, 100))\ntrain_df['ConfirmedCases_scaled'] = None\ntrain_df[['ConfirmedCases_scaled']] = scaler_c.fit_transform(train_df[['ConfirmedCases']])\n\nscaler_f = MinMaxScaler(feature_range=(0, 100))\ntrain_df['Fatalities_scaled'] = None\ntrain_df[['Fatalities_scaled']] = scaler_f.fit_transform(train_df[['Fatalities']])\n\n# Get dummy columns for geo location\ngeo_columns = []\nfor i in range(294):\n    geo_columns.append('Geo_{}'.format(i))\ntrain_df.drop(columns=geo_columns, inplace=True, errors='ignore')\n\nlbl_encoder = LabelEncoder()\nscaler_g = MinMaxScaler(feature_range=(0, 1))\nhot_encoder = OneHotEncoder(sparse=False)\ntrain_df['Geo'] = train_df['Country_Region'].astype(str) + '_' + train_df['Province_State'].astype(str)\ntrain_df[['Geo']] = lbl_encoder.fit_transform(train_df[['Geo']])\ntrain_df = pd.get_dummies(train_df, prefix_sep=\"_\", columns=['Geo'])\n\ntrain_df[['ConfirmedCases', 'ConfirmedCases_scaled', 'Fatalities', 'Fatalities_scaled',  'Geo_0']].head()    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Preparing Model Input & Target*\n\nIn this step, I prepare senquential input data with the historical steps of 7, which will be time series to predict the confirmed cases on the next day."},{"metadata":{"trusted":true},"cell_type":"code","source":"historical_steps = 7\nn_output_node = 1\n\ndef make_sequential_input(df):\n    \n    inputs_c, inputs_f, inputs_geo, targets_c, targets_f = [], [], [], [], []\n    \n    for i in range(len(df) - historical_steps - 1):\n        \n        if df.iloc[i]['Country_Region'] == df.iloc[i + historical_steps]['Country_Region'] and \\\n            df.iloc[i]['Province_State'] == df.iloc[i + historical_steps]['Province_State']:\n            \n            # iloc[a:b] startnig from index 'a' and ending before b\n            inputs_c.append(np.array(df.iloc[i : i + historical_steps][['ConfirmedCases_scaled']]).tolist()) # time seires until t-1\n            inputs_f.append(np.array(df.iloc[i : i + historical_steps][['Fatalities_scaled']]).tolist()) # time seires until t-1\n            inputs_geo.append(np.array(df.iloc[i + historical_steps][geo_columns]).tolist())\n            targets_c.append(np.array(df.iloc[i + historical_steps][['ConfirmedCases_scaled']]).tolist()) # result data at time t\n            targets_f.append(np.array(df.iloc[i + historical_steps][['Fatalities_scaled']]).tolist()) # result data at time t\n              \n    return inputs_c, inputs_f, inputs_geo, targets_c, targets_f\n\n# Make sequential input for training and validation\ntrain_inputs, train_inputs_f, train_inputs_geo, train_targets_c, train_targets_f = make_sequential_input(train_df)\n\nprint('Train input shape: {}'.format(np.shape(train_inputs)))\nprint('Train input geo shape: {}'.format(np.shape(train_inputs_geo)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I extract validataion dataset out of the prepared for modelling."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nmax_index = np.array(train_inputs).shape[0] - 1\nindices = []\n\nfor i in range(int(max_index*0.20)):\n    indices.append(random.randint(0, max_index))\n\nval_inputs = [ train_inputs[i] for i in indices ]\nval_inputs_f = [ train_inputs_f[i] for i in indices ]\nval_inputs_geo = [ train_inputs_geo[i] for i in indices  ] \nval_targets_c = [ train_targets_c[i] for i in indices ]\nval_targets_f = [ train_targets_f[i] for i in indices ]\n\ntrain_inputs = [ elem for i, elem in enumerate(train_inputs) if i not in indices ] \ntrain_inputs_f = [ elem for i, elem in enumerate(train_inputs_f) if i not in indices ] \ntrain_inputs_geo = [ elem for i, elem in enumerate(train_inputs_geo) if i not in indices ] \ntrain_targets_c = [ elem for i, elem in enumerate(train_targets_c) if i not in indices ] \ntrain_targets_f = [ elem for i, elem in enumerate(train_targets_f) if i not in indices ] \n\npd.set_option('display.max_colwidth', -1)\nprint('No. train data: {}'.format(len(train_inputs)))\nprint('No. validation data: {}'.format(len(val_inputs)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM Model \n> Baseline\n\nNote that I use mean squared log error which give more penalty on underestimation over overestimation, since the trend is likely to grow up."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\n#### Train the model ####\nn_output_node = 1\ninput_shape=np.array(train_inputs).shape[-2:]\n\nbatch_size = 64\nepochs = 200\nlr = 0.001\n\ndef create_model(inputs, targets, val_inputs, val_targets):\n    \n    model = Sequential()\n    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n    model.add(Dropout(0.05))\n    model.add(LSTM(32))\n    model.add(Dropout(0.05))\n    model.add(Dense(n_output_node, activation='relu'))\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=tf.keras.losses.MSLE, metrics=[ tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanSquaredError() ])\n\n    history = model.fit(inputs, targets, \\\n              epochs=epochs, \\\n              batch_size=batch_size, \\\n              validation_data=(val_inputs, val_targets))\n\n    scores = model.evaluate(inputs, targets)\n    print(\"Model Accuracy: {}\".format(scores))\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss over epochs')\n    plt.legend(['Train', 'Validation'], loc='best')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.show()\n    \n    return model\n\nprint('Tarining model for ConfirmedCases')\nmodel_cases = create_model(train_inputs, train_targets_c, val_inputs, val_targets_c)\nprint('Tarining model for Fatalities')\nmodel_fatality = create_model(train_inputs_f, train_targets_f, val_inputs_f, val_targets_f)\n\nmodel_cases.save('model_cases')\nmodel_fatality.save('model_fatality')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM with Initial State\n> Advanced"},{"metadata":{},"cell_type":"markdown","source":"The following codes were used when creating LSTM model with time series data for confirmed cases, as well as geo location. However, it did not give me higher accuracy versus the aformentioned model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n\nn_output_node = 1\ninput_shape=np.array(train_inputs).shape[-2:]\ninput_shape_geo=np.array(train_inputs_geo).shape\n\nbatch_size = 64\nepochs = 200\nlr = 0.001\n\n\ndef create_model(inputs, inputs_geo, targets, v_inputs, v_inputs_geo, v_targets):\n    \n    geo_input = Input(shape=(294,), name='input_geo')\n    h_state = Dense(64, activation='relu')(geo_input)\n    h_state = Dense(64, activation='relu')(h_state)\n    c_state = Dense(64, activation='relu')(geo_input)\n    c_state = Dense(64, activation='relu')(c_state)\n\n    ts_input = Input(shape=input_shape, name='input_ts')\n    lstm = LSTM(64, return_sequences=True)(ts_input, initial_state=[ h_state, c_state ])\n    lstm = Dropout(0.05)(lstm)\n    lstm = LSTM(32)(lstm)\n    lstm = Dropout(0.05)(lstm)\n    main_output = Dense(n_output_node, activation='relu', name='output_main')(lstm)\n    \n    model = Model(inputs=[ geo_input, ts_input ], outputs=main_output)\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=tf.keras.losses.MSLE, metrics=[ tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanSquaredError() ])\n    \n    history = model.fit([ inputs_geo, inputs ],  targets, \\\n              epochs=epochs, \\\n              batch_size=batch_size, \\\n              validation_data=({ 'input_geo': v_inputs_geo, 'input_ts': v_inputs },{ 'output_main': v_targets}))\n\n    scores = model.evaluate({ 'input_geo': inputs_geo, 'input_ts': inputs }, targets)\n    print(\"Model Accuracy: {}\".format(scores))\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss over epochs')\n    plt.legend(['Train', 'Validation'], loc='best')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.show()\n    \n    return model\n                        \n    \n\nmodel_cases = create_model(np.array(train_inputs), np.array(train_inputs_geo), np.array(train_targets_c), np.array(val_inputs), np.array(val_inputs_geo), np.array(val_targets_c))\nmodel_fatality = create_model(np.array(train_inputs_f), np.array(train_inputs_geo), np.array(train_targets_f), np.array(val_inputs_f), np.array(val_inputs_geo), np.array(val_targets_f))\n\nmodel_cases.save('model_c_with_state')\nmodel_fatality.save('model_f_with_state')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Interpreting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cases = tf.keras.models.load_model('model_c_with_state')\nmodel_fatality = tf.keras.models.load_model('model_f_with_state')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_cases(country, state):\n    \n    df = train_df[(train_df['Country_Region'] == country) & (train_df['Province_State'] == state) ]\n\n    inputs = np.array(df[['ConfirmedCases_scaled']][-historical_steps-1:-1])\n    inputs_geo = np.array(df.iloc[-1][geo_columns])\n    actuals = np.array(df.iloc[-1][['ConfirmedCases']])\n    \n    predictions = model_cases.predict([  np.array(inputs_geo).astype(np.float32).reshape(1, len(geo_columns)), np.array(inputs).reshape(1, input_shape[0], input_shape[1]) ]).reshape(-1).tolist()\n    \n    print('Inputs: {}, Pred: {}, Expected: {}'.format( \\\n        np.array(df[['ConfirmedCases']][-historical_steps-1:-1])[:,0].tolist(), \\\n        scaler_c.inverse_transform(np.array(predictions).reshape(-1,1)), \\\n        actuals))\n    \ndef predict_fatality(country, state):\n    \n    df = train_df[(train_df['Country_Region'] == country) & (train_df['Province_State'] == state) ]\n\n    inputs = np.array(df[['Fatalities_scaled']][-historical_steps-1:-1])\n    inputs_geo = np.array(df.iloc[-1][geo_columns])\n    actuals = np.array(df.iloc[-1][['Fatalities']])\n    \n    predictions = model_fatality.predict([  np.array(inputs_geo).astype(np.float32).reshape(1, len(geo_columns)), np.array(inputs).reshape(1, input_shape[0], input_shape[1]) ]).reshape(-1).tolist()\n    \n    print('Inputs: {}, Pred: {}, Expected: {}'.format( \\\n        np.array(df[['Fatalities']][-historical_steps-1:-1])[:,0].tolist(), \\\n        scaler_f.inverse_transform(np.array(predictions).reshape(-1,1)), \\\n        actuals))\n      \n\npredict_cases('Australia', 'Victoria')\npredict_cases('Australia', 'New South Wales')\npredict_cases('Korea, South', 'Unknown')\npredict_cases('Iran', 'Unknown')\npredict_cases('Italy', 'Unknown')\n\npredict_fatality('Australia', 'Victoria')\npredict_fatality('Australia', 'New South Wales')\npredict_fatality('Korea, South', 'Unknown')\npredict_fatality('Iran', 'Unknown')\npredict_fatality('Italy', 'Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ntest_df['ConfirmedCases'] = None\ntest_df['Fatalities'] = None\ngeo_df = None\n\ntic = time.perf_counter()\ntemp_df = train_df\n\n# For each test row\nfor i in range(len(test_df)):\n    \n    if i%1000 == 0:\n        toc = time.perf_counter()\n        print('Looping throught the index {} - {:.2f} sec(s) taken...'.format(i, (toc-tic)))\n   \n    current = test_df.iloc[i]         \n    geo_df = temp_df[(temp_df.Country_Region == current.Country_Region) & (temp_df.Province_State == current.Province_State) & (temp_df.Day >= (current.Day - historical_steps)) & (temp_df.Day <= (current.Day))]\n        \n     # Find historical steps in train data\n    if not geo_df.empty and geo_df.shape[0] >= 7:                 \n        \n        \n        if geo_df.shape[0] == 8:\n            input_geo = np.array(geo_df.iloc[-historical_steps-1][geo_columns]).astype(np.float32).reshape(1, len(geo_columns))\n            pred = model_cases.predict([ input_geo, np.array(geo_df.iloc[-historical_steps-1:-1][['ConfirmedCases_scaled']]).reshape(1, input_shape[0], input_shape[1]) ])\n            pred_f = model_fatality.predict([ input_geo, np.array(geo_df.iloc[-historical_steps-1:-1,][['Fatalities_scaled']]).reshape(1, input_shape[0], input_shape[1]) ])\n        else:\n            input_geo = np.array(geo_df.iloc[-historical_steps][geo_columns]).astype(np.float32).reshape(1, len(geo_columns))\n            pred = model_cases.predict([ input_geo, np.array(geo_df.iloc[-historical_steps:,][['ConfirmedCases_scaled']]).reshape(1, input_shape[0], input_shape[1]) ])\n            pred_f = model_fatality.predict([ input_geo, np.array(geo_df.iloc[-historical_steps:,][['Fatalities_scaled']]).reshape(1, input_shape[0], input_shape[1]) ])\n              \n        test_df.loc[i, 'ConfirmedCases_scaled'] = pred[0][0]\n        test_df.loc[i, 'Fatalities_scaled'] = pred_f[0][0]\n        \n        # Save current data in train_df for next if empty\n        if geo_df.iloc[-1:,].Day.values[0] != current.Day:  \n            \n            new_item = { 'ConfirmedCases_scaled': pred[0][0], 'Fatalities_scaled': pred_f[0][0], 'Day': current.Day, \\\n                         'Country_Region': current.Country_Region, 'Province_State': current.Province_State }\n            \n            for j in range(len(geo_columns)):\n                new_item['Geo_' + str(j)] = 1 if geo_df.iloc[-1:,]['Geo_' + str(j)].values[0] == 1 else 0\n            \n            temp_df = temp_df.append(new_item, ignore_index=True)\n\ntest_df[['ConfirmedCases']] = scaler_c.inverse_transform(test_df[['ConfirmedCases_scaled']])\ntest_df[['Fatalities']] = scaler_f.inverse_transform(test_df[['Fatalities_scaled']]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[(test_df.Country_Region=='Australia') & (test_df.Province_State=='New South Wales')].iloc[-35:,:]\n# temp_df[(temp_df.Country_Region=='Australia') & (temp_df.Province_State=='New South Wales')].iloc[-35:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_by_country(country, state):\n\n    from_day_predicting = 57\n    hist_df = train_df[(train_df.Country_Region == country) & (train_df.Province_State == state)].groupby(['Country_Region', 'Province_State', 'Day', 'Date']).agg({'ConfirmedCases': 'sum'}).reset_index()\n    pred_df = test_df[(test_df.Country_Region == country) & (test_df.Province_State == state)].groupby(['Country_Region', 'Province_State', 'Day', 'Date']).agg({'ConfirmedCases': 'sum'}).reset_index()\n\n    plt.title('{}, {}'.format(state, country))\n    plt.plot(hist_df.Day, hist_df.ConfirmedCases, label='Historical')\n    plt.plot(pred_df.Day, pred_df.ConfirmedCases, label='Predictive')\n    plt.axvline(x=57, color='r', linestyle='--', linewidth=1, label='2019-03-31')\n    plt.xlabel('Day')\n    plt.ylabel('Cases')\n    plt.legend()\n    plt.show()\n    \n[ plot_by_country(country, state) for country, state in [ \\\n    ('Australia', 'New South Wales'), ('Australia', 'Victoria'), ('Korea, South', 'Unknown'), ('China', 'Hubei'), ('Italy', 'Unknown')]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[['ForecastId', 'ConfirmedCases', 'Fatalities']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}