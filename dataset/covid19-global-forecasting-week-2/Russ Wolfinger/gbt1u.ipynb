{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, gc\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as ctb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom datetime import date, datetime, timedelta\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.neighbors import KNeighborsRegressor, NearestNeighbors\nfrom scipy.optimize import nnls\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 10)\nnp.set_printoptions(precision=6, suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# note: all data update 2020-04-01, china rule, smooth weights\nmname = 'gbt1u'\npath = '/kaggle/input/gbt1n-external/'\npathk = '/kaggle/input/covid19-global-forecasting-week-2/'\nnhorizon = 30\nskip = 0\n# kv = [3]\n# kv = [6]\nkv = [6,11]\n# kv = [13]\ntrain_full = True\nsave_data = False\n\n# booster = ['lgb','xgb']\nbooster = ['lgb','xgb','ctb']\n# booster = ['cas']\n\n# if using updated daily data, also update time-varying external data\n# in COVID-19 and covid-19-data, git pull origin master \n# ecdc wget https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\n# weather: https://www.kaggle.com/davidbnn92/weather-data/output?scriptVersionId=31103959\n# google trends: pytrends0b.ipynb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(pathk+'train.csv')\ntest = pd.read_csv(pathk+'test.csv')\nss = pd.read_csv(pathk+'submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tmax and dmax are the last day of training\ntmax = train.Date.max()\ndmax = datetime.strptime(tmax,'%Y-%m-%d').date()\nprint(tmax, dmax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ddate is the last day of validation training\nddate = dmax - timedelta(days=nhorizon)\nddate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fmax = test.Date.max()\nfdate = datetime.strptime(fmax,'%Y-%m-%d').date()\nfdate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmin = train.Date.min()\nfmin = test.Date.min()\ntmin, fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dmin = datetime.strptime(tmin,'%Y-%m-%d').date()\nprint(dmin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['ForecastId'] = train.Id - train.Id.max()\ncp = ['Country_Region','Province_State']\ncpd = cp + ['Date']\ntrain = train.merge(test[cpd+['ForecastId']], how='left', on=cpd)\ntrain['ForecastId'] = train['ForecastId'].fillna(0).astype(int)\ntrain['y0_pred'] = np.nan\ntrain['y1_pred'] = np.nan\n\ntest['Id'] = test.ForecastId + train.Id.max()\ntest['ConfirmedCases'] = np.nan\ntest['Fatalities'] = np.nan\n# use zeros here instead of nans so monotonic adjustment fills final dates if necessary\ntest['y0_pred'] = 0.0\ntest['y1_pred'] = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat non-overlapping part of test to train for feature engineering\nd = pd.concat((train,test[test.Date > train.Date.max()])).reset_index(drop=True)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(dmin + timedelta(30)).isoformat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['Date'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing province with blank, must also do this with external data before merging\nd[cp] = d[cp].fillna('')\n\n# create single location variable\nd['Loc'] = d['Country_Region'] + ' ' + d['Province_State']\nd['Loc'] = d['Loc'].str.strip()\nd['Loc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort by location then date\nd = d.sort_values(['Loc','Date']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['Country_Region'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['Province_State'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gt = pd.read_csv(path+'google_trends.csv')\ngt[cp] = gt[cp].fillna('')\ngt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# since trends data lags behind a day or two, shift the date to make it contemporaneous\ngmax = gt.Date.max()\ngmax = datetime.strptime(gmax,'%Y-%m-%d').date()\ngoff = (dmax - gmax).days\nprint(dmax, gmax, goff)\ngt['Date'] = (pd.to_datetime(gt.Date) + timedelta(goff)).dt.strftime('%Y-%m-%d')\ngt['google_covid'] = gt['coronavirus'] + gt['covid-19'] + gt['covid19']\ngt.drop(['coronavirus','covid-19','covid19'], axis=1, inplace=True)\ngoogle = ['google_covid']\ngt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = d.merge(gt, how='left', on=['Country_Region','Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d['google_covid'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge country info\ncountry = pd.read_csv(path+'covid19countryinfo1.csv')\n# country[\"pop\"] = country[\"pop\"].str.replace(\",\",\"\").astype(float)\ncountry","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first merge by country\nd = d.merge(country.loc[country.medianage.notnull(),['country','pop','testpop','medianage']],\n            how='left', left_on='Country_Region', right_on='country')\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# then merge by province\nc1 = country.loc[country.medianage.isnull(),['country','pop','testpop']]\nprint(c1.shape)\nc1.columns = ['Province_State','pop1','testpop1']\n# d.update(c1)\nd = d.merge(c1,how='left',on='Province_State')\nd.loc[d.pop1.notnull(),'pop'] = d.loc[d.pop1.notnull(),'pop1']\nd.loc[d.testpop1.notnull(),'testpop'] = d.loc[d.testpop1.notnull(),'testpop1']\nd.drop(['pop1','testpop1'], axis=1, inplace=True)\nprint(d.shape)\nprint(d.loc[(d.Date=='2020-03-25') & (d['Province_State']=='New York')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing data time series, us states only, would love to have this for all countries\nct = pd.read_csv(path+'states_daily_4pm_et.csv')\nsi = pd.read_csv(path+'states_info.csv')\nsi = si.rename(columns={'name':'Province_State'})\nct = ct.merge(si[['state','Province_State']], how='left', on='state')\nct['Date'] = ct['date'].apply(str).transform(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\nct.loc[ct.Province_State=='US Virgin Islands','Province_State'] = 'Virgin Islands'\nct.loc[ct.Province_State=='District Of Columbia','Province_State'] = 'District of Columbia'\npd.set_option('display.max_rows', 20)\nct\n# ct = ct['Date','state','total']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ckeep = ['positive','negative','totalTestResults']\nfor c in ckeep: ct[c] = np.log1p(ct[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(ct[['Province_State','Date']+ckeep], how='left',\n            on=['Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"w = pd.read_csv(path+'training_data_with_weather_info_week_2.csv')\nw.drop(['Id','Id.1','ConfirmedCases','Fatalities','country+province','day_from_jan_first'], axis=1, inplace=True)\nw[cp] = w[cp].fillna('')\nwf = list(w.columns[5:])\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# since weather data lags behind a day or two, adjust the date to make it contemporaneous\nwmax = w.Date.max()\nwmax = datetime.strptime(wmax,'%Y-%m-%d').date()\nwoff = (dmax - wmax).days\nprint(dmax, wmax, woff)\nw['Date'] = (pd.to_datetime(w.Date) + timedelta(woff)).dt.strftime('%Y-%m-%d')\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge Lat and Long for all times and the time-varying weather data based on date\nd = d.merge(w[cp+['Lat','Long']].drop_duplicates(), how='left', on=cp)\nw.drop(['Lat','Long'],axis=1,inplace=True)\nd = d.merge(w, how='left', on=cpd)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# combine ecdc and nytimes data\necdc = pd.read_csv(path+'ecdc.csv', encoding = 'latin')\necdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# combine ecdc and nytimes data as extra y0 and y1\n# https://opendata.ecdc.europa.eu/covid19/casedistribution/csv\necdc['Date'] = pd.to_datetime(ecdc[['year','month','day']]).dt.strftime('%Y-%m-%d')\necdc = ecdc.rename(mapper={'countriesAndTerritories':'Country_Region'}, axis=1)\necdc['Country_Region'] = ecdc['Country_Region'].replace('_',' ',regex=True)\necdc['Province_State'] = ''\necdc['cc'] = ecdc.groupby(cp)['cases'].cummax()\necdc['extra_y0'] = np.log1p(ecdc.cc)\necdc['cd'] = ecdc.groupby(cp)['deaths'].cummax()\necdc['extra_y1'] = np.log1p(ecdc.cd)\necdc = ecdc[cpd + ['extra_y0','extra_y1']]\necdc[::63]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ecdc = ecdc[ecdc.Date >= '2020-01-22']\necdc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# https://github.com/nytimes/covid-19-data\nnyt = pd.read_csv(path+'us-states.csv')\nnyt['extra_y0'] = np.log1p(nyt.cases)\nnyt['extra_y1'] = np.log1p(nyt.deaths)\nnyt['Country_Region'] = 'US'\nnyt = nyt.rename(mapper={'date':'Date','state':'Province_State'},axis=1)\nnyt.drop(['fips','cases','deaths'],axis=1,inplace=True)\nnyt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"extra = pd.concat([ecdc,nyt], sort=True)\nextra","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(extra, how='left', on=cpd)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d[['extra_y0','extra_y1']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# recovered data from hopkins, https://github.com/CSSEGISandData/COVID-19\nrecovered = pd.read_csv(path+'time_series_covid19_recovered_global.csv')\nrecovered = recovered.rename(mapper={'Country/Region':'Country_Region','Province/State':'Province_State'}, axis=1)\nrecovered[cp] = recovered[cp].fillna('')\nrecovered = recovered.drop(['Lat','Long'], axis=1)\nrecovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace US row with identical rows for every US state\nusp = d.loc[d.Country_Region=='US','Province_State'].unique()\nprint(usp, len(usp))\nrus = recovered[recovered.Country_Region=='US']\nrus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rus = rus.reindex(np.repeat(rus.index.values,len(usp)))\nrus.loc[:,'Province_State'] = usp\nrus","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"recovered =  recovered[recovered.Country_Region!='US']\nrecovered = pd.concat([recovered,rus]).reset_index(drop=True)\nrecovered","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# melt and merge\nrm = pd.melt(recovered, id_vars=cp, var_name='d', value_name='recov')\nrm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rm['Date'] = pd.to_datetime(rm.d)\nrm.drop('d',axis=1,inplace=True)\nrm['Date'] = rm['Date'].dt.strftime('%Y-%m-%d')\nrm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.merge(rm, how='left', on=['Country_Region','Province_State','Date'])\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['recov'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# approximate US state recovery via proportion of confirmed cases\nd['ccsum'] = d.groupby(['Country_Region','Date'])['ConfirmedCases'].transform(lambda x: x.sum())\nd.loc[d.Country_Region=='US','recov'] = d.loc[d.Country_Region=='US','recov'] * \\\n                                        d.loc[d.Country_Region=='US','ConfirmedCases'] / \\\n                                        (d.loc[d.Country_Region=='US','ccsum'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.loc[:,'recov'] = np.log1p(d.recov)\n# d.loc[:,'recov'] = d['recov'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.loc[d.Province_State=='North Carolina','recov'][45:55]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# log1p transform both targets\nynames = ['ConfirmedCases','Fatalities']\nny = len(ynames)\nyv = []\nfor i in range(ny):\n    v = 'y'+str(i)\n    d[v] = np.log1p(d[ynames[i]])\n    yv.append(v)\nprint(d[yv].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['rate0'] = d.y0 - np.log(d['pop'])\nd['rate1'] = d.y1 - np.log(d['pop'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d = d.sort_values(['Loc','Date']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute nearest neighbors\nregions = d[['Loc','Lat','Long']].drop_duplicates('Loc').reset_index(drop=True)\nregions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# regions.to_csv('regions.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nk = kv[0]\nnn = NearestNeighbors(k)\nnn.fit(regions[['Lat','Long']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# first matrix is distances, second indices to nearest neighbors including self\n# note two cruise ships are replicated and have identical lat, long values\nknn = nn.kneighbors(regions[['Lat','Long']])\nknn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ns = d['Loc'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# time series matrix\nky = d['y0'].values.reshape(ns,-1)\nprint(ky.shape)\n\nprint(ky[0])\n\n# use knn indices to create neighbors\nknny = ky[knn[1]]\nprint(knny.shape)\n\nknny = knny.transpose((0,2,1)).reshape(-1,k)\nprint(knny.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# knn max features\nnk = len(kv)\nkp = []\nkd = []\nns = regions.shape[0]\nfor k in kv:\n    nn = NearestNeighbors(k)\n    nn.fit(regions[['Lat','Long']])\n    knn = nn.kneighbors(regions[['Lat','Long']])\n    kp.append('knn'+str(k)+'_')\n    kd.append('kd'+str(k)+'_')\n    for i in range(ny):\n        yi = 'y'+str(i)\n        kc = kp[-1]+yi\n        # time series matrix\n        ky = d[yi].values.reshape(ns,-1)\n        # use knn indices to create neighbor matrix\n        km = ky[knn[1]].transpose((0,2,1)).reshape(-1,k)\n        \n        # take maximum value over all neighbors to approximate spreading\n        d[kc] = np.amax(km, axis=1)\n        print(d[kc].describe())\n        print()\n        \n        # distance to max\n        kc = kd[-1]+yi\n        ki = np.argmax(km, axis=1).reshape(ns,-1)\n        kw = np.zeros_like(ki).astype(float)\n        # inefficient indexing, surely some way to do it faster\n        for j in range(ns): \n            kw[j] = knn[0][j,ki[j]]\n        d[kc] = kw.flatten()\n        print(d[kc].describe())\n        print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ki[j]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# range of dates for training\n# dates = d[~d.y0.isnull()]['Date'].drop_duplicates()\ndates = d[d.y0.notnull()]['Date'].drop_duplicates()\ndates","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# correlations\ncols = []\nfor i in range(ny):\n    yi = yv[i]\n    cols.append(yi)\n    for k in kp:\n        cols.append(k+yi)\nd.loc[:,cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Date'] = pd.to_datetime(d['Date'])\nd['Date'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# days since beginning\n# basedate = train['Date'].min()\n# train['dint'] = train.apply(lambda x: (x.name.to_datetime() - basedate).days, axis=1)\nd['dint'] = (d['Date'] - d['Date'].min()).dt.days\nd['dint'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# reference days since exp(j)th occurrence\nfor i in range(ny):\n    \n    for j in range(3):\n\n        ij = str(i)+'_'+str(j)\n        \n        cut = 2**j if i==0 else j\n        \n        qd1 = (d[yv[i]] > cut) & (d[yv[i]].notnull())\n        d1 = d.loc[qd1,['Loc','dint']]\n        # d1.shape\n        # d1.head()\n\n        # get min for each location\n        d1['dmin'] = d1.groupby('Loc')['dint'].transform(lambda x: x.min())\n        # dintmax = d1['dint'].max()\n        # print(i,j,'dintmax',dintmax)\n        # d1.head()\n\n        d1.drop('dint',axis=1,inplace=True)\n        d1 = d1.drop_duplicates()\n        d = d.merge(d1,how='left',on=['Loc'])\n \n        # if dmin is missing then the series had no occurrences in the training set\n        # go ahead and assume there will be one at the beginning of the test period\n        # the average time between first occurrence and first death is 14 days\n        # if j==0: d[dmi] = d[dmi].fillna(dintmax + 1 + i*14)\n\n        # ref day is days since dmin, must clip at zero to avoid leakage\n        d['ref_day'+ij] = np.clip(d.dint - d.dmin, 0, 100000)\n        d.drop('dmin',axis=1,inplace=True)\n\n        # asymptotic curve may bin differently\n        d['recip_day'+ij] = 1 / (1 + (1 + d['ref_day'+ij])**(-1.0))\n    \n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['dint'].value_counts().std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# diffs and rolling means\ne = 1\nr = 5\nfor i in range(ny):\n    yi = 'y'+str(i)\n    dd = '_d'+str(e)\n    rr = '_r'+str(r)\n    \n    d[yi+dd] = d.groupby('Loc')[yi].transform(lambda x: x.diff(e))\n    d[yi+rr] = d.groupby('Loc')[yi].transform(lambda x: x.rolling(r).mean())\n    d['rate'+str(i)+dd] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.diff(e))\n    d['rate'+str(i)+rr] = d.groupby('Loc')['rate'+str(i)].transform(lambda x: x.rolling(r).mean())\n    d['extra_y'+str(i)+dd] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.diff(e))\n    d['extra_y'+str(i)+rr] = d.groupby('Loc')['extra_y'+str(i)].transform(lambda x: x.rolling(r).mean())\n\n    for k in kp:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n\n    for k in kd:\n        d[k+yi+dd] = d.groupby('Loc')[k+yi].transform(lambda x: x.diff(e))\n        d[k+yi+rr] = d.groupby('Loc')[k+yi].transform(lambda x: x.rolling(r).mean())\n        \nlaglist = ['recov'] + google + wf\n\nfor v in laglist:\n    d[v+dd] = d.groupby('Loc')[v].transform(lambda x: x.diff(e))\n    d[v+rr] = d.groupby('Loc')[v].transform(lambda x: x.rolling(r).mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# final sort before training\nd = d.sort_values(['Loc','dint']).reset_index(drop=True)\nd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# initial continuous and categorical features\ndogs = []\nfor i in range(ny):\n    for j in range(3):\n        dogs.append('ref_day'+str(i)+'_'+str(j))\ncats = ['Loc']\nprint(dogs, len(dogs))\nprint(cats, len(cats))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# one-hot encode categorical features\nohef = []\nfor i,c in enumerate(cats):\n    print(c, d[c].nunique())\n    ohe = pd.get_dummies(d[c], prefix=c)\n    ohec = [f.translate({ord(c): \"_\" for c in \" !@#$%^&*()[]{};:,./<>?\\|`~-=_+\"}) for f in list(ohe.columns)]\n    ohe.columns = ohec\n    d = pd.concat([d,ohe],axis=1)\n    ohef = ohef + ohec","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Loc_US_North_Carolina'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d['Loc_US_Colorado'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# boosting hyperparameters\nparams = {}\n\nparams[('lgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('lgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('xgb','y0')] = {'lambda_l2': 1.9079933811271934, 'max_depth': 5}\nparams[('xgb','y1')] = {'lambda_l2': 1.690407455211948, 'max_depth': 3}\nparams[('ctb','y0')] = {'l2_leaf_reg': 1.9079933811271934, 'max_depth': 5}\nparams[('ctb','y1')] = {'l2_leaf_reg': 1.690407455211948, 'max_depth': 3}","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# must start cas server before running this cell\nif 'cas' in booster:\n    from swat import *\n    s = CAS('server', 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# single horizon validation using one day at a time for 28 days\nnb = len(booster)\nnls = np.zeros((nhorizon,ny,nb))\nrallv = np.zeros((nhorizon,ny,nb))\niallv = np.zeros((nhorizon,ny,nb)).astype(int)\nyallv = []\npallv = []\nimps = []\n \n# loop over horizons\nfor horizon in range(1+skip,nhorizon+1):\n# for horizon in range(4,5):\n    \n    print()\n#     print('*'*20)\n#     print(f'horizon {horizon}')\n#     print('*'*20)\n    \n    gc.collect()\n    \n    hs = str(horizon)\n    if horizon < 10: hs = '0' + hs\n    \n    # build lists of features\n    lags = []\n    diffs = []\n    for i in range(ny):\n        yi = 'y'+str(i)\n        lags.append(yi)\n        lags.append('extra_'+yi)\n        lags.append('rate'+str(i))\n        lags.append(yi+dd)\n        lags.append('extra_'+yi+dd)\n        lags.append('rate'+str(i)+dd)\n        lags.append(yi+rr)\n        lags.append('extra_'+yi+rr)\n        lags.append('rate'+str(i)+rr)\n        for k in kp:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n        for k in kd:\n            lags.append(k+yi)\n            lags.append(k+yi+dd)\n            lags.append(k+yi+rr)\n       \n    lags.append('recov')\n    \n    lags = lags + google + wf + ckeep\n    \n#     cinfo = ['pop', 'tests', 'testpop', 'density', 'medianage',\n#        'urbanpop', 'hospibed', 'smokers']\n    cinfo0 = ['testpop']\n    cinfo1 = ['testpop','medianage']\n    \n    f0 = dogs + lags + cinfo0 + ohef\n    f1 = dogs + lags + cinfo1 + ohef\n    \n    # remove some features based on validation experiments\n    f0 = [f for f in f0 if not f.startswith('knn11') and not f.startswith('kd') \\\n         and not f.startswith('rate') and not f.endswith(dd) and not f.endswith(rr)]\n    f1 = [f for f in f1 if not f.startswith('knn6') and not f.startswith('kd6')]\n    \n    # remove any duplicates\n    # f0 = list(set(f0))\n    # f1 = list(set(f1))\n    \n    features = []\n    features.append(f0)\n    features.append(f1)\n    \n    nf = []\n    for i in range(ny):\n        nf.append(len(features[i]))\n        # print(nf[i], features[i][:10])\n        \n    qtrain = d['Date'] <= ddate.isoformat()\n\n    vdate = ddate + timedelta(days=horizon)\n    qval = d['Date'] == vdate.isoformat()\n    qvallag = d['Date'] == ddate.isoformat()\n    \n    x_train = d[qtrain].copy()\n    # make y training data monotonic nondecreasing\n    y_train = []\n    for i in range(ny):\n        y_train.append(pd.Series(d.loc[qtrain,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()))\n\n    x_val = d[qval].copy()\n    y_val = [d.loc[qval,'y0'].copy(), d.loc[qval,'y1'].copy()]\n    yallv.append(y_val)\n    \n    # lag features\n    x_train.loc[:,lags] = x_train.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n    x_val.loc[:,lags] = d.loc[qvallag,lags].values\n\n    print()\n    print(horizon, 'x_train', x_train.shape)\n    print(horizon, 'x_val', x_val.shape)\n    \n    if train_full:\n        \n        qfull = (d['Date'] <= tmax)\n        \n        tdate = dmax + timedelta(days=horizon)\n        qtest = d['Date'] == tdate.isoformat()\n        qtestlag = d['Date'] == dmax.isoformat()\n    \n        x_full = d[qfull].copy()\n        \n        # make y training data monotonic nondecreasing\n        y_full = []\n        for i in range(ny):\n            y_full.append(pd.Series(d.loc[qfull,['Loc',yv[i]]].groupby('Loc')[yv[i]].cummax()))\n        \n        x_test = d[qtest].copy()\n        \n        # lag features\n        x_full.loc[:,lags] = x_full.groupby('Loc')[lags].transform(lambda x: x.shift(horizon))\n        x_test.loc[:,lags] = d.loc[qtestlag,lags].values\n\n        print(horizon, 'x_full', x_full.shape)\n        print(horizon, 'x_test', x_test.shape)\n\n    train_set = []\n    val_set = []\n    ny = len(y_train)\n\n#     for i in range(ny):\n#         train_set.append(xgb.DMatrix(x_train[features[i]], y_train[i]))\n#         val_set.append(xgb.DMatrix(x_val[features[i]], y_val[i]))\n\n    gc.collect()\n\n    # loop over multiple targets\n    mod = []\n    pred = []\n    rez = []\n    iters = []\n    \n    for i in range(ny):\n#     for i in range(1):\n        print()\n        print('*'*40)\n        print(f'horizon {horizon} {yv[i]} {ynames[i]} {vdate}')\n        print('*'*40)\n        \n        # use catboost only for y1\n        # nb = 2 if i==0 else 3\n       \n        # matrices to store predictions\n        vpm = np.zeros((x_val.shape[0],nb))\n        tpm = np.zeros((x_test.shape[0],nb))\n        \n        for b in range(nb):\n            \n            if booster[b] == 'cas':\n                \n                x_train['Partition'] = 1\n                x_val['Partition'] = 0\n                x_cas_all = pd.concat([x_train, x_val], axis=0)\n                # make copy of target since it is also used for lags\n                x_cas_all['target'] = pd.concat([y_train[i], y_val[i]], axis=0).values\n                s.upload(x_cas_all, casout=\"x_cas_val\")\n\n                target = 'target'\n                inputs = features[i]\n                inputs.append(target)\n\n                s.loadactionset(\"autotune\")\n                res=s.autotune.tuneGradientBoostTree (\n                    trainOptions = {\n                        \"table\":{\"name\":'x_cas_val',\"where\":\"Partition=1\"},\n                        \"target\":target,\n                        \"inputs\":inputs,\n                        \"casOut\":{\"name\":\"model\", \"replace\":True}\n                    },\n                    scoreOptions = {\n                        \"table\":{\"name\":'x_cas_val', \"where\":\"Partition=0\"},\n                        \"model\":{\"name\":'model'},\n                        \"casout\":{\"name\":\"x_valid_preds\",\"replace\":True},\n                        \"copyvars\": ['Id','Loc','Date']\n                    },\n                    tunerOptions = {\n                        \"seed\":54321,  \n                        \"objective\":\"RASE\", \n                        \"userDefinedPartition\":True \n                    }\n                )\n                print()\n                print(res.TunerSummary)\n                print()\n                print(res.BestConfiguration)        \n\n                TunerSummary=pd.DataFrame(res['TunerSummary'])\n                TunerSummary[\"Value\"]=pd.to_numeric(TunerSummary[\"Value\"])\n                BestConf=pd.DataFrame(res['BestConfiguration'])\n                BestConf[\"Value\"]=pd.to_numeric(BestConf[\"Value\"])\n                vpt = s.CASTable(\"x_valid_preds\").to_frame()\n                #FG: resort the CAS predictions by Id\n                vpt = vpt.sort_values(['Loc','Date']).reset_index(drop=True)\n                vp = vpt['P_target'].values\n\n                s.dropTable(\"x_cas_val\")\n                s.dropTable(\"x_valid_preds\")\n                \n            else:\n                # scikit interface automatically uses best model for predictions\n                params[(booster[b],yv[i])]['n_estimators'] = 5000\n                if booster[b]=='lgb':\n                    model = lgb.LGBMRegressor(**params[(booster[b],yv[i])]) \n                elif booster[b]=='xgb':\n                    model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                else:\n                    # hack for categorical features, ctb must be last in booster list\n                    features[i] = features[i][:-294] + ['Loc']\n                    params[(booster[b],yv[i])]['cat_features'] = ['Loc']\n                    model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    \n                model.fit(x_train[features[i]], y_train[i],\n                                  eval_set=[(x_train[features[i]], y_train[i]),\n                                            (x_val[features[i]], y_val[i])],\n                                  early_stopping_rounds=30,\n                                  verbose=False)\n\n                vp = model.predict(x_val[features[i]])\n                \n                iallv[horizon-1,i,b] = model._best_iteration if booster[b]=='lgb' else \\\n                                       model.best_iteration if booster[b]=='xgb' else \\\n                                       model.best_iteration_\n\n                gain = model.feature_importances_\n        #         gain = model.get_score(importance_type='gain')\n        #         split = model.get_score(importance_type='weight')   \n            #     gain = model.feature_importance(importance_type='gain')\n            #     split = model.feature_importance(importance_type='split').astype(float)  \n            #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                imp = pd.DataFrame({'feature':features[i],'gain':gain})\n        #         imp = pd.DataFrame({'feature':features[i]})\n        #         imp['gain'] = imp['feature'].map(gain)\n        #         imp['split'] = imp['feature'].map(split)\n\n                imp.set_index(['feature'],inplace=True)\n\n                imp.gain /= np.sum(imp.gain)\n        #         imp.split /= np.sum(imp.split)\n\n                imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                print()\n                print(imp.head(n=10))\n                # print(imp.shape)\n\n                imp.reset_index(inplace=True)\n                imp['horizon'] = horizon\n                imp['target'] = yv[i]\n                imp['set'] = 'valid'\n                imp['booster'] = booster[b]\n\n                mod.append(model)\n                imps.append(imp)\n                \n            # china rule, last observation carried forward, set to zero here\n            qcv = (x_val['Country_Region'] == 'China') & \\\n                  (x_val['Province_State'] != 'Hong Kong') & \\\n                  (x_val['Province_State'] != 'Macau')\n            vp[qcv] = 0.0\n\n            # make sure horizon 1 prediction is not smaller than first lag\n            # because we know series is monotonic\n            # if horizon==1+skip:\n            if True:\n                a = np.zeros((len(vp),2))\n                a[:,0] = vp\n                a[:,1] = x_val[yv[i]].values\n                vp = np.nanmax(a,axis=1)\n            \n            val_score = np.sqrt(mean_squared_error(vp, y_val[i]))\n            vpm[:,b] = vp\n            \n            print()\n            print(f'{booster[b]} validation rmse {val_score:.6f}')\n            rallv[horizon-1,i,b] = val_score\n\n            gc.collect()\n    \n#             break\n\n            if train_full:\n                \n                print()\n                print(f'{booster[b]} training with full data and predicting', tdate.isoformat())\n                    \n                if booster[b] == 'cas':\n                    \n                    x_full['target'] = y_full[i].values\n                    s.upload(x_full, casout=\"x_full\")\n                    # use hyperparameters from validation fit\n                    s.loadactionset(\"decisionTree\")\n                    result = s.gbtreetrain(\n                        table={\"name\":'x_full'},\n                        target=target,\n                        inputs= inputs,\n                        varimp=True,\n                        ntree=BestConf.iat[0,2], \n                        m=BestConf.iat[1,2],\n                        learningRate=BestConf.iat[2,2],\n                        subSampleRate=BestConf.iat[3,2],\n                        lasso=BestConf.iat[4,2],\n                        ridge=BestConf.iat[5,2],\n                        nbins=BestConf.iat[6,2],\n                        maxLevel=BestConf.iat[7,2],\n                        #quantileBin=True,\n                        seed=326146718,\n                        #savestate={\"name\":\"aStore\",\"replace\":True}\n                        casOut={\"name\":'fullmodel', \"replace\":True}\n                        ) \n\n                    s.upload(x_test, casout=\"x_test_cas\")\n\n                    s.decisionTree.gbtreeScore(\n                        modelTable={\"name\":\"fullmodel\"},        \n                        table={\"name\":\"x_test_cas\"},\n                        casout={\"name\":\"x_test_preds\",\"replace\":True},\n                        copyvars= ['Loc','Date']\n                        ) \n                    # save test predictions back into main table\n                    forecast = s.CASTable(\"x_test_preds\").to_frame()\n                    forecast = forecast.sort_values(['Loc','Date']).reset_index(drop=True)\n                    tp = forecast['_GBT_PredMean_'].values\n                    \n                    s.dropTable(\"x_full\")\n                    s.dropTable(\"x_test_cas\")\n                     \n                else:\n                \n                    # use number of iterations from validation fit\n                    params[(booster[b],yv[i])]['n_estimators'] = iallv[horizon-1,i,b]\n                    if booster[b]=='lgb':\n                        model = lgb.LGBMRegressor(**params[(booster[b],yv[i])])\n                    elif booster[b]=='xgb':\n                        model = xgb.XGBRegressor(**params[(booster[b],yv[i])])\n                    else:\n                        model = ctb.CatBoostRegressor(**params[(booster[b],yv[i])])\n                    \n                    model.fit(x_full[features[i]], y_full[i], verbose=False)\n                    \n                    params[(booster[b],yv[i])]['n_estimators'] = 5000\n\n                    tp = model.predict(x_test[features[i]])\n                \n                    gain = model.feature_importances_\n            #         gain = model.get_score(importance_type='gain')\n            #         split = model.get_score(importance_type='weight')   \n                #     gain = model.feature_importance(importance_type='gain')\n                #     split = model.feature_importance(importance_type='split').astype(float)  \n                #     imp = pd.DataFrame({'feature':features,'gain':gain,'split':split})\n                    imp = pd.DataFrame({'feature':features[i],'gain':gain})\n            #         imp = pd.DataFrame({'feature':features[i]})\n            #         imp['gain'] = imp['feature'].map(gain)\n            #         imp['split'] = imp['feature'].map(split)\n\n                    imp.set_index(['feature'],inplace=True)\n\n                    imp.gain /= np.sum(imp.gain)\n            #         imp.split /= np.sum(imp.split)\n\n                    imp.sort_values(['gain'], ascending=False, inplace=True)\n\n                    print()\n                    print(imp.head(n=10))\n                    # print(imp.shape)\n\n                    imp.reset_index(inplace=True)\n                    imp['horizon'] = horizon\n                    imp['target'] = yv[i]\n                    imp['set'] = 'full'\n                    imp['booster'] = booster[b]\n\n                    imps.append(imp)\n\n                # china rule, last observation carried forward, set to zero here\n                qct = (x_test['Country_Region'] == 'China') & \\\n                      (x_test['Province_State'] != 'Hong Kong') & \\\n                      (x_test['Province_State'] != 'Macau')\n                tp[qct] = 0.0\n\n                # make sure first horizon prediction is not smaller than first lag\n                # because we know series is monotonic\n                # if horizon==1+skip:\n                if True:\n                    a = np.zeros((len(tp),2))\n                    a[:,0] = tp\n                    a[:,1] = x_test[yv[i]].values\n                    tp = np.nanmax(a,axis=1)\n\n                tpm[:,b] = tp\n                \n                gc.collect()\n                \n        # nonnegative least squares to estimate ensemble weights\n        x, rnorm = nnls(vpm, y_val[i])\n        \n        # smooth weights by shrinking towards all equal\n        # x = (x + np.ones(3)/3.)/2\n\n        # smooth weights with rolling mean, ewma\n        # if horizon+skip > 1: x = (x + nls[horizon+skip-2,i])/2\n        alpha = 0.1\n        if horizon+skip > 1: x = alpha * x + (1 - alpha) * nls[horizon+skip-2,i]\n\n        nls[horizon+skip-1,i] = x\n        \n        val_pred = np.matmul(vpm, x)\n        test_pred = np.matmul(tpm, x)\n        \n        # china rule in case weights do not sum to 1\n        # val_pred[qcv] = vpm[:,0][qcv]\n        # test_pred[qcv] = tpm[:,0][qct]\n        \n        # save validation and test predictions back into main table\n        d.loc[qval,yv[i]+'_pred'] = val_pred\n        d.loc[qtest,yv[i]+'_pred'] = test_pred\n\n        # ensemble validation score\n        # val_score = np.sqrt(rnorm/vpm.shape[0])\n        val_score = np.sqrt(mean_squared_error(val_pred, y_val[i]))\n        \n        rez.append(val_score)\n        pred.append(val_pred)\n\n    pallv.append(pred)\n    \n    # nnls weights\n    w0 = ''\n    w1 = ''\n    for b in range(nb):\n        w0 = w0 + f' {nls[horizon-1,0,b]:.2f}'\n        w1 = w1 + f' {nls[horizon-1,1,b]:.2f}'\n        \n    print()\n    print('Validation RMSLE')\n    print(f'{ynames[0]} \\t {rez[0]:.6f}  ' + w0)\n    print(f'{ynames[1]} \\t {rez[1]:.6f}  ' + w1)\n    print(f'Mean \\t \\t {np.mean(rez):.6f}')\n\n#     # break down RMSLE by day\n#     rp = np.zeros((2,7))\n#     for i in range(ny):\n#         for di in range(50,57):\n#             j = di - 50\n#             qf = x_val.dint == di\n#             rp[i,j] = np.sqrt(mean_squared_error(pred[i][qf], y_val[i][qf]))\n#             print(i,di,f'{rp[i,j]:.6f}')\n#         print(i,f'{np.mean(rp[i,:]):.6f}')\n#         plt.plot(rp[i])\n#         plt.title(ynames[i] + ' RMSLE')\n#         plt.show()\n        \n    # plot actual vs predicted\n    plt.figure(figsize=(10, 5))\n    for i in range(ny):\n        plt.subplot(1,2,i+1)\n        plt.plot([0, 12], [0, 12], 'black')\n        plt.plot(pred[i], y_val[i], '.')\n        plt.xlabel('Predicted')\n        plt.ylabel('Actual')\n        plt.title(ynames[i])\n        plt.grid()\n    plt.show()\n    \n# save one big table of importances\nimpall = pd.concat(imps)\n\n# remove number suffixes from lag names to aid in analysis\n# impall['feature1'] = impall['feature'].replace(to_replace='lag..', value='lag', regex=True)\n\nos.makedirs('imp', exist_ok=True)\nfname = 'imp/' + mname + '_imp.csv'\nimpall.to_csv(fname, index=False)\nprint()\nprint(fname, impall.shape)\n\n# save scores and weights\nos.makedirs('rez', exist_ok=True)\nfname = 'rez/' + mname+'_rallv.npy'\nnp.save(fname, rallv)\nprint(fname, rallv.shape)\n\nfname = 'rez/' + mname+'_nnls.npy'\nnp.save(fname, nls)\nprint(fname, nls.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if 'cas' in booster: s.shutdown()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tdate.isoformat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = [f for f in features[0] if f.startswith('ref')]\nd[rf].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"np.mean(iallv, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nfor i in range(ny):\n    plt.subplot(2,2,1+i)\n    plt.plot(rallv[:,i])\n    plt.title(ynames[i] + ' RMSLE vs Horizon')\n    plt.grid()\n    \n    plt.subplot(2,2,3+i)\n    plt.plot(nls[:,i])\n    plt.title(ynames[i] + ' Ensemble Weights')\n    plt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute validation rmsle\nm = 0\nlocs = d.loc[:,['Loc','Country_Region','Province_State']].drop_duplicates().reset_index(drop=True)\n# locs = x_val.copy().reset_index(drop=True)\n# print(locs.shape)\ny_truea = []\ny_preda = []\n\nprint(f'# {mname}')\nfor i in range(ny):\n    y_true = []\n    y_pred = []\n    for j in range(nhorizon-skip):\n        y_true.append(yallv[j][i])\n        y_pred.append(pallv[j][i])\n    y_true = np.stack(y_true)\n    y_pred = np.stack(y_pred)\n    # print(y_pred.shape)\n    # make each series monotonic increasing\n    for j in range(y_pred.shape[1]): \n        y_pred[:,j] = np.maximum.accumulate(y_pred[:,j])\n    # copy updated predictions into main table\n    for horizon in range(1+skip,nhorizon+1):\n        vdate = ddate + timedelta(days=horizon)\n        qval = d['Date'] == vdate.isoformat()\n        d.loc[qval,yv[i]+'_pred'] = y_pred[horizon-1-skip]\n    rmse = np.sqrt(mean_squared_error(y_pred, y_true))\n    print(f'# {rmse:.6f}')\n    m += rmse/2\n    locs['rmse'+str(i)] = np.sqrt(np.mean((y_true-y_pred)**2, axis=0))\n    y_truea.append(y_true)\n    y_preda.append(y_pred)\nprint(f'# {m:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gbt1u\n# 0.626124\n# 0.404578\n# 0.515351","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y0\nlocs = locs.sort_values('rmse0', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits of y0\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[0][:,li])\n    plt.plot(y_preda[0][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plt.plot(d.loc[d.Loc=='Belgium','y0'][39:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sort to find worst predictions of y1\nlocs = locs.sort_values('rmse1', ascending=False)\nlocs[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot worst fits of y1\nfor i in range(5):\n    li = locs.index[i]\n    plt.plot(y_truea[1][:,li])\n    plt.plot(y_preda[1][:,li])\n    plt.title(locs.loc[li,'Loc'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# enforce monotonicity of forecasts in test set after last date in training\nloc = d['Loc'].unique()\nfor l in loc:\n    # q = (d.Loc==l) & (d.ForecastId > 0)\n    q = (d.Loc==l) & (d.Date > tmax)\n    for yi in yv:\n        yp = yi+'_pred'\n        d.loc[q,yp] = np.maximum.accumulate(d.loc[q,yp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot actual and predicted curves over time for specific locations\nlocs = ['China Tibet','China Xinjiang','China Hong Kong', 'China Macau',\n        'Spain','Italy','India',\n        'US Washington','US New York','US California',\n        'US North Carolina','US Ohio']\nxlab = ['03-12','03-18','03-25','04-01','04-08','04-15','04-22']\nfor loc in locs:\n    plt.figure(figsize=(14,2))\n    \n    # fig, ax = plt.subplots()\n    # fig.autofmt_xdate()\n    \n    for i in range(ny):\n    \n        plt.subplot(1,2,i+1)\n        plt.plot(d.loc[d.Loc==loc,[yv[i],'Date']].set_index('Date'))\n        plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred','Date']].set_index('Date'))\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]]])\n        # plt.plot(d.loc[d.Loc==loc,[yv[i]+'_pred']])\n        # plt.xticks(np.arange(len(xlab)), xlab, rotation=-45)\n        # plt.xticks(np.arange(12), calendar.month_name[3:5], rotation=20)\n        # plt.xticks(rotation=-45)\n        plt.xticks([])\n        plt.title(loc + ' ' + ynames[i])\n       \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_rows', 100)\nloc = 'China Xinjiang'\nd.loc[d.Loc==loc,['Date',yv[0],yv[0]+'_pred']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tmax","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fmin","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compute public lb score\nq = (d.Date >= fmin) & (d.Date <= tmax)\nprint(f'# {tmax} {sum(q)/ns} {mname}')\ns0 = np.sqrt(mean_squared_error(d.loc[q,'y0'],d.loc[q,'y0_pred']))\ns1 = np.sqrt(mean_squared_error(d.loc[q,'y1'],d.loc[q,'y1_pred']))\nprint(f'# CC \\t {s0:.6f}')\nprint(f'# Fa \\t {s1:.6f}')\nprint(f'# Mean \\t {(s0+s1)/2:.6f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 2020-03-31 13.0 gbt1u\n# CC \t 0.744772\n# Fa \t 0.567829\n# Mean \t 0.656301","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create submission\nsub = d.loc[d.ForecastId > 0, ['ForecastId','y0_pred','y1_pred']]\nsub['ConfirmedCases'] = np.expm1(sub['y0_pred'])\nsub['Fatalities'] = np.expm1(sub['y1_pred'])\nsub.drop(['y0_pred','y1_pred'],axis=1,inplace=True)\nos.makedirs('sub',exist_ok=True)\n# fname = 'sub/' + mname + '.csv'\nfname = 'submission.csv'\nsub.to_csv(fname, index=False)\nprint(fname, sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save predictions\novars = ['Id','ForecastId','Country_Region','State_Provice','Loc','y0','y1','y0_pred','y1_pred']\noof = d.loc[:,ovars]\noof = oof.rename(mapper={'y0_pred':mname+'0','y1_pred':mname+'1'}, axis=1)\nos.makedirs('oof',exist_ok=True)\nfname = 'oof/' + mname + '.csv'\noof.to_csv(fname, index=False)\nprint(fname, oof.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"if save_data:\n    os.makedirs('data',exist_ok=True)\n    fname = 'data/' + mname + '_d.csv'\n    d.to_csv(fname, index=False)\n    print(fname, d.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"2f8637a583ba4bfa8946994db32cde94":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"624f68aaf5684d2ca6f88c940feb59e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6827a0694afe427ba904c7fe40befda3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b66678a5c7b64b90a555ccfeabb305c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"b937a9e2638e4fb4932fc16802ad725f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_624f68aaf5684d2ca6f88c940feb59e0","placeholder":"","style":"IPY_MODEL_cc713908016240288dac63ce30f32779","value":" 163/163 [00:05&lt;00:00, 31.15it/s]"}},"ba5137f29e61409db2c60bf628df8bcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_6827a0694afe427ba904c7fe40befda3","max":163,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b66678a5c7b64b90a555ccfeabb305c2","value":163}},"cc713908016240288dac63ce30f32779":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebb54321a1de47a1900c2b6f56668c6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba5137f29e61409db2c60bf628df8bcd","IPY_MODEL_b937a9e2638e4fb4932fc16802ad725f"],"layout":"IPY_MODEL_2f8637a583ba4bfa8946994db32cde94"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}