{"cells":[{"metadata":{},"cell_type":"markdown","source":"**What is the Math behind COVID-19?**\nThe curve used to model this situation is called a logistic curve, an S-shaped curve that describes population growth of both viruses and people, as well as other phenomena in economics and science.\n![curve](https://www.nctm.org/uploadedImages/Content/General/pandemics/Pandemic_curve.jpg)\nWe will fit this simple curve to our virus data (as provided by Kaggle) and see how well it performs on the leaderboard."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nimport scipy.optimize as opt\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-2/test.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filter out the 0's, countries are affected differently chronologically based on geography, holidaymakers, season etc... I could be wrong though, I mostly have online gambling experience, no epidemiological experience lol!\nedit: added them back because that produces a better score. Guess thats empirical testing for you."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ = train[train[\"ConfirmedCases\"] >= 0]\ntrain_.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us replace the empty Province_State cases with the Country_Region:"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state\n\ntrain_['Province_State'].fillna(EMPTY_VAL, inplace=True)\ntrain_['Province_State'] = train_.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntest['Province_State'].fillna(EMPTY_VAL, inplace=True)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic curve ð‘“(ð‘¥)=ð¿/(1+ð‘’âˆ’ð‘˜(ð‘¥âˆ’ð‘¥0)) is what we will use.\n\nWe will also use another variation of the function: ð‘“ð¿,b,ð‘˜,ð‘¥0(ð‘¥) = ð¿/(1+ð‘’âˆ’ð‘˜(ð‘¥âˆ’ð‘¥0))+b, depending on which works for the data.\n\nSome more commentary on scipy:\n\nIn SciPy, nonlinear least squares curve fitting works by minimizing the following cost function:\n\nS(Î²)=âˆ‘i=1n(yiâˆ’fÎ²(xi))2\n\nHere, Î² is the vector of parameters (in our example, Î²=(L,b,k,x0).\n\nNonlinear least squares is really similar to linear least squares for linear regression. Whereas the function f is linear in the parameters with the linear least squares method, it is not linear here. Therefore, the minimization of S(Î²) cannot be done analytically by solving the derivative of S with respect to Î². SciPy implements an iterative method called the Levenberg-Marquardt algorithm (an extension of the Gauss-Newton algorithm).\n\nLet us test the functions out:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_['row_number'] = train_.groupby(['Country_Region', 'Province_State']).cumcount()\nx = train_[train_[\"Country_Region\"] == 'China'][train_[\"Province_State\"] == 'Hubei']['row_number']\ny = train_[train_[\"Country_Region\"] == 'China'][train_[\"Province_State\"] == 'Hubei']['ConfirmedCases']\ny_ = train_[train_[\"Country_Region\"] == 'China'][train_[\"Province_State\"] == 'Hubei']['Fatalities']\n\ndef f(x, L, b, k, x_0):\n    return L / (1. + np.exp(-k * (x - x_0))) + b\n\n\ndef logistic(xs, L, k, x_0):\n    result = []\n    for x in xs:\n        xp = k*(x-x_0)\n        if xp >= 0:\n            result.append(L / ( 1. + np.exp(-xp) ) )\n        else:\n            result.append(L * np.exp(xp) / ( 1. + np.exp(xp) ) )\n    return result\n\np0 = [max(y), 0.0,max(x)]\np0_ = [max(y_), 0.0,max(x)]\nx_ = np.arange(0, 100, 1).tolist()\ntry:\n    popt, pcov = opt.curve_fit(logistic, x, y,p0)\n    yfit = logistic(x_, *popt)\n    popt_, pcov_ = opt.curve_fit(logistic, x, y_,p0_)\n    yfit_ = logistic(x_, *popt_)\nexcept:\n    popt, pcov = opt.curve_fit(f, x, y, method=\"lm\", maxfev=5000)\n    yfit = f(x_, *popt)\n    popt_, pcov_ = opt.curve_fit(f, x, y_, method=\"lm\", maxfev=5000)\n    yfit_ = f(x_, *popt_)\n    #print(\"problem\")\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nax.plot(x, y, 'o', label ='Actual Cases')\nax.plot(x_, yfit, '-', label ='Fitted Cases')\n\nax.plot(x, y_, 'o', label ='Actual Fatalities')\nax.plot(x_, yfit_, '-', label ='Fitted fatalities')\nax.title.set_text('China - Hubei province')\nplt.legend(loc=\"center right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems reasonable to me anyway. Go ahead and test a few. There are one or two that come back erroneous but for the most part its pretty good at it.\nNow lets create a dimensional lookup(LUP) of the composite (Country_Region,Province_State):"},{"metadata":{"trusted":true},"cell_type":"code","source":"unique = pd.DataFrame(train_.groupby(['Country_Region', 'Province_State'],as_index=False).count())\nunique.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to iterate through the trianing data and fit curves on the data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\ndef date_day_diff(d1, d2):\n    delta = dt.datetime.strptime(d1, \"%Y-%m-%d\") - dt.datetime.strptime(d2, \"%Y-%m-%d\")\n    return delta.days\n\nlog_regions = []\n\nfor index, region in unique.iterrows():\n    st = region['Province_State']\n    co = region['Country_Region']\n    \n    rdata = train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]\n\n    t = rdata['Date'].values\n    t = [float(date_day_diff(d, t[0])) for d in t]\n    y = rdata['ConfirmedCases'].values\n    y_ = rdata['Fatalities'].values\n\n    p0 = [max(y), 0.0, max(t)]\n    p0_ = [max(y_), 0.0, max(t)]\n    try:\n        popt, pcov = opt.curve_fit(logistic, t, y, p0, maxfev=10000)\n        try:\n            popt_, pcov_ = opt.curve_fit(logistic, t, y_, p0_, maxfev=10000)\n        except:\n            popt_, pcov_ = opt.curve_fit(f, t, y_,method=\"trf\", maxfev=10000)\n        log_regions.append((co,st,popt,popt_))\n    except:\n        popt, pcov = opt.curve_fit(f, t, y,method=\"trf\", maxfev=10000)\n        popt_, pcov_ = opt.curve_fit(f, t, y_,method=\"trf\", maxfev=10000)\n        log_regions.append((co,st,popt,popt_))\n\nprint(\"All done!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert it to a dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_regions = pd.DataFrame(log_regions)\nlog_regions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rename columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_regions.columns = ['Country_Region','Province_State','ConfirmedCases','Fatalities']\nlog_regions.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wondering if i should curbe outliers here:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = log_regions['ConfirmedCases'].str[1]\nbins = np.arange(0, 10, 0.01)\nplt.hist(data,bins=bins, alpha=0.5)\nplt.xlim([0,1])\nplt.ylabel('count')\nplt.show()\nlog_regions['ConfirmedCases'].str[1].quantile([.1, .25, .5, .75, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#log_regions.loc[log_regions['ConfirmedCases'].str[1] > 1.0, 'ConfirmedCases'] = 0.267766","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_regions['ConfirmedCases'].str[1].quantile([.1, .25, .5, .75, .95, .99])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us test graphing a curve from the fitted parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"T = np.arange(0, 100, 1).tolist()\npopt = list(log_regions[log_regions[\"Country_Region\"] == 'Italy'][log_regions[\"Province_State\"] == 'Italy']['ConfirmedCases'])[0]\npopt_ = list(log_regions[log_regions[\"Country_Region\"] == 'Italy'][log_regions[\"Province_State\"] == 'Italy']['Fatalities'])[0]\n\ntry:\n    yfit = logistic(T, *popt)\n    yfit_ = logistic(T, *popt_)\nexcept:\n    yfit = f(T, *popt)\n    yfit_ = f(T, *popt_)\n    \n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nax.plot(T, yfit, label=\"Fitted ConfirmedCases\")\nax.plot(T, yfit_, label=\"Fitted Fatalities\")\nax.title.set_text('Italy fitted params')\nplt.legend(loc=\"upper left\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we iterate our fits through the test data for submission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, rt in log_regions.iterrows():\n    st = rt['Province_State']\n    co = rt['Country_Region']\n    popt = list(['ConfirmedCases'])\n    popt_ = list(rt['Fatalities'])\n    print(co,st,popt,popt_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets try replace parameters for the locations where there havent been any fatalities so far, leaning on the notion that fatalities follow cases according to observation.\nwhat we will do is work out the median ratios of the parameters and apply them to the confirmed cases parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"data0 = log_regions['Fatalities'].str[0]/log_regions['ConfirmedCases'].str[0]\ndata1 = log_regions['Fatalities'].str[1]/log_regions['ConfirmedCases'].str[1]\ndata2 = log_regions['Fatalities'].str[2]/log_regions['ConfirmedCases'].str[2]\nbins = np.arange(0, 3, 0.01)\nplt.hist(data,bins=bins, alpha=0.5)\nplt.xlim([0,3])\nplt.ylabel('count')\nplt.ylim([0,5])\nplt.show()\nfp = np.array([data0.median(),data1.median(),data2.median()])\nfp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"see it in action first:"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"for index, rt in log_regions.iterrows():\n    st = rt['Province_State']\n    co = rt['Country_Region']\n    popt = list(rt['ConfirmedCases'])\n    popt_ = list(rt['Fatalities'])\n    \n    if popt_ == [0.0,0.0,69.0]:\n        popt_ = np.multiply(fp,popt)\n        print(co,st,popt,popt_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\n\nfor index, rt in log_regions.iterrows():\n    st = rt['Province_State']\n    co = rt['Country_Region']\n    popt = list(rt['ConfirmedCases'])\n    popt_ = list(rt['Fatalities'])\n    if popt_ == [0.0,0.0,69.0]:\n        popt_ = np.multiply(fp,popt)\n    print(co,st,popt,popt_)\n    rtest = test[(test['Province_State']==st) & (test['Country_Region']==co)]\n    for index, rt in rtest.iterrows():\n        try:\n            tdate = rt['Date']\n            ca = logistic([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt)\n            try:\n                fa = logistic([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt_)\n            except:\n                fa = f([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt_)\n            submission.append((rt['ForecastId'], int(ca[0]), int(fa[0])))\n        except:\n            tdate = rt['Date']\n            ca = f([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt)\n            fa = f([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt_)\n            submission.append((rt['ForecastId'], int(ca[0]), int(fa[0])))\n\nprint(\"All done!\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare results for submission:"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(submission)\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission.to_csv('./submission.csv', index = False)\nprint(\"submission ready!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Public Score\n0.19234\n\nRank 66/220 as at 12:45 SAST Sunday, 29 March 2020.\n\nNot too bad for a simple approach. Let me know if you have any suggestions and how you fare!\n\nT"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}