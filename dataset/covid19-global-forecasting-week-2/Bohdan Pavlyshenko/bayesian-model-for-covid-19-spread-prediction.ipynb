{"cells":[{"metadata":{},"cell_type":"markdown","source":"For the predictive analytics of coronavirus spread, we used a logistic curve model. This model can be written as follows:\n<center><img src=\"https://media-exp1.licdn.com/dms/image/C4D12AQGQDAsbKlUy9A/article-inline_image-shrink_1000_1488/0?e=1591228800&v=beta&t=rmELRsMhGY7-YEVAkIOFpqoRHztjU4TZgfVOGvFKaA0\" width=\"300px\"></center>\nwhere Date0 is a start day for observations in the historical data set, it is measured in weeks. Coefficient alpha denotes maximum cases of coronavirus, coefficient beta is an empirical coefficient which denotes the rate of coronavirus spreading. Bayesian inference makes it possible to obtain probability density functions for model parameters and estimate the uncertainty that is important in the risk assessment analytics. In Bayesian regression approach, we can take into account expert opinions via information prior distribution. For Bayesian inference calculations, we used python pystan package. New historical data will correct the distributions for model parameters and forecasting results. In the practical analytics, it is important to find the maximum of coronavirus cases per day, this point means  estimated half time of coronavirus spread in the region under investigation. More details of our study are [here](http://www.linkedin.com/pulse/using-logistic-curve-bayesian-inference-modeling-bohdan-pavlyshenko/)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pystan\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns \n%matplotlib inline\nimport pystan\nimport datetime\nsns.set()\n\n# Set up options \n\n# File name of historical data file\ndata_filename='/kaggle/input/covid19-global-forecasting-week-2/train.csv'\n\n# Name of fields in the hystorical data frame for in the following order: \n# date, region, confirmed cases, fatalities\nfield_names=['Date','Country_Region','ConfirmedCases','Fatalities']\n\n# List of regions for prediction\nregion_list=['China','US','Italy','Spain','Iran','Germany','France']\n\n# Number of days for prediction\nn_days_predict=25\n\n# fields names:\nregion_field, cases_field,fatalities_field='region','cases','deaths'\n\n# Normalization coefficients\ntarget_field_norm_coef=1/100000\ntime_var_norm_coef=1/7\n\n# fields names:\nregion_field, cases_field,fatalities_field='region','cases','deaths'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_logistic = \"\"\"\n    data {\n        int<lower=1> n;\n        int<lower=1> n_pred;\n        vector[n] y;\n        vector[n] t;\n        vector[n_pred] t_pred;\n    }\n    parameters {\n        real<lower=0> alpha;\n        real<lower=0> beta;\n        real<lower=0> t0;\n        real<lower=0> sigma; \n    }\n    model {\n    alpha~normal(1,1);\n    beta~normal(1,1);\n    t0~normal(10,10);\n    y ~ normal(alpha ./ (1 + exp(-(beta*(t-t0)))), sigma);\n    }\n    generated quantities {\n      vector[n_pred] pred;\n      for (i in 1:n_pred)\n      pred[i] = normal_rng(alpha / (1 + exp(-(beta*(t_pred[i]-t0)))),sigma);\n    }\n    \"\"\"\n    \ndef plot_results(df_res,target_field,region_value, fit_samples):\n    fig, ax = plt.subplots(2,2, sharex=False, figsize=(15,10))\n    fig.subplots_adjust(hspace=0.35, wspace=0.35)\n    df_res['pred'].plot(yerr=df_res['pred_yerr'].values,title='Prediction ($10^5$)',ecolor='#aaaaaa',ax=ax[0,0])\n    ax[0,0].scatter(df_res.index,df_res.y,s=10,c='green')\n    df_res[['n_per_day_real','n_per_day_prediction']]=df_res[['y','pred']]-df_res[['y','pred']].shift(1)\n    df_res[['n_per_day_real','n_per_day_prediction']].plot(ax = ax[0,1],title='Number per day ($10^5$)')\n    alpha_mean=np.round(fit_samples['alpha'].mean(),3)\n    alpha_std=np.round(fit_samples['alpha'].std(),3)\n    beta_mean=np.round(fit_samples['beta'].mean(),3)\n    beta_std=np.round(fit_samples['beta'].std(),3)\n    t0_mean=np.round(fit_samples['t0'].mean(),3)\n    t0_std=np.round(fit_samples['t0'].std(),3)\n    print (f'Model parameters: alpha={alpha_mean} (sd:{alpha_std}), beta={beta_mean}(sd:{beta_std}),\\\n    t0={t0_mean}(sd:{t0_std})')\n    alpha_samples=np.round(pd.Series(fit_samples['alpha']),3)\n    alpha_samples.plot(kind='density', \n    title=r'$\\alpha$'+f' (mean:{alpha_mean}, sd:{alpha_std})',ax = ax[1,0])\n    beta_samples=pd.Series(fit_samples['beta'])\n    beta_samples.plot(kind='density', \n    title=r'$\\beta$'+f' (mean:{beta_mean}, sd:{beta_std})',ax = ax[1,1])\n    fig.suptitle(f'Number of {target_field} for {region_value}')\n    plt.show()\n    \ndef get_prediction(df,stan_model, n_days_predict=25, target_field='cases',\n                   region_field='region', region_value='China',\n                   target_field_norm_coef=target_field_norm_coef,\n                   time_var_norm_coef=time_var_norm_coef):\n    df.date=pd.to_datetime(df.date)\n    df_res=df.loc[df[region_field]==region_value, ['date',target_field]].set_index('date')\\\n    .groupby(pd.Grouper(freq='D'))\\\n    [target_field].sum().to_frame('y').reset_index()\n    print ('Time Series size:',df_res.shape[0])\n    n_train=df_res.shape[0]\n    maxdate=df_res.date.max()\n    for i in np.arange(1,n_days_predict+1):\n        df_res=df_res.append(pd.DataFrame({'date':\\\n            [maxdate+datetime.timedelta(days=int(i))]}))\n    df_res['t']=time_var_norm_coef*np.arange(df_res.shape[0])\n    df_res.y=target_field_norm_coef*df_res.y\n    df_res.set_index('date',inplace=True)\n    data = {'n': n_train,'n_pred':df_res.shape[0],\n            'y': df_res.iloc[:n_train,:].y.values,'t':df_res.iloc[:n_train,:]\\\n            .t.values,'t_pred':df_res.t.values}\n    fit=stan_model.sampling(data=data, iter=5000, chains=3)\n    fit_samples = fit.extract(permuted=True)\n    pred=fit_samples['pred']\n    df_res['pred']=pred.mean(axis=0)\n    df_res['pred_yerr']=(pd.DataFrame(pred).quantile(q=0.95,axis=0).values-\\\n                       pd.DataFrame(pred).quantile(q=0.05,axis=0).values)/2\n    plot_results(df_res,target_field,region_value,fit_samples)\n    return(df_res)\n    \ndef get_regions_prediction(df,stan_model, region_list,n_days_predict=25, \n    region_field=region_field,target_field='cases'):\n    for i in region_list:\n        print (f'\\n{i}:')\n        df_res=get_prediction(df,stan_model, n_days_predict=n_days_predict, target_field=target_field, \n        region_field=region_field,region_value=i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compile model"},{"metadata":{"trusted":true},"cell_type":"code","source":"stan_model= pystan.StanModel(model_code=model_logistic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(data_filename)[field_names]\ndf.columns=['date',region_field,cases_field,fatalities_field]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get prediction for one region"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_res=get_prediction(df,stan_model,n_days_predict=25, target_field='cases', region_value='China')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get prediction for list of regions"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print ('\\nNumber of cases for regions:')\nget_regions_prediction(df,stan_model, region_list,target_field='cases')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}