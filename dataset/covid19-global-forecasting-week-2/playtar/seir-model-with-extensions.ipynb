{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, scipy as sp\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, timedelta\nfrom time import strptime","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Credits\n\nFirst and foremost, thank you to the following kernels provided by **datasaurus** and **Patrick Sanchez** for motivating the study of SEIR models and providing useful functionalities (I'm trying to learn how to properly tag these users as this is my first kaggle notebook)\n\nhttps://www.kaggle.com/saga21/covid-global-forecast-sir-model-ml-regressions\n\nhttps://www.kaggle.com/anjum48/seir-model-with-intervention\n\n\n**The first** extension I want to provide from the previous notebooks are a detailed explanation of the ODE parameters. While this notebook only obtains point-wise estimates of the model parameters as motivated by the above notebooks, understanding the parameters in detail will help in formulating a Bayesian model as the next step. As more data comes in, both times series and medical statistics related to the virus, it would be interesting to see how much insight we can get when modeling the ODE parameters with some suitable prior distributions and obtain histograms rather than point estimates. In such a Bayesian model we have the flexibility to also consider grouping parameters geographically, as neighbouring countries may take similar measures due to governmental/social similarities. **(Bayes model discussed at the end).**\n\n**The second** extension is to split $R$ into two groups (details in parameters section). Since we are very interested in the total confirmed cases and fatalities, we can divide the R group into two mutually exclusive groups: Recovered (R) and Deceased (D). This will yield the model of the form $S \\to E \\to I \\to R / D$ and allow us to model the $D$ curve explicitly.\n"},{"metadata":{},"cell_type":"markdown","source":"# Vanilla Model\n\nThe SEIR model is a widely known epidemiological model of disease transmission within a population. It considers four main groups: Susceptible, Exposed, Infectious, Recovered (S, E, I, R). It is an extension of the SIR model in that the exposed group has come into contact with an infectious member but have not yet become infectious themselves, thereby considering the realistic assumption that the disease has some incubation period. If we consider $S_t,E_t,I_t,R_t$ to be the number of members of these respective groups at any time $t$, the evolution of these four groups can be modeled with the following set of Ordinary Differential Equations (ODEs):\n\n\\begin{align}\n&\\frac{dS_t}{dt} =-\\alpha S_t I_t \\label{eq1}\\tag{1} \\\\\n&\\frac{dE_t}{dt} =\\alpha S_t I_t -\\beta E_t \\label{eq2}\\tag{2} \\\\\n&\\frac{dI_t}{dt} = \\beta E_t - \\gamma I_t \\label{eq3}\\tag{3} \\\\\n&\\frac{dR_t}{dt} =\\gamma I_t \\label{eq4}\\tag{4}\\\\\n&N = S_t + E_t + I_t + R_t \\label{eq5}\\tag{5}\n\\end{align}\n\nwhere each equation explains the change in size of each group per unit time as they transition from one to another over time; the transitions through the states are strictly forward $(S \\to E \\to I \\to R)$. Note that group $R$ includes both recoveries and fatalities together.\n\nThis model assumes a fixed population size $N$ with no immigration, no disease intervention, that natural birth and death rates are equal, and of course the most strict assumption that all members are uniformly well mixed.\n\nFor further reading:\n\nhttps://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SEIR_model\n\n\n## Parameter interpretations\n\n## $\\alpha$\n\nIn equation $(1)$ $\\alpha$ describes the rate at which susceptible members become truly exposed per unit time. $\\alpha$ from $(1)$ can be expanded further as a function of a few things. If we consider:\n\n$r$ is the average number of people that an infectious person comes into contact with during infection\n<br>\n$\\frac{S}{N}$ is the percentage of the population currently susceptible\n<br>\n$\\rho$ is the probability that the disease is transmitted from an infectious member to a susceptible member given that they contacted\n<br>\n$T$ is the average time it takes for an infectious member to transition into the $R$ group\n\nIt follows that given $I_t$ infectious members and $S_t$ members currently susceptible, we would expect the number of suseptible members becoming exposed per unit time to be\n\n\\begin{align}\n    \\frac{\\rho \\times r \\times \\frac{S_t}{N} \\times I_t}{T}\n\\end{align}\n\nwhich implies that\n\n\\begin{align}\n    \\alpha=\\frac{\\rho r}{N T}\n\\end{align}\n\nTo implement disease intervention we can consider $r$ no longer to be constant, but some decay function overtime described above, $r_t$. In particular we consider the Hill decay function as contributed by datasaurus. But we can also modify it to ensure that number of contacts will never be reduced to 0 as that is unrealistic for humans.\n\n\\begin{align}\n    r(t;k,L)=r_0 \\left( \\frac{c}{1+(\\frac{t}{L})^k} +1-c \\right)\n\\end{align}\n\nWe now have a time dependent $\\alpha$\n\n\\begin{align}\n    \\alpha_t = \\frac{\\rho \\times r_t }{NT}\n\\end{align}\n\nIn this notebook we will fix $k=2.5$ and $L=25$ and $c=1$ strictly for illustration because this is what seemed to work in a lot of cases with the data so far.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"k, L = 2.5, 25\nhill = lambda t : 1 / (1 + (t/L)**k)\ntimes = np.arange(100)\n\nplt.plot(hill(times))\nplt.ylabel('Decay')\nplt.xlabel('Days')\nplt.title('Hill function with k=2.5, L=25')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To implement extension 2) we can consider splitting $R$ into two mutually exclusive groups: $R$ (recoveries from the disease) and $D$ (fatalities from the disease). If we consider\n\n$T_R$: the average time for $I$ member to recover\n<br>\n$T_F$: the average time to fatality of $I$ member\n<br>\n$p_R$: probability of infectious member recovering\n<br>\n$p_F$: probability of infectious member dying\n\nIt follows that given $I_t$ infectious members (who will recover) and $S_t$ members currently susceptible, we would expect the number of suseptible members becoming exposed per unit time by future survivors to be\n\n\\begin{align}\n    \\frac{\\rho \\times r_t \\times \\frac{S_t}{N} \\times p_R \\times I_t}{T_R}\n\\end{align}\n\nwhich implies that\n\n\\begin{align}\n    \\alpha_t^R = \\frac{\\rho r_t p_R }{NT_R}\n\\end{align}\n\nAnd we would expect the number of suseptible members becoming exposed per unit time by those who will die in the future to be\n\n\\begin{align}\n    \\frac{\\rho \\times r_t \\times \\frac{S_t}{N} \\times p_F \\times I_t}{T_F}\n\\end{align}\n\nwhich implies that\n\n\\begin{align}\n    \\alpha_t^F = \\frac{\\rho r_t p_F }{NT_F}\n\\end{align}\n\n## $\\beta$\n\nIn equation $(2)$ $\\beta$ describes the rate at which exposed members become infected per unit time. Given that an exposed member has successfully been transmitted the disease, then assuming they will become infectious after incubation period $T_I$ with probability 1:\n\n\\begin{align}\n    \\beta = \\frac{1}{T_I}\n\\end{align}\n\n## $\\gamma$\n\nIn equation $(3)$ $\\gamma$ describes the rate at which infectious members transition into $R$. We know some will be recoveries and some will be fatalities, so the ones who will recover will do so on average in $T_R$ days with probability $p_R$ and the ones who will die will do so on average in $T_F$ days with probability $p_F$\n\n\\begin{align}\n    \\gamma = \\frac{p_R}{T_R} + \\frac{p_F}{T_F}\n\\end{align}\n\n\n## Final Model\n\n\\begin{align}\n&\\frac{dS_t}{dt} = -\\frac{\\rho r_t p_R }{NT_R} S_t I_t - \\frac{\\rho r_t p_F }{NT_F} S_t I_t\\\\\n&\\frac{dE_t}{dt} =\\frac{\\rho r_t p_R }{NT_R} S_t I_t + \\frac{\\rho r_t p_F }{NT_F} S_t I_t -\\frac{1}{T_I}E_t\\\\\n&\\frac{dI_t}{dt} = \\frac{1}{T_I}E_t - \\frac{p_R}{T_R}I_t - \\frac{p_F}{T_F}I_t\\\\\n&\\frac{dR_t}{dt} = \\frac{p_R}{T_R}I_t\\\\\n&\\frac{dD_t}{dt} = \\frac{p_F}{T_F}I_t\\\\\n&N = S_t + E_t + I_t + R_t + D_t\\\\\n&p_F + p_R =1\n\\end{align}\n\nWe will be modeling the fatalities $D$ and the total confirmed cases, the total number of distinct confirmed cases (survivals and fatalities) at time $t$, $C_t$, can be computed as\n\n\\begin{align}\n    C_t = \\sum_{\\tau=1}^t I_{\\tau} + R_{\\tau} + D_{\\tau} - I_{\\tau-1} - R_{\\tau-1} - D_{\\tau-1}\n\\end{align}\n\n$(S_0,E_0,I_0,R_0,D_0)$ are the initial conditions at $t=0$.\n"},{"metadata":{},"cell_type":"markdown","source":"# Data\n\nPopulaton data is provided by datasaurus"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')\npopulations = pd.read_csv('/kaggle/input/covid19-population-data/population_data.csv')\npopulations = populations.drop(columns=['Type']).set_index('Name').transpose()\npopulations = populations.to_dict()\ntrain.columns = ['Id', 'State', 'Country', 'Date', 'ConfirmedCases', 'Fatalities']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define some functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization function for later\n\ndef multi_plot(M, susceptible = True, labels=False, interventions=False):\n    n = M.shape[0]\n    CC0 = M[2,0]+M[3,0]+M[4,0]\n    CCases = np.diff(M[2]+M[3]+M[4], prepend=CC0).cumsum()\n    Deaths = M[4]\n    fig = plt.figure()\n    ax1 = fig.add_axes([0.1, 1, 1.25, 1], ylabel='# of People')\n    ax2 = fig.add_axes([0.1, 0, 1.25, 1], ylabel='# of People')    \n    if susceptible == True:\n        rows=range(0,n)\n    else:\n        rows=range(1,n)\n    for ii in rows:\n        if labels == False:\n            ax1.plot(M[ii])\n        else:\n            ax1.plot(M[ii], label = labels[ii])\n    if interventions==False:\n        ax1.set_title('Time Evolution without intervention')\n    else:\n        ax1.set_title('Time Evolution with intervention')\n        for action, day in zip(list(interventions.keys()), [interventions[kk]['day'] for kk in list(interventions.keys())]):\n            ax1.axvline(x=day,label=action, linestyle='--')\n            ax2.axvline(x=day,label=action, linestyle='--')\n    ax1.legend(loc='best')\n    ax2.plot(CCases, label='ConfirmedCases', color='brown')\n    ax2.plot(Deaths, label='Deaths', color='black')\n    ax2.legend(loc='best')\n    ax2.set_xlabel('Days')\n    plt.show()\n\ncategories = ['Susceptible','Exposed','Infected','Recovered','Deceased']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SEIRD model for simulation\n\ndef reproduction(t):\n    intervention_days = [interventions[kk]['day'] for kk in list(interventions.keys())]\n    reproduction_rates = [interventions[kk]['reproduction_rate'] for kk in list(interventions.keys())]\n    ix=np.where(np.array(intervention_days)<t)[0]\n    \n    if len(ix)==0:\n        return R0\n    else:\n        return reproduction_rates[ix.max()]\n\n\ndef dS_dt(S, I, reproduction_t, alpha1, alpha2):\n    return -alpha1*reproduction_t*S*I -alpha2*reproduction_t*S*I\n\ndef dE_dt(S, I, E, reproduction_t, alpha1, alpha2, beta):\n    return alpha1*reproduction_t*S*I + alpha2*reproduction_t*S*I - beta*E\n\ndef dI_dt(E, I, beta, gamma, psi):\n    return beta*E - gamma*I - psi*I\n\ndef dR_dt(I, gamma):\n    return gamma*I\n\ndef dD_dt(I, psi):\n    return psi*I\n\n\ndef ODE_model(t, y, Rt, alpha1, alpha2, beta, gamma, psi):\n\n    if callable(Rt):\n        reproduction_t = Rt(t)\n    else:\n        reproduction_t = Rt\n    \n    S, E, I, R, D = y\n    St = dS_dt(S, I, reproduction_t, alpha1, alpha2)\n    Et = dE_dt(S, I, E, reproduction_t, alpha1, alpha2, beta)\n    It = dI_dt(E, I, beta, gamma, psi)\n    Rt = dR_dt(I, gamma)\n    Dt = dD_dt(I, psi)\n    return [St, Et, It, Rt, Dt]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simulation\n\nHere we will look at what the evolution of SEIRD will look like under intervention. Interventions such as quarantines and social distancing reduce the number of members an infected person has contact with which can result in a slower rate of reproduction of the disease overtime.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix some population parameters for simulation (refer to previous sections for interpretations)\n\nN = 1000 # population size\nT_inc = 10 # days for average incubation\nT_rec = 14 # days for average recovery\nT_die = 10 # days for average infection duration given death\nR0 = 5 # average number of contacts an infected person has per day\ntau = 0.2 # probability of transmission given S <-> I contact\np_live = 0.95 # average survival rate\np_die = 0.05 # average pmortality rate\nndays = 100 # number of days simulated\n\n# ODE parameters\nalpha1 = tau*p_live/N # average % of susceptible people who get infected by survivor\nalpha2 = tau*p_die/N # average % of susceptible people who get infected by non-survivor\nbeta = 1/T_inc # transition rate of incubation to infection\ngamma = p_live/T_rec # transition rate of infection to recovery\npsi = p_die/T_die # transition rate of infection to mortality\n\ny0 = [N-1, 1, 0, 0, 0] # initial conditions\nt_span = [0, ndays] # dayspan to evaluate\nt_eval = np.arange(ndays) # days to evaluate\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we look at the evolution given no intervention\n\ninterventions = {}\n\nsolution = sp.integrate.solve_ivp(fun = ODE_model, t_span = t_span, t_eval = t_eval, y0 = y0, \n                                  args = (R0, alpha1, alpha2, beta, gamma, psi))\n\nY = np.maximum(solution.y,0)\n\nmulti_plot(Y, labels=categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this simulation, we look at the evolution given 2 interventions\n# we may consider what happens to the process after some intervention, which\n# reduces the average number of contacts an infected person has per day\n# such as social distancing and lockdown\n\ninterventions = {'social_distancing':{'day':25, 'reproduction_rate':2},\n                 'lockdown':{'day':35, 'reproduction_rate':.5}}\n\nsolution = sp.integrate.solve_ivp(fun = ODE_model, t_span = t_span, t_eval = t_eval, y0 = y0, \n                                  args = (reproduction, alpha1, alpha2, beta, gamma, psi))\n\nY = np.maximum(solution.y,0)\n\nmulti_plot(Y, labels=categories, interventions=interventions)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One of the key things to notice is that the effects of any measure will show a lag, and this is partly due to the incubation period of the disease. At the time when lockdown is implemented, total number of infectious people was less than 200 while those who were exposed and incubating the virus was already well over 200, this is why we see the confirmed cases continue to increase for some time after intervention. \n\nIn the first scenario with no intervention, we can see that the system had reached equilibrium well before day 100, in other words the entire population became infectious at some point. With intervention, we see that equilibrium has not yet been reached as the rates of transmission have significantly reduced. In fact, after 500 days there will be a portion of the population that never became exposed at all!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"ndays = 500\nt_span = [0, ndays] # dayspan to evaluate\nt_eval = np.arange(ndays) # days to evaluate\n\nsolution = sp.integrate.solve_ivp(fun = ODE_model, t_span = t_span, t_eval = t_eval, y0 = y0, \n                                  args = (reproduction, alpha1, alpha2, beta, gamma, psi))\n\nY = np.maximum(solution.y,0)\n\nmulti_plot(Y, labels=categories, interventions=interventions)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Extended Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dS_dt(S, I, alpha1_t, alpha2_t):\n    return -alpha1_t*S*I -alpha2_t*S*I\n\ndef dE_dt(S, I, E, alpha1_t, alpha2_t, beta):\n    return alpha1_t*S*I + alpha2_t*S*I - beta*E\n\ndef dI_dt(E, I, beta, gamma, psi):\n    return beta*E - gamma*I - psi*I\n\ndef dR_dt(I, gamma):\n    return gamma*I\n\ndef dD_dt(I, psi):\n    return psi*I\n\n\ndef ODE_model(t, y, alpha1t, alpha2t, beta, gamma, psi):\n\n    alpha1_t = alpha1t(t)\n    alpha2_t = alpha2t(t)\n    \n    S, E, I, R, D = y\n    St = dS_dt(S, I, alpha1_t, alpha2_t)\n    Et = dE_dt(S, I, E, alpha1_t, alpha2_t, beta)\n    It = dI_dt(E, I, beta, gamma, psi)\n    Rt = dR_dt(I, gamma)\n    Dt = dD_dt(I, psi)\n    return [St, Et, It, Rt, Dt]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(theta, data, population, k, L, nforecast=0, error=True):\n    alpha1_0, alpha2_0, beta, gamma, psi = theta\n    \n    Infected_0 = data.ConfirmedCases.iloc[0]\n    ndays = nforecast\n    ntrain = data.shape[0]\n    y0 = [(population-Infected_0)/population, 0, Infected_0/population, 0, 0]\n    t_span = [0, ndays] # dayspan to evaluate\n    t_eval = np.arange(ndays) # days to evaluate\n    \n    def a1_t(t):\n        return alpha1_0 / (1 + (t/L)**k)\n\n    def a2_t(t):\n        return alpha2_0 / (1 + (t/L)**k)\n\n    sol = sp.integrate.solve_ivp(fun = ODE_model, t_span = t_span, t_eval = t_eval, y0 = y0, \n                                 args = (a1_t, a2_t, beta, gamma, psi))\n    \n    pred_all = np.maximum(sol.y, 0)\n    ccases_pred = np.diff((pred_all[2] + pred_all[3] + pred_all[4])*population, n = 1, prepend = Infected_0).cumsum()\n    deaths_pred = pred_all[4]*population\n    ccases_act = data.ConfirmedCases.values\n    deaths_act = data.Fatalities.values\n    \n    if ccases_act[-1]<ccases_act[-2]:\n        ccases_act[-1]=ccases_act[-2]\n    if deaths_act[-1]<deaths_act[-2]:\n        deaths_act[-1]=deaths_act[-2]\n    \n    weights =  np.exp(np.arange(data.shape[0])/10)/np.exp((data.shape[0]-1)/10) \n\n    ccases_rmse = np.sqrt(mean_squared_error(ccases_act, ccases_pred[0:ntrain], sample_weight=weights))\n    deaths_rmse = np.sqrt(mean_squared_error(deaths_act, deaths_pred[0:ntrain], sample_weight=weights))\n\n    loss = np.mean((ccases_rmse, deaths_rmse))\n    \n    if error == True:\n        return loss\n    else:\n        return loss, ccases_pred, deaths_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DISCLAIMER: \nFirst as this is a fairly recent outbreak, there is still very little data available (as of March 25 of writing this). Second as this notebook is only considering $\\textbf{one model}$ for illustrative purposes only, there will be no validation, only fitting on the available training data. This is a work in progress and I will play around with a few more models as time goes on."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['location'] = train['State'].fillna(train['Country'])\nlocations=list(train['location'].drop_duplicates())\ntrain.set_index(['location', 'Date'], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"parms0 = [1.5, 1.5, 0.5, 0.05, 0.001]\nbnds = ((0.001, None), (0.001, None), (0, 10), (0, 10), (0, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_ODE_model(location, k, L):\n        \n    dat = train.loc[location].query('ConfirmedCases > 0')\n    nforecast = 75\n    population = populations[location]['Population']\n    n_infected = train['ConfirmedCases'].iloc[0]\n        \n    res = sp.optimize.minimize(fun = loss, x0 = parms0, \n                               args = (dat, population, k, L, nforecast),\n                               method='L-BFGS-B', bounds=bnds)\n    \n    dates_all = [str(datetime.strptime(dat.index[0], '%Y-%m-%d') + timedelta(days = ii))[0:10] for ii in range(nforecast)]\n    \n    err, ccases_pred, deaths_pred = loss(theta = res.x, data = dat, population = population, k=k, L=L, \n                                         nforecast=nforecast, error=False)\n    \n    predictions = pd.DataFrame({'ConfirmedCases': ccases_pred,\n                                'Fatalities': deaths_pred}, index=dates_all)\n    \n    train_true = dat[['ConfirmedCases',  'Fatalities']]\n    predictions.columns = ['ConfirmedCases_pred',  'Fatalities_pred']\n\n    plot_df = pd.merge(predictions,train_true,how='left', left_index=True, right_index=True)\n\n    plt.plot(plot_df.ConfirmedCases_pred.values, color='green',linestyle='--', linewidth=0.5, label='Confirmed Cases (pred)')\n    plt.plot(plot_df.Fatalities_pred.values, color='blue',linestyle='--', linewidth=0.5, label='Fatalities (pred)')\n    plt.plot(plot_df.Fatalities.values, color='red', label='Fatalities (real)')\n    plt.plot(plot_df.ConfirmedCases.values, color='orange', label='Confirmed Cases (real)')\n    plt.title(location)\n    plt.xlabel('Days since first case in '+location)\n    plt.ylabel('Confirmed Cases')\n    plt.legend(loc='best')\n    plt.show()        \n    \n    print(res.x)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vizualize"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_ODE_model('Korea, South', k, L)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just an observation that the parameter corresponding to $\\frac{p_F}{T_F}=0.0037784$. This may be in line with some current estimates that mortality rates can be as high as $5 \\%$ and incubation time is 14 days $(0.05/14 = 0.0035714)$. Much more analysis must be done to estimate uncertainties in these parameters from the data. Also keep in mind that both the numerator and denominator here can change substantially across countries (different countries implement different interventions and have different medical resources, population demographics, etc) so I am very open to this similarity being just chance."},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_ODE_model('Hubei', k, L)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_ODE_model('Netherlands', k, L)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_ODE_model('Spain', k, L)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit_ODE_model('Poland', k, L)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions\n\nIn practice, fitting ODE models can be very tricky. They are very sensitive to initial conditions and especially in this case because while the initial condition can be only 1 infected person, we are solving the ODE system with a susceptile (group S) population size of sometimes 50M+ people, and so the data we see so far is only a very recent evolution that has only reached a small portion of the total population, and given only a few weeks of more data we may start to see some very different dynamics.\n\nThe model, while considering intervention, does not account for many other significant factors such as (sub)population demographics, population densities in different areas, how populations move in reaction to intervention announcement, health resources (ICU beds, ventilators), how temperatures and local conditions can affect the spread, the list goes on.\n"},{"metadata":{},"cell_type":"markdown","source":"# Bayesian Model\n\nThe next step will be to infer parameters from posterior sampling. As new data comes in regarding viral characteristics we can start to form some prior distributions over the parameters.\n\nIn terms of a likelihood function, we can assume that the variance of the confirmed cases errors to some extent varies over time, which may be slightly more realtistic than assuming the variance is constant because one might expect that as the nuber of cases grows, then the so does the margin of error, etc. In other words, if we consider some error with respect to cumulative cases $(\\epsilon_t=C_t^{real}-C_t(\\Theta))$, where $C_t$ is the solution to the ODE parameterized by $\\Theta=(T_F,T_R,T_I,p_F,p_R,\\rho, L,k)$, we can impose a Gaussian error model of the form\n\n\\begin{align}\n    \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N} \\left(0, \\sigma_{t}^2 \\right)\\\\\n\\end{align}\n\nIf we consider instead $\\sigma_{t}^2 = \\frac{1}{2 w_t}$ then we have\n\n\\begin{align}\n    \\epsilon_t \\overset{iid}{\\sim} \\mathcal{N} \\left(0, \\frac{1}{2 w_t} \\right)\\\\\n\\end{align}\n\nIt turns out that maximizing this likelihood is equivalent to minimizing a weigthed sum of squared error loss function where $w_t$ is some weight assigned to each error at time $t$ (which is similar to what was done when fitting the model above already).\n\nIf we have some prior distribution such that $\\Theta \\sim \\pi(\\Theta)$ then we can sample $\\Theta$ from the posterior distribution where\n\n\\begin{align}\n    p(\\Theta | \\mathcal{D}) \\propto L(\\Theta | \\mathcal{D})\\pi(\\Theta)\n\\end{align}\n\nExpanding on this we would have a posterior that we sample of the form \n\n\\begin{align}\n    p(\\Theta | \\mathcal{D}) \\propto \\prod_{t} e^{-w_t\\left( C_t^{real}-C_t(\\Theta) \\right)^2} \\pi(\\Theta)\n\\end{align}\n\n\nFrom an optimization perspective, this would be equivalent to minimizing the negative log-posterior\n\n\\begin{align}\n    min_{\\Theta} \\quad -p(\\Theta | \\mathcal{D}) = \\sum_{t}w_t \\left(C_t^{real}-C_t(\\Theta) \\right)^2 -nlog \\pi(\\Theta) \\\\\n\\end{align}\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}