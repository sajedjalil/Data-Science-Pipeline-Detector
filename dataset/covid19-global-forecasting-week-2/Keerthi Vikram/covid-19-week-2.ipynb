{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Data science libraries used by me\n# import gc\n# import dtale\n# import model_evaluation_utils as meu\n# import OrignalFunctionsVikram as ofv\nimport os\nimport re\nimport warnings\nfrom collections import Counter\n# from pathlib import Path\n\n# import eli5\nimport featuretools as ft\nimport hyperopt as hp\nimport imblearn\n# import knime\nimport lightgbm\n# import lime\nimport matplotlib\n\n# Matplotlib visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Pandas and numpy for data manipulation\nimport pandas as pd\nimport pandas_profiling as pp\n# import pydotplus\n# import pylab\nimport scipy.stats as st\n\n# Seaborn for visualization\nimport seaborn as sns\n# import shap\nimport sklearn\n# import statsmodels.api as sm\n# import statsmodels.formula.api as smf\n# import statsmodels.stats.api as sms\n# import statsmodels.stats.stattools as stt\n# import statsmodels.tsa.api as smt\nimport tpot\n# from catboost import CatBoostClassifier\n# from graphviz import Source\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n# from imblearn.over_sampling import SMOTE\n\n# # Internal ipython tool for setting figure size\n# from IPython.core.pylabtools import figsize\n# from IPython.display import SVG, Image\n# from jupyterthemes import jtplot\n# from lime import lime_tabular\n# from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n# from pyforest import *\n# from scipy.cluster.hierarchy import cut_tree, dendrogram, linkage\n# from scipy.stats import expon as sp_expon\n# from scipy.stats import randint as sp_randint\n# from scipy.stats import ttest_1samp, ttest_ind, ttest_ind_from_stats\n# from scipy.stats import uniform as sp_uniform\n# from scipy.stats import wilcoxon\n# from six import StringIO\n\n# from skater.core.explanations import Interpretation\n# from skater.model import InMemoryModel\n# from skater.util.dataops import show_in_notebook\n# from sklearn import metrics, tree\n# from sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    RandomForestClassifier,\n    RandomForestRegressor,\n    VotingClassifier,\n)\n# from sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import (\n    ElasticNet,\n    ElasticNetCV,\n    Lasso,\n    LassoCV,\n    LinearRegression,\n    LogisticRegression,\n    Ridge,\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    r2_score,\n    roc_auc_score,\n    roc_curve,\n)\n\n# Splitting data into training and testing\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    KFold,\n    RandomizedSearchCV,\n    cross_val_score,\n    train_test_split,\n)\n# from sklearn.multiclass import OneVsRestClassifier\n# from sklearn.naive_bayes import BernoulliNB, GaussianNB\n# from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import (\n#     LabelEncoder,\n#     OneHotEncoder,\n#     PolynomialFeatures,\n#     StandardScaler,\n#     minmax_scale,\n# )\n# from sklearn.svm import SVC, SVR\n# from sklearn.tree import DecisionTreeClassifier, export_graphviz\n# from skrules import SkopeRules\n# from statsmodels.compat import lzip\n# from statsmodels.formula.api import ols\n# from statsmodels.graphics.gofplots import ProbPlot\n# from statsmodels.stats.anova import anova_lm\n# from statsmodels.stats.multicomp import pairwise_tukeyhsd\n# from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n# from statsmodels.stats.power import ttest_power\n# from tpot import TPOTClassifier, TPOTRegressor\n\nfrom xgboost import XGBClassifier,XGBRegressor\n\n# %load_ext nb_black\n\n\n# %load_ext autotime\n\n# No warnings about setting value on copy of slice\npd.options.mode.chained_assignment = None\n\n# Display up to 60 columns of a dataframe\npd.set_option(\"display.max_columns\", 60)\n\n%matplotlib inline\n\n# Set default font size\nplt.rcParams[\"font.size\"] = 24\n\nsns.set(font_scale=2)\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n# os.environ[\"PATH\"] = (\n#     os.environ[\"PATH\"] + \";\" + os.environ[\"CONDA_PREFIX\"] + r\"\\Library\\bin\\graphviz\"\n# )\n\n# matplotlib.rcParams.update({\"font.size\": 12})\n# # warnings.filterwarnings('ignore')\n# %config InlineBackend.figure_format = 'retina'\n# gc.collect()\n# jtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True, grid=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : READING THE DATASETS\ndf2 = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")\ndf1 = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df1)\ndisplay(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : CHANGE TO PD.DATETIME\ndf1.Date = pd.to_datetime(df1.Date,infer_datetime_format=True)\ndf2.Date = pd.to_datetime(df2.Date,infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Avoid Data Leakage\n### As the Train Dataset has records till 27th March 2020 and Test Dataset has partial intersection of records from 19th March 2020. Let us concise the Train Dataset to 18th March 2020."},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : CONCISING THE TRAIN DATASET TO 18TH MARCH 2020.\nMIN_TEST_DATE = df2.Date.min()\ndf1 = df1.loc[df1.Date < MIN_TEST_DATE, :]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : RESETTING INDEX\ndf1.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FILLING MISSING VALUES\ndf1.fillna('',inplace=True)\ndf2.fillna('',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : CREATING NEW REGION COLUMN\ndf1['Region'] = df1['Country_Region'] + df1['Province_State']\ndf2['Region'] = df2['Country_Region'] + df2['Province_State']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : DROPPING COUNTRY REGION AND STATE\ndf1.drop(['Country_Region','Province_State'],axis=1,inplace=True)\ndf2.drop(['Country_Region','Province_State'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : CONVERTING DATE COLUMN TO INTEGER\ndf1.loc[:, 'Date'] = df1.Date.dt.strftime(\"%m%d\")\ndf2.loc[:, 'Date'] = df2.Date.dt.strftime(\"%m%d\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(data=df1,x='Date',y='ConfirmedCases',hue='Region')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(data=df1,x='Date',y='Fatalities',hue='Region')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : CREATING X AND Y\nX1 = df1.drop(['ConfirmedCases','Fatalities'],axis=1)\nX2 = df1.drop(['ConfirmedCases','Fatalities'],axis=1)\ny1 = df1['ConfirmedCases']\ny2 = df1['Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : TEST 1 AND 2\ntest_1 = df2.copy()\ntest_2 = df2.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : FUNCTION FOR MEAN ENCODING\nfrom sklearn.base import BaseEstimator\nclass MeanEncoding(BaseEstimator):\n\n\n    \"\"\"   In Mean Encoding we take the number\n    of labels into account along with the target variable\n    to encode the labels into machine comprehensible values    \"\"\"\n\n    def __init__(self, feature, C=0.1):\n        self.C = C\n        self.feature = feature\n\n    def fit(self, X_train, y_train):\n\n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n\n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n\n        self.encoding = (self.global_mean * self.C + mean * size) / (self.C + size)\n\n    def transform(self, X_test):\n\n        X_test[self.feature] = X_test[self.feature].map(self.encoding).fillna(self.global_mean).values\n\n        return X_test\n\n    def fit_transform(self, X_train, y_train):\n\n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n\n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        self.encoding = (self.global_mean * self.C + mean * size) / (self.C + size)\n\n        X_train[self.feature] = X_train[self.feature].map(self.encoding).fillna(self.global_mean).values\n\n        return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f2 in ['Region']:\n    me2 = MeanEncoding(f2, C=0.01 * len(X2[f2].unique()))\n    me2.fit(X2, y2)\n    X2 = me2.transform(X2)\n    test_2 = me2.transform(test_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f1 in ['Region']:\n    me1 = MeanEncoding(f1, C=0.01 * len(X1[f1].unique()))\n    me1.fit(X1, y1)\n    X1 = me1.transform(X1)\n    test_1 = me1.transform(test_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : FUNCTION FOR COMPARING DIFFERENT REGRESSORS\ndef algorithim_boxplot_comparison(X,\n                                  y,\n                                  algo_list=[],\n                                  random_state=3,\n                                  scoring='r2',\n                                  n_splits=10):\n    \"\"\"To compare metric of different algorithims\n       Paramters-\n       algo_list : a list conataining algorithim models like random forest, decision trees etc.\n       X : dataframe without Target variable\n       y : dataframe with only Target variable\n       random_state : The seed of randomness. Default is 3\n       n_splits : Number of splits used. Default is 3\n       ( Default changes from organization to organization)\n       Returns-\n       median accuracy and the standard deviation accuracy.\n       Box Plot of Acuuracy\"\"\"\n    import matplotlib.pyplot as plt\n    from sklearn import model_selection\n    import numpy as np\n    results=[]\n    names=[]\n    for algo_name, algo_model in algo_list:\n        kfold=model_selection.KFold(shuffle=True,\n                                      n_splits=n_splits,\n                                      random_state=random_state)\n        cv_results=model_selection.cross_val_score(algo_model,\n                                                     X,\n                                                     y,\n                                                     cv=kfold,\n                                                     scoring=scoring)\n        results.append(cv_results)\n        names.append(algo_name)\n        msg=\"%s: %s : (%f) %s : (%f) %s : (%f)\" % (\n            algo_name, 'median', np.median(cv_results), 'mean',\n            np.mean(cv_results), 'variance', cv_results.var(ddof=1))\n        print(msg)\n    # boxplot algorithm comparison\n    fig=plt.figure()\n    fig.suptitle('Algorithm Comparison')\n    ax=fig.add_subplot(111)\n    plt.boxplot(results)\n    ax.set_xticklabels(names)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : REGRESSORS\nlr = LinearRegression(n_jobs=-1)\nrfr = RandomForestRegressor(random_state=96,n_jobs=-1)\ngbr = GradientBoostingRegressor(random_state=96)\nxgbr = XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : APPENDING THE REGRESSORS IN A LIST\nmodels = []\nmodels.append(('lr',lr))\nmodels.append(('rfr',rfr))\nmodels.append(('gbr',gbr))\nmodels.append(('xgbr',xgbr))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : COMPARING DIFFERENT REGRESSORS\nalgorithim_boxplot_comparison(X1,y1,models,random_state=96,scoring='neg_root_mean_squared_error',n_splits=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : HYPEROPT\n# TODO : USE MORE ADVANCED HYPERPARAMTER TUNING METHODS LIKE OPTUNA, KEARS-TUNER, HPBANDSTER,TUNE\n\ndef auc_model(params):\n    params = {'n_estimators': int(params['n_estimators']),\n              'max_features': int(params['max_features']),\n              'min_samples_leaf': int(params['min_samples_leaf']),\n              'min_samples_split': int(params['min_samples_split'])}\n    clf = RandomForestRegressor(**params,random_state=96,n_jobs=-1)\n    return cross_val_score(clf, X1, y1, cv=3, scoring='neg_mean_squared_log_error').mean()\n\n\nparams_space = {'n_estimators': hp.quniform('n_estimators', 0, 300, 50),\n                'max_features': hp.quniform('max_features', 1, 3, 1),\n                'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 50, 1),\n                'min_samples_split': hp.quniform('min_samples_split',1, 50, 1)}\nbest = 0\n\n\ndef f(params):\n    global best\n    auc = auc_model(params)\n    if auc > best:\n        print('New Best', best, params)\n    return {'loss': -auc, 'status': STATUS_OK}\n\n\ntrials = Trials()\nbest = fmin(f, params_space, algo=tpe.suggest, max_evals=200, trials=trials)\nprint('best:\\n',best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : HYPEROPT\n# TODO : USE MORE ADVANCED HYPERPARAMTER TUNING METHODS LIKE OPTUNA, KEARS-TUNER, HPBANDSTER,TUNE\n\ndef auc_model(params):\n    params = {'n_estimators': int(params['n_estimators']),\n              'max_features': int(params['max_features']),\n              'min_samples_leaf': int(params['min_samples_leaf']),\n              'min_samples_split': int(params['min_samples_split'])}\n    clf = RandomForestRegressor(**params,random_state=96,n_jobs=-1)\n    return cross_val_score(clf, X2, y2, cv=3, scoring='neg_mean_squared_log_error').mean()\n\n\nparams_space = {'n_estimators': hp.quniform('n_estimators', 0, 300, 50),\n                'max_features': hp.quniform('max_features', 1, 3, 1),\n                'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 50, 1),\n                'min_samples_split': hp.quniform('min_samples_split',1,50, 1)}\nbest = 0\n\n\ndef f(params):\n    global best\n    auc = auc_model(params)\n    if auc > best:\n        print('New Best', best, params)\n    return {'loss': -auc, 'status': STATUS_OK}\n\n\ntrials = Trials()\nbest = fmin(f, params_space, algo=tpe.suggest, max_evals=200, trials=trials)\nprint('best:\\n',best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : RANDOMFORESTREGRESSOR FOR CONFIRMEDCASUALTIES\nrfr1 = RandomForestRegressor(max_features= 3, min_samples_leaf= 25, min_samples_split= 26, n_estimators= 250,random_state=96,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : RANDOMFORESTREGRESSOR FOR FATALITIES\nrfr2= RandomForestRegressor(max_features= 3, min_samples_leaf= 17, min_samples_split= 23,n_estimators=100,random_state=96,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : FITTING RANDOMFORESTREGRESSOR FOR CONFIRMEDCASUALTIES\nrfr1.fit(X1,y1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : FITTING RANDOMFORESTREGRESSOR FOR FATALITIES\nrfr2.fit(X2,y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : PREDICTING CONFIRMEDCASUALTIES\ny_n_1 = rfr1.predict(test_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : PREDICTING FATALITIES\ny_n_2 = rfr2.predict(test_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : SUBMISSION.CSV\ndf3 = pd.read_csv('../input/covid19-global-forecasting-week-2/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : ADDING CONFIRMEDCASES\ndf3.ConfirmedCases = round(pd.DataFrame(y_n_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE : ADDING FATALITIES\ndf3.Fatalities = round(pd.DataFrame(y_n_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.7-final"},"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":4}