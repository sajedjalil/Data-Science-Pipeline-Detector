{"cells":[{"metadata":{},"cell_type":"markdown","source":"Spatial Regression is an approach popular across many disciplines to estimate regression models in the presence of spatial correlation.  For economist investigating the covariates which drive housing prices, controlling for spatial correlation in our models allows us to better control for neighbourhood effects. These neighbourhood effects may be as a result of the willingness of a buyer to pay more for a house near other nice houses or as the results of unobserved variables in the data which capture the distance to schools, shops and restaurants.  \n  \nIn analyzing the spread and impact of the COVID-19 disease, I have been fascinated by whether we observed uncontrolled spatial correlation in infections or fatalities between neighbouring countries.  This may be as the results of trade or migrations between neighbouring countries or as the results of other properties of the weather or geography which are difficult to account for in the model. \n\nIn this notebook, we are interested in identifying whether controlling for this autocorrelation provides for more stable estimates to our model, as we as whether the physical distances between countries reflect honest notions of similarity between nations which may otherwise be better captured by the total number of flights or migration between countries or trade. "},{"metadata":{},"cell_type":"markdown","source":"# Contents\n1. [Data](#Data)  \n2. [Spatial Regression Model](#Spatial-Regression-Model)  \n3. [Conclusion](#Conclusion)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"! apt install libgeos-dev\n! pip uninstall -y shapely; pip install --no-binary :all: shapely==1.6.4\n! pip uninstall -y cartopy; pip install --no-binary :all: cartopy==0.17.0\n! conda install -y geoviews=1.6.6 hvplot=0.5.2 bokeh==1.4.0 pysal","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nfrom operator import add, mul\nfrom time import time\nfrom functools import wraps\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport hvplot.pandas\nimport holoviews as hv\nimport cartopy.crs as ccrs\nimport geopandas as gpd\nfrom toolz.curried import map, partial, pipe, reduce\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport pysal as ps\nfrom sklearn.manifold import MDS, smacof\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.\nhv.extension('bokeh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"I chose to draw on a number of data sources on not only COVID cases but country indicators on GDP, infant mortality, etc., as well as data on population estimates and land size.  In order to better control for the variance in Fatalities and Confirmed Cases as a result of country sizes, I opted to look at Fatalities and Confirmed Cases per Capita.  As, at this stage in the virus, the pandemic is still dominated by the exponential growth in new cases, I opted to analyze the relationship between the percent change in Fatalities or Confirmed Cases per Capita, against percent changes in our factors.  Two interesting exceptions to this were variables representing the weeks since the first case in the country and the first case death, where I included both the log of the weeks since this event, to represent percent change, and the original value. This is used to model any effects relating to the logarithmic flattening of the curve late in the infection in a given country.  "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def timing(f):\n    @wraps(f)\n    def wrap(*args, **kw):\n        ts = time()\n        result = f(*args, **kw)\n        te = time()\n        print('func:%r took: %2.4f sec' % \\\n          (f.__name__, te-ts))\n        return result\n    return wrap\n\ndef shape(f, outputs=True, inputs=True):\n    @wraps(f)\n    def wrap(*args, **kw):\n        ts = time()\n        result = f(*args, **kw)\n        te = time()\n        \n        if inputs:\n            print('func:%r input shape:%r' % \\\n              (f.__name__, args[0].shape))\n            \n        if outputs:\n            print('func:%r output shape:%r' % \\\n              (f.__name__, result.shape))\n        return result\n    return wrap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')).replace('United States of America', 'US')\ncovid = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv', parse_dates=['Date'], index_col='Id')\nindicators = pd.read_csv('/kaggle/input/countries-of-the-world/countries of the world.csv', decimal=',').replace('United States', 'US')\n\ncountry_indicators = (countries.assign(name = lambda df: df.name.astype(str).str.strip())\n                     .merge(indicators.assign(Country = lambda df: df.Country.astype(str).str.strip()), \n                            left_on = 'name', right_on='Country', how='inner'))\nweeks = (covid\n         .assign(dayofweek = lambda df: df.Date.dt.dayofweek)\n         .set_index('Date')\n         .drop(columns=['Province_State'])\n         .groupby(['Country_Region', pd.Grouper(freq='W')]).agg({'ConfirmedCases':'sum', 'Fatalities':'sum', 'dayofweek':'max'})\n         .reset_index()\n         .where(lambda df: df.ConfirmedCases > 0)\n         .dropna(0)\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .sort_values('Date')\n                            .assign(week_of_infection = lambda df: pd.np.arange(df.shape[0]))))\n         .where(lambda df: df.dayofweek >= 6)\n         .drop(columns=['dayofweek'])\n         .dropna(0)\n         .reset_index(drop=True)\n         .merge(country_indicators, left_on='Country_Region', right_on='name', how='inner')\n         .pipe(lambda df: gpd.GeoDataFrame(df, geometry='geometry'))\n         .assign(ConfirmedCases_per_capita = lambda df: (df.ConfirmedCases / df.pop_est),\n                 Fatalities_per_capita= lambda df: (df.Fatalities / df.pop_est),\n                 land_area = lambda df: df.area.astype('float'),\n                 week_of_infection_exp = lambda df: df.week_of_infection.apply(np.exp))\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .assign(week_since_first_death = lambda x: (x.week_of_infection - x.where(lambda y: y.Fatalities > 0)\n                                                                        .week_of_infection.min())\n                                                                        .clip(lower=0)\n                                                                        .fillna(0))))\n         .assign(week_since_first_death_exp = lambda df: df.week_since_first_death.apply(np.exp))\n         .drop(columns = 'gdp_md_est'))\nweeks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to model the spatial correlation between countries, we critically need to find an approach to compute the distances between these nations.  For simplicity, we have opted to fall back on geopandas default projection and use the euclidean distances between the midpoints of the countries in order to esimate their distances.  For sparse countries, it may be more faithful to use the shortest distance between borders or between capital cities, but this appeared computationally intensive and may be grounds for future work.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"points = (weeks.geometry\n          .representative_point()\n          .apply(lambda df: pd.Series([df.x, df.y]))\n          .rename(columns={0:'x',1:'y'}))\nhv.Labels(points.assign(names = weeks.name).drop_duplicates(), kdims=['x','y'], vdims='names').opts(height=500, width=800, title='Midpoints of Countries')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"distances = euclidean_distances(points)\nD = pd.DataFrame(distances, index = weeks.name, columns = weeks.name).drop_duplicates().T.drop_duplicates()\nZ, score = smacof(D, n_components=2)\n\n(hv.Labels(pd.DataFrame(Z, columns=['x','y'])\n           .assign(names = D.index)\n           .drop_duplicates(), kdims=['x','y'], vdims='names').opts(height=500, width=800))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To construct our design matrix for our experiment, we opted to include all our numeric columns and, to account for country-specific effects, we opted to include dummy variables for countries.  This design matrix is reused in both models, to estimate covariates for %Δ Fatalities/capita.  The reason we opted to model %Δ Fatalities/capita and not %Δ Confirmed Cases/capita, is due to problems of testing which appears highly correlated with many indicators.  "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"X, y_fatalities = (weeks\n     .select_dtypes(include=['number'])\n     .drop(columns=['ConfirmedCases', 'Fatalities', 'ConfirmedCases_per_capita', 'Fatalities_per_capita'])\n     .replace(0, 1e-8)# add jitter\n     .transform(np.log)\n     .pipe(lambda df: df.fillna(df.mean()))\n     .rename(columns = lambda name: '%Δ ' + name)\n     .rename(columns = {'%Δ week_of_infection_exp': 'week_of_infection'})\n     .rename(columns = {'%Δ week_since_first_death_exp': 'week_since_first_death'})\n     .pipe(lambda df: pd.concat([df, pd.get_dummies(weeks.name, drop_first=True).rename(columns =lambda s: 'is_'+s)], axis=1))\n     .assign(const = 1),\n        \n    weeks\n    .loc[:, ['Fatalities_per_capita']]\n    .rename(columns={'Fatalities_per_capita': 'Fatalities/capita'})\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%Δ ' + name)\n    )\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spatial Regression Model"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"In order to perform feature selection, we opted to perform repeated step-wise forward-backward selection on a non-spatal regresison model. This was mainly due to frustration with the pysal API and prior work on modelling COVID-19 fatalities. This always allows us to more directly compare the effects of controlling for spatial correlation in our model between neighbouring countries. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       n_iter = 1,\n                       threshold_in=0.015, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    for _ in range(n_iter):\n        while True:\n            changed=False\n            # forward step\n            excluded = list(set(X.columns)-set(included))\n            new_pval = pd.Series(index=excluded).sample(frac=1)\n            for new_column in excluded:\n                model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n                new_pval[new_column] = model.pvalues[new_column]\n            best_pval = new_pval.min()\n            if best_pval < threshold_in:\n                best_feature = new_pval.idxmin()\n                included.append(best_feature)\n                changed=True\n                if verbose:\n                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n            # backward step\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n            # use all coefs except intercept\n            pvalues = model.pvalues.iloc[1:]\n            worst_pval = pvalues.max() # null if pvalues is empty\n            if worst_pval > threshold_out:\n                changed=True\n                worst_feature = pvalues.idxmax()\n                included.remove(worst_feature)\n                if verbose:\n                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n            if not changed:\n                break\n    return included\n\nparams_cases = stepwise_selection(X.loc[:, np.random.permutation(X.columns)], y_fatalities,\n                                  n_iter=3,\n                                  threshold_in=0.015, threshold_out=0.025)\n\nmodel_cases = OLS(y_fatalities, X.loc[:, params_cases])\nresults_cases = model_cases.fit()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results_cases.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"From our model below, we see interestingly see strong evidence for spatial correlation in the data when looking at our test for using Lagrange Multiplier (error), \nRobust LM (error) and Lagrange Multiplier (SARMA).  If we again look at the parameter estimates for our model, we see variables across the board significant at the 2% level.  What appears fascinating in our estimates are the positive coefficients on %Δ Net migration and %Δ Arable (%), which may provide policy makes insight into how best to support nations worst exposed to the virus. \n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_prime = X.loc[:,params_cases]\nX_prime = X_prime.loc[:, X_prime.min(0) != X_prime.max(0)]\nw = ps.lib.weights.full2W(distances)\n\nmodel = ps.model.spreg.OLS(y_fatalities.values, X_prime.values, w=w, spat_diag=True, name_x=X_prime.columns.tolist(), name_y=y_fatalities.columns.tolist()[0])\nprint(model.summary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spatial PCR"},{"metadata":{},"cell_type":"markdown","source":"When analyzing Variance Inflation Factor, explored in a previous notebook, I became interested in how our parameter estimates might change if we apply a Principle Component Regression Approach.  Using this approach, we chose to apply a power transform to our continuous features before standard scaling and computing our principle components. The number of principle components were chosen based using the ELBO rule, based on the varianced explained.  I opted to reintroduce the country indicator variables into the model after computing the principle components, in order to avoid conflating the distributional assumptions across our different types of data in scaling and to ease interprettation. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\n\nX_prime_all = X.loc[:, ~X.columns.str.startswith('is_')].drop(columns=['const'])\nX_prime_all = X_prime_all.loc[:, X_prime_all.min(0) != X_prime_all.max(0)]\n\npipeline = make_pipeline(PowerTransformer(), StandardScaler(), PCA(7))\nZ = pd.concat([pd.DataFrame(pipeline.fit_transform(X_prime_all)), X_prime.loc[:, X_prime.columns.str.startswith('is_')]], axis=1)\n\n(hv.Bars(pipeline.named_steps['pca'].explained_variance_ratio_)\n.opts(title='Variance Explained Ratio of Princple Components', ylabel='Variance Explained', xlabel='Component', width=600))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the outputs of the model, it appears spatital correlation in our errors is more apparent, based on our tests. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pcr_model = ps.model.spreg.OLS(y_fatalities.values, Z.values, w=w, spat_diag=True, name_x=Z.columns.tolist(), name_y=y_fatalities.columns.tolist()[0])\nprint(pcr_model.summary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I opted to reproject the coefficients from our Spatial PCR model into the original space of our data in order to analyze them further. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"betas = np.array(pcr_model.betas[1:-Z.columns.astype(str).str.startswith('is').sum()])\ncoef_ = pipeline.named_steps['pca'].inverse_transform(betas.reshape(1,-1))\n(pd.DataFrame(coef_.flatten(), index = X_prime_all.columns, columns=['Reprojected Coefficients'])\n .hvplot.bar().opts(xrotation=90, height=400))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I would love to hear your feedback on this notebook and any suggestions on how I may improve the analysis in anyway by included new data sources or new methodologies.  Please, if you liked this kernel, please give it a vote and check our some of my other intesting kernels on COVID-19 Survival Analysis.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}