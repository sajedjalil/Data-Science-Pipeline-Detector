{"cells":[{"metadata":{},"cell_type":"markdown","source":"Spatial Regression is an approach popular across many disciplines to estimate regression models in the presence of spatial correlation.  For economist investigating the covariates which drive housing prices, controlling for spatial correlation in our models allows us to better control for neighbourhood effects. These neighbourhood effects may be as a result of the willingness of a buyer to pay more for a house near other nice houses or as the results of unobserved variables in the data which capture the distance to schools, shops and restaurants.  \n  \nIn analyzing the spread and impact of the COVID-19 disease, I have been fascinated by whether we observed uncontrolled spatial correlation in infections or fatalities between neighbouring countries.  This may be as the results of trade or migrations between neighbouring countries or as the results of other properties of the weather or geography which are difficult to account for in the model. \n\nAdditionally, as new data has been released to the public on Government interventions, I have been interested about building on some of my previous notebooks to investigate the success of these programmes. \n\nIn this notebook, we are interested in identifying whether controlling for this autocorrelation provides for more stable estimates to our model, as we as whether the physical distances between countries reflect honest notions of similarity between nations which may otherwise be better captured by the total number of flights or migration between countries or trade. "},{"metadata":{},"cell_type":"markdown","source":"# Contents\n1. [Data](#Data)  \n2. [Spatial Regression Model](#Spatial-Regression-Model)  \n3. [Conclusion](#Conclusion)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"! apt install libgeos-dev\n! pip uninstall -y shapely; pip install --no-binary :all: shapely==1.6.4\n! pip uninstall -y cartopy; pip install --no-binary :all: cartopy==0.17.0\n! conda install -y geoviews=1.6.6 hvplot=0.5.2 bokeh==1.4.0 pysal","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nfrom operator import add, mul\nfrom time import time\nfrom functools import wraps\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport hvplot.pandas\nimport holoviews as hv\nimport cartopy.crs as ccrs\nimport geopandas as gpd\nfrom toolz.curried import map, partial, pipe, reduce\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport pysal as ps\nfrom sklearn.manifold import MDS, smacof\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Any results you write to the current directory are saved as output.\nhv.extension('bokeh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"I chose to draw on a number of data sources on not only COVID cases but country indicators on GDP, infant mortality, etc., as well as data on population estimates and land size.  In order to better control for the variance in Fatalities and Confirmed Cases as a result of country sizes, I opted to look at Fatalities and Confirmed Cases per Capita.  As, at this stage in the virus, the pandemic is still dominated by the exponential growth in new cases, I opted to analyze the relationship between the percent change in Fatalities or Confirmed Cases per Capita, against percent changes in our factors.  Two interesting exceptions to this were variables representing the weeks since the first case in the country and the first case death, where I included both the log of the weeks since this event, to represent percent change, and the original value. This is used to model any effects relating to the logarithmic flattening of the curve late in the infection in a given country.  \n\nWe have included in this analysis data using [Oxford University's Government Reponse Tracker Data](https://www.bsg.ox.ac.uk/research/research-projects/oxford-covid-19-government-response-tracker), as well as the [WHO 2000 World Health Report](https://www.who.int/whr/2000/en/). The WHO Report data is 20 years old, but does appear a reliable source, when contrasted with other data sources found. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def timing(f):\n    @wraps(f)\n    def wrap(*args, **kw):\n        ts = time()\n        result = f(*args, **kw)\n        te = time()\n        print('func:%r took: %2.4f sec' % \\\n          (f.__name__, te-ts))\n        return result\n    return wrap\n\ndef shape(f, outputs=True, inputs=True):\n    @wraps(f)\n    def wrap(*args, **kw):\n        ts = time()\n        result = f(*args, **kw)\n        te = time()\n        \n        if inputs:\n            print('func:%r input shape:%r' % \\\n              (f.__name__, args[0].shape))\n            \n        if outputs:\n            print('func:%r output shape:%r' % \\\n              (f.__name__, result.shape))\n        return result\n    return wrap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oxford = (pd.read_csv('/kaggle/input/oxford-covid19-government-response-tracker/OxCGRT_Download_120420_170601_Full.csv', parse_dates=['Date'])\n         .fillna(0)\n         .drop(columns=['CountryCode', 'StringencyIndexForDisplay', 'Unnamed: 39', 'ConfirmedCases', 'ConfirmedDeaths']))\nis_gen = ~oxford.columns.str.endswith('_IsGeneral') & ~oxford.columns.str.endswith('_Notes')\noxford = (oxford\n          .loc[:, is_gen]\n          .set_index(['CountryName','Date'])\n          .rename(columns=lambda s: 'Ox'+s)\n          .groupby('CountryName')\n          .cumsum()\n          .reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oxford","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"countries = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres')).replace('United States of America', 'US')\ncovid = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv', parse_dates=['Date'], index_col='Id')\nindicators = pd.read_csv('/kaggle/input/countries-of-the-world/countries of the world.csv', decimal=',').replace('United States', 'US')\nwho_scores = pd.read_csv('/kaggle/input/who-2000-health-index/WHO 2000 Health Index.csv')\n\ncountry_indicators = (countries.assign(name = lambda df: df.name.astype(str).str.strip())\n                     .merge(indicators.assign(Country = lambda df: df.Country.astype(str).str.strip()), \n                            left_on = 'name', right_on='Country', how='inner'))\nweeks = (covid\n         .assign(dayofweek = lambda df: df.Date.dt.dayofweek)\n         .merge(oxford, left_on=['Country_Region', 'Date'], right_on=['CountryName','Date'])\n         .set_index('Date')\n         .drop(columns=['Province_State'])\n         .groupby(['Country_Region', pd.Grouper(freq='W')]).agg({'ConfirmedCases':'sum',\n                                                                 'Fatalities':'sum',\n                                                                 'dayofweek':'max',\n                                                                 'OxS1_School closing':'mean',\n                                                                'OxS2_Workplace closing':'mean',\n                                                                'OxS3_Cancel public events':'mean',\n                                                                'OxS4_Close public transport':'mean',\n                                                                'OxS5_Public information campaigns':'mean',\n                                                                'OxS6_Restrictions on internal movement':'mean',\n                                                                'OxS7_International travel controls':'mean',\n                                                                'OxS8_Fiscal measures':'mean',\n                                                                'OxS9_Monetary measures':'mean',\n                                                                'OxS10_Emergency investment in health care':'mean',\n                                                                'OxS11_Investment in Vaccines':'mean',\n                                                                'OxS12_Testing framework':'mean',\n                                                                'OxS13_Contact tracing':'mean',\n                                                                'OxStringencyIndex':'mean'})\n         .reset_index()\n         .where(lambda df: df.ConfirmedCases > 0)\n         .dropna(0)\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .sort_values('Date')\n                            .assign(week_of_infection = lambda df: pd.np.arange(df.shape[0]))))\n         .where(lambda df: df.dayofweek >= 6)\n         .drop(columns=['dayofweek'])\n         .dropna(0)\n         .reset_index(drop=True)\n         .merge(country_indicators, left_on='Country_Region', right_on='name', how='inner')\n         .pipe(lambda df: gpd.GeoDataFrame(df, geometry='geometry'))\n         .assign(ConfirmedCases_per_capita = lambda df: (df.ConfirmedCases / df.pop_est),\n                 Fatalities_per_capita= lambda df: (df.Fatalities / df.pop_est),\n                 land_area = lambda df: df.area.astype('float'),\n                 week_of_infection_exp = lambda df: df.week_of_infection.apply(np.exp))\n         .groupby('Country_Region')\n         .apply(lambda df: (df\n                            .assign(week_since_first_death = lambda x: (x.week_of_infection - x.where(lambda y: y.Fatalities > 0)\n                                                                        .week_of_infection.min())\n                                                                        .clip(lower=0)\n                                                                        .fillna(0))))\n         .assign(week_since_first_death_exp = lambda df: df.week_since_first_death.apply(np.exp))\n         .drop(columns = 'gdp_md_est')\n         .merge(who_scores.assign(Country = lambda df: df.Country.str.normalize(\"NFKD\").str.strip()), \n                left_on='name', right_on='Country', how='left'))\nweeks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to model the spatial correlation between countries, we critically need to find an approach to compute the distances between these nations.  For simplicity, we have opted to fall back on geopandas default projection and use the euclidean distances between the midpoints of the countries in order to esimate their distances.  For sparse countries, it may be more faithful to use the shortest distance between borders or between capital cities, but this appeared computationally intensive and may be grounds for future work.  "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"points = (weeks.geometry\n          .representative_point()\n          .apply(lambda df: pd.Series([df.x, df.y]))\n          .rename(columns={0:'x',1:'y'}))\nhv.Labels(points.assign(names = weeks.name).drop_duplicates(), kdims=['x','y'], vdims='names').opts(height=500, width=800, title='Midpoints of Countries')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"distances = euclidean_distances(points)\ndistances = np.sin(np.pi * distances / distances.max().max())\nD = pd.DataFrame(distances, index = weeks.name, columns = weeks.name).drop_duplicates().T.drop_duplicates()\n\nZ, score = smacof(D.to_numpy(), n_components=2)\n\n(hv.Labels(pd.DataFrame(Z, columns=['x','y'])\n           .assign(names = D.index)\n           .drop_duplicates(), kdims=['x','y'], vdims='names').opts(height=500, width=800))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To construct our design matrix for our experiment, we opted to include all our numeric columns and, to account for country-specific effects, we opted to include dummy variables for countries.  This design matrix is reused in both models, to estimate covariates for %Δ Fatalities/capita.  The reason we opted to model %Δ Fatalities/capita and not %Δ Confirmed Cases/capita, is due to problems of testing which appears highly correlated with many indicators.  "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"X, y_fatalities = (weeks\n     .select_dtypes(include=['number'])\n     .drop(columns=['ConfirmedCases', 'Fatalities', 'ConfirmedCases_per_capita', 'Fatalities_per_capita'])\n     .replace(0, 1e-8)# add jitter\n     .transform(np.log)\n     .pipe(lambda df: df.fillna(df.mean()))\n     .rename(columns = lambda name: '%Δ ' + name)\n     .rename(columns = {'%Δ week_of_infection_exp': 'week_of_infection'})\n     .rename(columns = {'%Δ week_since_first_death_exp': 'week_since_first_death'})\n     .pipe(lambda df: pd.concat([df, pd.get_dummies(weeks.name, drop_first=True).rename(columns =lambda s: 'is_'+s)], axis=1))\n     .assign(const = 1),\n        \n    weeks\n    .loc[:, ['Fatalities_per_capita']]\n    .rename(columns={'Fatalities_per_capita': 'Fatalities/capita'})\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%Δ ' + name)\n    )\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A major challenge in working with Government Response data, is that the response of government is endogenously determined by the load of the public healthcare system. This makes that that our estimates will present that increase in government intervention are positively corrrelated with deaths. In order to deal with this phenomenon, I opted to orthogonalize the Government Response data with respect to the number of Confirmed Cases per Capital and %Δ Confirmed Cases per Capital. After performing this operation with still need to deal with the fact that our Government Response data is highly correlated, as governments tend to take actions simultaneously. To manage this challenge, I opted to use Confirmatory Factor Analysis to compute a Government Response Factor Latent Variable for use in my model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cases =(weeks\n    .loc[:, ['ConfirmedCases_per_capita']]\n    .rename(columns={'ConfirmedCases_per_capita': 'Cases/capita'})\n         )\n\nlog_y_cases = (y_cases\n    .replace(0, 1e-8)# add jitter\n    .transform(np.log)  \n    .rename(columns = lambda name: '%Δ ' + name))\n\nv = np.c_[y_cases.to_numpy(), log_y_cases.to_numpy()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"I = X.loc[:, X.columns.str.startswith('%Δ OxS')]\niv_names = [s + ' IV' for s in I.columns]\nI = I.to_numpy()\nIV = pd.DataFrame(I - v @ (np.linalg.pinv(v.T @ v) @ v.T @ I), columns=iv_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import FactorAnalysis\ngovernment_response = pd.DataFrame(FactorAnalysis(1).fit_transform(IV), columns=['Government Response Factor'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([X.loc[:, ~X.columns.str.startswith('%Δ OxS')], government_response], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spatial Regression Model"},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"In order to perform feature selection, we opted to perform repeated step-wise forward-backward selection on a non-spatal regresison model. This was mainly due to frustration with the pysal API and prior work on modelling COVID-19 fatalities. This always allows us to more directly compare the effects of controlling for spatial correlation in our model between neighbouring countries. "},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       n_iter = 1,\n                       threshold_in=0.015, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    for _ in range(n_iter):\n        while True:\n            changed=False\n            # forward step\n            excluded = list(set(X.columns)-set(included))\n            new_pval = pd.Series(index=excluded).sample(frac=1)\n            for new_column in excluded:\n                model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n                new_pval[new_column] = model.pvalues[new_column]\n            best_pval = new_pval.min()\n            if best_pval < threshold_in:\n                best_feature = new_pval.idxmin()\n                included.append(best_feature)\n                changed=True\n                if verbose:\n                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n            # backward step\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n            # use all coefs except intercept\n            pvalues = model.pvalues.iloc[1:]\n            worst_pval = pvalues.max() # null if pvalues is empty\n            if worst_pval > threshold_out:\n                changed=True\n                worst_feature = pvalues.idxmax()\n                included.remove(worst_feature)\n                if verbose:\n                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n            if not changed:\n                break\n    return included\n\nparams_cases = stepwise_selection(X.loc[:, np.random.permutation(X.columns)], y_fatalities,\n                                  n_iter=3,\n                                  threshold_in=0.015, threshold_out=0.025)\n\nmodel_cases = OLS(y_fatalities, X.loc[:, params_cases])\nresults_cases = model_cases.fit()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results_cases.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{},"cell_type":"markdown","source":"From our model below, we see interestingly see strong evidence for spatial correlation in the data when looking at our test for using Lagrange Multiplier (error), \nRobust LM (error) and Lagrange Multiplier (SARMA).  If we again look at the parameter estimates for our model, we see variables across the board significant at the 5% level.  What appears fascinating in our estimates are the positive coefficients on %Δ Arable (%), which may provide policy makes insight into how best to support nations worst exposed to the virus. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_prime = X.loc[:,params_cases]\nX_prime = X_prime.loc[:, X_prime.min(0) != X_prime.max(0)]\nw = ps.lib.weights.full2W(distances)\n\nmodel = ps.model.spreg.OLS(y_fatalities.values, X_prime.values, w=w, spat_diag=True, name_x=X_prime.columns.tolist(), name_y=y_fatalities.columns.tolist()[0])\nprint(model.summary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(model.betas[1:].flatten(), index=X_prime.columns.tolist(), name='Coefficients: Relationship with %Δ COVID-19 Fatalities').hvplot.bar().opts(xrotation=75, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spatial PCR"},{"metadata":{},"cell_type":"markdown","source":"When analyzing Variance Inflation Factor, explored in a previous notebook, I became interested in how our parameter estimates might change if we apply a Principle Component Regression Approach.  Using this approach, we chose to apply a power transform to our continuous features before standard scaling and computing our principle components. The number of principle components were chosen based using the ELBO rule, based on the varianced explained.  I opted to reintroduce the country indicator variables into the model after computing the principle components, in order to avoid conflating the distributional assumptions across our different types of data in scaling and to ease interprettation. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\n\nX_prime_all = pd.concat([X\n                         .loc[:, ~X.columns.str.startswith('is_')]\n                         .drop(columns=['const'])\n                         .drop(columns=['Government Response Factor']),\n                        IV], axis=1)\nX_prime_all = X_prime_all.loc[:, X_prime_all.min(0) != X_prime_all.max(0)]\n\npipeline = make_pipeline(PowerTransformer(), StandardScaler(), PCA(7))\nZ = pd.concat([pd.DataFrame(pipeline.fit_transform(X_prime_all)), X_prime.loc[:, X_prime.columns.str.startswith('is_')]], axis=1)\n\n(hv.Bars(pipeline.named_steps['pca'].explained_variance_ratio_)\n.opts(title='Variance Explained Ratio of Princple Components', ylabel='Variance Explained', xlabel='Component', width=600))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the outputs of the model, it appears spatital correlation in our errors is more apparent, based on our tests. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"pcr_model = ps.model.spreg.OLS(y_fatalities.values, Z.values, w=w, spat_diag=True, name_x=Z.columns.tolist(), name_y=y_fatalities.columns.tolist()[0])\nprint(pcr_model.summary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I opted to reproject the coefficients from our Spatial PCR model into the original space of our data in order to analyze them further. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"betas = np.array(pcr_model.betas[1:-Z.columns.astype(str).str.startswith('is').sum()])\ncoef_ = pipeline.named_steps['pca'].inverse_transform(betas.reshape(1,-1))\n(pd.DataFrame(coef_.flatten(), index = X_prime_all.columns, columns=['Reprojected Coefficients'])\n .hvplot.bar().opts(xrotation=90, height=400))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I would love to hear your feedback on this notebook and any suggestions on how I may improve the analysis in anyway by included new data sources or new methodologies.  Please, if you liked this kernel, please give it a vote and check our some of my other intesting kernels on COVID-19 Survival Analysis.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}