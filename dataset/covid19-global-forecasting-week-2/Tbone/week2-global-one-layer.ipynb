{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,roc_auc_score\nfrom google.cloud import bigquery\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom datetime import date\nfrom datetime import timedelta\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MyLabelEncode(coltr,colte):\n    levels=coltr.unique().tolist()\n    for l in levels:\n        if l is np.nan:\n            levels.remove(np.nan)\n    levelmap={e:i for i,e in enumerate(levels)}\n    return coltr.map(levelmap),colte.map(levelmap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MyLabelEncodeSingle(col):\n    levels=col.unique().tolist()\n    for l in levels:\n        if l is np.nan:\n            levels.remove(np.nan)\n    levelmap={e:i for i,e in enumerate(levels)}\n    return col.map(levelmap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/covid19-global-forecasting-week-2/train.csv\")\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-2/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop([\"Fatalities\", \"ConfirmedCases\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = X_train[\"Country_Region\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop([\"Id\"], axis=1)\nX_test = test.drop([\"ForecastId\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Province_State']=X_train['Province_State'].fillna('Unknown')\nX_test['Province_State']=X_test['Province_State'].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['UpToNow']=(pd.to_datetime(date.today())-pd.to_datetime(X_train['Date'])).dt.days.astype(float)\nX_test['UpToNow']=(pd.to_datetime(date.today())-pd.to_datetime(X_test['Date'])).dt.days.astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Date']= pd.to_datetime(X_train['Date']) \nX_test['Date']= pd.to_datetime(X_test['Date']) \n#X_train = X_train.set_index(['Date'])\n#X_test = X_test.set_index(['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_time_features(df):\n    \"\"\"\n    Creates time series features from datetime index\n    \"\"\"\n    #df['date'] = df.index\n    df['hour'] = df['Date'].dt.hour\n    df['dayofweek'] = df['Date'].dt.dayofweek\n    df['quarter'] = df['Date'].dt.quarter\n    df['month'] = df['Date'].dt.month\n    df['year'] = df['Date'].dt.year\n    df['dayofyear'] = df['Date'].dt.dayofyear\n    df['dayofmonth'] = df['Date'].dt.day\n    df['weekofyear'] = df['Date'].dt.weekofyear\n    \n    X = df[['hour','dayofweek','quarter','month','year',\n           'dayofyear','dayofmonth','weekofyear']]\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_time_features(X_train)\ncreate_time_features(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_index = pd.read_csv(\"../input/world-bank-datasets/World_Happiness_Index.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_index=world_happiness_index.iloc[:,:19]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_index.columns=[c.replace('(','').replace(')','').replace('(','').replace(',','').replace('-','_').replace('/','_').replace(' ','_') \n                               for c in world_happiness_index.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_year={}\ntemp_matrix=world_happiness_index.iloc[:,2:]\nfor y in world_happiness_index.Year.unique():\n    average_year[y]=temp_matrix.loc[world_happiness_index.Year==y,:].mean()\ndel temp_matrix\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distance=0\nwhile world_happiness_index.isna().sum().sum()!=0:\n    for y in world_happiness_index.Year.unique():\n        yhat=y-distance\n        if yhat>2018:\n            yhat=2018\n        elif yhat<2005:\n            yhat=2005\n        for c in world_happiness_index.columns[2:]:\n            world_happiness_index.loc[world_happiness_index.Year==y,c]=world_happiness_index.loc[world_happiness_index.Year==y,c].fillna(average_year[yhat][c])\n        yhat=y+distance\n        if yhat>2018:\n            yhat=2018\n        elif yhat<2005:\n            yhat=2005\n        for c in world_happiness_index.columns[2:]:\n            world_happiness_index.loc[world_happiness_index.Year==y,c]=world_happiness_index.loc[world_happiness_index.Year==y,c].fillna(average_year[yhat][c])\n        distance += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_latest = world_happiness_index.groupby('Country_name').nth(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_first = world_happiness_index.groupby('Country_name').agg('first')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_last = world_happiness_index.groupby('Country_name').agg('last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_count = world_happiness_index.groupby('Country_name').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_range=(world_happiness_last-world_happiness_first)/world_happiness_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"world_happiness_range.drop(\"Year\", axis=1, inplace=True)\nworld_happiness_latest.drop(\"Year\", axis=1, inplace=True)\nworld_happiness_range.columns=[c+'_range' for c in world_happiness_range.columns]\nworld_happiness_latest.columns=[c+'_latest' for c in world_happiness_latest.columns]\nworld_happiness_grouped=pd.concat((world_happiness_latest,world_happiness_range),axis=1).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')\nX_test = pd.merge(left=X_test, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"Country_name\", axis=1, inplace=True)\nX_test.drop(\"Country_name\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"malaria_world_health = pd.read_csv(\"../input/world-bank-datasets/Malaria_World_Health_Organization.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"malaria_world_health.columns=[c.replace(' ','_') for c in malaria_world_health.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"Country\", axis=1, inplace=True)\nX_test.drop(\"Country\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"human_development_index = pd.read_csv(\"../input/world-bank-datasets/Human_Development_Index.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"human_development_index.columns=[c.replace(')','').replace('(','').replace(' ','_') for c in human_development_index.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(\"Country\", axis=1, inplace=True)\nX_test.drop(\"Country\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"night_ranger_predictors = pd.read_csv(\"../input/covid19-demographic-predictors/covid19_by_country.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"night_ranger_predictors.columns=[c.replace(' ','_') for c in night_ranger_predictors.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There is a duplicate for Georgia in this dataset from Night Ranger, causing merge issues so we will just drop the Georgia rows\nnight_ranger_predictors = night_ranger_predictors[night_ranger_predictors.Country != \"Georgia\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"night_ranger_predictors=night_ranger_predictors[['Country','Median_Age','GDP_2018','Crime_Index','Population_2020','Smoking_2016','Females_2018']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=night_ranger_predictors, how='left', left_on='Country_Region', right_on='Country')\nX_test = pd.merge(left=X_test, right=night_ranger_predictors, how='left', left_on='Country_Region', right_on='Country')\nX_train.drop(\"Country\", axis=1, inplace=True)\nX_test.drop(\"Country\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Gross_national_income_GNI_per_capita_2018']= X_train['Gross_national_income_GNI_per_capita_2018'].apply(lambda x: x if x!=x else x.replace(',','')).astype(float)\nX_test['Gross_national_income_GNI_per_capita_2018']= X_test['Gross_national_income_GNI_per_capita_2018'].apply(lambda x: x if x!=x else x.replace(',','')).astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df = pd.read_csv(\"../input/covid19formattedweatherjan22march24/covid_dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df=weather_df[['Province/State',\n'Country/Region',\n'lat',\n'long',\n'day',\n'pop',\n'urbanpop',\n'density',\n'medianage',\n'smokers',\n'health_exp_pc',\n'hospibed',\n'temperature',\n'humidity']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df['Province/State']=weather_df['Province/State'].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df['day']=pd.to_datetime('2020-01-22')+weather_df['day'].apply(lambda x: timedelta(days=x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df['month']=weather_df['day'].dt.month\nweather_df.drop('day',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df=weather_df.groupby(['Province/State','Country/Region','month']).mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df=weather_df.replace(-999,np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df_latest = weather_df.groupby(['Province/State','Country/Region']).nth(-1).reset_index()\nweather_df_latest['month']=4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_df=pd.concat((weather_df,weather_df_latest),sort=True,axis=0,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country/Region','Province/State','month'])\nX_test = pd.merge(left=X_test, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country/Region','Province/State','month'])\nX_train.drop(['Country/Region','Province/State'], axis=1, inplace=True)\nX_test.drop(['Country/Region','Province/State'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_df = pd.read_csv(\"../input/world-happiness-report-2020/WHR20_DataForFigure2.1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_df.columns=[c.replace(':','').replace('+','').replace(' ','_') for c in happiness_df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness_df['Regional_indicator']=MyLabelEncodeSingle(happiness_df['Regional_indicator'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\nX_test = pd.merge(left=X_test, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\nX_train.drop('Country_name', axis=1, inplace=True)\nX_test.drop('Country_name', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_df = pd.read_csv(\"../input/world-population-by-age-group-2020/WorldPopulationByAge2020.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_df['AgeGrp']=MyLabelEncodeSingle(age_df['AgeGrp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def processAge(df):\n    ageindex=df['AgeGrp']\n    sexsum=df[['PopMale', 'PopFemale', 'PopTotal']].sum()\n    mp=sexsum['PopMale']/sexsum['PopTotal']\n    fp=sexsum['PopFemale']/sexsum['PopTotal']\n    p0=df.loc[ageindex==0,'PopTotal'].values[0]/sexsum['PopTotal']\n    p1=df.loc[ageindex==1,'PopTotal'].values[0]/sexsum['PopTotal']\n    p2=df.loc[ageindex==2,'PopTotal'].values[0]/sexsum['PopTotal']\n    p3=df.loc[ageindex==3,'PopTotal'].values[0]/sexsum['PopTotal']\n    m0=df.loc[ageindex==0,'PopMale'].values[0]/sexsum['PopMale']\n    m1=df.loc[ageindex==1,'PopMale'].values[0]/sexsum['PopMale']\n    m2=df.loc[ageindex==2,'PopMale'].values[0]/sexsum['PopMale']\n    m3=df.loc[ageindex==3,'PopMale'].values[0]/sexsum['PopMale']\n    f0=df.loc[ageindex==0,'PopFemale'].values[0]/sexsum['PopFemale']\n    f1=df.loc[ageindex==1,'PopFemale'].values[0]/sexsum['PopFemale']\n    f2=df.loc[ageindex==2,'PopFemale'].values[0]/sexsum['PopFemale']\n    f3=df.loc[ageindex==3,'PopFemale'].values[0]/sexsum['PopFemale']\n    return pd.DataFrame({'MaleP':mp,'MaleP_0':m0,'MaleP_1':m1,'MaleP_2':m2,'MaleP_3':m3,'FemaleP':fp,\n                         'FemaleP_0':f0,'FemaleP_1':f1,'FemaleP_2':f2,'FemaleP_3':f3,'PopTotal':sexsum['PopTotal'],\n                         'Pop_0':p0,'Pop_1':p1,'Pop_2':p2,'Pop_3':p3},index=[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age_df=age_df.groupby('Location').apply(processAge).reset_index().drop('level_1',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=age_df, how='left', left_on='Country_Region', right_on='Location')\nX_test = pd.merge(left=X_test, right=age_df, how='left', left_on='Country_Region', right_on='Location')\nX_train.drop('Location', axis=1, inplace=True)\nX_test.drop('Location', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"healthsys_df = pd.read_csv(\"../input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv\")\nhealthsys_df.columns=[c.replace('-','_') for c in healthsys_df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"healthsys_df.drop('World_Bank_Name',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_country=healthsys_df[['Country_Region', 'Province_State']].isna().all(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"healthsys_df=healthsys_df.loc[nan_country==False,:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"healthsys_df['Province_State']=healthsys_df['Province_State'].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=healthsys_df, how='left', left_on=['Country_Region','Province_State'], right_on=['Country_Region', 'Province_State'])\nX_test = pd.merge(left=X_test, right=healthsys_df, how='left', left_on=['Country_Region','Province_State'], right_on=['Country_Region', 'Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_df = pd.read_csv(\"../input/population-by-country-2020/population_by_country_2020.csv\")\npop_df.columns=[c.replace('.',' ').split(' ')[0]+'_pop2020' for c in pop_df.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_col=['Yearly_pop2020','Urban_pop2020', 'World_pop2020']\ndef depercent(x):\n    if x=='N.A.':\n        return np.nan \n    else:\n        return float(x.replace('%',''))\nfor c in percent_col:\n    pop_df[c]=pop_df[c].apply(lambda x: depercent(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_df=pop_df.replace('N.A.',np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pop_df[['Population_pop2020', 'Yearly_pop2020',\n       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']]=pop_df[['Population_pop2020', 'Yearly_pop2020',\n       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\nX_test = pd.merge(left=X_test, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\nX_train.drop('Country_pop2020', axis=1, inplace=True)\nX_test.drop('Country_pop2020', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compre_df = pd.read_csv(\"../input/countryinfo/covid19countryinfo.csv\")\n#testcase_df = pd.read_csv(\"../input/countryinfo/covid19tests.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compre_df['region']=compre_df['region'].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keepcol=['region', 'country', 'tests',\n       'testpop', 'density', 'medianage', 'urbanpop', 'quarantine', 'schools',\n       'publicplace', 'gatheringlimit', 'gathering', 'nonessential',\n       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung', 'gdp2019',\n       'healthexp', 'healthperpop', 'fertility', 'firstcase']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tempfun(x):\n    if x is np.nan:\n        return x\n    else:\n        return float(x.replace(',',''))\nfor c in ['gdp2019','healthexp']:\n    compre_df[c]=compre_df[c].apply(lambda x: tempfun(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"todate_col=['quarantine', 'schools','publicplace', 'gathering', 'nonessential','firstcase']\nfor c in todate_col:\n    compre_df[c]= (pd.to_datetime(date.today())-pd.to_datetime(compre_df[c])).dt.days.astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compre_df=compre_df[keepcol]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.merge(left=X_train, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\nX_test = pd.merge(left=X_test, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\nX_train.drop(['country','region'], axis=1, inplace=True)\nX_test.drop(['country','region'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encode the Province/State and the Country/Region columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['Country_Region'],X_test['Country_Region']=MyLabelEncode(X_train['Country_Region'],X_test['Country_Region'])\nX_train['Province_State'],X_test['Province_State']=MyLabelEncode(X_train['Province_State'],X_test['Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f_cat=['Country_Region','Province_State','Regional_indicator']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_F = train[\"Fatalities\"].reset_index(drop=True)\ntarget_C = train[\"ConfirmedCases\"].reset_index(drop=True)\n#X_train = X_train.reset_index(drop=True)\n#X_test = X_test.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.drop(['Date','year'], axis=1, inplace=True)\nX_test.drop(['Date','year'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Only use data prior to 2020-03-19 for predictions on the public leaderboard period. Use up to and including the most recent data for predictions on the private leaderboard period.\n\n* Public Leaderboard Period - 2020-03-19 - 2020-04-01\n* Private Leaderboard Period - 2020-04-02 - 2020-04-30"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_public_index=pd.DataFrame({'month':X_train['month']<=3,'day':X_train['dayofmonth']<19})\ntrain_public_index=train_public_index.all(axis=1)\nX_train_public=X_train.loc[train_public_index,:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_C_public=target_C[train_public_index].reset_index(drop=True)\ntarget_F_public=target_F[train_public_index].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_public_index=pd.DataFrame({'month':X_test['month']<=4,'day':X_test['dayofmonth']<=1})\ntest_public_index=test_public_index.all(axis=1)\ntest_private_index=~test_public_index\nX_test_public=X_test.loc[test_public_index,:].reset_index(drop=True)\nX_test_private=X_test.loc[test_private_index,:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_private.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"usedfeatures=X_train.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RMSLE(t1,p1):\n    return np.sqrt(np.mean((np.log(t1+1)-np.log(p1+1))**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country_col=X_train[f_cat[0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"int(np.nan)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(target_F_public)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=1990)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####################C+public\nparams = {#1\n        'learning_rate': 0.5,\n        'feature_fraction': 1,\n        'min_data_in_leaf' : 150,\n        'max_depth': 6,\n#        'max_bin':300,\n        'reg_alpha': 10,#l1\n#        'reg_lambda': 10,#l2\n        'num_leaves':15,\n        'objective': 'regression',\n        'metric': 'rmse',\n        'n_jobs': -1,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n#        'bagging_freq':5,\n#        'pos_bagging_fraction':0.8,\n#        'neg_bagging_fraction':0.8,\n        'boost_from_average': False}\ntraintion_public_c = np.zeros(len(X_train_public))\nvalidation_public_c = np.zeros(len(X_train_public))\npredictions_public_c = np.zeros(len(X_test_public))\nfeature_importance_df_public_c = pd.DataFrame()\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_public,target_C_public)):\n    print(\"fold n째{}\".format(fold_))\n    train_x=X_train_public.iloc[trn_idx][usedfeatures].reset_index(drop=True)\n    valid_x=X_train_public.iloc[val_idx][usedfeatures].reset_index(drop=True)\n    target_train=target_C_public.iloc[trn_idx].reset_index(drop=True)\n    target_valid=target_C_public.iloc[val_idx].reset_index(drop=True)\n    trn_data = lgb.Dataset(train_x,\n                           label=target_train,\n                           categorical_feature=f_cat\n                          )\n    val_data = lgb.Dataset(valid_x,\n                           label=target_valid,\n                           categorical_feature=f_cat\n                          )\n\n    num_round = 1000000\n    clf = lgb.train(params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=250,\n                    early_stopping_rounds = 150)\n    traintion_public_c[trn_idx] += clf.predict(train_x, num_iteration=clf.best_iteration)/(folds.n_splits-1)\n    validation_public_c[val_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = usedfeatures\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df_public_c = pd.concat([feature_importance_df_public_c, fold_importance_df], axis=0)\n    \n    predictions_public_c += clf.predict(X_test_public, num_iteration=clf.best_iteration) / folds.n_splits\nprint(\"==========C+Pub==============\")\nprint(\"Train RMSLE score: {:<8.5f}\".format(RMSLE(target_C_public,traintion_public_c)))\nprint(\"Valid RMSLE score: {:<8.5f}\".format(RMSLE(target_C_public,validation_public_c)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'learning_rate': 0.5,'feature_fraction': 0.8,'min_data_in_leaf' : 150,'max_depth': 6,#'max_bin':300,#'reg_alpha': 0.01,#'reg_lambda': 0.1,'num_leaves':10    \nTrain RMSLE score: 1.19481   \nValid RMSLE score: 2.86812   \n* 'learning_rate': 0.5,'feature_fraction': 1,'min_data_in_leaf' : 150,'max_depth': 6,#'max_bin':300,#'reg_alpha': 0.01,#'reg_lambda': 0.1,'num_leaves':10    \nTrain RMSLE score: 1.00885   \nValid RMSLE score: 2.75284   \n* 'learning_rate': 0.5,'feature_fraction': 1,'min_data_in_leaf' : 150,'max_depth': 6,#'max_bin':300,#'reg_alpha': 0.01,#'reg_lambda': 0.1,'num_leaves':20    \nTrain RMSLE score: 0.89804    \nValid RMSLE score: 2.68982    \n* 'learning_rate': 0.5,'feature_fraction': 1,'min_data_in_leaf' : 150,'max_depth': 6,#'max_bin':300,#'reg_alpha': 0.01,#'reg_lambda': 0.1,'num_leaves':15   \nTrain RMSLE score: 0.90553   \nValid RMSLE score: 2.68499  \n* 'learning_rate': 0.5,'feature_fraction': 1,'min_data_in_leaf' : 150,'max_depth': 6,#'max_bin':300,'reg_alpha': 10,#'reg_lambda': 0.1,'num_leaves':15   \nTrain RMSLE score: 0.86491    \nValid RMSLE score: 2.67276    "},{"metadata":{"trusted":true},"cell_type":"code","source":"####################C+private\nparams = {#1\n        'learning_rate': 0.1,\n        'feature_fraction': 1,\n        'min_data_in_leaf' : 1,\n        'max_depth': 8,\n       'max_bin':350,\n#        'reg_alpha': 0.05,#l1\n#        'reg_lambda': 0.05,#l2\n        'num_leaves':50,\n        'objective': 'regression',\n        'metric': 'rmse',\n        'n_jobs': -1,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n#        'bagging_freq':5,\n#        'pos_bagging_fraction':0.8,\n#        'neg_bagging_fraction':0.8,\n        'boost_from_average': False}\ntraintion_private_c = np.zeros(len(X_train))\nvalidation_private_c = np.zeros(len(X_train))\npredictions_private_c = np.zeros(len(X_test_private))\nfeature_importance_df_private_c = pd.DataFrame()\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train,target_C)):\n    print(\"fold n째{}\".format(fold_))\n    train_x=X_train.iloc[trn_idx][usedfeatures].reset_index(drop=True)\n    valid_x=X_train.iloc[val_idx][usedfeatures].reset_index(drop=True)\n    target_train=target_C.iloc[trn_idx].reset_index(drop=True)\n    target_valid=target_C.iloc[val_idx].reset_index(drop=True)\n    trn_data = lgb.Dataset(train_x,\n                           label=target_train,\n                           categorical_feature=f_cat\n                          )\n    val_data = lgb.Dataset(valid_x,\n                           label=target_valid,\n                           categorical_feature=f_cat\n                          )\n\n    num_round = 1000000\n    clf = lgb.train(params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=250,\n                    early_stopping_rounds = 200)\n    traintion_private_c[trn_idx] += clf.predict(train_x, num_iteration=clf.best_iteration)/(folds.n_splits-1)\n    validation_private_c[val_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = usedfeatures\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df_private_c = pd.concat([feature_importance_df_private_c, fold_importance_df], axis=0)\n    \n    predictions_private_c += clf.predict(X_test_private, num_iteration=clf.best_iteration) / folds.n_splits\nprint(\"==========C+Pri==============\")\nprint(\"Train RMSLE score: {:<8.5f}\".format(RMSLE(target_C,traintion_private_c)))\nprint(\"Valid RMSLE score: {:<8.5f}\".format(RMSLE(target_C,validation_private_c)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'learning_rate': 0.1,'feature_fraction': 1,'min_data_in_leaf' : 1,'max_depth': 8,# 'max_bin':200,#'reg_alpha': 0.01,#'reg_lambda': 1,'num_leaves':50, 'stop':200   \nTrain RMSLE score: 0.56486       \nValid RMSLE score: 0.79599           \n* 'learning_rate': 0.1,'feature_fraction': 1,'min_data_in_leaf' : 1,'max_depth': 8,'max_bin':300,#'reg_alpha': 0.01,#'reg_lambda': 1,'num_leaves':50, 'stop':200   \nTrain RMSLE score: 0.53879   \nValid RMSLE score: 0.79119    \n* 'learning_rate': 0.1,'feature_fraction': 1,'min_data_in_leaf' : 1,'max_depth': 8,'max_bin':350,#'reg_alpha': 0.01,#'reg_lambda': 1,'num_leaves':50, 'stop':200   \nTrain RMSLE score: 0.39589    \nValid RMSLE score: 0.72666    "},{"metadata":{"trusted":true},"cell_type":"code","source":"####################F+public\nparams = {#1\n        'learning_rate': 0.1,\n        'feature_fraction': 0.8,\n        'min_data_in_leaf' : 100,\n        'max_depth': 7,\n#        'max_bin':200,\n        'reg_alpha': 5,#l1\n#        'reg_lambda': 0.5,#l2\n        'objective': 'regression',\n        'num_leaves':30,\n        'metric': 'rmse',\n        'n_jobs': -1,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n#        'bagging_freq':5,\n#        'pos_bagging_fraction':0.8,\n#        'neg_bagging_fraction':0.8,\n        'boost_from_average': False}\ntraintion_public_f = np.zeros(len(X_train_public))\nvalidation_public_f = np.zeros(len(X_train_public))\npredictions_public_f = np.zeros(len(X_test_public))\nfeature_importance_df_public_f = pd.DataFrame()\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train_public,target_F_public)):\n    print(\"fold n째{}\".format(fold_))\n    train_x=X_train_public.iloc[trn_idx][usedfeatures].reset_index(drop=True)\n    valid_x=X_train_public.iloc[val_idx][usedfeatures].reset_index(drop=True)\n    target_train=target_F_public.iloc[trn_idx].reset_index(drop=True)\n    target_valid=target_F_public.iloc[val_idx].reset_index(drop=True)\n    trn_data = lgb.Dataset(train_x,\n                           label=target_train,\n                           categorical_feature=f_cat\n                          )\n    val_data = lgb.Dataset(valid_x,\n                           label=target_valid,\n                           categorical_feature=f_cat\n                          )\n\n    num_round = 1000000\n    clf = lgb.train(params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=250,\n                    early_stopping_rounds = 20)\n    traintion_public_f[trn_idx] += clf.predict(train_x, num_iteration=clf.best_iteration)/(folds.n_splits-1)\n    validation_public_f[val_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = usedfeatures\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df_public_f = pd.concat([feature_importance_df_public_f, fold_importance_df], axis=0)\n    \n    predictions_public_f += clf.predict(X_test_public, num_iteration=clf.best_iteration) / folds.n_splits\nprint(\"==========F+Pub==============\")\nprint(\"Train RMSLE score: {:<8.5f}\".format(RMSLE(target_F_public,traintion_public_f)))\nprint(\"Valid RMSLE score: {:<8.5f}\".format(RMSLE(target_F_public,validation_public_f)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* learning_rate': 0.1,'feature_fraction': 1,'min_data_in_leaf':100,'max_depth':7,#'max_bin':300,#   'reg_alpha': 5,#'reg_lambda': 10,'num_leaves':30     \nTrain RMSLE score: 0.48512      \nValid RMSLE score: 0.97629  \n* learning_rate': 0.1,'feature_fraction': 0.8,'min_data_in_leaf':100,'max_depth':7,#'max_bin':300,#   'reg_alpha': 5,#'reg_lambda': 10,'num_leaves':30     \nTrain RMSLE score: 0.55167    \nValid RMSLE score: 0.97150    \n* learning_rate': 0.1,'feature_fraction': 0.8,'min_data_in_leaf':100,'max_depth':7,#'max_bin':300,   'reg_alpha': 5,#'reg_lambda': 10,'num_leaves':30     \nTrain RMSLE score: 0.54260      \nValid RMSLE score: 0.96642     "},{"metadata":{"trusted":true},"cell_type":"code","source":"####################F+private\nparams = {#1\n        'learning_rate': 0.1,\n        'feature_fraction': 0.7,\n        'min_data_in_leaf' : 1,\n        'max_depth': 8,\n        'max_bin':300,\n#        'reg_alpha': 0.01,#l1\n#        'reg_lambda':0.01,#l2\n        'objective': 'regression',\n        'num_leaves':30,\n        'metric': 'rmse',\n        'n_jobs': -1,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'is_unbalance': False,\n#        'bagging_freq':5,\n#        'pos_bagging_fraction':0.8,\n#        'neg_bagging_fraction':0.8,\n        'boost_from_average': False}\ntraintion_private_f = np.zeros(len(X_train))\nvalidation_private_f = np.zeros(len(X_train))\npredictions_private_f = np.zeros(len(X_test_private))\nfeature_importance_df_private_f = pd.DataFrame()\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train,target_F)):\n    print(\"fold n째{}\".format(fold_))\n    train_x=X_train.iloc[trn_idx][usedfeatures].reset_index(drop=True)\n    valid_x=X_train.iloc[val_idx][usedfeatures].reset_index(drop=True)\n    target_train=target_F.iloc[trn_idx].reset_index(drop=True)\n    target_valid=target_F.iloc[val_idx].reset_index(drop=True)\n    trn_data = lgb.Dataset(train_x,\n                           label=target_train,\n                           categorical_feature=f_cat\n                          )\n    val_data = lgb.Dataset(valid_x,\n                           label=target_valid,\n                           categorical_feature=f_cat\n                          )\n\n    num_round = 1000000\n    clf = lgb.train(params,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=250,\n                    early_stopping_rounds = 350)\n    traintion_private_f[trn_idx] += clf.predict(train_x, num_iteration=clf.best_iteration)/(folds.n_splits-1)\n    validation_private_f[val_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = usedfeatures\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df_private_f = pd.concat([feature_importance_df_private_f, fold_importance_df], axis=0)\n    \n    predictions_private_f += clf.predict(X_test_private, num_iteration=clf.best_iteration) / folds.n_splits\nprint(\"==========F+Pri==============\")\nprint(\"Train RMSLE score: {:<8.5f}\".format(RMSLE(target_F,traintion_private_f)))\nprint(\"Valid RMSLE score: {:<8.5f}\".format(RMSLE(target_F,validation_private_f)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':50    \nTrain RMSLE score: 0.15307    \nValid RMSLE score: 0.30333    \n* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':100   \nTrain RMSLE score: 0.11962   \nValid RMSLE score: 0.29044   \n* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':150     \nTrain RMSLE score: 0.09059    \nValid RMSLE score: 0.27502     \n* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':200    \nTrain RMSLE score: 0.08214    \nValid RMSLE score: 0.26751     \n* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':250   \nTrain RMSLE score: 0.07351    \nValid RMSLE score: 0.26144    \n* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':300   \nTrain RMSLE score: 0.07005        \nValid RMSLE score: 0.25886    \n* 'learning_rate': 0.1,'feature_fraction': 0.5,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':350    \nTrain RMSLE score: 0.06105     \nValid RMSLE score: 0.25615    \n* 'learning_rate': 0.1,'feature_fraction': 0.7,'min_data_in_leaf' : 1,'max_depth': 9,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':350     \nTrain RMSLE score: 0.09511    \nValid RMSLE score: 0.25176       \n* 'learning_rate': 0.1,'feature_fraction': 0.7,'min_data_in_leaf' : 1,'max_depth': 8,#        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':350       \nTrain RMSLE score: 0.06084     \nValid RMSLE score: 0.23047       \n* 'learning_rate': 0.1,'feature_fraction': 0.7,'min_data_in_leaf' : 1,'max_depth': 8,        'max_bin':300,#'reg_alpha': 10,#'reg_lambda': 0.5,'num_leaves':30,' stop':350    \nTrain RMSLE score: 0.04702   \nValid RMSLE score: 0.23032   "},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/covid19-global-forecasting-week-2/submission.csv\")\nsub.loc[test_public_index,\"Fatalities\"] = predictions_public_f\nsub.loc[test_public_index,\"ConfirmedCases\"] = predictions_public_c\nsub.loc[test_private_index,\"Fatalities\"] = predictions_private_f\nsub.loc[test_private_index,\"ConfirmedCases\"] = predictions_private_c\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}