{"cells":[{"metadata":{},"cell_type":"markdown","source":"# An exponential-smoothing forecast of fatalities caused by the COVID-19 pandemic"},{"metadata":{},"cell_type":"markdown","source":"**Approach**<br>\nAs we are still in the early stages of the epidemy in most countries, we will use a rather intuitive technique, namely a [damped trend method](https://otexts.com/fpp2/holt.html), to interpolate fatalities per million for each country. The underlying model is a rather intuitive technique to extrapolate exponential trends, almost like a human would do in a drawing, and it is based on four underlying smoothing parameters:\n- Level-smoothing factor $\\alpha$<br>\n- Slope-smoothing or trend factor $\\beta$<br>\n- Slope-damping factor $\\phi$, typically comprised between 0.8 (resilience) and 1 (fatalities increase indefinitely)<br>\n- Seasonality factor $\\gamma$ (we have assumed no seasonality in our forecasts)<br>\n\nThe smoothing parameters will be learned from the dataset and used to forecast fatalities per million, mainly in countries were data is still limited. Our hope is to be able to learn more specifically the damping factor $\\phi$, which is of paramount importance to [***flatten the curve***](https://www.livescience.com/coronavirus-flatten-the-curve.html).\n\n**Important notes and disclaimers:** \n- We will not use the number of confirmed cases directly. We will use it to compute case fatality rates.\n- **Our proposed approach only yields acceptable $R^{2}$ scores with KNN regression** and we hope that with more data ,hopefully in a couple of weeks, we will be able to work with more evolved regression techniques, in the hope that we will be able to better understand COVID-19 risk factors. Our model parameters ($\\alpha$, $\\beta$ and $\\phi$) are very sensitive, i.e. there is not a unique combination of such parameters that best fits the historical fatalities curve for a given country. This is obviously a major issue, as we can only learn from robust parameters, but **we will keep working on this methodological issue.**\n- At this stage, the main purpose of this notebook is therefore to help whoever may want to explore similar forecasting techniques."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport warnings\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates\nimport datetime as dt\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import DistanceMetric, KNeighborsRegressor, RadiusNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\nfrom functools import reduce\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nidx = pd.IndexSlice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data"},{"metadata":{},"cell_type":"markdown","source":"### Load dataset"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/covid19-global-forecasting-week-2/train.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Typo\ndf['Country_Region'].replace('Taiwan*', 'Taiwan', inplace=True)\ndf = df[df['Country_Region'] != 'Diamond Princess']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Country Codes\ndf_codes = (pd.read_csv('../input/my-covid19-dataset/iso-country-codes.csv')\n            .rename(columns={'Country Name': 'Country_Region', 'Alpha-3 code': 'Country Code'})\n            .loc[:,['Country_Region', 'Country Code']])\ndf = (pd.merge(df, df_codes, how='left', on='Country_Region'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Locations\ndef location(state, country):\n    if type(state)==str and not state==country: \n        return country + ' - ' + state\n    else:\n        return country\n\n# Timeline \ndf['Date'] = df['Date'].apply(lambda x: (dt.datetime.strptime(x, '%Y-%m-%d')))\ndf['Location'] = df[['Province_State', 'Country_Region']].apply(lambda row: location(row[0],row[1]), axis=1)\n\nt_start = df['Date'].unique()[0]\nt_end = df['Date'].unique()[-1]\n\nprint('Number of Locations: ' + str(len(df['Location'].unique())))\nprint('Dates: from ' + np.datetime_as_string(t_start, unit='D') + ' to ' + \n                                  np.datetime_as_string(t_end, unit='D') + '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## Adjustements"},{"metadata":{},"cell_type":"markdown","source":"### Population adjustment (normalisation)"},{"metadata":{},"cell_type":"markdown","source":"#### Oversesas territories\nMost oversesas territories will be ignored. We will focus on major countries, as well as relevant states and provinces in China, the USA, Canada and Australia."},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_out = ['Cayman Islands', 'Curacao', 'Faroe Islands', 'French Guiana', 'French Polynesia', 'Guadeloupe', \n           'Mayotte', 'Martinique', 'Reunion', 'Saint Barthelemy', 'St Martin', 'Aruba', 'Channel Islands', \n           'Gibraltar', 'Montserrat', 'Diamond Princess', 'From Diamond Princess', 'Puerto Rico', \n           'Virgin Islands', 'Guam']\n\ndf_loc = (df.loc[[((c not in lst_out) and (p not in lst_out)) \n                 for (c, p) in zip(df['Province_State'], df['Country_Region'])], \n                ['Location','Province_State', 'Country_Region','Country Code']]\n          .drop_duplicates())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Provinces in China, USA, Canada and Australia\n**Source**: Population in for China, the USA, Canada and Australia: http://www.citypopulation.de/"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pop = pd.read_csv('../input/my-covid19-dataset/citypopulation-de/population.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_loc = pd.merge(df_loc, df_pop, how='left', on=['Province_State', 'Country_Region'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Other locations\n**Source:** 2020 Population Estimates, https://population.un.org/wpp/Download/Standard/Population/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Population estimate as of July 2020 published by the UN (in '000 people)\ndf_pop = (pd.read_csv('../input/my-covid19-dataset/un-org/population-2020.csv')\n          .rename(columns={'ISO 3166-1 alpha code': 'Country Code'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pop = df_pop.loc[~df_pop['Country Code'].isin(['CHN', 'USA', 'CAN', 'AUS']), ['Country Code', 'Population']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pop = pd.merge(df_loc, df_pop, how='left', on='Country Code', suffixes=('_P/S','_C/R'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Population ('000) of the State if available, of the Country otherwise\ndf_pop['Population'] = (df_pop[['Population_P/S','Population_C/R']]\n                        .apply(lambda x: x[1] if np.isnan(x[0]) else x[0], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pop = df_pop.loc[:,['Location','Population']].set_index('Location', verify_integrity=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fatalities per Million"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, df_pop, how='left', on='Location')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Fatalities per Million'] = df['Fatalities'] / df['Population'] * 1000\ndf['Confirmed Cases per Million'] = df['ConfirmedCases'] / df['Population'] * 1000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Time Adjustment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Day count since first confirmed case (resp. first fatality)\ndf['Day Count Confirmed'] = (df['ConfirmedCases']>0).groupby(df['Location']).cumsum().astype('int')\ndf['Day Count Fatalities'] = (df['Fatalities']>0).groupby(df['Location']).cumsum().astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We correct a few inconsistencies in the dataset to make sure that day counts are strictly monotonous\n# (the tuple (location, day counts) will be used as index)\ndf['Fatalities'] = df.loc[df['Day Count Fatalities']>0, 'Fatalities'].apply(lambda x: max(x,1))\ndf['Day Count Fatalities'] = (df['Fatalities']>0).groupby(df['Location']).cumsum().astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Locations of interests\nIn some locations, the number of fatalities remains very limited and progresses very slowly. We believe that these locations do not yield meaningful data on which to build our model and we decide to exclude them of our training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# New confirmed cases (resp. fatalities)\ndf['New Cases'] = df['ConfirmedCases'].groupby(df['Location']).diff() / df['ConfirmedCases']\ndf['New Fatalities'] = df['Fatalities'].groupby(df['Location']).diff() / df['Fatalities']\n\n# Case Fatality Rate (i.e. ratio between confirmed cases and confirmed fatalities)\n# (may help identify outliers, i.e. countries where actual cases may be particularly underestimated)\ndf['Case Fatality Rate'] = df['Fatalities'] / df['ConfirmedCases']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# Locations where the number of fatalities remains very limited and progresses very slowly are ignored \ndf_all = df\n\ndf = df[(df['Day Count Fatalities']>0) & \n        ((df['Fatalities per Million']>1) | # at least 1 fatality per million\n        (df['New Fatalities']>1/7))] # at least doubling every week\n#################################################################################################################","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing Capacity\n**Source:** Number of Tests per Million People, https://ourworldindata.org/coronavirus-testing-source-data#"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_testing = (pd.read_csv('../input/my-covid19-dataset/ourworldindata/tests/tests-vs-confirmed-cases-covid-19.csv')\n              .loc[:,['Entity', 'Total COVID-19 tests']]\n              .rename(columns={'Entity':'Location', 'Total COVID-19 tests': 'Tests'}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df, df_testing, how='left', on='Location')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of confirmed cases divided by number of tests (where the number of tests was unknown, we have made \n# the (strong) assumption that only infected people were tested)\ndf['Tests per Million'] = df['Tests'] / df['Population'] * 1000\ndf['Tests per Million'].fillna(df['Confirmed Cases per Million'], inplace=True)\ndf.drop(columns=['Province_State', 'Country_Region', 'Tests', 'Population'], inplace=True)\n\n# 'Confirmed Rate' is defined as the proportion of confirmed cases in the tested population\ndf['Confirmed Rate'] = df['Confirmed Cases per Million'] / df['Tests per Million']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mass testing capacity"},{"metadata":{},"cell_type":"markdown","source":"##### Proportion of confirmed cases among tested population"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt = df.set_index(['Location','Day Count Fatalities'], verify_integrity=True)\n\nmask = (~df_plt.index.get_level_values(0).duplicated(keep='last')) & (df_plt['Confirmed Rate']<1)\ndf_plt = df_plt.loc[mask, ['Fatalities per Million','Tests per Million','Confirmed Rate']].reset_index()\n\ndf_plt.plot(x='Day Count Fatalities', y='Confirmed Rate', c='Tests per Million', \n            kind='scatter', colormap='coolwarm_r', sharex=False, figsize=(17.5,7.5))\n\n# Annotations\nx = df_plt['Day Count Fatalities'].values\ny = df_plt['Confirmed Rate'].values\nz = df_plt['Location'].values\n\nfor i, txt in enumerate(z):\n    if y[i] > .25:\n        plt.text(x[i]+.005, y[i]+.005, txt, rotation=0, rotation_mode='anchor')\n\nplt.title('Proportion of confirmed cases among tested population\\n (Color = Number of Tests per Million)')\nplt.xlabel('Number of days since first fatality')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Proportion of fatalities among confirmed cases"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt = df.set_index(['Location','Day Count Fatalities'], verify_integrity=True)\n\nmask = ~df_plt.index.get_level_values(0).duplicated(keep='last')\ndf_plt = df_plt.loc[mask, ['Fatalities per Million','Tests per Million','Case Fatality Rate']].reset_index()\n\ndf_plt.plot(x='Day Count Fatalities', y='Case Fatality Rate', c='Tests per Million', \n            kind='scatter', colormap='coolwarm_r', sharex=False, figsize=(17.5,7.5))\n\n# Annotations\nx = df_plt['Day Count Fatalities'].values\ny = df_plt['Case Fatality Rate'].values\nz = df_plt['Location'].values\n\nfor i, txt in enumerate(z): # annotate outliers (case fatality rate above 5%)\n    if (y[i]>.05) and (y[i]<.25):\n        plt.text(x[i]+.005, y[i]+.005, txt, rotation=0, rotation_mode='anchor')\n\nplt.ylim((0,.25))\nplt.title('Proportion of fatalities among confirmed cases\\n (Color = Number of Tests per Million)')\nplt.xlabel('Number of days since first fatality')\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Notes:**\n- High case fatality rates in early days seem to indicate a strong biais towards testing and curing of infected people in priority. Lower levels may indicate better anticipation, especially where the number of tests per million is high.\n\nWe will use `Case Fatality Rate` instead of `Confirmed Rate` (which seems less reliable, some values are above 100% for instance)."},{"metadata":{},"cell_type":"markdown","source":"#### Mean Fatality Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute key statistics... \ndf_frate = (df.loc[:,['Day Count Fatalities', 'Case Fatality Rate']].groupby('Day Count Fatalities')\n            .agg(['count', 'mean', 'std']))\n\ny_count = df_frate.loc[:,('Case Fatality Rate', 'count')]\ny_mean = df_frate.loc[:,('Case Fatality Rate', 'mean')]\ny_std  = df_frate.loc[:,('Case Fatality Rate', 'std')]\n\n# ... and plot them\nplt.plot(df_frate.index, y_mean, c='w')\nplt.fill_between(df_frate.index, y_mean - y_std, y_mean + y_std, alpha=.5)\nplt.ylim(bottom=0)\nplt.title('Mean Case Fatality Rate (% of confirmed cases, since first fatality)')\nplt.show()\n\nplt.bar(df_frate.index, y_count, color='grey')\nplt.title('Number of countries')\nplt.gca().set_xlabel('Number of days since first fatality')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Note:*** *The fatality rate seems to reach a normative rate roughly 25 days after the first fatality (top chart). This hypothesis cannot be tested due to limited information thereafter (bottom chart).*"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Fatality rate 30 days after the first fatality\ndf_plt = df[df['Day Count Fatalities']==30]\ndf_plt.plot(x='Day Count Confirmed', y='Case Fatality Rate', c='Fatalities per Million', \n            kind='scatter', colormap='coolwarm', sharex=False, figsize=(10,7.5))\n\n# Annotations\nx = df_plt['Day Count Confirmed'].values\ny = df_plt['Case Fatality Rate'].values\nz = df_plt['Location'].values\n\nfor i, txt in enumerate(z):\n    # fatality rates are expected to be close to 2%\n    # (the number of confirmed cases is probably underestimated otherwise)\n    plt.text(x[i]+.005, y[i]+.005, txt, rotation=0, rotation_mode='anchor')\n\nplt.title('Case Fatality Rate 30 days after the first fatality')\nplt.xlabel('Number of days since first confirmed case')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### High-level consistency check"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We extract the most recent data available for each location and plot correlations\nmask = (~df.set_index(['Location','Date'], verify_integrity=True)\n        .index.get_level_values(0)\n        .duplicated(keep='last'))\ndf_plt = (df[mask].drop(columns=['Date','Country Code',\n                                 'Day Count Confirmed','ConfirmedCases','Confirmed Cases per Million',\n                                 'Confirmed Rate','Fatalities'])\n          .set_index(['Location'], verify_integrity=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\ndf_plt.corr().style.background_gradient(cmap='Reds').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** \nNot surprisingly, we observe a linear correlation bewteen (i) the number of confirmed cases and the number of fatalities (our dependent variables); and (ii) between the number of confirmed cases and the number of tests per million."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.plotting.scatter_matrix(df_plt, figsize=(15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Case Fatality Rates\nWe will focus on fatalities per million and case fatality rates, more reliable than raw numbers of confirmed cases."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last available data\ndf = df.set_index(['Location','Day Count Fatalities'], verify_integrity=True).sort_index()\nmask = ~df.index.get_level_values(0).duplicated(keep='last')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standardized case fatality rates\ndf_cfr = df.loc[mask,:].reset_index(level=1, drop=True).loc[:,['Country Code', 'Case Fatality Rate']]\ndf_cfr[['Case Fatality Rate']] = df_cfr[['Case Fatality Rate']].apply(lambda x: (x-x.min()) / (x.max()-x.min()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exponential smoothing\nWe use exponential smoothing with damping to predict the evolution of the pandemic.<br>\nFor the sake of illustration, we first forecast the epidemy in a given state. Then we compute parameters for all locations.\n\n**Documentation:** https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html"},{"metadata":{},"cell_type":"markdown","source":"### Illustration of the method"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selected state, and forecasting period in days\nstate = 'US - New York'\nfperiod = 90","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fatalities per million for the selected state\nfatalities = df.loc[idx[state, :], 'Fatalities per Million'].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Guess parameters\ninit_alpha = .3 #.5\ninit_beta = .7 #.1\ninit_phi = .8\ninitial_level = fatalities[0]\ninitial_slope = fatalities[1] / fatalities[0]\nstart_params = [init_alpha, init_beta, initial_level, initial_slope, init_phi]\n\n# Search for best parameters\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    fmodel = (ExponentialSmoothing(fatalities, trend='mul', damped=True, seasonal=None)\n              .fit(start_params=start_params, remove_bias=True, use_basinhopping=True))\n    fcast = fmodel.forecast(fperiod)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Visualisation \nplt.figure(figsize=(10,5))\nfcast.plot(style='--', marker='', color='green', legend=True, label='Forecast')\nfmodel.fittedvalues.plot(style='--', marker='', color='blue', legend=True, label='Smoothed')\nfatalities.plot(linestyle='', marker='.', color='red', legend=True, label='Actual values')\n\nkeys = ['smoothing_level', 'smoothing_slope', 'damping_slope']\nalpha, beta, phi = list(map(fmodel.model.params.get, keys))\ntxt_params = ('Exponential smoothing with parameters:\\n\\n\\t' + r'$\\alpha=${}'.format(alpha) + '\\n\\t' + \n              r'$\\beta=${}'.format(beta) + '\\n\\t' + r'$\\phi=${}'.format(phi))\n\nplt.gcf().text(1, 0.5, txt_params)\nplt.title(r'Fatalities per Million in {}'.format(state) + '\\n' + '(multiplicative damped trend)')\nplt.xlabel('Day Count Fatalities')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameters in all locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define placeholders for states of interests\ndf_params = pd.DataFrame(index=df.index.unique(level='Location'), \n                         columns=['Alpha', 'Beta', 'Phi' ,'Forecast per Million'])\nkeys = ['smoothing_level', 'smoothing_slope', 'damping_slope']\n\n# Loop through all locations\nfor state in df.index.get_level_values(level=0).unique():\n    fatalities = df.loc[idx[state, :], 'Fatalities per Million'].reset_index(drop=True)\n    \n    # At least two data points are required to run the model\n    if len(fatalities.dropna().index) < 2: continue\n    \n    # Get optimal parameters\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        \n        fmodel = (ExponentialSmoothing(fatalities, trend='mul', damped=True, seasonal=None)\n                  .fit(remove_bias=True, smoothing_seasonal=0))\n        _, beta, phi = list(map(fmodel.model.params.get, keys))\n        \n        # Re-run optimization if forecast is not constant\n        if not ((beta==0) and (phi==0)):\n            \n            # Guess parameters\n            init_alpha = .3 #.5\n            init_beta = .7 #.1\n            init_phi = .8\n            initial_level = fatalities[0]\n            initial_slope = fatalities[1] / fatalities[0]\n            start_params = [init_alpha, init_beta, initial_level, initial_slope, init_phi]\n\n            # Search for best parameters\n            fmodel = (ExponentialSmoothing(fatalities, trend='mul', damped=True, seasonal=None)\n                      .fit(start_params=start_params, remove_bias=True, use_basinhopping=True))\n            \n        # Consistency check: re-run where fatalities exceed 2% of the entire population\n        if fcast.iloc[-1]>.02*1e3:\n            \n            # Guess alpha and beta subject to phi=0.80\n            fmodel = (ExponentialSmoothing(fatalities, trend='mul', damped=True, seasonal=None)\n                      .fit(damping_slope=.8, remove_bias=True, use_basinhopping=True))\n    \n    # Save model parameters\n    df_params.loc[state, ['Alpha', 'Beta', 'Phi']] = list(map(fmodel.model.params.get, keys))\n    \n    # Save forecast (cumulated number of fatalities at the end of the [90]-day period)\n    df_params.loc[state, 'Forecast per Million'] = fmodel.forecast(fperiod).iloc[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert data to numeric values\ndf_params = df_params.apply(pd.to_numeric, errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training set (list of countries with interpretable results)\nmask = (\n    (pd.isnull(df_params['Phi'])) | # no solution found\n    ((df_params['Beta']==0) & (df_params['Phi']==1)) | # no damping (fatalities increase indefinitely)\n    (df_params['Phi']==0) # dummy forecast (fatality counts remain constant)\n)\n        \nidx_data = df_params.index[~mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\ndf_plt = df_params.dropna().reset_index()\ns, x, y, z = zip(*df_plt[['Location', 'Alpha', 'Beta', 'Phi']].values)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Plot smoothing level vs smoothing slope\nplt.figure(figsize=(10,10))\nplt.scatter(x=x, y=y, marker='.')\n\nfor i, txt in enumerate(s):\n    if not ((x[i] in [0,1]) or (y[i] in [0,1]) or (abs(x[i]-y[i])<1e-2)): # annotate non-naive model parameters\n        plt.text(x[i]-.02, y[i]+.02, i, rotation=0, rotation_mode='anchor', fontsize=8)\n\nplt.title(r'Model parameters: x-axis$=\\alpha$, y-axis$=\\beta$')\nplt.xlabel(r'Smoothing level ($\\alpha$)')\nplt.ylabel(r'Smoothing slope ($\\beta$)')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** For any $\\alpha$ between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name `exponential smoothing`. If $\\alpha$ is small (i.e., close to 0), more weight is given to observations from the more distant past. If $\\alpha$ is large (i.e., close to 1), more weight is given to the more recent observations. For the extreme case where $\\alpha$=1, the forecasts are equal to the naÃ¯ve forecasts.\n\nA very small value of $\\beta$ means that the slope hardly changes over time.\n\n\n**Source:** https://otexts.com/fpp2/expsmooth.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot smoothing slope vs damping factor\nplt.figure(figsize=(10,10))\nsc = plt.scatter(x=y, y=z, c=x, vmin=0, vmax=1, marker='.', cmap='Blues')\nplt.colorbar(sc)\n\nfor i, txt in enumerate(s):\n    if y[i]*(1-z[i])>1e-2: # annotate 'nicest' model parameters (i.e. s-shaped forecasts)\n        plt.text(y[i]-.02, z[i]+.01, i, rotation=0, rotation_mode='anchor', fontsize=8)\n\nt = np.arange(.01, 1., .01)\nplt.plot(t, 1-1e-2/t, 'r--')\nplt.ylim((0,1))\nplt.xlim((0,1))        \n        \nplt.title(r'Model parameters: x-axis$=\\beta$, y-axis$=\\phi$ and $color=\\alpha$')\nplt.xlabel(r'$\\beta$')\nplt.ylabel(r'$\\phi$')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** We had to set the damping factor $\\phi$ to 0.8 manually in order to ensure that all forecasts are actually feasible, i.e. no locations where the predicted number of fatalities exceeds 2% of the entire population (see below)."},{"metadata":{},"cell_type":"markdown","source":"##### Locations where the number of fatalities remains flat"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt.loc[(df_plt['Beta']==0)&(df_plt['Phi']==0)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Locations where the number of fatalities increases indefinitely"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt.loc[(df_plt['Beta']==0)&(df_plt['Phi']==1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In some cases, the number of fatalities is predicted to increase indefinitely. This typically happens when $\\beta$ (the slope-smoothing factor) is equal to 0 and $\\phi$ (the slope-damping factor) is equal to 1.<br>\nTo improve our forecasts, we need to learn parameters $\\alpha$, $\\beta$ and $\\phi$ from country-specific features describing how countries are exposed to the virus and how their respective populations may be infected.<br>\n\nGiven the very limited amount of data at our disposal, we will use very simple models, such as a linear regression or even a mere kNN interpolation."},{"metadata":{},"cell_type":"markdown","source":"##### Infeasible solutions (locations where the predicted number of fatalities exceeds 1% of the entire population...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt.loc[df_plt['Forecast per Million']>.01*1e6]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Non-naive model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt.loc[[not ((a in [0,1]) or \n                 (b in [0,1]) or \n                 (abs(a-b)<1e-2)) for (a,b) in df_plt[['Alpha','Beta']].values]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forecast\nWe show below the predicted number of fatalities by the end of the forecast period."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt = pd.merge(df_plt, df_pop, how='left', on='Location')\ndf_plt['Forecast'] = df_plt['Forecast per Million'] * df_plt['Population'] / 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt.sort_values(by='Forecast').iloc[100:].plot(\n    x='Location', y='Forecast', kind='barh', fontsize=18, legend=False)\nplt.gcf().set_size_inches(30, 50)\nplt.gca().set_xscale('log')\nplt.grid(color='grey', linestyle='--', linewidth=.5)\nplt.ylabel(None)\ntxt = 'Predicted number of fatalities within the next {} days (logarithmic scale)'\nplt.title(txt.format(fperiod), fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plt.sort_values(by='Forecast', ascending=False).iloc[:10,:].plot(\n    x='Location', y='Forecast', kind='bar', legend=False)\ntxt = 'Locations with the highest predicted number of fatalities within the next {} days'\nplt.title(txt.format(fperiod))\nplt.gcf().set_size_inches(12.5, 7.5)\nplt.xticks(rotation=45)\nplt.xlabel(None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** It is fundamental to note that those trends are being estimated before confinment and social distancing measures took their full effect in some countries. We recall that the effect of the confinment is controlled in our model by the damping factor $\\phi$."},{"metadata":{},"cell_type":"markdown","source":"## Additional features"},{"metadata":{},"cell_type":"markdown","source":"### Pairwise distances between countries\nSome countries have not yet recorded any fatalities due to the virus and for our method to produce meaningful results, we need to predict roughly when they will record their first fatality. To do so, we will use pairwise distances between countries and number of days since first confirmed case."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Position of each state on the Earth (in radians)\ndf_r = pd.read_csv('../input/my-covid19-dataset/latlong.csv').set_index('Location', verify_integrity=True).loc[:,['Lat', 'Long']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairwise distances between countries (in km, 6371 is the Earth's radius in km)\nR = 6371\nhs = DistanceMetric.get_metric('haversine')\ndf_dist = pd.DataFrame(data=hs.pairwise(np.radians(df_r)) * R, index=df_r.index, columns=df_r.index)\n\n# standardisation\ndf_dist /= df_dist.values.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### World Development Indicators\n**Sources:**\n* The World Bank: https://databank.worldbank.org/source/world-development-indicators#\n* OECD: https://stats.oecd.org/Index.aspx?ThemeTreeId=9\n\n**Note:** Values are age-standardized"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_wdi = (pd.read_csv('../input/my-covid19-dataset/world-bank/world-development-indicators.csv')\n          .dropna(subset=['Country Code','Series Name'], how='all')\n          .set_index(['Country Code', 'Series Name'], verify_integrity=True)\n          .drop(columns=['Country Name', 'Series Code'])\n          .replace({'..': np.nan})\n          .dropna(how='all')\n          .astype(float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take the latest data available (2018 figures in most cases)\ndef last_available_data(row):\n    res = [d for d in row if not np.isnan(d)]\n    return float(res[-1])\n\ndf_wdi = (df_wdi\n          .apply(lambda row: last_available_data(row), axis=1)\n          .reset_index()\n          .pivot(index='Country Code', columns='Series Name', values=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in missing values with world values if available, or global medians otherwise\nwdi_default = df_wdi.fillna(df_wdi.median()).loc['WLD',:]\ndf_wdi = df_wdi.fillna(wdi_default).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use Singapore as proxy for Taiwan for macroeconomic and demographic data\n# (Note: fatality figures are not available for Hong-Kong and Macao on a stand-alone basis)\nnew_row = df_wdi.loc[df_wdi['Country Code']=='SGP'].copy(deep=True)\nnew_row.loc[:,'Country Code'] = 'TWN'\ndf_wdi = df_wdi.append(new_row, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Risk factors and preexisting health conditions\n**Sources:**<br>\n- NCD RisC: http://www.ncdrisc.org/data-downloads.html<br>\n- Global Health Data Exchange: http://ghdx.healthdata.org/gbd-results-tool\n - Global Burden of Disease Collaborative Network.\n - Global Burden of Disease Study 2017 (GBD 2017) Results.\n - Seattle, United States: Institute for Health Metrics and Evaluation (IHME), 2018."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Risk factors: obesity (2016 figures, averaged between women and men)\ndf_ncd1 = (pd.read_csv('../input/my-covid19-dataset/ncd-risc/obesity/NCD_RisC_Lancet_2017_BMI_age_standardised_country.csv')\n              .loc[:,['ISO', 'Sex', 'Prevalence of BMI>=30 kg/m2 (obesity)']]\n              .rename(columns={'ISO': 'Country Code'})\n              .groupby('Country Code')\n              .mean()\n              .reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Risk factors: blood pressure (2015 figures, averaged between women and men)\ndf_ncd2 = (pd.read_csv('../input/my-covid19-dataset/ncd-risc/blood-pressure/NCD_RisC_Lancet_2016_BP_age_standardised_countries.csv')\n               .loc[:,['ISO', 'Sex', 'Prevalence of raised blood pressure']]\n               .rename(columns={'ISO': 'Country Code'})\n               .groupby('Country Code')\n               .mean()\n               .reset_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preexisting health conditions: cancer prevalence, cardiovascucular diseases, chronic respiratory condition, \n# diabetes and kidney diseases (2017 figures, age-standardized)\ndf_ihme = pd.read_csv('../input/my-covid19-dataset/ihme-gdb/IHME-GBD_2017_DATA-8e93cebf-1.csv')\nmask = [cause_name in ['Neoplasms','Cardiovascular diseases',\n                       'Chronic respiratory diseases','Diabetes and kidney diseases'] \n        for cause_name in df_ihme['cause_name']]\n\ndf_ihme = (df_ihme.rename(columns={'Neoplasms': 'Cancer prevalence'})\n           .loc[mask, ['location_name','cause_name','val']]\n           .pivot(index='location_name', columns='cause_name', values='val')\n           .reset_index()\n           .rename(columns={'location_name': 'Location'})\n           .merge(df_codes, how='left', left_on='Location', right_on='Country_Region')\n           .drop(columns=['Location','Country_Region']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Full dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional country-specific demographic features are merged together\nlst_df = [df_wdi, df_ncd1, df_ncd2, df_ihme]\ndf_feat = reduce(lambda df_left, df_right: pd.merge(df_left, df_right, how='left', on='Country Code'), lst_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last, we add (location-specific, standardized) case fatality rates\ndf_feat = (df_cfr.reset_index()\n          .merge(df_feat, how='left', on='Country Code').drop(columns=['Country Code']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features are standardized\ndf_feat = df_feat.set_index('Location', verify_integrity=True).apply(lambda x: (x-x.min()) / (x.max()-x.min()))\n\n# We replace missing values for obesity, diabetes, pressure and testing by medians\n# (this is certainly too simplistic)\ndf_feat.fillna(df_feat.median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Full set of features\n#df_feat.loc[df_feat.isnull().any(axis=1)]\ndf_feat.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model parameters\n#df_params.loc[df_params.isnull().any(axis=1)]\ndf_params.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other possible features (food for thought...)"},{"metadata":{},"cell_type":"markdown","source":"Other ex-ante (i.e. before the outbreak) features that could be tested to explain the level $\\alpha$ and the slope $\\beta$:<br>\n* Part of the population who has respiratory allergies (pollen)<br>\n\nOther ex-post (i.e. after the outbreak) features that could be tested to explain the damping factor $\\phi$:<br>\n* Day count since beginning of quarantine<br>\n* Proportion of workers who are able to work remotely (easier self-quarantine when confinment becomes necessary)<br>\n\nMore speculative indicators:\n* 'Intensity' of quarantine (how restrictive is the quarantine)<br>\n* Median age of confirmed cases of COVID-19 (this may explain why Germany is doing much better than France for instance)<br>\n* Number of fatalities from previous COVID epidemies (some countries were particularly well prepared as they learned their lesson from previous outbreaks)<br>\n* Proportion of elder people in retirement homes (where the virus can spread very quickly once it is inside)<br>\n* Number of gatherings of more than \\[1,000\\] people since \\[January\\] (it is believed that the virus spread so well in Italy and Spain (resp. in France and South-Korea) due to sports events (resp. religious gatherings))<br>\n* Education rate (better educated people may better understand self-quarantine and confinment instructions and be more inclined to naturally follow them)<br>\n* Political regime (strong political regimes may impose more stringent confinment and surveillance measures)<br>\n* Religiosity and frequency of religious gatherings, especially among elder people (Italy, Spain, Iran)<br>"},{"metadata":{},"cell_type":"markdown","source":"## Fatalities by country over a 90-day period"},{"metadata":{},"cell_type":"markdown","source":"### Interpolation\nWe use parameters $\\alpha$, $\\beta$ and $\\phi$ for locations with meaningful results obtained with exponential smoothing. In the absence of state-specific demographic figures, we use median values to estimate country-wide parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data for locations with meaningful model parameters \nX_data = (df_params\n          .loc[idx_data, ['Alpha', 'Beta', 'Phi']]\n          .merge(df_feat, how='left', left_index=True, right_index=True))\n\n# Parameters to predict\nidx_pred = [idx for idx in df_params.index if idx not in idx_data]\n\nX_pred = (df_params\n          .loc[idx_pred, ['Alpha', 'Beta', 'Phi']]\n          .merge(df_feat, how='left', left_index=True, right_index=True))\n\n# Full dataset\nX = X_data.iloc[:, 3:].values # standardised features\ny = X_data.iloc[:, :3].values # model parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=99)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Nearest Neighbors Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation to guess k=n_neighbors (one output at a time)\nlbl = [r'$\\alpha$', r'$\\beta$']\nc = ['green', 'blue']\nrng_k = range(1,15)\n\nfor i in range(2):\n    scores = []\n    \n    for k in rng_k:\n        rgr = KNeighborsRegressor(n_neighbors=k, weights='distance')\n        rgr.fit(X_train, y_train[:,i])\n        scores.append(rgr.score(X_test, y_test[:,i]))\n    \n    plt.plot(rng_k, scores, label=lbl[i], color=c[i], linestyle='dashed', marker='.', markerfacecolor='grey')\n\nplt.title('Coefficient of determination of the prediction')\nplt.xlabel('Number of neighbors')\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation to guess k=n_neighbors (multi-output) and metric ('minkowski' with p=3)\nrng_k = range(1,15)\nlst_weights = ['distance', 'uniform']\nlst_metrics = ['minkowski', 'chebyshev']\nrng_p = range(1,4)\n\ndef rgr_plot(w, m, p):\n    scores = []\n    \n    for k in rng_k:\n        rgr = KNeighborsRegressor(n_neighbors=k, weights=w, metric=m, p=p)\n        rgr.fit(X_train, y_train)\n        y_pred = rgr.predict(X_test)\n        score = r2_score(y_test, y_pred, multioutput='uniform_average')\n        scores.append(score)\n    \n    label = w + ' - ' + m + ' - ' + str(p)\n    plt.plot(rng_k, scores, linestyle='dashed', marker='.', label=label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in lst_weights:       \n    for m in lst_metrics:\n        if m=='minkowski':\n            for p in rng_p:\n                rgr_plot(w, m, p)\n        elif m=='chebyshev':\n            rgr_plot(w, m, p=-1)\n\nplt.title('Coefficient of determination of the prediction')\nplt.xlabel('Number of neighbors')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\nn_neighbors = 10\nknn = KNeighborsRegressor(n_neighbors=n_neighbors, weights='uniform', metric='minkowski', p=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Radius Neighbors Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation to guess best radius\nrng_r = [t*.25 for t in range(1,7)]\n\ndef rgr_plot(w, m, p):\n    scores = []\n    \n    for r in rng_r:\n        rgr = RadiusNeighborsRegressor(radius=r, weights=w, metric=m, p=p)\n        rgr.fit(X_train, y_train)\n        \n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            y_pred = rgr.predict(X_test)\n\n        if np.any(np.isnan(y_pred)):\n            score = np.nan\n        else:\n            score = r2_score(y_test, y_pred, multioutput='uniform_average')\n        scores.append(score)\n    \n    label = w + ' - ' + m + ' - ' + str(p)\n    plt.plot(rng_r, scores, linestyle='dashed', marker='.', label=label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for w in lst_weights:       \n    for m in lst_metrics:\n        if m=='minkowski':\n            for p in rng_p:\n                rgr_plot(w, m, p)\n        elif m=='chebyshev':\n            rgr_plot(w, m, p=-1)\n\nplt.title('Coefficient of determination of the prediction')\nplt.xlabel('Radius')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prediction\nradius = 1.0\nrnn = RadiusNeighborsRegressor(radius=radius, weights='uniform', metric='minkowski', p=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lnr = LinearRegression(copy_X=True, fit_intercept=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lnr.fit(X_train,y_train)\ny_pred = lnr.predict(X_test)\nscore = r2_score(y_test, y_pred, multioutput='uniform_average')\n\nprint('Coefficient of determination of the linear regression: {:.2%}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** Extremely disappointing score, which may be due to high biais. Let's try regularisation (ridge regressor) and higher degree (polynomial) regression."},{"metadata":{},"cell_type":"markdown","source":"#### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"rdg = RidgeCV(alphas=[10**n for n in range(-4,4)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdg.fit(X_train,y_train)\ny_pred = rdg.predict(X_test)\nscore = r2_score(y_test, y_pred, multioutput='uniform_average')\n\nprint('Coefficient of determination of the linear regression: {:.2%}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Polynomial Regression"},{"metadata":{},"cell_type":"markdown","source":"##### Cross-Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation to guess d=degree of the polynom\nscores = []\nrng_d = range(1,5)\n\nfor d in rng_d:\n    pln = Pipeline([('poly', PolynomialFeatures(degree=d)), \n                    ('linear', LinearRegression(fit_intercept=True))]) \n    pln.fit(X_train, y_train)\n    y_pred = pln.predict(X_test)\n    score = r2_score(y_test, y_pred, multioutput='uniform_average')\n    scores.append(score)\n\nplt.plot(rng_d, scores, color='red', linestyle='dashed', marker='.', markerfacecolor='grey')\nplt.title('Coefficient of determination of the prediction (Linear)')\nplt.xlabel('Degree')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross validation to guess d=degree of the polynom (with regularisation)\nscores = []\nrng_d = range(1,6)\n\nfor d in rng_d:\n    pln = Pipeline([('poly', PolynomialFeatures(degree=d)), \n                    ('ridge', Ridge(alpha=1e1, copy_X=True, fit_intercept=True))]) \n    pln.fit(X_train, y_train)\n    y_pred = pln.predict(X_test)\n    score = r2_score(y_test, y_pred, multioutput='uniform_average')\n    scores.append(score)\n\nplt.plot(rng_d, scores, color='red', linestyle='dashed', marker='.', markerfacecolor='grey')\nplt.title('Coefficient of determination of the prediction (Ridge)')\nplt.xlabel('Degree')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### FItted model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pln = Pipeline([('poly', PolynomialFeatures(degree=2)), \n                ('ridge', Ridge(alpha=1e1, copy_X=True, fit_intercept=False))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pln.fit(X_train,y_train)\ny_pred = pln.predict(X_test)\nscore = r2_score(y_test, y_pred, multioutput='uniform_average')\n\nprint('Coefficient of determination of the linear regression: {:.2%}'.format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Update of model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"rgr = knn # KNN Regressor\n#rgr = rnn # Radius Neighbors Regressor\n#rgr = lnr # Linear Regressor\n#rgr = rdg # Ridge\n#rgr = pln # Polynomial Regression\n#rgr = lgr # Logistic Regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit with all data available\nrgr.fit(X, y)\n\n# Prediction based on selected regressor\ny_pred = rgr.fit(X, y).predict(X_pred.iloc[:, 3:].values)\nX_pred.loc[:,['Alpha', 'Beta','Phi']] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update model parameters\ndf_RGR = (pd.concat([X_data, X_pred]).loc[:,['Alpha','Beta','Phi']].reset_index()\n          .merge(df_params.reset_index(), how='left', on='Location', suffixes=('_RGR',''))\n          .drop_duplicates().set_index('Location', verify_integrity=True))\n\ndf_RGR.loc[idx_pred, ['Alpha','Beta','Phi']] = (df_RGR.loc[idx_pred, ['Alpha_RGR','Beta_RGR','Phi_RGR']]\n                                                .apply(pd.to_numeric).values)\n\ndf_params = df_RGR.drop(columns=['Alpha_RGR','Beta_RGR','Phi_RGR'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Forecast"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Interpolation using customized parameters\nfperiod = 90 # forecasting period in days\ndf = (df_all\n      .loc[df_all['Day Count Fatalities']>0]\n      .set_index(['Location','Day Count Fatalities'], verify_integrity=True)\n      .sort_index(level=[0, 1], ascending=[1, 1]))","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"##### For a specific location"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# Pick a state at random\nstate = np.random.choice(idx_pred, 1)[0]\nprint(state)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# Historical curve\nmask = (df.index.get_level_values(0)==state) & (df.index.get_level_values(1)>0)\nfatalities = df.loc[mask, 'Fatalities'].reset_index(drop=True)\n\n# Exponential smoothing parameters\nalpha, beta, phi = df_params.loc[idx[state], ['Alpha', 'Beta', 'Phi']].values\n\nif len(fatalities)>2:\n    \n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        fmodel = (ExponentialSmoothing(fatalities, trend='mul', damped=True, seasonal=None)\n                  .fit(smoothing_level=alpha, smoothing_slope=beta, damping_slope=phi, \n                       remove_bias=True, use_basinhopping=True))\n        fcast = fmodel.forecast(fperiod)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# Visualisation \nif len(fatalities)>2:\n    \n    plt.figure(figsize=(10,5))\n    fcast.plot(style='--', marker='', color='green', legend=True, label='Forecast')\n    fmodel.fittedvalues.plot(style='--', marker='', color='blue', legend=True, label='Smoothed')\n    fatalities.plot(linestyle='', marker='.', color='red', legend=True, label='Actual values')\n\n    keys = ['smoothing_level', 'smoothing_slope', 'damping_slope']\n    alpha, beta, phi = list(map(fmodel.model.params.get, keys))\n    txt_params = ('Exponential smoothing with parameters:\\n\\n\\t' + r'$\\alpha=${}'.format(alpha) + '\\n\\t' + \n                  r'$\\beta=${}'.format(beta) + '\\n\\t' + r'$\\phi=${}'.format(phi))\n\n    plt.gcf().text(1, 0.5, txt_params)\n    plt.title(r'Fatalities in {}'.format(state) + '\\n' + '(multiplicative damped trend)')\n    plt.xlabel('Day Count Fatalities')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### For all locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through all locations\nlst_states = df.index.get_level_values(0).unique()\nlst_params = df_params.index.unique()\n\nfor state in lst_states:\n\n    # Historical curve\n    mask = (df.index.get_level_values(0)==state) & (df.index.get_level_values(1)>0)\n    fatalities = df.loc[mask, 'Fatalities'].reset_index(drop=True)\n\n    # Exponential smoothing parameters \n    alpha = beta = phi = 0 # constant forecast by default\n    if state in lst_params:\n        alpha, beta, phi = df_params.loc[idx[state], ['Alpha', 'Beta', 'Phi']].values\n\n    # At least two data points are needed for exponential smoothing\n    if len(fatalities)>1:\n\n        # Deal with a few inconsistencies\n        if (np.min(fatalities)==0) or (not fatalities.is_monotonic):\n            print('Inconsistent data identified for {}. Please check.'.format(state))\n            fatalities = fatalities.apply(lambda x: max(x,1)) # at least one fatality\n            # (must be strictly positive when using multiplicative trend)\n        \n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            fmodel = (ExponentialSmoothing(fatalities, trend='mul', damped=True, seasonal=None)\n                      .fit(smoothing_level=alpha, smoothing_slope=beta, damping_slope=phi, \n                           remove_bias=True, use_basinhopping=True))\n            fcast = fmodel.forecast(fperiod)\n            \n        # Add forecast to the dataset\n        date_start = df.loc[idx[state,:], 'Date'][-1]  + dt.timedelta(days=1)\n        rng_dt = pd.date_range(start=date_start, periods=fperiod)\n        \n        arrays = [[state]*len(fcast.index), fcast.index]\n        idx_fcast = pd.MultiIndex.from_arrays(arrays, names=('Location', 'Day Count Fatalities'))\n\n        df = df.append(pd.DataFrame(index=idx_fcast, \n                               data={'Fatalities': fcast.values, 'Date': rng_dt}), sort=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/covid19-global-forecasting-week-2/train.csv', index_col=0)\ndf_test = pd.read_csv('../input/covid19-global-forecasting-week-2/test.csv', index_col=0)\ndf_submit = pd.read_csv('../input/covid19-global-forecasting-week-2/submission.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Country_Region'].replace('Taiwan*', 'Taiwan', inplace=True)\ndf_test['Country_Region'].replace('Taiwan*', 'Taiwan', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Date'] = df_train['Date'].apply(lambda x: (dt.datetime.strptime(x, '%Y-%m-%d')))\ndf_train['Location'] = (df_train[['Province_State','Country_Region']]\n                        .apply(lambda row: location(row[0],row[1]), axis=1))\n\ndf_test['Date'] = df_test['Date'].apply(lambda x: (dt.datetime.strptime(x, '%Y-%m-%d')))\ndf_test['Location'] = (df_test[['Province_State','Country_Region']]\n                       .apply(lambda row: location(row[0],row[1]), axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in the test dataset\ndf_test = (df_test.reset_index()\n           .merge(df_train.reset_index().loc[:,['Location','Date','ConfirmedCases','Fatalities']], \n                  how='left', on=['Location','Date'])\n           .merge(df.reset_index().loc[:,['Location','Date','Fatalities']], \n                  how='left', on=['Location','Date'], suffixes=('_Train','_Forecast'))\n           .set_index('ForecastId', verify_integrity=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is some overlap between the train and test timelines\ndf_test['Fatalities'] = df_test.loc[:,['Fatalities_Train','Fatalities_Forecast']].apply(\n    lambda x: x[0] if not pd.isnull(x[0]) else x[1], axis=1)\n\ndf_test.drop(columns=['Fatalities_Train','Fatalities_Forecast'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Locations with no fatalities or with a negligible number of fatalities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Locations with day counts less than 1\nlst_states = df_test.loc[df_test['Fatalities'].isnull(),'Location'].unique()\n\n# Latest data available\ndt_latest = df_train['Date'].max()\n\ndf_flat = (df_test\n          .loc[df_test['Date']==dt_latest]\n          .set_index('Location', verify_integrity=True)\n          .loc[idx[lst_states]]\n          .sort_values('Fatalities', ascending=True))\n\n# Plot \ndf_flat.loc[df_flat['Fatalities']>0, 'Fatalities'].plot(figsize=(10,7.5), kind='barh')\nplt.title('Fatalities as of {} in locations with day counts less than 1:'.format(dt_latest))\nplt.ylabel('')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flat forecast\nfor state in lst_states:\n    mask = (df_test['Location']==state) & (df_test['Fatalities'].isnull())\n    df_test.loc[mask, ['ConfirmedCases','Fatalities']] = df_flat.loc[idx[state], \n                                                                     ['ConfirmedCases','Fatalities']].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Confirmed Cases\nWe simply divide by the last known case fatality rate to get a proxy for confirmed cases. Ideally, we should use a normative value, but this would lead to misleading results as the number of confirmed cases is likely underestimated in many countries."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get latest case fatality rates\ndf_train['Case Fatality Rate'] = (df_train[['Fatalities','ConfirmedCases']]\n                                  .apply(lambda x: 0 if ((x[0]==0) and (x[1]==0)) else x[0]/x[1], axis=1))\n\ndf_train = df_train.set_index(['Location','Date'], verify_integrity=True).sort_index()\nmask = ~df_train.index.get_level_values(0).duplicated(keep='last')\n\ndf_cfr = df_train.loc[mask,:].reset_index(level=1, drop=True).loc[:,['Case Fatality Rate']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Estimate confirmed cases based on fatalities and case fatality rates\nmask = df_test['ConfirmedCases'].isnull()\ndf_test.loc[mask,'ConfirmedCases'] = (df_test\n                                      .reset_index()\n                                      .merge(df_cfr.reset_index(), how='left', on='Location')\n                                      .set_index('ForecastId', verify_integrity=True)\n                                      .loc[mask,['Fatalities','Case Fatality Rate']]\n                                      .apply(lambda x: 0 if x[1]==0 else x[0]/x[1], axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Round float values to the nearest integer\ndf_test['ConfirmedCases'] = df_test['ConfirmedCases'].apply(lambda x: round(x, 0)).astype('int')\ndf_test['Fatalities'] = df_test['Fatalities'].apply(lambda x: round(x, 0)).astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset\ndf_test = df_test.reset_index().loc[:,['ForecastId','ConfirmedCases','Fatalities']]\ndf_submit = df_submit.reset_index().drop(columns=['ConfirmedCases','Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update\ndf_submit = df_submit.merge(df_test, how='left', on='ForecastId').set_index('ForecastId', verify_integrity=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submit\ndf_submit.to_csv('submission.csv', index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":410.85,"position":{"height":"40px","left":"1539px","right":"20px","top":"120px","width":"294px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"none","window_display":false}},"nbformat":4,"nbformat_minor":4}