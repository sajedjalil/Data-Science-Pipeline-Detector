{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install torchsummary","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchsummary import summary\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndevice = \"cuda\" if torch.cuda.is_available else \"cpu\"\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import transforms\nimport numpy as np\nimport torch\n\n# Returns a list of transformations when called\n\nclass GetTransforms():\n    '''Returns a list of transformations when type as requested amongst train/test\n       Transforms('train') = list of transforms to apply on training data\n       Transforms('test') = list of transforms to apply on testing data'''\n\n    def __init__(self):\n        pass\n\n    def trainparams(self):\n        train_transformations = [ #resises the image so it can be perfect for our model.\n            transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n            transforms.RandomRotation((-7,7)),     #Rotates the image to a specified angel\n            transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n            transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261)) #Normalize all the images\n            ]\n\n        return train_transformations\n\n    def testparams(self):\n        test_transforms = [\n            transforms.ToTensor(),\n            transforms.Normalize((0.491, 0.482, 0.446), (0.247, 0.243, 0.261))\n        ]\n        return test_transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import datasets\nfrom torchvision import transforms\n\n\ntransformations = GetTransforms()\ntrain_transforms = transforms.Compose(transformations.trainparams())\ntest_transforms = transforms.Compose(transformations.testparams())\n\n\nclass GetCIFAR10_TrainData():\n    def __init__(self, dir_name:str):\n        self.dirname = dir_name\n\n    def download_train_data(self):\n        return datasets.CIFAR10('./data', train=True, download=True, transform=train_transforms)\n\n    def download_test_data(self):\n        return datasets.CIFAR10('./data', train=False, download=True, transform=test_transforms)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = GetCIFAR10_TrainData(os.chdir(\"..\"))\ntrainset = data.download_train_data()\ntestset = data.download_test_data()\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=512,\n                                          shuffle=True, num_workers=4)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=512,\n                                         shuffle=False, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Model - RESNET18\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n    \n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n\n        DROPOUT = 0.1\n\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.dropout = nn.Dropout(DROPOUT)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.dropout = nn.Dropout(DROPOUT)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes),\n                nn.Dropout(DROPOUT)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.dropout(self.bn1(self.conv1(x))))\n        out = self.dropout(self.bn2(self.conv2(out)))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return F.log_softmax(out, dim=-1)\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Model and printing Summary\nmodel = ResNet18().to(device)\nsummary(model, input_size=(3,32,32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom torch import nn\nimport torch.nn\nfrom torch.functional import F\nimport os\n\n\ndef model_training(model, device, train_dataloader, optimizer, train_acc, train_losses):\n            \n    model.train()\n    pbar = tqdm(train_dataloader)\n    correct = 0\n    processed = 0\n    running_loss = 0.0\n\n    for batch_idx, (data, target) in enumerate(pbar):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        y_pred = model(data)\n        loss = F.nll_loss(y_pred, target)\n        \n\n        train_losses.append(loss)\n        loss.backward()\n        optimizer.step()\n\n        pred = y_pred.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n        processed += len(data)\n        # print statistics\n        running_loss += loss.item()\n        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n        train_acc.append(100*correct/processed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport os\nfrom torch.functional import F\n\ncwd = os.getcwd()\n\ndef model_testing(model, device, test_dataloader, test_acc, test_losses, misclassified = []):\n    \n    model.eval()\n    test_loss = 0\n    correct = 0\n    class_correct = list(0. for i in range(10))\n    class_total = list(0. for i in range(10))\n    # label = 0\n    classes = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n    \n    with torch.no_grad():\n\n        for index, (data, target) in enumerate(test_dataloader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True)\n            \n            for d,i,j in zip(data, pred, target):\n                if i != j:\n                    misclassified.append([d.cpu(),i[0].cpu(),j.cpu()])\n\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_dataloader.dataset)\n    test_losses.append(test_loss)\n    \n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_dataloader.dataset),\n        100. * correct / len(test_dataloader.dataset)))\n    \n    test_acc.append(100. * correct / len(test_dataloader.dataset))\n    return misclassified","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n# scheduler = StepLR(optimizer, step_size=15, gamma=0.1)\n\ntrain_acc = []\ntrain_losses = []\ntest_acc = []\ntest_losses = []\n\nEPOCHS = 40\n\nfor i in range(EPOCHS):\n    print(f'EPOCHS : {i}')\n    model_training(model, device, trainloader, optimizer, train_acc, train_losses)\n    scheduler.step(train_losses[-1])\n    misclassified = model_testing(model, device, testloader, test_acc, test_losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2,2, figsize=(25,20))\n\naxs[0,0].set_title('Train Losses')\naxs[0,1].set_title('Training Accuracy')\naxs[1,0].set_title('Test Losses')\naxs[1,1].set_title('Test Accuracy')\n\naxs[0,0].plot(train_losses)\naxs[0,1].plot(train_acc)\naxs[1,0].plot(test_losses)\naxs[1,1].plot(test_acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}