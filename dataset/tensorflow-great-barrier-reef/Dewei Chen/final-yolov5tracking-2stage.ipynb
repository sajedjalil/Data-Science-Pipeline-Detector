{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n%cd ..","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:44:13.483635Z","iopub.execute_input":"2022-02-13T20:44:13.483892Z","iopub.status.idle":"2022-02-13T20:45:28.191715Z","shell.execute_reply.started":"2022-02-13T20:44:13.483811Z","shell.execute_reply":"2022-02-13T20:45:28.190923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport torch\nfrom PIL import Image\nimport ast\nimport albumentations as albu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T20:45:28.193955Z","iopub.execute_input":"2022-02-13T20:45:28.194225Z","iopub.status.idle":"2022-02-13T20:45:31.503645Z","shell.execute_reply.started":"2022-02-13T20:45:28.194186Z","shell.execute_reply":"2022-02-13T20:45:31.502903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\n#CKPT_PATH = '../input/yolov5m62880-cut-2880-fineturn/exp_yolov5m6_mx05_fp_2880_f2_cut_2880_fineturn/weights/best.pt'\n#IMG_SIZE  = 8800\n#CONF      = 0.17\n#IOU       = 0.4\n#dis_thr   = 30\n#hit_min   = 3\n#hit_max   = 6\n#ini_dey   = 1\n#AUGMENT   = True\n#do_tracking = True\n#FDA_aug = False\n## CKPT_PATH = '../input/yolov5l6-fineturn/exp_yolov5l6_mx05_fp_1920_f2_1440_fineturn/weights/best.pt'\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:31.504847Z","iopub.execute_input":"2022-02-13T20:45:31.506621Z","iopub.status.idle":"2022-02-13T20:45:31.513825Z","shell.execute_reply.started":"2022-02-13T20:45:31.50659Z","shell.execute_reply":"2022-02-13T20:45:31.513113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nCKPT_PATH = '../input/yolov5l6-fineturn/exp_yolov5l6_mx05_fp_1920_f2_1440_fineturn/weights/best.pt'\nIMG_SIZE  = 3600\nCONF      = 0.01 #0.17\nIOU       = 0.4\ndis_thr   = 30\nhit_min   = 3\nhit_max   = 6\nini_dey   = 1\nAUGMENT   = False\ndo_tracking = True\nFDA_aug = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-13T20:45:31.517437Z","iopub.execute_input":"2022-02-13T20:45:31.517663Z","iopub.status.idle":"2022-02-13T20:45:31.555553Z","shell.execute_reply.started":"2022-02-13T20:45:31.517638Z","shell.execute_reply":"2022-02-13T20:45:31.554249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##############################################################\n#                      Tracking helpers                      #\n##############################################################\n\nimport numpy as np\nfrom norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:31.557409Z","iopub.execute_input":"2022-02-13T20:45:31.557679Z","iopub.status.idle":"2022-02-13T20:45:31.612251Z","shell.execute_reply.started":"2022-02-13T20:45:31.557638Z","shell.execute_reply":"2022-02-13T20:45:31.611603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-13T20:45:31.614966Z","iopub.execute_input":"2022-02-13T20:45:31.615357Z","iopub.status.idle":"2022-02-13T20:45:32.970598Z","shell.execute_reply.started":"2022-02-13T20:45:31.615328Z","shell.execute_reply":"2022-02-13T20:45:32.969643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.28, iou=0.40):\n    model = torch.hub.load('/kaggle/input/yolov5-lib-ds',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload=True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 20  # maximum number of detections per image\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-13T20:45:32.972397Z","iopub.execute_input":"2022-02-13T20:45:32.972953Z","iopub.status.idle":"2022-02-13T20:45:32.979802Z","shell.execute_reply.started":"2022-02-13T20:45:32.972911Z","shell.execute_reply":"2022-02-13T20:45:32.97913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ”­ Inference","metadata":{}},{"cell_type":"markdown","source":"## Helper","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=9000, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot\n\ndef show_img(img, bboxes, bbox_format='yolo'):\n    names  = ['starfish']*len(bboxes)\n    labels = [0]*len(bboxes)\n    img    = draw_bboxes(img = img,\n                           bboxes = bboxes, \n                           classes = names,\n                           class_ids = labels,\n                           class_name = True, \n                           colors = colors, \n                           bbox_format = bbox_format,\n                           line_thickness = 2)\n    return Image.fromarray(img).resize((800, 400))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-13T20:45:32.982144Z","iopub.execute_input":"2022-02-13T20:45:32.982721Z","iopub.status.idle":"2022-02-13T20:45:32.994289Z","shell.execute_reply.started":"2022-02-13T20:45:32.982683Z","shell.execute_reply":"2022-02-13T20:45:32.993597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tracking_function(tracker, frame_id, bboxes, scores):\n    \n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            ##å¤„ç†æžå°æ¡†åŠæ¯”ä¾‹æžå¤§æ¡†\n            w_rate = bbox_width/bbox_height\n            h_rate = bbox_height/bbox_width\n            \n            if ((bbox_width > 12) and (bbox_height > 12) and (0.2<w_rate<5)and (0.25<h_rate<5)):\n                detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n                predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             print(predictions[:-1])\n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:32.995314Z","iopub.execute_input":"2022-02-13T20:45:32.995525Z","iopub.status.idle":"2022-02-13T20:45:33.008808Z","shell.execute_reply.started":"2022-02-13T20:45:32.995493Z","shell.execute_reply":"2022-02-13T20:45:33.007977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stage 2 model","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nsys.path.append(\"../input/timmmaster/\")\n\nimport tez\nimport albumentations\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport timm\nimport torch.nn as nn\nfrom sklearn import metrics\nimport torch\nfrom tez.callbacks import EarlyStopping\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:33.01203Z","iopub.execute_input":"2022-02-13T20:45:33.012419Z","iopub.status.idle":"2022-02-13T20:45:38.607116Z","shell.execute_reply.started":"2022-02-13T20:45:33.012381Z","shell.execute_reply":"2022-02-13T20:45:38.606158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class args:\n    batch_size = 64\n    image_size = 96\n    fold = 0\n\nclass StarfishDataset:\n    def __init__(self, images,  targets, augmentations):\n        self.images = images\n        # self.dense_features = dense_features\n        self.targets = targets\n        self.augmentations = augmentations\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, item):\n        # image = cv2.imread(self.image_paths[item])\n        \n        image = self.images[item]\n        \n        if self.augmentations is not None:\n            augmented = self.augmentations(image=image)\n            image = augmented[\"image\"]\n            \n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n        # features = self.dense_features[item, :]\n        targets = self.targets[item]\n        \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            # \"features\": torch.tensor(features, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.float),\n        }\n\nclass StarfishModel(tez.Model):\n    def __init__(self, model_name):\n        super().__init__()\n\n        self.model = timm.create_model(model_name, pretrained=False, in_chans=3)\n        self.model.classifier = nn.Linear(self.model.classifier.in_features, 64)\n        self.dropout = nn.Dropout(0.1)\n        self.dense1 = nn.Linear(64, 32)\n        self.dense2 = nn.Linear(32, 1)\n\n    def forward(self, image, targets=None):\n\n        x = self.model(image)\n        x = self.dropout(x)\n        # x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = torch.relu(x)\n        x = self.dense2(x)\n        return x, 0, {}\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:38.609767Z","iopub.execute_input":"2022-02-13T20:45:38.610236Z","iopub.status.idle":"2022-02-13T20:45:38.623265Z","shell.execute_reply.started":"2022-02-13T20:45:38.610194Z","shell.execute_reply":"2022-02-13T20:45:38.622507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_stage2 = StarfishModel(model_name=\"tf_efficientnet_b0_ns\")\nmodel_stage2.load(f\"../input/starfish-model/model_f{args.fold}.bin\", device=\"cuda\", weights_only=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:38.624558Z","iopub.execute_input":"2022-02-13T20:45:38.624851Z","iopub.status.idle":"2022-02-13T20:45:43.497041Z","shell.execute_reply.started":"2022-02-13T20:45:38.624811Z","shell.execute_reply":"2022-02-13T20:45:43.49629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init `Env`","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-13T20:45:43.498435Z","iopub.execute_input":"2022-02-13T20:45:43.49873Z","iopub.status.idle":"2022-02-13T20:45:43.5205Z","shell.execute_reply.started":"2022-02-13T20:45:43.498691Z","shell.execute_reply":"2022-02-13T20:45:43.519777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd ../working","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:43.521766Z","iopub.execute_input":"2022-02-13T20:45:43.522035Z","iopub.status.idle":"2022-02-13T20:45:43.528867Z","shell.execute_reply.started":"2022-02-13T20:45:43.522Z","shell.execute_reply":"2022-02-13T20:45:43.528159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Inference on **Test**","metadata":{}},{"cell_type":"code","source":"#lets modify the function a bit so it doesn't show the pics each time. will speed it up a bit. Also no need to flip the BGR to RGB in the loop anymore.\ndef img_bb_cropper(img, annotation_fixed):\n    '''Accepts an image path as a string and an annotation as a stringified list of dictionaries.\n    output is saving the file to the /'''\n    #get image name from the path\n    #img_name = img_path[57:-4].replace('/','-')\n    \n    #get img from url\n    #img = cv2.imread(img_path)  #[:,:,::-1]\n    \n    #fix stringified list annotation\n    #annotation_fixed = ast.literal_eval(annotation)\n    \n    #save the cots image from each annotated bounding box to the crops folder\n    starfish_imgs = []\n    confs = []\n    boxes = []\n    ann_counter = 0\n    for ann in annotation_fixed:\n        ann_box = list(map(int,ann.split(' ')[1:]))\n        conf = ann.split(' ')[0]\n        x,y,w,h = ann_box[0], ann_box[1], ann_box[2], ann_box[3]\n        if w <= 0 or h<=0:\n            continue\n        if x + w >= 1280 or y + h >= 720:\n            continue\n        if x + w <= 0 or y + h <= 0:\n            continue\n        if x <= 0:\n            delta = w - abs(x)\n            x = 0\n            w = delta\n            if w <= 0:\n                continue\n        if y <= 0:\n            delta = h - abs(y)\n            y = 0\n            h = delta \n            if h <= 0:\n                continue\n        confs.append(conf)\n        boxes.append([x,y,w,h])\n        # x,y,w,h = ann['x'], ann['y'], ann['width'], ann['height']\n        cropped_img = img[y:y+h,x:x+w]\n        # print([x,y,w,h])\n        starfish_imgs.append(cropped_img)\n        # cv2.imwrite(f'cots_crops/cotscrop-{img_name}-{ann_counter}.jpg',cropped_img)\n        ann_counter+=1\n    return starfish_imgs, confs, boxes","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:43.530307Z","iopub.execute_input":"2022-02-13T20:45:43.531274Z","iopub.status.idle":"2022-02-13T20:45:43.539136Z","shell.execute_reply.started":"2022-02-13T20:45:43.531236Z","shell.execute_reply":"2022-02-13T20:45:43.538322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_starfish_score(test_images, test_aug):\n    \n    test_dataset = StarfishDataset(\n        images=test_images,\n        targets=np.ones(len(test_images)),\n        augmentations=test_aug,\n    )\n    test_predictions = model_stage2.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n\n    final_test_predictions = []\n    for preds in test_predictions:\n        final_test_predictions.extend(preds.ravel().tolist())\n\n    score = final_test_predictions\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:43.540305Z","iopub.execute_input":"2022-02-13T20:45:43.540643Z","iopub.status.idle":"2022-02-13T20:45:43.548895Z","shell.execute_reply.started":"2022-02-13T20:45:43.540607Z","shell.execute_reply":"2022-02-13T20:45:43.548147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_aug = albumentations.Compose(\n    [\n        albumentations.Resize(args.image_size, args.image_size, p=1),\n        albumentations.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225],\n            max_pixel_value=255.0,\n            p=1.0,\n        ),\n    ],\n    p=1.0,\n)\n\n\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=dis_thr,#30\n    hit_inertia_min=hit_min,#3\n    hit_inertia_max=hit_max,#6\n    initialization_delay=ini_dey,#1\n)\n\nmodel = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\nif do_tracking:\n\n    frame_id =0\n    for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n        if FDA_aug:\n            img = FDA_trans(image=img)['image']\n        bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n        #len(bboxes)\n        predictions = tracking_function(tracker, frame_id, bboxes, confs)\n        # print(predictions)\n        images_list, confs, bboxes = img_bb_cropper(img ,predictions)\n        #len(images_list)\n        scores = predict_starfish_score(images_list, test_aug)\n        #len(scores)\n        predictions = []\n        for i in range(len(scores)):\n            if scores[i] > 0.5:\n                box = bboxes[i]\n                predictions.append('{:.2f} {} {} {} {}'.format(float(confs[i]), int(box[0]), int(box[1]), int(box[2]), int(box[3])))\n\n        prediction_str = ' '.join(predictions)\n        pred_df['annotations'] = prediction_str\n        env.predict(pred_df)\n        if frame_id < 3:\n            if len(predictions)>0:\n                box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n            else:\n                box = []\n            display(show_img(img, box, bbox_format='coco'))\n    #     print('Prediction:', pred_df)\n        frame_id += 1\nelse:\n    for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n        bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n        annot          = format_prediction(bboxes, confs)\n        pred_df['annotations'] = annot\n        env.predict(pred_df)\n        if idx<3:\n            display(show_img(img, bboxes, bbox_format='coco'))\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-13T20:45:43.550203Z","iopub.execute_input":"2022-02-13T20:45:43.550474Z","iopub.status.idle":"2022-02-13T20:45:59.210364Z","shell.execute_reply.started":"2022-02-13T20:45:43.550427Z","shell.execute_reply":"2022-02-13T20:45:59.209616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T20:45:59.211899Z","iopub.execute_input":"2022-02-13T20:45:59.212688Z","iopub.status.idle":"2022-02-13T20:45:59.243659Z","shell.execute_reply.started":"2022-02-13T20:45:59.212647Z","shell.execute_reply":"2022-02-13T20:45:59.243019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}