{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nDATASET_PATH = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\nCKPT_PATH_1 = '/kaggle/input/cotsy5m/best0.pt'\nCKPT_PATH_2 = '/kaggle/input/cotsy5m/best1.pt'\nCKPT_PATH_3 = '/kaggle/input/cotsy5m/best2.pt'\n\n\nIMG_SIZE_1  = 2900\nCONF_1      = 0.1\nIOU_1       = 0.90\n\nIMG_SIZE_2  = 2900\nCONF_2      = 0.1\nIOU_2       = 0.90\n\nIMG_SIZE_3  = 2900\nCONF_3      = 0.1\nIOU_3       = 0.90\n\n\nDEBUG = True\n\nimport numpy as np\nimport pandas as pd\nimport sys\nimport cv2\nimport torch\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport torch\nimport importlib\n\n\nfrom PIL import Image\nfrom IPython.display import display\n\nsys.path.append('../input/tensorflow-great-barrier-reef')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:22:52.337892Z","iopub.execute_input":"2022-02-01T12:22:52.338153Z","iopub.status.idle":"2022-02-01T12:22:52.345134Z","shell.execute_reply.started":"2022-02-01T12:22:52.338124Z","shell.execute_reply":"2022-02-01T12:22:52.34442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pwd","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:09:36.393225Z","iopub.execute_input":"2022-02-01T12:09:36.393533Z","iopub.status.idle":"2022-02-01T12:09:36.402992Z","shell.execute_reply.started":"2022-02-01T12:09:36.393499Z","shell.execute_reply":"2022-02-01T12:09:36.402329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolo5data/Arial.ttf /root/.config/Ultralytics/","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T12:09:36.404303Z","iopub.execute_input":"2022-02-01T12:09:36.404612Z","iopub.status.idle":"2022-02-01T12:09:37.847636Z","shell.execute_reply.started":"2022-02-01T12:09:36.404573Z","shell.execute_reply":"2022-02-01T12:09:37.846557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-01T12:09:37.849299Z","iopub.execute_input":"2022-02-01T12:09:37.849598Z","iopub.status.idle":"2022-02-01T12:10:53.152161Z","shell.execute_reply.started":"2022-02-01T12:09:37.849567Z","shell.execute_reply":"2022-02-01T12:10:53.151284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport numpy as np\nfrom numba import jit\nimport time\n\n@jit(nopython=True)\ndef bb_intersection_over_union(A, B) -> float:\n    xA = max(A[0], B[0])\n    yA = max(A[1], B[1])\n    xB = min(A[2], B[2])\n    yB = min(A[3], B[3])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n\n    if interArea == 0:\n        return 0.0\n\n    # compute the area of both the prediction and ground-truth rectangles\n    boxAArea = (A[2] - A[0]) * (A[3] - A[1])\n    boxBArea = (B[2] - B[0]) * (B[3] - B[1])\n\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n\n\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\n\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            exit()\n\n        if len(boxes[t]) != len(labels[t]):\n            print('Error. Length of boxes arrays not equal to length of labels array: {} != {}'.format(len(boxes[t]), len(labels[t])))\n            exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = int(labels[t][j])\n            box_part = boxes[t][j]\n            x1 = float(box_part[0])\n            y1 = float(box_part[1])\n            x2 = float(box_part[2])\n            y2 = float(box_part[3])\n\n            # Box data checks\n            if x2 < x1:\n                warnings.warn('X2 < X1 value in box. Swap them.')\n                x1, x2 = x2, x1\n            if y2 < y1:\n                warnings.warn('Y2 < Y1 value in box. Swap them.')\n                y1, y2 = y2, y1\n            if x1 < 0:\n                warnings.warn('X1 < 0 in box. Set it to 0.')\n                x1 = 0\n            if x1 > 1:\n                warnings.warn('X1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x1 = 1\n            if x2 < 0:\n                warnings.warn('X2 < 0 in box. Set it to 0.')\n                x2 = 0\n            if x2 > 1:\n                warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x2 = 1\n            if y1 < 0:\n                warnings.warn('Y1 < 0 in box. Set it to 0.')\n                y1 = 0\n            if y1 > 1:\n                warnings.warn('Y1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y1 = 1\n            if y2 < 0:\n                warnings.warn('Y2 < 0 in box. Set it to 0.')\n                y2 = 0\n            if y2 > 1:\n                warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y2 = 1\n            if (x2 - x1) * (y2 - y1) == 0.0:\n                warnings.warn(\"Zero area box skipped: {}.\".format(box_part))\n                continue\n\n            # [label, score, weight, model index, x1, y1, x2, y2]\n            b = [int(label), float(score) * weights[t], weights[t], t, x1, y1, x2, y2]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes\n\n\ndef get_weighted_box(boxes, conf_type='avg'):\n    \n    box = np.zeros(8, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf / len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf / len(boxes)\n    box[2] = w\n    box[3] = -1 \n    box[4:] /= conf\n    return box\n\n\ndef find_matching_box(boxes_list, new_box, match_iou):\n    best_iou = match_iou\n    best_index = -1\n    for i in range(len(boxes_list)):\n        box = boxes_list[i]\n        if box[0] != new_box[0]:\n            continue\n        iou = bb_intersection_over_union(box[4:], new_box[4:])\n        if iou > best_iou:\n            best_index = i\n            best_iou = iou\n\n    return best_index, best_iou\n\n\ndef find_matching_box_quickly(boxes_list, new_box, match_iou):\n    \n    def bb_iou_array(boxes, new_box):\n        # bb interesection over union\n        xA = np.maximum(boxes[:, 0], new_box[0])\n        yA = np.maximum(boxes[:, 1], new_box[1])\n        xB = np.minimum(boxes[:, 2], new_box[2])\n        yB = np.minimum(boxes[:, 3], new_box[3])\n\n        interArea = np.maximum(xB - xA, 0) * np.maximum(yB - yA, 0)\n\n        # compute the area of both the prediction and ground-truth rectangles\n        boxAArea = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n        boxBArea = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])\n\n        iou = interArea / (boxAArea + boxBArea - interArea)\n\n        return iou\n\n    if boxes_list.shape[0] == 0:\n        return -1, match_iou\n\n    # boxes = np.array(boxes_list)\n    boxes = boxes_list\n\n    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n\n    ious[boxes[:, 0] != new_box[0]] = -1\n\n    best_idx = np.argmax(ious)\n    best_iou = ious[best_idx]\n\n    if best_iou <= match_iou:\n        best_iou = match_iou\n        best_idx = -1\n\n    return best_idx, best_iou\n\n\ndef weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 4)), np.zeros((0,)), np.zeros((0,))\n\n    overall_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = np.empty((0,8))\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n        # Rescale confidence based on number of models and boxes\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n            elif conf_type == 'max':\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n            elif not allows_overflow:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n            else:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n        overall_boxes.append(weighted_boxes)\n    overall_boxes = np.concatenate(overall_boxes, axis=0)\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    return boxes, scores, labels\n\ndef weighted_boxes_fusion_tresh(boxes_list, scores_list, labels_list, weights=None, conf_tresh = 0.2, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n    boxes_fin = []\n    scores_fin = []\n    labels_fin = []\n    \n    boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights, iou_thr, skip_box_thr, conf_type, allows_overflow)\n    \n    if len(boxes) >0 :\n        for score, box, label in zip(scores, boxes, labels):\n            if score < conf_tresh:\n                break\n\n            boxes_fin.append(box)\n            scores_fin.append(score)\n            labels_fin.append(label)\n    \n    \n    return np.array(boxes_fin), np.array(scores_fin), np.array(labels_fin)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:10:53.156213Z","iopub.execute_input":"2022-02-01T12:10:53.157124Z","iopub.status.idle":"2022-02-01T12:10:53.855839Z","shell.execute_reply.started":"2022-02-01T12:10:53.157082Z","shell.execute_reply":"2022-02-01T12:10:53.854956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RecoverCLAHE(sceneRadiance):\n    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(4, 4))\n    for i in range(3):\n        sceneRadiance[:, :, i] = clahe.apply((sceneRadiance[:, :, i]))\n\n    return sceneRadiance","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:10:53.874675Z","iopub.execute_input":"2022-02-01T12:10:53.875453Z","iopub.status.idle":"2022-02-01T12:10:53.883096Z","shell.execute_reply.started":"2022-02-01T12:10:53.875415Z","shell.execute_reply":"2022-02-01T12:10:53.882399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.15, iou=0.30):\n    model = torch.hub.load('/kaggle/input/yolo-lib',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload = True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 100  # maximum number of detections per image\n    return model","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-01T12:10:53.884392Z","iopub.execute_input":"2022-02-01T12:10:53.884775Z","iopub.status.idle":"2022-02-01T12:10:53.891455Z","shell.execute_reply.started":"2022-02-01T12:10:53.884736Z","shell.execute_reply":"2022-02-01T12:10:53.890281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, img, size = 1280, augment = False):\n    bboxes = []\n    scores = []\n    classes = []\n    results = model(img, size=size, augment = augment)  # custom inference size\n    preds   = results.pandas().xyxy[0].values\n    for bbox in preds:\n        xmin = bbox[0]\n        ymin = bbox[1]\n        xmax = bbox[2]\n        ymax = bbox[3]\n        \n        bboxes.append([xmin , ymin , xmax, ymax]) \n    \n    scores = preds[: , 4]\n    classes = preds[: , 5]\n    return bboxes, scores, classes","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:10:53.896752Z","iopub.execute_input":"2022-02-01T12:10:53.897236Z","iopub.status.idle":"2022-02-01T12:10:53.905944Z","shell.execute_reply.started":"2022-02-01T12:10:53.897201Z","shell.execute_reply":"2022-02-01T12:10:53.90507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from norfair import Detection, Tracker\n\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:10:53.908121Z","iopub.execute_input":"2022-02-01T12:10:53.908327Z","iopub.status.idle":"2022-02-01T12:10:54.583497Z","shell.execute_reply.started":"2022-02-01T12:10:53.908303Z","shell.execute_reply":"2022-02-01T12:10:54.5827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nframe_id = 0","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:10:54.586955Z","iopub.execute_input":"2022-02-01T12:10:54.587918Z","iopub.status.idle":"2022-02-01T12:10:54.592749Z","shell.execute_reply.started":"2022-02-01T12:10:54.587877Z","shell.execute_reply":"2022-02-01T12:10:54.592093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_prediction(img, bboxes):\n    colors = [(0, 0, 255)]\n\n    obj_names = [\"s\"]\n\n    for box in bboxes:\n        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n    \n    img = Image.fromarray(img).resize((1280, 720))\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:10:54.594429Z","iopub.execute_input":"2022-02-01T12:10:54.595098Z","iopub.status.idle":"2022-02-01T12:10:54.602974Z","shell.execute_reply.started":"2022-02-01T12:10:54.595059Z","shell.execute_reply":"2022-02-01T12:10:54.602199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_y5_1 = load_model(CKPT_PATH_1, CONF_1, IOU_1)\nmodel_y5_2 = load_model(CKPT_PATH_2, CONF_2, IOU_2)\nmodel_y5_3 = load_model(CKPT_PATH_3, CONF_3, IOU_3)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:23:00.862695Z","iopub.execute_input":"2022-02-01T12:23:00.86323Z","iopub.status.idle":"2022-02-01T12:23:02.646073Z","shell.execute_reply.started":"2022-02-01T12:23:00.863192Z","shell.execute_reply":"2022-02-01T12:23:02.645313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tensorflow-great-barrier-reef/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:04.611637Z","iopub.execute_input":"2022-02-01T12:11:04.612133Z","iopub.status.idle":"2022-02-01T12:11:04.663218Z","shell.execute_reply.started":"2022-02-01T12:11:04.612087Z","shell.execute_reply":"2022-02-01T12:11:04.662519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ast import literal_eval\n\n\ndef decode_annotations(annotaitons_str):\n    \"\"\"decode annotations in string to list of dict\"\"\"\n    return literal_eval(annotaitons_str)\n\ndef load_image_with_annotations(img, annotaitons_str):\n    annotations = decode_annotations(annotaitons_str)\n    if len(annotations) > 0:\n        for ann in annotations:\n            cv2.rectangle(img, (ann['x'], ann['y']),\n                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n                (255, 255, 255), thickness=2,)\n            cv2.rectangle(img, (ann['x'], ann['y'] - 15),\n                (ann['x'] + ann['width'], ann['y']),\n                (255, 255, 255), -1 ,)\n            cv2.putText(img, f'GT', (int(ann['x']), int(ann['y'] -3)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:04.66458Z","iopub.execute_input":"2022-02-01T12:11:04.664851Z","iopub.status.idle":"2022-02-01T12:11:04.675166Z","shell.execute_reply.started":"2022-02-01T12:11:04.664815Z","shell.execute_reply":"2022-02-01T12:11:04.674399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_0/9674.jpg',\n                          'video_0/20.jpg', \n                          'video_0/17.jpg', \n                          'video_0/100.jpg',\n                          'video_0/246.jpg',\n                          'video_0/337.jpg',\n                          'video_0/358.jpg',\n                          'video_0/1566.jpg',\n                          'video_0/1854.jpg',\n                          'video_0/1884.jpg',\n                          'video_0/4224.jpg',\n                          'video_0/4540.jpg',\n                          'video_0/4582.jpg',\n                          'video_0/4664.jpg',\n                          'video_0/8215.jpg',\n                          'video_0/8897.jpg',\n                          'video_0/8928.jpg',\n                          'video_0/9441.jpg',\n                          'video_0/9859.jpg',\n                          'video_0/12267.jpg',\n                          'video_2/5748.jpg',\n                          'video_2/5772.jpg',\n                          'video_2/5820.jpg',\n                          'video_2/5409.jpg',\n                          'video_2/5482.jpg',\n                          'video_2/5679.jpg',\n                          'video_2/5712.jpg',\n                          'video_2/5730.jpg',\n                          'video_2/5751.jpg',\n                          'video_2/5817.jpg',\n                          'video_2/5868.jpg',\n                          'video_2/6254.jpg',\n                          'video_2/6339.jpg',\n                          'video_2/10622.jpg',\n                          'video_1/4159.jpg', \n                          'video_1/4183.jpg', \n                          'video_1/4501.jpg',\n                          'video_1/5474.jpg',\n                          'video_1/625.jpg',\n                          'video_1/616.jpg',\n                          'video_1/672.jpg',\n                          'video_1/684.jpg',\n                          'video_1/850.jpg',\n                          'video_1/1927.jpg',\n                          'video_1/2000.jpg',\n                          'video_1/3903.jpg',\n                          'video_1/3945.jpg',\n                          'video_1/4051.jpg',\n                          'video_1/4078.jpg',\n                          'video_1/4096.jpg',\n                          'video_1/4126.jpg',\n                          'video_1/4456.jpg',\n                          'video_1/4525.jpg',\n                          'video_1/5267.jpg',\n                          'video_1/5366.jpg',\n                          'video_1/5411.jpg',\n                          'video_1/5429.jpg',\n                          'video_1/5492.jpg',\n                          'video_1/5661.jpg',\n                          'video_1/5892.jpg',\n                          'video_1/6747.jpg',\n                          'video_1/9082.jpg',)]\n\nids = ids = ['0-9674', '0-20', '0-17', '0-100', '0-246', '0-337', '0-358', '0-1566',\n             '0-1854', '0-1884', '0-4224', '0-4540', '0-4582', '0-4664', '0-8215',\n             '0-8897', '0-8928', '0-9441', '0-9859', '0-12267',\n             '2-5748', '2-5772', '2-5820', '2-5409', '2-5482',\n             '2-5679', '2-5712', '2-5730', '2-5751',  '2-5817',\n             '2-5868', '2-6254', '2-6339', '2-10622',\n             '1-4159', '1-4183', '1-4501', '1-5474', '1-625', '1-616', '1-672', '1-684',\n             '1-850', '1-1927', '1-2000', '1-3903', '1-3945',\n             '1-4051', '1-4078', '1-4096', '1-4126', '1-4456',\n             '1-4525', '1-5267', '1-5366', '1-5411', '1-5429',\n             '1-5492', '1-5661', '1-5892',  '1-6747', '1-9082',]\n\n\n# for img, idx in zip(imgs, ids):\n#     im = cv2.imread(img)\n#     #im = RecoverCLAHE(im)\n#     im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n#     bboxes, scores, classes = predict(model_y5_1, im, size = IMG_SIZE_1, augment = True)\n#     annot = df.query(\"image_id == @idx\").annotations.item()\n#     im = load_image_with_annotations(im, annot)\n#     display(show_prediction(im, bboxes))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:04.67644Z","iopub.execute_input":"2022-02-01T12:11:04.676839Z","iopub.status.idle":"2022-02-01T12:11:04.692114Z","shell.execute_reply.started":"2022-02-01T12:11:04.676801Z","shell.execute_reply":"2022-02-01T12:11:04.691359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ENSEMBLE\n","metadata":{}},{"cell_type":"code","source":"def final_pred(img_in, w_conf = 0.1, w_nms = 0.3):\n        bboxes_y5_1 = []\n        scores_y5_1 = []\n        classes_y5_1 = []\n        \n        bboxes_y5_2 = []\n        scores_y5_2 = []\n        classes_y5_2 = []\n        \n        bboxes_y5_3 = []\n        scores_y5_3 = []\n        classes_y5_3 = []\n        \n        bboxes_fin = [] \n        scores_fin = [] \n        bbclasses_fin = []\n\n        # predykcja Y5\n        bboxes_y5_1, scores_y5_1, classes_y5_1 = predict(model_y5_1, img_in, size = IMG_SIZE_1, augment = False)\n        bboxes_y5_1 = np.array(bboxes_y5_1)\n        \n        if DEBUG:\n            print(\"*********************\")\n            print(\"BBOXES MODEL - 1\")\n            print(bboxes_y5_1)\n            print(\"SCORES\")\n            print(scores_y5_1)\n            print(\"\\n\")\n        \n        if len(bboxes_y5_1)>0:\n            bboxes_y5_1[:, [0, 2]] /= 1280\n            bboxes_y5_1[:, [1, 3]] /= 720\n        \n        \n        \n        bboxes_y5_2, scores_y5_2, classes_y5_2 = predict(model_y5_2, img_in, size = IMG_SIZE_2, augment = False)\n        bboxes_y5_2 = np.array(bboxes_y5_2)\n        \n        if DEBUG:\n            print(\"BBOXES MODEL - 2\")\n            print(bboxes_y5_2)\n            print(\"SCORES\")\n            print(scores_y5_2)\n            print(\"\\n\")\n        \n        if len(bboxes_y5_2)>0:\n            bboxes_y5_2[:, [0, 2]] /= 1280\n            bboxes_y5_2[:, [1, 3]] /= 720\n            \n        \n        bboxes_y5_3, scores_y5_3, classes_y5_3 = predict(model_y5_3, img_in, size = IMG_SIZE_3, augment = False)\n        bboxes_y5_3 = np.array(bboxes_y5_3)\n        \n        if DEBUG:\n            print(\"*********************\")\n            print(\"BBOXES MODEL - 1\")\n            print(bboxes_y5_3)\n            print(\"SCORES\")\n            print(scores_y5_3)\n            print(\"\\n\")\n        \n        if len(bboxes_y5_3)>0:\n            bboxes_y5_3[:, [0, 2]] /= 1280\n            bboxes_y5_3[:, [1, 3]] /= 720\n\n        \n        \n        bboxes_fin, scores_fin, bbclasses_fin = weighted_boxes_fusion_tresh([bboxes_y5_1, bboxes_y5_2, bboxes_y5_3],\n                                                                      [scores_y5_1, scores_y5_2, scores_y5_3],\n                                                                      [classes_y5_1, classes_y5_2, classes_y5_3],\n                                                                      weights=[1,1, 1],\n                                                                      conf_tresh = 0.1,\n                                                                      iou_thr= 0.25,\n                                                                      skip_box_thr=0.01,\n                                                                      conf_type='avg')\n\n        #print(bboxes_fin.shape)\n        if len(bboxes_fin) > 0:\n            bboxes_fin[:, [0, 2]] *= 1280\n            bboxes_fin[:, [1, 3]] *= 720\n        else:\n            bboxes_fin = bboxes_fin.reshape(0,4)\n        \n        if DEBUG:\n            print(\"BBOXES FINAL\")\n            print(bboxes_fin)\n            print(\"SCORES\")\n            print(scores_fin)\n            print(\"\\n\\n\")\n\n        return bboxes_fin, scores_fin, bbclasses_fin\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:19:16.493681Z","iopub.execute_input":"2022-02-01T12:19:16.493958Z","iopub.status.idle":"2022-02-01T12:19:16.511176Z","shell.execute_reply.started":"2022-02-01T12:19:16.493923Z","shell.execute_reply":"2022-02-01T12:19:16.510285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = True\ndir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_0/9674.jpg',\n                          'video_0/20.jpg', \n                          'video_0/17.jpg', \n                          'video_0/100.jpg',\n                          'video_0/246.jpg',\n                          'video_0/337.jpg',\n                          'video_0/358.jpg',\n                          'video_0/1566.jpg',\n                          'video_0/1854.jpg',\n                          'video_0/1884.jpg',\n                          'video_0/4224.jpg',\n                          'video_0/4540.jpg',\n                          'video_0/4582.jpg',\n                          'video_0/4664.jpg',\n                          'video_0/8215.jpg',\n                          'video_0/8897.jpg',\n                          'video_0/8928.jpg',\n                          'video_0/9441.jpg',\n                          'video_0/9859.jpg',\n                          'video_0/12267.jpg',\n                          'video_2/5748.jpg',\n                          'video_2/5772.jpg',\n                          'video_2/5820.jpg',\n                          'video_1/4159.jpg', \n                          'video_1/4183.jpg', \n                          'video_1/4501.jpg',\n                          'video_1/5474.jpg',\n                          'video_1/625.jpg',\n                          'video_1/850.jpg',\n                          'video_1/1927.jpg',\n                          'video_1/2000.jpg',\n                          'video_1/3903.jpg',\n                          'video_1/3945.jpg',\n                          'video_1/4051.jpg',\n                          'video_1/4078.jpg',\n                          'video_1/4096.jpg',\n                          'video_1/4126.jpg',\n                          'video_1/4456.jpg',\n                          'video_1/4525.jpg',\n                          'video_1/5267.jpg',\n                          'video_1/5366.jpg',\n                          'video_1/5411.jpg',\n                          'video_1/5429.jpg',\n                          'video_1/5492.jpg',\n                          'video_1/5661.jpg',\n                          'video_1/5892.jpg',\n                          'video_1/6747.jpg',\n                          'video_1/9082.jpg',)]\n\nids = ids = ['0-9674', '0-20', '0-17', '0-100', '0-246', '0-337', '0-358', '0-1566',\n             '0-1854', '0-1884', '0-4224', '0-4540', '0-4582', '0-4664', '0-8215',\n             '0-8897', '0-8928', '0-9441', '0-9859', '0-12267',\n             '2-5748', '2-5772', '2-5820',\n             '1-4159', '1-4183', '1-4501', '1-5474', '1-625',\n             '1-850', '1-1927', '1-2000', '1-3903', '1-3945',\n             '1-4051', '1-4078', '1-4096', '1-4126', '1-4456',\n             '1-4525', '1-5267', '1-5366', '1-5411', '1-5429',\n             '1-5492', '1-5661', '1-5892',  '1-6747', '1-9082',]\n\n\nfor img, idx in zip(imgs, ids):\n    im = cv2.imread(img)\n    imy = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    print(f\"{img} -> {idx}\")\n    bboxes, scores, _ = final_pred(imy, w_conf = 0.2, w_nms = 0.1)\n    annot = df.query(\"image_id == @idx\").annotations.item()\n    imy = load_image_with_annotations(imy, annot)\n    display(show_prediction(imy, bboxes))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:23:09.827966Z","iopub.execute_input":"2022-02-01T12:23:09.828238Z","iopub.status.idle":"2022-02-01T12:23:49.205179Z","shell.execute_reply.started":"2022-02-01T12:23:09.828206Z","shell.execute_reply":"2022-02-01T12:23:49.20442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:52.953905Z","iopub.execute_input":"2022-02-01T12:11:52.95413Z","iopub.status.idle":"2022-02-01T12:11:52.960808Z","shell.execute_reply.started":"2022-02-01T12:11:52.9541Z","shell.execute_reply":"2022-02-01T12:11:52.959944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG = False","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:52.962075Z","iopub.execute_input":"2022-02-01T12:11:52.962815Z","iopub.status.idle":"2022-02-01T12:11:52.972458Z","shell.execute_reply.started":"2022-02-01T12:11:52.962779Z","shell.execute_reply":"2022-02-01T12:11:52.971646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-01T12:11:52.9739Z","iopub.execute_input":"2022-02-01T12:11:52.975441Z","iopub.status.idle":"2022-02-01T12:11:53.007304Z","shell.execute_reply.started":"2022-02-01T12:11:52.97514Z","shell.execute_reply":"2022-02-01T12:11:53.00672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\nfor (image_np, sample_prediction_df) in iter_test:\n    \n    #bboxes, scores = predict(model, image_np, size = IMG_SIZE, augment = True)\n    \n    bboxes, scores, _ = final_pred(image_np, w_conf = 0.4, w_nms = 0.1)\n    \n    predictions = []\n    detects = []\n    \n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        score = scores[i]\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        detects.append([x_min, y_min, x_max, y_max, score])\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    \n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n            \n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    frame_id += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:53.008509Z","iopub.execute_input":"2022-02-01T12:11:53.009477Z","iopub.status.idle":"2022-02-01T12:11:54.828552Z","shell.execute_reply.started":"2022-02-01T12:11:53.009442Z","shell.execute_reply":"2022-02-01T12:11:54.827751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:11:54.832426Z","iopub.execute_input":"2022-02-01T12:11:54.832878Z","iopub.status.idle":"2022-02-01T12:11:54.848024Z","shell.execute_reply.started":"2022-02-01T12:11:54.832825Z","shell.execute_reply":"2022-02-01T12:11:54.847276Z"},"trusted":true},"execution_count":null,"outputs":[]}]}