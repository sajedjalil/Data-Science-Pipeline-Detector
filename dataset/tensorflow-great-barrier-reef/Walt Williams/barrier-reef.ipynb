{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Notes\n\n+ All images are 720x1280x3","metadata":{}},{"cell_type":"markdown","source":"Next steps: \n+ Create dataset objects\n+ resize all images","metadata":{}},{"cell_type":"code","source":"import os\nfrom os.path import exists\nimport glob\nimport pandas as pd\nimport io\nimport json\nimport xml.etree.ElementTree as ET\nimport contextlib2\nimport argparse\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\nimport tensorflow.compat.v1 as tf\nfrom PIL import Image\nfrom object_detection.utils import dataset_util, label_map_util\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom collections import namedtuple","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-13T11:54:02.358744Z","iopub.execute_input":"2021-12-13T11:54:02.359376Z","iopub.status.idle":"2021-12-13T11:54:02.364805Z","shell.execute_reply.started":"2021-12-13T11:54:02.359328Z","shell.execute_reply":"2021-12-13T11:54:02.363931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA And Data Preprocessing","metadata":{}},{"cell_type":"code","source":"root = '/kaggle/input/'\ndf = pd.read_csv('../input/tensorflow-great-barrier-reef/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T12:27:22.445904Z","iopub.execute_input":"2021-12-13T12:27:22.446175Z","iopub.status.idle":"2021-12-13T12:27:22.482696Z","shell.execute_reply.started":"2021-12-13T12:27:22.446144Z","shell.execute_reply":"2021-12-13T12:27:22.481814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert image_id feature into image proper filepath\ntrainImgFolder = '../input/tensorflow-great-barrier-reef/train_images/'\ndef convertID(row):\n    imgID = row['image_id'].split('-')\n    videoFolder = trainImgFolder + 'video_' + imgID[0]\n    fullPath = os.path.join(videoFolder, imgID[1]+'.jpg')\n    return fullPath\n\ndf['img_path'] = df.apply(lambda row: convertID(row), axis=1)\n\n# Convert annotations from strings -> list\ndf['annotations'] = df.apply(lambda row: ast.literal_eval(row['annotations']), axis=1)\ndf.drop('image_id', axis=1, inplace=True)\ndf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T12:27:34.309411Z","iopub.execute_input":"2021-12-13T12:27:34.309677Z","iopub.status.idle":"2021-12-13T12:27:35.160683Z","shell.execute_reply.started":"2021-12-13T12:27:34.309645Z","shell.execute_reply":"2021-12-13T12:27:35.159816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count how many frames have no labels and how many do\nnumLabeledFrames0, numUnlabeledFrames0 = 0, 0\nnumLabeledFrames1, numUnlabeledFrames1= 0, 0\nnumLabeledFrames2, numUnlabeledFrames2= 0, 0\nfor i, row in df.iterrows():\n    vid = row['video_id']\n    if vid == 0:\n        if row['annotations'] != '[]':\n            numLabeledFrames0 += 1\n        else: numUnlabeledFrames0 += 1\n    elif vid == 1:\n        if row['annotations'] != '[]':\n            numLabeledFrames1 += 1\n        else: numUnlabeledFrames1 += 1\n    elif vid == 2:\n        if row['annotations'] != '[]':\n            numLabeledFrames2 += 1\n        else: numUnlabeledFrames2 += 1\n\nprint(f'''\n\nNumber of unlabeled frames in video_0 ({numUnlabeledFrames0})\nNumber of labeled frames in video_0 ({numLabeledFrames0})\nNumber of unlabeled frames in video_1 ({numUnlabeledFrames1})\nNumber of labeled frames in video_1 ({numLabeledFrames1})\nNumber of unlabeled frames in video_2 ({numUnlabeledFrames2})\nNumber of labeled frames in video_2 ({numLabeledFrames2})\n\n''')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T11:56:47.198898Z","iopub.execute_input":"2021-12-13T11:56:47.199179Z","iopub.status.idle":"2021-12-13T11:56:48.357894Z","shell.execute_reply.started":"2021-12-13T11:56:47.199145Z","shell.execute_reply":"2021-12-13T11:56:48.356956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Preprocessing functions\n# def decode_path(path):\n#     bits = tf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile generate_tfrecords.py\n\n# Initiate argument parser\nparser = argparse.ArgumentParser(\n    description=\"TensorFlow TFRecord Generator\")\nparser.add_argument(\"-c\",\n                    \"--csv_path\",\n                    help=\"Path to the train.csv file.\", type=str)\nparser.add_argument(\"-o\",\n                    \"--output_path\",\n                    help=\"Path of output TFRecord (.record) file.\", type=str)\nparser.add_argument(\"-i\",\n                    \"--image_dir\",\n                    help=\"Path to the folder where the input image files are stored.\", type=str)\nparser.add_argument(\"-t\",\n                    \"--train\",\n                    help=\"True if this is a training dataset, false if it is a validation dataset.\", type=str)\nparser.add_argument(\"-s\",\n                    \"--shards\",\n                    help=\"The number of shards for the dataset\", type=int)\nparser.add_argument(\"-f\",\n                    \"--holdout_fold\",\n                    help=\"The fold to holdout.\", type=int)\n\nargs = parser.parse_args()\n\ndef create_tf_example(data_df: pd.DataFrame, video_id: int, video_frame: int):\n    \"\"\"\n    Create a tf.Example entry for a given training image.\n    \"\"\"\n    full_path = os.path.join(args.image_dir, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] / width) \n            xmaxs.append((annotation['x'] + annotation['width']) / width) \n            ymins.append(annotation['y'] / height) \n            ymaxs.append((annotation['y'] + annotation['height']) / height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(filename),\n      'image/source_id': dataset_util.bytes_feature(filename),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature(image_format),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n\n    return tf_example\n\ndef create_labels_file():\n    label_map_str = \"\"\"\n    item {\n        id: 1\n        name: 'COTS'\n        }\n                    \"\"\"\n\n    if exists('dataset/label_map.pbtxt') is False:\n        with open('dataset/label_map.pbtxt', 'w') as f:\n            f.write(label_map_str)\n        print('Successfully created label_map.pbtxt file')\n\nif __name__ == '__main__':\n\n    # label file\n    create_labels_file()\n    #writer = tf.python_io.TFRecordWriter(args.output_path)\n    \n    # setup df\n    data_df = pd.read_csv(args.csv_path)\n    if args.train =='train':\n        data_df = data_df[data_df.fold != args.holdout_fold].reset_index(drop=True)\n    else:\n        data_df = data_df[data_df.fold == args.holdout_fold].reset_index(drop=True)\n    \n    # make records\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, args.output_path, args.shards)\n        \n        for index, row in data_df.iterrows():\n            tf_example = create_tf_example(data_df, row.video_id, row.video_frame)\n            output_shard_index = index % args.shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n    #writer.close()\n    print('Successfully created the TFRecord file: {}'.format(args.output_path))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-12-13T12:11:42.750206Z","iopub.execute_input":"2021-12-13T12:11:42.750734Z","iopub.status.idle":"2021-12-13T12:11:42.757436Z","shell.execute_reply.started":"2021-12-13T12:11:42.750691Z","shell.execute_reply":"2021-12-13T12:11:42.756401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}