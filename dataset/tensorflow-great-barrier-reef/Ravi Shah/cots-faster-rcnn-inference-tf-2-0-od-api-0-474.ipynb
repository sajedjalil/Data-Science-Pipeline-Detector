{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\n[Check out the training notebook here](https://www.kaggle.com/ravishah1/cots-faster-rcnn-training-w-tf-2-0-od-api-0-474)\n\n[I also took inspiration from this notebook by Khanh](https://www.kaggle.com/khanhlvg/inference-using-efficientdet-d0-model-tensorflow)\n\n### General Info\n\nThis notebook is the training for a faster-rcnn using the TensorFlow Object Detection API. It uses TensorFlow 2.0 API making it eligable for the TensorFlow Performance Prize. It scores a 0.474\n\nThe Model I trained was a Faster-RCNN with a Resnet-101 feature extractor. I applied a few augmentations, used a momentum optimizer, and split with a group k-fold on sequence. For more details, see the training notebook linked at the top.\n\nI predict with a dataset with weights generated from a notebook identical to the training notebook above.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport sys\nimport tensorflow as tf\nimport time\n\n# Import the library that is used to submit the prediction result.\nINPUT_DIR = '../input/tensorflow-great-barrier-reef/'\nsys.path.insert(0, INPUT_DIR)\nimport greatbarrierreef","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_DIR = '../input/cots-fasterrcnn-tfod-api-weights/cots_faster_rcnn_resnet101'\nstart_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" These functions are not currently being used \"\"\"\n\ndef RecoverHE(sceneRadiance):\n    for i in range(3):\n        sceneRadiance[:, :, i] =  cv2.equalizeHist(sceneRadiance[:, :, i])\n    return sceneRadiance\n\ndef apply_HE(img_path: str):\n    img = cv2.imread(img_path)\n    sceneRadiance = RecoverHE(img)\n    return sceneRadiance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_into_numpy_array(path):\n    \"\"\"Load an image from file into a numpy array.\n\n    Puts image into numpy array to feed into tensorflow graph.\n    Note that by convention we put it into a numpy array with shape\n    (height, width, channels), where channels=3 for RGB.\n\n    Args:\n    path: a file path (this can be local or on colossus)\n\n    Returns:\n    uint8 numpy array with shape (img_height, img_width, 3)\n    \"\"\"\n    #image = Image.fromarray(apply_HE(path))\n    \n    img_data = tf.io.gfile.GFile(path, 'rb').read()\n    image = Image.open(io.BytesIO(img_data))\n    (im_width, im_height) = image.size\n    \n    return np.array(image.getdata()).reshape(\n      (im_height, im_width, 3)).astype(np.uint8)\n\ndef detect(image_np):\n    \"\"\"Detect COTS from a given numpy image.\"\"\"\n\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = detect_fn_tf_odt(input_tensor)\n    return detections","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DETECTION_THRESHOLD = 0.50\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n    height, width, _ = image_np.shape\n    \n    # Run object detection using the TensorFlow model.\n    detections = detect(image_np)\n    \n    # Parse the detection result and generate a prediction string.\n    num_detections = detections['num_detections'][0].numpy().astype(np.int32)\n    predictions = []\n    for index in range(num_detections):\n        score = detections['detection_scores'][0][index].numpy()\n        if score < DETECTION_THRESHOLD:\n            continue\n\n        bbox = detections['detection_boxes'][0][index].numpy()\n        y_min = int(bbox[0] * height)\n        x_min = int(bbox[1] * width)\n        y_max = int(bbox[2] * height)\n        x_max = int(bbox[3] * width)\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    # Generate the submission data.\n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","metadata":{},"execution_count":null,"outputs":[]}]}