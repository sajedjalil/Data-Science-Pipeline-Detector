{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)\n> Detect crown-of-thorns starfish in underwater image data\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/31703/logos/header.png?t=2021-10-29-00-30-04\">","metadata":{}},{"cell_type":"markdown","source":"## å½“ãŸã‚Šå‰ã«ã‚„ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨\n- [ ] Dataã®ä¿®æ­£ãƒ»è¿½åŠ \n    - [ ] ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹ãƒ»å°ã•ã™ãã‚‹bboxã‚’ç„¡è¦–ï¼ˆå®Ÿè£…ï¼‰\n    - [ ] æ‰‹ä½œæ¥­ã§bboxã‚’è¿½åŠ ï¼ˆè§£èª¬ï¼‰\n- [ ] Data Augmentation\n    - [ ] Albumentationsãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ãŸaugmentationï¼ˆRandomSizedCrop, HueSaturationValue, RandomBrightnessContrast, ToGray, HorizontalFlip, VerticalFlip, Cutout, etcï¼‰\n    - [ ] mixupï¼ˆç”»åƒã®åˆæˆã€‚å®Ÿè£…ï¼‰\n    - [ ] cutmixï¼ˆcutout+mixup: cutoutã—ãŸéƒ¨åˆ†ã«åˆ¥ç”»åƒã‚’åˆæˆã€‚å®Ÿè£… ï¼‰\n    - [ ] ã‚¸ã‚°ã‚½ãƒ¼ãƒ‘ã‚ºãƒ«ã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆï¼ˆæ—¢è¿°ï¼‰\n- [ ] ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é¸æŠ\n    - [ ] YOLOï¼ˆã‚³ãƒ³ãƒšå‚åŠ ç›´å¾Œã«è§¦ã£ã¦ã„ãŸã€‚v5ã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®å•é¡Œã§ä½¿ç”¨ç¦æ­¢ã«ã€‚å˜ç‹¬ã§ã¯ãŠãã‚‰ãæœ€é«˜ç²¾åº¦ãŒå‡ºã›ã‚‹ãƒ¢ãƒ‡ãƒ«ã ã£ãŸï¼‰\n    - [ ] EfficientDetï¼ˆYOLOv5ãŒç¦æ­¢ã«ãªã£ã¦ã‹ã‚‰ã¯ã²ãŸã™ã‚‰D5ã‚’ä¸­å¿ƒã«EfficientDetã§å®Ÿé¨“ã—ã¦ã„ãŸã€‚EfficientNetã®è€ƒãˆæ–¹ã‚’å–ã‚Šå…¥ã‚ŒãŸç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã€‚å®Ÿè£…ï¼‰\n    - [ ] ä»–ã¯è©¦ã—ã¦ãªã„ãŒã€DetectorRSã‚„UniverseNetãŒè‰¯ã„ãªã©ã®å ±å‘Šã‚ã‚Š\n- [ ] é«˜è§£åƒåº¦ã§å­¦ç¿’\n    - [ ] ãƒªã‚µã‚¤ã‚ºã‚’è¡Œã‚ãš1024 x 1024ã®ç”»åƒã§å­¦ç¿’ï¼ˆColab Proã§ã¯batch size 1ã§ã‚®ãƒªã‚®ãƒªCUDA out of memoryã‚’å›é¿ã§ãã‚‹ï¼‰\n- [x] TTAï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚‚augmentationã€‚å®Ÿè£…ï¼‰\n- [ ] Pseudo Labeling (ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ãŠé¦´æŸ“ã¿ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’äºˆæ¸¬ã—ç¢ºä¿¡åº¦ã®é«˜ã„ãƒ©ãƒ™ãƒ«ã®ã¿è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å–ã‚Šå…¥ã‚Œã¦å†äºˆæ¸¬ã€‚å®Ÿè£…)\n- [ ] Ensemble (ç²¾åº¦ã‚’æ±‚ã‚ã‚‹Kaggleã§ã¯WBFãŒå¼·ã„å ´åˆãŒå¤šãã†ã€‚å®Ÿè£…, è§£èª¬)\n    - [ ] NMSï¼ˆIoUãŒã‚ã‚‹é–¾å€¤ã‚’è¶…ãˆã¦é‡ãªã£ã¦ã„ã‚‹bboxã®é›†åˆã‹ã‚‰ã€ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã®bboxã‚’æ®‹ã—ã¦ã€ãã‚Œä»¥å¤–ã‚’é™¤å»ï¼‰\n    - [ ] SoftNMSï¼ˆIoUé–¾å€¤ã‚’è¶…ãˆãŸbboxã‚’æ®‹ã—ã¤ã¤ã€ã‚¹ã‚³ã‚¢ãŒæœ€å¤§ã®bboxä»¥å¤–ã‚‚é™¤å»ã›ãšã€ã‚¹ã‚³ã‚¢ã‚’å‰²ã‚Šå¼•ã„ã¦æ®‹ã™ï¼‰\n    - [ ] NMW (é‡ãªã‚Šã‚ã£ãŸbboxã‚’ã‚¹ã‚³ã‚¢ã¨IoUã§é‡ã¿ä»˜ã‘ã—ã¦è¶³ã—åˆã‚ã›ã‚‹ã“ã¨ã§ã€1ã¤ã®æ–°ãŸãªbboxã‚’ä½œã‚Šå‡ºã™)\n    - [ ] WBFï¼ˆæ¤œå‡ºã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ•°ãŒå°‘ãªã„bboxã»ã©ã‚¹ã‚³ã‚¢ã‚’ä¸‹ã’ã‚‹ã“ã¨ã§ã€å°‘æ•°ã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã§æ¤œå‡ºã•ã‚ŒãŸbboxã‚’ã‚¹ã‚³ã‚¢ã§è¶³åˆ‡ã‚Šã™ã‚‹ï¼‰","metadata":{}},{"cell_type":"markdown","source":"ã‚¢ã‚¤ãƒ‡ã‚¢\n\n- ãƒ’ãƒˆãƒ‡ãŒæ˜ ã£ã¦ãªã„ç”»åƒã‚‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã—ãŸã‚‰ãƒ€ãƒ¡ã‹ï¼Ÿ\n    - å…¨éƒ¨ä½¿ã†ã¨ã»ã¨ã‚“ã©ãƒ’ãƒˆãƒ‡ãŒæ˜ ã£ã¦ãªã„ã€ã¨åˆ¤å®šã—ãã†\n        - åŒã˜æšæ•°(5k)ã ã£ãŸã‚‰ã„ã„ã‹ã‚‚ï¼Ÿ\n- ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰ãˆã‚‹\n    - YoloX\n    - EfficientDet\n    - FasterRCNN\n    - DETR\n- nofair tracking\n- ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå‡ç­‰åŒ–ã™ã‚‹\n- Augmentationã‚’ã—ãªã„\n    - ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚\n- Adamã‚’ä½¿ã†","metadata":{}},{"cell_type":"markdown","source":"ã‚ã‹ã£ãŸã“ã¨\n- yolov5ã«ã¤ã„ã¦\n    - å­¦ç¿’ã«ã¯train.pyã¨ã„ã†å…ƒã€…ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦ã„ã‚‹\n    - ãã‚Œã«å¼•æ•°ã¨ã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹ã‚„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æƒ…å ±ãŒè¨˜è¿°ã•ã‚ŒãŸ.yamlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸ãˆã¦ã„ã‚‹","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"ã‚„ã‚‹äº‹\n- yolov5ã¨ã¯ä½•ã‹èª¿æŸ»\n    - ä½•ã‚’å­¦ç¿’ã™ã‚‹ã®ã‹ï¼Ÿ\n    - ç²¾åº¦ã¨ã‹ã¯ã©ã†ã‚„ã£ã¦å‡ºã™ã®ã‹ï¼Ÿ\n    - è¦‹ã¤ã‘ãŸã„ã‚‚ã®ãŒæ˜ ã£ã¦ãªã„ãƒ‡ãƒ¼ã‚¿ã¯å­¦ç¿’ã«å«ã‚ã¦ã‚ˆã„ã®ã‹ï¼Ÿ\n- ã„ã‚‰ãªã„éƒ¨åˆ†ã‚’å‰Šã£ã¦ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹\n- W&Bã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦å­¦ç¿’çµŒéã®ã‚°ãƒ©ãƒ•ãªã©ã‚’è¦‹ã‚‹\n    - ~ã¨ã„ã†ã‹ã€ç´°ã‹ã„å‡¦ç†ã¨ã‹wandbã«ç™»éŒ²ã—ã¦ã“ã£ã¡ã§ã‚„ã£ã¦ãã†~\n- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ãƒ¢ãƒ‡ãƒ«5å€‹ä½œã£ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«","metadata":{}},{"cell_type":"markdown","source":"## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ \n\nworking/labels/ä»¥ä¸‹ã«ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®label.txtãŒå…¥ã£ã¦ã„ã‚‹\n\nimagesã‚’åŒã˜","metadata":{}},{"cell_type":"markdown","source":"# ğŸ›  Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -qU wandb\n!pip install -qU bbox-utility # check https://github.com/awsaf49/bbox for source code","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:22:47.906495Z","iopub.execute_input":"2022-02-03T12:22:47.906966Z","iopub.status.idle":"2022-02-03T12:23:08.793093Z","shell.execute_reply.started":"2022-02-03T12:22:47.906852Z","shell.execute_reply":"2022-02-03T12:23:08.792117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ“š Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport glob\n\nimport shutil\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\n\nfrom joblib import Parallel, delayed\n\nfrom IPython.display import display, HTML\n\nfrom matplotlib import animation, rc\nrc('animation', html='jshtml')\n\n# for DA\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as Data\nimport ast #?\nfrom fastprogress.fastprogress import master_bar, progress_bar #?","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T12:23:08.79951Z","iopub.execute_input":"2022-02-03T12:23:08.801189Z","iopub.status.idle":"2022-02-03T12:23:10.813715Z","shell.execute_reply.started":"2022-02-03T12:23:08.801145Z","shell.execute_reply":"2022-02-03T12:23:10.812994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ“Œ Key-Points\n* æä¾›ã•ã‚Œã¦ã„ã‚‹pythonæ™‚ç³»åˆ—APIã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’é€ä¿¡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã“ã®ã‚³ãƒ³ãƒ†ã‚¹ãƒˆã¯ä»¥å‰ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã‚³ãƒ³ãƒ†ã‚¹ãƒˆã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚\n* å„äºˆæ¸¬è¡Œã«ã¯ã€ç”»åƒã®ã™ã¹ã¦ã®å¢ƒç•Œãƒœãƒƒã‚¯ã‚¹ã‚’å«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚æå‡ºã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚‚COCOã®ã‚ˆã†ã§ã™ã€‚ã“ã‚Œã¯`[x_minã€y_minã€å¹…ã€é«˜ã•]`ã‚’æ„å‘³ã—ã¾ã™\n* Copmetitionãƒ¡ãƒˆãƒªãƒƒã‚¯F2ã¯ã€ãƒ’ãƒˆãƒ‡ã‚’è¦‹é€ƒã™ã“ã¨ãŒã»ã¨ã‚“ã©ãªã„ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã«ã€ã„ãã¤ã‹ã®èª¤æ¤œçŸ¥ï¼ˆFPï¼‰ã‚’è¨±å®¹ã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€èª¤æ¤œçŸ¥ï¼ˆFNï¼‰ã¯ã€èª¤æ¤œçŸ¥ï¼ˆFPï¼‰ã‚ˆã‚Šã‚‚é‡è¦ã§ã™ã€‚\n$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$","metadata":{}},{"cell_type":"markdown","source":"# â­ WandB","metadata":{}},{"cell_type":"code","source":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_team_iforine\")\n    wandb.login(key=api_key)\n    anonymous = None\nexcept:\n    wandb.login(anonymous='must')\n    print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your W&B access token. Use the Label name as WANDB. \\nGet your W&B access token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:10.814839Z","iopub.execute_input":"2022-02-03T12:23:10.815072Z","iopub.status.idle":"2022-02-03T12:23:13.145894Z","shell.execute_reply.started":"2022-02-03T12:23:10.815039Z","shell.execute_reply":"2022-02-03T12:23:13.145118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ“– Meta Data\n* `train_images/` - Folder containing training set photos of the form `video_{video_id}/{video_frame}.jpg`.\n\n* `[train/test].csv` - ç”»åƒã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ä»–ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒæ§˜ã«ã€ãƒ†ã‚¹ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã»ã¨ã‚“ã©ã¯ã€æå‡ºæ™‚ã«ã—ã‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã®ã¯æœ€åˆã®æ•°è¡Œã ã‘ã§ã™ã€‚\n\n* `video_id` - ç”»åƒãŒå«ã¾ã‚Œã‚‹ãƒ“ãƒ‡ã‚ªã®IDç•ªå·ã€‚ãƒ“ãƒ‡ã‚ªIDã¯æ„å‘³ã®ã‚ã‚‹é †åºã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n* `video_frame` - æ˜ åƒå†…ã®ç”»åƒã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã§ã™ã€‚ãƒ€ã‚¤ãƒãƒ¼ãŒæµ®ä¸Šã—ãŸã¨ãã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã«ãšã‚ŒãŒç”Ÿã˜ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.æŒ‡å®šã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªã®ã‚®ãƒ£ãƒƒãƒ—ãƒ•ãƒªãƒ¼éƒ¨åˆ†é›†åˆã®IDã€‚ã‚·ãƒ¼ã‚±ãƒ³ã‚¹IDã¯æ„å‘³ã®ã‚ã‚‹é †åºã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n* `sequence_frame` - æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·ã€‚\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).Pythonã§ç›´æ¥è©•ä¾¡å¯èƒ½ãªæ–‡å­—åˆ—å½¢å¼ã®ãƒ’ãƒˆãƒ‡æ¤œå‡ºã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã€‚æå‡ºã™ã‚‹äºˆæ¸¬å€¤ã¨åŒã˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚test.csvã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã¯ã€ç”»åƒå†…ã®å·¦ä¸‹éš…ã®ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ `(x_min, y_min)` ã¨ã€ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã® `width` ã¨ `height` ã§è¨˜è¿°ã•ã‚Œã‚‹ --> (COCO å½¢å¼)ã€‚","metadata":{}},{"cell_type":"code","source":"FOLD      = 4 # which fold to train\nDIM       = 2016\nMODEL     = 'yolov5s'\nBATCH     = 4\nEPOCHS    = 10\nOPTIM     = 'Adam'\nAUG       = 'HE'\n\nPROJECT   = 'iforine/great-barrier-reef-public' # w&b in yolov5\nNAME      = f'{MODEL}-dim{DIM}-fold{FOLD}-bat{BATCH}-opt{OPTIM}-aug{AUG}-epch{EPOCHS}-addNoCot' # w&b for yolov5\n\nREMOVE_NOBBOX = False # remove images with no bbox\nADD_NOBBOX = True # bboxã®ã‚ã‚‹ç”»åƒã¨åŒã˜æšæ•°åˆ†bboxã®ç„¡ã„ç”»åƒã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«åŠ ãˆã‚‹\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nIMAGE_DIR = '/kaggle/working/images' # directory to save images\nLABEL_DIR = '/kaggle/working/labels' # directory to save labels\n\nWORKER = 4 # ã‚ˆãã‚ã‹ã£ã¦ãªã„ã€‚ã‚¹ãƒ¬ãƒƒãƒ‰ã®æ•°ã¨ã‹ï¼Ÿ\n\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:13.147475Z","iopub.execute_input":"2022-02-03T12:23:13.148445Z","iopub.status.idle":"2022-02-03T12:23:13.155912Z","shell.execute_reply.started":"2022-02-03T12:23:13.148401Z","shell.execute_reply":"2022-02-03T12:23:13.155246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Directories","metadata":{}},{"cell_type":"code","source":"!mkdir -p {IMAGE_DIR}\n!mkdir -p {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:13.159112Z","iopub.execute_input":"2022-02-03T12:23:13.159665Z","iopub.status.idle":"2022-02-03T12:23:14.477984Z","shell.execute_reply.started":"2022-02-03T12:23:13.159636Z","shell.execute_reply":"2022-02-03T12:23:14.477058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Paths","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', 10)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:14.480137Z","iopub.execute_input":"2022-02-03T12:23:14.480387Z","iopub.status.idle":"2022-02-03T12:23:14.486042Z","shell.execute_reply.started":"2022-02-03T12:23:14.480359Z","shell.execute_reply":"2022-02-03T12:23:14.485276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pandas.eval()\n\næ§˜ã€…ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€Pythonå¼ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è©•ä¾¡ã—ã¾ã™ã€‚","metadata":{}},{"cell_type":"code","source":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\ndf['old_image_path'] = f'{ROOT_DIR}/train_images/video_'+df.video_id.astype(str)+'/'+df.video_frame.astype(str)+'.jpg'\ndf['image_path']  = f'{IMAGE_DIR}/'+df.image_id+'.jpg' # '/kaggle/working/images'\ndf['label_path']  = f'{LABEL_DIR}/'+df.image_id+'.txt' # '/kaggle/working/labels'\ndf['annotations'] = df['annotations'].progress_apply(eval) # apply(å„è¦ç´ ã«é–¢æ•°ã‚’é©ç”¨ã™ã‚‹)ã®é€²æ—ã‚’è¡¨ç¤ºã™ã‚‹ã€‚evalã¯ä½•ï¼Ÿ\ndisplay(df.head(100))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:14.48773Z","iopub.execute_input":"2022-02-03T12:23:14.488296Z","iopub.status.idle":"2022-02-03T12:23:15.063406Z","shell.execute_reply.started":"2022-02-03T12:23:14.48826Z","shell.execute_reply":"2022-02-03T12:23:15.062616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes\n> Nearly 80% images are without any bbox.","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100 # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè¦ç´ ã®å‡ºç¾é »åº¦ã‚’ç®—å‡ºã€‚normalize=Trueã«ã™ã‚‹ã¨åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«æ­£è¦åŒ–ã•ã‚Œã‚‹(å‰²åˆã«ãªã‚‹)\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.064874Z","iopub.execute_input":"2022-02-03T12:23:15.065134Z","iopub.status.idle":"2022-02-03T12:23:15.163444Z","shell.execute_reply.started":"2022-02-03T12:23:15.065097Z","shell.execute_reply":"2022-02-03T12:23:15.159608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Error displaying widget: model not found`\n\nä½•ã‚‰ã‹ã®åŸå› ã§progressã‚’å‡ºã›ãªã„ã®ã‹ã‚‚ã€‚ãƒ¢ãƒ‡ãƒ«ãŒãªã„ã€ã¨ã¯ã©ã†ã„ã†ã“ã¨ï¼Ÿ","metadata":{}},{"cell_type":"markdown","source":"å®Ÿéš›ã«bboxã®å­˜åœ¨ã™ã‚‹ç”»åƒã‚’è¦‹ã¦ã¿ã‚‹","metadata":{}},{"cell_type":"markdown","source":"# ğŸ§¹ Clean Data\n* In this notebook, we use only **bboxed-images** (`~5k`). We can use all `~23K` images for train but most of them don't have any labels. So it would be easier to carry out experiments using only **bboxed images**.\n* ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€bboxed-imagesï¼ˆã€œ5kï¼‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚trainã«ã¯ç´„23Kã®ç”»åƒã‚’ã™ã¹ã¦ä½¿ç”¨ã§ãã¾ã™ãŒã€ã»ã¨ã‚“ã©ã®ç”»åƒã«ã¯ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã—ãŸãŒã£ã¦ã€bboxedç”»åƒã®ã¿ã‚’ä½¿ç”¨ã—ã¦å®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹æ–¹ãŒç°¡å˜ã§ã™","metadata":{}},{"cell_type":"code","source":"if REMOVE_NOBBOX:\n    df = df.query(\"num_bbox>0\") # df[df['num_bbox'] > 0]ã¨åŒç­‰ã€‚ç›´è¦³çš„ã§ä¾¿åˆ©ã ãƒ»ãƒ»ãƒ»","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.164541Z","iopub.execute_input":"2022-02-03T12:23:15.164934Z","iopub.status.idle":"2022-02-03T12:23:15.169313Z","shell.execute_reply.started":"2022-02-03T12:23:15.164895Z","shell.execute_reply":"2022-02-03T12:23:15.168461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# èƒŒæ™¯ç”»åƒã‚’å…¨ä½“ã®10%å«ã‚ã‚‹\nif ADD_NOBBOX:\n    df = pd.concat([df.query(\"num_bbox>0\"), df.query(\"num_bbox==0\").sample(int(len(df.query(\"num_bbox>0\")) * 0.1))])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.170638Z","iopub.execute_input":"2022-02-03T12:23:15.171465Z","iopub.status.idle":"2022-02-03T12:23:15.201889Z","shell.execute_reply.started":"2022-02-03T12:23:15.171427Z","shell.execute_reply":"2022-02-03T12:23:15.201164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# âœï¸ Write Images\n* We need to copy the Images to Current Directory(`/kaggle/working`) as `/kaggle/input` doesn't have **write access** which is needed for **YOLOv5**.\n* We can make this process faster using **Joblib** which uses **Parallel** computing.\n\n* / kaggle / inputã«ã¯YOLOv5ã«å¿…è¦ãªæ›¸ãè¾¼ã¿ã‚¢ã‚¯ã‚»ã‚¹æ¨©ãŒãªã„ãŸã‚ã€ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ/ kaggle / workingï¼‰ã«ã‚³ãƒ”ãƒ¼ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n* ã“ã®å‡¦ç†ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã«ã¯ã€**ä¸¦åˆ—**è¨ˆç®—ã‚’åˆ©ç”¨ã™ã‚‹**Joblib**ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚","metadata":{}},{"cell_type":"markdown","source":"shutil.copyfile(src, dst, *, follow_symlinks=True)\n\nsrc ã¨ã„ã†åå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã¾ãªã„) ã‚’ dst ã¨ã„ã†åå‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚³ãƒ”ãƒ¼ã—ã€æœ€ã‚‚åŠ¹ç‡çš„ãªæ–¹æ³•ã§ dst ã‚’è¿”ã—ã¾ã™ã€‚ src ã¨ dst ã¯ path-like object ã¾ãŸã¯æ–‡å­—åˆ—ã§ãƒ‘ã‚¹åã‚’æŒ‡å®šã—ã¾ã™ã€‚","metadata":{}},{"cell_type":"code","source":"def make_copy(row):\n    shutil.copyfile(row.old_image_path, row.image_path)\n    return","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.204028Z","iopub.execute_input":"2022-02-03T12:23:15.20446Z","iopub.status.idle":"2022-02-03T12:23:15.208881Z","shell.execute_reply.started":"2022-02-03T12:23:15.204423Z","shell.execute_reply":"2022-02-03T12:23:15.20811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ä¸¦åˆ—å‡¦ç†\n\n```\njoblib.Parallel(<Parallelã¸ã®å¼•æ•°>)(\n    joblib.delayed(<å®Ÿè¡Œã™ã‚‹é–¢æ•°>)(<é–¢æ•°ã¸ã®å¼•æ•°>) for å¤‰æ•°å in ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«\n)\n```","metadata":{}},{"cell_type":"markdown","source":"iterrows()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ã¨ã€1è¡Œãšã¤ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åï¼ˆè¡Œåï¼‰ã¨ãã®è¡Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆpandas.Serieså‹ï¼‰ã®ã‚¿ãƒ—ãƒ«(index, Series)ã‚’å–å¾—ã§ãã‚‹ã€‚","metadata":{}},{"cell_type":"code","source":"image_paths = df.old_image_path.tolist()\n_ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(row) for _, row in tqdm(df.iterrows(), total=len(df)))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:15.210201Z","iopub.execute_input":"2022-02-03T12:23:15.210642Z","iopub.status.idle":"2022-02-03T12:23:47.548148Z","shell.execute_reply.started":"2022-02-03T12:23:15.210604Z","shell.execute_reply":"2022-02-03T12:23:47.547506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ”¨ Helper","metadata":{}},{"cell_type":"code","source":"# check https://github.com/awsaf49/bbox for source code of following utility functions\n# ä½œè€…ãŒä½œã£ãŸãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\nfrom bbox.utils import coco2yolo, coco2voc, voc2yolo\nfrom bbox.utils import draw_bboxes, load_image\nfrom bbox.utils import clip_bbox, str2annot, annot2str\n\n# bboxã‚’ãƒªã‚¹ãƒˆã«ã—ã¦è¿”ã™\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\n# rowã«å¹…ã¨é«˜ã•ã®åˆ—ã‚’è¿½åŠ ã™ã‚‹\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path']) # ã“ã®imagesizeã£ã¦ä½•ï¼Ÿ\n    return row\n\nnp.random.seed(32)\ncolors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n          for idx in range(1)]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-03T12:23:47.54924Z","iopub.execute_input":"2022-02-03T12:23:47.549434Z","iopub.status.idle":"2022-02-03T12:23:48.177477Z","shell.execute_reply.started":"2022-02-03T12:23:47.549409Z","shell.execute_reply":"2022-02-03T12:23:48.176769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create BBox","metadata":{}},{"cell_type":"code","source":"# annotionsã‹ã‚‰bboxesåˆ—ã‚’ä½œæˆ\ndf['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:48.181172Z","iopub.execute_input":"2022-02-03T12:23:48.181406Z","iopub.status.idle":"2022-02-03T12:23:49.823586Z","shell.execute_reply.started":"2022-02-03T12:23:48.18138Z","shell.execute_reply":"2022-02-03T12:23:49.8229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Image-Size\n> All Images have same dimension, [Width, Height] =  `[1280, 720]`","metadata":{}},{"cell_type":"code","source":"df['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:49.824893Z","iopub.execute_input":"2022-02-03T12:23:49.825291Z","iopub.status.idle":"2022-02-03T12:23:49.843354Z","shell.execute_reply.started":"2022-02-03T12:23:49.825254Z","shell.execute_reply":"2022-02-03T12:23:49.842622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ·ï¸ Create Labels\nWe need to export our labels to **YOLO** format, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The *.txt file specifications are:\n\n* One row per object\n* Each row is class `[x_center, y_center, width, height]` format.\n* Box coordinates must be in **normalized** `xywh` format (from `0 - 1`). If your boxes are in pixels, divide `x_center` and `width` by `image width`, and `y_center` and `height` by `image height`.\n* Class numbers are **zero-indexed** (start from `0`).\n\n> Competition bbox format is **COCO** hence `[x_min, y_min, width, height]`. So, we need to convert form **COCO** to **YOLO** format.\n\nå„ç”»åƒã«å¯¾ã—ã¦.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œã£ã¦YOLOã«å¯¾å¿œã™ã‚‹å½¢å¼ã«ã™ã‚‹\n\n* ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä¸€ã¤ã«ã¤ã1è¡Œ\n* å„è¡Œä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ`[x_center, y_center, width, height]`\n* boxåº§æ¨™ã¯**0-1ã§æ­£è¦åŒ–ã•ã‚ŒãŸ**xywhãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‚ãƒ”ã‚¯ã‚»ãƒ«å˜ä½ã®å ´åˆã¯ã€ `x_center` ã¨ `width` ã‚’ `image width` ã§å‰²ã£ã¦ã€ `y_center` ã¨ `height` ã‚’ `image height` ã§å‰²ã£ã¦ãã ã•ã„ã€‚\n* ã‚¯ãƒ©ã‚¹ç•ªå·ã¯ **0ã‹ã‚‰å§‹ã¾ã‚‹ã‚¼ãƒ­ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹** ã§ã™ã€‚\n\n> ã‚³ãƒ³ãƒšã®bboxå½¢å¼ã¯COCOã§ã‚ã‚‹ãŸã‚ã€[x_minã€y_minã€widthã€height]ã§ã™ã€‚ã—ãŸãŒã£ã¦ã€ãƒ•ã‚©ãƒ¼ãƒ COCOã‚’YOLOå½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', 100) # åˆ—ãŒå¤šã„ã¨çœç•¥ã•ã‚Œã‚‹ã®ã‚’é˜²ã","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:49.84488Z","iopub.execute_input":"2022-02-03T12:23:49.845369Z","iopub.status.idle":"2022-02-03T12:23:49.851906Z","shell.execute_reply.started":"2022-02-03T12:23:49.845331Z","shell.execute_reply":"2022-02-03T12:23:49.851253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ã“ã‚ŒãŒãƒ©ãƒ™ãƒ«ï¼Ÿ3ã®ã¨ã“ã«ã¯bboxã®æ•°ãŒå…¥ã‚‹","metadata":{}},{"cell_type":"markdown","source":"coco => **[xmin, ymin, w, h]**\n\nvoc  => **[xmin, ymin, xmax, ymax]**\n\nyolo => **[xmid, ymid, w, h]** (normalized)\n\n```\ndef clip_bbox(bboxes_voc, height=720, width=1280):\n\n    Clip bounding boxes to image boundaries.\n    Args:\n        bboxes_voc (np.ndarray): bboxes in [xmin, ymin, xmax, ymax] format.\n        height (int, optional): height of bbox. Defaults to 720.\n        width (int, optional): width of bbox. Defaults to 1280.\n    Returns:\n        np.ndarray : clipped bboxes in [xmin, ymin, xmax, ymax] format.\n```","metadata":{}},{"cell_type":"markdown","source":"## label.txtã‚’ä½œæˆ","metadata":{}},{"cell_type":"code","source":"cnt = 0\nall_bboxes = []\nbboxes_info = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy() #bboxesã‚’numpyå½¢å¼ã«å¤‰æ›\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = np.array([0]*num_bbox)[..., None].astype(str) # æ¬¡å…ƒã‚’++(ãƒªã‚¹ãƒˆã‹ã‚‰shape:(N, 1)ã®è¡Œåˆ—ã¸)\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_voc  = coco2voc(bboxes_coco, image_height, image_width)\n        bboxes_voc  = clip_bbox(bboxes_voc, image_height, image_width)\n        bboxes_yolo = voc2yolo(bboxes_voc, image_height, image_width).astype(str)\n        all_bboxes.extend(bboxes_yolo.astype(float)) # all_bboxesã«bboxes_yoloã‚’è¿½åŠ \n        bboxes_info.extend([[row.image_id, row.video_id, row.sequence]]*len(bboxes_yolo)) # bboxes_infoã«bboxã®æ•°ã ã‘[image_id, video_id, sequence]ã‚’è¿½åŠ \n        annots = np.concatenate([labels, bboxes_yolo], axis=1) # labelsã®æ¨ªã«bboxes_yoloã‚’ãã£ã¤ã‘ã‚‹(bboxã®æ•°ã ã‘è¡ŒãŒã§ãã‚‹)\n        string = annot2str(annots) # annotationã‚’strã«ã—ã¦ã‚‹\n        f.write(string)\nprint('Missing:',cnt)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-03T12:23:49.853375Z","iopub.execute_input":"2022-02-03T12:23:49.853931Z","iopub.status.idle":"2022-02-03T12:23:54.691956Z","shell.execute_reply.started":"2022-02-03T12:23:49.853893Z","shell.execute_reply":"2022-02-03T12:23:54.69119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"æŒ‡å®šã—ãŸå ´æ‰€`kaggle/imageã¨ã‹label`ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã§ããªã„ãï¼Ÿï¼Ÿï¼Ÿ","metadata":{}},{"cell_type":"markdown","source":"# ğŸ“ Create Folds\n> Number of samples aren't same in each fold which can create large variance in **Cross-Validation**.\n\n> å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒåŒã˜ã§ãªã„ãŸã‚ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§å¤§ããªã°ã‚‰ã¤ããŒç”Ÿã˜ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\nGroupKFold: åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ãŒç•°ãªã‚‹åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å‡ºç¾ã—ãªã„ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åˆ†å‰²ã™ã‚‹ã€‚\nå‚è€ƒï¼šhttps://upura.hatenablog.com/entry/2018/12/04/224436\n\n> ã‚¯ãƒ©ã‚¹ã¨ã¯åˆ¥ã®æ¦‚å¿µã¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã¯å‡ç­‰ãª10ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚°ãƒ«ãƒ¼ãƒ—ã¯ãªã‹ãªã‹ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒä»˜ãã¥ã‚‰ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ä¾‹ãˆã°ã€ŒåŒã˜ãƒ¦ãƒ¼ã‚¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¾ã¨ã‚ã¦ãŠãã€ã¨ã„ã£ãŸä½¿ã„æ–¹ãŒæƒ³å®šã§ãã¾ã™ã€‚**åŒã˜ãƒ¦ãƒ¼ã‚¶ã®ãƒ‡ãƒ¼ã‚¿ãŒtrainã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨validationã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸¡è€…ã«å­˜åœ¨ã™ã‚‹ã¨ã€ä¸å½“ã«ç²¾åº¦ãŒé«˜ããªã‚‹æã‚ŒãŒã‚ã‚‹**ãŸã‚ã§ã™ã€‚\n\nä»Šå›ã¯å‹•ç”»ã®æ•°(`len(df['sequence'].unique())`ã®äº‹)","metadata":{}},{"cell_type":"markdown","source":"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold\n\n> class sklearn.model_selection.GroupKFold(n_splits=5)[source]\n>\n> ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã—ãªã„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŒã¤K-foldã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã®å¤‰å½¢ã€‚\n>\n> åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ãŒ2ã¤ã®ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã«ç¾ã‚Œã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆç•°ãªã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°ã¯ã€å°‘ãªãã¨ã‚‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®æ•°ã¨åŒã˜ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ï¼‰ã€‚\n>\n> å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã¯ã€ãã‚Œãã‚Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§ç•°ãªã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°ãŒã»ã¼åŒã˜ã¨ã„ã†æ„å‘³ã§ã€ã»ã¼ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã¾ã™ã€‚","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nkf = GroupKFold(n_splits = 5) # n_split: train,valã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ•°ã€‚å…ƒãƒ‡ãƒ¼ã‚¿ã‚’5ãƒ‘ã‚¿ãƒ¼ãƒ³ã®train,valã«åˆ†ã‘ã‚‹\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.sequence)): # sequence: å‹•ç”»ã®ã‚µãƒ–ã‚»ãƒƒãƒˆID(åŒã˜IDã®ç”»åƒ§ã¯åŒã˜å‹•ç”»)\n    df.loc[val_idx, 'fold'] = fold\ndisplay(df.fold.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:54.693614Z","iopub.execute_input":"2022-02-03T12:23:54.694126Z","iopub.status.idle":"2022-02-03T12:23:55.216792Z","shell.execute_reply.started":"2022-02-03T12:23:54.694084Z","shell.execute_reply":"2022-02-03T12:23:55.216052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# âš™ï¸ Configuration\nThe dataset config file requires\n1. The dataset root directory path and relative paths to `train / val / test` image directories (or *.txt files with image paths)\n2. The number of classes `nc` and \n3. A list of class `names`:`['cots']`\n\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€ä»¥ä¸‹ã®ã‚‚ã®ãŒå¿…è¦ã§ã™ã€‚\n1. 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã¨ï¼Œ`train / val / test` ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç›¸å¯¾ãƒ‘ã‚¹ (ã¾ãŸã¯ç”»åƒãƒ‘ã‚¹ã‚’å«ã‚€ *.txt ãƒ•ã‚¡ã‚¤ãƒ«)\n2. ã‚¯ãƒ©ã‚¹æ•° `nc` ã¨ \n3. ã‚¯ãƒ©ã‚¹å`:`['cots']`ã®ãƒªã‚¹ãƒˆã€‚","metadata":{}},{"cell_type":"code","source":"train_files = []\nval_files   = []\ntrain_df = df.query(\"fold!=@FOLD\")\nvalid_df = df.query(\"fold==@FOLD\")\ntrain_files += list(train_df.image_path.unique())\nval_files += list(valid_df.image_path.unique())\nlen(train_files), len(val_files)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:55.21797Z","iopub.execute_input":"2022-02-03T12:23:55.2183Z","iopub.status.idle":"2022-02-03T12:23:55.237717Z","shell.execute_reply.started":"2022-02-03T12:23:55.218263Z","shell.execute_reply":"2022-02-03T12:23:55.236713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Augmentation**","metadata":{"execution":{"iopub.status.busy":"2022-01-31T13:27:35.106346Z","iopub.execute_input":"2022-01-31T13:27:35.106754Z","iopub.status.idle":"2022-01-31T13:27:35.145508Z","shell.execute_reply.started":"2022-01-31T13:27:35.106708Z","shell.execute_reply":"2022-01-31T13:27:35.144592Z"}}},{"cell_type":"markdown","source":"train_dfã«å¯¾ã—ã¦DAã‚’ã‹ã‘ã‚‹\n\nä»Šå›ã¯bboxã®ä½ç½®ãŒå¤‰ã‚ã‚‹å‡¦ç†ã¯ã—ãªã„(label.txtã‚’æµç”¨ã—ãŸã„ãŸã‚)\n\nã§ããŸç”»åƒã¯`working/images/{video_id}-{video_frame}-aug.jpg`ã«å…¥ã‚Œã‚‹ã€‚\n\nãã®ç”»åƒã«å¯¾ã™ã‚‹ãƒ©ãƒ™ãƒ«ã¯å…ƒç”»åƒã®label.txtã‹ã‚‰æµç”¨`working/labels/{video_id}-{video_frame}-aug.txt`ã«å…¥ã‚Œã‚‹ã€‚\n\ntrain_dfã«è¡Œã‚’è¿½åŠ ã€‚(å…ƒç”»åƒã®è¡Œã‚’ã‚³ãƒ”ãƒ¼ã€‚image_path, label_pathã‚’â†‘ã®ã‚‚ã®ã«å¤‰ãˆã‚Œã°OKã®ã¯ãš)","metadata":{}},{"cell_type":"code","source":"import albumentations as A","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:55.238985Z","iopub.execute_input":"2022-02-03T12:23:55.239447Z","iopub.status.idle":"2022-02-03T12:23:56.344226Z","shell.execute_reply.started":"2022-02-03T12:23:55.239412Z","shell.execute_reply":"2022-02-03T12:23:56.343413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HE_HSV(A.ImageOnlyTransform):\n    def __init__(self, p: float = 0.5, always_apply=False):\n        super().__init__(always_apply, p)\n        \n    def apply(self, image,**params):\n        img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n        # Histogram equalisation on the V-channel\n        img_hsv[:, :, 2] = cv2.equalizeHist(img_hsv[:, :, 2])\n\n        # convert image back from HSV to RGB\n        image_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2RGB)\n\n        return image_hsv","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:56.345693Z","iopub.execute_input":"2022-02-03T12:23:56.34597Z","iopub.status.idle":"2022-02-03T12:23:56.352854Z","shell.execute_reply.started":"2022-02-03T12:23:56.345932Z","shell.execute_reply":"2022-02-03T12:23:56.351452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AUG_DATASET(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        \n    def coco2yolo(self, bboxes, image_height=720, image_width=1280):\n        \"\"\"\n        coco => [xmin, ymin, w, h]\n        yolo => [xmid, ymid, w, h] (normalized)\n        \"\"\"\n        bboxes = np.array(bboxes)\n        bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n        # normolizinig\n        bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n        bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n\n        # converstion (xmin, ymin) => (xmid, ymid)\n        bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n\n        return bboxes\n    \n    def coord_to_box(self, bouding_box, image):\n        box_yolo_format = []\n        height, width = image.shape[0], image.shape[1]\n        \n        if False: # CFG.use_coco2yolo\n            box_yolo_format = self.coco2yolo(bouding_box)\n            box_yolo_format = np.clip(box_yolo_format,0,1)\n            label = np.repeat([0],box_yolo_format.shape[0]).reshape(-1,1)\n            box_yolo_format = np.append(box_yolo_format,label, axis=1)\n        else:\n            for bb in bouding_box:\n                label = [max(0,bb[0]), max(0,bb[1]), min(bb[0]+bb[2], 1280), min(720,bb[1]+bb[3]), '0']\n                bbox_albu = A.convert_bbox_to_albumentations(label, source_format='pascal_voc', rows=height, cols=width)\n                bbox_yolo = A.convert_bbox_from_albumentations(bbox_albu, target_format='yolo', rows=height, cols=width, check_validity=True)\n                clip_box = [np.clip(value,0,1) for value in bbox_yolo[:-1]] + [bbox_yolo[-1]]\n                box_yolo_format.append(clip_box)\n        return box_yolo_format\n\n    def bbox_to_txt(self, bboxes):\n        \"\"\"\n        Convert a list of bbox into a string in YOLO format (to write a file).\n        @bboxes : numpy array of bounding boxes \n        return : a string for each object in new line: <object-class> <x> <y> <width> <height>\n        \"\"\"\n        txt=''\n        for index,l in enumerate(bboxes):\n            l = [str(x) for x in l[:4]]\n            l = ' '.join(l)\n            txt +=  '0' +' ' + l + '\\n'\n        return txt\n\n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,index):\n        row = self.df.iloc[index]\n        path = row['image_path']\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        #aug_index = row['aug_index']\n        list_info = path.split('/')\n        image_name = list_info[-2] + '_' + list_info[-1].split('.')[0]\n        box = row['bboxes']\n        bounding_box = self.coord_to_box(box, img)\n\n        if self.transform is not None:\n            res = self.transform(image=img, bboxes=bounding_box)\n            img = res['image']\n            bounding_box = res['bboxes']\n            \n        box_yolo_format = self.bbox_to_txt(bounding_box)\n        return img, box_yolo_format, image_name","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:56.354162Z","iopub.execute_input":"2022-02-03T12:23:56.354614Z","iopub.status.idle":"2022-02-03T12:23:57.409139Z","shell.execute_reply.started":"2022-02-03T12:23:56.354579Z","shell.execute_reply":"2022-02-03T12:23:57.408243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transforms():\n    return A.Compose([\n            HE_HSV(always_apply=True) # ç¾åœ¨ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦DAã‹ã‘ãŸå‰æã§train_dfã®è¡Œã‚’è¿½åŠ ã—ã¦ã„ã‚‹\n            ], bbox_params=A.BboxParams(format='yolo' , min_visibility=0.4,min_area=500))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:57.411024Z","iopub.execute_input":"2022-02-03T12:23:57.411587Z","iopub.status.idle":"2022-02-03T12:23:57.443381Z","shell.execute_reply.started":"2022-02-03T12:23:57.411543Z","shell.execute_reply":"2022-02-03T12:23:57.442472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"class AUGDATAã€€ã®image_pathå‘¨ã‚Šã®è¨­å®šã„ã˜ã‚‹å¿…è¦ã‚ã‚Š","metadata":{}},{"cell_type":"markdown","source":"ã§ããŸç”»åƒã¯`working/images/{video_id}-{video_frame}-aug.jpg`ã«å…¥ã‚Œã‚‹ã€‚\n\nãã®ç”»åƒã«å¯¾ã™ã‚‹ãƒ©ãƒ™ãƒ«ã¯å…ƒç”»åƒã®label.txtã‹ã‚‰æµç”¨`working/labels/{video_id}-{video_frame}-aug.txt`ã«å…¥ã‚Œã‚‹ã€‚\n\ntrain_dfã«è¡Œã‚’è¿½åŠ ã€‚(å…ƒç”»åƒã®è¡Œã‚’ã‚³ãƒ”ãƒ¼ã€‚image_path, label_pathã‚’â†‘ã®ã‚‚ã®ã«å¤‰ãˆã‚Œã°OKã®ã¯ãš)\n\n---\n\næœ€åˆã‹ã‚‰train_dfã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚=tran_aug_df\n\ntrain_aug_dfã«å¯¾ã—ã¦DAã‚’ã‹ã‘ã‚‹\n\nåŒã˜trainãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã™ã‚Œã°ok","metadata":{}},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:57.444839Z","iopub.execute_input":"2022-02-03T12:23:57.454692Z","iopub.status.idle":"2022-02-03T12:23:58.086109Z","shell.execute_reply.started":"2022-02-03T12:23:57.454648Z","shell.execute_reply":"2022-02-03T12:23:58.085333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if AUG is not None:\n    \n    train_df_he = train_df.copy(deep=True)\n    \n    # dataloaderã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ\n    dataset = AUG_DATASET(train_df_he, transform = get_transforms())\n    dataloader = Data.DataLoader(dataset=dataset, num_workers=WORKER, batch_size=BATCH, shuffle=False, drop_last=False,\\\n                               pin_memory = False)\n    \n    for aug_img, aug_box, image_name in progress_bar(dataloader):\n        for idx, image in enumerate(aug_img):\n            name = image_name[idx]\n            new_name = \"{}_HE\".format(name)\n            image = aug_img[idx]\n            box = aug_box[idx]\n            \n            path_txt = LABEL_DIR + \"/\" + new_name + \".txt\"\n            path_jpg = IMAGE_DIR + \"/\" + new_name + \".jpg\"\n            is_path = os.path.exists(path_jpg)\n            image = image.numpy()\n            cv2.imwrite(path_jpg, image[...,::-1])\n            txt_file = open(path_txt, \"w\")\n            txt_file.write(box)\n            txt_file.close()\n        break\n    \n    # train_dfã«DAã—ãŸè¡Œã‚’è¿½åŠ \n    func_he_path = lambda x: '{}_HE'.format(x)\n    train_df_he.image_path = train_df_he.image_path.map(func_he_path) # image_pathã‚’æ›´æ–°\n    train_df_he.label_path = train_df_he.label_path.map(func_he_path) # label_pathã‚’æ›´æ–°\n    train_df = pd.concat([train_df, train_df_he], axis=0, ignore_index=True) #indexå†åº¦é™ã‚Šç›´ã—","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:58.09462Z","iopub.execute_input":"2022-02-03T12:23:58.102573Z","iopub.status.idle":"2022-02-03T12:23:59.570004Z","shell.execute_reply.started":"2022-02-03T12:23:58.102526Z","shell.execute_reply":"2022-02-03T12:23:59.569143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df.image_path.unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:59.571871Z","iopub.execute_input":"2022-02-03T12:23:59.57214Z","iopub.status.idle":"2022-02-03T12:23:59.581864Z","shell.execute_reply.started":"2022-02-03T12:23:59.572102Z","shell.execute_reply":"2022-02-03T12:23:59.580969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train.txt, val.txtã‚’ä½œã‚‹(å­¦ç¿’ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹)","metadata":{}},{"cell_type":"code","source":"import yaml\n\ncwd = '/kaggle/working/'\n\nwith open(os.path.join( cwd , 'train.txt'), 'w') as f:\n    for path in train_df.image_path.tolist():\n        f.write(path+'\\n')\n            \nwith open(os.path.join(cwd , 'val.txt'), 'w') as f:\n    for path in valid_df.image_path.tolist():\n        f.write(path+'\\n')\n\ndata = dict(\n    path  = '/kaggle/working',\n    train =  os.path.join( cwd , 'train.txt') ,\n    val   =  os.path.join( cwd , 'val.txt' ),\n    nc    = 1,\n    names = ['cots'],\n    )\n\nwith open(os.path.join( cwd , 'gbr.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'gbr.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:23:59.583114Z","iopub.execute_input":"2022-02-03T12:23:59.583614Z","iopub.status.idle":"2022-02-03T12:23:59.600709Z","shell.execute_reply.started":"2022-02-03T12:23:59.583579Z","shell.execute_reply":"2022-02-03T12:23:59.599851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /kaggle/working/hyp.yaml\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+/- deg)\ntranslate: 0.10  # image translation (+/- fraction)\nscale: 0.5  # image scale (+/- gain)\nshear: 0.0  # image shear (+/- deg)\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\nflipud: 0.5  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.5  # image mosaic (probability)\nmixup: 0.5 # image mixup (probability)\ncopy_paste: 0.0  # segment copy-paste (probability)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:23:59.602822Z","iopub.execute_input":"2022-02-03T12:23:59.603311Z","iopub.status.idle":"2022-02-03T12:23:59.610246Z","shell.execute_reply.started":"2022-02-03T12:23:59.603276Z","shell.execute_reply":"2022-02-03T12:23:59.609483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ“¦ [YOLOv5](https://github.com/ultralytics/yolov5/)","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -r /kaggle/working/yolov5\n# !git clone https://github.com/ultralytics/yolov5 # clone\n!cp -r /kaggle/input/yolov5-lib-ds /kaggle/working/yolov5\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nfrom yolov5 import utils\ndisplay = utils.notebook_init()  # check","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:23:59.611601Z","iopub.execute_input":"2022-02-03T12:23:59.612073Z","iopub.status.idle":"2022-02-03T12:24:12.291077Z","shell.execute_reply.started":"2022-02-03T12:23:59.612038Z","shell.execute_reply":"2022-02-03T12:24:12.290195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸš… Training","metadata":{}},{"cell_type":"code","source":"!python train.py --img {DIM}\\\n--batch {BATCH}\\\n--epochs {EPOCHS}\\\n--data /kaggle/working/gbr.yaml\\\n--hyp /kaggle/working/hyp.yaml\\\n--weights {MODEL}.pt\\\n--optimizer {OPTIM}\\\n--project {PROJECT} --name {NAME}\\\n--exist-ok","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-03T12:24:12.292782Z","iopub.execute_input":"2022-02-03T12:24:12.293699Z","iopub.status.idle":"2022-02-03T12:27:53.010773Z","shell.execute_reply.started":"2022-02-03T12:24:12.29365Z","shell.execute_reply":"2022-02-03T12:27:53.009887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# âœ¨ Overview","metadata":{}},{"cell_type":"markdown","source":"## Output Files","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = '{}/{}'.format(PROJECT, NAME)\n!ls {OUTPUT_DIR}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:53.014875Z","iopub.execute_input":"2022-02-03T12:27:53.015386Z","iopub.status.idle":"2022-02-03T12:27:53.778407Z","shell.execute_reply.started":"2022-02-03T12:27:53.015347Z","shell.execute_reply":"2022-02-03T12:27:53.777615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ“ˆ Class Distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/labels_correlogram.jpg'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:53.781368Z","iopub.execute_input":"2022-02-03T12:27:53.781593Z","iopub.status.idle":"2022-02-03T12:27:54.602482Z","shell.execute_reply.started":"2022-02-03T12:27:53.781564Z","shell.execute_reply":"2022-02-03T12:27:54.601553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/labels.jpg'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:54.604338Z","iopub.execute_input":"2022-02-03T12:27:54.605045Z","iopub.status.idle":"2022-02-03T12:27:55.31483Z","shell.execute_reply.started":"2022-02-03T12:27:54.605003Z","shell.execute_reply":"2022-02-03T12:27:55.314145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ”­ Batch Image","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch0.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch1.jpg'))\n\nplt.figure(figsize = (10, 10))\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/train_batch2.jpg'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:55.316108Z","iopub.execute_input":"2022-02-03T12:27:55.316497Z","iopub.status.idle":"2022-02-03T12:27:57.813941Z","shell.execute_reply.started":"2022-02-03T12:27:55.316461Z","shell.execute_reply":"2022-02-03T12:27:57.813289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GT Vs Pred","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 2, figsize = (2*9,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'{OUTPUT_DIR}/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'{OUTPUT_DIR}/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'{OUTPUT_DIR}/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'{OUTPUT_DIR}/val_batch{row}_pred.jpg', fontsize = 12)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:57.815155Z","iopub.execute_input":"2022-02-03T12:27:57.815699Z","iopub.status.idle":"2022-02-03T12:27:58.923098Z","shell.execute_reply.started":"2022-02-03T12:27:57.81566Z","shell.execute_reply":"2022-02-03T12:27:58.921183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ğŸ” Result","metadata":{}},{"cell_type":"markdown","source":"## Score Vs Epoch","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/results.png'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:58.925154Z","iopub.status.idle":"2022-02-03T12:27:58.925502Z","shell.execute_reply.started":"2022-02-03T12:27:58.92533Z","shell.execute_reply":"2022-02-03T12:27:58.925352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.axis('off')\nplt.imshow(plt.imread(f'{OUTPUT_DIR}/confusion_matrix.png'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:58.929393Z","iopub.status.idle":"2022-02-03T12:27:58.929686Z","shell.execute_reply.started":"2022-02-03T12:27:58.929532Z","shell.execute_reply":"2022-02-03T12:27:58.929553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"for metric in ['F1', 'PR', 'P', 'R']:\n    print(f'Metric: {metric}')\n    plt.figure(figsize=(12,10))\n    plt.axis('off')\n    plt.imshow(plt.imread(f'{OUTPUT_DIR}/{metric}_curve.png'));\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-03T12:27:58.931083Z","iopub.status.idle":"2022-02-03T12:27:58.93149Z","shell.execute_reply.started":"2022-02-03T12:27:58.931267Z","shell.execute_reply":"2022-02-03T12:27:58.931289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please Upvote if you find this Helpful","metadata":{}},{"cell_type":"markdown","source":"# âœ‚ï¸ Remove Files","metadata":{}},{"cell_type":"code","source":"!rm -r {IMAGE_DIR}\n!rm -r {LABEL_DIR}","metadata":{"execution":{"iopub.status.busy":"2022-02-03T12:27:58.93316Z","iopub.status.idle":"2022-02-03T12:27:58.933588Z","shell.execute_reply.started":"2022-02-03T12:27:58.933364Z","shell.execute_reply":"2022-02-03T12:27:58.933388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://www.pngall.com/wp-content/uploads/2018/04/Under-Construction-PNG-File.png\">","metadata":{}}]}