{"cells":[{"metadata":{},"cell_type":"markdown","source":"* This notebook will show you how you can use Pytorch and a pretrained Resnet model to develop an algorithm that can help you compare 2 images.\n* Underlying concept is to convert a high dimensional image to a manageable representative set of features using a pretrained DNN. In this case I have chosen resnet18 (not resnet34/50/101/152 - because of hardware limitations imposed by my laptop)\n* At work I had the opportunity to evalutate multiple different models - VGG16, VGG19 and InceptionV3, with respect to a retail use case where given a set of apparel data from one retailer, I had to find exact matches in another. Resnet50 gave me the best accuracy - consistently for multiple retailers. And I found it to be resilient to changes in image background, illumination etc which was great.\n* The idea here tries to exploit a vector space and plots each image in the high-dimensional vector space and use cosine distance to evaluate the distance between any 2 vectors."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The idea here is to convert an image into a compressed 1D representation from a 2D image.\n# The input image is converted to a 512 element vector from which we could hope to use a similarity metric\n# to compare against other 2D images. Using Resnet to extract the compressed representation.\nmodel = models.resnet18(pretrained='imagenet')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"content_pd = pd.read_csv('../input/traincsv/train.csv')\ncontent_pd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Code here shows how one can tap on a specific layer on Resnet to extract the vectorized feature representation of an image. Once you manage to do this you will be able to use Cosine/Euclidean distances to measure similarity between 2 images."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resize the image to 224x224 px\nscaler = transforms.Scale((224, 224))\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\n# Use the model object to select the desired layer\nlayer = model._modules.get('avgpool')\n\ndef extract_feature_vector(img):\n    # 2. Create a PyTorch Variable with the transformed image\n    #Unsqueeze actually converts to a tensor by adding the number of images as another dimension.\n    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n\n    # 3. Create a vector of zeros that will hold our feature vector\n    #    The 'avgpool' layer has an output size of 512\n    my_embedding = torch.zeros(1, 512, 1, 1)\n\n    # 4. Define a function that will copy the output of a layer\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n\n    # 5. Attach that function to our selected layer\n    h = layer.register_forward_hook(copy_data)\n\n    # 6. Run the model on our transformed image\n    model(t_img)\n\n    # 7. Detach our copy function from the layer\n    h.remove()\n\n    # 8. Return the feature vector\n    return my_embedding.squeeze().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nfrom IPython.core.display import HTML \n\ndef display_category(urls, category_name):\n    img_style = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"\n    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.head(12).iteritems()])\n\n    display(HTML(images_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display a few images in the url\nids =  content_pd.landmark_id.value_counts().keys()[25]\nurls = content_pd[content_pd.landmark_id == ids].url\ndisplay_category(urls, 'My Favourite')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\n\n#Download the image into memory given the URL.\ndef get_image_from_url(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that you have the means for extracting the 1D representation from the given input image, you can use the cosine_similarity metric to compute the distance between\n# the given image and all other images and sort them in the ascending order and return the results.\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef get_cosine_distance(im_url1, im_url2):\n    im1, im2 = get_image_from_url(im_url1), get_image_from_url(im_url2)\n    image1 = extract_feature_vector(im1).reshape(1, -1)\n    image2 = extract_feature_vector(im2).reshape(1, -1)\n    return cosine_similarity(image1, image2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the distance between any 2 landmark images\nids =  content_pd.landmark_id.value_counts().keys()[1]\nurls = content_pd[content_pd.landmark_id == ids].url\nurls = [i for i in urls]\nurl1, url2 = urls[0], urls[1]\nprint('Distance between 2 images :', get_cosine_distance(url1, url2))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}