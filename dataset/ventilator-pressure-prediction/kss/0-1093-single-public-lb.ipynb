{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Credits: \n\nhttps://www.kaggle.com/steubk/tensorflow-bidirectional-lstm-custom-mae-loss (Masked MAE loss function) @steubk\n\nhttps://www.kaggle.com/c/ventilator-pressure-prediction/discussion/281299 (some features) @nityasevak","metadata":{}},{"cell_type":"markdown","source":"# Key points of solution:\n\n1. Magic features based on aggregations over R, C, rank and rounded u_in value (f1 - f6)\n2. Training the model in reversed order (from 80th timestemp to the 1st one) and including into the model features generated over the right order too (for instance at 80th timestep model sees u_in value of the 80th timestep as well as the value of the 1st timestep).\n3. Small features as Quantile Transformation of u_in values for u_out == 0, considering pressure values for u_out == 0 as negative values.\n4. Training time: ~17 hours on TPU 3.8","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport os\nimport random\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize, QuantileTransformer\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\n\npd.set_option('display.max_columns',None)\n\nprint(tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_features(df): \n     \n    df['sum_per_breath'] = df.groupby(['breath_id'])['u_in'].transform('sum') \n    \n    # magic features start\n    df['rounded_u_in'] = df['u_in'].round(0) \n    df['rank'] = df.groupby(['breath_id'])['time_step'].rank() \n    df['uid'] = df['R'].astype(str)+'_' + df['C'].astype(str) + '_' + df['rounded_u_in'].astype(str) + '_' + df['rank'].astype(str) \n    df['uid_count'] = df.groupby(['uid'])['uid'].transform('count') \n    df['f1'] = df.groupby(['uid'])['u_in'].transform('mean') \n    df['f2'] = df.groupby(['uid'])['u_in'].transform('min') \n    df['f3'] = df.groupby(['uid'])['u_in'].transform('max') \n    df['f4'] = df['u_in'] - df.groupby(['uid'])['u_in'].transform('mean') \n    df['f5'] = df['u_in'] - df.groupby(['uid'])['u_in'].transform('min') \n    df['f6'] = df['u_in'] - df.groupby(['uid'])['u_in'].transform('max') \n     \n     \n    del df['rounded_u_in'],df['rank'],df['uid'] \n    # magic features end\n     \n         \n    df['u_in_diff_1'] = df.groupby(['breath_id'])['u_in'].diff(1) \n    df['u_in_diff_2'] = df.groupby(['breath_id'])['u_in'].diff(2) \n    df['u_in_diff_3'] = df.groupby(['breath_id'])['u_in'].diff(3) \n     \n    df['u_in_diff_3'] = df['u_in_diff_3'].fillna(method='bfill') \n    df['u_in_diff_2'] = df['u_in_diff_2'].fillna(method='bfill') \n    df['u_in_diff_1'] = df['u_in_diff_1'].fillna(method='bfill') \n     \n    df['time_step_diff_1'] = df.groupby(['breath_id'])['time_step'].diff(1).fillna(0) \n    df['time_step_diff_1_r'] = df.groupby(['breath_id'])['time_step'].diff(-1).fillna(0)  \n    df['delta'] = df['time_step_diff_1'] * df['u_in'] \n    df['delta2'] = df['time_step_diff_1_r'] * df['u_in']  \n    df['time_step_diff_2'] = df.groupby(['breath_id'])['time_step'].diff(2).fillna(0)  \n    df['time_step_diff_3'] = df.groupby(['breath_id'])['time_step'].diff(3).fillna(0)  \n \n    df['area'] = df.groupby(['breath_id'])['delta'].cumsum() \n    df['area2'] = df.groupby(['breath_id'])['delta2'].cumsum()  \n    df['cross']= df['u_in']*df['u_out']  \n    df['cross2']= df['time_step']*df['u_out']  \n    df['u_in_cumsum'] = df.groupby(['breath_id'])['u_in'].cumsum() \n    df['area_cumsum'] = df.groupby(['breath_id'])['area'].cumsum()  \n    df['max_to_cumsum_u_in_per_breath_id'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in_cumsum']  \n     \n    df['u_in_shift_1_past'] = df.groupby(['breath_id'])['u_in'].shift(1).fillna(0)  \n    df['u_in_shift_2_past'] = df.groupby(['breath_id'])['u_in'].shift(2).fillna(0)  \n    df['u_in_shift_3_past'] = df.groupby(['breath_id'])['u_in'].shift(3).fillna(0)  \n     \n    df['time_step_shift_1_past'] = df.groupby(['breath_id'])['time_step'].shift(1).fillna(0)  \n    df['time_step_shift_2_past'] = df.groupby(['breath_id'])['time_step'].shift(2).fillna(0)  \n    df['time_step_shift_3_past'] = df.groupby(['breath_id'])['time_step'].shift(3).fillna(0)  \n \n    df['u_in_shift_1_future'] = df.groupby(['breath_id'])['u_in'].shift(-1).fillna(0)  \n    df['u_in_shift_2_future'] = df.groupby(['breath_id'])['u_in'].shift(-2).fillna(0)  \n    df['u_in_shift_3_future'] = df.groupby(['breath_id'])['u_in'].shift(-3).fillna(0) \n     \n    df['time_step_shift_1_future'] = df.groupby(['breath_id'])['time_step'].shift(-1).fillna(0)  \n    df['time_step_shift_2_future'] = df.groupby(['breath_id'])['time_step'].shift(-2).fillna(0)  \n    df['time_step_shift_3_future'] = df.groupby(['breath_id'])['time_step'].shift(-3).fillna(0) \n     \n    df['breath_id_u_out'] = df['breath_id'].astype(str) + '_' + df['u_out'].astype(str)  \n    df['count_breath_id_u_out'] = df.groupby(['breath_id_u_out'])['breath_id_u_out'].transform('count')  \n    del df['breath_id_u_out']  \n \n    df['mean_u_in_per_R_C_u_out'] = df.groupby(['R','C','u_out'])['u_in'].transform('mean')  \n    df['diff_mean_u_in_per_R_C_u_out'] = df['u_in'] - df['mean_u_in_per_R_C_u_out']  \n    df['to_mean_u_in_per_R_C_u_out'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['mean_u_in_per_R_C_u_out']  \n     \n    df['max_u_in_per_R_C_u_out'] = df.groupby(['R','C','u_out'])['u_in'].transform('max')  \n    df['diff_max_u_in_per_R_C_u_out'] = df['u_in'] - df['max_u_in_per_R_C_u_out']  \n    df['to_max_u_in_per_R_C_u_out'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['max_u_in_per_R_C_u_out'] \n \n    df['mean_u_out_per_breath_id'] = df.groupby(['breath_id'])['u_out'].transform('mean') \n     \n     \n    df['R_u_in'] = df['u_in'] * df['R']  \n    df['C_u_in'] = df['u_in'] * df['C'] \n    df['u_out_shift_1_past'] = df.groupby(['breath_id'])['u_out'].shift(1).fillna(0)  \n    df['u_out_shift_2_past'] = df.groupby(['breath_id'])['u_out'].shift(2).fillna(0)  \n    df['u_out_shift_3_past'] = df.groupby(['breath_id'])['u_out'].shift(3).fillna(0) \n    df['u_out_shift_1_future'] = df.groupby(['breath_id'])['u_out'].shift(-1).fillna(0)  \n    df['u_out_shift_2_future'] = df.groupby(['breath_id'])['u_out'].shift(-2).fillna(0)  \n    df['u_out_shift_3_future'] = df.groupby(['breath_id'])['u_out'].shift(-3).fillna(0) \n    \n    df['exponent']=(-1*df['time_step'])/(df['R']*df['C']) \n    df['factor']=np.exp(df['exponent']) \n    df['vf']=(df['u_in_cumsum']*df['R'])/df['factor'] \n    \n    df['R'] = df['R'].astype(str)  \n    df['C'] = df['C'].astype(str)  \n    df['R_C'] = df['R'].astype(str) + '_' + df['C'].astype(str)  \n    df = pd.get_dummies(df) \n \n    return df\n\ndef add_features2(df):\n     \n    df['rounded_u_in'] = df['u_in'].round(0) \n    df['rank'] = df.groupby(['breath_id'])['time_step'].rank() \n    df['uid'] = df['R'].astype(str)+'_' + df['C'].astype(str) + '_' + df['rounded_u_in'].astype(str) + '_' + df['rank'].astype(str) \n    df['uid_count'] = df.groupby(['uid'])['uid'].transform('count') \n    df['f1'] = df.groupby(['uid'])['u_in'].transform('mean') \n    df['f2'] = df.groupby(['uid'])['u_in'].transform('min') \n    df['f3'] = df.groupby(['uid'])['u_in'].transform('max') \n    df['f4'] = df['u_in'] - df.groupby(['uid'])['u_in'].transform('mean') \n    df['f5'] = df['u_in'] - df.groupby(['uid'])['u_in'].transform('min') \n    df['f6'] = df['u_in'] - df.groupby(['uid'])['u_in'].transform('max') \n     \n     \n    del df['rounded_u_in'],df['rank'],df['uid'] \n     \n         \n    df['u_in_diff_1'] = df.groupby(['breath_id'])['u_in'].diff(1)\n    df['u_in_diff_2'] = df.groupby(['breath_id'])['u_in'].diff(2)\n\n    df['u_in_diff_2'] = df['u_in_diff_2'].fillna(method='bfill')\n    df['u_in_diff_1'] = df['u_in_diff_1'].fillna(method='bfill')\n     \n    df['time_step_diff_1'] = df.groupby(['breath_id'])['time_step'].diff(1).fillna(0) \n    df['delta'] = df['time_step_diff_1'] * df['u_in']\n    \n    del df['time_step_diff_1']\n \n    df['area'] = df.groupby(['breath_id'])['delta'].cumsum()\n    del df['delta']\n    \n    df['u_in_cumsum'] = df.groupby(['breath_id'])['u_in'].cumsum() \n    df['area_cumsum'] = df.groupby(['breath_id'])['area'].cumsum()  \n     \n    df['u_in_shift_1_past'] = df.groupby(['breath_id'])['u_in'].shift(1).fillna(0)  \n    df['u_in_shift_2_past'] = df.groupby(['breath_id'])['u_in'].shift(2).fillna(0) \n \n    df['u_in_shift_1_future'] = df.groupby(['breath_id'])['u_in'].shift(-1).fillna(0)  \n    df['u_in_shift_2_future'] = df.groupby(['breath_id'])['u_in'].shift(-2).fillna(0)   \n     \n    df['u_out_shift_1_past'] = df.groupby(['breath_id'])['u_out'].shift(1).fillna(0)  \n    df['u_out_shift_2_past'] = df.groupby(['breath_id'])['u_out'].shift(2).fillna(0)\n    df['u_out_shift_1_future'] = df.groupby(['breath_id'])['u_out'].shift(-1).fillna(0)\n    \n    del df['R'], df['C']\n \n    return df\n\n\ndef GBVPP_loss(y_true, y_pred, cols = 80):\n    u_out = y_true[:, cols: ]\n    y = y_true[:, :cols ]\n\n    w = 1 - u_out\n    mae = w * tf.abs(y - y_pred)\n    return tf.reduce_sum(mae, axis=-1) / tf.reduce_sum(w, axis=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 0\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n    \nN_FOLDS = 15","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ori = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntrain_ori.loc[train_ori['u_out'] == 1, 'pressure'] = train_ori['pressure'] * (-1)\n\ntest_ori = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\ntrain_ori2 = train_ori.copy()\ntest_ori2 = test_ori.copy()\n\ntrain_ori['rank'] = train_ori.groupby(['breath_id'])['time_step'].rank()\ntrain_ori['neg_rank'] = -1 * train_ori['rank']\ntest_ori['rank'] = test_ori.groupby(['breath_id'])['time_step'].rank()\ntest_ori['neg_rank'] = -1 * test_ori['rank']\n\ntrain_ori = train_ori.sort_values(by=['breath_id','neg_rank']).reset_index(drop=True)\ntest_ori = test_ori.sort_values(by=['breath_id','neg_rank']).reset_index(drop=True)\n\ndel train_ori['rank'],train_ori['neg_rank'],test_ori['rank'],test_ori['neg_rank']\ngc.collect()\n\ndf = pd.concat([train_ori,test_ori],axis=0,copy=False).reset_index(drop=True)\ndf = add_features(df)\n\ntrain = df.iloc[:len(train_ori),:]\ntest = df.iloc[len(train_ori):,:].reset_index(drop=True)\ndel test['pressure']\ngc.collect()\n\ndf2 = pd.concat([train_ori2,test_ori2],axis=0,copy=False).reset_index(drop=True)\ndf2 = add_features2(df2)\n\ntrain2 = df2.iloc[:len(train_ori2),:]\ntest2 = df2.iloc[len(train_ori2):,:].reset_index(drop=True)\ndel train2['pressure'], test2['pressure'], df2\ngc.collect()\n\ndel train2['id'],train2['breath_id'],test2['id'],test2['breath_id']\ngc.collect()\n\ntrain2.columns = ['shifted_' + str(s) for s in train2.columns]\ntest2.columns = ['shifted_' + str(s) for s in test2.columns]\n\ntrain = pd.concat([train,train2],axis=1,copy=False)\ntest = pd.concat([test,test2],axis=1,copy=False)\n\ndel train2, test2\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\nu_outs = train[['u_out']].to_numpy().reshape(-1, 80)\ntrain.drop(['pressure','id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"QT = QuantileTransformer(random_state=0,output_distribution='normal',subsample=1000000)\nQT.fit(df[df['u_out']==0]['u_in'].values.reshape(-1,1))\n\ntrain['q_u_in'] = QT.transform(train['u_in'].values.reshape(-1,1))\ntest['q_u_in'] = QT.transform(test['u_in'].values.reshape(-1,1))\n\ntrain.loc[train['u_out']==0,'u_in'] = train['q_u_in']\ntest.loc[test['u_out']==0,'u_in'] = test['q_u_in']\ndel train['q_u_in'], test['q_u_in'], df\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RS = RobustScaler(quantile_range=(20.0, 80.0))\nRS.fit(train[train['u_out']==0])\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\ngc.collect()\n\nprint(train.shape, test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 300\nBATCH_SIZE = 512\n\ntf.keras.backend.clear_session()\n\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nwith tpu_strategy.scope():\n        kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=2021)\n        oof_preds = np.zeros((train.shape[0],train.shape[1]))\n        test_preds = []\n        for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n\n            checkpoint_path = f'repeat:Fold:{fold+1}.hdf5'\n            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0,\n                                 save_best_only = True, \n                                 save_weights_only = True,\n                                 mode = 'min')\n\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n            u_out_train, u_out_valid = u_outs[train_idx], u_outs[test_idx] \n            inp = keras.layers.Input(shape=train.shape[-2:])\n            x1 = keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True))(inp)\n            concat1 = keras.layers.concatenate([inp,x1])\n    \n            x2 = keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True))(concat1)\n            concat2 = keras.layers.concatenate([x1,x2])\n    \n            x3 = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(concat2)\n            concat3 = keras.layers.concatenate([x2,x3])\n    \n            x4 = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(concat3)\n            concat4 = keras.layers.concatenate([x3,x4])\n    \n            x5 = keras.layers.Dense(128, activation='selu')(concat4)\n            output = keras.layers.Dense(1)(x5)\n\n            model = keras.models.Model(inputs=inp, outputs=output)\n            model.compile(optimizer='Adam',loss=GBVPP_loss)\n    \n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n    \n            es = EarlyStopping(monitor=\"val_loss\", patience=25, verbose=1, mode=\"min\", restore_best_weights=True)\n    \n            model.fit(X_train, np.append(y_train, u_out_train, axis =1), \n                      validation_data=(X_valid, np.append(y_valid, u_out_valid, axis =1)),\n                      epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr,es,cb_checkpt])\n            model.load_weights(checkpoint_path)\n            valid_preds = model.predict(X_valid)\n            oof_preds[test_idx] = valid_preds.reshape(valid_preds.shape[0],valid_preds.shape[1])\n    \n            test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n            del X_train, X_valid, y_train, y_valid\n            gc.collect()\n            \n    oof_preds = oof_preds.squeeze().reshape(-1,1).squeeze()\n    reshaped_targets = targets.squeeze().reshape(-1,1).squeeze()\n    print(mae(reshaped_targets,oof_preds))\n    submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n    \nidx = train_ori[train_ori['u_out']==0].index\ntrain_ori['prediction']=oof_preds\nprint(mae(train_ori.loc[idx,'pressure'],train_ori.loc[idx,'prediction']))\npd.DataFrame(oof_preds).to_csv('oof_preds.csv',index=0)\nsubmission.to_csv('test_preds.csv',index=0)","metadata":{},"execution_count":null,"outputs":[]}]}