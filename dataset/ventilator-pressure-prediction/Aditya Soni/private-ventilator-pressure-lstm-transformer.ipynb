{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nDATA_DIR = \"../input/ventilator-pressure-prediction/\"\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:13.156511Z","iopub.execute_input":"2021-10-10T11:03:13.157215Z","iopub.status.idle":"2021-10-10T11:03:13.164559Z","shell.execute_reply.started":"2021-10-10T11:03:13.157173Z","shell.execute_reply":"2021-10-10T11:03:13.163789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\n\nclass CFG:\n    competition='ventilator'\n    apex=True\n    print_freq=1000\n    num_workers=4\n    model_name='rnn'\n    scheduler='cosine'   # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    batch_scheduler=True # Valid For CosineAnnealingWarmRestarts, cosine\n    num_warmup_steps=100 # ['linear', 'cosine']\n    num_cycles=0.5 # 'cosine'\n    factor=0.995   # ReduceLROnPlateau\n    patience=7     # ReduceLROnPlateau\n    eps=1e-6       # ReduceLROnPlateau\n    T_max=50       # CosineAnnealingLR\n    T_0=20         # CosineAnnealingWarmRestarts\n    epochs=200\n    max_grad_norm=1000\n    gradient_accumulation_steps=1\n    hidden_size=512\n    lr=1e-3\n    min_lr=3e-5\n    weight_decay=1e-6\n    batch_size=256\n    n_fold = 5\n    trn_fold=[0]\n    cate_seq_cols=[]\n    cont_seq_cols=['R', 'C', 'time_step', 'u_in', 'u_out']\n    train=True\n    inference=True\n    debug=False\n\nif CFG.debug:\n    CFG.epochs = 25\n    CFG.trn_fold=[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:14.300448Z","iopub.execute_input":"2021-10-10T11:03:14.300763Z","iopub.status.idle":"2021-10-10T11:03:14.315727Z","shell.execute_reply.started":"2021-10-10T11:03:14.300725Z","shell.execute_reply":"2021-10-10T11:03:14.314595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport sys\nimport json\nimport math\nimport random\nfrom time import time\nfrom datetime import datetime\nfrom collections import Counter, defaultdict\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nfrom tqdm.auto import tqdm\nimport category_encoders as ce\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#if CFG.apex:\n#    from apex import amp\n\nDEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(\"Python     : \" + sys.version.split(\"\\n\")[0])\nprint(\"Numpy      : \" + np.__version__)\nprint(\"Pandas     : \" + pd.__version__)\nprint(\"PyTorch    : \" + torch.__version__)\nprint(\"Running on device: {}\".format(DEVICE))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:18.890995Z","iopub.execute_input":"2021-10-10T11:03:18.893085Z","iopub.status.idle":"2021-10-10T11:03:28.935981Z","shell.execute_reply.started":"2021-10-10T11:03:18.89305Z","shell.execute_reply":"2021-10-10T11:03:28.935217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_trues, y_preds):\n    score = mean_absolute_error(y_trues, y_preds)\n    return score\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()\n\ndef decorate(s: str, decoration=None):\n    if decoration is None:\n        decoration = '★' * 20\n\n    return ' '.join([decoration, str(s), decoration])\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n        self.verbose = verbose\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        if self.verbose is None:\n            return\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:28.93781Z","iopub.execute_input":"2021-10-10T11:03:28.938116Z","iopub.status.idle":"2021-10-10T11:03:28.955432Z","shell.execute_reply.started":"2021-10-10T11:03:28.938056Z","shell.execute_reply":"2021-10-10T11:03:28.954702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data loading","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Data Loading\n# ====================================================\n\ntrain = pd.read_csv(DATA_DIR + 'train.csv')\n\nif CFG.debug:\n    train = train[:80*10_000]\n    gc.collect()\n\ntest = pd.read_csv(DATA_DIR + 'test.csv')\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\ngc.collect()\n\nall_pressure = sorted(train.pressure.unique())\nPRESSURE_MIN = np.min(all_pressure)\nPRESSURE_MAX = np.max(all_pressure)\nPRESSURE_STEP = all_pressure[1] - all_pressure[0]\n\ndisplay(train.head())\ndisplay(test.head())\ndisplay(sub.head())","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:28.956864Z","iopub.execute_input":"2021-10-10T11:03:28.957293Z","iopub.status.idle":"2021-10-10T11:03:41.910884Z","shell.execute_reply.started":"2021-10-10T11:03:28.957258Z","shell.execute_reply":"2021-10-10T11:03:41.910256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# create features","metadata":{}},{"cell_type":"code","source":"class AbstractBaseBlock:\n    def fit(self, input_df: pd.DataFrame, y=None):\n        return self.transform(input_df)\n\n    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n        raise NotImplementedError()\n\n\nclass AddMultiplyingDividing(AbstractBaseBlock):\n    def transform(self, input_df):\n        input_df['area'] = input_df['time_step'] * input_df['u_in']\n        input_df['area'] = input_df.groupby('breath_id')['area'].cumsum()\n        input_df['cross'] = input_df['u_in']*input_df['u_out']\n        input_df['cross2'] = input_df['time_step']*input_df['u_out']\n        input_df['u_in_cumsum'] = (input_df['u_in']).groupby(input_df['breath_id']).cumsum()\n        input_df['one'] = 1\n        input_df['count'] = (input_df['one']).groupby(input_df['breath_id']).cumsum()\n        input_df['u_in_cummean'] = input_df['u_in_cumsum'] / input_df['count']\n        input_df = input_df.merge(\n            input_df[input_df[\"u_out\"]==0].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out0_\").reset_index(),\n            on=\"breath_id\"\n        )\n        input_df = input_df.merge(\n            input_df[input_df[\"u_out\"]==1].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out1_\").reset_index(),\n            on=\"breath_id\"\n        )\n\n        output_df = pd.DataFrame(\n            {\n                \"area\": input_df['area'],\n                #\"cross\": input_df['cross'],\n                #\"cross2\": input_df['cross2'],\n                \"u_in_cumsum\": input_df['u_in_cumsum'],\n                \"u_in_cummean\": input_df['u_in_cummean'],\n                \"u_out0_mean\": input_df['u_out0_mean'],\n                \"u_out0_max\": input_df['u_out0_max'],\n                \"u_out0_max\": input_df['u_out0_std'],\n                \"u_out1_mean\": input_df['u_out1_mean'],\n                \"u_out1_max\": input_df['u_out1_max'],\n                \"u_out1_max\": input_df['u_out1_std'],\n            }\n        )\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass RCDummry(AbstractBaseBlock):\n    def transform(self, input_df):\n        input_df['R_dummy'] = input_df['R'].astype(str)\n        input_df['C_dummy'] = input_df['C'].astype(str)\n        #input_df['RC_dummy'] = input_df['R_dummy'] + input_df['C_dummy']\n        output_df = pd.get_dummies(input_df[[\"R_dummy\", \"C_dummy\"]])\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\n\nclass AddBreathTimeAndUInTime(AbstractBaseBlock):\n    def transform(self, input_df):\n        output_df = pd.DataFrame(\n            {\n                \"breath_time\": input_df['time_step'] - input_df['time_step'].shift(1),\n                \"u_in_time\": input_df['u_in'] - input_df['u_in'].shift(1)\n            }\n        )\n        output_df.loc[input_df['time_step'] == 0, 'breath_time'] = output_df['breath_time'].mean()\n        output_df.loc[input_df['time_step'] == 0, 'u_in_time'] = output_df['u_in_time'].mean()\n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df\n\nclass LagFeatures(AbstractBaseBlock):\n    def transform(self, input_df):\n        output_df = pd.DataFrame(\n            {\n                \"u_in_lag1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(1).fillna(0),\n                \"u_in_lag2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(2).fillna(0),\n                \"u_in_lag3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(3).fillna(0),\n                \"u_in_lag4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(4).fillna(0),\n                \n                \"u_in_lag-1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-1).fillna(0),\n                \"u_in_lag-2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-2).fillna(0),\n                \"u_in_lag-3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-3).fillna(0),\n                \"u_in_lag-4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-4).fillna(0),\n                \n                \"u_out_lag1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(1).fillna(0),\n                \"u_out_lag2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(2).fillna(0),\n                \"u_out_lag3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(3).fillna(0),\n                \"u_out_lag4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(4).fillna(0),\n                \n                #\"u_out_lag-1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-1).fillna(0),\n                #\"u_out_lag-2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-2).fillna(0),\n                #\"u_out_lag-3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-3).fillna(0),\n                #\"u_out_lag-4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-4).fillna(0),\n            }\n        )\n        output_df[\"u_in_lag1_diff\"] = output_df[\"u_in_lag1\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag2_diff\"] = output_df[\"u_in_lag2\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag3_diff\"] = output_df[\"u_in_lag3\"] - input_df[\"u_in\"]\n        output_df[\"u_in_lag4_diff\"] = output_df[\"u_in_lag4\"] - input_df[\"u_in\"]\n\n        output_df[\"u_in_rolling_mean2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).mean()[\"u_in\"].reset_index(drop=True)\n        output_df[\"u_in_rolling_mean4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).mean()[\"u_in\"].reset_index(drop=True)\n        output_df[\"u_in_rolling_mean10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).mean()[\"u_in\"].reset_index(drop=True)\n        \n        if not CFG.debug:\n            output_df[\"u_in_rolling_max2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_max4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_max10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).max()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_min10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).min()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).std()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).std()[\"u_in\"].reset_index(drop=True)\n            output_df[\"u_in_rolling_std10\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(10).std()[\"u_in\"].reset_index(drop=True)\n        \n        for col in output_df.columns:\n            output_df[col] = output_df[col].fillna(output_df[col].mean())\n        \n        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n        return output_df","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:41.914314Z","iopub.execute_input":"2021-10-10T11:03:41.91451Z","iopub.status.idle":"2021-10-10T11:03:41.95035Z","shell.execute_reply.started":"2021-10-10T11:03:41.914486Z","shell.execute_reply":"2021-10-10T11:03:41.949621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_blocks = [\n    AddMultiplyingDividing(),\n    AddBreathTimeAndUInTime(),\n    RCDummry(),\n    LagFeatures(),\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:41.951612Z","iopub.execute_input":"2021-10-10T11:03:41.952744Z","iopub.status.idle":"2021-10-10T11:03:41.960672Z","shell.execute_reply.started":"2021-10-10T11:03:41.952707Z","shell.execute_reply":"2021-10-10T11:03:41.959943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_blocks(input_df, blocks, y=None, test=False):\n    out_df = pd.DataFrame()\n\n    print(decorate('start run blocks...'))\n\n    with Timer(prefix='run test={}'.format(test)):\n        for block in feature_blocks:\n            with Timer(prefix='out_df shape: {} \\t- {}'.format(out_df.shape, str(block))):\n                if not test:\n                    out_i = block.fit(input_df.copy(), y=y)\n                else:\n                    out_i = block.transform(input_df.copy())\n\n            assert len(input_df) == len(out_i), block\n            name = block.__class__.__name__\n            out_df = pd.concat([out_df, out_i.add_suffix(f'@{name}')], axis=1)\n    print(f\"out_df shape: {out_df.shape}\")\n    return pd.concat([input_df, out_df], axis=1)\n\n\ntrain = run_blocks(train, blocks=feature_blocks)\ntest = run_blocks(test, blocks=feature_blocks, test=True)\n\nCFG.cont_seq_cols = list(set(CFG.cont_seq_cols))\n\ndisplay(train.head())\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2021-10-10T11:03:41.961874Z","iopub.execute_input":"2021-10-10T11:03:41.962614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# normalization","metadata":{}},{"cell_type":"code","source":"# we might want to avoid scaling the dummy cols\n\ntrain_col_order = [\"u_out\"] + train.columns.drop(\"u_out\").tolist()\ntest_col_order = [\"u_out\"] + test.columns.drop(\"u_out\").tolist()\n\ntrain = train[train_col_order]\ntest = test[test_col_order]\n\n\nscaler = RobustScaler()\nscaler_targets = [col for col in CFG.cont_seq_cols if col != \"u_out\"]\n\nprint(f\"Apply Standerd Scaler these columns: {scaler_targets}\")\n\nfor scaler_target in tqdm(scaler_targets):\n    scaler.fit(train.loc[:,[scaler_target]])\n    train.loc[:,[scaler_target]] = scaler.transform(train.loc[:,[scaler_target]])\n    test.loc[:,[scaler_target]] = scaler.transform(test.loc[:,[scaler_target]])\n    gc.collect()\n\n# display(train.head())\n# display(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# reshape","metadata":{}},{"cell_type":"code","source":"print(set(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).columns) - set(CFG.cont_seq_cols))\nprint(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).shape)\nprint(len(CFG.cont_seq_cols))\n\nX = np.float32(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\ny = np.float32(train[\"pressure\"]).reshape(-1, 80, 1)\n\nX_test = np.float32(test.drop([\"id\", \"breath_id\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# cv split","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# CV split\n# ====================================================\n# Fold = GroupKFold(n_splits=5)\n# groups = train['breath_id'].values\n# for n, (train_index, val_index) in enumerate(Fold.split(train, train['pressure'], groups)):\n#     train.loc[val_index, 'fold'] = int(n)\n# train['fold'] = train['fold'].astype(int)\n# print(train.groupby('fold').size())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss","metadata":{}},{"cell_type":"code","source":"class L1Loss_masked(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, preds, y, u_out):\n\n        mask = 1 - u_out\n        mae = torch.abs(mask * (y - preds))\n        mae = torch.sum(mae) / torch.sum(mask)\n\n        return mae","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# scaling layer\n\nclass my_round_func(torch.autograd.Function):\n    \n    @staticmethod\n    def forward(ctx, input):\n        return torch.round(input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input\n\n\nclass ScaleLayer(nn.Module):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = PRESSURE_MIN\n        self.max = PRESSURE_MAX\n        self.step = PRESSURE_STEP\n        self.my_round_func = my_round_func()\n\n    def forward(self, inputs):\n        steps = inputs.add(-self.min).divide(self.step)\n        int_steps = self.my_round_func.apply(steps)\n        rescaled_steps = int_steps.multiply(self.step).add(self.min)\n        clipped = torch.clamp(rescaled_steps, self.min, self.max)\n        return clipped\n\nPRESSURE_MIN, PRESSURE_MAX, PRESSURE_STEP","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomTransformerEncoderLayer(nn.Module):\n    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n    This standard encoder layer is inspired by the paper \"Attention Is All You Need\"\n    where we have removed the dropouts and reduced the 1024 internal DIMS as well.\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n        batch_first: If ``True``, then the input and output tensors are provided\n            as (batch, seq, feature). Default: ``False``.\n\n    Examples::\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n        >>> src = torch.rand(10, 32, 512)\n        >>> out = encoder_layer(src)\n\n    Alternatively, when ``batch_first`` is ``True``:\n        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n        >>> src = torch.rand(32, 10, 512)\n        >>> out = encoder_layer(src)\n    \"\"\"\n    \n    __constants__ = ['batch_first']\n    # batch_first is False here\n    \n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 layer_norm_eps=1e-5\n                ) -> None:\n        \n        factory_kwargs = {}\n        super(CustomTransformerEncoderLayer, self).__init__()\n        \n        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, **factory_kwargs)\n        \n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n\n        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n        \n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.activation = nn.SELU()\n\n    def forward(self, src, src_mask= None, src_key_padding_mask= None):\n        r\"\"\"Pass the input through the encoder layer.\n\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"\n        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        \n        self.cfg = cfg\n        self.hidden_size = self.cfg.hidden_size\n        \n        self.seq_emb = nn.Sequential(\n            nn.Linear(len(cfg.cont_seq_cols), self.hidden_size),\n            nn.LayerNorm(self.hidden_size),\n            nn.SELU(),\n        )\n        \n        self.lstm1 = nn.LSTM(\n            self.hidden_size, self.hidden_size//2, dropout=0.1, batch_first=True, bidirectional=True\n        )\n        \n        transformer_encoder_layer = CustomTransformerEncoderLayer(\n            d_model=self.hidden_size, nhead=8, dim_feedforward=256, dropout=0.1,\n                 layer_norm_eps=1e-5,\n        )\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer = transformer_encoder_layer,\n            num_layers = 2,\n        )\n\n        self.head = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size//4),\n            nn.SELU(),\n            nn.Linear(self.hidden_size // 4, 1),\n            ScaleLayer(),\n        )\n        \n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n            elif isinstance(m, nn.GRU):\n                print(f\"init {m}\")\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        init.orthogonal_(param.data)\n                    else:\n                        init.normal_(param.data)\n            elif isinstance(m, nn.TransformerEncoder):\n                print(f\"init {m}\")\n                for param in m.parameters():\n                    if param.dim() >= 2:\n                        nn.init.xavier_uniform_(param.data)\n   \n    def generate_mask(self, size, diagonal=1):        \n        return torch.triu(torch.ones(size, size)==1, diagonal=diagonal)\n    \n    def forward(self, cont_seq_x):\n        bs = cont_seq_x.size(0)\n        \n        seq_emb = self.seq_emb(cont_seq_x)\n        seq_emb, _ = self.lstm1(seq_emb) # batch, seq_len, num_directions * hidden_size\n        \n        # transformer takes as input src: (S, N, E), so we have to permute it...\n        seq_emb = seq_emb.transpose(1, 0) # seq_len, batch, num_directions * hidden_size\n        \n        # src_mask = self.generate_mask(seq_emb.shape[0]).to(\"cuda\")\n        seq_emb = self.transformer(seq_emb, mask=None, src_key_padding_mask=None)\n        \n        seq_emb = seq_emb.transpose(1, 0) # (BS, seq_len, embedding_dim)\n        output = self.head(seq_emb) #.view(bs, -1)\n        return output\n\n# print(CustomModel(CFG))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# helper function","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# helper function\n# ====================================================\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\nscaler = GradScaler()\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train()\n    losses = AverageMeter()\n    start = end = time()\n    iters = len(train_loader)\n    \n    for step, (inputs, y) in enumerate(train_loader):\n        \n        inputs, y = inputs.to(device), y.to(device)\n        batch_size = inputs.size(0)\n        \n        with autocast():\n            pred = model(inputs)\n            loss = criterion(pred, y, inputs[:,:,0].reshape(-1, 80, 1))\n        \n        losses.update(loss.item(), batch_size)\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        \n        if CFG.apex:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n        \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        \n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            if CFG.apex:\n                scaler.step(optimizer)\n            else:\n                optimizer.step()\n            \n            optimizer.zero_grad()\n            lr = 0\n            \n            if CFG.batch_scheduler:\n                # scheduler.step(epoch + step/iters)\n                scheduler.step()\n                lr = scheduler.get_lr()[0]\n        \n        if CFG.apex:\n            scaler.update()\n        \n        end = time()\n    \n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    model.eval()\n    preds = []\n    losses = AverageMeter()\n    start = end = time()\n    \n    for step, (inputs, y) in enumerate(valid_loader):\n        \n        inputs, y = inputs.to(device), y.to(device)\n        batch_size = inputs.size(0)\n        \n        with torch.no_grad():\n            pred = model(inputs)\n        \n        loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n        losses.update(loss.item(), batch_size)\n        \n        preds.append(pred.view(-1).detach().cpu().numpy())\n        \n        if CFG.gradient_accumulation_steps > 1:\n            loss = loss / CFG.gradient_accumulation_steps\n        \n        end = time()\n    \n    preds = np.concatenate(preds)\n    return losses.avg, preds\n\n\ndef inference_fn(test_loader, model, device):\n    model.eval()\n    model.to(device)\n    preds = []\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    for step, (cont_seq_x) in tk0:\n        cont_seq_x = cont_seq_x.to(device)\n        with torch.no_grad():\n            pred = model(cont_seq_x)\n        preds.append(pred.view(-1).detach().cpu().numpy())\n    preds = np.concatenate(preds)\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# train loop\n# ====================================================\ndef train_loop(folds, fold, trn_idx, val_idx):\n\n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================    \n    \n    train_folds = X[trn_idx]\n    valid_folds = X[val_idx]\n    \n    y_train = y[trn_idx]\n    y_true = y[val_idx]\n    \n    groups = train[\"breath_id\"].unique()[val_idx]\n    oof_folds = train[train[\"breath_id\"].isin(groups)].reset_index(drop=True)\n    oof_folds = oof_folds[[\"breath_id\", \"time_step\", \"pressure\", \"u_out\"]]\n    gc.collect()\n    \n    train_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(train_folds),\n        torch.from_numpy(y_train)\n    )\n    \n    valid_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(valid_folds),\n        torch.from_numpy(y_true)\n    )\n\n    train_loader = DataLoader(train_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=True,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n    \n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=CFG.batch_size,\n                              shuffle=False,\n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = CustomModel(CFG)\n    model.to(device)\n    print(model)\n\n    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n    num_warmup_steps = int(.10*num_train_steps)\n    CFG.num_warmup_steps = num_warmup_steps\n    \n    print(\"warmup_steps is applicable is \", num_warmup_steps)\n    \n    def get_scheduler(optimizer):\n        if CFG.scheduler=='linear':\n            scheduler = get_linear_schedule_with_warmup(\n                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n            )\n        elif CFG.scheduler=='cosine':\n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n            )\n        elif CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # apex\n    # ====================================================\n    #if CFG.apex:\n    #    model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = L1Loss_masked()\n\n    best_score = np.inf\n\n    avg_losses = []\n    avg_val_losses = []\n    \n    for epoch in range(CFG.epochs):\n\n        start_time = time()\n\n        # train\n        # avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, None, device) # no scheduler?\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n        avg_losses.append(avg_loss)\n        \n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        avg_val_losses.append(avg_val_loss)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            pass\n\n        # scoring\n        score = avg_val_loss\n\n        elapsed = time() - start_time\n\n        best_notice = \"\"\n        if score < best_score:\n            best_notice = \"Best Score\"\n            best_score = score\n            torch.save(\n                {\n                    'model': model.state_dict(),\n                    'preds': preds,\n                },\n                OUTPUT_DIR+f\"fold{fold}_best.pth\"\n            )\n    \n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s, lr: {optimizer.param_groups[0][\"lr\"]:.5f}, MAE Score: {score:.4f}, {best_notice}')\n\n    plt.figure(figsize=(14,6))\n    plt.plot(avg_losses, label=\"Train Loss\", color=\"blue\")\n    plt.plot(avg_val_losses, label=\"Valid Loss\", color=\"red\")\n    plt.title(f\"Fold {fold + 1} - Best score {best_score:.4f}\", size=18)\n    plt.legend(loc=\"best\")\n    plt.show(block=False)\n\n    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n    oof_folds['preds'] = preds.flatten()\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return oof_folds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# main\n# ====================================================\n\ndef main():\n    \n    \"\"\"\n    Prepare: 1.train 2.test\n    \"\"\"\n    \n    def get_result(result_df):\n        result_df.reset_index(drop=True, inplace=True)\n        preds = result_df['preds'].values\n        labels = result_df['pressure'].values\n        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n        LOGGER.info(f'Score (without expiratory phase): {score:<.6f}')\n    \n    if CFG.train:\n        # train\n        oof_df = pd.DataFrame()\n        kfold = KFold(n_splits=CFG.n_fold, random_state=42, shuffle=True)\n        \n        for fold, (trn_idx, val_idx) in enumerate(kfold.split(X=X, y=y)):\n            if fold in CFG.trn_fold:\n                _oof_df = train_loop(X, fold, trn_idx, val_idx)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        \n        # CV result\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        \n        # save result\n        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n    \n    for i, breath_id in enumerate(oof_df[\"breath_id\"].unique()):\n        oof_df[oof_df[\"breath_id\"]==breath_id].plot(\n            x=\"time_step\", \n            y=[\"preds\", \"pressure\", \"u_out\"],\n            figsize=(16, 5),\n            title = f\"Preds Plot for breath_id : {breath_id}\",\n        )\n        plt.show(block=False)\n        if i == 15:\n            break\n    \n    if CFG.inference:\n        test_loader = torch.utils.data.DataLoader(X_test, batch_size=512, shuffle=False, pin_memory=True)\n        \n        for fold in CFG.trn_fold:\n            model = CustomModel(CFG)\n            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n            state = torch.load(path, map_location=torch.device('cpu'))\n            model.load_state_dict(state['model'])\n            predictions = inference_fn(test_loader, model, device)\n            test[f'fold{fold}'] = predictions\n            del state, predictions; gc.collect()\n            torch.cuda.empty_cache()\n        \n        # submission\n        test['pressure'] = test[[f'fold{fold}' for fold in CFG.trn_fold]].mean(1)\n        test[['id', 'pressure'] + [f'fold{fold}' for fold in CFG.trn_fold]].to_csv(OUTPUT_DIR+'raw_submission.csv', index=False)\n        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X, X_test\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_result_oof(result_df):\n    # reset_index is important.\n    result_df.reset_index(drop=True, inplace=True)\n    preds = result_df['preds'].values\n    labels = result_df['pressure'].values\n    non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n    score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_df = pd.read_csv(\"oof_df.csv\", usecols=[\"breath_id\", \"pressure\", \"preds\", \"u_out\"])\nunique_breath_ids = set(oof_df[\"breath_id\"].unique())\noof_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nall_breath_ids_scores = []\nfor breath_id in tqdm(unique_breath_ids, total=len(unique_breath_ids), desc=\"computing mae's for all OOF\"):\n    _result_df = oof_df[oof_df[\"breath_id\"] == breath_id]\n    all_breath_ids_scores.append((breath_id, get_result_oof(_result_df)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(all_breath_ids_scores, columns=[\"breath_id\", \"mae_oof\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sort_values(by=\"mae_oof\", ascending=False, inplace=True)\ndf.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.express as px\n\nfig = px.scatter(\n    data_frame=df[df[\"mae_oof\"] >=1], x=\"breath_id\", y=\"mae_oof\", color=\"mae_oof\", hover_data={'breath_id':':.6d', 'mae_oof': ':.4f'},\n)\nfig.show(block=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.scatter(\n    data_frame=df, x=\"breath_id\", y=\"mae_oof\", color=\"mae_oof\", hover_data={'breath_id':':.6d', 'mae_oof': ':.4f'},\n)\nfig.show(block=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import _LRScheduler\n\nclass NoamLR(_LRScheduler):\n    \"\"\"\n    Implements the Noam Learning rate schedule. This corresponds to increasing the learning rate\n    linearly for the first ``warmup_steps`` training steps, and decreasing it thereafter proportionally\n    to the inverse square root of the step number, scaled by the inverse square root of the\n    dimensionality of the model. \n    Time will tell if this is just madness or it's actually important.\n    \n    Parameters\n    ----------\n    warmup_steps: ``int``, required.\n        The number of steps to linearly increase the learning rate.\n    \"\"\"\n\n    def __init__(self, optimizer, warmup_steps):\n        self.warmup_steps = warmup_steps\n        super().__init__(optimizer)\n\n    def get_lr(self):\n        last_epoch = max(1, self.last_epoch)\n        scale = self.warmup_steps ** 0.5 * min(last_epoch ** (-0.5), last_epoch * self.warmup_steps ** (-1.5))\n        return [base_lr * scale for base_lr in self.base_lrs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nThe general idea of hard example mining is once the loss(and gradients) are computed for every sample in the batch, \nyou sort batch samples in the descending order of losses and pick top-k samples from it and do backward pass only for those k samples. \ni.e samples which contribute to more learning(aka hard example).\nThis ensures that samples which do not contribute much to the network’s learning is ignored.\n\"\"\";","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"kaggle commit done!\"}' https://hooks.slack.com/services/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","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}