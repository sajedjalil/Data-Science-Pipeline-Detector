{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import RepeatedKFold\nfrom datetime import datetime","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-10T19:21:58.767819Z","iopub.execute_input":"2021-10-10T19:21:58.768286Z","iopub.status.idle":"2021-10-10T19:22:05.318735Z","shell.execute_reply.started":"2021-10-10T19:21:58.768159Z","shell.execute_reply":"2021-10-10T19:22:05.317819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading in Data","metadata":{}},{"cell_type":"code","source":"nrows=None # ~10k row rapid model iteration\n\ntrain_data = pd.read_csv(\"../input/ventilator-pressure-prediction/train.csv\", nrows=nrows)\ntest_data = pd.read_csv(\"../input/ventilator-pressure-prediction/test.csv\", nrows=nrows)\nsubmission = pd.read_csv(\"../input/ventilator-pressure-prediction/sample_submission.csv\", nrows=nrows)\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T19:23:08.481904Z","iopub.execute_input":"2021-10-10T19:23:08.482255Z","iopub.status.idle":"2021-10-10T19:23:12.713867Z","shell.execute_reply.started":"2021-10-10T19:23:08.48222Z","shell.execute_reply":"2021-10-10T19:23:12.712668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\nAdding some additional features, removing unncessary features, and normalizing data using RobustScaler","metadata":{}},{"cell_type":"code","source":"def difference_operator(df, feature):\n    col_name = f\"{feature}_diff\"\n\n    # (next point - previous point) / (next time / previous time) ~= d/dt at the point\n    # iterate for further derivatives\n    df[col_name] = (\n        df[feature].shift(-1).fillna(method=\"ffill\")\n        - df[feature].shift(1).fillna(method=\"bfill\")\n    ) / (\n        df[\"time_step\"].shift(-1).fillna(method=\"ffill\")\n        - df[\"time_step\"].shift(1).fillna(method=\"bfill\")\n    )\n\n    return df\n\n\ndef add_features(df):\n    # Desc Stats\n    df[\"u_in_mean\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"mean\")\n    df[\"u_in_median\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"median\")\n    df[\"u_in_min\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"min\")\n    df[\"u_in_max\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"max\")\n    df[\"u_in_delta\"] = df[\"u_in_max\"] - df[\"u_in_min\"]\n    df[\"first_value_u_in\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"first\")\n    df[\"last_value_u_in\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"last\")\n\n    # Leads and Lags\n    df[\"u_in_lag1\"] = df.groupby(\"breath_id\")[\"u_in\"].shift(1).fillna(method=\"bfill\")\n    df[\"u_in_lag_back1\"] = (df.groupby(\"breath_id\")[\"u_in\"].shift(-1).fillna(method=\"ffill\"))\n    df[\"u_in_lag2\"] = df.groupby(\"breath_id\")[\"u_in\"].shift(2).fillna(method=\"bfill\")\n    df[\"u_in_lag_back2\"] = (df.groupby(\"breath_id\")[\"u_in\"].shift(-2).fillna(method=\"ffill\"))\n    df[\"u_in_lag3\"] = df.groupby(\"breath_id\")[\"u_in\"].shift(3).fillna(method=\"bfill\")\n    df[\"u_in_lag_back3\"] = (df.groupby(\"breath_id\")[\"u_in\"].shift(-3).fillna(method=\"ffill\"))\n\n    df[\"time_lag\"] = (df.groupby(\"breath_id\")[\"time_step\"].shift(1).fillna(method=\"bfill\"))\n    df[\"time_lag_back\"] = (df.groupby(\"breath_id\")[\"time_step\"].shift(-1).fillna(method=\"ffill\"))\n\n    # Derivatives at the point.\n    difference_operator(df, \"u_in\")\n    difference_operator(df, \"u_in_diff\")\n    difference_operator(df, \"u_in_diff_diff\")\n    difference_operator(df, \"u_in_diff_diff_diff\")\n\n    # Area under u_in curve\n    df[\"area\"] = df[\"time_lag_back\"] * df[\"u_in\"]\n    df[\"area_uout_open\"] = df[\"time_lag_back\"] * df[\"u_in\"] * df[\"u_out\"]\n\n    df[\"tot_area\"] = df.groupby([\"breath_id\"])[\"area\"].transform(\"sum\")\n    df[\"tot_area_uout_open\"] = df.groupby([\"breath_id\"])[\"area_uout_open\"].transform(\"sum\")\n    df[\"tot_area_cum_sum\"] = df.groupby([\"breath_id\"])[\"area\"].cumsum()\n\n    # COMBINE R AND C\n    df[\"R__C\"] = df[\"R\"].astype(str) + \"__\" + df[\"C\"].astype(str)\n\n    # One Hot Encoding of R, C and R__C\n    df = df.merge(pd.get_dummies(df[\"R\"], prefix=\"R\"), left_index=True, right_index=True).drop([\"R\"], axis=1)\n    df = df.merge(pd.get_dummies(df[\"C\"], prefix=\"C\"), left_index=True, right_index=True).drop([\"C\"], axis=1)\n    df = df.merge(pd.get_dummies(df[\"R__C\"], prefix=\"R__C\"), left_index=True, right_index=True).drop([\"R__C\"], axis=1)\n\n    # https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/273974\n    df[\"u_in_cumsum\"] = df.groupby([\"breath_id\"])[\"u_in\"].cumsum()\n    return df\n\n\ndef remove_features(df):\n    drop_list = [\"pressure\", \"id\", \"breath_id\", \"u_out\"]\n    drop_list = [feat for feat in drop_list if feat in df.columns]\n    df.drop(drop_list, axis=1, inplace=True)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:40:12.633956Z","iopub.execute_input":"2021-10-02T03:40:12.634288Z","iopub.status.idle":"2021-10-02T03:40:12.662729Z","shell.execute_reply.started":"2021-10-02T03:40:12.634249Z","shell.execute_reply":"2021-10-02T03:40:12.661882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets = train_data[[\"pressure\"]].to_numpy()\n\n# drop some unneeded features\ntrain_df = remove_features(add_features(train_data))\ntest_df = remove_features(add_features(test_data))\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:40:12.664054Z","iopub.execute_input":"2021-10-02T03:40:12.66431Z","iopub.status.idle":"2021-10-02T03:41:12.48843Z","shell.execute_reply.started":"2021-10-02T03:40:12.66427Z","shell.execute_reply":"2021-10-02T03:41:12.487387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RS = RobustScaler()\ntrain_df = RS.fit_transform(train_df)\ntest_df = RS.transform(test_df)\n\ntrain_df[:5]","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:41:12.491088Z","iopub.execute_input":"2021-10-02T03:41:12.491343Z","iopub.status.idle":"2021-10-02T03:41:22.579597Z","shell.execute_reply.started":"2021-10-02T03:41:12.491302Z","shell.execute_reply":"2021-10-02T03:41:22.57869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Setup - KFold Validation\n","metadata":{}},{"cell_type":"code","source":"# training params\nn_splits=3\nn_repeats=1\nepochs = 100\n\nkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)\n\ntest_preds = []","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:41:22.580806Z","iopub.execute_input":"2021-10-02T03:41:22.581039Z","iopub.status.idle":"2021-10-02T03:41:27.358585Z","shell.execute_reply.started":"2021-10-02T03:41:22.581013Z","shell.execute_reply":"2021-10-02T03:41:27.357548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building and Training the Model","metadata":{}},{"cell_type":"code","source":"for fold, fold_indices in enumerate(kf.split(train_df, targets)):\n\n    x_train, x_valid = train_df[fold_indices[0]], train_df[fold_indices[1]]\n    y_train, y_valid = targets[fold_indices[0]], targets[fold_indices[1]]\n\n    start_units = 256 # 128 for rapid model development, 512 seems to be the point of diminishing returns\n    model = keras.models.Sequential(\n        [\n            keras.layers.Dense(units=start_units, input_dim=x_train.shape[1], activation=\"relu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.2),\n            keras.layers.Dense(units=start_units, input_dim=x_train.shape[1], activation=\"relu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.2),\n            keras.layers.Dense(units=start_units / 2, activation=\"relu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            keras.layers.Dense(units=start_units / 2, activation=\"relu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.1),\n            keras.layers.Dense(units=start_units / 4, activation=\"relu\"),\n            keras.layers.Dense(units=1, activation=\"linear\"),\n        ],\n        name=f\"fold_{fold}_dnn\",\n    )\n\n    model.summary()\n\n    optimizer = keras.optimizers.Adam()\n\n    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n\n    # save checkpoints from internal epochs\n    checkpoint_name = \"checkpoints/checkpoints-{epoch:03d}--{val_loss:.5f}.hdf5\"\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        checkpoint_name, \n        monitor=\"val_loss\", \n        verbose=1, \n        save_best_only=True, \n        mode=\"auto\"\n    )\n    callbacks_list = [checkpoint]\n\n    model.fit(\n        x_train,\n        y_train,\n        validation_data=(x_valid, y_valid),\n        epochs=epochs,\n        batch_size=1024,\n        callbacks=callbacks_list,\n        verbose=1,\n    )\n    \n    # save final model\n    model.save(f\"models/dnn_vp_fold_{fold}_{datetime.now()}\")\n    \n    # save preds from final model for the given fold\n    test_preds.append(model.predict(test_df).squeeze().reshape(-1, 1).squeeze())","metadata":{"execution":{"iopub.status.busy":"2021-10-02T03:41:27.35974Z","iopub.execute_input":"2021-10-02T03:41:27.35998Z","iopub.status.idle":"2021-10-02T03:41:27.618742Z","shell.execute_reply.started":"2021-10-02T03:41:27.359953Z","shell.execute_reply":"2021-10-02T03:41:27.616888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Predictions\n\nBuild and create csv of test submission details","metadata":{}},{"cell_type":"code","source":"submission[\"pressure\"] = sum(test_preds)/n_splits/n_repeats\nsubmission.to_csv(\"submission.csv\", index=False)\n\n\nprint(submission.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}