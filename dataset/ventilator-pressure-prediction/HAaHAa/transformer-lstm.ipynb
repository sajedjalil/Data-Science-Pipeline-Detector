{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time, logging, gc\nfrom sklearn.preprocessing import RobustScaler\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import *","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-28T04:44:33.2445Z","iopub.execute_input":"2021-10-28T04:44:33.245281Z","iopub.status.idle":"2021-10-28T04:44:38.894356Z","shell.execute_reply.started":"2021-10-28T04:44:33.245172Z","shell.execute_reply":"2021-10-28T04:44:38.893376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:44:38.896218Z","iopub.execute_input":"2021-10-28T04:44:38.896456Z","iopub.status.idle":"2021-10-28T04:44:50.983629Z","shell.execute_reply.started":"2021-10-28T04:44:38.896421Z","shell.execute_reply":"2021-10-28T04:44:50.982706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:44:50.984762Z","iopub.execute_input":"2021-10-28T04:44:50.984971Z","iopub.status.idle":"2021-10-28T04:44:50.997984Z","shell.execute_reply.started":"2021-10-28T04:44:50.984948Z","shell.execute_reply":"2021-10-28T04:44:50.99702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy from https://www.kaggle.com/hijest/gaps-features-tf-lstm-resnet-like-ff/notebook?scriptVersionId=77500256&cellId=3\ndef add_features(df): #CV 0.1579\n    \n    df['u_in'] = np.sqrt(2*df['u_in'])\n    \n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['state'] = np.array([1 if x>0 else 0 for x in df['u_in']]) - df['u_out']\n    \n    df['exhale'] = df.groupby('breath_id')['u_out'].cumsum()\n        \n    df['delta_time'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta_u_in'] = df.groupby(df['breath_id'])['u_in'].diff().fillna(0).reset_index(level=0,drop=True)     \n            \n    df['u_in_1st_derivative'] = (df['delta_u_in'] /df['delta_time']).fillna(0)\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1).fillna(0).reset_index(level=0,drop=True)   \n    #df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_back1'] = df.groupby('breath_id')['u_in'].shift(-1).fillna(0).reset_index(level=0,drop=True)   \n    #df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_back3'] = df.groupby('breath_id')['u_in'].shift(-3).fillna(0).reset_index(level=0,drop=True)\n    #df['u_in_lag6'] = df.groupby('breath_id')['u_in'].shift(6).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n#     df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n#     df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    \n    df['volume_mean']=((df['u_in'] + df['u_in'].shift(1).fillna(0))/2 * df['time_step'].diff().fillna(0)).clip(0,)\n      \n    df['volume_mean_10']=df.groupby(df['breath_id'])['volume_mean'].rolling(window=10, min_periods=1).sum().reset_index(level=0,drop=True)\n    df['volume_mean_20']=df.groupby(df['breath_id'])['volume_mean'].rolling(window=20, min_periods=1).sum().reset_index(level=0,drop=True)\n       \n    df['volume_in_cumsum']=df.groupby('breath_id')['volume_mean'].cumsum()     \n        \n    df['_volume'] = df['volume_in_cumsum'] * (1 - df['u_out'])\n    df['tidal_volume']=df.groupby(df['breath_id'])['_volume'].transform('max')\n    df['volume_out'] = -(df['tidal_volume']/ np.log(df['exhale'])).replace(np.inf,0)\n    \n    df['volume_out_cumsum']=df.groupby('breath_id')['volume_out'].cumsum() \n    \n    #df['u_in_rol_q0.1'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.1).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.25'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.25).reset_index(level=0,drop=True)\n    #df['u_in_rol_q0.5'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.5).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.75'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.75).reset_index(level=0,drop=True)\n    #df['u_in_rol_q0.9'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1).quantile(0.9).reset_index(level=0,drop=True)  \n\n    df['PdV'] = df['delta_time']*df['volume_mean'] \n    df['PdV'] = df.groupby(df['breath_id'])['PdV'].cumsum()\n    df['PdV'] = df['PdV'] / df['C'] #см вд столба\n    \n    df['PdV-'] = df['delta_time']*df['volume_out']\n    df['PdV-'] = df.groupby(df['breath_id'])['PdV-'].cumsum()\n    df['PdV-'] = df['PdV-'] / df['C'] #см вд столба\n    \n    df['dP'] = df['PdV'] + (df['volume_mean'] * df['R']) /1000 #см вд столба\n    \n    df['RC'] = (df['R']*df['C']).astype(str)\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df = pd.get_dummies(df)\n\n    \n    df = df.fillna(0)\n    \n    return df\n\n#\ntrain = add_features(train)\ntest = add_features(test)\n#-------------cell------------------\ntargets = train[['pressure']].to_numpy().reshape(-1, 80)\ntrain.drop(['id','breath_id','time_step','pressure'], axis=1, inplace=True)\ntest = test.drop(['id','breath_id','time_step'], axis=1)\n#-------------cell------------------\nCOLS = train.columns.to_numpy()\n#-------------cell------------------\n# train = reduce_mem_usage(train)\n# test = reduce_mem_usage(test)\n#-------------cell------------------\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n#-------------cell------------------\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n#-------------cell------------------","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:44:50.999617Z","iopub.execute_input":"2021-10-28T04:44:51.000224Z","iopub.status.idle":"2021-10-28T04:47:13.040907Z","shell.execute_reply.started":"2021-10-28T04:44:51.000177Z","shell.execute_reply":"2021-10-28T04:47:13.040066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Detect hardware, return appropriate distribution strategy\nimport tensorflow as tf\nprint(tf.version.VERSION)\n#tf.get_logger().setLevel(logging.ERROR)\ntry: # detect TPU\n    tpu = None\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError: # detect GPU(s) and enable mixed precision\n    strategy = tf.distribute.MirroredStrategy() # works on GPU and multi-GPU\n    policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    tf.config.optimizer.set_jit(True) # XLA compilation\n    tf.keras.mixed_precision.experimental.set_policy(policy)\n    print('Mixed precision enabled')\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:47:13.116394Z","iopub.execute_input":"2021-10-28T04:47:13.117218Z","iopub.status.idle":"2021-10-28T04:47:13.161382Z","shell.execute_reply.started":"2021-10-28T04:47:13.117189Z","shell.execute_reply":"2021-10-28T04:47:13.160746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_hist(hist):\n    plt.plot(hist.history[\"loss\"])\n    plt.plot(hist.history[\"val_loss\"])\n    plt.title(\"model performance\")\n    plt.ylabel(\"mean_absolute_error\")\n    plt.xlabel(\"epoch\")\n    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.96075Z","iopub.status.idle":"2021-10-28T04:49:15.961057Z","shell.execute_reply.started":"2021-10-28T04:49:15.960893Z","shell.execute_reply":"2021-10-28T04:49:15.960912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow_addons.layers import MultiHeadAttention\nfrom tensorflow.keras import backend as K\nfrom tensorflow import keras \nclass Time2Vec(tf.keras.layers.Layer):\n    def __init__(self, kernel_size=1):\n        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n        self.k = kernel_size\n    \n    def build(self, input_shape):\n        # trend\n        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n        # periodic\n        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n        super(Time2Vec, self).build(input_shape)\n    \n    def call(self, inputs, **kwargs):\n        bias = self.wb * inputs + self.bb\n        dp = K.dot(inputs, self.wa) + self.ba\n        wgts = K.sin(dp) # or K.cos(.)\n\n        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n        return ret\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1]*(self.k + 1))\n#\nclass AttentionBlock(tf.keras.Model):\n    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n        if ff_dim is None:\n            ff_dim = head_size\n\n        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n        self.attention_dropout = keras.layers.Dropout(dropout)\n        self.attention_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n        # self.ff_conv2 at build()\n        self.ff_dropout = keras.layers.Dropout(dropout)\n        self.ff_norm = keras.layers.LayerNormalization(epsilon=1e-6)\n\n    def build(self, input_shape):\n        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n\n    def call(self, inputs):\n        x = self.attention([inputs, inputs])\n        x = self.attention_dropout(x)\n        x = self.attention_norm(inputs + x)\n\n        x = self.ff_conv1(x)\n        x = self.ff_conv2(x)\n        x = self.ff_dropout(x)\n\n        x = self.ff_norm(inputs + x)\n        return x\n#\nclass ModelTrunk(tf.keras.Model):\n    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n        if ff_dim is None:\n            ff_dim = head_size\n        self.dropout = dropout\n        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n        #self.pooling=tf.keras.layers.AveragePooling1D(pool_size=4,data_format='channels_first')\n        self.final_layer = Sequential([\n                Bidirectional(LSTM(256, return_sequences=True)),\n                Bidirectional(LSTM(256, return_sequences=True)),\n                Dense(128, activation='selu'),\n                Dense(1)])\n    def call(self, inputs):\n        time_embedding = tf.keras.layers.TimeDistributed(self.time2vec)(inputs)\n        #time_embedding=inputs\n        #print(time_embedding.shape)\n        x = K.concatenate([inputs, time_embedding], -1)\n        for attention_layer in self.attention_layers:\n            x = attention_layer(x)\n        x=self.final_layer(x)\n        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\nt2v=ModelTrunk(ff_dim=256,num_heads=4,num_layers=2,)\nx=tf.random.uniform((64,80,68))\nx=t2v(x)\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:24.076839Z","iopub.execute_input":"2021-10-28T04:49:24.077119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_lr():\n    initial_learning_rate=0.001\n    decay_rate=0.9#1e-5\n    batch_size=512\n    train_len=60360\n    epoch_iter=int(train_len/batch_size)\n    decay_steps=5#200*epoch_iter\n    def decayed_learning_rate(step):\n        return initial_learning_rate * pow(decay_rate , (step / decay_steps))\n    lr_lst=[]\n    for i in range(300):\n        lr=decayed_learning_rate(i)\n        lr_lst.append(lr)\n    print(min(lr_lst))\n    plt.plot(lr_lst)\n    plt.show()\n    plt.close()\nplot_lr()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.948788Z","iopub.status.idle":"2021-10-28T04:49:15.949121Z","shell.execute_reply.started":"2021-10-28T04:49:15.94896Z","shell.execute_reply":"2021-10-28T04:49:15.94898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=126)\n\ntest_preds = []\nbatch_size=128\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n    print(f\"****** fold: {fold+1} *******\")\n    X_train, X_valid = train[train_idx], train[test_idx]\n    y_train, y_valid = targets[train_idx], targets[test_idx]\n    #scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 200*((len(train)*0.8)/batch_size), 1e-5)\n    scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 5, 0.9)\n    es = EarlyStopping(monitor='val_loss',mode='min', patience=35, verbose=1,restore_best_weights=True)\n    checkpoint_filepath = f\"folds{fold}.tf\"#f\"folds{fold}.hdf5\"\n#     sv = keras.callbacks.ModelCheckpoint(\n#             checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n#             save_weights_only=False, mode='auto', save_freq='epoch',\n#             options=None\n#         )\n    with strategy.scope():\n        model = ModelTrunk(ff_dim=256,num_heads=8,num_layers=2,)\n        model.compile(optimizer=\"adam\",loss = \"mae\")\n    history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=250, batch_size = batch_size,\n                        callbacks = [es,tf.keras.callbacks.LearningRateScheduler(scheduler)])\n    test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())\n    #plot_hist(history)\n    del X_train, X_valid, y_train, y_valid, model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.950582Z","iopub.status.idle":"2021-10-28T04:49:15.950896Z","shell.execute_reply.started":"2021-10-28T04:49:15.950746Z","shell.execute_reply":"2021-10-28T04:49:15.950761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\nsubmission[\"pressure\"] =sum(test_preds)/len(test_preds)# test_preds[0]\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.955156Z","iopub.status.idle":"2021-10-28T04:49:15.955431Z","shell.execute_reply.started":"2021-10-28T04:49:15.955288Z","shell.execute_reply":"2021-10-28T04:49:15.955301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gf = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\nall_pressure = sorted(train_gf.pressure.unique())\nPRESSURE_MIN = all_pressure[0]\nPRESSURE_MAX = all_pressure[-1]\nPRESSURE_STEP = (all_pressure[1] - all_pressure[0])\nsubmission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\nsubmission[\"pressure\"] =np.round( (submission.pressure - PRESSURE_MIN)/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\nsubmission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\npressure_unique = np.array(sorted(train_gf['pressure'].unique()))\nsubmission['pressure'] = submission['pressure'].map(lambda x: pressure_unique[np.abs(pressure_unique-x).argmin()])\nsubmission.to_csv('submission.csv', index = 0)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.956517Z","iopub.status.idle":"2021-10-28T04:49:15.957027Z","shell.execute_reply.started":"2021-10-28T04:49:15.956867Z","shell.execute_reply":"2021-10-28T04:49:15.956884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import cupy, cudf, matplotlib.pyplot as plt\n# train_gf = cudf.read_csv('../input/ventilator-pressure-prediction/train.csv')\n# # plt.title('Histogram of Train Pressures',size=14)\n# # plt.hist(train_gf.sample(100_000).pressure.to_array(),bins=100)\n# # plt.show()\n# print('Max pressure =',train_gf.pressure.max(), 'Min pressure =',train_gf.pressure.min())\n# all_pressure = cupy.sort( train_gf.pressure.unique().values )\n# print('The first 25 unique pressures...')\n# PRESSURE_MIN = all_pressure[0].item()\n# PRESSURE_MAX = all_pressure[-1].item()\n# #\n# PRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.957943Z","iopub.status.idle":"2021-10-28T04:49:15.958436Z","shell.execute_reply.started":"2021-10-28T04:49:15.958269Z","shell.execute_reply":"2021-10-28T04:49:15.958286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n# sub_1=pd.read_csv('../input/submission-list/submission172.csv')\n# sub_2=pd.read_csv('../input/submission-list/submission_mean_LB157.csv')\n# submission['pressure'] = (sub_1['pressure'].values*0.2)+(sub_2['pressure'].values*0.8)\n# submission[\"pressure\"]=np.round((submission.pressure-PRESSURE_MIN)/PRESSURE_STEP)*PRESSURE_STEP+PRESSURE_MIN\n# submission.pressure=np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n# submission.to_csv('submission.csv', index=False)\n# submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T04:49:15.959307Z","iopub.status.idle":"2021-10-28T04:49:15.959609Z","shell.execute_reply.started":"2021-10-28T04:49:15.959452Z","shell.execute_reply":"2021-10-28T04:49:15.959472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}