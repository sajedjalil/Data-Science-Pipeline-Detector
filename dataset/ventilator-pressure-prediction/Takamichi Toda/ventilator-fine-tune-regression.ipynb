{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ventilator Fine-tune Regression","metadata":{}},{"cell_type":"markdown","source":"I train as a classification task. (https://www.kaggle.com/takamichitoda/ventilator-train-classification?scriptVersionId=76597714)\n\nThe classification converges earlier than regression.\n\nIn my experience, to got  OOF score=0.17xx by using 4 layer bi-LSTM model\n- regression needs 150 epoch\n- classification needs 50 epoch\n\nHowever, the classification case easily makes overfit.\n\n<img src=\"https://raw.githubusercontent.com/trtd56/RFCX/main/tmp/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202021-10-14%2015.40.55.png\" width=\"400\">\n\nSo in this code, I fine-tuning the regression task by using [classification model weight](https://www.kaggle.com/takamichitoda/ventilator-classification-model).\n\nClassification model result is CV=0.1708 / LB=0.155, this model result is CV=0.1765 / LB=0.165.\n\nIf I use custom header based on 1d-CNN (the implementation like [here](https://www.kaggle.com/takamichitoda/ventilator-1dcnn-lstm)), I got CV=0.1622 / LB=0.150.\n\n## UPDATE\n\n|Date|Version|Detail|CV|LB|\n|--|--|--|--|--|\n|2021.10.27|4|Transformer Header|0.1665|0.1545|","metadata":{}},{"cell_type":"code","source":"!pip install -U torch wandb transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!wandb login $secret_value","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport os\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AdamW\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn.preprocessing import RobustScaler\n\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:10.434964Z","iopub.execute_input":"2021-10-14T01:31:10.43559Z","iopub.status.idle":"2021-10-14T01:31:13.960375Z","shell.execute_reply.started":"2021-10-14T01:31:10.435484Z","shell.execute_reply":"2021-10-14T01:31:13.959205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    EXP_NAME = \"exp137_transformer_head\"\n    CLS_MODEL = \"../input/ventilator-classification-model/exp075_dropout0\"\n    \n    INPUT = \"/kaggle/input/ventilator-pressure-prediction\"\n    OUTPUT = \"/kaggle/working\"\n    N_FOLD = 5\n    SEED = 0\n    \n    LR = 5e-3\n    N_EPOCHS = 50\n    EMBED_SIZE = 64\n    HIDDEN_SIZE = 256\n    BS = 512\n    WEIGHT_DECAY = 1e-3\n\n    USE_LAG = 4\n    CATE_FEATURES = ['R_cate', 'C_cate', 'RC_dot', 'RC_sum']\n    CONT_FEATURES = ['u_in', 'u_out', 'time_step'] + ['u_in_cumsum', 'u_in_cummean', 'area', 'cross', 'cross2']\n    LAG_FEATURES = ['breath_time']\n    LAG_FEATURES += [f'u_in_lag_{i}' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_in_time{i}' for i in range(1, USE_LAG+1)]\n    LAG_FEATURES += [f'u_out_lag_{i}' for i in range(1, USE_LAG+1)]\n    ALL_FEATURES = CATE_FEATURES + CONT_FEATURES + LAG_FEATURES\n    NORM_FEATURES = CONT_FEATURES + LAG_FEATURES\n    \n    NOT_WATCH_PARAM = ['INPUT']","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:13.962839Z","iopub.execute_input":"2021-10-14T01:31:13.963057Z","iopub.status.idle":"2021-10-14T01:31:13.975328Z","shell.execute_reply.started":"2021-10-14T01:31:13.963029Z","shell.execute_reply":"2021-10-14T01:31:13.974101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:13.977308Z","iopub.execute_input":"2021-10-14T01:31:13.97788Z","iopub.status.idle":"2021-10-14T01:31:13.990074Z","shell.execute_reply.started":"2021-10-14T01:31:13.977836Z","shell.execute_reply":"2021-10-14T01:31:13.988832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df, label_dic=None):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        self.label_dic = label_dic\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        X = df[config.ALL_FEATURES].values\n        y = df['pressure'].values\n\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\" : torch.tensor(y).float(),\n        }\n        return d","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:13.993754Z","iopub.execute_input":"2021-10-14T01:31:13.994207Z","iopub.status.idle":"2021-10-14T01:31:14.004287Z","shell.execute_reply.started":"2021-10-14T01:31:13.994162Z","shell.execute_reply":"2021-10-14T01:31:14.002939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.rc_dot_emb = nn.Embedding(8, 4, padding_idx=0)\n        self.rc_sum_emb = nn.Embedding(8, 4, padding_idx=0)\n        self.seq_emb = nn.Sequential(\n            nn.Linear(12+len(config.NORM_FEATURES), config.EMBED_SIZE),\n            nn.LayerNorm(config.EMBED_SIZE),\n        )\n        \n        self.lstm = nn.LSTM(config.EMBED_SIZE, config.HIDDEN_SIZE, batch_first=True, bidirectional=True, num_layers=4, dropout=0.0)\n        self.head = nn.Sequential(\n            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(config.HIDDEN_SIZE * 2, 950),\n        )\n\n    def forward(self, X):\n        # embed\n        bs = X.shape[0]\n        r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n        c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n        rc_dot_emb = self.rc_dot_emb(X[:,:,2].long()).view(bs, 80, -1)\n        rc_sum_emb = self.rc_sum_emb(X[:,:,3].long()).view(bs, 80, -1)\n        \n        seq_x = torch.cat((r_emb, c_emb, rc_dot_emb, rc_sum_emb, X[:, :, 4:]), 2)\n        emb_x = self.seq_emb(seq_x)\n        \n        out, _ = self.lstm(emb_x, None)\n\n        return out\n\n\nclass VentilatorModelRegr(nn.Module):\n    \n    def __init__(self, load_path):\n        super(VentilatorModelRegr, self).__init__()\n        self.cls_model = VentilatorModel()\n        self.cls_model.load_state_dict(torch.load(load_path))\n\n        encoder_layers = nn.TransformerEncoderLayer(d_model=config.HIDDEN_SIZE * 2, nhead=1, dim_feedforward=2048, dropout=0.0, batch_first=True)\n        self.regression = nn.Sequential(\n            #nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.TransformerEncoder(encoder_layers, num_layers=1),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Linear(config.HIDDEN_SIZE * 2, 1),\n        )\n\n    def forward(self, X, y=None):\n        out = self.cls_model(X)\n        regr = self.regression(out)\n\n        if y is None:\n            loss = None\n        else:\n            loss = self.loss_fn(regr.squeeze(2), y)\n            \n        return regr, loss\n    \n    def loss_fn(self, y_pred, y_true):\n        criterion = nn.SmoothL1Loss()\n        loss = criterion(y_pred, y_true)\n        return loss\n\n    def freeze_cls(self):\n        for param in self.cls_model.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:14.03418Z","iopub.execute_input":"2021-10-14T01:31:14.03577Z","iopub.status.idle":"2021-10-14T01:31:14.058583Z","shell.execute_reply.started":"2021-10-14T01:31:14.035723Z","shell.execute_reply":"2021-10-14T01:31:14.05739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(model, optimizer, scheduler, loader):\n    losses, lrs = [], []\n    model.train()\n    optimizer.zero_grad()\n    for d in loader:\n        out, loss = model(d['X'].to(device), d['y'].to(device))\n        \n        losses.append(loss.item())\n        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n        lrs.append(step_lr)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        scheduler.step()\n\n    return np.array(losses).mean(), np.array(lrs).mean()\n\ndef valid_loop(model, loader, target_dic_inv):\n    losses, predicts = [], []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, loss = model(d['X'].to(device), d['y'].to(device))\n        losses.append(loss.item())\n        predicts.append(out.cpu())\n    return np.array(losses).mean(), torch.vstack(predicts).squeeze(2).numpy().reshape(-1)\n\ndef test_loop(model, loader, target_dic_inv):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        predicts.append(out.cpu())\n    return torch.vstack(predicts).squeeze(2).numpy().reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:14.062989Z","iopub.execute_input":"2021-10-14T01:31:14.063405Z","iopub.status.idle":"2021-10-14T01:31:14.079349Z","shell.execute_reply.started":"2021-10-14T01:31:14.06334Z","shell.execute_reply":"2021-10-14T01:31:14.078181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_feature(df):\n    df['time_delta'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta'] = df['time_delta'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['delta'].cumsum()\n\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] / df['count']\n    \n    df = df.drop(['count','one'], axis=1)\n    return df\n\ndef add_lag_feature(df):\n    # https://www.kaggle.com/kensit/improvement-base-on-tensor-bidirect-lstm-0-173\n    for lag in range(1, config.USE_LAG+1):\n        df[f'breath_id_lag{lag}']=df['breath_id'].shift(lag).fillna(0)\n        df[f'breath_id_lag{lag}same']=np.select([df[f'breath_id_lag{lag}']==df['breath_id']], [1], 0)\n\n        # u_in \n        df[f'u_in_lag_{lag}'] = df['u_in'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n        df[f'u_in_time{lag}'] = df['u_in'] - df[f'u_in_lag_{lag}']\n        df[f'u_out_lag_{lag}'] = df['u_out'].shift(lag).fillna(0) * df[f'breath_id_lag{lag}same']\n\n    # breath_time\n    df['time_step_lag'] = df['time_step'].shift(1).fillna(0) * df[f'breath_id_lag{lag}same']\n    df['breath_time'] = df['time_step'] - df['time_step_lag']\n\n    drop_columns = ['time_step_lag']\n    drop_columns += [f'breath_id_lag{i}' for i in range(1, config.USE_LAG+1)]\n    drop_columns += [f'breath_id_lag{i}same' for i in range(1, config.USE_LAG+1)]\n    df = df.drop(drop_columns, axis=1)\n\n    # fill na by zero\n    df = df.fillna(0)\n    return df\n\nc_dic = {10: 0, 20: 1, 50:2}\nr_dic = {5: 0, 20: 1, 50:2}\nrc_sum_dic = {v: i for i, v in enumerate([15, 25, 30, 40, 55, 60, 70, 100])}\nrc_dot_dic = {v: i for i, v in enumerate([50, 100, 200, 250, 400, 500, 2500, 1000])}    \n\ndef add_category_features(df):\n    df['C_cate'] = df['C'].map(c_dic)\n    df['R_cate'] = df['R'].map(r_dic)\n    df['RC_sum'] = (df['R'] + df['C']).map(rc_sum_dic)\n    df['RC_dot'] = (df['R'] * df['C']).map(rc_dot_dic)\n    return df\n\nnorm_features = config.CONT_FEATURES + config.LAG_FEATURES\ndef norm_scale(train_df, test_df):\n    scaler = RobustScaler()\n    all_u_in = np.vstack([train_df[norm_features].values, test_df[norm_features].values])\n    scaler.fit(all_u_in)\n    train_df[norm_features] = scaler.transform(train_df[norm_features].values)\n    test_df[norm_features] = scaler.transform(test_df[norm_features].values)\n    return train_df, test_df","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:14.083051Z","iopub.execute_input":"2021-10-14T01:31:14.083283Z","iopub.status.idle":"2021-10-14T01:31:14.106695Z","shell.execute_reply.started":"2021-10-14T01:31:14.083255Z","shell.execute_reply":"2021-10-14T01:31:14.105468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n    \n    train_df = pd.read_csv(f\"{config.INPUT}/train.csv\")\n    test_df = pd.read_csv(f\"{config.INPUT}/test.csv\")\n    sub_df = pd.read_csv(f\"{config.INPUT}/sample_submission.csv\")\n    oof = np.zeros(len(train_df))\n    test_preds_lst = []\n\n    target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n    target_dic_inv = {v: k for k, v in target_dic.items()}\n\n    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n    for fold, (_, valid_idx) in enumerate(gkf):\n        train_df.loc[valid_idx, 'fold'] = fold\n\n    train_df = add_feature(train_df)\n    test_df = add_feature(test_df)\n    train_df = add_lag_feature(train_df)\n    test_df = add_lag_feature(test_df)\n    train_df = add_category_features(train_df)\n    test_df = add_category_features(test_df)\n    train_df, test_df = norm_scale(train_df, test_df)\n\n    test_df['pressure'] = -1\n    test_dset = VentilatorDataset(test_df)\n    test_loader = DataLoader(test_dset, batch_size=config.BS,\n                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n    \n    for fold in range(config.N_FOLD):\n        print(f'Fold-{fold}')\n        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"), target_dic)\n        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"), target_dic)\n\n        set_seed()\n        train_loader = DataLoader(train_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n        load_path = f\"{config.OUTPUT}/{config.CLS_MODEL}/ventilator_f{fold}_best_model.bin\"\n        model = VentilatorModelRegr(load_path)\n        model.to(device)\n        model.freeze_cls()\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n        num_train_steps = int(len(train_loader) * config.N_EPOCHS)\n        num_warmup_steps = int(num_train_steps / 10)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n\n        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n        wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n        wandb_config = wandb.config\n        wandb_config.fold = fold\n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        \n        os.makedirs(f'{config.OUTPUT}/{config.EXP_NAME}', exist_ok=True)\n        model_path = f\"{config.OUTPUT}/{config.EXP_NAME}/ventilator_f{fold}_best_model.bin\"\n        \n        valid_best_score = float('inf')\n        valid_best_score_mask = float('inf')\n        for epoch in tqdm(range(config.N_EPOCHS)):\n            train_loss, lrs = train_loop(model, optimizer, scheduler, train_loader)\n            valid_loss, valid_predict = valid_loop(model, valid_loader, target_dic_inv)\n            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n\n            mask = (train_df.query(f\"fold=={fold}\")['u_out'] == -1).values  # u_out is normalized [-1, 0]\n            _score = valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values\n            valid_score_mask = np.abs(_score[mask]).mean()\n\n            if valid_score < valid_best_score:\n                valid_best_score = valid_score\n                torch.save(model.state_dict(), model_path)\n                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n\n            if valid_score_mask < valid_best_score_mask:\n                valid_best_score_mask = valid_score_mask\n\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"valid_loss\": valid_loss,\n                \"valid_score\": valid_score,\n                \"valid_best_score\": valid_best_score,\n                \"valid_score_mask\": valid_score_mask,\n                \"valid_best_score_mask\": valid_best_score_mask,\n                \"learning_rate\": lrs,\n            })\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        model.load_state_dict(torch.load(model_path))\n        test_preds = test_loop(model, test_loader, target_dic_inv)\n        test_preds_lst.append(test_preds)\n        \n        sub_df['pressure'] = test_preds\n        sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/sub_f{fold}.csv\", index=None)\n\n        train_df['oof'] = oof\n        train_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/oof.csv\", index=None)\n        wandb.finish()\n\n        del model, optimizer, scheduler, train_loader, valid_loader, train_dset, valid_dset\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean.csv\", index=None)\n\n    sub_df['pressure'] = np.median(np.stack(test_preds_lst), axis=0)\n    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median.csv\", index=None)\n    \n    # Post Processing: https://www.kaggle.com/snnclsr/a-dummy-approach-to-improve-your-score-postprocess\n    unique_pressures = train_df[\"pressure\"].unique()\n    sorted_pressures = np.sort(unique_pressures)\n    total_pressures_len = len(sorted_pressures)\n\n    def find_nearest(prediction):\n        insert_idx = np.searchsorted(sorted_pressures, prediction)\n        if insert_idx == total_pressures_len:\n            # If the predicted value is bigger than the highest pressure in the train dataset,\n            # return the max value.\n            return sorted_pressures[-1]\n        elif insert_idx == 0:\n            # Same control but for the lower bound.\n            return sorted_pressures[0]\n        lower_val = sorted_pressures[insert_idx - 1]\n        upper_val = sorted_pressures[insert_idx]\n        return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n    \n    sub_df = pd.read_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean.csv\")\n    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_mean_pp.csv\", index=None)\n    \n    sub_df = pd.read_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median.csv\")\n    sub_df[\"pressure\"] = sub_df[\"pressure\"].apply(find_nearest)\n    sub_df.to_csv(f\"{config.OUTPUT}/{config.EXP_NAME}/submission_median_pp.csv\", index=None)\n    \n    cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n    print(\"CV:\", cv_score)","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:14.108987Z","iopub.execute_input":"2021-10-14T01:31:14.109653Z","iopub.status.idle":"2021-10-14T01:31:14.147082Z","shell.execute_reply.started":"2021-10-14T01:31:14.109568Z","shell.execute_reply":"2021-10-14T01:31:14.145888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2021-10-14T01:31:14.149753Z","iopub.execute_input":"2021-10-14T01:31:14.150798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}