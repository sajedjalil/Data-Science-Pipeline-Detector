{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport os \nimport time \nimport json \nimport requests \nfrom tqdm import tqdm \nimport wandb \nfrom wandb.keras import WandbCallback \nfrom kaggle_secrets import UserSecretsClient \nimport random \nfrom typing import Tuple \nimport gc \n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras import layers , Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Input, Dropout, Dense, Conv1D, LayerNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\n# from keras_pos_embd import PositionEmbedding\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything()\npd.set_option(\"display.max_columns\", None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:47:42.054247Z","iopub.execute_input":"2021-10-09T06:47:42.054598Z","iopub.status.idle":"2021-10-09T06:47:42.068616Z","shell.execute_reply.started":"2021-10-09T06:47:42.054566Z","shell.execute_reply":"2021-10-09T06:47:42.067874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config ","metadata":{}},{"cell_type":"code","source":"config = dict(\n    competition = \"ventilator\", \n    infra = \"kaggle\", \n    train = True, \n    type = \"train\", \n    debug = False, \n    inference = True, \n    \n    model_name = \"transformer\", \n    frame_word = \"tensorflow\", \n    device = \"tpu\", \n    n_fold = 5, \n    early_stopping_rounds = 30, \n    batch_size = 1024, \n    epoch = 530, \n    verbose = 100, \n    seed = 42 \n)\n\nparams = {\n    \"input_size\": (80, 73),\n    \"hidden_dim\": 128, \n    \"head_size\": 256, \n    \"num_heads\": 12, \n    \"ff_dim\": 4, \n    \"num_transformer_blocks\": 4, \n    \"mlp_units\": [128], \n    \"dropout\": 0.2, \n    \"mlp_dropout\": 0 , \n}","metadata":{"execution":{"iopub.status.busy":"2021-10-09T06:47:42.069879Z","iopub.execute_input":"2021-10-09T06:47:42.070224Z","iopub.status.idle":"2021-10-09T06:47:42.086235Z","shell.execute_reply.started":"2021-10-09T06:47:42.070195Z","shell.execute_reply":"2021-10-09T06:47:42.085099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nurl = user_secrets.get_secret(\"WEB_HOOK_URL\") \n\nuser_secrets = UserSecretsClient()\napi = user_secrets.get_secret(\"wandb_api\")\n\n\ndef setup_db():\n    wandb.login(key=api)\n    run = wandb.init(\n        project = config[\"competition\"], \n        name = config[\"model_name\"], \n        config = config, \n        group = config[\"model_name\"], \n        job_type = config[\"type\"]\n    )\n    return run\n\ndef slack(txt):\n    requests.post(url, data=json.dumps({\n        \"username\": \"kaggle\", \n        \"text\": txt \n    }))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:47:42.087585Z","iopub.execute_input":"2021-10-09T06:47:42.088374Z","iopub.status.idle":"2021-10-09T06:47:42.469411Z","shell.execute_reply.started":"2021-10-09T06:47:42.088342Z","shell.execute_reply":"2021-10-09T06:47:42.468595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering ","metadata":{}},{"cell_type":"code","source":"if config[\"debug\"]:\n    train = pd.read_csv(\"../input/ventilator-pressure-prediction/train.csv\", nrows=80*100)\n    test = pd.read_csv(\"../input/ventilator-pressure-prediction/test.csv\", nrows=80*100)\nelse:\n    train = pd.read_csv(\"../input/ventilator-pressure-prediction/train.csv\")\n    test = pd.read_csv(\"../input/ventilator-pressure-prediction/test.csv\")\n\nsort = np.sort(train.pressure.unique())\nPRESSURE_MIN = sort[0]\nPRESSURE_MAX = sort[-1]\nPRESSURE_STEP = sort[1] - sort[0]","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:47:42.470486Z","iopub.execute_input":"2021-10-09T06:47:42.470691Z","iopub.status.idle":"2021-10-09T06:47:55.992535Z","shell.execute_reply.started":"2021-10-09T06:47:42.470667Z","shell.execute_reply":"2021-10-09T06:47:55.991937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(train_data):\n    start_mem = train_data.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return train_data","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:47:55.993784Z","iopub.execute_input":"2021-10-09T06:47:55.994197Z","iopub.status.idle":"2021-10-09T06:47:56.009017Z","shell.execute_reply.started":"2021-10-09T06:47:55.99415Z","shell.execute_reply":"2021-10-09T06:47:56.008323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T06:47:56.010221Z","iopub.execute_input":"2021-10-09T06:47:56.010625Z","iopub.status.idle":"2021-10-09T06:47:56.805871Z","shell.execute_reply.started":"2021-10-09T06:47:56.010582Z","shell.execute_reply":"2021-10-09T06:47:56.804932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lag_feature(df) -> pd.DataFrame:\n    df[\"area\"] = df.time_step * df.u_in \n    df[\"area\"] = df.groupby(\"breath_id\")[\"area\"].cumsum()\n    \n    df[\"u_in_cumsum\"] = df.groupby(\"breath_id\")[\"u_in\"].cumsum()\n    \n    for i in range(4):\n        df[\"u_in_\"+f\"lag{i+1}\"] = df.groupby(\"breath_id\")[\"u_in\"].shift(i+1).fillna(0)\n        df[\"u_out_\"+f\"lag{i+1}\"] = df.groupby(\"breath_id\")[\"u_out\"].shift(i+1).fillna(0)\n\n        df[\"u_in_\"+f\"back{i+1}\"] = df.groupby(\"breath_id\")[\"u_in\"].shift((-1)*(i+1)).fillna(0)\n        df[\"u_out_\"+f\"back{i+1}\"] = df.groupby(\"breath_id\")[\"u_out\"].shift((-1)*(i+1)).fillna(0)\n\n    df[\"u_out_rolling_10\"] = df.groupby(\"breath_id\")[\"u_out\"].rolling(window=10).mean().reset_index(drop=True).fillna(0)\n    df[\"u_in_rolling_10\"] = df.groupby(\"breath_id\")[\"u_in\"].rolling(window=10).mean().reset_index(drop=True).fillna(0)\n    \n    df[\"u_in_max\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"max\")\n    df[\"u_in_min\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"min\")\n    df[\"u_in_mean\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"mean\")\n    df[\"u_out_max\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"max\")\n    df[\"u_out_min\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"min\")\n    df[\"u_out_mean\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"mean\")\n    \n    df[\"u_in_first\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"first\")\n    df[\"u_in_last\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"last\")\n    \n    for i in range(4):\n        df[\"u_in\"+f\"_diff{i+1}\"] = df[\"u_in\"] - df[f\"u_in_lag{i+1}\"]\n        df[\"u_in\"+f\"_diff_back{i+1}\"] = df[\"u_in\"] - df[f\"u_in_back{i+1}\"]\n\n        df[\"u_out\"+f\"_diff{i+1}\"] = df[\"u_out\"] - df[f\"u_out_lag{i+1}\"]\n        df[\"u_out\"+f\"_diff_back{i+1}\"] = df[\"u_out\"] - df[f\"u_out_back{i+1}\"]\n\n    df[\"u_in_diff_max\"] = df[\"u_in_max\"] - df[\"u_in\"]\n    df[\"u_in_diff_min\"] = df[\"u_in_min\"] - df[\"u_in\"]\n    df[\"u_in_diff_mean\"] = df[\"u_in_mean\"] - df[\"u_in\"]\n    \n    df[\"cross\"] = df[\"u_in\"] * df[\"u_out\"]\n    df[\"cross2\"] = df[\"time_step\"] * df[\"u_out\"]\n    \n    df[\"time_class\"] = df.groupby(\"breath_id\").cumcount()\n    df[\"R\"] = df.R.astype(str)\n    df[\"C\"] = df.C.astype(str)\n    df[\"R_C\"] = df.R + \"_\" + df.C \n    gc.collect()\n    return df\n\ndef group_feature(train, test) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    # time_class x u_in\n    time_grp = train.groupby(\"time_class\").mean().loc[:, [\"u_in\"]]\n    time_grp = time_grp.rename(columns={\"u_in\": \"u_in_time_class\"})\n    train = pd.merge(train, time_grp, how=\"left\", left_on=\"time_class\", right_index=True)\n    test = pd.merge(test, time_grp, how=\"left\", left_on=\"time_class\", right_index=True)\n    del time_grp \n    gc.collect()\n    \n    print(1)\n    \n    # R x u_in \n    r = train.groupby(\"R\").mean().loc[:, [\"u_in\"]]\n    r = r.rename(columns={\"u_in\": \"u_in_r_mean\"})\n    train = pd.merge(train, r, how=\"left\", left_on=\"R\", right_index=True)\n    test = pd.merge(test, r, how=\"left\", left_on=\"R\", right_index=True)\n    del r \n    gc.collect()\n\n    \n    # c x u_in \n    c = train.groupby(\"C\").mean().loc[:, [\"u_in\"]]\n    c = c.rename(columns={\"u_in\": \"u_in_c_mean\"})\n    train = pd.merge(train, c, how=\"left\", left_on=\"C\", right_index=True)\n    test = pd.merge(test, c, how=\"left\", left_on=\"C\", right_index=True)\n    del c \n    gc.collect()\n    \n    print(2)\n\n    # r_c x u_in \n    rc = train.groupby(\"R_C\").mean().loc[:, [\"u_in\"]]\n    rc = rc.rename(columns={\"u_in\": \"u_in_rc_mean\"})\n    train = pd.merge(train, rc, how=\"left\", left_on=\"R_C\", right_index=True)\n    test = pd.merge(test, rc, how=\"left\", left_on=\"R_C\", right_index=True)\n    del rc \n    gc.collect()\n    \n    print(3)\n\n    # r_c, time_class x u_in \n    rc = train.groupby([\"R_C\", \"time_class\"]).mean().loc[:, [\"u_in\"]]\n    rc = rc.rename(columns={\"u_in\": \"u_in_rc_time_mean\"})\n    train = pd.merge(train, rc, how=\"left\", left_on=[\"R_C\", \"time_class\"], right_index=True)\n    test = pd.merge(test, rc, how=\"left\", left_on=[\"R_C\", \"time_class\"], right_index=True)\n    del rc \n    gc.collect()\n    \n    print(4)\n    \n    # get dummmies object\n    last_train_shape = train.shape[0]\n    y = train.pressure.values.ravel()\n    df = pd.concat([train.drop(\"pressure\", axis=1), test])\n    df = pd.get_dummies(data=df, columns=[\"R\", \"C\", \"R_C\"])\n    train, test = df.iloc[:last_train_shape, :], df.iloc[last_train_shape:, :]\n    del df \n    train[\"pressure\"] = y \n    del y \n    gc.collect()\n    return train, test ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:47:56.807539Z","iopub.execute_input":"2021-10-09T06:47:56.808419Z","iopub.status.idle":"2021-10-09T06:47:56.844658Z","shell.execute_reply.started":"2021-10-09T06:47:56.808372Z","shell.execute_reply":"2021-10-09T06:47:56.843737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\ntrain = lag_feature(train)\ntest = lag_feature(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T06:47:56.846112Z","iopub.execute_input":"2021-10-09T06:47:56.84637Z","iopub.status.idle":"2021-10-09T06:49:05.062453Z","shell.execute_reply.started":"2021-10-09T06:47:56.846342Z","shell.execute_reply":"2021-10-09T06:49:05.061589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\ntrain, test = group_feature(train, test)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T06:49:05.063691Z","iopub.execute_input":"2021-10-09T06:49:05.063993Z","iopub.status.idle":"2021-10-09T06:50:15.278049Z","shell.execute_reply.started":"2021-10-09T06:49:05.063962Z","shell.execute_reply":"2021-10-09T06:50:15.277168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T06:50:15.281826Z","iopub.execute_input":"2021-10-09T06:50:15.282168Z","iopub.status.idle":"2021-10-09T06:50:25.634817Z","shell.execute_reply.started":"2021-10-09T06:50:15.282127Z","shell.execute_reply":"2021-10-09T06:50:25.633825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).columns) # use train features = model input size ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:50:25.636323Z","iopub.execute_input":"2021-10-09T06:50:25.637181Z","iopub.status.idle":"2021-10-09T06:50:27.169675Z","shell.execute_reply.started":"2021-10-09T06:50:25.637133Z","shell.execute_reply":"2021-10-09T06:50:27.168874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transformer Model ","metadata":{}},{"cell_type":"code","source":"@tf.custom_gradient\ndef round_with_gradients(x):\n    def grad(dy):\n        return dy\n    return tf.round(x), grad\n\nclass ScaleLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = tf.constant(PRESSURE_MIN, dtype=np.float32)\n        self.max = tf.constant(PRESSURE_MAX, dtype=np.float32)\n        self.step = tf.constant(PRESSURE_STEP, dtype=np.float32)\n\n    def call(self, inputs):\n        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n        int_steps = round_with_gradients(steps)\n        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n        return clipped\n    \n    \nclass PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n        position = 80 \n        emb_dim = params[\"hidden_dim\"]\n        self.pos_encoding = self._positional_encoding(position, emb_dim)\n        \n    def _get_angles(self, position, i, emb_dim):\n        \"\"\"\n        assign position, i and emb_dim to the expression of the angle of positional encoding formulae\n        outputs: shape=(position.shape[0], i.shape[1])\n        \"\"\"\n        denominator = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(emb_dim, tf.float32))\n        return position * denominator\n\n    def _positional_encoding(self, sentence_length, emb_dim):\n        \"\"\"\n        inputs:\n        sentence_length: int\n        emb_dim: int\n        \n        outputs:\n        output: shape=(1, sentence_length, emb_dim), float32\n        \"\"\"\n        # 計算を効率化するためにpositionとiを行列にしてangle計算を行列の積で一度に実行する\n        angle = self._get_angles(\n            position=tf.expand_dims(tf.range(sentence_length, dtype=tf.float32), -1),\n            i=tf.expand_dims(tf.range(emb_dim, dtype=tf.float32), 0),\n            emb_dim=emb_dim\n        )\n        \n        # インデックスが偶数のものはサイン関数に適応\n        sine = tf.math.sin(angle[:, 0::2])\n        # インデックスが奇数のものはコサイン関数に適応\n        cos = tf.math.cos(angle[:, 1::2])\n        \n        pos_encoding = tf.concat([sine, cos], axis=-1)\n        pos_encoding = tf.expand_dims(pos_encoding, 0)\n        return tf.cast(pos_encoding, tf.float32)\n    \n    def call(self, inputs):\n        \"\"\"\n        inputs: shape=(batch, sentence_length, emb_dim)\n        \"\"\"\n        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n    \n    \ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n    # Normalization and Attention\n    x = LayerNormalization(epsilon=1e-6)(inputs)\n    x = MultiHeadAttention(\n        key_dim=head_size, num_heads=num_heads, dropout=dropout\n    )(x, x)\n    x = layers.Dropout(dropout)(x)\n    res = x + inputs\n\n    # Feed Forward Part\n    x = LayerNormalization(epsilon=1e-6)(res)\n    x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    return x + res\n\n\ndef base_model(\n    input_shape=params[\"input_size\"], \n    head_size=params[\"head_size\"],\n    hidden_dim = params[\"hidden_dim\"],\n    num_heads=params[\"num_heads\"],\n    ff_dim=params[\"ff_dim\"],\n    num_transformer_blocks=params[\"num_transformer_blocks\"], \n    mlp_units=params[\"mlp_units\"], \n    dropout=params[\"dropout\"],\n    mlp_dropout=params[\"mlp_dropout\"]\n):\n    inputs = Input(shape=input_shape)\n    x = inputs\n    x = Dense(hidden_dim, activation=\"relu\")(x)\n    x = PositionalEncoding()(x)\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n\n    for dim in mlp_units:\n        x = Dense(dim, activation=\"selu\")(x)\n        x = Dropout(mlp_dropout)(x)\n    outputs = Dense(1)(x)\n    output = ScaleLayer()(x)\n    return Model(inputs, outputs)\n\ndef build_model():\n    model = base_model()\n    model.compile(loss=\"mae\", optimizer=\"adam\")\n    return model \n\nmodel = build_model()\nmodel.summary()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:50:27.171462Z","iopub.execute_input":"2021-10-09T06:50:27.171801Z","iopub.status.idle":"2021-10-09T06:50:27.818972Z","shell.execute_reply.started":"2021-10-09T06:50:27.171747Z","shell.execute_reply":"2021-10-09T06:50:27.818136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train ","metadata":{}},{"cell_type":"code","source":"if config[\"debug\"] is not True and config[\"device\"] == \"tpu\":\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\ndef mae(corr, pred):\n    return np.mean(np.abs(corr - pred))\n\n\ndef scaler(tr, va, te):\n    RS = RobustScaler()\n    return RS.fit_transform(tr), RS.transform(va), RS.transform(te)\n\n\ndef callback_tools(fold) -> Tuple[object, object, object, object]:\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, \n                           mode=\"min\", restore_best_weights=True)\n    os.makedirs(\"models\", exist_ok=True)\n    checkpoint_filepath = f\"models/{fold}.hdf5\"\n    sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n    )\n    wb = WandbCallback(log_weights=True)\n    return lr, es, sv, wb \n\n\ndef submit(pred, name):\n    sub = pd.read_csv(\"../input/ventilator-pressure-prediction/sample_submission.csv\")\n    sub[\"pressure\"] = pred \n    sub.to_csv(f\"submission_lstm_{name}.csv\", index=False)\n    del sub \n    \n    \ndef viz_predict(corr, pred):\n    plt.figure(figsize=(15, 6))\n    \n    plt.subplot(121)\n    sns.histplot(corr)\n    plt.title(\"Label\")\n    \n    plt.subplot(122)\n    sns.histplot(pred)\n    plt.title(\"Predict\")\n    \n    plt.show()\n    \n\ndef train_fn(train, test):\n    \n    with tpu_strategy.scope():\n\n        predict_val, val_idx, predict_test = [], [], []\n        kf = GroupKFold(n_splits=2 if config[\"debug\"] else config[\"n_fold\"])\n\n        for fold, (tr, va) in enumerate(kf.split(train, train.pressure, train.breath_id)):\n            print(f\"=====================fold: {fold+1}==========================\")\n            x_train, x_val = train.iloc[tr].drop([\"id\", \"pressure\", \"breath_id\"], axis=1), train.iloc[va].drop([\"id\", \"pressure\", \"breath_id\"], axis=1)\n            y_train, y_val = train.iloc[tr][\"pressure\"], train.iloc[va][\"pressure\"]\n            use_col = x_train.columns \n            x_test = test[use_col]\n\n            # scaler \n            x_train, x_val, x_test = scaler(x_train, x_val, x_test)\n\n            # transformer batch shape \n            x_train = x_train.reshape(-1, 80, params[\"input_size\"][1])\n            x_val = x_val.reshape(-1, 80, params[\"input_size\"][1]) \n            x_test = x_test.reshape(-1, 80, params[\"input_size\"][1])        \n            y_train = y_train.values.reshape(-1, 80, 1)\n            y_val = y_val.values.reshape(-1, 80, 1)        \n\n            # set up tools \n            run = setup_db()\n            model = build_model()\n            lr, es, sv, ws = callback_tools(fold)\n            wandb.config.fold = fold \n\n            model.fit(x_train,\n                     y_train,\n                     validation_data=(x_val, y_val),\n                     callbacks=[lr, es, sv, ws],\n                     epochs=1 if config[\"debug\"] else config[\"epoch\"],\n                     batch_size=config[\"batch_size\"])\n\n            pred_v = model.predict(x_val, batch_size=config[\"batch_size\"], verbose=config[\"verbose\"]).squeeze().reshape(-1, 1).squeeze()\n            pred_t = model.predict(x_test, batch_size=config[\"batch_size\"], verbose=config[\"verbose\"]).squeeze().reshape(-1, 1).squeeze()\n\n            predict_val.append(pred_v)\n            predict_test.append(pred_t)        \n            val_idx.append(va)\n\n            print(f\"fold: {fold+1} | mae: {mae(y_val.squeeze().reshape(-1, 1).squeeze(), pred_v)}\")\n\n            del x_train, x_val, x_test, model \n            gc.collect()\n\n        predict_val = np.concatenate(predict_val)\n        val_idx = np.concatenate(val_idx)\n        val_idx = np.argsort(val_idx)\n        predict_val = predict_val[val_idx]\n\n        print(\"##############################################################\")\n        print(f\"CV SCORE: {mae(train.pressure.values.ravel(), predict_val)}\")\n        print(\"##############################################################\")\n\n        predict_test_mean = np.mean(predict_test, 0)\n        predict_test_median = np.median(predict_test, 0)\n\n        predict_test_mean_clip =(np.round(predict_test_mean - PRESSURE_MIN)/ PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n        predict_test_mean_clip = np.clip(predict_test_mean_clip, PRESSURE_MIN, PRESSURE_MAX)\n        predict_test_median_clip =(np.round(predict_test_median - PRESSURE_MIN)/ PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n        predict_test_median_clip = np.clip(predict_test_median_clip, PRESSURE_MIN, PRESSURE_MAX)\n\n        # submit \n        if config[\"debug\"] is not True:\n            submit(predict_test_mean, \"mean\")\n            submit(predict_test_median, \"median\")\n            submit(predict_test_mean_clip, \"mean_clip\")\n            submit(predict_test_median_clip, \"median_clip\")\n\n        gc.collect()\n        slack(\"Transfomer model done.\")\n        return predict_val ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-10-09T06:50:27.820607Z","iopub.execute_input":"2021-10-09T06:50:27.82089Z","iopub.status.idle":"2021-10-09T06:50:33.912328Z","shell.execute_reply.started":"2021-10-09T06:50:27.820859Z","shell.execute_reply":"2021-10-09T06:50:33.911399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_v = train_fn(train, test)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T06:50:33.917305Z","iopub.execute_input":"2021-10-09T06:50:33.917593Z","iopub.status.idle":"2021-10-09T08:43:56.834678Z","shell.execute_reply.started":"2021-10-09T06:50:33.917551Z","shell.execute_reply":"2021-10-09T08:43:56.83373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_predict(pred_v, train.pressure.values.ravel())","metadata":{"execution":{"iopub.status.busy":"2021-10-09T08:43:56.8375Z","iopub.execute_input":"2021-10-09T08:43:56.837794Z","iopub.status.idle":"2021-10-09T08:44:10.714555Z","shell.execute_reply.started":"2021-10-09T08:43:56.837741Z","shell.execute_reply":"2021-10-09T08:44:10.713934Z"},"trusted":true},"execution_count":null,"outputs":[]}]}