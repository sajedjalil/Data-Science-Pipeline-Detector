{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hybrid Model CNN + ENC_DEC LSTM/GRU\n\n**This model produces ~0.161 MAE loss on test dataset. Model can be investigated much more on the optimization to reduce overfitting. It was performed on external GPU supported machine.**","metadata":{}},{"cell_type":"code","source":"#basic imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, GroupKFold, KFold\nfrom tqdm import tqdm\nfrom IPython.display import display\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#upload train, test and sample sumbition datasets\ntrain = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\ntest = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\nsubmission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n\nall_pressure = np.sort(train.pressure.unique())\nPRESSURE_MIN = all_pressure[0].item()\nPRESSURE_MAX = all_pressure[-1].item()\nPRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nFeature engineering process it literally based on two public notebooks with some little modifications. Due to pandas 1.15.0 bug I had to turn off **ewm_u_in_mean** parameter. There is option to use continuous representations of **R** and **C** values - **if flag == True**.\n\n***Notebooks:***\n* https://www.kaggle.com/dlaststark/gb-vpp-whoppity-dub-dub\n* https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153","metadata":{}},{"cell_type":"code","source":"def add_features(df, deg, flag):\n    '''\n    df - processed dataframe\n    deg - degree of lag shifts\n    flag - if True add continuous R, C params\n    '''\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    for i in range(1,deg,1):\n        df['u_in_lag{}'.format(i)] = df.groupby('breath_id')['u_in'].shift(i)\n        df['u_out_lag{}'.format(i)] = df.groupby('breath_id')['u_out'].shift(i)\n        df['u_in_lag_back{}'.format(i)] = df.groupby('breath_id')['u_in'].shift(-i)\n        df['u_out_lag_back{}'.format(i)] = df.groupby('breath_id')['u_out'].shift(-i)\n        \n    df = df.fillna(0)\n    \n    if flag == True:\n    #has to turn off this parameter due to pandas 1.15.0 bug\n        df['ewm_u_in_mean'] = (df\\\n                               .groupby('breath_id')['u_in']\\\n                               .ewm(halflife=9)\\\n                               .mean()\\\n                               .reset_index(level=0,drop=True))\n\n    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n                                                              .groupby('breath_id')['u_in']\\\n                                                              .rolling(window=15,min_periods=1)\\\n                                                              .agg({\"15_in_sum\":\"sum\",\n                                                                    \"15_in_min\":\"min\",\n                                                                    \"15_in_max\":\"max\",\n                                                                    \"15_in_mean\":\"mean\"})\\\n                                                               .reset_index(level=0,drop=True))\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_in__mean'] =df.groupby(['breath_id'])['u_in'].mean()\n    #df['breath_id__u_in__min'] = df.groupby(['breath_id'])['u_in'].transform('min')\n    \n    for i in range(1,deg,1):\n        df['u_in_diff{}'.format(i)] = df['u_in'] - df['u_in_lag{}'.format(i)]\n        df['u_out_diff{}'.format(i)] = df['u_out'] - df['u_out_lag{}'.format(i)]\n\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n\n    \n    df['one'] = 1\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n    \n    \n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n    \n    \n    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n\n    \n    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n    \n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    \n    if flag == True:\n        df['RCmul'] = df['R'].astype(float) * df['C'].astype(float)\n        df['CRdiv'] = df['C'].astype(float) / df['R'].astype(float)\n        df['Ruin'] = df['R'].astype(float) * df['u_in'].astype(float)\n        df['Cuin'] = df['C'].astype(float) * df['u_in'].astype(float)\n    else:\n        pass\n\n    df = pd.get_dummies(df)\n    print(df.shape)\n\n    df = df.fillna(0)\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#perform feature engineering process\nimport gc\n\ntrain = add_features(train, 5, False)\ntest = add_features(test, 5, False)\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check columns encoded columns\ntrain.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling and Sample Weights Mechanism\nCode below prepares final form of model input data by droping useless columns and performing a Robust Scale. This code also includes simple sample weights implementation idea proposed by **Chris Deotte** in the discussion below (here is turned off by setting **do_sample_weights** flag to **False**).\n\nhttps://www.kaggle.com/c/ventilator-pressure-prediction/discussion/278360","metadata":{}},{"cell_type":"code","source":"#set targets and remove unused columns\ntargets = train['pressure'].values.reshape(-1, 80)\n    \ntrain.drop(['pressure','id', 'breath_id','one','count',\n        'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n        'breath_id_lag2same'], axis=1, inplace=True)\n\n\ntest = test.drop(['id', 'breath_id','one','count','breath_id_lag',\n              'breath_id_lag2','breath_id_lagsame',\n              'breath_id_lag2same'], axis=1)\n\nprint(train.shape, test.shape, targets.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### SAMPLE WEIGHTS ###\ndef get_sample_weight_param(train, targets, u_out_1_weight):\n    cols = train.columns.tolist()\n    u_out_index = cols.index(\"u_out\")\n    cols = cols[u_out_index]\n    x_train = train[[cols]].values.reshape((-1, 80, len([cols])))\n\n    # GET SAMPLE WEIGHT\n    U_OUT_IDX = cols.index(\"u_out\")\n    y_weight = np.ones_like(targets)\n    u_out_values = x_train[:,:,U_OUT_IDX]\n\n    #DEFINE U_out == 1 samples weights, if 1 => sampling is turned off\n    y_weight[u_out_values==1] = u_out_1_weight\n    del x_train\n    return y_weight\n\n#set True to do sample weighting during training\ndo_sample_weights = False\n\nif do_sample_weights == True:\n    y_weight = get_sample_weight_param(train, targets, 0.1)\nelse:\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scale Data by Robust Scaler\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check final dimensions\nprint(train.shape, test.shape, targets.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Definition\n\nBasing on Enc/Dec LSTM and GRU model by **DLASTSTARK** I've decided to modify it. I added a CNN/LSTM independent branch which is ten concatenated with ENC/DEC model output. Training and testing process also includes saving partial results to csv files and sample weight mechanism defined before.","metadata":{}},{"cell_type":"code","source":"import time\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, Add, GRU, Conv1D\nfrom tensorflow.keras.layers import Bidirectional, LSTM\nfrom tensorflow.keras.models import Model, load_model\nfrom keras.models import Model\n\ndef hybrid_model():\n    \n    x_input = Input(shape=(train.shape[-2:]))\n    \n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=256, return_sequences=True))(x2)\n    \n    z2 = Bidirectional(GRU(units=256, return_sequences=True))(x2)\n    z3 = Bidirectional(GRU(units=128, return_sequences=True))(Add()([x3, z2]))\n    \n    y1 = Conv1D(128, kernel_size=15, padding='same', activation='relu')(x_input)\n    y2 = Bidirectional(LSTM(units=128, return_sequences=True))(y1)\n    y3 = Bidirectional(LSTM(units=64, return_sequences=True))(y2)\n    \n    x = Concatenate(axis=2)([x3, z2, z3])\n    x = Bidirectional(LSTM(units=256, return_sequences=True))(x)\n    x = Concatenate(axis=2)([x, y3])\n    x = Dense(units=196, activation='selu')(x)\n    x = Dropout(0.01)(x)\n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='Hybrid_Model')\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#init and summarize model\nmodel = hybrid_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot structure of defined model\nplot_model(\n    model, \n    to_file='Hybrid_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GPU training process\nEPOCH = 300\nBATCH_SIZE = 512\nNUM_FOLDS = 5\nTRAIN_MODEL = False\ngpu_strategy = tf.distribute.get_strategy()\n\nif TRAIN_MODEL:\n    if os.path.isdir('./lstm_models'):\n        pass\n    else:\n        os.mkdir('./lstm_models/')\n\n    if os.path.isdir('./logs/'):\n        pass\n    else:\n        os.mkdir('./logs/')\n    \n\n\nwith gpu_strategy.scope():\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        K.clear_session()\n        print(\"{} - Starting {} fold...\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\"), fold))\n        if TRAIN_MODEL:\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n\n            if do_sample_weights == True:\n                y_w_train, y_w_valid = y_weight[train_idx], y_weight[test_idx]\n                y_w_train = y_w_train.reshape(y_w_train.shape[0], y_w_train.shape[1], 1)\n                y_w_valid = y_w_valid.reshape(y_w_valid.shape[0], y_w_valid.shape[1], 1)\n                print(y_train.shape, y_w_train.shape)\n            else:\n                pass\n\n            checkpoint_filepath = './lstm_models/hybrid_folds_{}.hdf5'.format(fold)\n            csv_name = './logs/hybrid_folds_{}.csv'.format(fold)\n        \n        \n            model = hybrid_model()\n            model.compile(optimizer=\"adam\",\n                          #sample_weight_mode=\"temporal\",\n                          loss=\"mae\")\n\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1, mode=\"min\", restore_best_weights=True)\n            csv = keras.callbacks.CSVLogger(csv_name, separator=\",\", append=False)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            \n            if do_sample_weights == True:\n                val_data = (X_valid, y_valid, y_w_valid)\n                sw = y_w_train\n            else:\n                val_data = (X_valid, y_valid)\n                sw = None\n            \n            model.fit(X_train, y_train,\n                      validation_data=val_data,\n                      sample_weight=sw,\n                      epochs=EPOCH, \n                      batch_size=BATCH_SIZE, \n                      callbacks=[lr, es, sv, csv])\n            \n            test_preds.append(model.predict(test, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze())\n            del model, X_train, X_valid; gc.collect()\n            \n        else:\n            checkpoint_filepath = '../input/hybrid-data/hybrid_folds_{}.hdf5'.format(fold)\n            model = keras.models.load_model('{}'.format(checkpoint_filepath))\n            test_preds.append(model.predict(test, batch_size=BATCH_SIZE, verbose=2).squeeze().reshape(-1, 1).squeeze())\n            del model\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"from matplotlib.pyplot import figure\nfrom os import listdir\nfrom os.path import isfile, join","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_results(file):\n    df = pd.read_csv(file)\n    x = df.epoch.values\n    y1 = df.val_loss.values\n    y2 = df.loss.values\n    y3 = df.lr.values * 100\n    print('Min value of val_loss is equal {}'.format(df.val_loss.min()))\n    \n    plt.figure(figsize=(10, 8))\n    \n    plt.plot(x, y1, color = 'g', linestyle = 'dashed',\n            marker = 'o',label = 'val_loss')\n    \n    plt.plot(x, y2, color = 'r', linestyle = 'dashed',\n            marker = 'o',label = \"loss\")\n    \n    plt.plot(x, y3, color = 'b', linestyle = 'dashed',\n            marker = 'o',label = \"lr * 100\")\n    \n    \n    plt.ylim(0, max(y1))\n    plt.xticks(rotation = 25)\n    plt.yticks(np.arange(0, max(y1)+0.1, 0.05))\n    plt.xlabel('epoch')\n    plt.ylabel('value')\n    try:\n        plt.title('Training on fold {}'.format(file.split('.')[2][-1]), fontsize = 10)\n    except:\n        plt.title('Training on fold {}'.format(file.split('.')[1][-1]), fontsize = 10)\n    plt.grid()\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get logs files and print graphic results per fold\nif TRAIN_MODEL == True:\n    path = './logs/'\n    csvs = [f for f in listdir(path) if isfile(join(path, f))]\nelse:\n    path = '../input/hybrid-data/'\n    files = [f for f in listdir(path) if isfile(join(path, f))]\n    csvs = []\n    for f in files:\n        if f[-4:] == '.csv':\n            csvs.append(f)\n        else:\n            pass\n\ncsvs.remove('submission_median_hybrid.csv')\ncsvs.sort()\n\nfor c in csvs:\n    print('-----------------------------------------------')\n    file = path + c\n    print_results(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ENSEMBLE FOLDS WITH MEDIAN\nsubmission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}