{"cells":[{"cell_type":"markdown","id":"expired-athletics","metadata":{},"source":"\n# What is Serial Dependence? #\n\nIn earlier lessons, we investigated properties of time series that were most easily modeled as *time dependent* properties, that is, with features we could derive directly from the time index. Some time series properties, however, can only be modeled as *serially dependent* properties, that is, using as features past values of the target series. The structure of these time series may not be apparent from a plot over time; plotted against past values, however, the structure becomes clear -- as we see in the figure below below.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/X0sSnwp.png\" width=800, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>These two series have serial dependence, but not time dependence. Points on the right have coordinates <code>(value at time t-1, value at time t)</code>.\n</center></figcaption>\n</figure>\n\nWith trend and seasonality, we trained models to fit curves to plots like those on the left in the figure above -- the models were learning time dependence. The goal in this lesson is to train models to fit curves to plots like those on the right -- we want them to learn serial dependence.\n\n### Cycles\n\nOne especially common way for serial dependence to manifest is in **cycles**. Cycles are patterns of growth and decay in a time series associated with how the value in a series at one time depends on values at previous times, but not necessarily on the time step itself.  Cyclic behavior is characteristic of systems that can affect themselves or whose reactions persist over time. Economies, epidemics, animal populations, volcano eruptions, and similar natural phenomena often display cyclic behavior.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/CC3TkAf.png\" width=800, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Four time series with cyclic behavior.\n</center></figcaption>\n</figure>\n\nWhat distinguishes cyclic behavior from seasonality is that cycles are not necessarily time dependent, as seasons are. What happens in a cycle is less about the particular date of occurence, and more about what has happened in the recent past. The (at least relative) independence from time means that cyclic behavior can be much more irregular than seasonality.\n\n# Lagged Series and Lag Plots #\n\nTo investigate possible serial dependence (like cycles) in a time series, we need to create \"lagged\" copies of the series. **Lagging** a time series means to shift its values forward one or more time steps, or equivalently, to shift the times in its index backward one or more steps. In either case, the effect is that the observations in the lagged series will appear to have happened later in time.\n\nThis shows the monthly unemployment rate in the US (`y`) together with its first and second lagged series (`y_lag_1` and `y_lag_2`, respectively). Notice how the values of the lagged series are shifted forward in time."},{"cell_type":"code","execution_count":null,"id":"parallel-carry","metadata":{"_kg_hide-input":true},"outputs":[],"source":"\nimport pandas as pd\n\n# Federal Reserve dataset: https://www.kaggle.com/federalreserve/interest-rates\nreserve = pd.read_csv(\n    \"../input/ts-course-data/reserve.csv\",\n    parse_dates={'Date': ['Year', 'Month', 'Day']},\n    index_col='Date',\n)\n\ny = reserve.loc[:, 'Unemployment Rate'].dropna().to_period('M')\ndf = pd.DataFrame({\n    'y': y,\n    'y_lag_1': y.shift(1),\n    'y_lag_2': y.shift(2),    \n})\n\ndf.head()"},{"cell_type":"markdown","id":"rising-richmond","metadata":{},"source":"By lagging a time series, we can make its past values appear contemporaneous with the values we are trying to predict (in the same row, in other words). This makes lagged series useful as features for modeling serial dependence. To forecast the US unemployment rate series, we could use `y_lag_1` and `y_lag_2` as features to predict the target `y`. This would forecast the future unemployment rate as a function of the unemployment rate in the prior two months.\n\n### Lag plots\n\nA **lag plot** of a time series shows its values plotted against its lags. Serial dependence in a time series will often become apparent by looking at a lag plot. We can see from this lag plot of *US Unemployment* that there is a strong and apparently linear relationship between the current unemployment rate and past rates.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/Hvrboya.png\" width=600, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Lag plot of US Unemployment with autocorrelations indicated.\n</center></figcaption>\n</figure>\n\nThe most commonly used measure of serial dependence is known as **autocorrelation**, which is simply the correlation a time series has with one of its lags. *US Unemployment* has an autocorrelation of 0.99 at lag 1, 0.98 at lag 2, and so on.\n\n### Choosing lags\n\nWhen choosing lags to use as features, it generally won't be useful to include *every* lag with a large autocorrelation. In *US Unemployment*, for instance, the autocorrelation at lag 2 might result entirely from \"decayed\" information from lag 1 -- just correlation that's carried over from the previous step. If lag 2 doesn't contain anything new, there would be no reason to include it if we already have lag 1.\n\nThe **partial autocorrelation** tells you the correlation of a lag accounting for all of the previous lags -- the amount of \"new\" correlation the lag contributes, so to speak. Plotting the partial autocorrelation can help you choose which lag features to use. In the figure below, lag 1 through lag 6 fall outside the intervals of \"no correlation\" (in blue), so we might choose lags 1 through lag 6 as features for *US Unemployment*. (Lag 11 is likely a false positive.)\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/6nTe94E.png\" width=600, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Partial autocorrelations of US Unemployment through lag 12 with 95% confidence intervals of no correlation.\n</center></figcaption>\n</figure>\n\nA plot like that above is known as a *correlogram*. The correlogram is for lag features essentially what the periodogram is for Fourier features.\n\nFinally, we need to be mindful that autocorrelation and partial autocorrelation are measures of *linear* dependence. Because real-world time series often have substantial non-linear dependences, it's best to look at a lag plot (or use some more general measure of dependence, like [mutual information](https://www.kaggle.com/ryanholbrook/mutual-information)) when choosing lag features. The *Sunspots* series has lags with non-linear dependence which we might overlook with autocorrelation.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/Q38UVOu.png\" width=350, alt=\"\">\n    <figcaption style=\"textalign: center; font-style: italic\"><center>Lag plot of the <em>Sunspots</em> series.\n</center></figcaption>\n</figure>\n\nNon-linear relationships like these can either be transformed to be linear or else learned by an appropriate algorithm.\n\n# Example - Flu Trends #\n\nThe *Flu Trends* dataset contains records of doctor's visits for the flu for weeks between 2009 and 2016. Our goal is to forecast the number of flu cases for the coming weeks.\n\nWe will take two approaches. In the first we'll forecast doctor's visits using lag features. Our second approach will be to forecast doctor's visits using lags of *another* set of time series: flu-related search terms as captured by Google Trends."},{"cell_type":"code","execution_count":null,"id":"seven-hollow","metadata":{"_kg_hide-input":true},"outputs":[],"source":"\nfrom pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.signal import periodogram\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nsimplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 4))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n%config InlineBackend.figure_format = 'retina'\n\n\ndef lagplot(x, y=None, lag=1, standardize=False, ax=None, **kwargs):\n    from matplotlib.offsetbox import AnchoredText\n    x_ = x.shift(lag)\n    if standardize:\n        x_ = (x_ - x_.mean()) / x_.std()\n    if y is not None:\n        y_ = (y - y.mean()) / y.std() if standardize else y\n    else:\n        y_ = x\n    corr = y_.corr(x_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    scatter_kws = dict(\n        alpha=0.75,\n        s=3,\n    )\n    line_kws = dict(color='C3', )\n    ax = sns.regplot(x=x_,\n                     y=y_,\n                     scatter_kws=scatter_kws,\n                     line_kws=line_kws,\n                     lowess=True,\n                     ax=ax,\n                     **kwargs)\n    at = AnchoredText(\n        f\"{corr:.2f}\",\n        prop=dict(size=\"large\"),\n        frameon=True,\n        loc=\"upper left\",\n    )\n    at.patch.set_boxstyle(\"square, pad=0.0\")\n    ax.add_artist(at)\n    ax.set(title=f\"Lag {lag}\", xlabel=x_.name, ylabel=y_.name)\n    return ax\n\n\ndef plot_lags(x, y=None, lags=6, nrows=1, lagplot_kwargs={}, **kwargs):\n    import math\n    kwargs.setdefault('nrows', nrows)\n    kwargs.setdefault('ncols', math.ceil(lags / nrows))\n    kwargs.setdefault('figsize', (kwargs['ncols'] * 2, nrows * 2 + 0.5))\n    fig, axs = plt.subplots(sharex=True, sharey=True, squeeze=False, **kwargs)\n    for ax, k in zip(fig.get_axes(), range(kwargs['nrows'] * kwargs['ncols'])):\n        if k + 1 <= lags:\n            ax = lagplot(x, y, lag=k + 1, ax=ax, **lagplot_kwargs)\n            ax.set_title(f\"Lag {k + 1}\", fontdict=dict(fontsize=14))\n            ax.set(xlabel=\"\", ylabel=\"\")\n        else:\n            ax.axis('off')\n    plt.setp(axs[-1, :], xlabel=x.name)\n    plt.setp(axs[:, 0], ylabel=y.name if y is not None else x.name)\n    fig.tight_layout(w_pad=0.1, h_pad=0.1)\n    return fig\n\n\ndata_dir = Path(\"../input/ts-course-data\")\nflu_trends = pd.read_csv(data_dir / \"flu-trends.csv\")\nflu_trends.set_index(\n    pd.PeriodIndex(flu_trends.Week, freq=\"W\"),\n    inplace=True,\n)\nflu_trends.drop(\"Week\", axis=1, inplace=True)\n\nax = flu_trends.FluVisits.plot(title='Flu Trends', **plot_params)\n_ = ax.set(ylabel=\"Office Visits\")"},{"cell_type":"markdown","id":"reported-canvas","metadata":{},"source":"Our *Flu Trends* data shows irregular cycles instead of a regular seasonality: the peak tends to occur around the new year, but sometimes earlier or later, sometimes larger or smaller. Modeling these cycles with lag features will allow our forecaster to react dynamically to changing conditions instead of being constrained to exact dates and times as with seasonal features.\n\nLet's take a look at the lag and autocorrelation plots first:"},{"cell_type":"code","execution_count":null,"id":"choice-seventh","metadata":{},"outputs":[],"source":"_ = plot_lags(flu_trends.FluVisits, lags=12, nrows=2)\n_ = plot_pacf(flu_trends.FluVisits, lags=12)"},{"cell_type":"markdown","id":"analyzed-demonstration","metadata":{},"source":"The lag plots indicate that the relationship of `FluVisits` to its lags is mostly linear, while the partial autocorrelations suggest the dependence can be captured using lags 1, 2, 3, and 4. We can lag a time series in Pandas with the `shift` method. For this problem, we'll fill in the missing values the lagging creates with `0.0`."},{"cell_type":"code","execution_count":null,"id":"changing-yemen","metadata":{},"outputs":[],"source":"def make_lags(ts, lags):\n    return pd.concat(\n        {\n            f'y_lag_{i}': ts.shift(i)\n            for i in range(1, lags + 1)\n        },\n        axis=1)\n\n\nX = make_lags(flu_trends.FluVisits, lags=4)\nX = X.fillna(0.0)"},{"cell_type":"markdown","id":"viral-shelter","metadata":{},"source":"In previous lessons, we were able to create forecasts for as many steps as we liked beyond the training data. When using lag features, however, we are limited to forecasting time steps whose lagged values are available. Using a lag 1 feature on Monday, we can't make a forecast for Wednesday because the lag 1 value needed is Tuesday which hasn't happened yet.\n\nWe'll see strategies for handling this problem in Lesson 6. For this example, we'll just use a values from a test set."},{"cell_type":"code","execution_count":null,"id":"unauthorized-sound","metadata":{"_kg_hide-input":true,"lines_to_next_cell":0},"outputs":[],"source":"\n# Create target series and data splits\ny = flu_trends.FluVisits.copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False)\n\n# Fit and predict\nmodel = LinearRegression()  # `fit_intercept=True` since we didn't use DeterministicProcess\nmodel.fit(X_train, y_train)\ny_pred = pd.Series(model.predict(X_train), index=y_train.index)\ny_fore = pd.Series(model.predict(X_test), index=y_test.index)"},{"cell_type":"code","execution_count":null,"id":"appointed-jonathan","metadata":{"_kg_hide-input":true},"outputs":[],"source":"\nax = y_train.plot(**plot_params)\nax = y_test.plot(**plot_params)\nax = y_pred.plot(ax=ax)\n_ = y_fore.plot(ax=ax, color='C3')"},{"cell_type":"markdown","id":"colonial-boards","metadata":{},"source":"---\n\nLooking just at the forecast values, we can see how our model needs a time step to react to sudden changes in the target series. This is a common limitation of models using only lags of the target series as features."},{"cell_type":"code","execution_count":null,"id":"abroad-possession","metadata":{"_kg_hide-input":true},"outputs":[],"source":"\nax = y_test.plot(**plot_params)\n_ = y_fore.plot(ax=ax, color='C3')"},{"cell_type":"markdown","id":"assigned-criminal","metadata":{},"source":"To improve the forecast we could try to find *leading indicators*, time series that could provide an \"early warning\" for changes in flu cases. For our second approach then we'll add to our training data the popularity of some flu-related search terms as measured by Google Trends.\n\nPlotting the search phrase `'FluCough'` against the target `'FluVisits'` suggests such search terms could be useful as leading indicators: flu-related searches tend to become more popular in the weeks prior to office visits."},{"cell_type":"code","execution_count":null,"id":"unavailable-logan","metadata":{"_kg_hide-input":true},"outputs":[],"source":"\nax = flu_trends.plot(\n    y=[\"FluCough\", \"FluVisits\"],\n    secondary_y=\"FluCough\",\n)"},{"cell_type":"markdown","id":"structural-product","metadata":{},"source":"The dataset contains 129 such terms, but we'll just use a few."},{"cell_type":"code","execution_count":null,"id":"spectacular-exception","metadata":{},"outputs":[],"source":"search_terms = [\"FluContagious\", \"FluCough\", \"FluFever\", \"InfluenzaA\", \"TreatFlu\", \"IHaveTheFlu\", \"OverTheCounterFlu\", \"HowLongFlu\"]\n\n# Create three lags for each search term\nX0 = make_lags(flu_trends[search_terms], lags=3)\n\n# Create four lags for the target, as before\nX1 = make_lags(flu_trends['FluVisits'], lags=4)\n\n# Combine to create the training data\nX = pd.concat([X0, X1], axis=1).fillna(0.0)"},{"cell_type":"markdown","id":"backed-travel","metadata":{},"source":"Our forecasts are a bit rougher, but our model appears to be better able to anticipate sudden increases in flu visits, suggesting that the several time series of search popularity were indeed effective as leading indicators."},{"cell_type":"code","execution_count":null,"id":"given-pittsburgh","metadata":{"_kg_hide-input":true,"lines_to_next_cell":2},"outputs":[],"source":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=60, shuffle=False)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = pd.Series(model.predict(X_train), index=y_train.index)\ny_fore = pd.Series(model.predict(X_test), index=y_test.index)\n\nax = y_test.plot(**plot_params)\n_ = y_fore.plot(ax=ax, color='C3')"},{"cell_type":"markdown","id":"polished-building","metadata":{},"source":"---\n\nThe time series illustrated in this lesson are what you might call \"purely cyclic\": they have no obvious trend or seasonality. It's not uncommon though for time series to possess trend, seasonality, and cycles -- all three components at once. You could model such series with linear regression by just adding the appropriate features for each component. You can even combine models trained to learn the components separately, which we'll learn how to do in the next lesson with *forecasting hybrids*."},{"cell_type":"markdown","id":"painful-baptist","metadata":{},"source":"# Your Turn #\n\n[**Create lag features for Store Sales**](https://www.kaggle.com/kernels/fork/19616002) and explore other kinds of time series features."},{"cell_type":"markdown","id":"b566a405","metadata":{},"source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/time-series/discussion) to chat with other learners.*"}],"metadata":{"jupytext":{"formats":"ipynb,md"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":5}