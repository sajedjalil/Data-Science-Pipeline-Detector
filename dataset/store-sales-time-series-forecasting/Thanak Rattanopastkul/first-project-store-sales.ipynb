{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Note:\nThis is really my first attempt to learn Time Series. I'm sure that, despite the fact that the I'm really tried on EDA to get some data characteristics (and I'm happy about the EDA), The data formatting and model training is not what I'm proud of. I keep this notebook as a score record for the next iterations to come.\n\nYou can find the other version with the highter score here:\n\n* [2# attempt : Focusing on data formatting and expanding windows](https://www.kaggle.com/code/thanakr/beginner-s-project-2nd-attempt)\n* [3# attempt : Pipeline, Feature Selection, More complicated models](https://www.kaggle.com/thanakr/beginner-s-project-3-pipeline-and-models)\n\n# Intro\n\nHere is my very first attempt to learn the time series data exploration and machine learning. After studied many MOOCS and some articles, I think the only way to make me understand is to practice the knowledge with some project. And here I am. This is still a work in progress. I don't think I can do everything in one go, thus the contents will be added eventually. If someone wander around and see this notebook, feel free to leave any comments or suggestions in the discussion panel (I'm pretty sure that, for the first project, my code is dirty). I would be really appreciated with any comment for further improvment.\n\n**Note:** Since this is a competition used in kaggle's time series course and the score I get just form copying the example that used statsmodels.tsa.deterministic.CalenderFourier() and statsmodels.tsa.deterministic.DeterministicProcess is **0.51090**. Let's see what score I'd get from doing the task again without the structured guide.\n\n**Reference:**\n\nhttps://www.kaggle.com/ilyakondrusevich/54-stores-54-models\n\nhttps://www.kaggle.com/hiro5299834/store-sales-ridge-voting-bagging-et-bagging-rf  \n\nhttps://www.kaggle.com/andrej0marinchenko/hyperparamaters#DeterministicProcess  \n\nhttps://www.kaggle.com/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide  ","metadata":{}},{"cell_type":"markdown","source":"# The Plan\n\n1. define the goal\n2. define problem\n2. get aquainted the source dataset\n    * data exploration\n3. reserch models to use\n4. data processing + feature engineering\n5. built model(s)\n6. validate model(s)\n7. test the model(s)\n8. submit the prediction to evauate the project by the submission score (dont forget to log the sore and changes in each iteration)\n9. summarize the insight gained from the process\n10. What could have done better??\n11. Notes\n\n","metadata":{}},{"cell_type":"markdown","source":"# 1. define the goal\n\nThis is the kaggle's eduacationnal dataset for learning time series prediction. The task is to predict the sales of (thoundsands) of pruducts of an Equadorian shopping mall.\n\nTo do that, Kaggle have provided few datasets to be used as a features in the ML trainig dataset.\n\nThe data consist of 7 datasets namely:\n1. **training.csv** : the training dataset\n2. **transaction.csv** : log of the amount of daily 1transaction across all the stores\n3. **stores.csv** : every store informations such as area etc.\n4. **holiday_events.csv** : list of holiday held in Equador \n5. **oil.csv** : Historical price of crude oil (Equador main economic drive is the oil production)\n6. **test.csv** : dataset for testing the model\n7. **sample_submission.csv** : example of submission csv format\n\nAll the data will be further explored in the data exploration part.\n\nNote: the task also state some additional informations that might be handy later on.\n1. Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Supermarket sales could be affected by this.\n2. A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.\n","metadata":{}},{"cell_type":"markdown","source":"First of all let's take a note about the goal of the competition, which is:\n    \n### Predict the price of products in the next 15 days from the lastday of the training data.\n\nLet's take a look at test.csv and sample_submission.csv","metadata":{}},{"cell_type":"code","source":"from warnings import simplefilter\nsimplefilter(\"ignore\")  # ignore warnings to clean up output cells\n\nimport gc # for garbage cleaning\n\n%config Completer.use_jedi = False # for autocomplete ","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:15.458042Z","iopub.execute_input":"2022-03-10T14:13:15.458405Z","iopub.status.idle":"2022-03-10T14:13:15.496481Z","shell.execute_reply.started":"2022-03-10T14:13:15.458317Z","shell.execute_reply":"2022-03-10T14:13:15.495768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:15.498186Z","iopub.execute_input":"2022-03-10T14:13:15.498705Z","iopub.status.idle":"2022-03-10T14:13:15.506761Z","shell.execute_reply.started":"2022-03-10T14:13:15.498661Z","shell.execute_reply":"2022-03-10T14:13:15.506133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-10T14:13:15.507873Z","iopub.execute_input":"2022-03-10T14:13:15.508479Z","iopub.status.idle":"2022-03-10T14:13:15.515606Z","shell.execute_reply.started":"2022-03-10T14:13:15.508424Z","shell.execute_reply":"2022-03-10T14:13:15.514808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\")\nholiday_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\")\nstore_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\")\ntrain_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\ntransaction_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/transactions.csv\")\nsample = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv')","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-03-10T14:13:15.517807Z","iopub.execute_input":"2022-03-10T14:13:15.518151Z","iopub.status.idle":"2022-03-10T14:13:18.917783Z","shell.execute_reply.started":"2022-03-10T14:13:15.518112Z","shell.execute_reply":"2022-03-10T14:13:18.9169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:18.919313Z","iopub.execute_input":"2022-03-10T14:13:18.919731Z","iopub.status.idle":"2022-03-10T14:13:18.942791Z","shell.execute_reply.started":"2022-03-10T14:13:18.919681Z","shell.execute_reply":"2022-03-10T14:13:18.941841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:18.944396Z","iopub.execute_input":"2022-03-10T14:13:18.944713Z","iopub.status.idle":"2022-03-10T14:13:18.956319Z","shell.execute_reply.started":"2022-03-10T14:13:18.944669Z","shell.execute_reply":"2022-03-10T14:13:18.9551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to predict the sales on 2017-08-16 to 2017-08-31 for various kind of product and submit with this format.","metadata":{}},{"cell_type":"markdown","source":"# 2. Define the problem\n\nSince this is the time series prediction problem, I will utilize the linear regression models to predict the expected sales for each product. However, there are so many features and complimentary datasets. Therefore, additional model might be useful too.","metadata":{}},{"cell_type":"markdown","source":"\n# 3. The data exploration\n\nHere, I planed to explore these areas within all the datasets. In order to get the idea of what might be an interesting features to train the model.\n\nTask:\n\n1. **Total sales** : Hope to see an overall trend or the some spike in sales (perhaps the earthquake would have a big effect.)\n2. **Daily sales by each stores** : Hope to see the trend and pattern in sales throughout the timeframe.\n3. **Sales by product family, by time** : Hope to see the trend and pattern in sales throughout the timeframe.\n4. **Store Location/Cluster Effect on sales** : This might be a good feature in further sale prediction.\n5. **Onpromotion effect on sales**\n6. **Cycle and Seasonal Effect on total sales** : Hope to see some bi-weekly effect or something interesting.\n7. **Map the holiday to the sales record:** I guess that holiday will boost the sales.\n8. **Calculate the correlation in change in oil price to change in total sales:** At this point I assume that the fluctuation in oil price won't affect the sales in short term. Since the change in oil price take times to effect the national income and ultimately business activites..\n9. **Explore the transaction data** ","metadata":{}},{"cell_type":"code","source":"train_data.head()\n#seems like the store is close on new year day since they are all 0 sales in 1 Jan.","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:18.958334Z","iopub.execute_input":"2022-03-10T14:13:18.958694Z","iopub.status.idle":"2022-03-10T14:13:18.976003Z","shell.execute_reply.started":"2022-03-10T14:13:18.958643Z","shell.execute_reply":"2022-03-10T14:13:18.974915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()\n#over 3m. lines of data with a date formatted as a string dtype","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:18.97778Z","iopub.execute_input":"2022-03-10T14:13:18.978045Z","iopub.status.idle":"2022-03-10T14:13:19.00788Z","shell.execute_reply.started":"2022-03-10T14:13:18.978016Z","shell.execute_reply":"2022-03-10T14:13:19.006914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('there are {} different product families'.format(train_data.family.nunique()))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:19.009603Z","iopub.execute_input":"2022-03-10T14:13:19.009924Z","iopub.status.idle":"2022-03-10T14:13:19.255791Z","shell.execute_reply.started":"2022-03-10T14:13:19.00989Z","shell.execute_reply":"2022-03-10T14:13:19.255119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('there are {} different stores'.format(train_data.store_nbr.nunique()))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:19.258421Z","iopub.execute_input":"2022-03-10T14:13:19.258816Z","iopub.status.idle":"2022-03-10T14:13:19.281426Z","shell.execute_reply.started":"2022-03-10T14:13:19.258783Z","shell.execute_reply":"2022-03-10T14:13:19.280442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"train_data consist of several column and over 3 million rows\n* id -> will be dropped\n* date -> need to format to dt and use as index\n* store number -> 1 to 54\n* (product) family\n* sales (trying to predict this)\n* onpromotion. -> gives the total number of items in a product family that were being promoted at a store at a given date","metadata":{}},{"cell_type":"code","source":"#let's format the datetime and use as index and drop the id column.\nprocess_train = train_data.copy()\n\ndel train_data\n\nprocess_train['date'] = pd.to_datetime(process_train['date'])\nprocess_train = process_train.set_index('date')\nprocess_train = process_train.drop('id',axis = 1)\nprocess_train[['store_nbr','family']].astype('category') #to reduce ram usage\nprocess_train","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:19.283045Z","iopub.execute_input":"2022-03-10T14:13:19.28363Z","iopub.status.idle":"2022-03-10T14:13:20.506169Z","shell.execute_reply.started":"2022-03-10T14:13:19.283583Z","shell.execute_reply":"2022-03-10T14:13:20.505304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's check the possible missing data / observation.","metadata":{}},{"cell_type":"code","source":"process_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:20.50747Z","iopub.execute_input":"2022-03-10T14:13:20.50773Z","iopub.status.idle":"2022-03-10T14:13:20.845398Z","shell.execute_reply.started":"2022-03-10T14:13:20.5077Z","shell.execute_reply":"2022-03-10T14:13:20.844388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"check the missing date (missing the entire observation).\n\nLet's begin by checking the time-lag between each observation.","metadata":{}},{"cell_type":"code","source":"def count_day_gap(df):\n    temp = df.reset_index().groupby(['date']).sales.sum()\n    return (temp.index[1:]-temp.index[:-1]).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:20.846593Z","iopub.execute_input":"2022-03-10T14:13:20.846818Z","iopub.status.idle":"2022-03-10T14:13:20.852554Z","shell.execute_reply.started":"2022-03-10T14:13:20.846791Z","shell.execute_reply":"2022-03-10T14:13:20.85158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_day_gap(process_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:20.853922Z","iopub.execute_input":"2022-03-10T14:13:20.854175Z","iopub.status.idle":"2022-03-10T14:13:20.998212Z","shell.execute_reply.started":"2022-03-10T14:13:20.854137Z","shell.execute_reply":"2022-03-10T14:13:20.997218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4 rows of the data are missing as we see that the 2 days time-gap are presented 4 time. Where are thay?","metadata":{}},{"cell_type":"code","source":"temp = process_train.reset_index().groupby(['date']).sales.sum().to_frame()\ngap = (temp.index[1:]-temp.index[:-1]).to_list()\ngap.insert(0,'first day') #add to gap list to have the same length as temp to combine it together\ntemp['gap'] = gap\n#print(temp.gap.unique())\n\nday_skip = temp.groupby('gap').get_group(temp.gap.unique()[2])\nday_skip","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:21.000306Z","iopub.execute_input":"2022-03-10T14:13:21.000546Z","iopub.status.idle":"2022-03-10T14:13:21.173994Z","shell.execute_reply.started":"2022-03-10T14:13:21.000518Z","shell.execute_reply":"2022-03-10T14:13:21.173127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del temp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:21.175415Z","iopub.execute_input":"2022-03-10T14:13:21.175688Z","iopub.status.idle":"2022-03-10T14:13:21.180088Z","shell.execute_reply.started":"2022-03-10T14:13:21.175642Z","shell.execute_reply":"2022-03-10T14:13:21.179148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2013 - 2014 - 2015 -2016 Christmas days (20xx-12-25) are all missing. \n\nLet's see if there are other days that the total sales across country is zero. ","metadata":{}},{"cell_type":"code","source":"process_train.groupby('date').sales.sum().sort_values().head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:21.181392Z","iopub.execute_input":"2022-03-10T14:13:21.181649Z","iopub.status.idle":"2022-03-10T14:13:21.258772Z","shell.execute_reply.started":"2022-03-10T14:13:21.18162Z","shell.execute_reply":"2022-03-10T14:13:21.257734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Besides Christmas day, there are no other day that have zero sales. The Christmas will be add back in the holiday EDA section.\n","metadata":{}},{"cell_type":"markdown","source":"### Let's see a total sales combined","metadata":{}},{"cell_type":"markdown","source":"First of all, let's see total sales as a stackpolot. I could get the idea of what's selling well and the major trend of total sales. Also, I will mark the earthquake to see if the quake noticably affect the sales.","metadata":{}},{"cell_type":"code","source":"date_fam_sale = process_train.groupby(['date','family']).sum().sales\nunstack = date_fam_sale.unstack()\nunstack = unstack.resample('1M').sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:21.260059Z","iopub.execute_input":"2022-03-10T14:13:21.260445Z","iopub.status.idle":"2022-03-10T14:13:21.764914Z","shell.execute_reply.started":"2022-03-10T14:13:21.260412Z","shell.execute_reply":"2022-03-10T14:13:21.76386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nax.set(title=\"'Total Monthly Sales From each Family\")\nplt.stackplot(unstack.index,unstack.T,labels=unstack.T.index)\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nplt.axvline(x=pd.Timestamp('2016-04-16'),color='black',linestyle='--',linewidth=4,alpha=0.5)\nplt.text(pd.Timestamp('2016-04-20'),30000000,'  The Earthquake',rotation=360,c='black',size=16)\nplt.xticks(rotation=90)\nplt.legend(loc='lower center',bbox_to_anchor=(0.5,-0.2),ncol=11)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:21.767004Z","iopub.execute_input":"2022-03-10T14:13:21.767256Z","iopub.status.idle":"2022-03-10T14:13:23.245849Z","shell.execute_reply.started":"2022-03-10T14:13:21.767228Z","shell.execute_reply":"2022-03-10T14:13:23.244879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del unstack\ndel date_fam_sale","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:23.246942Z","iopub.execute_input":"2022-03-10T14:13:23.24766Z","iopub.status.idle":"2022-03-10T14:13:23.251511Z","shell.execute_reply.started":"2022-03-10T14:13:23.247609Z","shell.execute_reply":"2022-03-10T14:13:23.25074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The stackplot shows that only few product families accounted for a majority of sales. And the earthquake may cause a spike in sales for a short while. The steep drop at the end of the data is caused by the 2017-08 data that has been given from 2017-08-1 to 2017-08-15. So the drop is assosiated with the missing of the other half month.\n\nThe color is still confusing. Let's do the barchart to rank the product which are sold the most.","metadata":{}},{"cell_type":"code","source":"month_family = process_train.groupby('family').resample('M').sales.sum() #resample to monthly sales \n\nfig, ax = plt.subplots(figsize=(7,7))\nplt.barh(month_family.groupby('family').sum().sort_values().index,month_family.groupby('family').sum().sort_values())\nax.set(title='Total Sales by Product Family')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:23.252701Z","iopub.execute_input":"2022-03-10T14:13:23.252949Z","iopub.status.idle":"2022-03-10T14:13:24.390024Z","shell.execute_reply.started":"2022-03-10T14:13:23.25292Z","shell.execute_reply":"2022-03-10T14:13:24.389086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's calculte the proportaion of sales by category\n","metadata":{}},{"cell_type":"code","source":"total_sale = month_family.sum()\nfamily_sale = month_family.groupby('family').sum().sort_values()\nproportion = ((family_sale/total_sale)*100).sort_values(ascending=False)\nproportion = pd.DataFrame(proportion)\nproportion.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:24.391645Z","iopub.execute_input":"2022-03-10T14:13:24.392009Z","iopub.status.idle":"2022-03-10T14:13:24.407054Z","shell.execute_reply.started":"2022-03-10T14:13:24.391961Z","shell.execute_reply":"2022-03-10T14:13:24.406241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del family_sale","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:24.408792Z","iopub.execute_input":"2022-03-10T14:13:24.409374Z","iopub.status.idle":"2022-03-10T14:13:24.420495Z","shell.execute_reply.started":"2022-03-10T14:13:24.409328Z","shell.execute_reply":"2022-03-10T14:13:24.419425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The product families which accounted for the most sales are\n1. GROCERY I - 31.99%\n2. BEVERAGES - 20.21%\n3. PRODUCE - 11.43%\n4. CLEANING - 9.08%\n5. DAIRY - 6%\n\nJust for the top five, they have accounted for more than 75% of total sales already.","metadata":{"_kg_hide-output":true}},{"cell_type":"markdown","source":"Let's take a look at daily total sales.","metadata":{}},{"cell_type":"code","source":"import matplotlib.dates as mdates\n\nfig, ax = plt.subplots(figsize=(18, 7))\nax.set(title=\"'Total Sales Across All Stores\")\ntotal_sales = process_train.sales.groupby(\"date\").sum()\nplt.plot(total_sales)\n\nax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\nplt.xticks(rotation=70)\nplt.axvline(x=pd.Timestamp('2016-04-16'),color='r',linestyle='--',linewidth=4,alpha=0.3)\nplt.text(pd.Timestamp('2016-04-20'),1400000,'The Earthquake',rotation=360,c='r')\n\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:13:24.421926Z","iopub.execute_input":"2022-03-10T14:13:24.422526Z","iopub.status.idle":"2022-03-10T14:13:25.480773Z","shell.execute_reply.started":"2022-03-10T14:13:24.422492Z","shell.execute_reply":"2022-03-10T14:13:25.477027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** Overall, I see an uptrend in total sales across the country since 2013. Let's check why there are those super deep valleys near many year's end. My guess now is that all the shop are closed at the New Year or New Year Eve for holiday so there is no sales. I will reconfirm this again at the holiday analyses.\n\nNoted that the earthquake marked with the red dotted band seems to boost the sales few days after.","metadata":{}},{"cell_type":"markdown","source":"### Let's plot each store sale by date","metadata":{}},{"cell_type":"code","source":"#create a dic that contain each store total daily sale.\ndaily_sale_dict = {}\nfor i in process_train.store_nbr.unique():\n    daily_sale = process_train[process_train['store_nbr']==i]\n    daily_sale_dict[i] = daily_sale","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:25.4829Z","iopub.execute_input":"2022-03-10T14:13:25.483384Z","iopub.status.idle":"2022-03-10T14:13:25.892495Z","shell.execute_reply.started":"2022-03-10T14:13:25.483323Z","shell.execute_reply":"2022-03-10T14:13:25.891292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30,30))\nfor i in daily_sale_dict.keys():\n    plt.subplot(8,7,i)\n    plt.title('Store {} sale'.format(i))\n    plt.tight_layout(pad=5)\n    sale = daily_sale_dict[i].sales\n    sale.plot()\n    plt.axvline(x=pd.Timestamp('2016-04-16'),color='r',linestyle='--',linewidth=2,alpha=0.3) #mark the earthquake","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:13:25.893821Z","iopub.execute_input":"2022-03-10T14:13:25.894128Z","iopub.status.idle":"2022-03-10T14:14:30.600411Z","shell.execute_reply.started":"2022-03-10T14:13:25.894084Z","shell.execute_reply":"2022-03-10T14:14:30.599278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** There are so many \"gaps\" or \"white space\" in some stores sales record. I guess they are from either temporary store close or the store not yet operate. \n\nOne thing to note is that, from times to times, there will be a sharp spike in sales (ex. store # 35 which had 3 spikes around the year's end of 2014, 2015, 2016. This may need futher scrutinization.\n\nConsidering the earthquake, it affected the sales variedly from store to store. Some store such as 18,20,21,etc.. saw a great one time spike. Some store such as store 5,26,35,etc.. didn't have any huge sales changes. However, for store 53, it's the gamechanger. Store 53 had gone through a long duration of sale increase for years.\n\nThe main theme here is that, on average, store sales are in uptrend since 2013.","metadata":{}},{"cell_type":"markdown","source":"### What about the sales by Product Family.","metadata":{}},{"cell_type":"code","source":"by_fam_dic = {}\nfam_list = process_train.family.unique()\n\nfor fam in fam_list:\n    by_fam_dic[fam] = process_train[process_train['family']==fam].sales","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:14:30.601744Z","iopub.execute_input":"2022-03-10T14:14:30.602232Z","iopub.status.idle":"2022-03-10T14:14:45.451852Z","shell.execute_reply.started":"2022-03-10T14:14:30.602196Z","shell.execute_reply":"2022-03-10T14:14:45.45092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(30,50))\n\nfor i,fam in enumerate(by_fam_dic.keys()):\n    plt.subplot(11,3,i+1)\n    plt.title('{} sale'.format(fam))\n    plt.tight_layout(pad=5)\n    sale = by_fam_dic[fam]\n    sale.plot()\n    plt.axvline(x=pd.Timestamp('2016-04-16'),color='r',linestyle='--',linewidth=2,alpha=0.3) #mark the earthquake\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:14:45.457678Z","iopub.execute_input":"2022-03-10T14:14:45.457972Z","iopub.status.idle":"2022-03-10T14:15:26.577691Z","shell.execute_reply.started":"2022-03-10T14:14:45.457938Z","shell.execute_reply":"2022-03-10T14:15:26.5767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** Each family has their own selling paterns. However, Frozen Food and School and Office Supplies shown highly seasonal cycle.\n* Frozen Food : Sell more on New Year\n* School and Office Supplies: Sale more around AUG","metadata":{}},{"cell_type":"code","source":"del by_fam_dic\ndel fam_list","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:26.600165Z","iopub.execute_input":"2022-03-10T14:15:26.600401Z","iopub.status.idle":"2022-03-10T14:15:26.604813Z","shell.execute_reply.started":"2022-03-10T14:15:26.600372Z","shell.execute_reply":"2022-03-10T14:15:26.603679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What about the stores themselves.\n\nThe competition data description explicitly describe that \"cluster is a grouping of similar store\". This might explan some of the different sales patterns in the subplots above. Let's see what can I get from this.","metadata":{}},{"cell_type":"code","source":"store_data.head(3)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:26.606041Z","iopub.execute_input":"2022-03-10T14:15:26.606419Z","iopub.status.idle":"2022-03-10T14:15:26.625023Z","shell.execute_reply.started":"2022-03-10T14:15:26.606337Z","shell.execute_reply":"2022-03-10T14:15:26.624281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add the cluster to the process_train df\njoin_df = process_train.merge(store_data,on='store_nbr')\n\n#get the missing time after merge back in place\njoin_df.set_index(process_train.index)\njoin_df['date'] = process_train.index\njoin_df = join_df.set_index('date')\n\njoin_df.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:26:33.41424Z","iopub.execute_input":"2022-03-10T14:26:33.414672Z","iopub.status.idle":"2022-03-10T14:26:35.71894Z","shell.execute_reply.started":"2022-03-10T14:26:33.414642Z","shell.execute_reply":"2022-03-10T14:26:35.718052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm curious about the store_data Let's compare the average sales between types and alo between cluster. Hopefully I can see some pattern. ","metadata":{}},{"cell_type":"code","source":" def show_type_df(join_store_type_df):\n    mean_sales_type = join_store_type_df.groupby('type').sales.mean()\n    median_sales_type = join_store_type_df.groupby('type').sales.median()\n    number=join_store_type_df.groupby('type').store_nbr.nunique()\n\n    type_df = pd.DataFrame((mean_sales_type,median_sales_type,number))\n    type_df = type_df.T\n    type_df.columns = ['mean','median','number of store']\n\n    return type_df","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:28.338377Z","iopub.execute_input":"2022-03-10T14:15:28.338879Z","iopub.status.idle":"2022-03-10T14:15:28.346995Z","shell.execute_reply.started":"2022-03-10T14:15:28.338833Z","shell.execute_reply":"2022-03-10T14:15:28.34569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_type_df(join_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:28.348465Z","iopub.execute_input":"2022-03-10T14:15:28.34871Z","iopub.status.idle":"2022-03-10T14:15:29.191687Z","shell.execute_reply.started":"2022-03-10T14:15:28.348682Z","shell.execute_reply":"2022-03-10T14:15:29.191015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although type C and D are most common store type, but the biggest individual performance of the sales came from type A and D. In fact, type C is the worst proforming store in term of sales.","metadata":{}},{"cell_type":"markdown","source":"### What about the store cluster.","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:24:25.526126Z","iopub.execute_input":"2022-02-24T06:24:25.52645Z","iopub.status.idle":"2022-02-24T06:24:25.532965Z","shell.execute_reply.started":"2022-02-24T06:24:25.52642Z","shell.execute_reply":"2022-02-24T06:24:25.53123Z"}}},{"cell_type":"code","source":"def show_cluster_summary(join_store_type_df):\n    mean_sales_cluster = join_store_type_df.groupby('cluster').sales.mean()\n    median_sales_cluster = join_store_type_df.groupby('cluster').sales.median()\n    number=join_store_type_df.groupby('cluster').store_nbr.nunique()\n\n    cluster_df = pd.DataFrame((mean_sales_cluster,median_sales_cluster,number))\n    cluster_df = cluster_df.T\n    cluster_df.columns = ['mean','median','number of store']\n\n    return cluster_df.sort_values('mean', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:29.193008Z","iopub.execute_input":"2022-03-10T14:15:29.193463Z","iopub.status.idle":"2022-03-10T14:15:29.199556Z","shell.execute_reply.started":"2022-03-10T14:15:29.193414Z","shell.execute_reply":"2022-03-10T14:15:29.198629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_cluster_summary(join_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:29.200787Z","iopub.execute_input":"2022-03-10T14:15:29.201028Z","iopub.status.idle":"2022-03-10T14:15:29.503172Z","shell.execute_reply.started":"2022-03-10T14:15:29.200999Z","shell.execute_reply":"2022-03-10T14:15:29.502399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cluster #5 is unique. It has only 1 store but contribute to the biggest individual cluster sales. The other's are as the dataframe states. I'm not sure if cluster will be a reall good featues since the sales are not really distictive except the #5. We reconfirm this thought see in the model later.","metadata":{}},{"cell_type":"markdown","source":"### let's see the City characteristic\n\n","metadata":{}},{"cell_type":"code","source":"def show_city_df(join_store_type_df):    \n    mean_sales_city = join_store_type_df.groupby('city').sales.mean()\n    median_sales_city = join_store_type_df.groupby('city').sales.median()\n    number=join_store_type_df.groupby('city').store_nbr.nunique()\n\n    city_df = pd.DataFrame((mean_sales_city,median_sales_city,number))\n    city_df = city_df.T\n    city_df.columns = ['mean','median','number of store']\n\n    return city_df.sort_values('mean', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:26:16.781568Z","iopub.execute_input":"2022-03-10T14:26:16.781953Z","iopub.status.idle":"2022-03-10T14:26:16.789729Z","shell.execute_reply.started":"2022-03-10T14:26:16.78192Z","shell.execute_reply":"2022-03-10T14:26:16.788829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_city_df(join_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:26:39.958728Z","iopub.execute_input":"2022-03-10T14:26:39.95949Z","iopub.status.idle":"2022-03-10T14:26:40.827572Z","shell.execute_reply.started":"2022-03-10T14:26:39.959449Z","shell.execute_reply":"2022-03-10T14:26:40.826839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del join_df","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:30.430505Z","iopub.execute_input":"2022-03-10T14:15:30.430889Z","iopub.status.idle":"2022-03-10T14:15:30.435661Z","shell.execute_reply.started":"2022-03-10T14:15:30.430852Z","shell.execute_reply":"2022-03-10T14:15:30.434762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This looks really interesting. Quito seem to be the main operating city for our chain store and accounted for the most product sales across the country.","metadata":{}},{"cell_type":"markdown","source":"Last thing I'd like to explore within stores is a heatmap coorelation between the sales pattern of each stores.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\na = process_train[[\"store_nbr\", \"sales\"]] #slice only the two columns\na[\"ind\"] = 1 # create a column of ones\n\n#cumulate the 1 column til the last observation. Ultimately, count the lines for a store.\na[\"ind\"] = a.groupby(\"store_nbr\").ind.cumsum().values \n\n#create a corelation pivot table between sales of each stores\na = pd.pivot(a, index = \"ind\", columns = \"store_nbr\", values = \"sales\").corr() \n\nmask = np.triu(a.corr()) # slice the top triangle of the a.corr() dataFrame\n\nplt.figure(figsize=(20, 20))\nsns.heatmap(a,\n         annot=True,\n         fmt='.1f',\n         square=True,\n         mask=mask,\n         linewidths=1,\n         cbar=False)\nplt.title(\"Correlations among stores\",fontsize = 20)\nplt.show()\n\n#ref: https://www.kaggle.com/ekrembayar/store-sales-ts-forecasting-a-comprehensive-guide/notebook","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:30.437546Z","iopub.execute_input":"2022-03-10T14:15:30.437836Z","iopub.status.idle":"2022-03-10T14:15:38.963951Z","shell.execute_reply.started":"2022-03-10T14:15:30.437803Z","shell.execute_reply":"2022-03-10T14:15:38.963166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del a\ndel mask","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:38.965106Z","iopub.execute_input":"2022-03-10T14:15:38.965912Z","iopub.status.idle":"2022-03-10T14:15:38.969228Z","shell.execute_reply.started":"2022-03-10T14:15:38.965876Z","shell.execute_reply":"2022-03-10T14:15:38.968523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:38.970684Z","iopub.execute_input":"2022-03-10T14:15:38.971169Z","iopub.status.idle":"2022-03-10T14:15:39.637626Z","shell.execute_reply.started":"2022-03-10T14:15:38.971135Z","shell.execute_reply":"2022-03-10T14:15:39.636934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the store sales seems to go together except the store number 52. \n\nBut.... scrolling up to the grid of store sales above (just ctrl+f \"Let's plot each store sale by date\"). \nStore 52 is just a newly opened store. and has almost no sales record compared to the others. \n\nIn fact, all the dark cell contain lower corelation from store 20,21,22,42,52 are stores that are newly operated compared to the others.","metadata":{}},{"cell_type":"markdown","source":"let's check the first entry of store 20 to make sure.","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"store_20 = process_train.groupby(['store_nbr','date']).sales.sum().loc[20]\nstore_20.loc['2013-01-01':'2015-01-01']","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:39.638899Z","iopub.execute_input":"2022-03-10T14:15:39.639969Z","iopub.status.idle":"2022-03-10T14:15:39.826987Z","shell.execute_reply.started":"2022-03-10T14:15:39.639913Z","shell.execute_reply":"2022-03-10T14:15:39.826143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del store_20","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:39.828382Z","iopub.execute_input":"2022-03-10T14:15:39.828623Z","iopub.status.idle":"2022-03-10T14:15:39.832617Z","shell.execute_reply.started":"2022-03-10T14:15:39.828588Z","shell.execute_reply":"2022-03-10T14:15:39.83191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"really no sales at all","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"### Effect of promotion\n\nthe competition overview state that the **onpromotion** column \"gives the total number of items in a product family that were being promoted at a store at a given date.\"\n\nlet's take a look with the corelation between onpromotion and sales.","metadata":{}},{"cell_type":"code","source":"print('Spearman Rank Correlation = {}'.format(\n    process_train.sales.corr(process_train.onpromotion,method='spearman')))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:39.834021Z","iopub.execute_input":"2022-03-10T14:15:39.834323Z","iopub.status.idle":"2022-03-10T14:15:40.757088Z","shell.execute_reply.started":"2022-03-10T14:15:39.834293Z","shell.execute_reply":"2022-03-10T14:15:40.75638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are  coorelation between promotion and sales of certain family of product.\n\nThe reason I choose Spearman rank correlation here is because the default (Pearson's) correlation need the distribution of both variable to be normal distributed. While the promotion itself is define by the marketing plan.\n\n[For more detail](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/correlation-pearson-kendall-spearman/)","metadata":{}},{"cell_type":"markdown","source":"### Let's explopre trend, seasonal, and cycles\n\nJust for simplicity of the first attempt, I'm not gonna use any determininstic process or Fourier series to determine the cycle of the sales. Instead, I will used rolling window average to project a trend and used as a feature for trend.\n\nAs for cycle, I don't think I can overlook this feature since the day of the week (or month of year) are crucial in retail business. I will use one-hot encoding for and feed as a feature.\n\nFirst, let's make sure that the seasonal effect exist.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime as dt\n\ndef show_dow_sales():\n    day_group = process_train.reset_index()[['date','sales']]\n    day_group = day_group.groupby('date')\n    day_group = day_group.sales.mean().to_frame()\n    day_group['dow'] = day_group.index.day_of_week\n    day_group = day_group.groupby('dow').sum()\n    plt.bar(day_group.index,day_group['sales'])\n    plt.title('Average sales on day of week')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:40.75843Z","iopub.execute_input":"2022-03-10T14:15:40.758877Z","iopub.status.idle":"2022-03-10T14:15:40.765798Z","shell.execute_reply.started":"2022-03-10T14:15:40.758827Z","shell.execute_reply":"2022-03-10T14:15:40.764829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_month_group_sale():\n    month_group = process_train['sales'].to_frame()\n    month_group['moy'] = month_group.index.month\n    month_group = month_group.groupby('moy').sales.mean().to_frame()\n    plt.bar(month_group.index,month_group['sales'])\n    plt.title('Average sales on Month of Year')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:40.76722Z","iopub.execute_input":"2022-03-10T14:15:40.768308Z","iopub.status.idle":"2022-03-10T14:15:40.779376Z","shell.execute_reply.started":"2022-03-10T14:15:40.768249Z","shell.execute_reply":"2022-03-10T14:15:40.778562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_dow_sales()\nshow_month_group_sale()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:40.780918Z","iopub.execute_input":"2022-03-10T14:15:40.781217Z","iopub.status.idle":"2022-03-10T14:15:41.531781Z","shell.execute_reply.started":"2022-03-10T14:15:40.781183Z","shell.execute_reply":"2022-03-10T14:15:41.530753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On average, business is good in the weekends and quarter 4.\n\nAfter extracting some basic seasonality, let's explore the effect of holiday.","metadata":{}},{"cell_type":"markdown","source":"### Holiday Effect on sales\n\n* NOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.\n* Additional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday). ","metadata":{"execution":{"iopub.status.busy":"2022-02-25T14:35:50.897582Z","iopub.status.idle":"2022-02-25T14:35:50.897882Z","shell.execute_reply.started":"2022-02-25T14:35:50.897725Z","shell.execute_reply":"2022-02-25T14:35:50.89774Z"}}},{"cell_type":"code","source":"holiday_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\",index_col='date',parse_dates=['date'])\nholiday_data","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:29:47.710476Z","iopub.execute_input":"2022-03-10T14:29:47.710956Z","iopub.status.idle":"2022-03-10T14:29:47.74909Z","shell.execute_reply.started":"2022-03-10T14:29:47.710921Z","shell.execute_reply":"2022-03-10T14:29:47.747832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if Quito (the main operating area) is here ","metadata":{}},{"cell_type":"code","source":"holiday_data.locale_name.value_counts().head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:32:08.035355Z","iopub.execute_input":"2022-03-10T14:32:08.035648Z","iopub.status.idle":"2022-03-10T14:32:08.043904Z","shell.execute_reply.started":"2022-03-10T14:32:08.035613Z","shell.execute_reply":"2022-03-10T14:32:08.043155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quito is here and it has the most regional holidays. ","metadata":{}},{"cell_type":"markdown","source":"Didn't see New Year and Christmas day in the holiday_data. This day is so impactful since the sales across the counrty is dropped to about zero. Let's add it back.","metadata":{}},{"cell_type":"code","source":"ny_dic = {'type': 'Holiday','locale':'National','locale_name':'Ecuador','description': 'New Year Day','transferred':'False'}\nny_date = pd.to_datetime(['2012-01-01','2013-01-01','2014-01-01','2015-01-01','2016-01-01','2017-01-01','2018-01-01'])\n\ncm_dic = {'type': 'Holiday','locale':'National','locale_name':'Ecuador','description': 'Christmas Day','transferred':'False'}\ncm_date = pd.to_datetime(['2012-12-25','2013-12-25','2014-12-25','2015-12-25','2016-12-25','2017-12-25','2018-12-25'])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.564115Z","iopub.execute_input":"2022-03-10T14:15:41.564352Z","iopub.status.idle":"2022-03-10T14:15:41.571731Z","shell.execute_reply.started":"2022-03-10T14:15:41.564325Z","shell.execute_reply":"2022-03-10T14:15:41.570737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for date in ny_date:\n    holiday_data.loc[date] = ['Holiday','National', 'Ecuador', 'New Year day','False']\n    \nfor date in cm_date:\n    holiday_data.loc[date] = ['Holiday','National', 'Ecuador', 'Christmas day','False']","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.573341Z","iopub.execute_input":"2022-03-10T14:15:41.573689Z","iopub.status.idle":"2022-03-10T14:15:41.608751Z","shell.execute_reply.started":"2022-03-10T14:15:41.573658Z","shell.execute_reply":"2022-03-10T14:15:41.607755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"holiday_data = holiday_data.sort_index()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:41.609875Z","iopub.execute_input":"2022-03-10T14:15:41.610113Z","iopub.status.idle":"2022-03-10T14:15:41.615653Z","shell.execute_reply.started":"2022-03-10T14:15:41.610085Z","shell.execute_reply":"2022-03-10T14:15:41.614705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From taking a look at the holiday df above, I think there are someting need formatting.\n* Group Football Events(They might have the different effect from other ceremonial event.\n* Deal with transfered events\n* Emphasize Cyber Monday and Black Friday (Since these are practically shopping day.)\n* Creat a dummy DF that contain the \n    * date \n    * type of event (one-hot)\n        * is_football\n        * is_Earthquake\n        * is_shopping_event\n        * other events\n    * where the event was held (one-hot)\n        * Other dummy columns for the location\n    * work day or not\n","metadata":{}},{"cell_type":"markdown","source":"(My guide for formatting holiday's here :https://www.kaggle.com/andrej0marinchenko/hyperparamaters)","metadata":{}},{"cell_type":"code","source":"calendar = pd.DataFrame(index = pd.date_range('2013-01-01','2017-08-31'))\ncalendar = calendar.join(holiday_data).fillna(0)\ndel holiday_data\ncalendar","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.617421Z","iopub.execute_input":"2022-03-10T14:15:41.617734Z","iopub.status.idle":"2022-03-10T14:15:41.649039Z","shell.execute_reply.started":"2022-03-10T14:15:41.617629Z","shell.execute_reply":"2022-03-10T14:15:41.648151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar['dow'] = calendar.index.dayofweek+1\ncalendar['workday'] = True\ncalendar.loc[calendar['dow']>5 , 'workday'] = False #make work_day false for sat and sun (6/7 in dow)\n\ncalendar.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.650305Z","iopub.execute_input":"2022-03-10T14:15:41.650607Z","iopub.status.idle":"2022-03-10T14:15:41.67212Z","shell.execute_reply.started":"2022-03-10T14:15:41.650564Z","shell.execute_reply":"2022-03-10T14:15:41.671452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, I'm setting workday in each **National** Holiday to ba False no matter dayofweek. Local holiday maybe a normal working day for the nation.","metadata":{}},{"cell_type":"code","source":"calendar.loc[(calendar['type']=='Holiday') & (calendar['locale'].str.contains('National')), 'workday'] = False\ncalendar.loc[(calendar['type']=='Additional') & (calendar['locale'].str.contains('National')), 'workday'] = False\ncalendar.loc[(calendar['type']=='Bridge') & (calendar['locale'].str.contains('National')), 'workday'] = False\ncalendar.loc[(calendar['type']=='Transfer') & (calendar['locale'].str.contains('National')), 'workday'] = False\n\n#some holiday are explicitly said it is workday\ncalendar.loc[calendar['type']=='Work Day' , 'workday'] = True","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.673428Z","iopub.execute_input":"2022-03-10T14:15:41.67368Z","iopub.status.idle":"2022-03-10T14:15:41.710317Z","shell.execute_reply.started":"2022-03-10T14:15:41.673651Z","shell.execute_reply":"2022-03-10T14:15:41.709595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the transfered holiday. They supposed to become working day if they are not Sat and Sun. Let's see that.","metadata":{}},{"cell_type":"code","source":"calendar.where(calendar['transferred'] == True).dropna()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.711781Z","iopub.execute_input":"2022-03-10T14:15:41.712211Z","iopub.status.idle":"2022-03-10T14:15:41.737006Z","shell.execute_reply.started":"2022-03-10T14:15:41.712178Z","shell.execute_reply":"2022-03-10T14:15:41.736315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They are all weekday (since 'dow' are all less than 5). Therefore, format the work_day to True","metadata":{}},{"cell_type":"code","source":"calendar.loc[(calendar['transferred'] == True), 'workday'] = True","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.73811Z","iopub.execute_input":"2022-03-10T14:15:41.738454Z","iopub.status.idle":"2022-03-10T14:15:41.744103Z","shell.execute_reply.started":"2022-03-10T14:15:41.738425Z","shell.execute_reply":"2022-03-10T14:15:41.74311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Format the football to be the one same kind of event. Do the same for the earthquake that last multiple days.","metadata":{}},{"cell_type":"code","source":"calendar.where(calendar['description'].str.contains('futbol')).dropna()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.745558Z","iopub.execute_input":"2022-03-10T14:15:41.746335Z","iopub.status.idle":"2022-03-10T14:15:41.796607Z","shell.execute_reply.started":"2022-03-10T14:15:41.746289Z","shell.execute_reply":"2022-03-10T14:15:41.795676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar['is_football'] = 0\ncalendar['is_eq'] = 0\n\ncalendar.loc[(calendar['is_football'] == 0) & (calendar['description'].str.contains('futbol')), 'is_football'] = 1\ncalendar.loc[(calendar['is_eq'] == 0) & (calendar['description'].str.contains('Terremoto')), 'is_eq'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.797823Z","iopub.execute_input":"2022-03-10T14:15:41.798055Z","iopub.status.idle":"2022-03-10T14:15:41.816813Z","shell.execute_reply.started":"2022-03-10T14:15:41.798027Z","shell.execute_reply":"2022-03-10T14:15:41.81613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar.where(calendar['is_football']==1).dropna().head() # just checking","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.817974Z","iopub.execute_input":"2022-03-10T14:15:41.818571Z","iopub.status.idle":"2022-03-10T14:15:41.854426Z","shell.execute_reply.started":"2022-03-10T14:15:41.818538Z","shell.execute_reply":"2022-03-10T14:15:41.853508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar.where(calendar['is_eq']==1).dropna().head() # just checking","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.855548Z","iopub.execute_input":"2022-03-10T14:15:41.855783Z","iopub.status.idle":"2022-03-10T14:15:41.883553Z","shell.execute_reply.started":"2022-03-10T14:15:41.855754Z","shell.execute_reply":"2022-03-10T14:15:41.8829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's test if the event (such as football / earthquake / Black Friday) affect sales.","metadata":{}},{"cell_type":"code","source":"calendar.loc[calendar['is_football']==1,'description'] = 'football'\ncalendar.loc[calendar['is_eq']==1,'description'] = 'earthquake'\n\n#for simplicity just format the description of the football event and earthquake to be the same","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.884614Z","iopub.execute_input":"2022-03-10T14:15:41.884945Z","iopub.status.idle":"2022-03-10T14:15:41.891354Z","shell.execute_reply.started":"2022-03-10T14:15:41.884917Z","shell.execute_reply":"2022-03-10T14:15:41.890169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = process_train.groupby('date').sales.sum()\nevent = calendar[calendar['type']=='Event']\n\nevent_merge = event.merge(sales,how='left',left_index=True,right_index=True)\nevent_merge\n\ndel sales\ndel event","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.892779Z","iopub.execute_input":"2022-03-10T14:15:41.893029Z","iopub.status.idle":"2022-03-10T14:15:41.960768Z","shell.execute_reply.started":"2022-03-10T14:15:41.892999Z","shell.execute_reply":"2022-03-10T14:15:41.959762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('mean of daily sale across country: {}'.format(process_train.groupby('date').sales.sum().mean()))\nprint('--------------------')\n\nprint(('mean of sale across country in event day: {}'.format(event_merge.groupby('description').sales.mean())))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:41.962477Z","iopub.execute_input":"2022-03-10T14:15:41.963163Z","iopub.status.idle":"2022-03-10T14:15:42.028455Z","shell.execute_reply.started":"2022-03-10T14:15:41.963059Z","shell.execute_reply":"2022-03-10T14:15:42.027335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"comparing those means, black friday and Dia de la Madre seems to not effect total sales much while all the other event affect the sales more.\n\nNow, from the plan\n\n* Group Football Events(They might have the different effect from other ceremonial event.\n* Deal with transfered events\n* Emphasize Cyber Monday and Black Friday (Since these are practically shopping day.)\n* Creat a dummy DF that contain the\n    * date\n    * type of event (one-hot)\n    * is football\n    * is Earthquake\n    * is cyber monday\n    * is black friday\n    * are other events\n    * where the event was held (one-hot)\n    * is workday\n    * day of week\n    \nlet's do the dummy DF beginnig by drop the transfered holidays.\n\n","metadata":{}},{"cell_type":"code","source":"calendar.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.030037Z","iopub.execute_input":"2022-03-10T14:15:42.03031Z","iopub.status.idle":"2022-03-10T14:15:42.045487Z","shell.execute_reply.started":"2022-03-10T14:15:42.030278Z","shell.execute_reply":"2022-03-10T14:15:42.044644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar['workday'] = calendar['workday'].map({False:0,True:1})\ncalendar['transferred'] = calendar['transferred'].map({'False':0,False:0,True:1})","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.046711Z","iopub.execute_input":"2022-03-10T14:15:42.046961Z","iopub.status.idle":"2022-03-10T14:15:42.058528Z","shell.execute_reply.started":"2022-03-10T14:15:42.046922Z","shell.execute_reply":"2022-03-10T14:15:42.057845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar['is_ny'] = 0\ncalendar['is_christmas'] = 0\ncalendar['is_shopping'] = 0\n\ncalendar.loc[calendar['description'] == 'New Year day', 'is_ny'] = 1\ncalendar.loc[calendar['description'] == 'Christmas day', 'is_christmas'] = 1\ncalendar.loc[calendar['description'] == 'Black Friday', 'is_shopping'] = 1\ncalendar.loc[calendar['description'] == 'Cyber Monday' , 'is_shopping'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.059724Z","iopub.execute_input":"2022-03-10T14:15:42.060649Z","iopub.status.idle":"2022-03-10T14:15:42.076293Z","shell.execute_reply.started":"2022-03-10T14:15:42.060609Z","shell.execute_reply":"2022-03-10T14:15:42.07556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar.loc['2014-12-25'].to_frame().T #check the christmas","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.077564Z","iopub.execute_input":"2022-03-10T14:15:42.078139Z","iopub.status.idle":"2022-03-10T14:15:42.09906Z","shell.execute_reply.started":"2022-03-10T14:15:42.078102Z","shell.execute_reply":"2022-03-10T14:15:42.098125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar = calendar.drop(['type','locale'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.100514Z","iopub.execute_input":"2022-03-10T14:15:42.100785Z","iopub.status.idle":"2022-03-10T14:15:42.10721Z","shell.execute_reply.started":"2022-03-10T14:15:42.100747Z","shell.execute_reply":"2022-03-10T14:15:42.106052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.108825Z","iopub.execute_input":"2022-03-10T14:15:42.10917Z","iopub.status.idle":"2022-03-10T14:15:42.126003Z","shell.execute_reply.started":"2022-03-10T14:15:42.109137Z","shell.execute_reply":"2022-03-10T14:15:42.124994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"locale_dummy = pd.get_dummies(calendar['locale_name'],prefix='holiday_')\ncalendar = locale_dummy.join(calendar,how='left')\ncalendar = calendar.drop('locale_name',axis=1)\n\ndel locale_dummy","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.127605Z","iopub.execute_input":"2022-03-10T14:15:42.127947Z","iopub.status.idle":"2022-03-10T14:15:42.144481Z","shell.execute_reply.started":"2022-03-10T14:15:42.127908Z","shell.execute_reply":"2022-03-10T14:15:42.143291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar_checkpoint = calendar","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.146142Z","iopub.execute_input":"2022-03-10T14:15:42.146484Z","iopub.status.idle":"2022-03-10T14:15:42.156664Z","shell.execute_reply.started":"2022-03-10T14:15:42.146449Z","shell.execute_reply":"2022-03-10T14:15:42.15562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar_checkpoint = calendar_checkpoint.drop('description',axis = 1) #don't need description anymore we have dummied them all\ncalendar_checkpoint = calendar_checkpoint[~calendar_checkpoint.index.duplicated(keep='first')] \ncalendar_checkpoint = calendar_checkpoint.iloc[:,1:-1]","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.158225Z","iopub.execute_input":"2022-03-10T14:15:42.158484Z","iopub.status.idle":"2022-03-10T14:15:42.174094Z","shell.execute_reply.started":"2022-03-10T14:15:42.158453Z","shell.execute_reply":"2022-03-10T14:15:42.172974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calendar_checkpoint","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.175755Z","iopub.execute_input":"2022-03-10T14:15:42.176154Z","iopub.status.idle":"2022-03-10T14:15:42.207104Z","shell.execute_reply.started":"2022-03-10T14:15:42.176106Z","shell.execute_reply":"2022-03-10T14:15:42.20609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del calendar\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:42.208935Z","iopub.execute_input":"2022-03-10T14:15:42.209205Z","iopub.status.idle":"2022-03-10T14:15:42.819373Z","shell.execute_reply.started":"2022-03-10T14:15:42.209173Z","shell.execute_reply":"2022-03-10T14:15:42.818384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the dummy table for event are ready to be join to the main sales datafame. Noted that I leave the day of week in ordinal manner since the sales seem to increase toward the weekend. Please let me know if I souldn't do this.","metadata":{"execution":{"iopub.status.busy":"2022-02-28T04:37:22.47094Z","iopub.execute_input":"2022-02-28T04:37:22.471667Z","iopub.status.idle":"2022-02-28T04:37:22.478417Z","shell.execute_reply.started":"2022-02-28T04:37:22.471634Z","shell.execute_reply":"2022-02-28T04:37:22.477567Z"}}},{"cell_type":"markdown","source":"### The effect of oil price on the sales\n\nI put an assumption that the short term effect from change in oil price to the sales is negligible. Let's see if my guessy assumption holds. ","metadata":{}},{"cell_type":"code","source":"oil_data = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.832129Z","iopub.execute_input":"2022-03-10T14:15:42.832406Z","iopub.status.idle":"2022-03-10T14:15:42.840326Z","shell.execute_reply.started":"2022-03-10T14:15:42.832377Z","shell.execute_reply":"2022-03-10T14:15:42.839131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.852298Z","iopub.execute_input":"2022-03-10T14:15:42.852546Z","iopub.status.idle":"2022-03-10T14:15:42.862611Z","shell.execute_reply.started":"2022-03-10T14:15:42.85251Z","shell.execute_reply":"2022-03-10T14:15:42.861749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I need to refill the NaN data and also maybe add a price change / percent chance to be additional features.","metadata":{}},{"cell_type":"markdown","source":"let's check the missing date. (remember that the training data are from 2013-01-01 to 2017-08-15)","metadata":{}},{"cell_type":"code","source":"pd.date_range(start = '2013-01-01', end = '2017-08-15' ).difference(oil_data.index)\n\n# this useful code is from https://stackoverflow.com/questions/52044348/check-for-any-missing-dates-in-the-index","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.864032Z","iopub.execute_input":"2022-03-10T14:15:42.864313Z","iopub.status.idle":"2022-03-10T14:15:42.877303Z","shell.execute_reply.started":"2022-03-10T14:15:42.86428Z","shell.execute_reply":"2022-03-10T14:15:42.876343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So much missing let's interpolate the missing data.","metadata":{}},{"cell_type":"code","source":"oil_data['date'] = pd.to_datetime(oil_data['date'])\noil_data = oil_data.set_index('date')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.8785Z","iopub.execute_input":"2022-03-10T14:15:42.878765Z","iopub.status.idle":"2022-03-10T14:15:42.889883Z","shell.execute_reply.started":"2022-03-10T14:15:42.878735Z","shell.execute_reply":"2022-03-10T14:15:42.88905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data = oil_data.resample('1D').sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.891336Z","iopub.execute_input":"2022-03-10T14:15:42.891613Z","iopub.status.idle":"2022-03-10T14:15:42.900707Z","shell.execute_reply.started":"2022-03-10T14:15:42.891581Z","shell.execute_reply":"2022-03-10T14:15:42.900012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.901922Z","iopub.execute_input":"2022-03-10T14:15:42.902654Z","iopub.status.idle":"2022-03-10T14:15:42.923288Z","shell.execute_reply.started":"2022-03-10T14:15:42.902614Z","shell.execute_reply":"2022-03-10T14:15:42.922464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.date_range(start = '2013-01-01', end = '2017-08-15' ).difference(oil_data.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.92455Z","iopub.execute_input":"2022-03-10T14:15:42.924764Z","iopub.status.idle":"2022-03-10T14:15:42.934377Z","shell.execute_reply.started":"2022-03-10T14:15:42.924737Z","shell.execute_reply":"2022-03-10T14:15:42.933445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing date now. Let's interpolate","metadata":{}},{"cell_type":"code","source":"oil_data['dcoilwtico'] = np.where(oil_data['dcoilwtico']==0, np.nan, oil_data['dcoilwtico'])\noil_data['interpolated_price'] = oil_data.dcoilwtico.interpolate()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.935998Z","iopub.execute_input":"2022-03-10T14:15:42.936641Z","iopub.status.idle":"2022-03-10T14:15:42.947903Z","shell.execute_reply.started":"2022-03-10T14:15:42.936589Z","shell.execute_reply":"2022-03-10T14:15:42.947092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data = oil_data.drop('dcoilwtico',axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.949731Z","iopub.execute_input":"2022-03-10T14:15:42.949987Z","iopub.status.idle":"2022-03-10T14:15:42.958874Z","shell.execute_reply.started":"2022-03-10T14:15:42.949957Z","shell.execute_reply":"2022-03-10T14:15:42.958014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.960369Z","iopub.execute_input":"2022-03-10T14:15:42.960815Z","iopub.status.idle":"2022-03-10T14:15:42.974606Z","shell.execute_reply.started":"2022-03-10T14:15:42.96077Z","shell.execute_reply":"2022-03-10T14:15:42.973926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data['price_chg'] = oil_data.interpolated_price - oil_data.interpolated_price.shift(1)\noil_data['pct_chg'] = oil_data['price_chg']/oil_data.interpolated_price.shift(-1)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.976325Z","iopub.execute_input":"2022-03-10T14:15:42.976786Z","iopub.status.idle":"2022-03-10T14:15:42.984532Z","shell.execute_reply.started":"2022-03-10T14:15:42.976738Z","shell.execute_reply":"2022-03-10T14:15:42.983772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:42.985866Z","iopub.execute_input":"2022-03-10T14:15:42.986459Z","iopub.status.idle":"2022-03-10T14:15:43.001198Z","shell.execute_reply.started":"2022-03-10T14:15:42.986422Z","shell.execute_reply":"2022-03-10T14:15:43.000443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(15,5))\nplt.plot(oil_data['interpolated_price'])\nplt.title('Oil Price over time')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.002503Z","iopub.execute_input":"2022-03-10T14:15:43.003091Z","iopub.status.idle":"2022-03-10T14:15:43.227861Z","shell.execute_reply.started":"2022-03-10T14:15:43.003025Z","shell.execute_reply":"2022-03-10T14:15:43.226922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's calculate the correlation between oil price and total sales.\nLet's begin by a scatterplot","metadata":{}},{"cell_type":"code","source":"daily_total_sales = total_sales.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.229423Z","iopub.execute_input":"2022-03-10T14:15:43.229659Z","iopub.status.idle":"2022-03-10T14:15:43.23455Z","shell.execute_reply.started":"2022-03-10T14:15:43.229631Z","shell.execute_reply":"2022-03-10T14:15:43.233577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_total_sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.236223Z","iopub.execute_input":"2022-03-10T14:15:43.237164Z","iopub.status.idle":"2022-03-10T14:15:43.249456Z","shell.execute_reply.started":"2022-03-10T14:15:43.237114Z","shell.execute_reply":"2022-03-10T14:15:43.248723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_total_sales = daily_total_sales.resample('1D').sum()\ndaily_total_sales","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.250814Z","iopub.execute_input":"2022-03-10T14:15:43.252038Z","iopub.status.idle":"2022-03-10T14:15:43.265662Z","shell.execute_reply.started":"2022-03-10T14:15:43.251997Z","shell.execute_reply":"2022-03-10T14:15:43.265013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_data.interpolated_price.loc['2013-01-01':'2017-08-15']","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.266964Z","iopub.execute_input":"2022-03-10T14:15:43.26768Z","iopub.status.idle":"2022-03-10T14:15:43.278546Z","shell.execute_reply.started":"2022-03-10T14:15:43.267646Z","shell.execute_reply":"2022-03-10T14:15:43.277896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(daily_total_sales,oil_data.interpolated_price.loc['2013-01-01':'2017-08-15'],alpha=0.2)\nplt.ylabel('oil price')\nplt.xlabel('daily total sales')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.280112Z","iopub.execute_input":"2022-03-10T14:15:43.280511Z","iopub.status.idle":"2022-03-10T14:15:43.498199Z","shell.execute_reply.started":"2022-03-10T14:15:43.280465Z","shell.execute_reply":"2022-03-10T14:15:43.497258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cosidering the scatterplot there seems to be no obvious linear correlation between both of them. the most I could say is that higher the oil price, lower the sales. However let's consider another angle. The oil price changes vs total sales changes. We already have oil price change and percentage change. Let's prepare the daily_total_sales.","metadata":{}},{"cell_type":"code","source":"daily_total_sales = pd.DataFrame(daily_total_sales)\ndaily_total_sales['sales_chg'] = daily_total_sales['sales']-daily_total_sales['sales'].shift(1)\ndaily_total_sales['sales_pct_chg'] = daily_total_sales['sales_chg']/daily_total_sales['sales'].shift(-1)\n\ndaily_total_sales.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.499765Z","iopub.execute_input":"2022-03-10T14:15:43.500039Z","iopub.status.idle":"2022-03-10T14:15:43.518977Z","shell.execute_reply.started":"2022-03-10T14:15:43.500008Z","shell.execute_reply":"2022-03-10T14:15:43.518018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's find correlation betwenn the amount change and percentage change of both oil and sales.","metadata":{}},{"cell_type":"code","source":"print('Spearman Rank Correlation = {}'.format(\n    process_train.sales.corr(process_train.onpromotion,method='spearman')))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:43.52071Z","iopub.execute_input":"2022-03-10T14:15:43.521202Z","iopub.status.idle":"2022-03-10T14:15:44.448218Z","shell.execute_reply.started":"2022-03-10T14:15:43.521162Z","shell.execute_reply":"2022-03-10T14:15:44.447173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Pearson Correlation between oil price change and total sales change = {}'.format(\noil_data.price_chg.corr(daily_total_sales.sales_chg,method='pearson')))\nprint('Spearman Rank Correlation between oil price change and total sales change = {}'.format(\noil_data.price_chg.corr(daily_total_sales.sales_chg,method='spearman')))\n\nprint('============================================================================')\n\nprint('Pearson Correlation between % oil price change and % total sales change = {}'.format(\noil_data.pct_chg.corr(daily_total_sales.sales_pct_chg,method='pearson')))\nprint('Spearman Rank Correlation between oil price change and total sales change = {}'.format(\noil_data.pct_chg.corr(daily_total_sales.sales_pct_chg,method='spearman')))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.449722Z","iopub.execute_input":"2022-03-10T14:15:44.449976Z","iopub.status.idle":"2022-03-10T14:15:44.470973Z","shell.execute_reply.started":"2022-03-10T14:15:44.449944Z","shell.execute_reply":"2022-03-10T14:15:44.470127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What about Oil Lag??\nLet's check another angle. The coorelation between the oil price lag and the sales just in case that the effect from change in oil price talk times to be fully accounted into people shopping pattern.","metadata":{}},{"cell_type":"code","source":"import  statsmodels.graphics.tsaplots # for partial autocorrelation\n\nax = statsmodels.graphics.tsaplots.plot_pacf(oil_data.interpolated_price.dropna(), \n                                                 lags=31,\n                                                 title = 'oil lag PACF')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.472697Z","iopub.execute_input":"2022-03-10T14:15:44.472932Z","iopub.status.idle":"2022-03-10T14:15:44.896207Z","shell.execute_reply.started":"2022-03-10T14:15:44.472902Z","shell.execute_reply":"2022-03-10T14:15:44.895234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oil price lag 10,15,26,27 is good to used. Let's see if there are any corelation to the sales.","metadata":{}},{"cell_type":"code","source":"oil_lag = [10,15,26,27]\nfor lag in oil_lag:\n    oil_data['price_lag_{}'.format(lag)] = oil_data.interpolated_price.shift(lag)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.897543Z","iopub.execute_input":"2022-03-10T14:15:44.897871Z","iopub.status.idle":"2022-03-10T14:15:44.908428Z","shell.execute_reply.started":"2022-03-10T14:15:44.897828Z","shell.execute_reply":"2022-03-10T14:15:44.907585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_for_lag_coor = oil_data.dropna()\noil_for_lag_coor.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.909785Z","iopub.execute_input":"2022-03-10T14:15:44.910247Z","iopub.status.idle":"2022-03-10T14:15:44.935587Z","shell.execute_reply.started":"2022-03-10T14:15:44.910211Z","shell.execute_reply":"2022-03-10T14:15:44.934495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_for_lag_coor = oil_for_lag_coor.merge(daily_total_sales,how='inner',left_index=True,right_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.937306Z","iopub.execute_input":"2022-03-10T14:15:44.937653Z","iopub.status.idle":"2022-03-10T14:15:44.94785Z","shell.execute_reply.started":"2022-03-10T14:15:44.937606Z","shell.execute_reply":"2022-03-10T14:15:44.946779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oil_for_lag_coor = oil_for_lag_coor[['price_lag_10','price_lag_15','price_lag_26','price_lag_27','sales']]\noil_for_lag_coor.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.949914Z","iopub.execute_input":"2022-03-10T14:15:44.951214Z","iopub.status.idle":"2022-03-10T14:15:44.976802Z","shell.execute_reply.started":"2022-03-10T14:15:44.951013Z","shell.execute_reply":"2022-03-10T14:15:44.975553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nlag_col =  ['price_lag_10','price_lag_15','price_lag_26','price_lag_27']\n\nfor lag in lag_col:\n    plt.scatter(oil_for_lag_coor['sales'],oil_for_lag_coor[lag],alpha=0.25)\n    plt.title('Oil Lag {} vs Sales'.format(lag))\n    plt.xlabel('oil price lag {}'.format(lag))\n    plt.ylabel('amount sold')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:44.978833Z","iopub.execute_input":"2022-03-10T14:15:44.979195Z","iopub.status.idle":"2022-03-10T14:15:45.913919Z","shell.execute_reply.started":"2022-03-10T14:15:44.979146Z","shell.execute_reply":"2022-03-10T14:15:45.912989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Noting Obvoius again. Higher the lag price, lower the sales. This just mightbe the result from the fact that the oil price had been dropping while the sales is getting better. A correlation, maybe, a causation, I don't know. \n\nPS. If someone could shed some light on this topic, please elaborate on the discussion panel please.\n\nAs of now, I woudn't use the oil price as a feature.","metadata":{}},{"cell_type":"code","source":"del [oil_lag,oil_data,oil_for_lag_coor,lag_col]\n\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:15:45.915169Z","iopub.execute_input":"2022-03-10T14:15:45.915417Z","iopub.status.idle":"2022-03-10T14:15:46.925324Z","shell.execute_reply.started":"2022-03-10T14:15:45.915387Z","shell.execute_reply":"2022-03-10T14:15:46.924341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Transaction record","metadata":{}},{"cell_type":"code","source":"transactions = transaction_data.copy()\ntransactions = transactions.set_index('date')\n\ndel transaction_data","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:46.926561Z","iopub.execute_input":"2022-03-10T14:15:46.926808Z","iopub.status.idle":"2022-03-10T14:15:46.93943Z","shell.execute_reply.started":"2022-03-10T14:15:46.926777Z","shell.execute_reply":"2022-03-10T14:15:46.938626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I really feel like this must be a very powerful feature. The transaction amount must be highly corelated with the sales. Let's process it by\n1. Make a dic cotain store_nbr as a key and transaction as elements\n2. Interpolate the missing date in the dic\n3. scatterplot the transaction with total sales record\n4. decide if assumption is good to be used in the model ","metadata":{}},{"cell_type":"code","source":"transactions.index = pd.to_datetime(transactions.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:46.940635Z","iopub.execute_input":"2022-03-10T14:15:46.940997Z","iopub.status.idle":"2022-03-10T14:15:46.96515Z","shell.execute_reply.started":"2022-03-10T14:15:46.940968Z","shell.execute_reply":"2022-03-10T14:15:46.964202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:46.96644Z","iopub.execute_input":"2022-03-10T14:15:46.966693Z","iopub.status.idle":"2022-03-10T14:15:46.976693Z","shell.execute_reply.started":"2022-03-10T14:15:46.966663Z","shell.execute_reply":"2022-03-10T14:15:46.975932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transaction_sales_dic(transaction_df,sale_dic):\n    transaction_dic = {}\n    sale_dict = sale_dic.copy()\n\n    for i in transaction_df['store_nbr'].unique():\n        store_transacion = transaction_df.loc[transaction_df['store_nbr'] == i]\n        transaction_dic[i] = store_transacion['transactions']\n        \n    for i in sale_dict.keys():\n        sale_dict[i] = sale_dict[i].groupby(['date','store_nbr']).sales.sum()\n        sale_dict[i] = sale_dict[i].reset_index()\n        sale_dict[i] = sale_dict[i].drop('store_nbr', axis=1)\n        sale_dict[i] = sale_dict[i].groupby('date').sales.sum()\n            \n    return transaction_dic, sale_dict\n        \ndef  series_merge_inner_index(dic1, dic2):\n    merged_dic = {}\n    for key in dic1.keys():\n        merged_dic[key] = dic1[key].to_frame().merge(dic2[key].to_frame(), how='inner',\n                                                    left_index=True, right_index=True)\n    return merged_dic","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:46.977847Z","iopub.execute_input":"2022-03-10T14:15:46.97806Z","iopub.status.idle":"2022-03-10T14:15:46.988852Z","shell.execute_reply.started":"2022-03-10T14:15:46.978031Z","shell.execute_reply":"2022-03-10T14:15:46.988089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transaction_dic, sale_dic = transaction_sales_dic(transactions,daily_sale_dict)\nmerged_sales_transaction = series_merge_inner_index(transaction_dic, sale_dic)\nmerged_sales_transaction[1] #just for checking the outcome","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:46.989956Z","iopub.execute_input":"2022-03-10T14:15:46.990669Z","iopub.status.idle":"2022-03-10T14:15:47.557934Z","shell.execute_reply.started":"2022-03-10T14:15:46.990634Z","shell.execute_reply":"2022-03-10T14:15:47.557048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15,7))\nfor key in merged_sales_transaction.keys():\n    plt.scatter(merged_sales_transaction[key].transactions,\n                merged_sales_transaction[key].sales)\n\nplt.title('Transaction vs Amount Sold')\nplt.xlabel('transactions number')\nplt.ylabel('amount sold')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:47.559219Z","iopub.execute_input":"2022-03-10T14:15:47.559433Z","iopub.status.idle":"2022-03-10T14:15:48.555548Z","shell.execute_reply.started":"2022-03-10T14:15:47.559407Z","shell.execute_reply":"2022-03-10T14:15:48.554689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is very obvious that transaction number and amount sold are highly corelated. This feature will be used in the model. Considering the nature of test dataset that didn't provide the amount of transaction on those days (otherwise there will be a data leakage), Transaction must be used as a lag feature. In fact, it need to be a 16 or more lag since the test horizon is 15 days from the last day of the training dataset.\n\nLet's see transaction lag. Beginning with the transaction lag partial correlation for picking the good lags to use.","metadata":{}},{"cell_type":"code","source":"import  statsmodels.graphics.tsaplots # for partial autocorrelation\n\nfor store_nbr in merged_sales_transaction.keys():\n    ax = statsmodels.graphics.tsaplots.plot_pacf(merged_sales_transaction[store_nbr].transactions, \n                                                 lags=range(16,31),\n                                                 title = 'Store {} transactions lag'.format(store_nbr))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:15:48.557129Z","iopub.execute_input":"2022-03-10T14:15:48.557386Z","iopub.status.idle":"2022-03-10T14:16:00.756458Z","shell.execute_reply.started":"2022-03-10T14:15:48.557353Z","shell.execute_reply":"2022-03-10T14:16:00.755354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, Transaction lag of 21,22,28 seem to be good used for almost all stores. will used them as feature.","metadata":{}},{"cell_type":"markdown","source":"### From what I plan to explore:\n\n1. Total sales\n2. Daily sales by each stores\n3. Sales by product family, by time\n4. Store Location/Cluster Effect on sales\n5. Onpromotion effect on sales\n6. Cycle and Seasonal Effect on total sales\n7. Map the holiday to the sales record\n8. Calculate the correlation in change in oil price to change in total sales\n9. Explore the transaction data\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### This is what I have\n1. The overall trend is up for total sales \n2. the top 5\n   * GROCERY I - 31.99%\n   * BEVERAGES - 20.21%\n   * PRODUCE - 11.43%\n   * CLEANING - 9.08%\n   * DAIRY - 6% \n   \n   accounted for more than 75% of total sales\n   * Frozen Food / School and Office Supply, though not a major share, has a highly cyclic selling pattern\n  \n3. Some store have a lot of missing dates. I guess it's not operate or not yet open on those days\n   * Closing days besides, all the other store have high corelation in daily sales change.\n4. There are some interesting paterns within the sales need further explorations such as:\n    * The earthquale effect\n    * The New Year\n5. Store characteristic are categorized by cluster, format. Let's see their significance later when we build and train the model.\n    * Type and Cluster seem to have some effect on sales.\n    * The sores located most in Pichincha and Guayas\n    \n6. There are strong corelation between onpromotion and sales of the product on promotion\n7. On average, sales are higher around the end of the week  on weekly basis an higher around Q4 on yearly basis\n8. The dummy DF for holiday is ready to be merge to the main sales DF\n9. Oil price seem to have no corralation to the sales amount.\n10. Transaction number is highly corelated to total sales.\n    * Will be used as lag feature (lag 21,22,28)","metadata":{}},{"cell_type":"markdown","source":"# 4. Model research: Which model should I use here.\n\nSince this is the first attempt and I already have a benchmark score from the kaggele time series course of **0.51090**, I think I will begin with simple linear regression. Later, I plan to refine or replace the model or methodology used for building the model for better outcome.","metadata":{}},{"cell_type":"markdown","source":"# 5. Data processing and Feature engineering\n\nWith the exploration sofar. I plan to train the model with these features\n* Daily Sales lag (Will check again for different lag correlations)\n* Total daily sales lag Across the country\n* The amount of the promotion for that product family\n* Store Area (to be crossed with the locale name of holiday)\n* Features from Holiday\n    * type\n    * locale name\n    * day of week\n    * is_worlday\n    * Earthquake\n    * Football\n    * New Year\n    * shopping events (such as Black Friday)\n* Daily Transaction Lag for each shop\n\nWe are already have the holiday_df ready to be merge. Let's begin making inputs dataframe. \n\n\n### Lag Feature\nFirst thing first, let's explore the sales lag correlation to the sales itself.\n\nAlso the test dataset come in different product family and different store, I don't want to construct the lag for each product family and each store and the majority of the product sold are consist of only 5 family. Therefore,I'm gonna use only the lag from store total sales.\n","metadata":{}},{"cell_type":"code","source":"import  statsmodels.graphics.tsaplots # for partial autocorrelation\n\n\ndaily_store_sale_dict = {}\nfor i in daily_sale_dict.keys():\n    daily_store_sale_dict[i] = daily_sale_dict[i].groupby(['date','store_nbr']).sales.sum().to_frame()    ","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:00.758019Z","iopub.execute_input":"2022-03-10T14:16:00.758289Z","iopub.status.idle":"2022-03-10T14:16:01.034146Z","shell.execute_reply.started":"2022-03-10T14:16:00.758258Z","shell.execute_reply":"2022-03-10T14:16:01.033032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del daily_sale_dict","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:16:01.035652Z","iopub.execute_input":"2022-03-10T14:16:01.035907Z","iopub.status.idle":"2022-03-10T14:16:01.046672Z","shell.execute_reply.started":"2022-03-10T14:16:01.035876Z","shell.execute_reply":"2022-03-10T14:16:01.045694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"daily_store_sale_dict[1].head() #just checking for store 1 dic","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:01.048372Z","iopub.execute_input":"2022-03-10T14:16:01.048619Z","iopub.status.idle":"2022-03-10T14:16:01.066005Z","shell.execute_reply.started":"2022-03-10T14:16:01.048588Z","shell.execute_reply":"2022-03-10T14:16:01.065164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in daily_store_sale_dict.keys():\n    daily_store_sale_dict[i] = daily_store_sale_dict[i].droplevel(1) \n    #remove store_nbt from index leaving only date","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:01.067356Z","iopub.execute_input":"2022-03-10T14:16:01.068125Z","iopub.status.idle":"2022-03-10T14:16:01.089503Z","shell.execute_reply.started":"2022-03-10T14:16:01.068081Z","shell.execute_reply":"2022-03-10T14:16:01.088826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in daily_store_sale_dict.keys():\n\n    ax = statsmodels.graphics.tsaplots.plot_pacf(daily_store_sale_dict[i],lags=14, \n                                                 title = 'store {} PA'.format(i))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:01.090744Z","iopub.execute_input":"2022-03-10T14:16:01.090999Z","iopub.status.idle":"2022-03-10T14:16:12.15372Z","shell.execute_reply.started":"2022-03-10T14:16:01.090959Z","shell.execute_reply":"2022-03-10T14:16:12.152818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I thin I will keep the 1 2 3 4 5 6 7 8 13 14 Lags.","metadata":{}},{"cell_type":"markdown","source":"The lag is prepared. Let' start constructing the input df along the way.","metadata":{}},{"cell_type":"markdown","source":"Let's combine the process_train and test for the ease of formatting","metadata":{}},{"cell_type":"code","source":"process_train","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:12.154957Z","iopub.execute_input":"2022-03-10T14:16:12.155593Z","iopub.status.idle":"2022-03-10T14:16:12.172904Z","shell.execute_reply.started":"2022-03-10T14:16:12.155553Z","shell.execute_reply":"2022-03-10T14:16:12.171787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:12.174536Z","iopub.execute_input":"2022-03-10T14:16:12.174845Z","iopub.status.idle":"2022-03-10T14:16:12.192908Z","shell.execute_reply.started":"2022-03-10T14:16:12.174798Z","shell.execute_reply":"2022-03-10T14:16:12.192015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['sales'] = 0\ntest_data = test_data[['id','date','store_nbr','family','sales','onpromotion']]\n\ntest_data","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:12.19428Z","iopub.execute_input":"2022-03-10T14:16:12.194822Z","iopub.status.idle":"2022-03-10T14:16:12.217717Z","shell.execute_reply.started":"2022-03-10T14:16:12.19465Z","shell.execute_reply":"2022-03-10T14:16:12.216812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_train['id'] = process_train.reset_index().index","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:12.21898Z","iopub.execute_input":"2022-03-10T14:16:12.219246Z","iopub.status.idle":"2022-03-10T14:16:12.261771Z","shell.execute_reply.started":"2022-03-10T14:16:12.219215Z","shell.execute_reply":"2022-03-10T14:16:12.260867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_train = process_train.reset_index()\nprocess_train = process_train[['id','date','store_nbr','family','sales','onpromotion']]\nprocess_train","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:12.263314Z","iopub.execute_input":"2022-03-10T14:16:12.264249Z","iopub.status.idle":"2022-03-10T14:16:12.405944Z","shell.execute_reply.started":"2022-03-10T14:16:12.264208Z","shell.execute_reply":"2022-03-10T14:16:12.405056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train = pd.concat([process_train,test_data])\nmerged_train = merged_train.set_index(['date','store_nbr','family'])\nmerged_train","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:12.407279Z","iopub.execute_input":"2022-03-10T14:16:12.407502Z","iopub.status.idle":"2022-03-10T14:16:32.355523Z","shell.execute_reply.started":"2022-03-10T14:16:12.407475Z","shell.execute_reply":"2022-03-10T14:16:32.3545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_train","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:32.356844Z","iopub.execute_input":"2022-03-10T14:16:32.357166Z","iopub.status.idle":"2022-03-10T14:16:32.377802Z","shell.execute_reply.started":"2022-03-10T14:16:32.357125Z","shell.execute_reply":"2022-03-10T14:16:32.376945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_data\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T14:16:32.379304Z","iopub.execute_input":"2022-03-10T14:16:32.379547Z","iopub.status.idle":"2022-03-10T14:16:32.51157Z","shell.execute_reply.started":"2022-03-10T14:16:32.379514Z","shell.execute_reply":"2022-03-10T14:16:32.510504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's add the store location as another index.","metadata":{}},{"cell_type":"code","source":"store_location = store_data.drop(['state','type','cluster'],axis=1) # the city column is the same scale as holiday location.\nstore_location = store_location.set_index('store_nbr')\nstore_location = pd.get_dummies(store_location,prefix='store_loc_')\nstore_location","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:32.513779Z","iopub.execute_input":"2022-03-10T14:16:32.51442Z","iopub.status.idle":"2022-03-10T14:16:32.561222Z","shell.execute_reply.started":"2022-03-10T14:16:32.514368Z","shell.execute_reply":"2022-03-10T14:16:32.560601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"merge the store location into inputs DF ","metadata":{}},{"cell_type":"code","source":"inputs = merged_train.reset_index().merge(store_location,how='outer',left_on='store_nbr',right_on=store_location.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:32.562487Z","iopub.execute_input":"2022-03-10T14:16:32.562715Z","iopub.status.idle":"2022-03-10T14:16:33.542101Z","shell.execute_reply.started":"2022-03-10T14:16:32.562685Z","shell.execute_reply":"2022-03-10T14:16:33.541235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:33.543274Z","iopub.execute_input":"2022-03-10T14:16:33.543506Z","iopub.status.idle":"2022-03-10T14:16:34.009093Z","shell.execute_reply.started":"2022-03-10T14:16:33.543478Z","shell.execute_reply":"2022-03-10T14:16:34.00819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del store_location\ndel merged_train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.010622Z","iopub.execute_input":"2022-03-10T14:16:34.010879Z","iopub.status.idle":"2022-03-10T14:16:34.146702Z","shell.execute_reply.started":"2022-03-10T14:16:34.010847Z","shell.execute_reply":"2022-03-10T14:16:34.145989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, lag features: sales lag, total daily sales lag, transaction lag. I have prepared the store sales lag and tranasation lag, let's do the total country sales lag.","metadata":{}},{"cell_type":"code","source":"total_sales","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.147733Z","iopub.execute_input":"2022-03-10T14:16:34.147963Z","iopub.status.idle":"2022-03-10T14:16:34.162591Z","shell.execute_reply.started":"2022-03-10T14:16:34.147935Z","shell.execute_reply":"2022-03-10T14:16:34.161504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Need to project the index further to 2017-08-31.","metadata":{}},{"cell_type":"code","source":"total_sales_to_scale = pd.DataFrame(index=pd.date_range(start='2013-01-01',end='2017-08-31'))\ntotal_sales_to_scale = total_sales_to_scale.merge(total_sales,how='left',left_index=True,right_index=True)\ntotal_sales_to_scale = total_sales_to_scale.rename(columns={'sales':'national_sales'})","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.164586Z","iopub.execute_input":"2022-03-10T14:16:34.164907Z","iopub.status.idle":"2022-03-10T14:16:34.175735Z","shell.execute_reply.started":"2022-03-10T14:16:34.164863Z","shell.execute_reply":"2022-03-10T14:16:34.174813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_to_scale","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.178247Z","iopub.execute_input":"2022-03-10T14:16:34.178566Z","iopub.status.idle":"2022-03-10T14:16:34.197563Z","shell.execute_reply.started":"2022-03-10T14:16:34.178524Z","shell.execute_reply":"2022-03-10T14:16:34.196629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmmScale = MinMaxScaler()\nmmScale.fit(total_sales_to_scale['national_sales'].to_numpy().reshape(-1,1))\n\ntotal_sales_to_scale['scaled_nat_sales'] = mmScale.transform(total_sales_to_scale['national_sales'].to_numpy().reshape(-1,1))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.19964Z","iopub.execute_input":"2022-03-10T14:16:34.200525Z","iopub.status.idle":"2022-03-10T14:16:34.363874Z","shell.execute_reply.started":"2022-03-10T14:16:34.200473Z","shell.execute_reply":"2022-03-10T14:16:34.362951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_to_scale","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.364962Z","iopub.execute_input":"2022-03-10T14:16:34.365191Z","iopub.status.idle":"2022-03-10T14:16:34.380384Z","shell.execute_reply.started":"2022-03-10T14:16:34.365162Z","shell.execute_reply":"2022-03-10T14:16:34.379281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we need to make some lags to the scaled_nat_sales. The lag will also be 16-30 for the same reason with other lag.","metadata":{"execution":{"iopub.status.busy":"2022-03-07T11:32:53.501688Z","iopub.execute_input":"2022-03-07T11:32:53.502045Z","iopub.status.idle":"2022-03-07T11:32:53.507929Z","shell.execute_reply.started":"2022-03-07T11:32:53.502008Z","shell.execute_reply":"2022-03-07T11:32:53.507011Z"}}},{"cell_type":"code","source":"import  statsmodels.graphics.tsaplots # for partial autocorrelation\n\nax = statsmodels.graphics.tsaplots.plot_pacf(total_sales_to_scale['scaled_nat_sales'].dropna(), \n                                                 lags=range(16,31),\n                                                 title = 'PACF of national sales')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.381562Z","iopub.execute_input":"2022-03-10T14:16:34.382335Z","iopub.status.idle":"2022-03-10T14:16:34.645295Z","shell.execute_reply.started":"2022-03-10T14:16:34.382299Z","shell.execute_reply":"2022-03-10T14:16:34.644611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lag 16 - 24 and 27,28 seems good to be used here.","metadata":{}},{"cell_type":"code","source":"lags= [16,17,18,19,20,21,22,23,24,27,28]\nfor lag in lags:\n    total_sales_to_scale['nat_scaled_sales_lag{}'.format(lag)] = total_sales_to_scale['scaled_nat_sales'].shift(lag)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.646494Z","iopub.execute_input":"2022-03-10T14:16:34.646859Z","iopub.status.idle":"2022-03-10T14:16:34.660894Z","shell.execute_reply.started":"2022-03-10T14:16:34.646828Z","shell.execute_reply":"2022-03-10T14:16:34.660135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_to_scale = total_sales_to_scale.drop(['national_sales','scaled_nat_sales'],axis=1) \n#don't think the current sales will be practical in real situation","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.66203Z","iopub.execute_input":"2022-03-10T14:16:34.662377Z","iopub.status.idle":"2022-03-10T14:16:34.676546Z","shell.execute_reply.started":"2022-03-10T14:16:34.662347Z","shell.execute_reply":"2022-03-10T14:16:34.675648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_sales_to_scale.reset_index().tail() #reset index for ease of merge","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.677934Z","iopub.execute_input":"2022-03-10T14:16:34.678346Z","iopub.status.idle":"2022-03-10T14:16:34.706477Z","shell.execute_reply.started":"2022-03-10T14:16:34.67831Z","shell.execute_reply":"2022-03-10T14:16:34.705756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = inputs.merge(total_sales_to_scale.reset_index(),how='left',left_on='date',right_on='index')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:34.707586Z","iopub.execute_input":"2022-03-10T14:16:34.708013Z","iopub.status.idle":"2022-03-10T14:16:35.723365Z","shell.execute_reply.started":"2022-03-10T14:16:34.707978Z","shell.execute_reply":"2022-03-10T14:16:35.722506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del total_sales_to_scale","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:35.724686Z","iopub.execute_input":"2022-03-10T14:16:35.724912Z","iopub.status.idle":"2022-03-10T14:16:35.728711Z","shell.execute_reply.started":"2022-03-10T14:16:35.724885Z","shell.execute_reply":"2022-03-10T14:16:35.727853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.columns #just checking what are in the df","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:35.729889Z","iopub.execute_input":"2022-03-10T14:16:35.73014Z","iopub.status.idle":"2022-03-10T14:16:35.747165Z","shell.execute_reply.started":"2022-03-10T14:16:35.730105Z","shell.execute_reply":"2022-03-10T14:16:35.746297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.drop(['index'],axis=1,inplace=True) #column named 'index' in dt format don't need it anymore","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:35.749244Z","iopub.execute_input":"2022-03-10T14:16:35.749509Z","iopub.status.idle":"2022-03-10T14:16:36.402654Z","shell.execute_reply.started":"2022-03-10T14:16:35.749478Z","shell.execute_reply":"2022-03-10T14:16:36.402046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's continue on individual store sales lag. From the lag ananyses above, I have conclude that the lags which will be used here are lag = [1 2 3 4 5 6 7 8 13 14]. Let's put it to good use.","metadata":{}},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:36.404136Z","iopub.execute_input":"2022-03-10T14:16:36.404401Z","iopub.status.idle":"2022-03-10T14:16:37.072885Z","shell.execute_reply.started":"2022-03-10T14:16:36.40437Z","shell.execute_reply":"2022-03-10T14:16:37.071981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lags = [1, 2, 3, 4, 5, 6, 7, 8, 13, 14]\nfor lag in lags:\n    inputs['store_fam_sales_lag_{}'.format(lag)] = inputs['sales'].shift(lag)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.074038Z","iopub.execute_input":"2022-03-10T14:16:37.07437Z","iopub.status.idle":"2022-03-10T14:16:37.210577Z","shell.execute_reply.started":"2022-03-10T14:16:37.074335Z","shell.execute_reply":"2022-03-10T14:16:37.209638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.211792Z","iopub.execute_input":"2022-03-10T14:16:37.212062Z","iopub.status.idle":"2022-03-10T14:16:37.219959Z","shell.execute_reply.started":"2022-03-10T14:16:37.212029Z","shell.execute_reply":"2022-03-10T14:16:37.219056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now add the transaction lag to the inputs.","metadata":{"execution":{"iopub.status.busy":"2022-03-07T11:56:09.751193Z","iopub.execute_input":"2022-03-07T11:56:09.751711Z","iopub.status.idle":"2022-03-07T11:56:09.757383Z","shell.execute_reply.started":"2022-03-07T11:56:09.751636Z","shell.execute_reply":"2022-03-07T11:56:09.756138Z"}}},{"cell_type":"code","source":"transactions","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.22129Z","iopub.execute_input":"2022-03-10T14:16:37.221521Z","iopub.status.idle":"2022-03-10T14:16:37.242726Z","shell.execute_reply.started":"2022-03-10T14:16:37.221493Z","shell.execute_reply":"2022-03-10T14:16:37.241771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"still left the row from 2017-08-16 to 2017-08-31 and notice that someday, some store won't open (such as new year that we only have store 25. How do I map this to the inputs dataframe?? ","metadata":{}},{"cell_type":"code","source":"store_nbr = range(1,55)\ndates = pd.date_range('2013-01-01','2017-08-31')\nmul_index = pd.MultiIndex.from_product([dates,store_nbr],names=['date','store_nbr'])\ndf = pd.DataFrame(index=mul_index)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.244563Z","iopub.execute_input":"2022-03-10T14:16:37.244817Z","iopub.status.idle":"2022-03-10T14:16:37.25846Z","shell.execute_reply.started":"2022-03-10T14:16:37.244789Z","shell.execute_reply":"2022-03-10T14:16:37.257463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.260312Z","iopub.execute_input":"2022-03-10T14:16:37.261395Z","iopub.status.idle":"2022-03-10T14:16:37.282914Z","shell.execute_reply.started":"2022-03-10T14:16:37.261341Z","shell.execute_reply":"2022-03-10T14:16:37.282005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transactions.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.284188Z","iopub.execute_input":"2022-03-10T14:16:37.284433Z","iopub.status.idle":"2022-03-10T14:16:37.299629Z","shell.execute_reply.started":"2022-03-10T14:16:37.284405Z","shell.execute_reply":"2022-03-10T14:16:37.298829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction = df.reset_index().merge(transactions.reset_index(),\n                                        how='left',\n                                        left_on=['date','store_nbr'],\n                                        right_on=['date','store_nbr']\n                                       )","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.30076Z","iopub.execute_input":"2022-03-10T14:16:37.301264Z","iopub.status.idle":"2022-03-10T14:16:37.349254Z","shell.execute_reply.started":"2022-03-10T14:16:37.301218Z","shell.execute_reply":"2022-03-10T14:16:37.348307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction.fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:37.350376Z","iopub.execute_input":"2022-03-10T14:16:37.350591Z","iopub.status.idle":"2022-03-10T14:16:38.098042Z","shell.execute_reply.started":"2022-03-10T14:16:37.350565Z","shell.execute_reply":"2022-03-10T14:16:38.097363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction.loc[30020:30026] #just checking","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.099159Z","iopub.execute_input":"2022-03-10T14:16:38.099498Z","iopub.status.idle":"2022-03-10T14:16:38.113037Z","shell.execute_reply.started":"2022-03-10T14:16:38.099469Z","shell.execute_reply":"2022-03-10T14:16:38.112265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_transaction.rename(columns={'level_0':'date','level_1':'store_nbr'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.114274Z","iopub.execute_input":"2022-03-10T14:16:38.114632Z","iopub.status.idle":"2022-03-10T14:16:38.118479Z","shell.execute_reply.started":"2022-03-10T14:16:38.1146Z","shell.execute_reply":"2022-03-10T14:16:38.117623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.120329Z","iopub.execute_input":"2022-03-10T14:16:38.121Z","iopub.status.idle":"2022-03-10T14:16:38.148477Z","shell.execute_reply.started":"2022-03-10T14:16:38.12095Z","shell.execute_reply":"2022-03-10T14:16:38.147567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now adding the lag. Remember the meaningful lag for transaction are lag = [21,22,28]","metadata":{}},{"cell_type":"code","source":"lags = [21,22,28]\nfor lag in lags:\n    df_transaction['trans_lag_{}'.format(lag)] = df_transaction['transactions'].shift(lag)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.149878Z","iopub.execute_input":"2022-03-10T14:16:38.150714Z","iopub.status.idle":"2022-03-10T14:16:38.160933Z","shell.execute_reply.started":"2022-03-10T14:16:38.150672Z","shell.execute_reply":"2022-03-10T14:16:38.159768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction = df_transaction.drop('transactions',axis=1) #in real life daily transaction came after day's end","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.163182Z","iopub.execute_input":"2022-03-10T14:16:38.164029Z","iopub.status.idle":"2022-03-10T14:16:38.177045Z","shell.execute_reply.started":"2022-03-10T14:16:38.163975Z","shell.execute_reply":"2022-03-10T14:16:38.175842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction = df_transaction.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.178739Z","iopub.execute_input":"2022-03-10T14:16:38.1793Z","iopub.status.idle":"2022-03-10T14:16:38.933259Z","shell.execute_reply.started":"2022-03-10T14:16:38.179255Z","shell.execute_reply":"2022-03-10T14:16:38.932177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_transaction.loc[30030:30040] #ust checking","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.934917Z","iopub.execute_input":"2022-03-10T14:16:38.935158Z","iopub.status.idle":"2022-03-10T14:16:38.951331Z","shell.execute_reply.started":"2022-03-10T14:16:38.935131Z","shell.execute_reply":"2022-03-10T14:16:38.950401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, merge the df_transaction to the inputs df.","metadata":{}},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:38.952891Z","iopub.execute_input":"2022-03-10T14:16:38.953176Z","iopub.status.idle":"2022-03-10T14:16:39.796799Z","shell.execute_reply.started":"2022-03-10T14:16:38.953143Z","shell.execute_reply":"2022-03-10T14:16:39.795787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = inputs.merge(df_transaction, how='left', left_on = ['date','store_nbr'],right_on = ['date','store_nbr'])","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:39.798567Z","iopub.execute_input":"2022-03-10T14:16:39.798893Z","iopub.status.idle":"2022-03-10T14:16:41.379293Z","shell.execute_reply.started":"2022-03-10T14:16:39.798849Z","shell.execute_reply":"2022-03-10T14:16:41.37849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:41.380397Z","iopub.execute_input":"2022-03-10T14:16:41.38061Z","iopub.status.idle":"2022-03-10T14:16:42.266539Z","shell.execute_reply.started":"2022-03-10T14:16:41.380582Z","shell.execute_reply":"2022-03-10T14:16:42.264447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, add the holiday features into the inputs DF","metadata":{}},{"cell_type":"code","source":"calendar_checkpoint.reset_index().tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:42.268159Z","iopub.execute_input":"2022-03-10T14:16:42.268497Z","iopub.status.idle":"2022-03-10T14:16:42.293583Z","shell.execute_reply.started":"2022-03-10T14:16:42.268454Z","shell.execute_reply":"2022-03-10T14:16:42.292614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = inputs.merge(calendar_checkpoint,how='left',left_on=['date'],right_on=calendar_checkpoint.index)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:42.295133Z","iopub.execute_input":"2022-03-10T14:16:42.2954Z","iopub.status.idle":"2022-03-10T14:16:44.168478Z","shell.execute_reply.started":"2022-03-10T14:16:42.295368Z","shell.execute_reply":"2022-03-10T14:16:44.167468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:44.169775Z","iopub.execute_input":"2022-03-10T14:16:44.170038Z","iopub.status.idle":"2022-03-10T14:16:44.177354Z","shell.execute_reply.started":"2022-03-10T14:16:44.170007Z","shell.execute_reply":"2022-03-10T14:16:44.176401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's check for missing data.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows',None)\ninputs.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:44.178871Z","iopub.execute_input":"2022-03-10T14:16:44.179571Z","iopub.status.idle":"2022-03-10T14:16:44.916942Z","shell.execute_reply.started":"2022-03-10T14:16:44.179532Z","shell.execute_reply":"2022-03-10T14:16:44.916112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.reset_option('display.max_rows','display.max_columns')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-10T14:16:44.918413Z","iopub.execute_input":"2022-03-10T14:16:44.918672Z","iopub.status.idle":"2022-03-10T14:16:44.923021Z","shell.execute_reply.started":"2022-03-10T14:16:44.91864Z","shell.execute_reply":"2022-03-10T14:16:44.922029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop all the NaN (which are all lagging features)","metadata":{}},{"cell_type":"code","source":"inputs.dropna(inplace = True)\ninputs.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:44.924448Z","iopub.execute_input":"2022-03-10T14:16:44.9247Z","iopub.status.idle":"2022-03-10T14:16:48.761297Z","shell.execute_reply.started":"2022-03-10T14:16:44.924669Z","shell.execute_reply":"2022-03-10T14:16:48.760417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = inputs.set_index('date')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:48.76347Z","iopub.execute_input":"2022-03-10T14:16:48.764408Z","iopub.status.idle":"2022-03-10T14:16:49.161915Z","shell.execute_reply.started":"2022-03-10T14:16:48.764356Z","shell.execute_reply":"2022-03-10T14:16:49.161159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:49.163513Z","iopub.execute_input":"2022-03-10T14:16:49.163778Z","iopub.status.idle":"2022-03-10T14:16:49.187737Z","shell.execute_reply.started":"2022-03-10T14:16:49.163747Z","shell.execute_reply":"2022-03-10T14:16:49.186927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From all the features list I planned to use.\n* Sales lag ---- DONE\n* Total daily sales lag Across the country ---- DONE\n* The amount of the promotion for that product family ---- DONE\n* Store Area (to be crossed with the locale name of holiday) ---- DONE\n* Features from Holiday ---- DONE\n    * type ---- DONE\n    * locale name ---- DONE\n    * day of week ---- DONE\n    * is_worlday ---- DONE\n    * Earthquake ---- DONE\n    * Football ---- DONE\n    * New Year ---- DONE\n    * shopping events (such as Black Friday) ---- DONE\n* Daily Transaction Lag for each shop ---- DONE","metadata":{}},{"cell_type":"markdown","source":"Let's make the training data and testing data.","metadata":{}},{"cell_type":"code","source":"y_train = inputs.loc['2013-01-01':'2017-08-15', 'sales']\ny_train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:49.189047Z","iopub.execute_input":"2022-03-10T14:16:49.189383Z","iopub.status.idle":"2022-03-10T14:16:49.295442Z","shell.execute_reply.started":"2022-03-10T14:16:49.189347Z","shell.execute_reply":"2022-03-10T14:16:49.294707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = inputs.loc['2013-01-01':'2017-08-15'].drop(['sales','id'],axis=1)\nx_train = x_train.reset_index()\nx_train = x_train.set_index(['date','store_nbr','family'])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:49.297048Z","iopub.execute_input":"2022-03-10T14:16:49.297748Z","iopub.status.idle":"2022-03-10T14:16:53.167989Z","shell.execute_reply.started":"2022-03-10T14:16:49.297712Z","shell.execute_reply":"2022-03-10T14:16:53.167255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:53.170116Z","iopub.execute_input":"2022-03-10T14:16:53.171045Z","iopub.status.idle":"2022-03-10T14:16:53.20824Z","shell.execute_reply.started":"2022-03-10T14:16:53.170995Z","shell.execute_reply":"2022-03-10T14:16:53.207375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T15:05:10.922021Z","iopub.execute_input":"2022-03-10T15:05:10.922373Z","iopub.status.idle":"2022-03-10T15:05:16.044041Z","shell.execute_reply.started":"2022-03-10T15:05:10.922339Z","shell.execute_reply":"2022-03-10T15:05:16.04324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = inputs.loc['2017-08-16': ]\ntest_id = x_test['id'] #Keep for later\n\nx_test.drop(['sales','id'],axis = 1,inplace = True)\n\nx_test = x_test.reset_index()\nx_test = x_test.set_index(['date','store_nbr','family'])\nx_test","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:53.210349Z","iopub.execute_input":"2022-03-10T14:16:53.211208Z","iopub.status.idle":"2022-03-10T14:16:53.329565Z","shell.execute_reply.started":"2022-03-10T14:16:53.211161Z","shell.execute_reply":"2022-03-10T14:16:53.328481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Built model(s) and Validate model(s)\n\nFinally it's time for the model training. As I mentioned above, this very first attempt will be for the simple linear regression model and the metrics will be the \"Root Mean Squared Logarithmic Error\" (RMSLE), in other words, the submission score itself. I am aimimg to surpass the benchmark score of **0.5109** from the Kaggle time series course. I didn't expect anythin more than \"abysmal\" for the result in this first attempt with simple model and inexperience feature eugineering. However, I hope that the next iterations to come will yield better results. \n\n### Let's do it.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nln = LinearRegression()\nln.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:16:53.330861Z","iopub.execute_input":"2022-03-10T14:16:53.33142Z","iopub.status.idle":"2022-03-10T14:17:09.882404Z","shell.execute_reply.started":"2022-03-10T14:16:53.331384Z","shell.execute_reply":"2022-03-10T14:17:09.881204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = ln.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:09.885527Z","iopub.execute_input":"2022-03-10T14:17:09.88653Z","iopub.status.idle":"2022-03-10T14:17:09.944309Z","shell.execute_reply.started":"2022-03-10T14:17:09.886479Z","shell.execute_reply":"2022-03-10T14:17:09.943358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Test the models and Submission\n\nSince this is a regression prediction, I think I won't need a validation data. I will just measure the model by the score.","metadata":{}},{"cell_type":"code","source":"len(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:09.947336Z","iopub.execute_input":"2022-03-10T14:17:09.94925Z","iopub.status.idle":"2022-03-10T14:17:09.960742Z","shell.execute_reply.started":"2022-03-10T14:17:09.949196Z","shell.execute_reply":"2022-03-10T14:17:09.959844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"prepare the submission.","metadata":{}},{"cell_type":"markdown","source":"First, make sure that the id are matched between the train data and y_pred.","metadata":{}},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:09.96494Z","iopub.execute_input":"2022-03-10T14:17:09.966617Z","iopub.status.idle":"2022-03-10T14:17:09.994857Z","shell.execute_reply.started":"2022-03-10T14:17:09.966562Z","shell.execute_reply":"2022-03-10T14:17:09.993885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id ","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:09.997411Z","iopub.execute_input":"2022-03-10T14:17:09.998501Z","iopub.status.idle":"2022-03-10T14:17:10.02834Z","shell.execute_reply.started":"2022-03-10T14:17:09.998443Z","shell.execute_reply":"2022-03-10T14:17:10.027182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The id are matched. Let's submit","metadata":{}},{"cell_type":"code","source":"sample['sales'] = y_pred\nsample","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.030928Z","iopub.execute_input":"2022-03-10T14:17:10.032056Z","iopub.status.idle":"2022-03-10T14:17:10.059306Z","shell.execute_reply.started":"2022-03-10T14:17:10.031998Z","shell.execute_reply":"2022-03-10T14:17:10.058442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The actual submission\nsample.to_csv('submission.csv', index = False) # Submit","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.061543Z","iopub.execute_input":"2022-03-10T14:17:10.062474Z","iopub.status.idle":"2022-03-10T14:17:10.181034Z","shell.execute_reply.started":"2022-03-10T14:17:10.062423Z","shell.execute_reply":"2022-03-10T14:17:10.180215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Problems Notes\n\nThese are the notes I plan to use in the next iteration of this project or others. I plan to do this project as a freely unguide - simple project to see what would happen if I want to try time series with some crude skills and knowledge. I hope that, as I keep going, I will develop better understanding and more mastery. I hope that this note book in the next next iteration will be better and more robust.\n\n\n### Problems\n1. Need more coding, statistic and ML mastery to fully tackle the problem \n    * -> Practicing\n2. Still Have no idea how to deal with the different selling patterns between different Product Families.\n    * -> Some great notebooks are dealing with this by using more complicated models. I will try to understand them.\n3. I feel like the Oil price must be beneficial for training the model. Just don't know how to utilize it.\n    * -> Explore more idea from other notebooks\n4. Need more practice using Pandas snd Matplotlib.\n5. Sometimes I'm not sure how to impliment the insight from the EDA into the model.\n6. Lacking of project planning experience seems to cost me a lot of time fixing and refixing the code and also the procedure. For example, I had construct the holiday dummy table just to find that I need to to the entire formatting and dummying again in the main calendar DF since the Holiday DF are note date complete.\n    * -> I guess this is how the lesson is learned\n7. Memory leak: I was writing code without thinking about the memory leak. This is really bad practice and the first time I got it in ML. This stall my entire process and need to be fix right away.\n    * -> Concern more \n    * -> I Google and found this great [notebook](https://www.kaggle.com/pavansanagapati/14-simple-tips-to-save-ram-memory-for-1-gb-dataset/comments). Upvoted.\n        * simpler methods I think I'm gonna use here in this notebook are:\n            * Free Memory using gc.collect()\n            * Datatype Conversions (for convert dummy or categorical data into category using data.astype('category')\n            * Dealing with memory leak by using in-function variable (which will be cleared after the function done executing.\n    * When merging BIG dataframes, it might be better for limited memory to convert the bigger one in to csv at a managable chunk using chunksize= pharameter.\n    * Still, due to my lake of experience the DF structure in have in mind still won't work. It got over 4M rows. Need to fix immediately.\n","metadata":{}},{"cell_type":"markdown","source":"# 9. Conclusion\n\nThe result is really bad. Let's breakdown the model to see why.","metadata":{}},{"cell_type":"markdown","source":"Calculate the Coeficients of the model.","metadata":{}},{"cell_type":"code","source":"coef = pd.DataFrame(data = ln.coef_, index=x_train.columns,columns=['coef'])\ncoef['abs_value'] = abs(coef['coef'])\n\ncoef = coef.sort_values(by='abs_value',ascending=False)\n\n\npd.set_option('display.max_rows',None)\ndisplay(coef)\npd.reset_option('display.max_rows')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.182049Z","iopub.execute_input":"2022-03-10T14:17:10.182316Z","iopub.status.idle":"2022-03-10T14:17:10.208691Z","shell.execute_reply.started":"2022-03-10T14:17:10.182287Z","shell.execute_reply":"2022-03-10T14:17:10.207774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is very peculiar. I'm not expected that the top features will be those of the store locations (with exactly equal weight), also the weight is really high. Something is really really wrong here","metadata":{}},{"cell_type":"markdown","source":"Next' the intercept.","metadata":{}},{"cell_type":"code","source":"ln.intercept_","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.210487Z","iopub.execute_input":"2022-03-10T14:17:10.210721Z","iopub.status.idle":"2022-03-10T14:17:10.221985Z","shell.execute_reply.started":"2022-03-10T14:17:10.210694Z","shell.execute_reply":"2022-03-10T14:17:10.22114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In intercept is really strange. begining at a super big number. This is a sigh of really big residuals.\n\nLet's plot the predictions. I might find something even worse.","metadata":{}},{"cell_type":"code","source":"result = x_test\nresult['predicted_sales'] = y_pred\nresult = result['predicted_sales'].groupby(['date','store_nbr']).sum()\n\nresult","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.223892Z","iopub.execute_input":"2022-03-10T14:17:10.225107Z","iopub.status.idle":"2022-03-10T14:17:10.258322Z","shell.execute_reply.started":"2022-03-10T14:17:10.225032Z","shell.execute_reply":"2022-03-10T14:17:10.257441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = result.unstack()\n\npd.set_option('display.max_columns',None)\nresult","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.26022Z","iopub.execute_input":"2022-03-10T14:17:10.260457Z","iopub.status.idle":"2022-03-10T14:17:10.337205Z","shell.execute_reply.started":"2022-03-10T14:17:10.260428Z","shell.execute_reply":"2022-03-10T14:17:10.336472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (15,10))\nplt.plot(result)\nplt.title('Predicted sales for each store')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T14:17:10.338787Z","iopub.execute_input":"2022-03-10T14:17:10.339298Z","iopub.status.idle":"2022-03-10T14:17:10.693507Z","shell.execute_reply.started":"2022-03-10T14:17:10.339267Z","shell.execute_reply":"2022-03-10T14:17:10.692224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Too bad. each store follow the same pattern since I use only one model to predict each store. Few (store 10 16 33 36) has high peak during the last days. I wonder what do thay have in common? let's dig deeper","metadata":{}},{"cell_type":"markdown","source":"# 10. What could have done better? + Idea for the next iteration.\n\nThis is what I have in mind after the first run. I would be very appreciated to all suggestion in the discussion panel. \n\n1. I forgot to include the insight that some store don't sell some product. Those combination sales could be set to zero flat.\n2. Removing the store locations from the inputs might be a good idea. (Perhaps Check the colinearity between all the features)\n3. Perhaps, each store should have the model trained individually since different store has different sales pattern, therefore, diffenet weight characteristics.\n4. Take in account only the holiday from the main operating area (Quito City) and national holiday\n5. Instead of dropna for many lag feature, maybe I should fill_na with zero to preserve the observation.\n6. Try scale the onpromotion too\n7. Dummy the DOW variable.\n8. Try using MA instead of lag feature\n9. Using Pipeline in the data processing\n10. Using Sliding windows technique for training.","metadata":{}},{"cell_type":"markdown","source":"# 11. Footnote\n\nIt's been a really daunting task for a first-timer like me just to do this from the beginning to the submission. For this simple-educational competition, and using only the linear regression model, It took me many more days than I expected. However, after the submission, I felt really happy that I had finally finised the first attempt.\n\nThe score I got is **3.44178**, which, is noting less than an \"abysmal\" just like the level I expected to have compared to the **0.5xx** from the Kaggle Course. Still, I'm very happy about the process and what I've learn from practicing with this notebook.\n\nThe code here, I'm sure, must be very scrappy, but for the next iteration with more experience and more complicated model (I'm thinking about DeterministicProcess(), Fourier Combination, Forest etc.), I hope to produce better result and better understanding of data science.\n\nFor Kaggle and everyone that read through my notebook, thank you.","metadata":{}}]}