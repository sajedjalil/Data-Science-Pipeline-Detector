{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [1. Introduction](#Introduction)\n* [2. Data Analysis](#Data_Analysis)\n* [3. Feature Engineering](#Feature_Engineering)\n* [4. Model Building](#Model_Building)\n    * [4.1. First Model](#First_Model)\n    * [4.2. Second Model](#Second_Model)\n    * [4.3. Third Model](#Third_Model)\n* [5. Model Comparing](#Model_Comparing)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Introduction\"></a>\n# 1. Introduction\n\nKaggle describes this competition as [follows](https://www.kaggle.com/c/covid19-global-forecasting-week-5/overview)\n\n**The Challenge**\n<br>Kaggle is launching a companion COVID-19 forecasting challenges to help answer a subset of the NASEM/WHO questions. While the challenge involves developing quantile estimates intervals for confirmed cases and fatalities between May 12 and June 7 by region, the primary goal isn't only to produce accurate forecasts. Itâ€™s also to identify factors that appear to impact the transmission rate of COVID-19."},{"metadata":{},"cell_type":"markdown","source":"## The Story of COVID-19\n#### The COVID-19 pandemic is the defining global health crisis of our time and the greatest global humanitarian challenge the world has faced since World War II. The virus has spread widely, and the number of cases is rising daily as governments work to slow its spread. India has moved quickly, implementing a proactive, nationwide, lockdown, with the goal of flattening the curve and using the time to plan and resource responses adequately."},{"metadata":{},"cell_type":"markdown","source":"![alt text](https://kesk.org.tr/wp-content/uploads/2020/04/covid.png)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\npd.pandas.set_option('display.max_columns', None)\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\npd.pandas.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nfrom fbprophet import Prophet\nimport plotly.express as px\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import ParameterGrid\nfrom tqdm import tqdm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/covid19-global-forecasting-week-5/train.csv\"\npath2 = \"../input/covid19-global-forecasting-week-5/test.csv\"\npath3=\"../input/covid19-useful-features-by-country/Countries_usefulFeatures.csv\"\npath4=\"../input/covid19-global-forecasting-week-5/submission.csv\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(path,encoding = 'unicode_escape')\ndf_test = pd.read_csv(path2,encoding = 'unicode_escape')\ndf_count_feat=pd.read_csv(path3,encoding = 'unicode_escape')\ndf_sub=pd.read_csv(path4,encoding = 'unicode_escape')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Data_Analysis\"></a>\n# 2. Data_Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below We see that County and Province_State variable have null values..**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below We see that TargetValue variable has negatif values at some point. But the number of cases must be at least 0..**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We drop the negatif values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(df_train[df_train.TargetValue < 0].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"country_wise=df_train[df_train['Province_State'].isnull()]\ncountry_wise=country_wise[country_wise['Target']=='ConfirmedCases']\ncountry_wise=country_wise.groupby('Country_Region')['TargetValue'].sum().reset_index()\ncountry_wise=country_wise.rename(columns={\"Country_Region\":\"Country_Region\",\"TargetValue\":\"ConfimedCases\"})\ncountry_wise","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### According to Map and the graph below, we can say that the number of cases is mostly in the US and then in Brazil.."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_map(df, col, pal):\n    df = df[df[col]>0]\n    fig = px.choropleth(df, locations=\"Country_Region\", locationmode='country names', \n                  color=col, hover_name=\"Country_Region\", \n                  title=col, hover_data=[col], color_continuous_scale=pal)\n#     fig.update_layout(coloraxis_showscale=False)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_map(country_wise, 'ConfimedCases', 'matter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Top 20 Countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_hbar(df, col, n, hover_data=[]):\n    fig = px.bar(df.sort_values(col).tail(n), \n                 x=col, y=\"Country_Region\", color='Country_Region',  \n                 text=col, orientation='h', width=700, hover_data=hover_data,\n                 color_discrete_sequence = px.colors.qualitative.Dark2)\n    fig.update_layout(title=col, xaxis_title=\"\", yaxis_title=\"\", \n                      yaxis_categoryorder = 'total ascending',\n                      uniformtext_minsize=8, uniformtext_mode='hide')\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_hbar(country_wise, 'ConfimedCases', 15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We merge the 3 countries with the highest number of cases in a single table..**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Us=df_train[df_train['Country_Region']=='US']\ndf_Us=df_Us[df_Us['Target']=='ConfirmedCases']\ndf_Us=df_Us[df_Us['Province_State'].isnull()]\ndf_plot=df_Us.rename(columns={\"Date\":\"Date\",\"TargetValue\": \"US_TotalCase\"})\ndf_plot=df_plot[[\"Date\",\"US_TotalCase\"]]\ndf_Br=df_train[df_train['Country_Region']=='Brazil']\ndf_Br=df_Br[df_Br['Target']=='ConfirmedCases']\ndf_Br=df_Br.rename(columns={\"Date\":\"Date\",\"TargetValue\": \"Brazil_TotalCase\"})\ndf_Br=df_Br[[\"Date\",\"Brazil_TotalCase\"]]\ndf_Rus=df_train[df_train['Country_Region']=='Russia']\ndf_Rus=df_Rus[df_Rus['Target']=='ConfirmedCases']\ndf_Rus=df_Rus.rename(columns={\"Date\":\"Date\",\"TargetValue\": \"Russia_TotalCase\"})\ndf_Rus=df_Rus[[\"Date\",\"Russia_TotalCase\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_plot=df_plot.merge(df_Br,on='Date').merge(df_Rus,on='Date')\ndf_plot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### When we look at the graph below, we see the top 3 country's behaviours. After May, the cases in US and Brazil increase while those in Russia decrease."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df_plot.groupby('Date')['Russia_TotalCase','Brazil_TotalCase','US_TotalCase'].sum().reset_index()\ntemp = temp.melt(id_vars=\"Date\", value_vars=['Russia_TotalCase','Brazil_TotalCase','US_TotalCase'],\n                 var_name='Case', value_name='Count')\ntemp.head()\n\nfig = px.area(temp, x=\"Date\", y=\"Count\", color='Case', height=600, width=700,\n             title='Cases over time', color_discrete_sequence = [\"blue\", \"green\", \"red\"])\nfig.update_layout(xaxis_rangeslider_visible=True)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train2=df_train.merge(df_count_feat[['Country_Region','Tourism','Latitude','Longtitude','Mean_Age','Lockdown_Date','Lockdown_Type']], on='Country_Region', how='inner', sort=False)\ndf_train2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Feature_Engineering\"></a>\n# 3. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Creating ConfirmedCases and Fatalities variables for some calculation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def confatal(df):\n    df_Confirmed_Cases=df[df[\"Target\"]==\"ConfirmedCases\"]\n    df_Confirmed_Cases=df_Confirmed_Cases.rename(columns={\"TargetValue\": \"ConfirmedCases\"})\n    df_Confirmed_Cases=df_Confirmed_Cases.drop(['Target'], axis=1)\n    df_Fatalities=df[df[\"Target\"]==\"Fatalities\"]\n    df_Fatalities=df_Fatalities.rename(columns={\"TargetValue\": \"Fatalities\"})\n    df_Fatalities=df_Fatalities.drop(['Target'], axis=1)\n    df=pd.merge(df_Confirmed_Cases,df_Fatalities[['Date','County','Province_State','Country_Region','Fatalities']],on=['Date','County','Province_State','Country_Region'], how='inner')\n    df=df[['Id','County','Province_State','Country_Region','Population','Weight','Date','ConfirmedCases','Fatalities','Tourism','Latitude','Longtitude','Mean_Age','Lockdown_Date','Lockdown_Type']]\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_confat=confatal(df_train2)\ndf_confat.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Datetime Feautures"},{"metadata":{},"cell_type":"markdown","source":"**We create datetime features to use in the model..**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ndef create_date_features(df):\n    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n    df['Year']=df.Date.dt.year\n    df['Month']=df.Date.dt.month\n    df['Day']=df['Date'].dt.strftime('%d')\n    df['Day_number_of_week'] = df.Date.dt.weekday\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_date_features(df_confat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transformation of datetime features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getdayofweek(dow):\n    if (dow == 0):\n        return \"Monday\"\n    elif(dow == 1):\n        return \"Tuesday\"\n    elif(dow ==2):\n        return \"Wednesday\"\n    elif(dow == 3):\n        return \"Thursday\"\n    elif(dow ==4):\n        return \"Friday\"\n    elif(dow == 5):\n        return \"Saturday\"\n    elif(dow ==6):\n        return \"Sunday\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_confat['Dayofweek'] = df_confat.Day_number_of_week.apply(getdayofweek)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Model_Building\"></a>\n# 4. Model Building"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"First_Model\"></a>\n>## 4.1 Time Series for US - with Prophet\nI use Prophet for time series because it provides intuitive parameters which are easy to tune.."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Us=df_train[df_train['Country_Region']=='US']\ndf_Us=df_Us[df_Us['Target']=='ConfirmedCases']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Us=df_Us[df_Us['Province_State'].isnull()]\ndf_Us","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nsns.lineplot(data=df_Us[df_Us['Date']<\"2020-05-01\"], x=\"Date\", y=\"TargetValue\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are asked to predict the cases after May of 12, we split our observations from this point as Train-Test. Train data set includes 110 observations, test data set 30 observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_us=df_Us[df_Us[\"Date\"]<\"2020-05-12\"]\nTest_us=df_Us[df_Us[\"Date\"]>=\"2020-05-12\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_us=Train_us[[\"Date\",\"TargetValue\"]]\nTest_us=Test_us[[\"Date\",\"TargetValue\"]]\nTrain_us=Train_us.rename(columns={\"Date\":\"ds\",\"TargetValue\":\"y\"})\nTest_us=Test_us.rename(columns={\"Date\":\"ds\",\"TargetValue\":\"y\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Prophet(growth='linear',changepoint_prior_scale=60)\nmodel.fit(Train_us)\nforecast = model.predict(Test_us)\nfig = model.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = model.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_us['yhat']=forecast['yhat'].values\nTest_us","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we look at the Actual-Prediction chart, we see that the model can catch the change-points, but the difference between the actual and the prediction increased after May."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\nplt.plot(Test_us['ds'], Test_us['y'], 'b-', label = 'Actual')\nplt.plot(Test_us['ds'], Test_us['yhat'], 'r--', label = 'Prediction')\nplt.xlabel('Date',rotation=90); plt.ylabel('Sales'); plt.title('Actual vs Prediction')\nplt.xticks(rotation=90)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is %84 for the prophet model with default parameters.. We should try to do parameter tuning to increase accuracy.."},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_us['diff']=(Test_us.y-Test_us.yhat).abs()\nacc_ts=(1-(Test_us['diff'].sum()/Test_us['y'].sum()))*100\nacc_ts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I use MAE, MSE, RMSE performance metric, because it is easy to explain. Prediction differs 3378 case from the actual.. It is not bad, because there are around 20000, 25000 case in US in a day.."},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_ts=metrics.mean_absolute_error(Test_us['y'], Test_us['yhat'])\nMSE_ts=metrics.mean_squared_error(Test_us['y'], Test_us['yhat'])\nRMSE_ts=np.sqrt(metrics.mean_squared_error(Test_us['y'], Test_us['yhat']))\nprint('MAE:', MAE_ts)\nprint('MSE:', MSE_ts)\nprint('RMSE:', RMSE_ts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameter Tuning for Prophet"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_grid = {'seasonality_mode':('multiplicative','additive'),\n               'changepoint_prior_scale':[0.5,1.2,2.5],\n              'seasonality_prior_scale':[0.5,1.2,2.5]\n              }\ngrid = ParameterGrid(params_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_parameters = pd.DataFrame(columns = ['Acc','Parameters'])\nfor p in tqdm(grid):\n    \n    Train=Train_us.copy()\n    Valid=Test_us[['ds','y']]\n            \n    m =Prophet(changepoint_prior_scale = p['changepoint_prior_scale'],\n               seasonality_prior_scale = p['seasonality_prior_scale'],\n               seasonality_mode = p['seasonality_mode'],\n               interval_width=0.95)\n            \n    m.fit(Train_us)\n            \n            \n    forecast = m.predict(Valid[['ds']])\n    forecast = forecast.astype({\"ds\": object})\n    Valid=Valid.merge(forecast[['ds', 'yhat']],'inner',['ds'])\n    \n    #performance metric\n    Valid['diff']=(Valid.y-Valid.yhat).abs()\n    acc=(1-((Valid['diff'].sum()/Valid['y'].sum())))*100\n    \n    model_parameters = model_parameters.append({'Acc':acc,'Parameters':p},ignore_index=True)\n            \nparameters = model_parameters.sort_values(by=['Acc'],ascending=False)\nparameters = parameters.reset_index(drop=True)\n        \nbest_parameters=parameters['Parameters'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fit the model with best parameters.."},{"metadata":{"trusted":true},"cell_type":"code","source":"m = Prophet(\n        growth=\"linear\",\n        seasonality_mode=best_parameters['seasonality_mode'],\n        changepoint_prior_scale=best_parameters['changepoint_prior_scale'],\n        seasonality_prior_scale=best_parameters['seasonality_prior_scale']\n        )\nm.fit(Train_us)\nforecast=m.predict(Test_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see trend and weekly seasonality for train dataset.."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = m.plot_components(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot = m.plot(forecast)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_us['yhat']=forecast['yhat'].values\nplt.figure(figsize=(20, 8))\nplt.plot(Test_us['ds'], Test_us['y'], 'b-', label = 'Actual')\nplt.plot(Test_us['ds'], Test_us['yhat'], 'r--', label = 'Prediction')\nplt.xlabel('Date',rotation=90); plt.ylabel('Sales'); plt.title('Actual vs Prediction')\nplt.xticks(rotation=90)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is better with the best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_us['diff']=(Test_us.y-Test_us.yhat).abs()\nacc_ts2=(1-(Test_us['diff'].sum()/Test_us['y'].sum()))*100\nacc_ts2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_ts2=metrics.mean_absolute_error(Test_us['y'], Test_us['yhat'])\nMSE_ts2=metrics.mean_squared_error(Test_us['y'], Test_us['yhat'])\nRMSE_ts2=np.sqrt(metrics.mean_squared_error(Test_us['y'], Test_us['yhat']))\nprint('MAE:', MAE_ts2)\nprint('MSE:', MSE_ts2)\nprint('RMSE:', RMSE_ts2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Second_Model\"></a>\n>## 4.2 US Confimed Case Forecasting with Random Forest Reggressor"},{"metadata":{},"cell_type":"markdown","source":"**Creating Date Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_us=create_date_features(df_Us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating day of week variable from Date**"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_us['Dayofweek'] = reg_us.Day_number_of_week.apply(getdayofweek)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One hot encoding for Dayofweek Feature**\n<br>We use one hot encoding to transform categorical variables (Day0fweek) to use in our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_us=pd.get_dummies(reg_us,columns=['Dayofweek'])\nreg_us.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting Data - Train and Test**\n<br> We will try to predict the cases after 2020-05-12, we are spliting the dataset from here.."},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_reg_us=reg_us[reg_us[\"Date\"]<\"2020-05-12\"]\nTest_reg_us=reg_us[reg_us[\"Date\"]>=\"2020-05-12\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_reg_us=Train_reg_us[['Month','Dayofweek_Monday','Dayofweek_Tuesday','Dayofweek_Wednesday','Dayofweek_Thursday','Dayofweek_Friday','Dayofweek_Saturday','Dayofweek_Sunday']]\ny_train_reg_us=Train_reg_us[['TargetValue']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_reg_us=Test_reg_us[['Month','Dayofweek_Monday','Dayofweek_Tuesday','Dayofweek_Wednesday','Dayofweek_Thursday','Dayofweek_Friday','Dayofweek_Saturday','Dayofweek_Sunday']]\ny_test_reg_us=Test_reg_us[['TargetValue']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_us = RandomForestRegressor(n_estimators=100)\nrf_us.fit(x_train_reg_us,y_train_reg_us)\npred_rf = rf_us.predict(x_test_reg_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)Accuracy is %83.42 for RandomForest Regressor model with default parameters.. We should try to do parameter tuning to increase accuracy.."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_reg_us['diff']=(y_test_reg_us.TargetValue-pred_rf).abs()\nacc_rf=(1-(y_test_reg_us['diff'].sum()/y_test_reg_us['TargetValue'].sum()))*100\nacc_rf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_rf=metrics.mean_absolute_error(y_test_reg_us.TargetValue, pred_rf)\nMSE_rf=metrics.mean_squared_error(y_test_reg_us.TargetValue, pred_rf)\nRMSE_rf=np.sqrt(metrics.mean_squared_error(y_test_reg_us.TargetValue, pred_rf))\nprint('MAE:', MAE_rf)\nprint('MSE:', MSE_rf)\nprint('RMSE:', RMSE_rf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameter Tuning for RandomForest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n        \"n_estimators\"      : [10,20,300,100,200,500],\n        \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n        \"min_samples_split\" : [2,4,6,8],\n        \"bootstrap\": [True, False],\n            }\ngrid = ParameterGrid(param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_parameters = pd.DataFrame(columns = ['Acc','Parameters'])\nfor p in tqdm(grid):\n    \n    X_Train=x_train_reg_us.copy()\n    Y_Train=y_train_reg_us.copy()\n    X_Valid=x_test_reg_us.copy()\n    Y_Valid=y_test_reg_us.copy()\n    m = RandomForestRegressor(n_estimators = p['n_estimators'],\n               max_features = p['max_features'],\n               min_samples_split = p['min_samples_split'],\n               bootstrap=p['bootstrap'])\n    \n    \n    \n    m.fit(X_Train,Y_Train)\n    pred_rf2 = m.predict(X_Valid)\n            \n    Y_Valid['yhat']=pred_rf2\n    \n    #performance metric\n    Y_Valid['diff']=(Y_Valid.TargetValue-Y_Valid.yhat).abs()\n    acc=(1-((Y_Valid['diff'].sum()/Y_Valid['TargetValue'].sum())))*100\n    \n    model_parameters = model_parameters.append({'Acc':acc,'Parameters':p},ignore_index=True)\n            \nparameters = model_parameters.sort_values(by=['Acc'],ascending=False)\nparameters = parameters.reset_index(drop=True)\n        \nbest_parameters=parameters['Parameters'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = RandomForestRegressor(\n        bootstrap=best_parameters['bootstrap'],\n        max_features=best_parameters['max_features'],\n        min_samples_split=best_parameters['min_samples_split'],\n        n_estimators=best_parameters['n_estimators']\n        )\nm.fit(x_train_reg_us,y_train_reg_us)\npred_rf2 = m.predict(x_test_reg_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is better with the best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_reg_us['diff']=(y_test_reg_us.TargetValue-pred_rf2).abs()\nacc_rf2=(1-(y_test_reg_us['diff'].sum()/y_test_reg_us['TargetValue'].sum()))*100\nacc_rf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_rf2=metrics.mean_absolute_error(y_test_reg_us.TargetValue, pred_rf2)\nMSE_rf2=metrics.mean_squared_error(y_test_reg_us.TargetValue, pred_rf2)\nRMSE_rf2=np.sqrt(metrics.mean_squared_error(y_test_reg_us.TargetValue, pred_rf2))\nprint('MAE:', MAE_rf2)\nprint('MSE:', MSE_rf2)\nprint('RMSE:', RMSE_rf2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Third_Model\"></a>\n>## 4.3 US Confimed Case Forecasting with  XGBOOST Reggressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_us = XGBRegressor(n_estimators=100)\nxgb_us.fit(x_train_reg_us,y_train_reg_us)\nxgb_pred = xgb_us.predict(x_test_reg_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is %83 for XGBOOST Regressor model with default parameters.. We should try to do parameter tuning to increase accuracy.."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_reg_us['diff']=(y_test_reg_us.TargetValue-xgb_pred).abs()\nacc_xg=(1-(y_test_reg_us['diff'].sum()/y_test_reg_us['TargetValue'].sum()))*100\nacc_xg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_xgb=metrics.mean_absolute_error(y_test_reg_us.TargetValue, xgb_pred)\nMSE_xgb=metrics.mean_squared_error(y_test_reg_us.TargetValue, xgb_pred)\nRMSE_xgb=np.sqrt(metrics.mean_squared_error(y_test_reg_us.TargetValue, xgb_pred))\nprint('MAE:', MAE_xgb)\nprint('MSE:', MSE_xgb)\nprint('RMSE:', RMSE_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameter Tuning for XGBOOST Regressor"},{"metadata":{},"cell_type":"markdown","source":"XGBOOST Regressor Fit with GridSearch Parameters for cv data"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \n            'nthread':[4], #when use hyperthread, xgboost may become slower,\n            'learning_rate': [.03, 0.05, .07], #so called `eta` value\n            'max_depth': [5, 6, 7],\n            'min_child_weight': [1,4],\n            'subsample': [0.7],\n            'colsample_bytree': [0.7],\n            'n_estimators': [100,200,500]\n            }\ngrid = ParameterGrid(param_grid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_parameters = pd.DataFrame(columns = ['Acc','Parameters'])\nfor p in tqdm(grid):\n    \n    X_Train=x_train_reg_us.copy()\n    Y_Train=y_train_reg_us.copy()\n    X_Valid=x_test_reg_us.copy()\n    Y_Valid=y_test_reg_us.copy()\n    m = XGBRegressor(nthread = p['nthread'],\n               learning_rate = p['learning_rate'],\n               max_depth=p['max_depth'],\n               min_child_weight = p['min_child_weight'],\n               subsample = p['subsample'],\n               colsample_bytree=p['colsample_bytree'],\n               n_estimators=p['n_estimators']             )\n    \n    \n    \n    m.fit(X_Train,Y_Train)\n    pred_xg2 = m.predict(X_Valid)\n            \n    Y_Valid['yhat']=pred_xg2\n    \n    #performance metric\n    Y_Valid['diff']=(Y_Valid.TargetValue-Y_Valid.yhat).abs()\n    acc=(1-((Y_Valid['diff'].sum()/Y_Valid['TargetValue'].sum())))*100\n    \n    model_parameters = model_parameters.append({'Acc':acc,'Parameters':p},ignore_index=True)\n            \nparameters = model_parameters.sort_values(by=['Acc'],ascending=False)\nparameters = parameters.reset_index(drop=True)\n        \nbest_parameters=parameters['Parameters'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = XGBRegressor(\n        nthread=best_parameters['nthread'],\n        learning_rate=best_parameters['learning_rate'],\n        max_depth=best_parameters['max_depth'],\n        min_child_weight=best_parameters['min_child_weight'],\n        subsample=best_parameters['subsample'],\n        colsample_bytree=best_parameters['colsample_bytree'],\n        n_estimators=best_parameters['n_estimators']\n        )\nm.fit(x_train_reg_us,y_train_reg_us)\npred_xg2 = m.predict(x_test_reg_us)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Accuracy is better with the best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_reg_us['diff']=(y_test_reg_us.TargetValue-pred_xg2).abs()\nacc_xg2=(1-(y_test_reg_us['diff'].sum()/y_test_reg_us['TargetValue'].sum()))*100\nacc_xg2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAE_xgb2=metrics.mean_absolute_error(y_test_reg_us.TargetValue, pred_xg2)\nMSE_xgb2=metrics.mean_squared_error(y_test_reg_us.TargetValue, pred_xg2)\nRMSE_xgb2=np.sqrt(metrics.mean_squared_error(y_test_reg_us.TargetValue, pred_xg2))\nprint('MAE:', MAE_xgb2)\nprint('MSE:', MSE_xgb2)\nprint('RMSE:', RMSE_xgb2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"Model_Comparing\"></a>\n## 5. Model Comparison for Confirmed Cases for US"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_performance = {'Model':['Prophet','Prophet','Random Forest','Random Forest','XGBoost','XGBoost'],\n        'Parameters':['Default','Best','Default','Best','Default','Best'],\n        'Accuracy':[acc_ts,acc_ts2,acc_rf,acc_rf2,acc_xg,acc_xg2],\n        'MAE': [MAE_ts,MAE_ts2, MAE_rf,MAE_rf2,MAE_xgb,MAE_xgb2],\n        'MSE': ['{:f}'.format(MSE_ts),'{:f}'.format(MSE_ts2),'{:f}'.format(MSE_rf),'{:f}'.format(MSE_rf2),'{:f}'.format(MSE_xgb),'{:f}'.format(MSE_xgb2)], \n        'RMSE': ['{:f}'.format(RMSE_ts),'{:f}'.format(RMSE_ts2),'{:f}'.format(RMSE_rf),'{:f}'.format(RMSE_rf2),'{:f}'.format(RMSE_xgb),'{:f}'.format(RMSE_xgb2)]}\npd.DataFrame.from_dict(df_performance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(9,4))\na=sns.barplot(data=df_performance,x=\"Model\", y=\"Accuracy\",hue = 'Parameters')\na.set_title(\"Model Performance\",fontsize=15)\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.ylim(60, 92)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(9,4))\na=sns.barplot(data=df_performance,x=\"Model\", y=\"MAE\",hue = 'Parameters')\na.set_title(\"Model Performance\",fontsize=15)\nplt.xlabel('Model')\nplt.ylabel('Mean Absolute Error')\nplt.ylim(500, 4500)\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}