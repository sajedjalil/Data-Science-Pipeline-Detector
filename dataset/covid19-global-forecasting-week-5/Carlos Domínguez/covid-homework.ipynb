{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook has all my submissions to the week 5 of the COVID19 Global Forecasting competition. Each submission is a version of this notebook.\n\nPlease ignore the latest versions of the notebook, I was trying some tweeks a few hours before the deadline and the results turned to be a bit embarrassing ðŸ˜… with a failed submission and a public score with a lot of digits.\n\n__The best two submissions in public LB are in [version 9](https://www.kaggle.com/carlosdg/covid-homework?scriptVersionId=33788702) and in [version 11](https://www.kaggle.com/carlosdg/covid-homework?scriptVersionId=33797228)__. The one used for the private LB right now is the version 11. Both are fast.ai tabular models that after quite some time I could finally made possible thanks to this awesome notebook: https://www.kaggle.com/syzymon/fast-ai-v2-starter-pack-covid19-5-non-leaky by [@syzymon](https://www.kaggle.com/syzymon). \n\nThe idea behind all my submissions were to use models that could use quantile loss for the predictions. Because there were just a few days left I just wanted to try some ideas that didn't take too much time. My first submissions were using catboost. Although I know that trees don't extrapolate I thought that maybe using all the data could help most places with lower cases/fatalities. Then I saw [this medium article with an implementation of the QuantileLoss for Pytorch](https://medium.com/the-artificial-impostor/quantile-regression-part-2-6fdbc26b2629) and I decided to switch to fast.ai. The choices of catboost and fast.ai tabular models were because they already have something integrated to deal with categorical variables and they are very simple to use. As for feature engineering I didn't do much, I just added datetime variables and try scaling some of them and cyclic encoding but in the end I think that I left them as categorical variables. I wanted to use some more complicated features but in the end I didn't ðŸ˜…\n\nI don't think my submissions have leaks, I tried to use training data before April 27. If you find anything wrong or have any question, please let me now ðŸ™‚","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install fastai2 --quiet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom fastai2.basics import *\nfrom fastai2.tabular.all import *\n\nimport catboost\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/train.csv\", parse_dates=[\"Date\"])\ndf_test = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/test.csv\", parse_dates=[\"Date\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_nulls(df):\n    df[\"County\"].fillna(\"<Unk>\", inplace=True)\n    df[\"Province_State\"].fillna(\"<Unk>\", inplace=True)\n    \nfill_nulls(df_train)\nfill_nulls(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_cases = df_train[df_train.Target == \"ConfirmedCases\"].drop(columns=[\"Target\"])\ndf_train_fatalities = df_train[df_train.Target == \"Fatalities\"].drop(columns=[\"Target\"])\n\ndf_test_cases = df_test[df_test.Target == \"ConfirmedCases\"].drop(columns=[\"Target\"])\ndf_test_fatalities = df_test[df_test.Target == \"Fatalities\"].drop(columns=[\"Target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(df_train_cases, 'Date', drop=False)\nadd_datepart(df_train_fatalities, 'Date', drop=False)\n\nadd_datepart(df_test_cases, 'Date', drop=False)\nadd_datepart(df_test_fatalities, 'Date', drop=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avoiding the leak will be done in the Predictor class\n\n#df_train_cases = df_train_cases[df_train_cases.Date < \"2020-04-27\"]\n#df_train_fatalities = df_train_fatalities[df_train_fatalities.Date < \"2020-04-27\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FROM https://medium.com/the-artificial-impostor/quantile-regression-part-2-6fdbc26b2629\n\nclass QuantileLoss(nn.Module):\n    def __init__(self, quantiles):\n        super().__init__()\n        self.quantiles = quantiles\n        \n    def forward(self, preds, target):\n        assert not target.requires_grad\n        assert preds.size(0) == target.size(0)\n        losses = []\n\n        for i, q in enumerate(self.quantiles):\n            errors = target[:, 0].unsqueeze(1) - preds[:, i].unsqueeze(1)\n\n            losses.append(\n                torch.max(\n                   (q-1) * errors, \n                   q * errors\n            ).unsqueeze(1))\n            \n        loss = torch.mean(\n            torch.sum(torch.cat(losses, dim=1), dim=1))\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pinball(preds, target):\n    assert preds.size(0) == target.size(0)\n    target_vals = target[:, 0]\n    target_weights = target[:, 1]\n    \n    losses = []\n\n    for i, q in enumerate(quantiles):\n        errors = (target_vals - preds[:, i]) * target_weights\n        losses.append(\n            torch.max(\n               (q-1) * errors, \n               q * errors\n            ).unsqueeze(1)\n        )\n\n    return torch.mean(\n        torch.mean(torch.cat(losses, dim=1), dim=1)\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_TRAIN_DATE = \"2020-04-27\"\nquantiles = [0.95, 0.5, 0.05]\ncont_vars = [\"Population\", 'Elapsed']\ncat_vars = [\"County\", \"Province_State\", \"Country_Region\",\n            \"Month\", \"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\", \n            'Is_month_end', 'Is_month_start', 'Is_quarter_end',\n           'Is_quarter_start', 'Is_year_end', 'Is_year_start']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Predictor():\n    def __init__(self, train_df, test_df):\n        self.dep_var = [\"TargetValue\", \"Weight\"]\n        self.train_df = self.train_df_processed(train_df)\n        \n        self.MAX_TRAIN_IDX = self.train_df[self.train_df['Date'] < MAX_TRAIN_DATE].shape[0]\n        \n        self.df_wrapper = self.prepare_df_wrapper(self.train_df)\n        self.dls = self.df_wrapper.dataloaders(bs=500, path='/kaggle/working/')\n        self.dls.c = len(quantiles) # Number of outputs of the network\n        \n        self.learner = tabular_learner(self.dls, \n                                      layers=[1000, 500],\n                                      opt_func=ranger, \n                                      loss_func=QuantileLoss(quantiles),\n                                      metrics=[pinball])\n        \n        self.test_dls = self.prepare_test_dl(test_df)\n       \n    \n    def train_df_processed(self, train_df):\n        df = train_df[cont_vars + cat_vars + self.dep_var + ['Date']].copy().sort_values('Date')\n        df = df[df.TargetValue >= 0]\n        df[\"TargetValue\"] = np.log1p(df.TargetValue)\n        return df\n    \n    \n    def prepare_df_wrapper(self, train_df_processed):\n        procs=[FillMissing, Categorify, Normalize]\n\n        splits = list(range(self.MAX_TRAIN_IDX)), list(range(self.MAX_TRAIN_IDX, len(train_df_processed)))\n\n        return TabularPandas(train_df_processed, \n                             procs,\n                             cat_vars.copy(), \n                             cont_vars.copy(), \n                             self.dep_var,\n                             y_block=TransformBlock(), \n                             splits=splits, )\n    \n\n    def prepare_test_dl(self, test_df_raw):\n        to_tst = self.df_wrapper.new(test_df_raw)\n        to_tst.process()\n        return self.dls.valid.new(to_tst)\n    \n    \n    def predict(self) -> np.ndarray:\n        tst_preds, _ = self.learner.get_preds(dl=self.test_dls)\n        tst_preds = tst_preds.data.numpy()\n        return np.expm1(tst_preds)\n    \n    \n    def lc(self):\n        emb_szs = get_emb_sz(self.df_wrapper)\n        print(emb_szs)\n        self.dls.show_batch()\n        self.test_dls.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cases = Predictor(df_train_cases, df_test_cases)\nmodel_fatalities = Predictor(df_train_fatalities, df_test_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_cases.learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cases.learner.fit_one_cycle(10, 0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_cases.learner.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_fatalities.learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fatalities.learner.fit_one_cycle(5, lr_max=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fatalities.learner.recorder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_cases = model_cases.predict()\npred_fatalities = model_fatalities.predict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_preds(preds=pred_cases, df_test=df_test_cases):\n    forecast_ids = []\n    for index in df_test[\"ForecastId\"]:\n        for quantile in quantiles:\n            forecast_ids.append(f\"{index}_{quantile}\")\n\n    return pd.Series(data=preds.reshape(-1), index=forecast_ids)\n\ncases_preds      = format_preds(pred_cases, df_test_cases)\nfatalities_preds = format_preds(pred_fatalities, df_test_fatalities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([cases_preds, fatalities_preds])\nsubmission = pd.DataFrame(submission, columns=[\"TargetValue\"])\nsubmission = submission.reset_index().rename(columns={'index': 'ForecastId_Quantile'})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}