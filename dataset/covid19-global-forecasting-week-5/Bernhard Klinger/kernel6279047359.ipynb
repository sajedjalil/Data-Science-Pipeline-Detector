{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Linear regression analysis for the COVID 19 Global Forecasting Challenge Week 5Â¶\n\nUpdated model from Week 4 to adjust for revised training data and evaluation metric - not enough time to make further changes to obtain better fit to training data.\n\n### Data import\n\nThe external data for the submission has been derived from some of the World Development Indicators from the World Bank Open Data (Population, GDP and health spending) - with some adjustments and estimates for missing data items.\n\nYou can find the full dateset and licence here: https://www.kaggle.com/theworldbank/world-development-indicators","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom datetime import datetime\n\npd.set_option('display.max_rows', None)\n\ntrain = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-5/train.csv\")\ntrain.columns = [\"Id\",\"Cty\",\"Prov\",\"Ctry\",\"Pop\",\"Weight\",\"Date\",\"Type\",\"Value\"]\ntest = pd.read_csv(\"../input/covid19-global-forecasting-week-5/test.csv\")\ntest.columns = [\"Id\",\"Cty\",\"Prov\",\"Ctry\",\"Pop\",\"Weight\",\"Date\",\"Type\"]\ntest[\"Value\"]=0\ntrain[\"Date\"]= pd.to_datetime(train.Date,infer_datetime_format=True)\ntest[\"Date\"]= pd.to_datetime(test.Date,infer_datetime_format=True)\nworld = pd.read_csv(\"/kaggle/input/wb0904/WorldBankData.csv\")\nfor col in world.columns[1:]:\n    avgcol= world[col].mean()\n    world.loc[world[col]==0,col]=avgcol\n\nsample_sub = pd.read_csv(\"../input/covid19-global-forecasting-week-5/submission.csv\")\nold_cols = sample_sub.columns\nsample_sub.columns=[\"Id\",\"Value\"]\n\nmysub=sample_sub.set_index('Id')\n\ntrain[\"Test\"]=0\ntest[\"Test\"]=1\n\nX_full = pd.concat((train[train.Date < \"2020-04-27\"], test[test.Date >= \"2020-04-27\"]),sort=True).reset_index(drop=True)\n\nX_full[\"Reg\"]=X_full[\"Ctry\"].astype(str)+X_full[\"Prov\"].astype(str)+X_full[\"Cty\"].astype(str)\n#pop[\"Reg\"]=pop[\"Ctry\"]+pop[\"Prov\"].fillna(\"None\")\n\nX_full= X_full.merge(world, on=[\"Ctry\"],how=\"left\")\n#X_full= X_full.merge(pop[[\"Pop\",\"Reg\"]], on=[\"Reg\"],how=\"left\")\n\nX_full.loc[:,\"GDPPerc\"]= X_full.GDPPerc.astype(\"float\")\nX_full.loc[:,\"GDPperCapita\"]= X_full.GDPperCapita.astype(\"float\")\nX_full.loc[X_full.GDPperCapita == 0,\"GDPperCapita\"]=10000\n    \nX_full.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_full.drop(columns=[\"DollarPPP\",\"Physicians\",\"Nurses and midwives\",\"Specialists\"],inplace=True)\nX_full[\"Test\"]=pd.to_numeric(X_full[\"Test\"],downcast=\"integer\")\nX_full[\"Pop\"]=pd.to_numeric(X_full[\"Pop\"],downcast=\"integer\")\nX_full[\"Value\"]=pd.to_numeric(X_full[\"Value\"],downcast=\"integer\")\n\nX_full.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', None)\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n\ndef model_fit(model,X,target_col,folds=3):\n    \n    kf = KFold(folds, shuffle=True, random_state=4991)\n    \n    drop_cols = set(X.columns).intersection({\"Test\",\"Id\",\"Value\",\"LogD\",\"Week\",\"Day\",\"Date\",\n                                             \"Prov\",\"FirstDate\",\"DayYear\",\"Sub\",\"Ctry\",\"Cty\",\"Reg\",\"Type\",\n                                             \"Pred\",\"Res\",\"Cluster\",\"Cluster1\",\"Pop\",\"TotalCases\",\"index\",\"Weight\",target_col})\n    \n    # create predictors for each region\n    \n    X_r=X\n    X[\"RegType\"]=X.Type+X.Reg\n    X[\"WDay\"] = (X.DayYear % 7).astype(str)\n    X_r =pd.get_dummies(X.copy(),columns=[\"RegType\",\"WDay\"])\n    \n    # add an indicator for countries/states with a significant number of cases\n    \n    for col in X_r.columns:\n        if col[:8]== \"RegType_\":\n            regtype = col[:8]\n            if X.loc[(X.RegType==regtype) & (X.DayYear == (107)),\"TotalCases\"].mean() > 25000:\n                \n                X_r[col+\"1\"]=X_r[\"Week1\"]*X_r[col]\n                X_r[col+\"2\"]=X_r[\"Week2\"]*X_r[col]\n                X_r[col+\"3\"]=X_r[\"Week3\"]*X_r[col]\n                          \n    \n    # add interactions with health spending indicators \n    # the relationship is very weak but I have kept these features in the model for now\n    \n    inter_features ={\"GDPPerc\",\"Week1\",\"Week2\",\"Week3\",\"Age65Perc\",\"GDPperCapita\",\"LogPop\"}.difference(set(drop_cols)).intersection(set(X.columns)) \n    poly = PolynomialFeatures(degree=2,include_bias=False) \n    inter_cols = poly.fit_transform(X[inter_features])\n    X1= pd.DataFrame(inter_cols,columns= poly.get_feature_names(list(inter_features)),index=X.index)                            \n    X_r = pd.concat([X1,X_r.drop(columns=inter_features)],axis=1)\n    \n    X_train = X_r[X_r.Test==0].drop(columns=drop_cols).copy()\n    \n    y_train = X.loc[X_r.Test==0,target_col]\n    #print(X_train.columns)\n    model.fit(X_train,y_train)\n    X[\"Pred\"] = np.maximum(model.predict(X_r.drop(columns=drop_cols)),0)\n    X[\"Res\"]= X.Pred-X[target_col]\n    score = (-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))**0.5\n    \n    return X, score\n\n\nmodel =Ridge(alpha=0.0065,random_state=35591,max_iter=10000,fit_intercept=True,normalize=True)\n#model =Ridge(alpha=0.01,random_state=35591,max_iter=10000,fit_intercept=True,normalize=True)\n\n\ngraph = []\n\n# create separate predictions for the public and private leaderboard (i.e. remove values from 1/4 from public submission)\n\nlast_train = \"2020-05-10\"    \nX_full.loc[(X_full.Date > \"2020-04-26\") & (X_full.Date <= last_train),\"Value\"]=train.loc[\n    (train.Date > \"2020-04-26\") & (train.Date <= last_train),\"Value\"].values\n\nX_full[\"DayYear\"]=X_full.Date.dt.dayofyear\n######## Data adjustments for Ecuador\n\nX_full.loc[(X_full.Type==\"ConfirmedCases\") & (X_full.Ctry==\"Ecuador\") & X_full.DayYear.isin([115,116,117]),\"Value\"] = [2000,2000,2000]\nX_full.loc[(X_full.Type==\"Fatalities\") & (X_full.Ctry==\"Ecuador\") & X_full.DayYear.isin([115,116,117]),\"Value\"] = [100,100,100]\n\n# reset sample sub\n\nmysub=sample_sub.set_index('Id')\naddsub=None\n\nfor loop in [\"Public\",\"Private\"]:\n    \n    if loop == \"Public\":\n        start=116 # last day of train data \n        val_end=117+14 # end date for validation data (public submission only )\n        sub_start=118\n        sub_end=118+13\n        startint= 107 # fit more recent data only - this is a key parameter\n    \n        X_full.loc[:,\"Test\"] = (X_full.Date > \"2020-04-26\") *1\n        \n    else:\n        start=117+14\n        sub_start=118+14\n        sub_end=999\n        X_full.loc[:,\"Test\"] = (X_full.Date > \"2020-05-10\") *1 \n        val_end=117+14\n        startint= 107 # fit more recent data only - this is a key parameter\n    \n    \n    \n    \n    \n    for cl in set({\"New\"}): #set(X_full.Cluster):\n        X_all= X_full[X_full.Ctry!=\"US1\"].reset_index()\n        cum_cases_map= X_all[[\"Type\",\"Value\",\"Reg\",\"Date\"]].groupby([\"Type\",\"Reg\",\"Date\"]).sum().groupby(level=1).cumsum().to_dict()[\"Value\"]\n        X_all[\"TotalCases\"]= X_all.apply(lambda x: cum_cases_map[x[\"Type\"],x[\"Reg\"],x[\"Date\"]],axis=1)\n        X_reg= X_all[~((X_all.Test ==0) & (((X_all.TotalCases < np.maximum(X_all.Pop*0.00001,3)) & (X_all.Type==\"ConfirmedCases\")) | \n                       ((X_all.TotalCases <10) & (X_all.Type==\"Fatalities\"))))].copy()\n\n        X_reg[\"Date\"]= pd.to_datetime(X_reg.Date,infer_datetime_format=True)\n        first_p_map= X_reg.loc[X_reg.Type==\"ConfirmedCases\",[\"Reg\",\"Date\"]].groupby(\"Reg\").min().to_dict()[\"Date\"]\n        X_reg[\"FirstDate\"]=X_reg[\"Reg\"].map(first_p_map)\n\n        X_reg[\"Day\"]=(X_reg.Date-X_reg.FirstDate).dt.days\n        X_reg[\"Week\"]=X_reg.Day/7\n        \n        X_reg.Value= np.maximum(X_reg.Value,0)\n\n        X_reg[\"LogVal\"]= np.log(X_reg.Value+1)\n        X_reg[\"LogPop\"]= np.log(X_reg.Pop+1)\n\n        X_reg[\"Week1\"]=np.tanh((X_reg.Week)/10)\n        X_reg[\"Week2\"]=np.tanh((X_reg.Week)/10*3)\n        X_reg[\"Week3\"]=np.tanh((X_reg.Week)/10*5)\n        X_reg[\"Week4\"]=np.tanh((X_reg.Week-1)/10)\n        X_reg[\"Week5\"]=np.tanh((X_reg.Week-3)/10)\n        X_reg[\"Week6\"]=np.tanh((X_reg.Week-5)/10)\n        X_reg[\"Week7\"]=np.tanh((X_reg.Week-7)/10)\n        X_reg[\"Week9\"]=np.tanh((X_reg.Week-9)/10)\n        X_reg[\"Sub\"]=loop\n\n        X= X_reg[X_reg.DayYear >= startint].copy()\n        if cl[:2] != \"US\":\n            print(\"\\nLast day of year for train\",start,\": \",X.loc[X.DayYear== start,\"Date\"].min())\n            print(\"First day of year for test\",start+1,\": \",X.loc[X.Test == 1,\"Date\"].min())\n        \n        # fit regressmodel to log of cases to align with evaluation metric\n        if len(X[(X.Test==0) & (X.Type == \"ConfirmedCases\")]) > 0:\n            X_res, score = model_fit(model,X.loc[(X.Type == \"ConfirmedCases\")].copy(),\"LogVal\")\n            X.loc[(X.Type == \"ConfirmedCases\") ,\"Pred\"]=np.exp(X_res.Pred)-1\n            print(\"ConfirmedCases complete\")\n            X_res, score = model_fit(model,X.loc[(X.Type == \"Fatalities\")].copy(),\"LogVal\")\n            X.loc[(X.Type == \"Fatalities\") ,\"Pred\"]=np.exp(X_res.Pred)-1\n        else:\n            X[\"Pred\"]=0\n\n        # average last two observation to scale data\n        scalemap =X.loc[(X.Test==0) & (X.DayYear >= start-2),[\"Type\",\"Value\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).mean().to_dict()[\"Value\"]\n        scalepmap =X.loc[(X.Test==0) & (X.DayYear >= start-2),[\"Type\",\"Pred\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).mean().to_dict()[\"Pred\"]\n        scalemap.setdefault(\"Reg\",)\n        X.loc[X.Ctry!=\"US\",\"Pred\"]= X.loc[X.Ctry!=\"US\",\"Pred\"]* np.maximum(X.apply(lambda x: scalemap[x[\"Type\"],x[\"Reg\"]]\n                    if (x[\"Type\"],x[\"Reg\"]) in scalemap.keys() else 0,axis=1).fillna(0),1)/np.maximum(X.apply\n                    (lambda x: scalepmap[x[\"Type\"],x[\"Reg\"]] if (x[\"Type\"],x[\"Reg\"]) in scalepmap.keys() else 0,axis=1).fillna(0),1)\n        \n        print(\"Model complete\")\n        X[\"Res\"]=X.Value-X.Pred\n        X.loc[X.Test==1,\"Res\"]=0\n        # include first test day to have full data\n        stdresmap =X.loc[X.DayYear <=sub_start,[\"Type\",\"Res\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).std().to_dict()[\"Res\"]\n        meanmap =X.loc[X.DayYear <=sub_start,[\"Type\",\"Value\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).mean().to_dict()[\"Value\"]\n        X[\"Mean\"]= np.maximum(X.apply(lambda x: meanmap[x[\"Type\"],x[\"Reg\"]],axis=1),1)\n        stdmap =X.loc[X.DayYear <=sub_start,[\"Type\",\"Pred\",\"Reg\"]].groupby([\"Type\",\"Reg\"]).std().to_dict()[\"Pred\"]\n        X[\"Std\"]= np.maximum(X.apply(lambda x: stdmap[x[\"Type\"],x[\"Reg\"]],axis=1),1) \n        X[\"StdRes\"]= np.maximum(X.apply(lambda x: stdresmap[x[\"Type\"],x[\"Reg\"]],axis=1),1)\n          \n        X[\"Pred05\"]=np.maximum(X.Pred - (X.Std+X.StdRes)*2 /np.sqrt(X.Mean)*np.sqrt(X.Pred),0)\n        X[\"Pred95\"]=np.maximum(X.Pred + (X.Std+X.StdRes)*2 /np.sqrt(X.Mean)*np.sqrt(X.Pred),0)\n        \n        #print(\"\\nCV score: \",score,\"\\nMean: {:.4f} Std: {:.4f}\\n\".format(score.mean(), score.std()))\n\n        X.fillna(0,inplace=True)\n        \n        X[\"ResLV\"] = (X.Pred-X.Value)*X.Weight\n        X.loc[X.DayYear > val_end,\"ResLV\"]=0\n        \n        if loop == \"Public\": \n            #print(X.loc[(X.DayYear > start) & (X.DayYear <=val_end),[\"ResLV\",\"Pred\",\"Value\"]].describe())\n            print(cl,\": \",loop,\" submission - Score: \",np.sqrt(np.abs(X.loc[(X.DayYear >= sub_start) & (X.DayYear <=val_end),[\"ResLV\"]]).mean()))\n        X[\"Target\"]=X.Pred\n        X[\"Quantile\"]=0.5\n        X[\"IdSub\"]=X.Id.astype(str)+\"_0.5\"\n        addsub = pd.concat([addsub,X.loc[X.Test==1,[\"IdSub\",\"Target\",\"ResLV\",\"Quantile\",\"Type\",\"DayYear\",\"Ctry\",\"Prov\",\"Cty\",\"Sub\"]]])\n        X[\"IdSub\"]=X.Id.astype(str)+\"_0.05\"\n        X.Quantile=0.05\n        X.Target=X.Pred05\n        X.ResLV=(X.Pred05-X.Value)*X.Weight\n        addsub = pd.concat([addsub,X.loc[X.Test==1,[\"IdSub\",\"Target\",\"ResLV\",\"Quantile\",\"Type\",\"DayYear\",\"Ctry\",\"Prov\",\"Cty\",\"Sub\"]]])\n        X[\"IdSub\"]=X.Id.astype(str)+\"_0.95\"\n        X.Quantile=0.95\n        X.Target=X.Pred95\n        X.ResLV=(X.Pred95-X.Value)*X.Weight\n        addsub = pd.concat([addsub,X.loc[X.Test==1,[\"IdSub\",\"Target\",\"ResLV\",\"Quantile\",\"Type\",\"DayYear\",\"Ctry\",\"Prov\",\"Cty\",\"Sub\"]]])\n        addsub.fillna(0,inplace=True)\n                      \n    addsub[\"Err\"]= (addsub.ResLV > 0)*(1-addsub.Quantile)*(addsub.ResLV)+(addsub.ResLV <= 0)*(addsub.Quantile)*(-addsub.ResLV) \n    print (\"Final Score\",addsub.loc[addsub.DayYear <= val_end,\"Err\"].mean())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(addsub[(addsub.Ctry==\"Austria\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"Austria\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"United Kingdom\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"United Kingdom\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"Russia\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Ctry==\"Russia\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"addsub.loc[(addsub.DayYear > 117+14) & (addsub.Ctry == \"US\"),[\"Target\",\"Type\",\"Sub\",\"Quantile\",\"Prov\"]].groupby([\"Prov\",\"Type\",\"Quantile\",\"Sub\"]).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(addsub[(addsub.Cty==\"New York\") & (addsub.Type == \"ConfirmedCases\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()\nfig = px.scatter(addsub[(addsub.Cty==\"New York\") & (addsub.Type == \"Fatalities\")], x='DayYear', y='Target', color=\"Sub\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"addsub[addsub.Sub==\"Private\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"addsub[addsub.Sub==\"Public\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mysub=sample_sub.set_index('Id')\nfinalsub = addsub[((addsub.Sub==\"Public\") & (addsub.DayYear < 132)) | ((addsub.Sub==\"Private\") & (addsub.DayYear >= 132))]\nmysub= mysub.merge(finalsub[[\"IdSub\",\"Target\"]],left_index=True,right_on=\"IdSub\",how=\"left\")\nmysub.drop(columns=[\"Value\"],inplace=True)\nmysub.columns=[\"ForecastId_Quantile\",\"TargetValue\"]\nmysub.fillna(0,inplace=True)\nmysub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mysub.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mysub.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(300)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}