{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data manipulation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\n\n# Listing files\nDIR = '/kaggle/input/covid19-global-forecasting-week-5/'\nimport os\nfor dirname, _, filenames in os.walk(DIR):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data\ndf_train   = pd.read_csv(DIR + 'train.csv')\ndf_test    = pd.read_csv(DIR + 'test.csv')\nsubmission = pd.read_csv(DIR + 'submission.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making Date become a number"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\ndf_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\nmin_timestamp = np.min(df_train['Date'])\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\ndf_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\ndf_train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Merging Location Info"},{"metadata":{"trusted":true},"cell_type":"code","source":"combine_columns = ['Country_Region', 'Province_State', 'County']\ndf_train.loc[:, combine_columns] = df_train.loc[:, combine_columns].fillna(\"\")\ndf_train['Location'] = df_train[combine_columns].apply(lambda x: '_'.join(x), axis=1)\ndf_test.loc[:, combine_columns] = df_test.loc[:, combine_columns].fillna(\"\")\ndf_test['Location'] = df_train[combine_columns].apply(lambda x: '_'.join(x), axis=1)\ndf_train.drop(columns=combine_columns, inplace=True)\ndf_test.drop(columns=combine_columns, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting into ConfirmedCases and Fatalities sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_cols = ['Population', 'Date', 'Location', 'TargetValue'] # Keeping Location and TargetValue for values handling\n\nX_train_cc = df_train.loc[df_train['Target']=='ConfirmedCases', features_cols]\nX_test_cc  = df_test.loc[df_test['Target']=='ConfirmedCases', ['Population', 'Date', 'Location']]\nX_train_ft = df_train.loc[df_train['Target']=='Fatalities', features_cols]\nX_test_ft  = df_test.loc[df_test['Target']=='Fatalities', ['Population', 'Date', 'Location']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fixing some weird values\n\nIn a certain location, confirmed cases and fatalities should not decrease nor be negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"locs = list(set(df_train['Location']))\n\ndef handle_weird_stuff (v):\n    v[v<0]=0\n    for i in range(1, len(v)):\n        if v[i] < v[i-1]:\n            v[i] = v[i-1]\n    return v\n\nfor location in tqdm(locs):\n    X_train_cc.loc[X_train_cc['Location'] == location, 'TargetValue'] = handle_weird_stuff(X_train_cc.loc[X_train_cc['Location'] == location, 'TargetValue'].values)\n    X_train_ft.loc[X_train_ft['Location'] == location, 'TargetValue'] = handle_weird_stuff(X_train_ft.loc[X_train_ft['Location'] == location, 'TargetValue'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finally getting target"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\ny_train_cc = X_train_cc['TargetValue'].values\ny_train_ft = X_train_ft['TargetValue'].values\n\nX_train_cc.drop(columns=['Location', 'TargetValue'], inplace=True)\nX_test_cc.drop(columns=['Location'], inplace=True)\nX_train_ft.drop(columns=['Location', 'TargetValue'], inplace=True)\nX_test_ft.drop(columns=['Location'], inplace=True)\n\nX_train_cc = X_train_cc.values\nX_train_ft = X_train_ft.values\nX_test_cc  = X_test_cc.values\nX_test_ft  = X_test_ft.values\n\ncc_scale_factor = 1.5 * np.max(y_train_cc)\nft_scale_factor = 1.5 * np.max(y_train_ft)\ncc_scaler = MinMaxScaler()\nft_scaler = MinMaxScaler()\n\nX_train_cc = cc_scaler.fit_transform(X_train_cc)\nX_test_cc  = cc_scaler.transform(X_test_cc)\nX_train_ft = ft_scaler.fit_transform(X_train_ft)\nX_test_ft  = ft_scaler.transform(X_test_ft)\ny_train_cc /= cc_scale_factor\ny_train_ft /= ft_scale_factor\n\nX_cc_train, X_cc_valid, y_cc_train, y_cc_valid = train_test_split(X_train_cc, y_train_cc, test_size=0.2, random_state=42)\nX_ft_train, X_ft_valid, y_ft_train, y_ft_valid = train_test_split(X_train_ft, y_train_ft, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom keras.callbacks import EarlyStopping\nfrom functools import partial\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import mean_absolute_error\n\nclass CustomNN ():\n\n    def __init__ (self, layers_list, loss='mse', optimizer='adam', epochs=5000, patience=25, batch_size=256, verbose=True, callbacks=[]):\n        self.__verbose = verbose\n        self.__model = Sequential()\n        for layer in layers_list:\n            self.__model.add(layer)\n        self.__model.compile(optimizer, loss=loss)\n        self.__epochs = epochs\n        self.__batch_size = batch_size\n        self.__es = EarlyStopping(monitor='loss', mode='min', verbose=self.__verbose)\n        self.__callbacks = []\n        self.__callbacks.append(self.__es)\n        self.__loss = loss\n        self.__optimizer = optimizer\n\n    def fit (self, X_train, y_train):\n        self.__history = self.__model.fit (\n            X_train,\n            y_train,\n            epochs = self.__epochs,\n            batch_size = self.__batch_size,\n            verbose = self.__verbose,\n            callbacks = self.__callbacks,\n        )\n        return self\n\n    def predict (self, X_test):\n        return self.__model.predict(X_test)\n\n    def save_weights (self, fname):\n        self.__model.save_weights(fname)\n\n    def load_weights (self, fname):\n        self.__model.load_weights(fname)\n        self.__model.compile(self.__optimizer, loss=self.__loss)\n\n    def __str__ (self):\n        return self.__model.summary()\n\n    def __repr__ (self):\n        return self.__str__()\n\n    @property\n    def model (self):\n        return self.__model\n\ndef pinball_loss(y_true, y_pred, tau=0.1, *args, **kwargs):\n    err = y_true - y_pred\n    return K.mean(K.maximum(tau * err, (tau - 1) * err), axis=-1)\n\ndef plot_learning_curves_nn (model, X_train, y_train, X_valid, y_valid):\n    scores_train = []\n    scores_val = []\n    model.save_weights('model.h5')\n    this_range = np.arange(1, len(X_train), len(X_train)/10)\n    for i in tqdm(this_range):\n        i = int(i)\n        model.load_weights('model.h5')\n        model.fit(X_train[:i], y_train[:i])\n        p_train = model.predict(X_train[:i])\n        p_val = model.predict(X_valid)\n        scores_train.append(mean_absolute_error(y_train[:i], p_train))\n        scores_val.append(mean_absolute_error(y_valid, p_val))\n    plt.figure(figsize=(16,10))\n    plt.title(\"Learning Curves\")\n    plt.scatter(this_range, scores_train)\n    plt.scatter(this_range, scores_val)\n    plt.legend([\"Train\", \"Validation\"])\n    plt.xlabel(\"# Samples\")\n    plt.ylabel(\"SP\")\n    plt.show()\n\ninitial_cc_model_config = [\n    Input(shape=(X_train_cc.shape[1],)),\n    Dense(3, activation='linear'),\n    Activation('sigmoid'),\n    Dense(1, activation='linear'),\n    Activation('sigmoid')\n]\n\ncc_model = CustomNN(initial_cc_model_config, loss=pinball_loss, verbose=False)\nplot_learning_curves_nn(cc_model, X_cc_train, y_cc_train, X_cc_valid, y_cc_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\ndf_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)\n#df_intersection = df_test[df_test['Date'] <= np.max(df_train['Date'])]\n#df_intersection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Following the idea at\n# https://www.kaggle.com/ranjithks/25-lines-of-code-results-better-score#Fill-NaN-from-State-feature\n# Filling NaN states with the Country\n\nEMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state\n\ndef replaceGeorgiaState (state, country):\n    if (state == 'Georgia') and (country == 'US'):\n        return 'Georgia_State'\n    else:\n        return state\n\ndf_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ndf_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\ndf_test['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ndf_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\n# df_intersection['Province_State'].fillna(EMPTY_VAL, inplace=True)\n# df_intersection['Province_State'] = df_intersection.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\n# df_intersection['Province_State'] = df_intersection.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\n\ndf_train[df_train['Province_State'] == 'Georgia_State']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making Date become timestamp\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\n# df_intersection['Date'] = df_intersection['Date'].apply(lambda s: time.mktime(s.timetuple()))\n\nmin_timestamp = np.min(df_train['Date'])\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\ndf_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# df_intersection['Date'] = df_intersection['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Country_Region']=='Brazil']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding validation data into the Intersection DF\nstates = sorted(set(df_intersection['Province_State']))\ndf_intersection['ConfirmedCases'] = float('NaN')\ndf_intersection['Fatalities'] = float('NaN')\n\nfor state in states:\n    dates = sorted(set(df_intersection[df_intersection['Province_State'] == state]['Date']))\n    min_date = np.min(dates)\n    max_date = np.max(dates)\n    idx = df_intersection[df_intersection['Province_State'] == state].index\n    values = df_train[(df_train['Province_State'] == state) & (df_train['Date'] >= min_date) & (df_train['Date'] <= max_date)][['ConfirmedCases', 'Fatalities']].values\n    values = pd.DataFrame(values, index = list(idx), columns=['ConfirmedCases', 'Fatalities'])\n    df_intersection['ConfirmedCases'].loc[idx] = values['ConfirmedCases']\n    df_intersection['Fatalities'].loc[idx] = values['Fatalities']\ndf_intersection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filtering data for public leaderboard\ndf_train = df_train[df_train['Date'] < np.min(df_test['Date'])]\n# Check if any Province_State value on test dataset isn't on train dataset\n# If nothing prints, everything is okay\nfor a in set(df_test['Province_State']):\n    if a not in set(df_train['Province_State']):\n        print (a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating features based on evolution of COVID-19\n# Idea from https://www.kaggle.com/binhlc/sars-cov-2-exponential-model-week-2\nprint (\"Generating features on evolution of COVID-19\")\nfrom tqdm import tqdm\nevolution = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000]\ndef generateFeatures (state):\n    should_filter = False\n    train = df_train[df_train['Province_State'] == state].drop(columns=['Id'])\n    test  = df_test[df_test['Province_State'] == state].drop(columns=['ForecastId'])\n    y_cases = train['ConfirmedCases']\n    y_fatal = train['Fatalities']\n    for evo_type in ['ConfirmedCases', 'Fatalities']:\n        for value in evolution:\n            min_day = train[train[evo_type] >= value]['Date']\n            if min_day.count() > 0:\n                min_day = np.min(min_day)\n                should_filter = True\n            else:\n                #print (\"{} -> Not found min_day for {} {}\".format(state, evo_type, value))\n                continue\n            train['{}_{}'.format(evo_type, value)] = train['Date'].apply(lambda x: x - min_day)\n            test ['{}_{}'.format(evo_type, value)] = test ['Date'].apply(lambda x: x - min_day)\n    train.drop(columns=['ConfirmedCases', 'Fatalities', 'Province_State', 'Country_Region'], inplace=True)\n    test.drop(columns=['Province_State', 'Country_Region'], inplace=True)\n    if should_filter:\n        idx     = train[train['ConfirmedCases_1'] >= 0].index\n        train   = train.loc[idx]\n        y_cases = y_cases.loc[idx]\n        y_fatal = y_fatal.loc[idx]\n    return train, test, y_cases, y_fatal\n\ndataframes = {}\nstates = sorted(set(df_train['Province_State']))\nfor state in tqdm(states):\n    dataframes[state] = {}\n    train, test, y_cases, y_fatal = generateFeatures(state)\n    dataframes[state]['train']   = train\n    dataframes[state]['test']    = test\n    dataframes[state]['y_cases'] = y_cases\n    dataframes[state]['y_fatal'] = y_fatal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Checking shapes\n# state = 'Georgia'\n# print (dataframes[state]['train'].shape, dataframes[state]['test'].shape, dataframes[state]['y_cases'].shape, dataframes[state]['y_fatal'].shape)\n# dataframes[state]['test'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling and predicting for competition"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, Lasso\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.base import TransformerMixin\nfrom sklearn.datasets import make_regression\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble.weight_boosting import AdaBoostRegressor\nfrom sklearn.linear_model.base import LinearRegression\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveRegressor\nfrom sklearn.linear_model.theil_sen import TheilSenRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\n\ndef handle_predictions (predictions, lowest = 0):\n    #predictions = np.round(predictions, 0)\n    # Predictions can't be negative\n    predictions[predictions < 0] = 0\n    # Predictions can't decrease from greatest value on train dataset\n    # predictions[predictions < lowest] = lowest\n    # Predictions can't decrease over time\n    for i in range(1, len(predictions)):\n        if predictions[i] < predictions[i - 1]:\n            predictions[i] = predictions[i - 1]\n    #return predictions.astype(int)\n    return predictions\n\ndef fillSubmission (state, column, values,):\n    idx = df_test[df_test['Province_State'] == state].index\n    values = pd.DataFrame(np.array(values), index = list(idx), columns=[column])\n    submission[column].loc[idx] = values[column]\n    return submission\n\ndef avg_rmsle():\n    idx = df_intersection.index\n    my_sub = submission.loc[idx][['ConfirmedCases', 'Fatalities']]\n    cases_pred = my_sub['ConfirmedCases'].values\n    fatal_pred = my_sub['Fatalities'].values\n    pred = np.append(cases_pred, fatal_pred)\n    cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n    fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n    targ = np.append(cases_targ, fatal_targ)\n    score = np.sqrt(mean_squared_log_error( targ, pred ))\n    return score\n\ndef make_combinations (iterable):\n    from itertools import combinations\n    my_combs = []\n    for item in iterable.copy():\n        iterable.remove(item)\n        for i in range(len(iterable)):\n            for comb in combinations(iterable, i+1):\n                my_combs.append((item, comb))\n        iterable.append(item)\n    return my_combs\n\ntest_models = [\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), LinearRegression()),                  # 0.43248400978264234\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), LinearRegression()),                  # --> 0.41772679149716213\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), LinearRegression()),                # 0.4334087200925956\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), LinearRegression()),                  # 0.4401684177017516\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), LinearRegression()),                    # 0.5073105363889515\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), LinearRegression()),           # 0.5436308167750011\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), LinearRegression()),              # 3.723951842476838\n\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), TheilSenRegressor()),                  # 0.429714197718972\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), TheilSenRegressor()),                  # --> 0.416881016597718\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), TheilSenRegressor()),                # 0.46545608570380087\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), TheilSenRegressor()),                  # 0.46325130998855407\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), TheilSenRegressor()),                    # 0.5066829122736244\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), TheilSenRegressor()),           # 0.5436262279295473\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), TheilSenRegressor()),              # 3.7357046570144115\n\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), BayesianRidge()),                  # 0.41814599178288325\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), BayesianRidge()),                  # 0.41277237195607513\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), BayesianRidge()),                # 0.41564703233710143\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), BayesianRidge()),                  # 0.4152229761235273\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), BayesianRidge()),                    # --> 0.3906797240096884\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), BayesianRidge()),           # 0.5453994559085396\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), BayesianRidge()),              # 0.4807433364820885\n\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), Lasso()),                  # 0.4863083627388429\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), Lasso()),                  # 0.47408909074033034\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), Lasso()),                # 0.4509150256440627\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), Lasso()),                  # 0.4732912500189848\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), Lasso()),                    # 0.53992733050457\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), Lasso()),           # 0.5509175786196774\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), Lasso()),              # --> 0.3916916463210968\n\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), LGBMRegressor()),                  # 0.5512175074492168\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), LGBMRegressor()),                  # --> 0.5512154967563805\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), LGBMRegressor()),                # 0.5512174701464707\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), LGBMRegressor()),                  # 0.5512174701464707\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), LGBMRegressor()),                    # 0.5512174651618788\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), LGBMRegressor()),           # 0.5512175074492168\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), LGBMRegressor()),              # 0.5512174729007725\n\n#     make_pipeline(MinMaxScaler(), PolynomialFeatures(2), XGBRegressor()),                  # 0.5512174419704444\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), XGBRegressor()),                  # --> 0.5512173836794546\n#     make_pipeline(StandardScaler(), PolynomialFeatures(2), XGBRegressor()),                # 0.5512174424297206\n#     make_pipeline(RobustScaler(), PolynomialFeatures(2), XGBRegressor()),                  # 0.5512174424297206\n#     make_pipeline(Normalizer(), PolynomialFeatures(2), XGBRegressor()),                    # 0.5512174325463026\n#     make_pipeline(QuantileTransformer(), PolynomialFeatures(2), XGBRegressor()),           # 0.5512174419704444\n#     make_pipeline(PowerTransformer(), PolynomialFeatures(2), XGBRegressor()),              # 0.5512174424209609\n\n]\n\nfor model in test_models:\n    print (' * Model: {}'.format(model))\n    for state in tqdm(states):\n        try:\n            train   = dataframes[state]['train']\n            test    = dataframes[state]['test']\n            y_cases = dataframes[state]['y_cases']\n            y_fatal = dataframes[state]['y_fatal']\n            model.fit(train, y_cases)\n            cases = model.predict(test)\n            lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n            cases = handle_predictions(cases, lowest_pred)\n            submission = fillSubmission (state, 'ConfirmedCases', cases)\n            model.fit(train, y_fatal)\n            fatal = model.predict(test)\n            lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n            fatal = handle_predictions(fatal, lowest_pred)\n            submission = fillSubmission (state, 'Fatalities', fatal)\n        except:\n            print (\"Model {}\\n failed to predict country {}. Will continue...\".format(model, state))\n    print ('   - Score: {}'.format(avg_rmsle()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomEnsemble (BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models, meta_model, scaler=MaxAbsScaler(), feature_generator=None):\n        self.models = models\n        if scaler:\n            if feature_generator:\n                self.meta_model = make_pipeline(scaler, feature_generator, meta_model)\n            else:\n                self.meta_model = make_pipeline(scaler, meta_model)\n        else:\n            if feature_generator:\n                self.meta_model = make_pipeline(feature_generator, meta_model)\n            else:\n                self.meta_model = meta_model\n    def fit(self,X,y):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            model.fit (X, y)\n            predictions[:,i] = model.predict(X)\n        self.meta_model.fit(predictions, y)\n    def predict(self,X):\n        predictions = np.zeros((X.shape[0], len(self.models)))\n        for i, model in enumerate(self.models):\n            predictions[:,i] = model.predict(X)\n        return self.meta_model.predict(predictions)\n    def __str__ (self):\n        return \"<CustomEnsemble (meta={}, models={})>\".format(self.meta_model, self.models)\n    def __repr__ (self):\n        return self.__str__()\n    \ntest_models = [\n    make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), LinearRegression()),       # --> 0.41772679149716213\n#     make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), TheilSenRegressor()),      # --> 0.416881016597718\n    make_pipeline(Normalizer(), PolynomialFeatures(2), BayesianRidge()),            # --> 0.3906797240096884\n    make_pipeline(PowerTransformer(), PolynomialFeatures(2), Lasso()),              # --> 0.3916916463210968\n    make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), LGBMRegressor()),          # --> 0.5512154967563805\n    make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), XGBRegressor()),           # --> 0.5512173836794546\n]\n\n# Version 6 best model. Data will be used to test few more models\n# <CustomEnsemble (meta=Pipeline(memory=None,\n#          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),\n#                 ('pipeline',\n#                  Pipeline(memory=None,\n#                           steps=[('maxabsscaler', MaxAbsScaler(copy=True)),\n#                                  ('polynomialfeatures',\n#                                   PolynomialFeatures(degree=2,\n#                                                      include_bias=True,\n#                                                      interaction_only=False,\n#                                                      order='C')),\n#                                  ('linearregression',\n#                                   LinearRegression(copy_X=True,\n#                                                    fit_intercept=True,\n#                                                    n_jobs=None,\n#                                                    normalize=False))],\n#                           verbose=False))],\n#          verbose=False), models=[Pipeline(memory=None,\n#          steps=[('normalizer', Normalizer(copy=True, norm='l2')),\n#                 ('polynomialfeatures',\n#                  PolynomialFeatures(degree=2, include_bias=True,\n#                                     interaction_only=False, order='C')),\n#                 ('bayesianridge',\n#                  BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n#                                compute_score=False, copy_X=True,\n#                                fit_intercept=True, lambda_1=1e-06,\n#                                lambda_2=1e-06, lambda_init=None, n_iter=300,\n#                                normalize=False, tol=0.001, verbose=False))],\n#          verbose=False)])>\n\n# Version 7 best model. It's the same as version 6.\n# <CustomEnsemble (meta=Pipeline(memory=None,\n#          steps=[('maxabsscaler', MaxAbsScaler(copy=True)),\n#                 ('pipeline',\n#                  Pipeline(memory=None,\n#                           steps=[('maxabsscaler', MaxAbsScaler(copy=True)),\n#                                  ('polynomialfeatures',\n#                                   PolynomialFeatures(degree=2,\n#                                                      include_bias=True,\n#                                                      interaction_only=False,\n#                                                      order='C')),\n#                                  ('linearregression',\n#                                   LinearRegression(copy_X=True,\n#                                                    fit_intercept=True,\n#                                                    n_jobs=None,\n#                                                    normalize=False))],\n#                           verbose=False))],\n#          verbose=False), models=[Pipeline(memory=None,\n#          steps=[('normalizer', Normalizer(copy=True, norm='l2')),\n#                 ('polynomialfeatures',\n#                  PolynomialFeatures(degree=2, include_bias=True,\n#                                     interaction_only=False, order='C')),\n#                 ('bayesianridge',\n#                  BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None,\n#                                compute_score=False, copy_X=True,\n#                                fit_intercept=True, lambda_1=1e-06,\n#                                lambda_2=1e-06, lambda_init=None, n_iter=300,\n#                                normalize=False, tol=0.001, verbose=False))],\n#          verbose=False)])>\n\nmy_combs = make_combinations(test_models)\nprint (\"I tested {} models =)\".format(len(my_combs)))\nbest = 10000\nresults = []\nwith tqdm(total = len(my_combs) * len(states)) as pbar:\n    for comb in my_combs:\n        try:\n            for state in states:\n                train   = dataframes[state]['train']\n                test    = dataframes[state]['test']\n                y_cases = dataframes[state]['y_cases']\n                y_fatal = dataframes[state]['y_fatal']\n                model = CustomEnsemble(list(comb[1]), comb[0])\n                model.fit(train, y_cases)\n                cases = model.predict(test)\n                lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n                cases = handle_predictions(cases, lowest_pred)\n                submission = fillSubmission (state, 'ConfirmedCases', cases)\n                model.fit(train, y_fatal)\n                fatal = model.predict(test)\n                lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n                fatal = handle_predictions(fatal, lowest_pred)\n                submission = fillSubmission (state, 'Fatalities', fatal)\n                pbar.update(1)\n            score = avg_rmsle()\n            results.append(score)\n            if (score < best):\n                print (\"Score {:.4f} is better than previous best. Saving...\".format(score))\n                best = score\n                best_model = model\n        except:\n            print(\"Model {}\\nfailed. Will continue now...\".format(model))\n# best_model = CustomEnsemble(\n#     meta_model = make_pipeline(MaxAbsScaler(), PolynomialFeatures(2), LinearRegression()),\n#     models = [\n#         make_pipeline(Normalizer(), PolynomialFeatures(2), BayesianRidge())\n#     ]\n# )\n# best = 0.4330928705901224\n            \nprint (\"And the best model goes to...\")\nprint (best_model)\nprint (\"with score {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predicitons using the best model for the private leaderboard\nprint (\"Reloading data and making predictions...\")\n# Load raw train\ndf_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n# Handle it\ndf_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\nEMPTY_VAL = \"EMPTY_VAL\"\ndf_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# Re-generate features and predict\ndataframes = {}\nstates = sorted(set(df_train['Province_State']))\nfor state in tqdm(states):\n    dataframes[state] = {}\n    train, test, y_cases, y_fatal = generateFeatures(state)\n    model = best_model\n    model.fit(train, y_cases)\n    cases = model.predict(test)\n    lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n    cases = handle_predictions(cases, lowest_pred)\n    submission = fillSubmission (state, 'ConfirmedCases', cases)\n    model.fit(train, y_fatal)\n    fatal = model.predict(test)\n    lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n    fatal = handle_predictions(fatal, lowest_pred)\n    submission = fillSubmission (state, 'Fatalities', fatal)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sanity check with random samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime, timedelta\n\ndef checkState (state):\n    idx = df_test[df_test['Province_State'] == state].index\n    return submission.loc[idx]\n\ndef plotStatus (states):\n    if type(states) == list:\n        for state in states:\n            initial_date = datetime (2020, 1, 22)\n            df = df_train[df_train['Province_State'] == state]\n            dates_train = sorted(list(set(df['Date'])))\n            dates_test  = sorted(list(set(df_test['Date'])))\n            for i in range(len(dates_train)):\n                dates_train[i] = initial_date + timedelta(days=dates_train[i])\n            for i in range(len(dates_test)):\n                dates_test[i] = initial_date + timedelta(days=dates_test[i])\n            idx = df_test[df_test['Province_State'] == state].index\n            plt.figure(figsize=(14,8))\n            plt.title('COVID-19 cases on {}'.format(state))\n            plt.xlabel('Date')\n            plt.ylabel('Number')\n            plt.plot(dates_train, df['ConfirmedCases'], linewidth=2, color='#ff9933')\n            plt.plot(dates_test , submission['ConfirmedCases'].loc[idx], linewidth=2, color='#e67300', linestyle='dashed')\n            legend = []\n            legend.append('{} confirmed cases'.format(state))\n            legend.append('{} predicted cases'.format(state))\n            plt.legend(legend)\n            plt.show()\n            plt.figure(figsize=(14,8))\n            plt.title('COVID-19 fatalities on {}'.format(state))\n            plt.xlabel('Date')\n            plt.ylabel('Number')\n            plt.plot(dates_train, df['Fatalities'], linewidth=2, color='#ff9933')\n            plt.plot(dates_test , submission['Fatalities'].loc[idx], linewidth=2, color='#e67300', linestyle='dashed')\n            legend = []\n            legend.append('{} fatalities'.format(state))\n            legend.append('{} predicted fatalities'.format(state))\n            plt.legend(legend)\n            plt.show()\n    else:\n        print (\"Please send me a list\")\n\nraw_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\nraw_test  = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\nraw_train['Date'] = pd.to_datetime(raw_train['Date'], infer_datetime_format=True)\nraw_test['Date']  = pd.to_datetime(raw_test['Date'], infer_datetime_format=True)\n\ndef rmsle (state):\n    idx = df_intersection[df_intersection['Province_State'] == state].index\n    my_sub = submission.loc[idx][['ConfirmedCases', 'Fatalities']]\n    cases_pred = my_sub['ConfirmedCases'].values\n    fatal_pred = my_sub['Fatalities'].values\n    cases_targ = df_intersection.loc[idx]['ConfirmedCases'].values\n    fatal_targ = df_intersection.loc[idx]['Fatalities'].values\n    cases = np.sqrt(mean_squared_log_error( cases_targ, cases_pred ))\n    fatal = np.sqrt(mean_squared_log_error( fatal_targ, fatal_pred ))\n    return cases, fatal\n\nsamples = list(df_train['Province_State'].sample(n=1))\nsamples.append('Brazil')\nplotStatus(samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sanity check with global data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotGlobalStatus ():\n    legend = []\n    initial_date = datetime (2020, 1, 22)\n    df = df_train.groupby('Date').sum()\n    df['Date'] = df.index\n    test = df_test\n    test['ConfirmedCases'] = submission['ConfirmedCases']\n    test['Fatalities'] = submission['Fatalities']\n    test = test.groupby('Date').sum()\n    test['Date'] = test.index\n    dates_train = sorted(list(set(df['Date'])))\n    dates_test  = sorted(list(set(test['Date'])))\n    for i in range(len(dates_train)):\n        dates_train[i] = initial_date + timedelta(days=dates_train[i])\n    for i in range(len(dates_test)):\n        dates_test[i] = initial_date + timedelta(days=dates_test[i])\n    plt.figure(figsize=(14,8))\n    plt.title('Global COVID-19 cases')\n    plt.xlabel('Date')\n    plt.ylabel('Number')\n    plt.plot(dates_train, df['ConfirmedCases'], linewidth=2, color='#ff9933')\n    plt.plot(dates_test , test['ConfirmedCases'], linewidth=2, color='#e67300', linestyle='dashed')\n    legend.append('{} confirmed cases'.format('World'))\n    legend.append('{} predicted cases'.format('World'))\n    plt.legend(legend)\n    plt.show()\n    plt.figure(figsize=(14,8))\n    plt.title('Global COVID-19 fatalities')\n    plt.xlabel('Date')\n    plt.ylabel('Number')\n    plt.plot(dates_train, df['Fatalities'], linewidth=2, color='#ff9933')\n    plt.plot(dates_test , test['Fatalities'], linewidth=2, color='#e67300', linestyle='dashed')\n    legend = []\n    legend.append('{} fatalities'.format('World'))\n    legend.append('{} predicted fatalities'.format('World'))\n    plt.legend(legend)\n    plt.show()\n\nplotGlobalStatus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting for the future - when will this end?\n\nI know this certainly is not accurate, but just wanted to have an idea of when and how this ends, based on the model I chose."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train   = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\ndf_test    = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\ndf_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\ndf_test['Date'] = pd.to_datetime(df_test['Date'], infer_datetime_format=True)\ndf_test['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ndf_test['Province_State'] = df_test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : replaceGeorgiaState(x['Province_State'], x['Country_Region']), axis=1)\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_test['Date'] = df_test['Date'].apply(lambda s: time.mktime(s.timetuple()))\nmin_timestamp = np.min(df_train['Date'])\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\ndf_test['Date'] = df_test['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\nmax_date = 113\ndays_to_add = 200\nwith tqdm(total = days_to_add * len(states)) as pbar:\n    for state in states:\n        for i in range(1, days_to_add + 1):\n            row_df = pd.DataFrame([[-1, state, state, max_date + i]], columns = ['ForecastId', 'Country_Region', 'Province_State', 'Date'])\n            df_test = pd.concat([df_test, row_df], ignore_index=True)\n            pbar.update(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['ConfirmedCases'] = -1\ndf_test['Fatalities'] = -1\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predicitons using the best model for the private leaderboard\nprint (\"Reloading data and making predictions...\")\n# Load raw train\ndf_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n# Handle it\ndf_train['Date'] = pd.to_datetime(df_train['Date'], infer_datetime_format=True)\nEMPTY_VAL = \"EMPTY_VAL\"\ndf_train['Province_State'].fillna(EMPTY_VAL, inplace=True)\ndf_train['Province_State'] = df_train.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ndf_train['Date'] = df_train['Date'].apply(lambda s: time.mktime(s.timetuple()))\ndf_train['Date'] = df_train['Date'].apply(lambda s: (s - min_timestamp) / 86400.0)\n# Re-generate features and predict\ndataframes = {}\nstates = sorted(set(df_train['Province_State']))\nfor state in tqdm(states):\n    idx = df_test[df_test['Province_State'] == state].index\n    dataframes[state] = {}\n    train, test, y_cases, y_fatal = generateFeatures(state)\n    model = best_model\n    model.fit(train, y_cases)\n    test.drop(columns=['ConfirmedCases', 'Fatalities'], inplace=True)\n    cases = model.predict(test)\n    lowest_pred = np.max(df_train[df_train['Province_State'] == state]['ConfirmedCases'].values)\n    cases = handle_predictions(cases, lowest_pred)\n    df_test.loc[idx, 'ConfirmedCases'] = cases\n    model.fit(train, y_fatal)\n    fatal = model.predict(test)\n    lowest_pred = np.max(df_train[df_train['Province_State'] == state]['Fatalities'].values)\n    fatal = handle_predictions(fatal, lowest_pred)\n    df_test.loc[idx, 'Fatalities'] = fatal\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotStatus (states):\n    if type(states) == list:\n        for state in states:\n            initial_date = datetime (2020, 1, 22)\n            df = df_train[df_train['Province_State'] == state]\n            dates_train = sorted(list(set(df['Date'])))\n            dates_test  = sorted(list(set(df_test['Date'])))\n            for i in range(len(dates_train)):\n                dates_train[i] = initial_date + timedelta(days=dates_train[i])\n            for i in range(len(dates_test)):\n                dates_test[i] = initial_date + timedelta(days=dates_test[i])\n            idx = df_test[df_test['Province_State'] == state].index\n            plt.figure(figsize=(14,8))\n            plt.title('COVID-19 cases on {}'.format(state))\n            plt.xlabel('Date')\n            plt.ylabel('Number')\n            plt.plot(dates_train, df['ConfirmedCases'], linewidth=2, color='#ff9933')\n            plt.plot(dates_test , df_test['ConfirmedCases'].loc[idx], linewidth=2, color='#e67300', linestyle='dashed')\n            legend = []\n            legend.append('{} confirmed cases'.format(state))\n            legend.append('{} predicted cases'.format(state))\n            plt.legend(legend)\n            plt.show()\n            plt.figure(figsize=(14,8))\n            plt.title('COVID-19 fatalities on {}'.format(state))\n            plt.xlabel('Date')\n            plt.ylabel('Number')\n            plt.plot(dates_train, df['Fatalities'], linewidth=2, color='#ff9933')\n            plt.plot(dates_test , df_test['Fatalities'].loc[idx], linewidth=2, color='#e67300', linestyle='dashed')\n            legend = []\n            legend.append('{} fatalities'.format(state))\n            legend.append('{} predicted fatalities'.format(state))\n            plt.legend(legend)\n            plt.show()\n    else:\n        print (\"Please send me a list\")\n\nplotStatus(['Brazil'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime, timedelta\ndef plotGlobalStatus ():\n    legend = []\n    initial_date = datetime (2020, 1, 22)\n    df = df_train.groupby('Date').sum()\n    df['Date'] = df.index\n    test = df_test\n    test['ConfirmedCases'] = df_test['ConfirmedCases']\n    test['Fatalities'] = df_test['Fatalities']\n    test = test.groupby('Date').sum()\n    test['Date'] = test.index\n    dates_train = sorted(list(set(df['Date'])))\n    dates_test  = sorted(list(set(test['Date'])))\n    for i in range(len(dates_train)):\n        dates_train[i] = initial_date + timedelta(days=dates_train[i])\n    for i in range(len(dates_test)):\n        dates_test[i] = initial_date + timedelta(days=dates_test[i])\n    plt.figure(figsize=(14,8))\n    plt.title('Global COVID-19 cases')\n    plt.xlabel('Date')\n    plt.ylabel('Number')\n    plt.plot(dates_train, df['ConfirmedCases'], linewidth=2, color='#ff9933')\n    plt.plot(dates_test , test['ConfirmedCases'], linewidth=2, color='#e67300', linestyle='dashed')\n    legend.append('{} confirmed cases'.format('World'))\n    legend.append('{} predicted cases'.format('World'))\n    plt.legend(legend)\n    plt.show()\n    plt.figure(figsize=(14,8))\n    plt.title('Global COVID-19 fatalities')\n    plt.xlabel('Date')\n    plt.ylabel('Number')\n    plt.plot(dates_train, df['Fatalities'], linewidth=2, color='#ff9933')\n    plt.plot(dates_test , test['Fatalities'], linewidth=2, color='#e67300', linestyle='dashed')\n    legend = []\n    legend.append('{} fatalities'.format('World'))\n    legend.append('{} predicted fatalities'.format('World'))\n    plt.legend(legend)\n    plt.show()\n\nplotGlobalStatus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_date = datetime (2020, 1, 22)\ndf_test['Date'] = df_test['Date'].apply(lambda x: initial_date + timedelta(days=x))\ndf_test.drop(columns=['ForecastId'], inplace=True)\ndf_test = df_test[['Date', 'Country_Region', 'Province_State', 'ConfirmedCases', 'Fatalities']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.to_csv('future.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}