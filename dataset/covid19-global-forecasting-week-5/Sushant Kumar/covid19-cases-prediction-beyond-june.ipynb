{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show(_df, count=2):\n    print(_df.shape)\n    size = _df.shape[0]\n    count = count if size > count else size\n    display(_df.head(count))\n    \n    \n    display(_df.sample(count))\n    display(_df.tail(count))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport array as arr\nimport pandas_profiling\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file = '/test.csv'\ntrain_file = '/train.csv'\ntrain_df = pd.read_csv(dirname + train_file)\ntest_df = pd.read_csv(dirname + test_file)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define variables \npredCol = 'TargetValue'\nidCol = 'Id'\nfidCol = 'ForecastId'\nignoreCols = [predCol, idCol, fidCol, 'Date']\nctry = 'Country_Region'\nste = 'Province_State'\ncty = 'County'\nlcn='Location'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nd2 = pd.to_datetime('2020-12-31')\nlr = test_df[test_df[ctry]=='India'].tail(1)[['ForecastId','Date']].values[0]\nd1 = pd.to_datetime(lr[1]) + timedelta(1)\nid = lr[0]\nprint(d1)\nprint (d2)\nppd = pd.DataFrame(\n    [[d]\n     for d  in pd.date_range(d1, d2, freq='D')])\nppd.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef getid(id1):\n    global id\n    id1 = id + 1\n    id = id1\n    return id1;\n\nprint(id)\ncc= pd.DataFrame(\n    [[getid(id), np.NaN,np.NaN, 'India',1295210000,0.04766, d, 'ConfirmedCases']\n     for d  in pd.date_range(d1, d2, freq='D')],\n    columns=['ForecastId', cty, ste, ctry, 'Population', 'Weight', 'Date', 'Target']\n)\nprint(cc.tail(2))\n\nff= pd.DataFrame(\n    [[getid(id), np.NaN,np.NaN, 'India',1295210000,0.47660, d, 'Fatalities']\n     for d in pd.date_range(d1, d2, freq='D')],\n    columns=['ForecastId', cty, ste, ctry, 'Population', 'Weight', 'Date', 'Target']\n)\nprint(ff.head(2))\n\n\nmoretest = pd.concat([cc,ff])\n#moretest['ForecastId'] = moretest.apply(lambda x: getid(id))['ForecastId']\n\nmoretest[(moretest[ctry]=='India') & (moretest['Date']== '2020-06-10')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df =pd.concat([test_df, moretest])\n#test_df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[(test_df[ctry]=='India') & (test_df['Date']== '2020-06-10')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"    Let's see the train data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why do we have state and country. I thought we have to analyze at country level. Let's drill down.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets merge three location columns. We will also merge the two data set so that we get uniform location feature code.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.concat([train_df, test_df], sort=False)\nshow(df_all, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets join three regional columns into one and convert it to feature column. We also have to merge test and train to have uniform feature columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all[lcn] = df_all[ctry] + '_' +  df_all[ste].fillna('NA') + '_' + df_all[cty].fillna('NA')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Put a filter so that it executes fast. Later remove the filter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"country = [\"US\", \"India\",\"China\", \"Spain\", \"Italy\", \"Pakistan\", \"Mexico\", \"United Kingdom\", \"France\", \"Japan\", \"South Korea\", \"Russia\", \"Cananda\", \"Peru\", \"Turkey\"]\ncountry =[x + \"_NA_NA\" for x in country]\nprint(country)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nif len(country) >0 :\n    df_all = df_all[df_all[lcn].isin(country )]\n\nshow(df_all, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find the begining of pandamic ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"locMin = df_all[df_all[predCol]>0 | df_all[predCol].notnull()].groupby([lcn])['Date'].min()\nlocMin","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on analysis below following are the new dates of spread start.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_startdate(_ctry, _dt):\n    idex = locMin[locMin.index.str.startswith(_ctry)].index\n    print(idex)\n    _dt = pd.to_datetime(_dt, infer_datetime_format=True)\n    print(_dt.strftime(\"%d-%m\"))\n    if len(idex) ==1:\n        print(locMin.at[idex[0]])\n        locMin.at[idex[0]] = _dt\n        print(locMin.at[idex[0]])\n\n\nreplace_startdate('Russia', '03-05-2020')\nreplace_startdate('Italy', '02-20-2020')\nreplace_startdate('Germany', '02-24-2020')\nreplace_startdate('Turkey', '03-15-2020')\nreplace_startdate('Canada_NA_NA', '02-05-2020')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Assign no of days to each entries from start of case in the area.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all= pd.merge(df_all, locMin, on=[lcn,lcn])\nshow(df_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\ndf_all['Date_x'] = pd.to_datetime(df_all['Date_x'], infer_datetime_format=True)\ndf_all['Date_y'] = pd.to_datetime(df_all['Date_y'], infer_datetime_format=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"drop records which are zero before pandamic starts","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = df_all[df_all['Date_x']>=df_all['Date_y']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Days'] = df_all.apply(lambda x: ((x['Date_x'] -  x['Date_y']).days +1) ,axis=1)\ndf_all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize the growth**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import timedelta\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\ndef trim_axs(axs, N):\n    axs = axs.flat\n    for ax in axs[N:]:\n        ax.remove()\n    return axs[:N]\ndef makeMonotoic(df):\n    state = df - df.shift(1)\n    state = state.fillna(0)\n    for st in state:\n        if (st < 0):\n            df = df.shift(1);\n\ndef topCharts(_dg, _n=[0,1], _s=0, _d=0, _w=18, _h=5, _m=400,_dif=2, _l=''):\n    \n    dg = _dg[_dg[predCol].notnull()]\n    \n    if _l != '':\n        dg = dg[dg[lcn].str.startswith(_l)]\n    dg = dg.groupby([lcn, 'Target' ]) \\\n        .sum() \\\n        .sort_values(by=predCol, ascending=False) \\\n        .reset_index()\n    dg1 = dg[(dg['Target']=='ConfirmedCases') | (dg['Target'] == 1)]\n    dg1 = dg1.groupby([lcn]).sum()\n    dg1 = dg1.sort_values(predCol, ascending=False)\n    top_c = dg1[_n[0]:_n[0] + _n[1]]\n    if top_c.shape[0] == 0 :\n        print(\"Nothing to plot for any country\")\n        return\n    \n  \n    _c = 4 if _n[1] > 4 else _n[1] \n    #show(top_c,1)\n    top_10 = top_c.index\n    #vfunc = np.vectorize(visualize, excluded=['_dg','_days'])\n    vt = 4\n    rows = len(top_10) // _c + 1\n    figsize = (_w,_h*rows)\n    fig ,axes= plt.subplots(rows, _c, figsize=figsize, constrained_layout=True)\n    plt.grid(True)\n    axes = trim_axs(axes, len(top_10))\n    for ax, _ctry in zip(axes, top_10):\n        ax2 = ax.twinx()\n   \n        tr = _dg[_dg[lcn]==_ctry]\n        if _d > 0 :\n            tr = tr.iloc[_s:_d*2]\n        tr = tr.sort_values(by='Days', ascending= True)\n        tr = tr[tr[predCol].notnull()]\n\n        cases = tr[(tr['Target']=='ConfirmedCases') | (tr['Target'] == 1)]\n        cases_dates = np.array(cases.apply(lambda x:  x['Date_x'].strftime(\"%d-%m\") , axis=1).unique())\n        days = np.array(tr.apply(lambda x:  x['Days'], axis=1).unique())\n        confirmedCases = np.array(cases[predCol]).cumsum()\n        \n        \n        \n        fatals = tr[(tr['Target']=='Fatalities')  | (tr['Target'] == 2)]\n        fatals_dates = np.array(fatals.apply(lambda x:  x['Date_x'].strftime(\"%d-%m\") , axis=1).unique())\n        Fatalities = np.array(fatals[predCol]).cumsum()\n\n\n        \n        #print(cc05)\n     \n        #print(Fatalities)\n\n        #low = np.ma.masked_where(confirmedCases<=10, confirmedCases)\n        #high = np.ma.masked_where(confirmedCases>10, confirmedCases)\n        #ax.plot(low, high)\n        \n                    \n        ax.plot(cases_dates, confirmedCases, '-b.', color='purple')\n        qf= 'q95'\n        qf1 = 'q05'\n        if 'q05' in cases:\n            cc05 = np.array( (cases[predCol] - cases[qf1]).cumsum())\n            ax.plot(cases_dates, cc05, color='purple', linestyle='--')\n            cc95 = np.array( (cases[predCol] + cases[qf]).cumsum())\n            ax.plot(cases_dates, cc95, color='purple', linestyle='--')\n            \n                    \n        if 'q05' in fatals:\n            ff = fatals[predCol] - fatals[qf1]\n            # print(fatals[predCol])\n            #print(fatals[qf])\n            # print(ff)\n            ff05 = np.array( ff.cumsum() )\n            ax2.plot(fatals_dates, ff05, color='orange', linestyle='--')\n            ff95 = np.array((fatals[predCol] + fatals[qf]).cumsum())\n            ax2.plot(fatals_dates, ff95, color='orange', linestyle='--')\n        \n        #if 'cc05' in locals() and 'cc95' in locals():\n            #ax.bar(cases_dates, cc05,  color='')\n            #ax.bar(cases_dates, cc95, bottom=cc05, color='grey')\n            \n       # if 'ff05' in locals() and 'ff95' in locals():\n           \n            #ax2.bar(fatals_dates, ff05,  color='pink')\n            #ax2.bar(fatals_dates, ff95, bottom=ff05, color='red')\n        \n        ax.set_ylabel(\"Cases\",fontsize=14,color='blue')\n        \n        #ax.text(days, confirmedCases, str(confirmedCases))\n        \n\n        ax2.plot(fatals_dates, Fatalities, '-b.', color='orange', linestyle='-')\n\n        flim = ax2.get_ylim()\n        ax2.set_ylim([-10, flim[1]*1.3])\n        vt = int(len(fatals_dates)*_c/50 + 1)\n        ax2.set_ylabel(\"Fatal\",fontsize=14,color='blue')\n        ax.set_title(_ctry + '(' + tr['Date_y'].iloc[0].strftime(\"%d-%m\") + ')')\n        ax.set_xticklabels(fatals_dates[0::vt], rotation=90)\n        plt.xticks(fatals_dates[0::vt], rotation=90)\n        #ax2.text(dates, Fatalities)\n        ax.grid(True)\n        ax2.grid(True)\n        secax = ax.secondary_xaxis('top')\n        \n        secax.set_xlabel('days')\n        \n        lbly = (ax.get_ylim()[1]/_h) ;\n        p = 0\n        if _dif > 0:\n            for i, v in enumerate(confirmedCases):\n                if(v < _m and v - p >= _dif) :\n                    ax.text(days[i] -1, v ,  str(v.astype(int)) , rotation=90, ha='left')\n                    p = v\n\n    \n#topCharts(dfv1, _dif=10000, _l='India')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets analyze top 20 locations for initial 30 days and / or first 400 cases to change actual spread start date.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[0,2], _d=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[2,1], _d=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[3,1], _d=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[4,1], _d=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[5,1], _d=20, _dif=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[6,4], _d=15,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[10,4], _d=15,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[14,1], _d=30,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[16,1], _d=30,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[17,2], _d=30,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[19,2], _d=30,_dif=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[21,4], _d=10,_dif=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[25,4], _d=10,_dif=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[29,4], _d=10,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(df_all, _n=[33,16], _d=30,_dif=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have US data starting march 10, by then US already more 200 cases. So we ignore it.\n* Russia had first 2 cases on Jan 31, but actual spread can be considered from Mar 5, when next days it went up from 5 to 13. \n* Italy started on Feb 20\n* Germany had no or less spread for first 25 days. Lets consider Feb 24 as start date \n* Turkey - 15 Mar\n* Canada - 5 Feb\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Set new start date above and start ntebook again.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Data Validation : Check empty columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ecol = df_all.isna().sum().sort_values(ascending=False)[df_all.isna().sum()>0].index;\nprint(ecol)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know empty values exist in these columns. Lets make location as feature columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndflcn = df_all[[lcn, ctry, cty, ste]]\n\ndf_all[lcn] = encoder.fit_transform(df_all[lcn])\ndflcn['code'] = df_all[lcn]\n#pd.get_dummies(df, columns=['Location'], drop_first=True)\n\nshow(dflcn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Target'] = df_all['Target'].apply(lambda s: 1 if s== \"ConfirmedCases\" else 2 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Begin Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import tree\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble.weight_boosting import AdaBoostRegressor\nfrom sklearn.linear_model.base import LinearRegression\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveRegressor\nfrom sklearn.linear_model.theil_sen import TheilSenRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [\n    ExtraTreesRegressor(n_estimators=500,n_jobs=-1,verbose=1),\n    XGBRegressor(n_estimators = 2300 , alpha = 0, gamma = 0, learning_rate = 0.04,  random_state = 42 , max_depth = 23),\n    #LGBMRegressor(),\n    #RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',max_depth=None, max_features='auto', max_leaf_nodes=None,max_samples=None, min_impurity_decrease=0.0,                      min_impurity_split=None, min_samples_leaf=1,                      min_samples_split=2, min_weight_fraction_leaf=0.0,                      n_estimators=100, n_jobs=None, oob_score=False,                      random_state=None, verbose=0, warm_start=False),\n    #KNeighborsRegressor(),\n    #AdaBoostRegressor(),\n    #PassiveAggressiveRegressor(),\n    #TheilSenRegressor()\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain = df_all[df_all[predCol].notnull()].drop(columns=['ForecastId'])\n\ntest = df_all[df_all[predCol].isna()].drop([predCol], axis=1).drop(columns=['Id'])\n\ntest.rename(columns={'ForecastId':'Id'}, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['Id', 'Population', 'Weight', 'Location', 'Days', 'Target']\n\ntest= test[cols]\ntrain= train[cols + ['TargetValue']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Xtrn, Xtest, Ytrn, Ytest = train_test_split(train.drop([predCol], axis=1), train[[predCol]], test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_predictions (predictions):\n    predictions[predictions < 0] = 0   \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TestModels = pd.DataFrame()\ntmp = {}\n \nfor model in models:\n    # get model name\n    \n    tmp['Model'] = str(model)\n    # fit model on training dataset\n    model.fit(Xtrn, Ytrn[predCol])\n    pred= model.predict(Xtest)\n    pred = handle_predictions(pred)\n\n   \n    act = pred\n    targ = handle_predictions(Ytest[predCol])\n    \n    tmp['accuracy'] = r2_score(targ, act)\n    tmp['rmsle'] = (mean_squared_log_error(targ,act))\n    tmp['rmse'] = (mean_squared_error(targ, act))\n    # write obtained data\n    TestModels = TestModels.append([tmp])\n        \nTestModels.set_index('Model', inplace=True)\nTestModels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom scipy.interpolate import make_interp_spline, BSpline\nimport math \n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(TestModels)\nprint(scaled_data.min())\nscaled_data = scaled_data + abs(scaled_data.min())+1\naccuracy = scaled_data[0:,0:1]\nrmsle = scaled_data[0:,1:2]\nrmse =   scaled_data[0:,2:]\nallmet =  1/accuracy\n\nprint(scaled_data)\nprint(1/accuracy)\nfig, axes = plt.subplots(figsize=(15, 6))\naxes.plot(range(len(accuracy)),accuracy, color='blue' , marker='.')\naxes.plot(range(len(accuracy)),rmsle, color='green' , marker='.')\naxes.plot(range(len(accuracy)),rmse, color='orange' , marker='.')\n\naxes.plot(range(len(accuracy)),allmet, color='red' , marker='.')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestModel = models[allmet.argmin()]\nTestModels.iloc[allmet.argmin():allmet.argmin()+1].index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bestModelName = TestModels.iloc[allmet.argmin():allmet.argmin()+1].index[0]\nbestScore = allmet[allmet.argmin()]      \nprint('Best Score : ' + str(bestScore))\nselectedModel = models[allmet.argmin()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selectedModel.fit(train.drop([predCol], axis=1), train[predCol])\nprediction =selectedModel.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[predCol] = prediction\ntest['istrain'] = 0\ntrain['istrain'] = 1\nprint(test.shape)\nprint(train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [lcn, 'Days','Target']\n#print(train[(train['Date_x']=='2020-05-17') & (train[ctry]=='India')])\ntrain1=train.rename(columns={'Id':'tid'})\n#print(train1[(train1['Date_x']=='2020-05-17') & (train1[ctry]=='India')])\n\nintersect = pd.merge(train1[ [predCol, 'tid'] + cols], test[cols + ['Id']], how='inner')#.set_index('tid')\ntidx = intersect['tid']\ntrain = train[~train['Id'].isin(tidx)]\n#show(intersect,2)\n#print(intersect[(intersect['Days']==77) & (intersect[lcn]==142) ])\nintersect.drop(columns=['tid'],axis=1, inplace=True)\n\nintersect= intersect.set_index('Id')\ntt = test#[(test['Date_x']=='2020-05-17') & (test[ctry]=='India')]\ntt = tt.drop(columns=[predCol])#.set_index('Id')\n#print(tt[(tt['Days']==77) & (tt[lcn]==142)])\n#print(intersect[(intersect['Days']==77) & (intersect[lcn]==142)])\n\nintersect = pd.merge(tt, intersect, how='inner')\n\ntest = test[~test['Id'].isin(intersect['Id'])]\n#print(intersect)\n#print()\nprint(intersect[(intersect['Days']==77) & (intersect[lcn]==142)])\ntest1 = test\ntest1 = pd.concat([intersect, test1], sort=False)\n#print(test1[(test1['Date_x']=='2020-05-17') & (test1[ctry]=='India')])\ntest = test1.drop_duplicates()\n#print(test1[(test1['Date_x']=='2020-05-17') & (test1[ctry]=='India')])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dflcn = dflcn.drop_duplicates()\n\ndfv = pd.concat([train, test ], sort=False)\n\ndfv = pd.merge(dfv, dflcn, left_on=lcn, right_on='code',how='left')\ndfv.rename(columns={'Location_y':lcn}, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfv['Date_x'] = dfv.apply(lambda x: pd.to_datetime(locMin.at[x[lcn],]) + timedelta(x['Days']) , axis=1)\ndfv['Date_y'] = dfv.apply(lambda x: pd.to_datetime(locMin.at[x[lcn],]) , axis=1)\ndfv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfv['Target'] = dfv['Target'].apply(lambda s: \"ConfirmedCases\" if s == 1 else \"Fatalities\" )\ndfv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfv1= dfv\n\nwinsize = len(dfv1[lcn].unique())\nprint (winsize)\ndfv1['q05'] = dfv1.groupby([lcn, 'Target'])[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=winsize).quantile(0.05)).reset_index(name='q05')['q05']\ndfv1['q95'] = dfv1.groupby([lcn, 'Target'])[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=winsize).quantile(0.95)).reset_index(name='q95')['q95']\ndfv1['q50'] = dfv1.groupby([lcn, 'Target'])[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=winsize).quantile(0.5)).reset_index(name='q50')['q50']\n\n#dfv1['q05'] = dfv1.groupby(['Id'])[predCol].quantile(0.05).reset_index(name='q05')['q05']\n#dfv1['q95'] = dfv1.groupby(['Id'])[predCol].quantile(0.95).reset_index(name='q95')['q95']\n#dfv1['q50'] = dfv1.groupby(['Id'])[predCol].quantile(0.5).reset_index(name='q50')['q50']\n\n#dfv1[predCol] = dfv1['q50']\n\ndfv1[dfv1['Target']=='ConfirmedCases'].tail(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfv1[dfv1[lcn]=='India_NA_NA']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\n\ntoday = pd.to_datetime(date.today())\noffset = today - timedelta (1)\n\ndfv1.loc[(dfv1['Date_x'] < offset), 'q05'] = 0\ndfv1.loc[(dfv1['Date_x'] < offset), 'q50'] = 0\ndfv1.loc[(dfv1['Date_x'] < offset), 'q95'] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"dfv1= dfv.groupby([lcn, 'Target']).apply(lambda x: x.set_index('Date_x').resample('1D').first())\n\ndfv1['q05'] = dfv1.groupby(level=1)[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=8).quantile(0.05)).reset_index(name='q05')['q05']\ndfv1['q95'] = dfv1.groupby(level=1)[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=8).quantile(0.95)).reset_index(name='q95')['q95']\n\ndfv1.groupby(level=1)[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=8).quantile(0.05)).reset_index(name='q05')","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_r = dfv1[dfv1['istrain']==0]\n#test_r['q50'] = test_r[predCol]\nsub=pd.melt(test_r, id_vars=['Id'], value_vars=['q05','q50','q95'])\nsub['variable']=sub['variable'].str.replace(\"q\",\"0.\", regex=False)\nsub['ForecastId_Quantile']=sub['Id'].astype(int).astype(str)+'_'+sub['variable']\nsub['TargetValue']=sub['value']\nsub=sub[['ForecastId_Quantile','TargetValue']]\nsub.reset_index(drop=True,inplace=True)\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfc = dfv[dfv[lcn]=='Italy_NA_NA']\ndfc[dfc['Date_x']=='2020-04-28']\n#dfc = dfc[dfc['Target']=='Fatalities']\n#dfc = dfc.groupby('Date_x').count()\n#dfc[dfc['code']!=1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[0,1], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[2,1], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[3,4], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[7,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[10,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[13,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[16,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[19,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[22,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _n=[25,3], _s=10, _d=180, _dif=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _l='US')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _l='Bra',  _dif=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='India')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='Russia')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='United K')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='Spain')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='Italy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='China')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='Mexi')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topCharts(dfv1, _dif=10000, _l='Pak')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topcc = pd.DataFrame(dfv[(dfv[ste].isnull()) & (dfv['Target']=='ConfirmedCases')].groupby(lcn)[predCol].sum())\n#result['countries'] =.index\ntopcc['fatals'] = dfv[(dfv[ste].isnull()) & (dfv['Target']=='Fatalities')].groupby(lcn)[predCol].sum().values\ntopcc.sort_values(by=predCol, ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}