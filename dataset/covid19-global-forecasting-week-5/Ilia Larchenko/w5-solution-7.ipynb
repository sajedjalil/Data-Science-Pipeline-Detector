{"cells":[{"metadata":{},"cell_type":"markdown","source":"Quite messy notebook using mix of several lightgmb models predictions."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\n\nfrom tqdm.notebook import tqdm\n\nfrom functools import partial\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed_everything(seed=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_lambda = 0.90 # reflect the weight decay for distant days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print([weights_lambda ** i for i in range(100)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-5/train.csv')\ntrain_df = train_df.fillna('')\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-5/test.csv')\ntest_df = test_df.fillna('')\ntest_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_dates_total = len(np.unique(list(train_df['Date']) + list(test_df['Date'])))\nprint(num_dates_total)\nnum_dates_test = len(np.unique(list(test_df['Date'])))\nprint(num_dates_test)\nnum_dates_train = len(np.unique(list(train_df['Date'])))\nprint(num_dates_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-5/submission.csv')\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sample_submission) / 3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"cases = train_df[\"TargetValue\"][train_df[\"Target\"] == 'ConfirmedCases'].values.reshape((-1, num_dates_train))\ncases","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fatalities = train_df[\"TargetValue\"][train_df[\"Target\"] == 'Fatalities'].values.reshape((-1, num_dates_train))\nfatalities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making the weights equal\nfatalities = fatalities * 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"population = train_df[\"Population\"].values[::num_dates_train * 2]\npopulation\nlen(population)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weignts_cases = train_df[\"Weight\"][train_df[\"Target\"] == 'ConfirmedCases'].values[::num_dates_train]\nweignts_cases\nlen(weignts_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgbm\nparams = {\n    \"metric\":\"mse\",\n}\nf = lgbm.LGBMRegressor(**params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if train:\n    days_to_predict = 45\nelse:\n    days_to_predict = 31\ntrain_days_used = num_dates_total - days_to_predict\nassert train_days_used <= num_dates_train\n\n\nfeatures_days = 40\ntest_days_predict = 40\ntrain_data = []\ntarget_data = []\ntest_data = []\nweights = []\n\n\nfat_index_add = features_days\nfat_index_add_test = test_days_predict ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(population)):\n    for j in range(train_days_used - 1 - features_days):\n        full_train = True\n        \n        data = []\n        data = data + [population[i]]\n        max_cases = cases[i][:j+features_days].max()\n        data = data + [max_cases]\n        max_fatalities = fatalities[i][:j+features_days].max()\n        data = data + [max_fatalities]\n        data = data + list(cases[i][j:j+features_days]/(max_cases+1))\n        data = data + list(fatalities[i][j:j+features_days]/(max_fatalities+1))\n        train_data.append(data)\n\n        target = []\n        target_list = list(cases[i][j+features_days:j+features_days + test_days_predict]/(max_cases+1))\n        target_list = target_list + [None]*(test_days_predict - len(target_list))\n        target = target + target_list\n\n        target_list = list(fatalities[i][j+features_days:j+features_days + test_days_predict]/(max_fatalities+1))\n        target_list = target_list + [None]*(test_days_predict - len(target_list))\n        target = target + target_list           \n\n        target_data.append(target)\n        weights.append(weignts_cases[i]*weights_lambda**(train_days_used - test_days_predict - features_days - j - 1))\n    \n    test = []\n    test = test + [population[i]]\n    max_cases = cases[i][:train_days_used].max()\n    test = test + [max_cases]\n    max_fatalities = fatalities[i][:train_days_used].max()\n    test = test + [max_fatalities]\n    test = test + list(cases[i][train_days_used - features_days:train_days_used]/(max_cases+1))\n    test = test + list(fatalities[i][train_days_used - features_days:train_days_used]/(max_fatalities+1))\n    test_data.append(test)        \n\ninitial_train_data = np.stack(train_data)\ntarget_data = np.stack(target_data)\nmax_features_days = features_days\nmax_test_days_predict = test_days_predict\ninitial_test_data = np.stack(test_data)\nweights = np.array(weights)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\ntest = []\ntest = test + [population[i]]\nmax_cases = cases[i][:train_days_used].max()\ntest = test + [max_cases]\nmax_fatalities = fatalities[i][:train_days_used].max()\ntest = test + [max_fatalities]\ntest = test + list(cases[i][train_days_used - features_days:train_days_used]/(max_cases+1))\ntest = test + list(fatalities[i][train_days_used - features_days:train_days_used]/(max_fatalities+1))\ntest_data.append(test)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_cases_global = []\npredictions_cases_max_global = np.zeros((len(population), days_to_predict))\npredictions_cases_min_global = np.zeros((len(population), days_to_predict)) + 1000000\n\npredictions_fatalities_global = []\npredictions_fatalities_max_global = np.zeros((len(population), days_to_predict))\npredictions_fatalities_min_global = np.zeros((len(population), days_to_predict)) + 1000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for features_days in tqdm(range(5, 40, 10)):\n    fat_index_add = features_days\n    train_data = np.concatenate([initial_train_data[:,:3], \n                                 initial_train_data[:,3+max_features_days-features_days:3+max_features_days],\n                                 initial_train_data[:,3+2*max_features_days-features_days:3+2*max_features_days],\n                                ], axis = 1).copy()\n    \n    case_predictors = []\n    for i in range(max_test_days_predict):\n        f = lgbm.LGBMRegressor(**params)\n        mask = np.logical_not(pd.isnull(target_data[:,i]))\n        f.fit(train_data[mask], target_data[:,i][mask],sample_weight = weights[mask], verbose=False)\n        case_predictors.append(f)\n\n    fatalities_predictors = []\n    for i in range(max_test_days_predict):\n        f = lgbm.LGBMRegressor(**params)\n        mask = np.logical_not(pd.isnull(target_data[:,max_test_days_predict + i]))\n        f.fit(train_data[mask], target_data[:,max_test_days_predict + i][mask],sample_weight = weights[mask], verbose=False)\n        fatalities_predictors.append(f)\n        \n    \n    for test_days_predict in range(1,32, 5):\n        \n        test_data = np.concatenate([initial_test_data[:,:3], \n                             initial_test_data[:,3+max_features_days-features_days:3+max_features_days],\n                             initial_test_data[:,3+2*max_features_days-features_days:3+2*max_features_days],\n                            ], axis = 1).copy()\n        \n        predictions_cases_sum = np.zeros((len(population), days_to_predict))\n        predictions_cases_max = np.zeros((len(population), days_to_predict))\n        predictions_cases_min= np.zeros((len(population), days_to_predict))+ 1000000\n        predictions_cases_counts = np.zeros(days_to_predict)\n\n\n        predictions_fatalities_sum = np.zeros((len(population), days_to_predict))\n        predictions_fatalities_max = np.zeros((len(population), days_to_predict))\n        predictions_fatalities_min= np.zeros((len(population), days_to_predict))+ 1000000\n        predictions_fatalities_counts = np.zeros(days_to_predict)\n\n        for step in range(days_to_predict - test_days_predict + 1):\n            predictions_cases_local = np.zeros((len(population), test_days_predict))\n            predictions_fatalities_local = np.zeros((len(population), test_days_predict))\n\n            for i in range(test_days_predict):\n                predictions_cases_local[:,i] = case_predictors[i].predict(test_data) * (test_data[:,1] + 1)\n\n            for i in range(test_days_predict):\n                predictions_fatalities_local[:,i] = fatalities_predictors[i].predict(test_data) * (test_data[:,2] + 1)\n\n\n            predictions_cases_sum[:,step:step+test_days_predict] += predictions_cases_local\n            predictions_cases_max[:,step:step+test_days_predict] = np.maximum(predictions_cases_max[:,step:step+test_days_predict], predictions_cases_local)\n            predictions_cases_min[:,step:step+test_days_predict] = np.minimum(predictions_cases_min[:,step:step+test_days_predict], predictions_cases_local)\n            predictions_cases_counts[step:step+test_days_predict] += 1\n\n            current_predictions_cases = predictions_cases_sum[:,step] / predictions_cases_counts[step]\n\n\n            predictions_fatalities_sum[:,step:step+test_days_predict] += predictions_fatalities_local\n            predictions_fatalities_max[:,step:step+test_days_predict] = np.maximum(predictions_fatalities_max[:,step:step+test_days_predict], predictions_fatalities_local)\n            predictions_fatalities_min[:,step:step+test_days_predict] = np.minimum(predictions_fatalities_min[:,step:step+test_days_predict], predictions_fatalities_local)\n            predictions_fatalities_counts[step:step+test_days_predict] += 1\n\n            current_predictions_fatalities = predictions_fatalities_sum[:,step] / predictions_fatalities_counts[step]\n\n            test_data[:,3:3+features_days-1] = test_data[:,4:3+features_days]\n            new_max = np.maximum(test_data[:,1], current_predictions_cases)\n            test_data[:,3:3+features_days -1] *= ((test_data[:,1] + 1) / (new_max + 1)).reshape((-1,1))\n            test_data[:,2+features_days] = current_predictions_cases / (new_max+1)\n            test_data[:,1] = new_max\n\n\n\n            test_data[:,3+features_days:3+features_days-1+features_days] = test_data[:,4+features_days:3+features_days+features_days]\n            new_max = np.maximum(test_data[:,2], current_predictions_fatalities)\n            test_data[:,3+features_days:3+features_days+features_days-1] *= ((test_data[:,2] + 1) /(new_max + 1) ).reshape((-1,1))\n            test_data[:,2+features_days+features_days] = current_predictions_fatalities / (new_max+1)\n            test_data[:,2] = new_max\n\n\n        predictions_cases_global.append(predictions_cases_sum / predictions_cases_counts)\n        predictions_cases_min_global = np.minimum(predictions_cases_min_global, predictions_cases_min)\n        predictions_cases_max_global = np.maximum(predictions_cases_max_global, predictions_cases_max)\n\n        predictions_fatalities_global.append(predictions_fatalities_sum / predictions_fatalities_counts)\n        predictions_fatalities_min_global = np.minimum(predictions_fatalities_min_global, predictions_fatalities_min)\n        predictions_fatalities_max_global = np.maximum(predictions_fatalities_max_global, predictions_fatalities_max)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions_cases = np.stack(predictions_cases_global).mean(0)\nfinal_predictions_fatalities = np.stack(predictions_fatalities_global).mean(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions_cases_std = np.stack(predictions_cases_global).std(0)\nfinal_predictions_fatalities_std = np.stack(predictions_fatalities_global).std(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making simple submission and valiadtion"},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_len = num_dates_train + days_to_predict - num_dates_total\nvalid_len","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_true_cases = cases[:,-valid_len:]\nvalid_true_fatalities = fatalities[:, -valid_len:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_mean_cases = final_predictions_cases[:,:valid_len].copy()\npredict_min_cases = predictions_cases_min_global[:,:valid_len].copy()\npredict_max_cases = predictions_cases_max_global[:,:valid_len].copy()\n\npredict_mean_fatalities = final_predictions_fatalities[:,:valid_len].copy()\npredict_min_fatalities = predictions_fatalities_min_global[:,:valid_len].copy()\npredict_max_fatalities = predictions_fatalities_max_global[:,:valid_len].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_loss_L(true_array, predicted_array, tau, weight):\n    array = predicted_array * (predicted_array > 0)\n    abs_diff = np.absolute(true_array - array)\n    result = abs_diff * (1 -tau) * (array > true_array) + abs_diff * (tau) * (array <= true_array)\n    result = (result.mean(1)) * weight\n#     print(result.mean())\n    return result.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_loss(true_array, mean_array, min_array, max_array, weights):\n    result = (compute_loss_L(true_array, max_array, 0.95, weights) + \n              compute_loss_L(true_array, min_array, 0.05, weights) + \n              compute_loss_L(true_array, mean_array, 0.5, weights))\n    return result / 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x0 = [1,0,0,0]\n\ndef normalize(x, mean_array, min_array, max_array, base = None):\n    if base is None:\n        base = np.zeros_like(mean_array)\n        lamb = 0\n    else:\n        lamb = x[3]\n    deviation = np.array([lamb * n for n in range(base.shape[1])])\n    new_array = base + (x[0] * mean_array + x[1] * min_array + x[2]*max_array) * (deviation + 1).reshape((1,-1))\n#     print(deviation)\n    return new_array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fun(x, mean_array, min_array, max_array, true_array, weights, tau, base = None):\n    new_array = normalize(x, mean_array, min_array, max_array, base = base)\n    return compute_loss_L(true_array, new_array, tau, weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import minimize\nfrom functools import partial","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [1.13952572e+00,  5.49225865e-02, -2.41205637e-01,  0]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.5, true_array = valid_true_cases)\n    res = minimize(part_func, x0 = [1,0,0,0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_final_predictions_cases = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global)\n\n\nx = [ 0.15951811, -0.16219168, -0.45721537, -0.01818924]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.05, true_array = valid_true_cases, \n                        base = new_final_predictions_cases[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_cases_min_global = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global, base = new_final_predictions_cases)\n\n\nx = [0.69243738, -0.25191857,  0.14696969,  0.00829207]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_cases, min_array = predict_min_cases, max_array = predict_max_cases, weights = weignts_cases, tau = 0.95, true_array = valid_true_cases,\n                       base = new_final_predictions_cases[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_cases_max_global = normalize(x, final_predictions_cases, predictions_cases_min_global, predictions_cases_max_global, base = new_final_predictions_cases)\n\nfinal_predictions_cases = new_final_predictions_cases\npredictions_cases_min_global = new_predictions_cases_min_global\npredictions_cases_max_global = new_predictions_cases_max_global","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [ 8.14860611e-01,  8.80132452e-02, -1.60911727e-01,  0]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.5, true_array = valid_true_fatalities)\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_final_predictions_fatalities = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global)\n\n\nx = [-0.92953006,  0.998952,   -0.40161728, -0.04630638]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.05,\n                        true_array = valid_true_fatalities, base = new_final_predictions_fatalities[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_fatalities_min_global = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global,\n                                                  base = new_final_predictions_fatalities)\n\n\nx = [0.8965989,   0.02662769, -0.01948303,  0.00444532]\nif train:\n    part_func = partial(fun, mean_array = predict_mean_fatalities, min_array = predict_min_fatalities, max_array = predict_max_fatalities, weights = weignts_cases, \n                        tau = 0.95, \n                        true_array = valid_true_fatalities, base = new_final_predictions_fatalities[:,:valid_len])\n    res = minimize(part_func, x0 = [1,0,0, 0], method='Powell', tol=1e-6)\n    print(res.fun)\n    print(res.x)\n    x = res.x\n\nnew_predictions_fatalities_max_global = normalize(x, final_predictions_fatalities, predictions_fatalities_min_global, predictions_fatalities_max_global,\n                                                  base = new_final_predictions_fatalities)\n\nfinal_predictions_fatalities = new_final_predictions_fatalities\npredictions_fatalities_min_global = new_predictions_fatalities_min_global\npredictions_fatalities_max_global = new_predictions_fatalities_max_global","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_mean_cases = final_predictions_cases[:,:valid_len].copy()\npredict_min_cases = predictions_cases_min_global[:,:valid_len].copy()\npredict_max_cases = predictions_cases_max_global[:,:valid_len].copy()\n\npredict_mean_fatalities = final_predictions_fatalities[:,:valid_len].copy()\npredict_min_fatalities = predictions_fatalities_min_global[:,:valid_len].copy()\npredict_max_fatalities = predictions_fatalities_max_global[:,:valid_len].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if train:\n    total_loss = (compute_loss(valid_true_cases, predict_mean_cases, predict_min_cases, \n                                   predict_max_cases , weignts_cases) + \n                     compute_loss(valid_true_fatalities, predict_mean_fatalities, predict_min_fatalities, \n                                  predict_max_fatalities, weignts_cases)) / 2\n\n    print(total_loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making prediction file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_mean_cases = np.zeros((len(population), 45))\nsubmission_min_cases = np.zeros((len(population), 45))\nsubmission_max_cases = np.zeros((len(population), 45))\n\nsubmission_mean_fatalities = np.zeros((len(population), 45))\nsubmission_min_fatalities = np.zeros((len(population), 45))\nsubmission_max_fatalities = np.zeros((len(population), 45))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_mean_cases[:, -days_to_predict:] = final_predictions_cases\nsubmission_min_cases[:, -days_to_predict:] = predictions_cases_min_global\nsubmission_max_cases[:, -days_to_predict:] = predictions_cases_max_global\n\n\nsubmission_mean_fatalities[:, -days_to_predict:] = final_predictions_fatalities / 10\nsubmission_min_fatalities[:, -days_to_predict:] = predictions_fatalities_min_global / 10\nsubmission_max_fatalities[:, -days_to_predict:] = predictions_fatalities_max_global / 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_mean_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\nsubmission_min_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\nsubmission_max_cases[:, :-days_to_predict] = cases[:, num_dates_total-45:num_dates_total-days_to_predict]\n\n\nsubmission_mean_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] / 10\nsubmission_min_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] / 10\nsubmission_max_fatalities[:, :-days_to_predict] = fatalities[:, num_dates_total-45:num_dates_total-days_to_predict] / 10 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_1 = compute_loss(fatalities[:,-14:] / 10, submission_mean_fatalities[:,:14] , submission_min_fatalities[:,:14], \n                                   submission_max_fatalities[:,:14],weignts_cases * 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_2 = compute_loss(cases[:,-14:], submission_mean_cases[:,:14] , submission_min_cases[:,:14], \n                                   submission_max_cases[:,:14],weignts_cases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(loss_1 + loss_2) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#0.25734741800417726","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-5/submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = []\nfor i in range(len(submission_mean_cases)):\n    for j in range(len(submission_mean_cases[0])):\n        submission.append(submission_min_cases[i][j])\n        submission.append(submission_mean_cases[i][j])\n        submission.append(submission_max_cases[i][j])\n        \n        submission.append(submission_min_fatalities[i][j])\n        submission.append(submission_mean_fatalities[i][j])\n        submission.append(submission_max_fatalities[i][j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = [max(0,x) for x in submission]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file['TargetValue'] = submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisation of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_results(true_array, mean_array, max_array, min_array):\n    nans = np.array([None]*len(true_array))\n    plt.plot(true_array)\n    plt.plot(np.concatenate([nans,mean_array[-days_to_predict:]]))\n    plt.plot(np.concatenate([nans,min_array[-days_to_predict:]]))\n    plt.plot(np.concatenate([nans,max_array[-days_to_predict:]]))\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in np.random.randint(0, len(cases),30):\n    plot_results(cases[i], submission_mean_cases[i], submission_max_cases[i], submission_min_cases[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in np.random.randint(0, len(cases),10):\n    plot_results(fatalities[i], submission_mean_fatalities[i], submission_max_fatalities[i], submission_min_fatalities[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}