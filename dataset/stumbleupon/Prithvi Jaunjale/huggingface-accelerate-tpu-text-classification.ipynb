{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n! python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T12:06:23.765622Z","iopub.execute_input":"2021-08-11T12:06:23.766073Z","iopub.status.idle":"2021-08-11T12:07:39.883163Z","shell.execute_reply.started":"2021-08-11T12:06:23.765967Z","shell.execute_reply":"2021-08-11T12:07:39.881955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install transformers sentencepiece accelerate","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:07:39.885228Z","iopub.execute_input":"2021-08-11T12:07:39.885591Z","iopub.status.idle":"2021-08-11T12:07:47.769547Z","shell.execute_reply.started":"2021-08-11T12:07:39.88555Z","shell.execute_reply":"2021-08-11T12:07:47.768418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom accelerate import notebook_launcher","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:07:47.774154Z","iopub.execute_input":"2021-08-11T12:07:47.774559Z","iopub.status.idle":"2021-08-11T12:07:48.567516Z","shell.execute_reply.started":"2021-08-11T12:07:47.77452Z","shell.execute_reply":"2021-08-11T12:07:48.566465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport copy\nimport csv\nimport json\nimport random\nfrom tqdm.auto import tqdm\nimport time\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\nfrom pprint import pprint\nimport pickle\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport torch\n\nimport transformers\nfrom sklearn.metrics import (\n    roc_auc_score\n)\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:07:48.569283Z","iopub.execute_input":"2021-08-11T12:07:48.569663Z","iopub.status.idle":"2021-08-11T12:07:50.576602Z","shell.execute_reply.started":"2021-08-11T12:07:48.56963Z","shell.execute_reply":"2021-08-11T12:07:50.575671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"SEED = 42\n\n# paths\nDATA_DIR = \"../input/stumbleupon/\"\nTF_MODEL_PATH = \"roberta-base\"\nTRAINED_MODELS_PATH = f\"stumbleupon_{TF_MODEL_PATH}.pt\"\n\n# data\nTITLE_MAX_LENGTH = 64\nBODY_MAX_LENGTH = 1024\nTRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 8\nVALIDATION_SPLIT = 0.2\nN_LABELS = 2\n\n# model\nTF_HIDDEN = 768\nFULL_FINETUNING = True\nLR = 3e-5\nOPTIMIZER = 'AdamW'\nCRITERION = 'CrossEntropyLoss'\nSAVE_BEST_MODEL = True\nEPOCHS = 1","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:07:50.577893Z","iopub.execute_input":"2021-08-11T12:07:50.578211Z","iopub.status.idle":"2021-08-11T12:07:50.585177Z","shell.execute_reply.started":"2021-08-11T12:07:50.57818Z","shell.execute_reply":"2021-08-11T12:07:50.58404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(DATA_DIR + \"train.tsv\", sep='\\t')\ndf_test = pd.read_csv(DATA_DIR + \"test.tsv\", sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:08:27.228968Z","iopub.execute_input":"2021-08-11T12:08:27.229385Z","iopub.status.idle":"2021-08-11T12:08:28.458129Z","shell.execute_reply.started":"2021-08-11T12:08:27.229345Z","shell.execute_reply":"2021-08-11T12:08:28.45708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:08:31.082474Z","iopub.execute_input":"2021-08-11T12:08:31.082855Z","iopub.status.idle":"2021-08-11T12:08:31.154027Z","shell.execute_reply.started":"2021-08-11T12:08:31.082822Z","shell.execute_reply":"2021-08-11T12:08:31.153017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_df_text(df, is_test=False):\n    lem = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english'))\n\n    def clean_text(s):\n        tokens = s.split()\n        no_stop = [tok for tok in tokens if tok.lower() not in stop_words]\n        lemmas = [lem.lemmatize(tok) for tok in no_stop]\n        return ' '.join(lemmas)\n    \n    def foo(s, keyword):\n        fin = \"\"\n        di = json.loads(s)\n        if keyword in di.keys() and di[keyword]:\n            fin = di[keyword]\n            \n        if fin == \"\":\n            if keyword == \"title\":\n                if \"url\" in di.keys() and di[\"url\"]:\n                    fin = di[\"url\"]\n                    fin = clean_text(fin)\n                else:\n                    fin = \"None\"\n            else:\n                fin = \"None\"\n        else:\n            fin = clean_text(fin)\n            \n        return fin\n\n    df_text = pd.DataFrame()\n    df_text[\"urlid\"] = df[\"urlid\"]\n    df_text[\"title\"] = df[\"boilerplate\"].apply(lambda x: foo(x, \"title\"))\n    df_text[\"body\"] = df[\"boilerplate\"].apply(lambda x: foo(x, \"body\"))\n    \n    if not is_test:\n        df_text[\"label\"] = df[\"label\"]\n    \n    return df_text","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:08:31.590765Z","iopub.execute_input":"2021-08-11T12:08:31.591134Z","iopub.status.idle":"2021-08-11T12:08:31.604032Z","shell.execute_reply.started":"2021-08-11T12:08:31.591103Z","shell.execute_reply":"2021-08-11T12:08:31.602862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_text = get_df_text(df_train)\ndf_test_text = get_df_text(df_test, is_test=True)\ndf_train_text","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:09:11.22966Z","iopub.execute_input":"2021-08-11T12:09:11.230158Z","iopub.status.idle":"2021-08-11T12:09:30.890403Z","shell.execute_reply.started":"2021-08-11T12:09:11.230126Z","shell.execute_reply":"2021-08-11T12:09:30.889431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class TransformerDataset(torch.utils.data.Dataset):\n    def __init__(self, df, is_test=False):\n        super(TransformerDataset, self).__init__()\n        \n        self.titles = df[\"title\"].values\n        self.bodies = df[\"body\"].values\n        self.is_test = is_test\n        if not self.is_test:\n            self.labels = df[\"label\"].values\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(TF_MODEL_PATH)\n\n    def __len__(self):\n        return len(self.titles)\n    \n    def __getitem__(self, index):\n        title_tokenized = self.tokenizer.encode_plus(\n            str(self.titles[index]),\n            max_length=TITLE_MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n        body_tokenized = self.tokenizer.encode_plus(\n            str(self.bodies[index]),\n            max_length=BODY_MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n\n        if not self.is_test:\n            return {\n                \"title\": {\n                    \"input_ids\": title_tokenized[\"input_ids\"].squeeze().long(),\n                    \"attention_mask\": title_tokenized[\"attention_mask\"].squeeze().long()\n                },\n                \"body\": {\n                    \"input_ids\": body_tokenized[\"input_ids\"].squeeze().long(),\n                    \"attention_mask\": body_tokenized[\"attention_mask\"].squeeze().long()\n                },\n                \"labels\": torch.Tensor([self.labels[index]]).long().squeeze()\n            }\n\n        return {\n            \"title\": {\n                    \"input_ids\": title_tokenized[\"input_ids\"].squeeze().long(),\n                    \"attention_mask\": title_tokenized[\"attention_mask\"].squeeze().long()\n                },\n            \"body\": {\n                \"input_ids\": body_tokenized[\"input_ids\"].squeeze().long(),\n                \"attention_mask\": body_tokenized[\"attention_mask\"].squeeze().long()\n            },\n        }","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:09:38.395026Z","iopub.execute_input":"2021-08-11T12:09:38.395419Z","iopub.status.idle":"2021-08-11T12:09:38.410468Z","shell.execute_reply.started":"2021-08-11T12:09:38.395387Z","shell.execute_reply":"2021-08-11T12:09:38.408994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TFDualHeadModel(torch.nn.Module):\n    def __init__(self):\n        super(TFDualHeadModel, self).__init__()\n\n        self.tf = transformers.AutoModel.from_pretrained(TF_MODEL_PATH)\n        self.dropout = torch.nn.Dropout(p=0.3)\n        self.output = torch.nn.Linear(TF_HIDDEN * 2, N_LABELS)\n\n    def forward(\n        self,\n        title_input_ids,\n        title_attention_mask,\n        body_input_ids,\n        body_attention_mask \n        ):\n\n        title_tf_out = self.tf(\n            input_ids=title_input_ids,\n            attention_mask=title_attention_mask\n        )\n        \n        title_drop = self.dropout(title_tf_out.pooler_output)\n        \n        body_tf_out = self.tf(\n            input_ids=body_input_ids,\n            attention_mask=body_attention_mask\n        )\n        \n        body_drop = self.dropout(body_tf_out.pooler_output)\n        \n        combined = torch.cat([title_drop, body_drop], dim=1)\n        x = self.output(combined)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:09:38.974291Z","iopub.execute_input":"2021-08-11T12:09:38.974898Z","iopub.status.idle":"2021-08-11T12:09:38.983159Z","shell.execute_reply.started":"2021-08-11T12:09:38.974862Z","shell.execute_reply":"2021-08-11T12:09:38.982325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"code","source":"def get_model(model_path):\n    print(\"\\n-- Loading model\")\n    model = TFDualHeadModel()\n    accelerator = Accelerator()\n    \n    unwrapped_model = accelerator.unwrap_model(model)\n    \n    chkpt = torch.load(model_path)\n    \n    unwrapped_model.load_state_dict(chkpt[\"state_dict\"]) \n    roc = chkpt[\"roc\"]\n    return unwrapped_model, roc\n\n\ndef load_data(df):\n    dataset_size = len(df)\n    indices = list(range(dataset_size))\n    split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n    \n    np.random.seed(SEED)\n    np.random.shuffle(indices)\n\n    train_indices, val_indices = indices[split:], indices[:split]\n    \n    df_train = df.iloc[train_indices]\n    df_val = df.iloc[val_indices]\n    \n    return df_train, df_val\n\n\ndef run(model_path=None, checkpoint=None):\n    torch.manual_seed(SEED)\n\n    # Initialize accelerator\n    accelerator = Accelerator()\n\n    df_train, df_val = load_data(df_train_text)\n    train_data = TransformerDataset(df_train)\n    val_data = TransformerDataset(df_val)\n\n    train_dataloader = torch.utils.data.DataLoader(\n        train_data, \n        batch_size=TRAIN_BATCH_SIZE,\n        drop_last=True\n    )\n\n    val_dataloader = torch.utils.data.DataLoader(\n        val_data, \n        batch_size=VAL_BATCH_SIZE,\n    )\n\n    # init model\n    if model_path:\n        model = get_model(model_path, checkpoint)\n    else:\n        accelerator.print('\\n-- Initializing Model')\n        model = TFDualHeadModel()\n    \n    criterion = getattr(torch.nn, CRITERION)()\n\n    # define the parameters to be optmized -\n    # - and add regularization\n    if FULL_FINETUNING:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = getattr(torch.optim, OPTIMIZER)(optimizer_parameters, lr=LR)\n\n    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, val_dataloader\n    )\n\n    num_training_steps = len(train_dataloader) * EPOCHS\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    #######################\n    # training & validation\n    accelerator.print(\"\\n-- Training\")\n\n    max_val_roc = float('-inf')\n    for epoch in range(EPOCHS):\n        progress_bar = tqdm(\n            range(len(train_dataloader)), \n            desc=\"Epoch \" + str(epoch),\n            disable=not accelerator.is_main_process\n        )\n\n        # Training\n        train_loss = 0\n        for step, batch in enumerate(train_dataloader):\n            # set model.eval() every time during training\n            model.train()\n            \n            # unpack the batch contents and push them to the DEVICE (cuda or cpu).\n            b_title_input_ids = batch[\"title\"]['input_ids']\n            b_title_attention_mask = batch[\"title\"]['attention_mask']\n            b_body_input_ids = batch[\"body\"]['input_ids']\n            b_body_attention_mask = batch[\"body\"]['attention_mask']\n            b_labels = batch['labels']\n\n            # clear accumulated gradients\n            optimizer.zero_grad()\n\n            # forward pass\n            logits = model(\n                b_title_input_ids,\n                b_title_attention_mask,\n                b_body_input_ids,\n                b_body_attention_mask\n            )\n\n            # calculate loss\n            loss = criterion(logits, b_labels)\n            train_loss += loss.item()\n\n            # backward pass\n            accelerator.backward(loss)\n\n            # update weights\n            optimizer.step()\n            \n            # update scheduler\n            scheduler.step()\n\n            progress_bar.update(1)\n            progress_bar.set_postfix({\"loss\": loss.item()})\n        \n        avg_train_loss = train_loss / len(train_dataloader)\n        accelerator.print('Training loss:', avg_train_loss)\n\n        # Validation\n        val_loss = 0\n        preds = []\n        labels = []\n        \n        # set model.eval() every time during evaluation\n        model.eval()\n        \n        for step, batch in enumerate(val_dataloader):\n            b_title_input_ids = batch[\"title\"]['input_ids']\n            b_title_attention_mask = batch[\"title\"]['attention_mask']\n            b_body_input_ids = batch[\"body\"]['input_ids']\n            b_body_attention_mask = batch[\"body\"]['attention_mask']\n            b_labels = batch['labels']\n\n            with torch.no_grad():\n                logits = model(\n                    b_title_input_ids,\n                    b_title_attention_mask,\n                    b_body_input_ids,\n                    b_body_attention_mask\n                )\n\n            loss = criterion(logits, b_labels)\n            val_loss += loss.item()\n\n            b_pred = torch.argmax(logits, dim=1)\n\n            preds.append(accelerator.gather(b_pred))\n            labels.append(accelerator.gather(b_labels))\n\n        preds = torch.cat(preds)[:len(val_data)]\n        labels = torch.cat(labels)[:len(val_data)]\n\n        avg_val_loss = val_loss / len(val_dataloader)\n        accelerator.print('Val loss:', avg_val_loss)\n        val_roc = roc_auc_score(labels, preds)\n        accelerator.print('Val roc-auc:', val_roc)\n\n        if val_roc > max_val_roc:\n            accelerator.print(f\"-- Best Model. Val loss: {max_val_roc} -> {val_roc}\")\n            max_val_roc = val_roc\n            if SAVE_BEST_MODEL:\n                accelerator.print(\"-- Saving model.\")\n                accelerator.wait_for_everyone()\n                unwrapped_model = accelerator.unwrap_model(model)\n                accelerator.save(\n                    {\n                        \"state_dict\": unwrapped_model.state_dict(), \n                        \"roc\": max_val_roc\n                    },\n                    TRAINED_MODELS_PATH\n                )","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:10:18.496034Z","iopub.execute_input":"2021-08-11T12:10:18.496451Z","iopub.status.idle":"2021-08-11T12:10:18.529564Z","shell.execute_reply.started":"2021-08-11T12:10:18.496416Z","shell.execute_reply":"2021-08-11T12:10:18.528239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    notebook_launcher(run())\nexcept ValueError:\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:10:19.178052Z","iopub.execute_input":"2021-08-11T12:10:19.178433Z","iopub.status.idle":"2021-08-11T12:13:23.189178Z","shell.execute_reply.started":"2021-08-11T12:10:19.178401Z","shell.execute_reply":"2021-08-11T12:13:23.186525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference & Submission","metadata":{}},{"cell_type":"code","source":"def predict(model, roc):\n    accelerator = Accelerator()\n    \n    test_data = TransformerDataset(df_test_text, is_test=True)\n    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=VAL_BATCH_SIZE)\n    \n    model, test_dataloader = accelerator.prepare(\n        model, test_dataloader\n    )\n    \n    preds = []\n    model.eval()\n    \n    print(\"\\n-- Predicting\")\n    tq = tqdm(test_dataloader, total=len(test_dataloader))     \n    for step, batch in enumerate(tq):\n        b_title_input_ids = batch[\"title\"]['input_ids']\n        b_title_attention_mask = batch[\"title\"]['attention_mask']\n        b_body_input_ids = batch[\"body\"]['input_ids']\n        b_body_attention_mask = batch[\"body\"]['attention_mask']\n\n        with torch.no_grad():\n            logits = model(\n                b_title_input_ids,\n                b_title_attention_mask,\n                b_body_input_ids,\n                b_body_attention_mask\n            )\n\n            b_pred = torch.argmax(logits, dim=1).detach().cpu().numpy()\n            preds.extend(b_pred)\n\n    urlids = df_test_text[\"urlid\"]\n    submission = pd.DataFrame({\"urlid\": urlids, \"label\": preds})\n    roc = round(roc * 100, 2)\n    submission.to_csv(f\"{TF_MODEL_PATH}_titlebody_roc{roc}.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:09:48.746102Z","iopub.status.idle":"2021-08-11T12:09:48.74662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, roc = get_model(TRAINED_MODELS_PATH)\npredict(model, roc)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T12:09:48.74781Z","iopub.status.idle":"2021-08-11T12:09:48.748324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}