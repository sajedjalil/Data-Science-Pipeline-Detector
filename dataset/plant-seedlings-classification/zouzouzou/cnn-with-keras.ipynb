{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"code","source":"import glob\nimport os\nfrom PIL import Image, ImageOps\nimport numpy as np\nfrom scipy.misc import imresize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e542f02f-2ba9-4eda-a521-55e12bf28178","collapsed":true,"_uuid":"03e7f1631f5f3adbbd04b3dfbbfc7bb9c9b90b8d"}},{"cell_type":"code","source":"z = glob.glob('../input/train/*/*.png')\nori_label = []\nori_imgs = []\nfor fn in z:\n    if fn[-3:] != 'png':\n        continue\n    ori_label.append(fn.split('/')[-2])\n    new_img = Image.open(fn)\n    ori_imgs.append(ImageOps.fit(new_img, (48, 48), Image.ANTIALIAS).convert('RGB'))\n    \n    ","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"a5dd64e1-3e32-42dc-a3ac-c03ca0e9e31c","_uuid":"4f0efe6ef8f57aa45e217f55bd379bbcb79846d2"}},{"cell_type":"code","source":"ori_imgs[1]","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"545b72a2-edd6-4b1c-acb6-93e407169ab2","_uuid":"3a3cda504c889472a7e5c16c95478b22a317a9ac"}},{"cell_type":"code","source":"imgs = np.array([np.array(im) for im in ori_imgs])\nimgs = imgs.reshape(imgs.shape[0], 48, 48, 3) / 255\nlb = LabelBinarizer().fit(ori_label)\nlabel = lb.transform(ori_label) ","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ca1dd2ef-f1aa-4670-92de-c39c481b8463","collapsed":true,"_uuid":"7d10dfdba4b460ad1e8b20407ea27ac454a7906c"}},{"cell_type":"code","source":"trainX, validX, trainY, validY = train_test_split(imgs, label, test_size=0.05, random_state=42)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"1de7b866-7102-41e8-a92b-86471c5f5706","collapsed":true,"_uuid":"557c146f2f9c6631acc2f2a907b9b49282a649ee"}},{"cell_type":"code","source":"from keras.layers import Dropout, Input, Dense, Activation,GlobalMaxPooling2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"cf5455a0-c6ef-4795-871f-3a2fff25a569","_uuid":"e1ed352922952c8dd83c1699e6308ca182ae4dd4"}},{"cell_type":"code","source":"IM_input = Input((48, 48, 3))\nIM = Conv2D(16, (3, 3))(IM_input)\nIM = BatchNormalization(axis = 3)(IM)\nIM = Activation('relu')(IM)\nIM = Conv2D(16, (3, 3))(IM)\nIM = BatchNormalization(axis = 3)(IM)\nIM = Activation('relu')(IM)\nIM = MaxPooling2D((2, 2), strides=(2, 2))(IM)\nIM = Conv2D(32, (3, 3))(IM)\nIM = BatchNormalization(axis = 3)(IM)\nIM = Activation('relu')(IM)\nIM = Conv2D(32, (3, 3))(IM)\nIM = BatchNormalization(axis = 3)(IM)\nIM = Activation('relu')(IM)\nIM = GlobalMaxPooling2D()(IM)\n\nIM = Dense(64, activation='relu')(IM)\nIM = Dropout(0.5)(IM)\nIM = Dense(32, activation='relu')(IM)\nIM = Dropout(0.5)(IM)\nIM = Dense(12, activation='softmax')(IM)\nmodel = Model(inputs=IM_input, outputs=IM)\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=1e-4), metrics=['acc'])","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"3aa8b322-1011-468b-b730-b8cd584576bf","_uuid":"664da8ae62cbeb72bb7b696c8351a62167248206"}},{"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler, EarlyStopping\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"e99dbd17-369c-4617-9d7e-a630f97d79d1","collapsed":true,"_uuid":"3bdc0043d3c74479763cffb1df6c67df9224ec9c"}},{"cell_type":"code","source":"batch_size = 64\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)\nearlystop = EarlyStopping(patience=10)\nmodelsave = ModelCheckpoint(\n    filepath='model.h5', save_best_only=True, verbose=1)\nmodel.fit(\n    trainX, trainY, batch_size=batch_size,\n    epochs=200, \n    validation_data=(validX, validY),\n    callbacks=[annealer, earlystop, modelsave]\n)\n","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"6f72071b-14cf-4e3f-845f-992be6f7f52c","scrolled":false,"_uuid":"3776b4ad6a904fa3c204d31e19402d9a2d76dee1"}},{"cell_type":"code","source":"z = glob.glob('../input/test/*.png')\ntest_imgs = []\nnames = []\nfor fn in z:\n    if fn[-3:] != 'png':\n        continue\n    names.append(fn.split('/')[-1])\n    new_img = Image.open(fn)\n    test_img = ImageOps.fit(new_img, (48, 48), Image.ANTIALIAS).convert('RGB')\n    test_imgs.append(test_img)\nmodel = load_model('model.h5')","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"1f953f1c-39b3-4160-89aa-e546719f2fef","collapsed":true,"_uuid":"20834884a92a67eb750e7d70350ca86585abfef3"}},{"cell_type":"code","source":"timgs = np.array([np.array(im) for im in test_imgs])\ntestX = timgs.reshape(timgs.shape[0], 48, 48, 3) / 255","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"42ee9ca8-ae2f-469e-b881-3962281f6967","_uuid":"b6bff74f20c089f41c3b4a68ccd10df6a48afd2b"}},{"cell_type":"code","source":"yhat = model.predict(testX)\ntest_y = lb.inverse_transform(yhat)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"4036ca95-2dd7-4bc2-83cc-49a61b5516cb","_uuid":"fb2e7a394c6de6daff7963689cec3783a7160846"}},{"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"eeec9d02-0100-48aa-b0e8-0e462b547b7c","collapsed":true,"_uuid":"4e8c0c56c92a518e984a2e8a392c437a4ccb4bc9"}},{"cell_type":"code","source":"df = pd.DataFrame(data={'file': names, 'species': test_y})\ndf_sort = df.sort_values(by=['file'])\ndf_sort.to_csv('results.csv', index=False)","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"ba5e4b1e-5e87-4648-9c3e-c6a68064c4fb","collapsed":true,"_uuid":"0058bfe0909c8d0fc881602a7edd9c7e16a597f0"}}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","name":"python","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python"}}}