{"nbformat_minor":1,"metadata":{"language_info":{"file_extension":".py","version":"3.6.3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"metadata":{"_uuid":"8ed8814047633de72a7112880bd46c460776d1d3","_cell_guid":"a0cb0f67-b40f-4611-b889-035c91ebb0c3"},"source":"# Plant Seedlings Segmentation with pure Computer Vision","cell_type":"markdown"},{"metadata":{"_uuid":"05dde1ddf47a7e42f7075700b90f2b9e8be3b975","_cell_guid":"67496b8d-27df-4498-a587-79051a5c1f35"},"source":"First of all, thanks for the popularity of this kernel. I hope it will help for you to create more accurate predictions","cell_type":"markdown"},{"source":"%matplotlib inline\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport cv2\nimport numpy as np\nfrom glob import glob\nimport seaborn as sns","metadata":{"collapsed":true,"_uuid":"f39f0c6a929d0db7d1aa48c2722cce5c37aa3b5d","_cell_guid":"a1767b4c-9ea3-4342-8ee5-56660856183b"},"outputs":[],"cell_type":"code","execution_count":1},{"source":"BASE_DATA_FOLDER = \"../input\"\nTRAin_DATA_FOLDER = os.path.join(BASE_DATA_FOLDER, \"train\")","metadata":{"collapsed":true,"_uuid":"622180f74f171af387a6f40fadd8f23b6806885a","_cell_guid":"c7a5628f-95de-419f-b850-eafe47b1e35d"},"outputs":[],"cell_type":"code","execution_count":2},{"metadata":{"_uuid":"d069eff7a882e41838282d7e10f5d97e835d4447","_cell_guid":"c10f40f2-f71e-4085-a744-42a142677558"},"source":"### Read images\nFirst, I'll just read all the images. The images are in BGR (Blue/Green/Red) format because OpenCV uses this.\n\nBtw... If you'd like to use RGB format, than you can use it, it won't effect the segmentation because we will use the HSV (Hue/Saturation/Value) color space for that.","cell_type":"markdown"},{"source":"images_per_class = {}\nfor class_folder_name in os.listdir(TRAin_DATA_FOLDER):\n    class_folder_path = os.path.join(TRAin_DATA_FOLDER, class_folder_name)\n    class_label = class_folder_name\n    images_per_class[class_label] = []\n    for image_path in glob(os.path.join(class_folder_path, \"*.png\")):\n        image_bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        images_per_class[class_label].append(image_bgr)","metadata":{"collapsed":true,"_uuid":"1d421d3a24167567cbc26d5f668acac290dc7b0f","_cell_guid":"1019fbd1-3adb-402c-864a-3030694a12d8"},"outputs":[],"cell_type":"code","execution_count":3},{"metadata":{"_uuid":"62c42371237dfd9f56d01a725bf69d4627642d1a","_cell_guid":"5b048546-3d90-4361-93de-f2a6172ff0df"},"source":"### Number of images per class","cell_type":"markdown"},{"source":"for key,value in images_per_class.items():\n    print(\"{0} -> {1}\".format(key, len(value)))","metadata":{"_uuid":"d3f0406443e03dab008abcd2aa315f14108e5814","_cell_guid":"b3a465ca-6600-46e2-a923-631eb4ec1b23"},"outputs":[],"cell_type":"code","execution_count":4},{"metadata":{"_uuid":"a4f0376c290e3efa6344f1a49e30ce580d45f17f","_cell_guid":"59db49a3-8d9f-4a86-b2f3-12ea2db779f4"},"source":"### Plot images\nPlot images so we can see what the input looks like","cell_type":"markdown"},{"source":"def plot_for_class(label):\n    nb_rows = 3\n    nb_cols = 3\n    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=(6, 6))\n\n    n = 0\n    for i in range(0, nb_rows):\n        for j in range(0, nb_cols):\n            axs[i, j].xaxis.set_ticklabels([])\n            axs[i, j].yaxis.set_ticklabels([])\n            axs[i, j].imshow(images_per_class[label][n])\n            n += 1        ","metadata":{"collapsed":true,"_uuid":"d908e6a36213ed4807f36ff771031e5c45e1e506","_cell_guid":"98e34b57-34de-4c04-8442-1b6e46a7e92d"},"outputs":[],"cell_type":"code","execution_count":5},{"source":"plot_for_class(\"Small-flowered Cranesbill\")","metadata":{"_uuid":"c186f23d145dbb2987b6c41d21372db87dbbed95","_cell_guid":"d4c11145-1017-47ca-9465-dff77a90937e"},"outputs":[],"cell_type":"code","execution_count":6},{"source":"plot_for_class(\"Maize\")","metadata":{"_uuid":"c33768fb8bb0e80726846f7ccef279a031d778eb","_cell_guid":"521a656a-377e-4555-a3ac-3b9e8849b47a","scrolled":false},"outputs":[],"cell_type":"code","execution_count":7},{"metadata":{"_uuid":"a4d9f5707e90e3a71dbb1ae65f424fc820210ce3","_cell_guid":"a8c70b81-a900-4c71-8fc5-c907600b5d7f"},"source":"### Preprocessing for the images:\n\nNow comes the interesting and fun part!\n\nI created separate functions so if you'd like to use these it is easier.\n\nIn the next block I'll explain what I am doing to make the segmentation happen.","cell_type":"markdown"},{"source":"def create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n\n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\ndef segment_plant(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\n\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp","metadata":{"collapsed":true,"_uuid":"a2f2e7898dbbc27369a5ca467556ed533a0a9c23","_cell_guid":"84b0f059-d103-4e96-9d79-d87c6229b0fe"},"outputs":[],"cell_type":"code","execution_count":8},{"metadata":{"_uuid":"83110ea07ecb54c83f1f40a6278327aafd6b34fd","_cell_guid":"d5f432e4-dab9-4361-b07f-b0518879e7da"},"source":"The `create_mask_for_plant` function: This function returns an image mask: Matrix with shape `(image_height, image_width)`. In this matrix there are only `0` and `1` values. The 1 values define the interesting part of the original image. But the question is...How do we create this mask?\n\nThis is a simple object detection problem, where we can use the color of the object.\n\nThe HSV color-space is suitable for color detection because with the Hue we can define the color and the saturation and value will define \"different kinds\" of the color. (For example it will detect the red, darker red, lighter red too). We cannot do this with the original BGR color space.\n\n![](https://www.mathworks.com/help/images/hsvcone.gif)\n\n*image from https://www.mathworks.com/help/images/convert-from-hsv-to-rgb-color-space.html*\n\nWe have to set a range, which color should be detected:\n\n    sensitivity = 35\n    lower_hsv = np.array([60 - sensitivity, 100, 50])\n    upper_hsv = np.array([60 + sensitivity, 255, 255])\n    \nAfter the mask is created with the `inRange` function, we can do a little *CV magic* (not close to magic, because this is almost the most basic thing in CV, but it is a cool buzzword, and this opertation is as awesome as simple it is) which is called *morphological operations* ([You can read more here](https://www.cs.auckland.ac.nz/courses/compsci773s1c/lectures/ImageProcessing-html/topic4.htm)).\n\nBasically with the *Close* operation we would like to keep the shape of the original objects (1 blobs on the mask image) but close the small holes. That way we can clarify our detection mask more.\n\n![](https://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/closebin.gif)\n\n*image from https://www.cs.auckland.ac.nz/courses/compsci773s1c/lectures/ImageProcessing-html/topic4.htm*\n\nAfter these steps we created the mask for the object.\n","cell_type":"markdown"},{"source":"# Test image to see the changes\nimage = images_per_class[\"Small-flowered Cranesbill\"][97]\n\nimage_mask = create_mask_for_plant(image)\nimage_segmented = segment_plant(image)\nimage_sharpen = sharpen_image(image_segmented)\n\nfig, axs = plt.subplots(1, 4, figsize=(20, 20))\naxs[0].imshow(image)\naxs[1].imshow(image_mask)\naxs[2].imshow(image_segmented)\naxs[3].imshow(image_sharpen)","metadata":{"_uuid":"b27217b1e5bbeacaca1dad627117d0a7a5953826","_cell_guid":"4064997e-2a20-49a6-9110-1863e2af5f4d","scrolled":true},"outputs":[],"cell_type":"code","execution_count":9},{"metadata":{"_uuid":"8d27e084e8f79bad6e191ce7562f5af71b0e7048","_cell_guid":"b1542194-f8a5-43b3-8a0a-5a3e5db6019d"},"source":"After this step we can see that the image on the right is more recognizable than the original image on the left.","cell_type":"markdown"},{"metadata":{"collapsed":true,"_uuid":"f6c3b077b7bd2c436c05d2ba2d0f1a62e8a2a914","_cell_guid":"31468800-4545-4761-91f5-9fdb643cda46"},"source":"----------------------------------------------","cell_type":"markdown"},{"metadata":{"_uuid":"5ed851b89b2aa39fcf135c854ba178d740f2ffea","_cell_guid":"489ed19d-5973-4b2b-b1de-4210faf11cb2"},"source":"From the mask image what we created (because we need that for the segmentation), we can extract some features. For example we can see how the area of the plant changes based on their classes.","cell_type":"markdown"},{"metadata":{"_uuid":"ddc5eb913130f3d15437c73e103d524a8f9ad6a2","_cell_guid":"91beb15f-bfff-40f0-88d6-05b4d5fa55ae"},"source":"Of course from the contours we can extract much more information than the area of the\ncontour and the number of components, but this is the one I would like to show you.\n\nAdditional read: https://en.wikipedia.org/wiki/Image_moment","cell_type":"markdown"},{"source":"def find_contours(mask_image):\n    return cv2.findContours(mask_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n\ndef calculate_largest_contour_area(contours):\n    if len(contours) == 0:\n        return 0\n    c = max(contours, key=cv2.contourArea)\n    return cv2.contourArea(c)\n\ndef calculate_contours_area(contours, min_contour_area = 250):\n    area = 0\n    for c in contours:\n        c_area = cv2.contourArea(c)\n        if c_area >= min_contour_area:\n            area += c_area\n    return area","metadata":{"collapsed":true,"_uuid":"0c2f88819cbd188b4ef275e6f5825ef981bca6e1","_cell_guid":"1348bdc1-86e9-4fa8-a79a-669546a73d46"},"outputs":[],"cell_type":"code","execution_count":10},{"source":"areas = []\nlarges_contour_areas = []\nlabels = []\nnb_of_contours = []\nimages_height = []\nimages_width = []\n\nfor class_label in images_per_class.keys():\n    for image in images_per_class[class_label]:\n        mask = create_mask_for_plant(image)\n        contours = find_contours(mask)\n        \n        area = calculate_contours_area(contours)\n        largest_area = calculate_largest_contour_area(contours)\n        height, width, channels = image.shape\n        \n        images_height.append(height)\n        images_width.append(width)\n        areas.append(area)\n        nb_of_contours.append(len(contours))\n        larges_contour_areas.append(largest_area)\n        labels.append(class_label)","metadata":{"collapsed":true,"_uuid":"00221628e47f5c7b31d520a6b9087e538d02c7af","_cell_guid":"ca8d916b-a09c-4a39-a896-6856a38532f7"},"outputs":[],"cell_type":"code","execution_count":11},{"source":"features_df = pd.DataFrame()\nfeatures_df[\"label\"] = labels\nfeatures_df[\"area\"] = areas\nfeatures_df[\"largest_area\"] = larges_contour_areas\nfeatures_df[\"number_of_components\"] = nb_of_contours\nfeatures_df[\"height\"] = images_height\nfeatures_df[\"width\"] = images_width","metadata":{"collapsed":true,"_uuid":"f6150ac38910b3534f969f34bae3b124eba8df65","_cell_guid":"3b4703f4-e585-4851-b003-04384b065cb4"},"outputs":[],"cell_type":"code","execution_count":12},{"source":"features_df.groupby(\"label\").describe()","metadata":{"_uuid":"3f00158f8f3f391a7a1c43f88d5b87f781ff3fac","_cell_guid":"b7d1986f-746c-4711-a569-c38af8d05c86","scrolled":true},"outputs":[],"cell_type":"code","execution_count":13},{"source":"","metadata":{"collapsed":true,"_uuid":"272c7e2991dac3c535acd0e9407f53e1975afb45","_cell_guid":"1a8bb81e-27ed-4656-82a4-2ad1ae194de4"},"outputs":[],"cell_type":"code","execution_count":null}],"nbformat":4}