{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Neccesary Imports**\n\n> This section just imports all the neccesary packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport itertools\nimport pandas as pd\nimport os\nimport math\nimport random\nimport cv2\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import resnet50\nfrom tensorflow.keras.applications import inception_v3\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, ReLU\nfrom tensorflow.keras.activations import swish\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.manifold import TSNE\nfrom imgaug import augmenters as iaa\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Global Variables**\n\n> In this section, we'll define some neccessary variables like paths and images sizes.\n\n---\n\n* This is done at the beginning so that they can be used and re-used later all throughout the notebook.\n\n* Another reason for this is so that the notebook can easily be ported to a script later on.\n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set some global variables\ntrain_dir = \"../input/plant-seedlings-classification/train/\"\ntest_dir = \"../input/plant-seedlings-classification/test/\"\nsave_dir = \"/kaggle/working/plant-seedlings-classification/train\"\ntarget_size = (224, 224)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1. Understanding the Dataset Distribution**\n\n> In this section, primarily, we will look at the **class distributions.**\n\n---\n\n* Knowing whether there are imbalances in the data, can help us take proper precautions during training.\n* And by taking proper precautions, we can make sure the model does not only learn the over-represented data. \n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get names of all the categories \ncategories = [category for category in sorted(os.listdir(train_dir))]\n\n# Get the number of images in each cateogry\nimages_per_category = [len(os.listdir(os.path.join(train_dir, category))) for category in categories]\n\n# Plot to see the distribution\nplt.figure(figsize=(24,12))\nsns.barplot(categories, images_per_category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Conclusions**\n\n---\n\n* Clearly, there is **class imbalance.**\n\n* The **lowest number of images (for a class) is 221** and the **highest number of images (for a class) is 654**\n\n* Although the **scale of imbalance is not very severe**, and **deep learning models are fairly robust** to such less-severe imbalances, we will still **use augmentation to balance the classes** and see what happens.\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# **2. Defining the DataLoader and Utilities Classes**\n\n> **In this section we'll be defining two Classes (`DataLoader` and `Utilities`) that will abstract away the notebook logic into two parts.** \n\n> **This leaves us with a clean implementation which is easy to follow, modify and debug.**\n\n---\n\n* `DataLoader` -> Builds a clean interface to handle all the preprocessing, balancing and loading of the dataset for visualization or training.\n\n* `Utilities`  -> Allows easy plotting of graphs and summarizing models after training them. \n\n* We will instantiate objects of both classes at the end of this section, so that they are ready to use later. \n\n* We will also visualize the effect of preprocessing by calling the `show_sample_images()` method\n\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataLoader():\n    \"\"\"\n    Args:\n    train_dir -- points to the training directory\n    test_dir -- points to the test directory\n    save_dir -- points to the path where augmented data should be stored\n    segmentation -- determines whether to apply segmentation during preprocessing or not\n    target_size -- the size to which each image in the dataset should be resized\n    \n    Returns:\n    An instance of itself\n    \"\"\"\n    \n    def __init__(self, **kwargs):\n        \n        self.train_dir = kwargs.get('train_dir')\n        self.test_dir = kwargs.get('test_dir')\n        self.save_dir = kwargs.get('save_dir')\n        self.segmentation = kwargs.get('segmentation')\n        self.target_size = kwargs.get('target_size')\n        categories = [category for category in sorted(os.listdir(self.train_dir))]\n        self.data_og = [self.preprocessing_pipeline(os.path.join(self.train_dir, category, img_path)) for category in categories for img_path in os.listdir(os.path.join(self.train_dir, category))]\n        if self.segmentation:\n            self.data_seg = [self.segmentation_pipeline(self.preprocessing_pipeline(os.path.join(self.train_dir, category, img_path))) for category in categories for img_path in os.listdir(os.path.join(self.train_dir, category))]\n            \n        \n    # Helper Function 1\n    # Create a binary mask for a given HSV range\n    def create_mask_for_plant(self, image):\n        image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        lower_hsv = np.array([25, 50, 50])\n        upper_hsv = np.array([95, 255, 255])\n        mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n        return mask\n\n    # Helper Function 2\n    # Use the binary mask to segment the image\n    def segment_plant(self, image):\n        mask = self.create_mask_for_plant(image)\n        output = cv2.bitwise_and(image, image, mask = mask)\n        return output\n\n    # Helper Function 3\n    # Sharpen the segmented image for contrast\n    def sharpen_image(self, image):\n        #image_median_blurred = cv2.medianBlur(image, 3)\n        image_sharp = cv2.bilateralFilter(image, 11, 11, 11) \n        #image_blurred = cv2.GaussianBlur(image_median_blurred, (0, 0), 3)\n        #image_sharp = cv2.addWeighted(image, 1.5, image_median_blurred, -0.5, 0)\n        return image_sharp\n\n    # Helper Function 4 \n    # Augment passed images\n    def augment_images(self, class_images):\n        seq = iaa.Sequential([\n            iaa.Fliplr(0.5),\n            iaa.Flipud(0.5),\n            iaa.Affine(rotate=(-45, 45)),\n            iaa.TranslateX(percent=(-0.1, 0.1)),\n            iaa.TranslateY(percent=(-0.1, 0.1))\n        ], random_order=True)\n\n        images_aug = seq(images = class_images)\n        return images_aug\n\n    # Helper Function 5\n    # Randomly sample images from a set of passed images\n    def random_unique_sampling(self, class_images, remainder):\n        random_unique_indices = random.sample(range(0, len(class_images)), remainder)\n        random_unique_images = [class_images[idx] for idx in random_unique_indices]\n        return random_unique_images\n        \n\n    def augmentation_pipeline(self, class_images, number_of_images):\n        \"\"\"Accepts a batch of images (of a single class) and returns a required number of augmented images\"\"\"\n\n        if number_of_images == 0:\n                return []\n\n        elif number_of_images >= len(class_images):\n            batches = math.floor(number_of_images / len(class_images))\n            remainder = number_of_images % len(class_images)\n            remainder_images = self.random_unique_sampling(class_images, remainder)\n            class_images = class_images * batches\n            class_images.extend(remainder_images)\n            images_aug = self.augment_images(class_images)\n            return images_aug\n\n        else:\n            assert number_of_images < len(class_images)\n            class_images = self.random_unique_sampling(class_images, number_of_images)\n            images_aug = self.augment_images(class_images)\n            return images_aug\n        \n    def preprocessing_pipeline(self, path):\n        \"\"\"Accepts a path and returns a processed image involving reading and resizing\"\"\"\n        image = cv2.resize(cv2.imread(path), self.target_size, interpolation = cv2.INTER_NEAREST)\n        return image\n\n\n    def segmentation_pipeline(self, image):\n        \"\"\"Accepts an image and returns a HSV segmented version of the image\"\"\"\n        image_segmented = self.segment_plant(image)\n        image_sharpen = self.sharpen_image(image_segmented)\n        return image_sharpen\n    \n    \n    def balance_dataset(self):\n        \"\"\"Create augmented data to balance classes from the passed training data path\"\"\"\n        \n        # Make a directory for augmented dataset\n        os.makedirs(self.save_dir, exist_ok=True)\n        \n        # Get categories\n        categories = [category for category in sorted(os.listdir(self.train_dir))]\n\n        # Get the maximum amount of images that exists in a class\n        max_in_class = max([len(os.listdir(os.path.join(self.train_dir, category))) for category in categories])\n\n        # Find out the augmentations required for each class\n        images_per_category = {category : len(os.listdir(os.path.join(self.train_dir, category))) for category in categories}\n\n        # Find out the augmentations required for each class\n        required_augmentations = dict(zip(categories,  [max_in_class - num_in_class for num_in_class in list(images_per_category.values())]))\n\n        # Augment each unbalanced class and save the new dataset to disk\n        # We preferring saving the data to disk\n        # Because we prefer to not hold large numpy arrays in the RAM\n        # This allows for large models to be loaded and trained on\n        # We use for loops here instead of list comprehensions for readiblity\n        for category in tqdm(categories):\n            try:\n                os.mkdir(os.path.join(self.save_dir, category))\n            except FileExistsError:\n                pass\n            class_images = list()\n\n            # Preprocessing and Augmentation\n            for img_path in sorted(os.listdir(os.path.join(self.train_dir, category))):\n                image = self.preprocessing_pipeline(os.path.join(self.train_dir, category, img_path))\n                if self.segmentation == True:\n                    image = self.segmentation_pipeline(image)\n                class_images.append(image)\n            augmented_images = self.augmentation_pipeline(class_images, required_augmentations[category])\n            class_images.extend(augmented_images)\n\n            # Writing the augmented data to disk\n            for image_number, class_image in enumerate(class_images):\n                cv2.imwrite(os.path.join(self.save_dir, category, \"{}.png\".format(image_number + 1)), class_image)\n        \n    def load_for_train(self, model):\n        \n        if model == \"resnet50\":\n            datagen = ImageDataGenerator(preprocessing_function = resnet50.preprocess_input, validation_split=0.15)\n            target_size = (224, 224)\n        elif model == \"inception_v3\":\n            datagen = ImageDataGenerator(preprocessing_function = inception_v3.preprocess_input, validation_split=0.15)\n            target_size = (299, 299)\n        elif model == 'vgg16':\n            datagen = ImageDataGenerator(preprocessing_function = vgg16.preprocess_input, validation_split=0.15)\n            target_size = (224, 224)\n        else:\n            sys.exit('Fatal Error: Invalid Model Requested.')\n\n\n        train_generator = datagen.flow_from_directory(\n                directory= os.path.join(self.save_dir),\n                target_size= self.target_size,\n                class_mode = \"categorical\",\n                batch_size=32,\n                shuffle=True,\n                subset='training'\n            )\n        \n        val_generator = datagen.flow_from_directory(\n                directory= os.path.join(self.save_dir),\n                target_size= self.target_size,\n                class_mode = 'categorical',\n                batch_size=32,\n                shuffle=False,\n                subset='validation'\n            )\n\n        return train_generator, val_generator\n        \n    def load_for_viz(self, model):\n        \n        if model == \"resnet50\":\n            datagen = ImageDataGenerator(preprocessing_function = resnet50.preprocess_input, validation_split=0.15)\n            target_size = (224, 224)\n        elif model == \"inception_v3\":\n            datagen = ImageDataGenerator(preprocessing_function = inception_v3.preprocess_input, validation_split=0.15)\n            target_size = (299, 299)\n        elif model == 'vgg16':\n            datagen = ImageDataGenerator(preprocessing_function = vgg16.preprocess_input, validation_split=0.15)\n            target_size = (224, 224)\n        else:\n            sys.exit('Fatal Error: Invalid Model Requested.')\n\n        generator = datagen.flow_from_directory(\n        directory= os.path.join(self.save_dir),\n        target_size= self.target_size,\n        batch_size=1,\n        class_mode=None,\n        shuffle=False\n        )\n        \n        categories = [category for category in sorted(os.listdir(self.train_dir))]\n        max_in_class = max([len(os.listdir(os.path.join(self.train_dir, category))) for category in categories])\n        categories_rep = list(itertools.chain.from_iterable(itertools.repeat(x, max_in_class) for x in categories))\n        data_df = pd.DataFrame(categories_rep, columns = [\"categories\"])\n        \n        return generator, data_df\n    \n    def load_for_inference(self):\n        \n        test_images = np.array([self.segmentation_pipeline(self.preprocessing_pipeline(os.path.join(self.test_dir, img_path))) for img_path in sorted(os.listdir(self.test_dir))])\n        filenames = [filename for filename in sorted(os.listdir(self.test_dir))]\n        return test_images, filenames\n        \n    \n    def show_sample_images(self):\n        categories = [category for category in sorted(os.listdir(self.train_dir))]\n        random_indices = random.sample(range(0, len(self.data_og)), 4)\n        \n        # Plot some sample images from the dataset\n        _, axs = plt.subplots(1, 4, figsize=(20, 20))\n        for i in range(4):\n            axs[i].imshow(self.data_og[random_indices[i]])\n        \n        # Plot segmented images if segmentation is True\n        if self.segmentation:\n            _, axs = plt.subplots(1, 4, figsize=(20, 20))\n            for i in range(4):\n                axs[i].imshow(self.data_seg[random_indices[i]])\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Utilities:\n    \"\"\"\n    Boilerplate code that can be re-used multiple times to plot training graphs, visualization plots, training summary.\n    \n    Args:\n    train_dir -- points to the training directory\n    \n    Returns:\n    An instance of itself\n    \"\"\"\n    \n    def __init__(self, train_dir, save_dir):\n        self.train_dir = train_dir\n        self.save_dir = save_dir\n        \n        \n    def plot_tSNE(self, data_df, base_model, generator, title):\n        \n        feature_vector = base_model.predict_generator(generator, 7848, verbose =1)\n        print('Extratced feature dimensionality: {}'.format(feature_vector.shape))\n        tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=1000)\n        tsne_results = tsne.fit_transform(feature_vector)\n        \n        data_df['tsne-2d-one'] = tsne_results[:,0]\n        data_df['tsne-2d-two'] = tsne_results[:,1]\n\n        plt.figure(figsize=(20,16))\n        plt.title(\"tSNE Visualization - \" + title)\n        sns.scatterplot(\n            x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n            hue=\"categories\",\n            palette=sns.color_palette(\"hls\", 12),\n            data=data_df,\n            legend=\"full\",\n            alpha=0.3\n        )\n        plt.savefig(os.path.join(os.path.normpath(self.save_dir + os.sep + os.pardir), title + \".png\"), dip=300, bbox_inches='tight')\n        \n    def summarize_model(self, history_model, model, val_generator):\n        categories = [category for category in sorted(os.listdir(self.train_dir))]\n        self.plot_curves(history_model)\n        self.plot_classification_metrics(categories, model, val_generator)\n\n\n    def plot_curves(self, history_model):\n        plt.style.use('seaborn')\n\n        # Summarize history for accuracy\n        plt.figure(1, figsize=(16, 10))\n        plt.plot(history_model.history['accuracy'])\n        plt.plot(history_model.history['val_accuracy'])\n        plt.title('Train and Validation Accuracy', fontsize = 16)\n        plt.ylabel('Accuracy', fontsize = 14)\n        plt.xlabel('Epoch', fontsize = 14)\n        plt.legend(['Train', 'Validation'], fontsize = 14)\n        plt.show()\n\n        # Summarize history for loss\n        plt.figure(2, figsize=(16, 10))\n        plt.plot(history_model.history['loss'])\n        plt.plot(history_model.history['val_loss'])\n        plt.title('Train and Validation Loss', fontsize = 16)\n        plt.ylabel('Loss', fontsize = 14)\n        plt.xlabel('Epoch', fontsize = 14)\n        plt.legend(['Train', 'Validation'], fontsize = 14)\n        plt.show()\n\n    def plot_classification_metrics(self, categories, model, val_generator):\n\n        predictiions = model.predict_generator(val_generator, 48)\n        y_pred = np.argmax(predictiions, axis=1)\n        cf_matrix = confusion_matrix(val_generator.classes, y_pred)\n        print('Classification Report')\n        print(classification_report(val_generator.classes, y_pred, target_names=categories))\n        plt.figure(figsize=(20,20))\n        sns.heatmap(cf_matrix, annot=True, xticklabels=categories, yticklabels=categories, cmap='Blues')\n        \n        \n    def infer(self, test_images, model):\n        categories = [category for category in sorted(os.listdir(self.train_dir))]\n        predictions = model.predict(test_images, batch_size = 32)\n        y_pred = np.argmax(predictions, axis = 1)\n        y_pred_categories = [categories[i] for i in y_pred]\n        return y_pred_categories\n        \n        \n    def make_csv(self, y_pred_categories, filenames, save_path):\n        inference = pd.DataFrame(zip(filenames, y_pred_categories), columns = ['Filename', \"Prediction\"])\n        inference.to_csv(os.path.join(save_path, \"inference.csv\"))\n        print('Saved inferences to disk!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate objects of both the classes\n# We will then use the methods of these two classes to handle various tasks\n\n# Initialize DataLoader\ndataloader = DataLoader(train_dir = train_dir, test_dir = test_dir, save_dir = save_dir, target_size = target_size, segmentation = True)\n\n# Initialize Utilities\nutils = Utilities(train_dir, save_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can also check out some images of the dataset we have generated\ndataloader.show_sample_images()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Balancing the Dataset using DataLoader\n\n> This section deals with balancing the dataset using data augmentation.\n\n---\n\n* The goal is to balance the dataset such that each class has as many images as any. \n\n* This means, we will augment each class to match 654 images of the maximum class (Loose-silky Bent)\n\n* This can be handled by calling the `balance_dataset()` method of `DataLoader` \n\n---\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Balance the dataset\ndataloader.balance_dataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can confirm that the dataset was balanced\n\n# Get names of all the categories \ncategories = [category for category in sorted(os.listdir(save_dir))]\n\n# Get the number of images in each cateogry\nimages_per_category = [len(os.listdir(os.path.join(save_dir, category))) for category in categories]\n\n# Plot to see the distribution\nplt.figure(figsize=(24,12))\nsns.barplot(categories, images_per_category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\n> We can verify that the dataset is now balanced!\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# 4. Combining Dimensionality Reduction and Visualization. \n\n> In this section we'll visualize the datset by using pretrained models as feature extractors\n\n---\n\n* Visualization is important because it allows us to determine the seperability of the classes that we have. \n\n* It gives us an intuition as to how easy or difficult the classification problem will be. \n\n* In fact, we can go a step further by using pretrained models as feature extractos and then compare if one model can achieve better seperability than the other.\n\n* We will use VGG, ResNet50 and InceptionV3 for our visualizations. \n---"},{"metadata":{},"cell_type":"markdown","source":"## a.   Visualization using InceptionV3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Load a generator for the data using the DataLoader Class\n# generator, data_df = dataloader.load_for_viz(model = \"inception_v3\")\n\n# # Define the InceptionV3 model\n# base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg', input_shape=(299, 299, 3))\n\n# # Plot a tSNE visualization using InceptionV3 as feature extractor using Utilities\n# utils.plot_tSNE(data_df, base_model, generator, title = 'Inception v3')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## b.   Visualization using VGG16"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Load a generator for the data using the DataLoader Class\n# generator, data_df = dataloader.load_for_viz(model = \"vgg16\")\n\n# # Define the VGG16 model\n# base_model = VGG16(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n\n# # Plot a tSNE visualization using VGG16 as feature extractor\n# utils.plot_tSNE(data_df, base_model, generator, title = 'VGG16')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## c.   Visualization using ResNet50"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Load a generator for the data using the DataLoader Class\n# generator, data_df = dataloader.load_for_viz(model = \"resnet50\")\n\n# # Define the ResNet50 model\n# base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n\n# # Plot a tSNE visualization using ResNet50 as feature extractor\n# utils.plot_tSNE(data_df, base_model, generator, title = 'ResNet50')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Conclusions**\n\n* All the three networks manage to give distinct clusters for most classes. \n* In Inception v3, some clusters are very clearly diffrentiable but others are somewhat of a mix. \n* VGG16, on the other hand, has more compact and distant clusters than Inception v3.\n* ResNet50 too, like VGG16 has compact clusters and we can make a hypothesis here that VGG16 and ResNet50 might outperform the Inception. \n* We may be very well proven wrong since the visualization is done on pretrained weights but after training, the scenario may change. \n* Overall, we can say with reasonable confidence that the dataset can surely be solved with a high degree of accuracy using the pre-trained models. "},{"metadata":{},"cell_type":"markdown","source":"# **5. Train Pre-Trained Models**\n\n\nWe will train the following models to see which gives the best validation and test accuracies\n\n* Inception v3\n* VGG16\n* ResNet50\n\n___\n\nWe will follow a standard procedure for training each of the networks in order to evaluate them farily:\n\n* We will unfreeze the approximately 50% of the network as opposed to freezing the entire network. \n* The reason for this is that the models are trained on the ImageNet which has lots of real life backgrounds whereas our dataset has essentially no background due to segmentation. \n* As a result, we need to give the network some space to adapt to the new images.\n* As a added advantage, it will also help speed up convergence. \n* We will \"slow cook\" the network with a very low learning rate so that large updates don't happen and end up destroying the knowledge stored in the weights. \n\n___\n"},{"metadata":{},"cell_type":"markdown","source":"## **1. InceptionV3**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load generators for the data using the DataLoader Class\ntrain_generator, val_generator = dataloader.load_for_train(model = \"inception_v3\")\n\n# Define callbacks\nmodel_save_path = '/kaggle/working/model_inceptionv3'\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure model for transfer learning\nbase_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg', input_shape=(299, 299, 3))\nx = base_model.output\nx = Dropout(0.5)(x)\npredictions = Dense(12, activation='softmax')(x)\nmodel = Model(inputs = base_model.input, outputs = predictions)\n\n# Freeze the earlier layers\nfor layer in model.layers[:152]:\n    layer.trainable = False\n    \n    \n# Compile the model    \nmodel.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model\nhistory_inception_v3 = model.fit_generator(train_generator,\n                      steps_per_epoch = 196,\n                      validation_data = val_generator,\n                      validation_steps = 48,\n                      epochs = 32,\n                      verbose = 1,\n                      callbacks = [reduce_lr, early_stop])\n\n# Save the model\nmodel.save(model_save_path)\n\n# Load the best model\nmodel = load_model(model_save_path)\n# Summarize the best model\nutils.summarize_model(history_inception_v3, model, val_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2. VGG16**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load generators for the data using the DataLoader Class\ntrain_generator, val_generator = dataloader.load_for_train(model = \"vgg16\")\n\n# Define callbacks\nmodel_save_path = '/kaggle/working/model_vgg16'\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure model for transfer learning\nbase_model = VGG16(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\nx = base_model.output\nx = Dropout(0.5)(x)\npredictions = Dense(12, activation='softmax')(x)\nmodel = Model(inputs = base_model.input, outputs = predictions)\n\n# Freeze the earlier layers\nfor layer in model.layers[:-11]:\n    layer.trainable = False\n    \n    \n# Compile the model    \nmodel.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model\nhistory_vgg16 = model.fit_generator(train_generator,\n                      steps_per_epoch = 196,\n                      validation_data = val_generator,\n                      validation_steps = 48,\n                      epochs = 32,\n                      verbose = 1,\n                      callbacks = [reduce_lr, early_stop])\n\n# Save the model\nmodel.save(model_save_path)\n\n# Load the best model\nmodel = load_model(model_save_path)\n# Summarize the best model\nutils.summarize_model(history_vgg16, model, val_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **3. ResNet50**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load generators for the data using the DataLoader Class\ntrain_generator, val_generator = dataloader.load_for_train(model = \"resnet50\")\n\n# Define callbacks\nmodel_save_path = '/kaggle/working/model_resent50'\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.00000001)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='min', restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configure model for transfer learning\nbase_model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\nx = base_model.output\nx = Dropout(0.5)(x)\npredictions = Dense(12, activation='softmax')(x)\nmodel = Model(inputs = base_model.input, outputs = predictions)\n\n# Freeze the earlier layers\nfor layer in model.layers[:81]:\n    layer.trainable = False\n    \n    \n# Compile the model    \nmodel.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n\n# Train the model\nhistory_resent50 = model.fit_generator(train_generator,\n                      steps_per_epoch = 196,\n                      validation_data = val_generator,\n                      validation_steps = 48,\n                      epochs = 32,\n                      verbose = 1,\n                      callbacks = [reduce_lr, early_stop])\n\n# Save the model\nmodel.save(model_save_path)\n\n# Load the best model\nmodel = load_model(model_save_path)\n# Summarize the best model\nutils.summarize_model(history_resent50, model, val_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Conclusion**\n\n___\n\n### On the models:\n\n* Even though all models give out roughly the same validation accuracy (~91%), an important point to note is the misclassifications between Black-grass and Loose Silky-bent. \n\n* The most \"confusion\" occurs between Loose Silky-bent and Black-grass for all models as can be seen from the confusion matrix plots as well as the classification reports. \n\n* However, VGG16 has the least amount of \"confusion\" between the two classes as evident from the plots and the report. \n\n* This confirms our initial hypothesis of VGG16 performing the best among all three models. \n\n* Perhaps this was because VGG16 is a shallower and simpler architecture than the other two which is why the other two ended up overfitting more instead of generalizing. \n\n* It can also be because VGG16 (as seen in the visualization earlier on) is able to distinctly form compact clusters in the 2D tSNE space.\n\n### On the preprocessing:\n\n* Regarding the segmentation vs a non-segmented approach, we come to the conclusion that an ensemble of the models trained on both kinds of data will achieve better resuls. \n\n* This inference comes after observing the activations of test images with both models. Some images are better picked up by the segmented approach, and some are better picked up with the non-segmented approach. \n\n___"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}