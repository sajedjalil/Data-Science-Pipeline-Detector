{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport imageio\nimport matplotlib.pyplot as plt\n\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Maximum\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras import regularizers\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize as imresize\nfrom tqdm import tqdm\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\nBATCH_SIZE = 16\nEPOCHS = 100\nRANDOM_STATE = 11\n\n\nCLASS = {\n    'Black-grass': 0,\n    'Charlock': 1,\n    'Cleavers': 2,\n    'Common Chickweed': 3,\n    'Common wheat': 4,\n    'Fat Hen': 5,\n    'Loose Silky-bent': 6,\n    'Maize': 7,\n    'Scentless Mayweed': 8,\n    'Shepherds Purse': 9,\n    'Small-flowered Cranesbill': 10,\n    'Sugar beet': 11\n}\n\nINV_CLASS = {\n    0: 'Black-grass',\n    1: 'Charlock',\n    2: 'Cleavers',\n    3: 'Common Chickweed',\n    4: 'Common wheat',\n    5: 'Fat Hen',\n    6: 'Loose Silky-bent',\n    7: 'Maize',\n    8: 'Scentless Mayweed',\n    9: 'Shepherds Purse',\n    10: 'Small-flowered Cranesbill',\n    11: 'Sugar beet'\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Exploration and Distribution**\n\nTo get an idea of training example distributions, I  checkout how many images are included in each class-specific folder."},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_IMG_PATH = \"../input/plant-seedlings-classification/train\"\nclasses = {}\nfor class_name in os.listdir(TRAIN_IMG_PATH):\n    classes[class_name] = len(os.listdir(os.path.join(TRAIN_IMG_PATH, class_name)))\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Visualization**\n\nLet try and visualize the data in a more easy way in a bar plot\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(classes.keys(), classes.values())\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CNN Implementation**\n\nNow that i know what am dealing with.I would love to build a network from the ground up, and train it from scratch\nI will be using two type of neural networks blocks that will make up the final model. The usual Convolutional Layer which is made up of a (optional) padding operation, the usual Conv2D, followed by batch-normalization and a LeakyReLU activation. Then the Dense Layer, which are applied towards the end of the network, is prepended with a dropout operation, and then BN and a specifiable activation (which in my case, would be tanh and softmax for the last two layers.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dense layers set\ndef dense_set(inp_layer, n, activation, drop_rate=0.):\n    dp = Dropout(drop_rate)(inp_layer)\n    dns = Dense(n)(dp)\n    bn = BatchNormalization(axis=-1)(dns)\n    act = Activation(activation=activation)(bn)\n    return act","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conv. layers set\ndef conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n    if zp_flag:\n        zp = ZeroPadding2D((1,1))(feature_batch)\n    else:\n        zp = feature_batch\n    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides)(zp)\n    bn = BatchNormalization(axis=3)(conv)\n    act = LeakyReLU(1/10)(bn)\n    return act","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I define a get_model function, which builds the model I would use for our CNN. We start with 51x51 input images, which is smaller than most of our original training images. This reduces the amount of computation required, but may lead to slightly poorer results. Then it is a series of conv layers stacked together, with MaxPooling layers in between, Towards the end, I flatten out the network, and switch to FC layers to perform the final classification, which is 12-dimensional as we have 12 classes.\n\nNotice that the notebook added a comment to switch from Adam to SGD optimizer after 50 epochs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# simple model \ndef get_model():\n    inp_img = Input(shape=(51, 51, 3))\n\n    # 51\n    conv1 = conv_layer(inp_img, 64, zp_flag=False)\n    conv2 = conv_layer(conv1, 64, zp_flag=False)\n    mp1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n    # 23\n    conv3 = conv_layer(mp1, 128, zp_flag=False)\n    conv4 = conv_layer(conv3, 128, zp_flag=False)\n    mp2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv4)\n    # 9\n    conv7 = conv_layer(mp2, 256, zp_flag=False)\n    conv8 = conv_layer(conv7, 256, zp_flag=False)\n    conv9 = conv_layer(conv8, 256, zp_flag=False)\n    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv9)\n    # 1\n    # dense layers\n    flt = Flatten()(mp3)\n    ds1 = dense_set(flt, 128, activation='tanh')\n    out = dense_set(ds1, 12, activation='softmax')\n\n    model = Model(inputs=inp_img, outputs=out)\n    \n    # The first 50 epochs are used by Adam opt.\n    # Then 30 epochs are used by SGD opt.\n    \n    mypotim = Adam(lr=0.5 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n    #mypotim = SGD(lr=0.25 * 1e-1, momentum=0.9, nesterov=True)\n    model.compile(loss='categorical_crossentropy',\n                   optimizer=mypotim,\n                   metrics=['accuracy'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here I defined a callback, which is called upon after each epoch. The particular callback defiend here does 2 things: 1. If the validation accuracy starts to plateau, I reduce our learning rate to perform the final few epochs, and 2. Save the current weights, if it has achieved better results than earlier weights. Notice that just as with our optimizer, the we will specify slightly different models for the earlier and later part of the training. In particular, when i switch to SGD, i would want to load the weights as the result of the Adam training rounds, and proceed from there."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_callbacks(filepath, patience=5):\n    lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=patience, verbose=1)\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [lr_reduce, msave]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(img, target):\n    callbacks = get_callbacks(filepath='../input/plant-weight/model_weight_SGD.hdf5', patience=5)\n    gmodel = get_model()\n    gmodel.load_weights(filepath='../input/plant-weight/model_weight_Adam.hdf5')\n    x_train, x_valid, y_train, y_valid = train_test_split(\n                                                        img,\n                                                        target,\n                                                        shuffle=True,\n                                                        train_size=0.8,\n                                                        random_state=RANDOM_STATE\n                                                        )\n    gen = ImageDataGenerator(\n            rotation_range=360.,\n            width_shift_range=0.3,\n            height_shift_range=0.3,\n            zoom_range=0.6,\n            horizontal_flip=True,\n            vertical_flip=True\n    )\n    gmodel.fit_generator(gen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n               steps_per_epoch=10*len(x_train)/BATCH_SIZE,\n               epochs=EPOCHS,\n               verbose=1,\n               shuffle=True,\n               validation_data=(x_valid, y_valid),\n               callbacks=callbacks)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(img, label):\n    gmodel = get_model()\n    gmodel.load_weights(filepath='../input/plant-weight/model_weight_SGD.hdf5')\n    prob = gmodel.predict(img, verbose=1)\n    pred = prob.argmax(axis=-1)\n    sub = pd.DataFrame({\"file\": label,\n                         \"species\": [INV_CLASS[p] for p in pred]})\n    sub.to_csv(\"sub.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Resize all image to 51x51 \ndef img_reshape(img):\n    img = imresize(img, (51, 51, 3))\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get image tag\ndef img_label(path):\n    return str(str(path.split('/')[-1]))\n\n# get plant class on image\ndef img_class(path):\n    return str(path.split('/')[-2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill train and test dict\ndef fill_dict(paths, some_dict):\n    text = ''\n    if 'train' in paths[0]:\n        text = 'Start fill train_dict'\n    elif 'test' in paths[0]:\n        text = 'Start fill test_dict'\n\n    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n        img = imageio.imread(p)\n        img = img_reshape(img)\n        some_dict['image'].append(img)\n        some_dict['label'].append(img_label(p))\n        if 'train' in paths[0]:\n            some_dict['class'].append(img_class(p))\n\n    return some_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read image from dir. and fill train and test dict\ndef reader():\n    file_ext = []\n    train_path = []\n    test_path = []\n\n    for root, dirs, files in os.walk('../input'):\n        if dirs != []:\n            print('Root:\\n'+str(root))\n            print('Dirs:\\n'+str(dirs))\n        else:\n            for f in files:\n                ext = os.path.splitext(str(f))[1][1:]\n\n                if ext not in file_ext:\n                    file_ext.append(ext)\n\n                if 'train' in root:\n                    path = os.path.join(root, f)\n                    train_path.append(path)\n                elif 'test' in root:\n                    path = os.path.join(root, f)\n                    test_path.append(path)\n    train_dict = {\n        'image': [],\n        'label': [],\n        'class': []\n    }\n    test_dict = {\n        'image': [],\n        'label': []\n    }\n\n    train_dict = fill_dict(train_path, train_dict)\n    test_dict = fill_dict(test_path, test_dict)\n    return train_dict, test_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def main():\n    train_dict, test_dict = reader()\n    X_train = np.array(train_dict['image'])\n    y_train = to_categorical(np.array([CLASS[l] for l in train_dict['class']]))\n\n    X_test = np.array(test_dict['image'])\n    label = test_dict['label']\n    \n    # I do not recommend trying to train the model on a kaggle.\n    train_model(X_train, y_train)\n    test_model(X_test, label)\n    \n\nif __name__=='__main__':\n    main()  \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}