{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressinAW21Xg Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-17T03:54:03.137533Z","iopub.execute_input":"2022-03-17T03:54:03.138042Z","iopub.status.idle":"2022-03-17T03:54:03.164703Z","shell.execute_reply.started":"2022-03-17T03:54:03.13795Z","shell.execute_reply":"2022-03-17T03:54:03.164009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# import libraries","metadata":{}},{"cell_type":"code","source":"import gc\nimport json\nimport math\nimport string\nimport pickle\nimport warnings\nimport spacy\nimport random\nimport itertools\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_rows',20)\npd.set_option('display.max_columns',500)\npd.set_option('display.width',1000)\n\nfrom sklearn.metrics import f1_score\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader,Dataset\nfrom sklearn.model_selection import train_test_split\nimport tokenizers \nimport transformers\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom transformers import models\nfrom transformers import AutoTokenizer, AutoConfig, TFAutoModel\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:13:34.857953Z","iopub.execute_input":"2022-03-17T05:13:34.858508Z","iopub.status.idle":"2022-03-17T05:13:34.989964Z","shell.execute_reply.started":"2022-03-17T05:13:34.858472Z","shell.execute_reply":"2022-03-17T05:13:34.989198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/train.csv')\ntest=pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/test.csv')\nfeatures=pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/features.csv')\npatient_notes=pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv')\nsample_submission=pd.read_csv('/kaggle/input/nbme-score-clinical-patient-notes/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.214195Z","iopub.execute_input":"2022-03-17T05:08:15.214812Z","iopub.status.idle":"2022-03-17T05:08:15.547587Z","shell.execute_reply.started":"2022-03-17T05:08:15.214779Z","shell.execute_reply":"2022-03-17T05:08:15.546819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# config","metadata":{}},{"cell_type":"markdown","source":"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining.","metadata":{}},{"cell_type":"code","source":"# ---------- Model ---------- \nMODEL_NAME = 'microsoft/deberta-base'\nTOKENIZER_PATH = \"microsoft/deberta-base_tokenizer\"\nMAX_LEN = 512\n\n# ---------- Training ----------\nBATCH_SIZE = 8\nEPOCHS = 10\nLEARNING_RATE = 2e-5\nCLIP_NORM = 1000\n\n# ---------- Dataset ----------\nseed=42\nn_fold=5\ntrn_fold=[0, 1, 2, 3, 4]\n\ndebug=False\n\nif debug:\n    EPOCHS = 5\n    trn_fold = [0]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.548993Z","iopub.execute_input":"2022-03-17T05:08:15.549259Z","iopub.status.idle":"2022-03-17T05:08:15.557671Z","shell.execute_reply.started":"2022-03-17T05:08:15.549223Z","shell.execute_reply":"2022-03-17T05:08:15.556999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ast\ntrain['annotation'] = train['annotation'].apply(ast.literal_eval) # Construct an object from a string\ntrain['location'] = train['location'].apply(ast.literal_eval) # Construct an object from a string\nprint(f\"train.shape: {train.shape}\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.558807Z","iopub.execute_input":"2022-03-17T05:08:15.559099Z","iopub.status.idle":"2022-03-17T05:08:15.776981Z","shell.execute_reply.started":"2022-03-17T05:08:15.559065Z","shell.execute_reply":"2022-03-17T05:08:15.77631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.778983Z","iopub.execute_input":"2022-03-17T05:08:15.779435Z","iopub.status.idle":"2022-03-17T05:08:15.788962Z","shell.execute_reply.started":"2022-03-17T05:08:15.7794Z","shell.execute_reply":"2022-03-17T05:08:15.788203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.790481Z","iopub.execute_input":"2022-03-17T05:08:15.790728Z","iopub.status.idle":"2022-03-17T05:08:15.800951Z","shell.execute_reply.started":"2022-03-17T05:08:15.790695Z","shell.execute_reply":"2022-03-17T05:08:15.799995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_notes.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.802308Z","iopub.execute_input":"2022-03-17T05:08:15.802693Z","iopub.status.idle":"2022-03-17T05:08:15.813615Z","shell.execute_reply.started":"2022-03-17T05:08:15.802656Z","shell.execute_reply":"2022-03-17T05:08:15.81286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging","metadata":{"execution":{"iopub.status.busy":"2022-03-15T16:20:31.748683Z","iopub.execute_input":"2022-03-15T16:20:31.749197Z","iopub.status.idle":"2022-03-15T16:20:31.762756Z","shell.execute_reply.started":"2022-03-15T16:20:31.749158Z","shell.execute_reply":"2022-03-15T16:20:31.761906Z"}}},{"cell_type":"code","source":"train=train.merge(features,on=['feature_num','case_num'],how='left')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.815173Z","iopub.execute_input":"2022-03-17T05:08:15.815702Z","iopub.status.idle":"2022-03-17T05:08:15.836701Z","shell.execute_reply.started":"2022-03-17T05:08:15.815668Z","shell.execute_reply":"2022-03-17T05:08:15.835974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=train.merge(patient_notes,on=['case_num','pn_num'],how='left')\ntrain.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.837886Z","iopub.execute_input":"2022-03-17T05:08:15.838147Z","iopub.status.idle":"2022-03-17T05:08:15.865152Z","shell.execute_reply.started":"2022-03-17T05:08:15.838094Z","shell.execute_reply":"2022-03-17T05:08:15.864477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['annotation_length'] = train['annotation'].apply(len)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.866402Z","iopub.execute_input":"2022-03-17T05:08:15.866802Z","iopub.status.idle":"2022-03-17T05:08:15.888251Z","shell.execute_reply.started":"2022-03-17T05:08:15.866767Z","shell.execute_reply":"2022-03-17T05:08:15.887561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['pn_history'][5910]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.891065Z","iopub.execute_input":"2022-03-17T05:08:15.891637Z","iopub.status.idle":"2022-03-17T05:08:15.898672Z","shell.execute_reply.started":"2022-03-17T05:08:15.8916Z","shell.execute_reply":"2022-03-17T05:08:15.89782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx=5910\n\nlocations = train.loc[idx,'location']\npn_history= train.loc[idx,'pn_history']\n\nstart_pos = []\nend_pos = []\nfor location in locations:\n    for loc in [s.split() for s in location.split(';')]:\n        start_pos.append(int(loc[0]))\n        end_pos.append(int(loc[1]))\n\n\nents = []\nfor i in range(len(start_pos)):\n    ents.append({\n        'start': int(start_pos[i]), \n        'end' : int(end_pos[i]),\n        \"label\" : \"Annotation\"\n    })\ndoc = {\n    'text' : pn_history,\n    \"ents\" : ents\n}\n\ncolors = {\"Annotation\": \"linear-gradient(0deg, #888, #eeaaaa)\"} \noptions = {\"colors\": colors}\nspacy.displacy.render(doc, style=\"ent\", options=options , manual=True, jupyter=True);","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.899882Z","iopub.execute_input":"2022-03-17T05:08:15.900521Z","iopub.status.idle":"2022-03-17T05:08:15.912284Z","shell.execute_reply.started":"2022-03-17T05:08:15.900484Z","shell.execute_reply":"2022-03-17T05:08:15.911589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"locations","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.913655Z","iopub.execute_input":"2022-03-17T05:08:15.913897Z","iopub.status.idle":"2022-03-17T05:08:15.923918Z","shell.execute_reply.started":"2022-03-17T05:08:15.913864Z","shell.execute_reply":"2022-03-17T05:08:15.923158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train,test=train_test_split(train[['pn_history','feature_text','annotation_length','location']],\n                           test_size=0.2,\n                           random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.925499Z","iopub.execute_input":"2022-03-17T05:08:15.92598Z","iopub.status.idle":"2022-03-17T05:08:15.935908Z","shell.execute_reply.started":"2022-03-17T05:08:15.925944Z","shell.execute_reply":"2022-03-17T05:08:15.935035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.936776Z","iopub.execute_input":"2022-03-17T05:08:15.936958Z","iopub.status.idle":"2022-03-17T05:08:15.947702Z","shell.execute_reply.started":"2022-03-17T05:08:15.936936Z","shell.execute_reply":"2022-03-17T05:08:15.94698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['feature_text'][1]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.948911Z","iopub.execute_input":"2022-03-17T05:08:15.949757Z","iopub.status.idle":"2022-03-17T05:08:15.956934Z","shell.execute_reply.started":"2022-03-17T05:08:15.94972Z","shell.execute_reply":"2022-03-17T05:08:15.956209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['pn_history'][1]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.95825Z","iopub.execute_input":"2022-03-17T05:08:15.958996Z","iopub.status.idle":"2022-03-17T05:08:15.965586Z","shell.execute_reply.started":"2022-03-17T05:08:15.958961Z","shell.execute_reply":"2022-03-17T05:08:15.964744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.save_pretrained(f'{TOKENIZER_PATH}')\n\nconfig = AutoConfig.from_pretrained(MODEL_NAME)\nconfig.save_pretrained(f'{TOKENIZER_PATH}')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:15.967007Z","iopub.execute_input":"2022-03-17T05:08:15.967725Z","iopub.status.idle":"2022-03-17T05:08:18.020786Z","shell.execute_reply.started":"2022-03-17T05:08:15.967688Z","shell.execute_reply":"2022-03-17T05:08:18.019975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# preprocess the data","metadata":{}},{"cell_type":"code","source":"# ------------------------- prepare_location ------------------------------\ndef prepare_location(locations: str):\n    \"\"\"\n    This function returns list of tuples of locations\n    \"\"\"\n    location_tuple_list = []\n    for location in locations:\n        for loc in [s.split() for s in location.split(';')]:\n            start, end = int(loc[0]), int(loc[1])\n            location_tuple_list.append((start, end))\n    \n    return location_tuple_list\n# ------------------------- prepare_input ------------------------------\ndef prepare_input(pn_history: str, feature_text: str):\n    \"\"\"\n    This function tokenizes pn_history and feature text and\n    returns numpy array of input_ids and attention_masks\n    \"\"\"\n    tokens = tokenizer(\n        pn_history,\n        feature_text,\n        max_length=MAX_LEN,\n        padding=\"max_length\",\n        add_special_tokens=True,\n    )\n    \n    input_ids = tokens['input_ids']\n    attention_mask = tokens[\"attention_mask\"]\n    return (np.array(input_ids), np.array(attention_mask))\n# ------------------------- prepare_labels ------------------------------\n# Thanks yasufuminakama \n# https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\ndef prepare_labels(pn_history, annotation_length, location_list):\n    \"\"\"\n    This function creates labels with are vectors of zeros (no entity)\n    and ones (entity)\n    \"\"\"\n    tokenized = tokenizer(\n        pn_history,\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding=\"max_length\",\n        return_offsets_mapping=True\n    )\n    offset_mapping = tokenized[\"offset_mapping\"]\n    label = np.zeros(len(offset_mapping))\n    if annotation_length != 0:\n        locations = prepare_location(location_list)\n        for location in locations:\n            start_idx, end_idx = -1, -1\n            start, end = location\n            for idx in range(len(offset_mapping)):\n                if (start_idx == -1) & (start < offset_mapping[idx][0]):\n                    start_idx = idx - 1\n                if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n                    end_idx = idx + 1\n            if start_idx == -1:\n                start_idx = end_idx\n            if (start_idx != -1) & (end_idx != -1):\n                label[start_idx:end_idx] = 1\n            \n    return np.array(label)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:18.023309Z","iopub.execute_input":"2022-03-17T05:08:18.023561Z","iopub.status.idle":"2022-03-17T05:08:18.035796Z","shell.execute_reply.started":"2022-03-17T05:08:18.023526Z","shell.execute_reply":"2022-03-17T05:08:18.035127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Dataset_generator(dataframe: pd.DataFrame):\n    def arg_generator():\n        pn_history = dataframe[\"pn_history\"].values\n        feature_text = dataframe[\"feature_text\"].values\n        annotation_length = dataframe['annotation_length'].values\n        location = dataframe['location'].values\n\n        for i in range(len(dataframe)):\n            inputs, masks = prepare_input(pn_history[i], feature_text[i])\n            labels = prepare_labels(pn_history[i], annotation_length[i], location[i])\n            yield (inputs, masks), labels\n    return arg_generator","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:18.037252Z","iopub.execute_input":"2022-03-17T05:08:18.037893Z","iopub.status.idle":"2022-03-17T05:08:18.047974Z","shell.execute_reply.started":"2022-03-17T05:08:18.037855Z","shell.execute_reply":"2022-03-17T05:08:18.047164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_training = tf.data.Dataset.from_generator(\n        Dataset_generator(train),\n        output_signature=(\n            (\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"inputs\"),\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"attention_masks\"),\n            ),\n            tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"labels\"),\n        )\n    )\nds_training = ds_training.batch(BATCH_SIZE)\n\nds_valid = tf.data.Dataset.from_generator(\n        Dataset_generator(test),\n        output_signature=(\n            (\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"inputs\"),\n                tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"attention_masks\"),\n            ),\n            tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"labels\"),\n        )\n    )\n\nds_valid = ds_valid.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:18.049468Z","iopub.execute_input":"2022-03-17T05:08:18.049958Z","iopub.status.idle":"2022-03-17T05:08:18.091689Z","shell.execute_reply.started":"2022-03-17T05:08:18.049905Z","shell.execute_reply":"2022-03-17T05:08:18.091146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save= tf.keras.callbacks.ModelCheckpoint(\n    './model_deberta.h5', \n    save_best_only = True, \n    save_weights_only = False,\n    monitor = 'val_loss', \n    mode = 'min', verbose = 1\n)\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    min_delta=1e-5, \n    patience=5, \n    verbose=1,\n    mode='auto', \n    restore_best_weights=True\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.5, \n    patience=2, \n    mode='auto', \n    min_delta=0.001,\n    verbose = 1\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:08:18.144021Z","iopub.execute_input":"2022-03-17T05:08:18.144313Z","iopub.status.idle":"2022-03-17T05:08:18.151789Z","shell.execute_reply.started":"2022-03-17T05:08:18.144286Z","shell.execute_reply":"2022-03-17T05:08:18.151126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics\nclass F1Score(tf.keras.metrics.Metric):\n    def __init__(self, name='f1', **kwargs):\n        super(F1Score, self).__init__(name=name, **kwargs)\n        self.f1 = tfa.metrics.F1Score(num_classes=2, average='micro', threshold=0.50)\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_true = tf.reshape(y_true, (-1,MAX_LEN))\n        y_pred = tf.reshape(y_pred, (-1,MAX_LEN))\n        self.f1.update_state(y_true, y_pred)\n        \n    def reset_state(self):\n        self.f1.reset_state()\n    \n    def result(self):\n        return self.f1.result()\n    \nmetrics = [\n    F1Score(), \n    tf.keras.metrics.Recall(thresholds=[0.5]), \n    tf.keras.metrics.Precision(thresholds=[0.5])\n]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:13:39.137537Z","iopub.execute_input":"2022-03-17T05:13:39.138079Z","iopub.status.idle":"2022-03-17T05:13:39.158422Z","shell.execute_reply.started":"2022-03-17T05:13:39.13804Z","shell.execute_reply":"2022-03-17T05:13:39.157777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    input_tokens=tf.keras.layers.Input(shape=(MAX_LEN,),dtype=tf.int32)\n    attention_mask=tf.keras.layers.Input(shape=(MAX_LEN,),dtype=tf.int32)\n    \n    config=AutoConfig.from_pretrained(MODEL_NAME,output_hiddin_states=True)\n    backbone=TFAutoModel.from_pretrained(MODEL_NAME,config=config)\n    \n    out=backbone(input_tokens,attention_mask=attention_mask)[0]\n    out=tf.keras.layers.Dropout(0.2)(out)\n    out=tf.keras.layers.Dense(1,activation='sigmoid')(out)\n    \n    return tf.keras.Model(inputs=[input_tokens,attention_mask],outputs=out)\n\nmodel=create_model()\nmodel.summary()\noptimizer = tf.keras.optimizers.Adam(LEARNING_RATE, clipnorm=CLIP_NORM)\nloss = tf.keras.losses.BinaryCrossentropy(reduction=\"none\")\nmodel.compile(optimizer=optimizer,loss=loss,metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:13:44.688233Z","iopub.execute_input":"2022-03-17T05:13:44.688845Z","iopub.status.idle":"2022-03-17T05:13:49.872582Z","shell.execute_reply.started":"2022-03-17T05:13:44.688804Z","shell.execute_reply":"2022-03-17T05:13:49.871798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"his=model.fit(ds_training,epochs=EPOCHS,validation_data=ds_valid,callbacks=[model_save,early_stop,reduce_lr])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\ntest = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ntest.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"his=model.fit(ds_training,epochs=EPOCHS,validation_data=ds_valid,callbacks=[model_save,early_stop,reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:16:31.270297Z","iopub.execute_input":"2022-03-17T05:16:31.270547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the generator which yields inputs for test dataset\ndef Dataset_generator_test(dataframe: pd.DataFrame):\n    def arg_generator_test():\n        pn_history = dataframe[\"pn_history\"].values\n        feature_text = dataframe[\"feature_text\"].values\n        \n        for i in range(len(dataframe)):\n            inputs, masks = prepare_input(pn_history[i], feature_text[i])\n            labels = prepare_labels(pn_history[i], 0, '') # just to build BatchDataset  \n            yield (inputs, masks),labels\n    return arg_generator_test\n\n\n\nds_test = tf.data.Dataset.from_generator(\n        Dataset_generator_test(test),\n         output_signature=(\n                    (\n                        tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"inputs\"),\n                        tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"attention_masks\"),\n                    ),\n                    tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.dtypes.int32, name=\"labels\"),\n                )\n            )\nds_test = ds_test.batch(BATCH_SIZE)\n\nidxx=0\nfor dst in ds_test.take(1):\n    inputs_masks,labels = dst # ignore labels\n    inputs_ids=inputs_masks[0]\n    attention_masks=inputs_masks[1]\n    print(\"inputs_ids shape=\",inputs_ids.shape)\n    print(\"-----------------------------------------------------------\")\n    print(\"attention_masks shape=\",attention_masks.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks yasufuminakama \n# https://www.kaggle.com/yasufuminakama/nbme-deberta-base-baseline-train\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npreds = model.predict(ds_test)\npreds = preds.reshape(len(test), MAX_LEN)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"char_probs = get_char_probs(test['pn_history'].values, preds, tokenizer)\nresults = get_results(char_probs, th=0.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}