{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport pdb\nimport torch\nfrom torch import cuda\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_fscore_support\nimport datasets\nfrom functools import partial\nfrom ast import literal_eval\nfrom datetime import datetime\nimport gc\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {'model_name': '/kaggle/input/deberta-v3-base/deberta-v3-base/',\n         'max_length': 512,\n         'train_batch_size':4,\n         'valid_batch_size':8,\n         'epochs':3,\n         'learning_rate':2e-05,\n         'max_grad_norm':10,\n          'warmup':0.1,\n          \"grad_acc\":8,\n          \"model_save_path\":\"deberta-trained\",\n          \"folds\":5,\n          \"seed\":42,\n          'num_proc' : 2,\n          'dropout':0.2,\n          \"no_unlabelled\":1000,\n         'device': 'cuda' if cuda.is_available() else 'cpu'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/features.csv\")\ndf_patients = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/patient_notes.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/nbme-score-clinical-patient-notes/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patients.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patients['pn_num'].nunique(), df_patients['case_num'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patients.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path / convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in [\n    \"tokenization_deberta_v2.py\",\n    \"tokenization_deberta_v2_fast.py\",\n    \"deberta__init__.py\",\n]:\n    if str(filename).startswith(\"deberta\"):\n        filepath = deberta_v2_path / str(filename).replace(\"deberta\", \"\")\n    else:\n        filepath = deberta_v2_path / filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir / filename, filepath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained(config['model_name'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_patients.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_process_data(df_train):\n    print(f\"before converting annotations of type :{type(df_train.annotation[0])}, {df_train.annotation[0]}, location of type: {type(df_train.location[0])}, {df_train.location[0]}\")\n    df_train['anno_list'] = [literal_eval(x) for x in df_train.annotation]\n    df_train['loc_list'] = [literal_eval(x) for x in df_train.location]\n    print(f\"after converting annotations of type :{type(df_train.annotation[0])}, {df_train.annotation[0]}, location of type: {type(df_train.location[0])}, {df_train.location[0]}\")\n    print(f\"column names of df_train : {df_train.columns}\")\n    merged = df_train.merge(df_patients, how='left')\n    print(f\"column names of df_train after merging with patietns: {merged.columns}\")\n    merged = merged.merge(df_features, how='left')\n    print(f\"column names of df_train after merging with features: {merged.columns}\")\n    return merged","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = pre_process_data(df_train)\nmerged.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# incorrect annotations\nmerged.loc[338, \"anno_list\"] =  '[\"father heart attack\"]'\nmerged.loc[338, \"loc_list\"] =  '[\"764 783\"]'\n\nmerged.loc[621, \"anno_list\"] =  '[\"for the last 2-3 months\", \"over the last 2 months\"]'\nmerged.loc[621, \"loc_list\"] =  '[\"77 100\", \"398 420\"]'\n\nmerged.loc[655, \"anno_list\"] =  '[\"no heat intolerance\", \"no cold intolerance\"]'\nmerged.loc[655, \"loc_list\"] =  '[\"285 292;301 312\", \"285 287;296 312\"]'\n\nmerged.loc[1262, \"anno_list\"] =  '[\"mother thyroid problem\"]'\nmerged.loc[1262, \"loc_list\"] =  '[\"551 557;565 580\"]'\n\nmerged.loc[1265, \"anno_list\"] =  '[\\'felt like he was going to \"pass out\"\\']'\nmerged.loc[1265, \"loc_list\"] =  '[\"131 135;181 212\"]'\n\nmerged.loc[1396, \"anno_list\"] =  '[\"stool , with no blood\"]'\nmerged.loc[1396, \"loc_list\"] =  '[\"259 280\"]'\n\nmerged.loc[1591, \"anno_list\"] =  '[\"diarrhoe non blooody\"]'\nmerged.loc[1591, \"loc_list\"] =  '[\"176 184;201 212\"]'\n\nmerged.loc[1615, \"anno_list\"] =  '[\"diarrhea for last 2-3 days\"]'\nmerged.loc[1615, \"loc_list\"] =  '[\"249 257;271 288\"]'\n\nmerged.loc[1664, \"anno_list\"] =  '[\"no vaginal discharge\"]'\nmerged.loc[1664, \"loc_list\"] =  '[\"822 824;907 924\"]'\n\nmerged.loc[1714, \"anno_list\"] =  '[\"started about 8-10 hours ago\"]'\nmerged.loc[1714, \"loc_list\"] =  '[\"101 129\"]'\n\nmerged.loc[1929, \"anno_list\"] =  '[\"no blood in the stool\"]'\nmerged.loc[1929, \"loc_list\"] =  '[\"531 539;549 561\"]'\n\nmerged.loc[2134, \"anno_list\"] =  '[\"last sexually active 9 months ago\"]'\nmerged.loc[2134, \"loc_list\"] =  '[\"540 560;581 593\"]'\n\nmerged.loc[2191, \"anno_list\"] =  '[\"right lower quadrant pain\"]'\nmerged.loc[2191, \"loc_list\"] =  '[\"32 57\"]'\n\nmerged.loc[2553, \"anno_list\"] =  '[\"diarrhoea no blood\"]'\nmerged.loc[2553, \"loc_list\"] =  '[\"308 317;376 384\"]'\n\nmerged.loc[3124, \"anno_list\"] =  '[\"sweating\"]'\nmerged.loc[3124, \"loc_list\"] =  '[\"549 557\"]'\n\nmerged.loc[3858, \"anno_list\"] =  '[\"previously as regular\", \"previously eveyr 28-29 days\", \"previously lasting 5 days\", \"previously regular flow\"]'\nmerged.loc[3858, \"loc_list\"] =  '[\"102 123\", \"102 112;125 141\", \"102 112;143 157\", \"102 112;159 171\"]'\n\nmerged.loc[4373, \"anno_list\"] =  '[\"for 2 months\"]'\nmerged.loc[4373, \"loc_list\"] =  '[\"33 45\"]'\n\nmerged.loc[4763, \"anno_list\"] =  '[\"35 year old\"]'\nmerged.loc[4763, \"loc_list\"] =  '[\"5 16\"]'\n\nmerged.loc[4782, \"anno_list\"] =  '[\"darker brown stools\"]'\nmerged.loc[4782, \"loc_list\"] =  '[\"175 194\"]'\n\nmerged.loc[4908, \"anno_list\"] =  '[\"uncle with peptic ulcer\"]'\nmerged.loc[4908, \"loc_list\"] =  '[\"700 723\"]'\n\nmerged.loc[6016, \"anno_list\"] =  '[\"difficulty falling asleep\"]'\nmerged.loc[6016, \"loc_list\"] =  '[\"225 250\"]'\n\nmerged.loc[6192, \"anno_list\"] =  '[\"helps to take care of aging mother and in-laws\"]'\nmerged.loc[6192, \"loc_list\"] =  '[\"197 218;236 260\"]'\n\nmerged.loc[6380, \"anno_list\"] =  '[\"No hair changes\", \"No skin changes\", \"No GI changes\", \"No palpitations\", \"No excessive sweating\"]'\nmerged.loc[6380, \"loc_list\"] =  '[\"480 482;507 519\", \"480 482;499 503;512 519\", \"480 482;521 531\", \"480 482;533 545\", \"480 482;564 582\"]'\n\nmerged.loc[6562, \"anno_list\"] =  '[\"stressed due to taking care of her mother\", \"stressed due to taking care of husbands parents\"]'\nmerged.loc[6562, \"loc_list\"] =  '[\"290 320;327 337\", \"290 320;342 358\"]'\n\nmerged.loc[6862, \"anno_list\"] =  '[\"stressor taking care of many sick family members\"]'\nmerged.loc[6862, \"loc_list\"] =  '[\"288 296;324 363\"]'\n\nmerged.loc[7022, \"anno_list\"] =  '[\"heart started racing and felt numbness for the 1st time in her finger tips\"]'\nmerged.loc[7022, \"loc_list\"] =  '[\"108 182\"]'\n\nmerged.loc[7422, \"anno_list\"] =  '[\"first started 5 yrs\"]'\nmerged.loc[7422, \"loc_list\"] =  '[\"102 121\"]'\n\nmerged.loc[8876, \"anno_list\"] =  '[\"No shortness of breath\"]'\nmerged.loc[8876, \"loc_list\"] =  '[\"481 483;533 552\"]'\n\nmerged.loc[9027, \"anno_list\"] =  '[\"recent URI\", \"nasal stuffines, rhinorrhea, for 3-4 days\"]'\nmerged.loc[9027, \"loc_list\"] =  '[\"92 102\", \"123 164\"]'\n\nmerged.loc[9938, \"anno_list\"] =  '[\"irregularity with her cycles\", \"heavier bleeding\", \"changes her pad every couple hours\"]'\nmerged.loc[9938, \"loc_list\"] =  '[\"89 117\", \"122 138\", \"368 402\"]'\n\nmerged.loc[9973, \"anno_list\"] =  '[\"gaining 10-15 lbs\"]'\nmerged.loc[9973, \"loc_list\"] =  '[\"344 361\"]'\n\nmerged.loc[10513, \"anno_list\"] =  '[\"weight gain\", \"gain of 10-16lbs\"]'\nmerged.loc[10513, \"loc_list\"] =  '[\"600 611\", \"607 623\"]'\n\nmerged.loc[11551, \"anno_list\"] =  '[\"seeing her son knows are not real\"]'\nmerged.loc[11551, \"loc_list\"] =  '[\"386 400;443 461\"]'\n\nmerged.loc[11677, \"anno_list\"] =  '[\"saw him once in the kitchen after he died\"]'\nmerged.loc[11677, \"loc_list\"] =  '[\"160 201\"]'\n\nmerged.loc[12124, \"anno_list\"] =  '[\"tried Ambien but it didnt work\"]'\nmerged.loc[12124, \"loc_list\"] =  '[\"325 337;349 366\"]'\n\nmerged.loc[12279, \"anno_list\"] =  '[\"heard what she described as a party later than evening these things did not actually happen\"]'\nmerged.loc[12279, \"loc_list\"] =  '[\"405 459;488 524\"]'\n\nmerged.loc[12289, \"anno_list\"] =  '[\"experienced seeing her son at the kitchen table these things did not actually happen\"]'\nmerged.loc[12289, \"loc_list\"] =  '[\"353 400;488 524\"]'\n\nmerged.loc[13238, \"anno_list\"] =  '[\"SCRACHY THROAT\", \"RUNNY NOSE\"]'\nmerged.loc[13238, \"loc_list\"] =  '[\"293 307\", \"321 331\"]'\n\nmerged.loc[13297, \"anno_list\"] =  '[\"without improvement when taking tylenol\", \"without improvement when taking ibuprofen\"]'\nmerged.loc[13297, \"loc_list\"] =  '[\"182 221\", \"182 213;225 234\"]'\n\nmerged.loc[13299, \"anno_list\"] =  '[\"yesterday\", \"yesterday\"]'\nmerged.loc[13299, \"loc_list\"] =  '[\"79 88\", \"409 418\"]'\n\nmerged.loc[13845, \"anno_list\"] =  '[\"headache global\", \"headache throughout her head\"]'\nmerged.loc[13845, \"loc_list\"] =  '[\"86 94;230 236\", \"86 94;237 256\"]'\n\nmerged.loc[14083, \"anno_list\"] =  '[\"headache generalized in her head\"]'\nmerged.loc[14083, \"loc_list\"] =  '[\"56 64;156 179\"]'\n\nmerged[\"anno_list\"] = [\n    literal_eval(x) if isinstance(x, str) else x for x in merged[\"anno_list\"]\n]\nmerged[\"loc_list\"] = [\n    literal_eval(x) if isinstance(x, str) else x for x in merged[\"loc_list\"]\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = merged[~merged['pn_history'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_data(merged):\n    print(f\"before clearning: count of empty annotations :{merged.loc[merged['annotation'] == '[]'].shape}\")\n    merged = merged.loc[merged['annotation'] != \"[]\"].copy().reset_index(drop=False)\n    print(f\"after clearning: count of empty annotations :{merged.loc[merged['annotation'] == '[]'].shape}\")\n    print(f\"before clearning: count of '-OR-' in feature text: {merged[merged['feature_text'].str.contains('-OR-')].shape}\")\n    merged['feature_text'] = merged['feature_text'].apply(lambda x:x.replace(\"-OR-\", ';-').replace(\"-\", \" \").lower())\n    print(f\"after clearning: count of '-OR-' in feature text: {merged[merged['feature_text'].str.contains('-OR-')].shape}\")\n    print(f\"before clearning: lower pn_history {merged['pn_history'].values[1]}\")\n    merged['pn_history'] = merged['pn_history'].apply(lambda x:x.lower())\n    print(f\"before clearning: lower pn_history {merged['pn_history'].values[1]}\")\n    return merged","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = clean_data(merged)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# skf = StratifiedKFold(n_splits=config['folds'], random_state=config['seed'], shuffle=True)\n\n# merged[\"fold\"] = -1\n\n# for fold, (_, val_idx) in enumerate(skf.split(merged, y=merged[\"case_num\"])):\n#     merged.loc[val_idx, \"fold\"] = fold\n    \n# counts = merged.groupby([\"fold\", \"pn_num\"], as_index=False).count()\n\n# # If the number of rows is the same as the number of \n# # unique pn_num, then each pn_num is only in one fold.\n# # Also if all the counts=1\n# print(counts.shape, counts.pn_num.nunique(), counts.case_num.unique(), merged['pn_num'].nunique())\n# merged['fold'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_patients = merged['pn_num'].unique()\nlabelled_patients[:10], merged['pn_num'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_patients = df_patients[~df_patients['pn_num'].isin(labelled_patients)][:config['no_unlabelled']]\nul_patients.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_features.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_final = []\nfor row in ul_patients.iterrows():\n#     print(row[1]['case_num'], row[1]['pn_history'][:10])\n#     print(df_features[df_features['case_num'] == row[1]['case_num']].shape)\n    ul_final.extend([[row[1]['pn_num'],row[1]['pn_history'], row[1]['case_num'],  feat] for feat in df_features[df_features['case_num'] == row[1]['case_num']]['feature_text']])\n#     if row[0] == 10:\n#         break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_final_df = pd.DataFrame.from_records(ul_final, columns=['pn_num', 'pn_history', 'case_num', \"feature_text\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_final_df.tail(), ul_final_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_patients['pn_num'].nunique())\nul_final_df['pn_num'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"first = merged.loc[35]\n\nexample = {\"feature_text\": first.feature_text,\n          \"pn_history\": first.pn_history,\n          \"loc_list\": first.loc_list,\n          \"annotation_list\": first.anno_list}\n\nfor key in example.keys():\n    print(key)\n    print(example[key])\n    print('='*10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loc_list_to_tuples(loc_list):\n    to_return = []\n    for loc_str in loc_list:\n        loc_strs = loc_str.split(\";\")\n        for loc in loc_strs:\n            start, end = loc.split()\n            to_return.append((int(start), int(end)))\n    return to_return\n\nprint(example['loc_list'])\nexample_loc_ints = loc_list_to_tuples(example['loc_list'])\nprint(example_loc_ints)\nfor loc in example_loc_ints:\n    print(example['pn_history'][loc[0] : loc[1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_label(example):\n    tokenized_inputs = tokenizer(example['feature_text'],\n                                example['pn_history'],\n                                truncation='only_second',\n                                max_length = config['max_length'],\n                                padding='max_length',\n                                return_offsets_mapping=True,)\n#                                 return_tensors='pt')\n    labels = [0.0] * len(tokenized_inputs['input_ids'])\n    tokenized_inputs['location'] = loc_list_to_tuples(example['loc_list'])\n    tokenized_inputs['sequence_ids'] = tokenized_inputs.sequence_ids()\n    \n    if len(tokenized_inputs[\"location\"]) > 0:\n        for idx, (seq_id, offsets) in enumerate(\n            zip(tokenized_inputs[\"sequence_ids\"], tokenized_inputs[\"offset_mapping\"])\n        ):\n            if seq_id is None or seq_id == 0:\n                # don't calculate loss on question part or special tokens\n                labels[idx] = -100.0\n                continue\n\n            token_start, token_end = offsets\n            for label_start, label_end in tokenized_inputs[\"location\"]:\n                if (\n                    token_start <= label_start < token_end\n                    or token_start < label_end <= token_end\n                    or label_start <= token_start < label_end\n                ):\n                    labels[idx] = 1.0  # labels should be float\n\n    tokenized_inputs[\"labels\"] = labels\n    \n    return tokenized_inputs","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_inputs = tokenize_and_label(example)\ntokenized_inputs.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged = merged[[\"pn_history\", \"feature_text\", \"loc_list\"]]\nmerged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_dataset(merged, func_):\n    dataset = datasets.Dataset.from_pandas(merged)\n    print(f\"keys before applying tokenization: {dataset[0].keys()}\")\n    dataset_mapped = dataset.map(func_, num_proc=config['num_proc'])\n    if 'labels' in dataset_mapped.features:\n        dataset_mapped.set_format(type='torch', columns=['input_ids', 'attention_mask','token_type_ids', 'labels','offset_mapping', 'sequence_ids'], output_all_columns=False)\n    else:\n        dataset_mapped.set_format(type='torch', columns=['input_ids', 'attention_mask', 'token_type_ids','offset_mapping', 'sequence_ids'], output_all_columns=False)\n    # dataset_mapped = dataset_mapped.remove_columns(['pn_history',\"feature_text\",\"loc_list\", \"token_type_ids\",\"offset_mapping\", \"location_int\", \"sequence_ids\"])\n    print(f\"keys after applying tokenization: {dataset_mapped[0].keys()}\")\n    return dataset_mapped","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_mapped = convert_to_dataset(merged[:], tokenize_and_label)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:15.107645Z","iopub.execute_input":"2022-04-18T06:35:15.10846Z","iopub.status.idle":"2022-04-18T06:35:16.183014Z","shell.execute_reply.started":"2022-04-18T06:35:15.108424Z","shell.execute_reply":"2022-04-18T06:35:16.182209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_mapped = dataset_mapped.train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.184678Z","iopub.execute_input":"2022-04-18T06:35:16.185243Z","iopub.status.idle":"2022-04-18T06:35:16.199219Z","shell.execute_reply.started":"2022-04-18T06:35:16.185198Z","shell.execute_reply":"2022-04-18T06:35:16.198532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_mapped","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.201894Z","iopub.execute_input":"2022-04-18T06:35:16.202093Z","iopub.status.idle":"2022-04-18T06:35:16.21067Z","shell.execute_reply.started":"2022-04-18T06:35:16.202069Z","shell.execute_reply":"2022-04-18T06:35:16.209816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(examples):\n    return tokenizer.pad(examples, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.212231Z","iopub.execute_input":"2022-04-18T06:35:16.212753Z","iopub.status.idle":"2022-04-18T06:35:16.217904Z","shell.execute_reply.started":"2022-04-18T06:35:16.212717Z","shell.execute_reply":"2022-04-18T06:35:16.217058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nl_dataloader_train = DataLoader(dataset_mapped['train'], batch_size=config['train_batch_size'],shuffle=True, collate_fn=collate_fn)\nl_dataloader_test = DataLoader(dataset_mapped['test'], batch_size=config['train_batch_size'],shuffle=True, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.22044Z","iopub.execute_input":"2022-04-18T06:35:16.220894Z","iopub.status.idle":"2022-04-18T06:35:16.227439Z","shell.execute_reply.started":"2022-04-18T06:35:16.220858Z","shell.execute_reply":"2022-04-18T06:35:16.226707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_dataloader_train, l_dataloader_test","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.228847Z","iopub.execute_input":"2022-04-18T06:35:16.229404Z","iopub.status.idle":"2022-04-18T06:35:16.239156Z","shell.execute_reply.started":"2022-04-18T06:35:16.229351Z","shell.execute_reply":"2022-04-18T06:35:16.238236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_tokenize(example, tokenizer=tokenizer):\n    tokens = tokenizer(example['feature_text'],\n                                example['pn_history'],\n                                truncation='only_second',\n                                max_length = config['max_length'],\n                                padding='max_length',\n                                return_offsets_mapping=True)\n    tokens['sequence_ids'] = tokens.sequence_ids()\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.240739Z","iopub.execute_input":"2022-04-18T06:35:16.241235Z","iopub.status.idle":"2022-04-18T06:35:16.247312Z","shell.execute_reply.started":"2022-04-18T06:35:16.241199Z","shell.execute_reply":"2022-04-18T06:35:16.246611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_mapped = convert_to_dataset(ul_final_df, predict_tokenize)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:16.248751Z","iopub.execute_input":"2022-04-18T06:35:16.249226Z","iopub.status.idle":"2022-04-18T06:35:18.392045Z","shell.execute_reply.started":"2022-04-18T06:35:16.249187Z","shell.execute_reply":"2022-04-18T06:35:18.3912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_mapped[0]['input_ids'].shape, ul_mapped[0]['token_type_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:18.393718Z","iopub.execute_input":"2022-04-18T06:35:18.394273Z","iopub.status.idle":"2022-04-18T06:35:18.44993Z","shell.execute_reply.started":"2022-04-18T06:35:18.394228Z","shell.execute_reply":"2022-04-18T06:35:18.449085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ul_mapped)\nul_dataloader = DataLoader(ul_mapped, batch_size=config['train_batch_size'], shuffle=True, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:18.459767Z","iopub.execute_input":"2022-04-18T06:35:18.460588Z","iopub.status.idle":"2022-04-18T06:35:18.467336Z","shell.execute_reply.started":"2022-04-18T06:35:18.460534Z","shell.execute_reply":"2022-04-18T06:35:18.4664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = config['device']","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:18.468633Z","iopub.execute_input":"2022-04-18T06:35:18.469172Z","iopub.status.idle":"2022-04-18T06:35:18.474805Z","shell.execute_reply.started":"2022-04-18T06:35:18.469135Z","shell.execute_reply":"2022-04-18T06:35:18.473644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pdb","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:18.476059Z","iopub.execute_input":"2022-04-18T06:35:18.476469Z","iopub.status.idle":"2022-04-18T06:35:18.483089Z","shell.execute_reply.started":"2022-04-18T06:35:18.476429Z","shell.execute_reply":"2022-04-18T06:35:18.482275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel\n\nclass ClassificationModel(torch.nn.Module):\n    def __init__(self, model, num_labels):\n        super().__init__()\n        self.dberta = AutoModel.from_pretrained(model)\n        self.dropout = torch.nn.Dropout(p=config['dropout'])\n        self.linear1 = torch.nn.Linear(768, 512)\n        self.classifier = torch.nn.Linear(512, 1)\n    \n    def forward(self, x):\n        output = self.dberta(input_ids=x[\"input_ids\"], attention_mask=x[\"attention_mask\"], token_type_ids=x[\"token_type_ids\"])\n#         print(output[0].shape)\n#         pdb.set_trace()\n        out = self.linear1(self.dropout(output[0]))\n        logits = self.classifier(out)\n        return logits.squeeze(-1)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:18.484439Z","iopub.execute_input":"2022-04-18T06:35:18.484734Z","iopub.status.idle":"2022-04-18T06:35:18.494019Z","shell.execute_reply.started":"2022-04-18T06:35:18.484692Z","shell.execute_reply":"2022-04-18T06:35:18.492862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ClassificationModel(config['model_name'], num_labels=1).to(device)\nloss_fct = torch.nn.BCEWithLogitsLoss(reduction=\"none\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:18.495303Z","iopub.execute_input":"2022-04-18T06:35:18.496194Z","iopub.status.idle":"2022-04-18T06:35:28.810118Z","shell.execute_reply.started":"2022-04-18T06:35:18.496119Z","shell.execute_reply":"2022-04-18T06:35:28.809297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# del model\n# import gc\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.814558Z","iopub.execute_input":"2022-04-18T06:35:28.816548Z","iopub.status.idle":"2022-04-18T06:35:28.821657Z","shell.execute_reply.started":"2022-04-18T06:35:28.816494Z","shell.execute_reply":"2022-04-18T06:35:28.820933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import optim\nfrom torch import nn\noptimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.82587Z","iopub.execute_input":"2022-04-18T06:35:28.828365Z","iopub.status.idle":"2022-04-18T06:35:28.837316Z","shell.execute_reply.started":"2022-04-18T06:35:28.828307Z","shell.execute_reply":"2022-04-18T06:35:28.836433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, data_loader, optimizer, criterion):\n    model.train()\n    train_loss = []\n    for batch in tqdm(data_loader):\n        batch = {key: val.to(device) for key, val in batch.items()}\n        labels = batch['labels'].to(device)\n        logits = model(batch)\n#         pdb.set_trace()\n        loss = criterion(logits, labels)\n        # since, we have\n        loss = torch.masked_select(loss, labels > -1.0).mean()\n        train_loss.append(loss.item() * labels.size(0))\n        loss.backward()\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        # it's also improve f1 accuracy slightly\n        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n    return sum(train_loss)/len(train_loss)\n\ndef eval_model(model, dataloader, criterion):\n        model.eval()\n        valid_loss = []\n        preds = []\n        offsets = []\n        seq_ids = []\n        valid_labels = []\n        input_ids = []\n        for batch in tqdm(dataloader):\n#             batch = {key: val.to(device) for key, val in batch.items()}\n            batch['input_ids'] = batch['input_ids'].to(device)\n            batch['attention_mask'] = batch[\"attention_mask\"].to(device)\n            batch['token_type_ids'] = batch['token_type_ids'].to(device)\n            labels = batch['labels'].to(device)\n            offset_mapping = batch['offset_mapping']\n            sequence_ids = batch['sequence_ids']\n\n            logits = model(batch)\n            loss = criterion(logits, labels)\n            loss = torch.masked_select(loss, labels > -1.0).mean()\n            valid_loss.append(loss.item() * labels.size(0))\n            \n            input_ids.append(batch['input_ids'].detach().cpu().numpy())\n            preds.append(logits.detach().cpu().numpy())\n            offsets.append(offset_mapping.numpy())\n            seq_ids.append(sequence_ids.numpy())\n            valid_labels.append(labels.detach().cpu().numpy())\n\n        preds = np.concatenate(preds, axis=0)\n        offsets = np.concatenate(offsets, axis=0)\n        seq_ids = np.concatenate(seq_ids, axis=0)\n        valid_labels = np.concatenate(valid_labels, axis=0)\n        input_ids = np.concatenate(input_ids, axis=0)\n        \n        location_preds = get_location_predictions(preds, offsets, seq_ids, input_ids, test=False)\n        score = calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n\n        return sum(valid_loss)/len(valid_loss), score","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.838844Z","iopub.execute_input":"2022-04-18T06:35:28.839288Z","iopub.status.idle":"2022-04-18T06:35:28.877769Z","shell.execute_reply.started":"2022-04-18T06:35:28.839245Z","shell.execute_reply":"2022-04-18T06:35:28.8769Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom itertools import chain\nimport re\n\ndef get_location_predictions(preds, offset_mapping, sequence_ids, input_ids, test=False):\n    all_predictions = []\n    for pred, offsets, seq_ids, input_id in zip(preds, offset_mapping, sequence_ids, input_ids):\n        pred = 1 / (1 + np.exp(-pred))\n        start_idx = None\n        end_idx = None\n        current_preds = []\n        for pred_, offset, seq_id, id_ in zip(pred, offsets, seq_ids, input_id):\n            if seq_id is None or seq_id == 0:\n                continue\n\n            if pred_ > 0.5:\n#                 pdb.set_trace()\n                if start_idx is None:\n                    start_idx = offset[0]\n                    if re.match(r'^â–',tokenizer.convert_ids_to_tokens([id_])[0]):\n                        start_idx += 1\n                end_idx = offset[1]\n            elif start_idx is not None:\n                if test:\n                    current_preds.append(f\"{start_idx} {end_idx}\")\n                else:\n                    current_preds.append((start_idx, end_idx))\n                start_idx = None\n        if test:\n            all_predictions.append(\"; \".join(current_preds))\n        else:\n            all_predictions.append(current_preds)\n            \n    return all_predictions\n\n\ndef calculate_char_cv(predictions, offset_mapping, sequence_ids, labels):\n    all_labels = []\n    all_preds = []\n    for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n\n        num_chars = max(list(chain(*offsets)))\n        char_labels = np.zeros(num_chars)\n\n        for o, s_id, label in zip(offsets, seq_ids, labels):\n            if s_id is None or s_id == 0:\n                continue\n            if int(label) == 1:\n                char_labels[o[0]:o[1]] = 1\n                \n        char_preds = np.zeros(num_chars)\n\n        for idx, (start_idx, end_idx) in enumerate(preds):\n            char_preds[start_idx:end_idx] = 1\n#             pdb.set_trace()\n#             if (\n#                 tokenizer.converinput_id[idx].isspace()\n#                 and start_idx > 0\n#                 and not char_preds[start_idx - 1]\n#             ):\n#                 char_preds[start_idx] = 0\n\n        all_labels.extend(char_labels)\n        all_preds.extend(char_preds)\n\n    results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n\n    return {\n        \"Accuracy\": accuracy,\n        \"precision\": results[0],\n        \"recall\": results[1],\n        \"f1\": results[2]\n    }","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.879124Z","iopub.execute_input":"2022-04-18T06:35:28.879649Z","iopub.status.idle":"2022-04-18T06:35:28.901552Z","shell.execute_reply.started":"2022-04-18T06:35:28.879613Z","shell.execute_reply":"2022-04-18T06:35:28.900729Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(config['epochs']):\n#     print(f\"training epoch {i}\")\n#     train_loss = train_model(model, l_dataloader_train, optimizer, loss_fct)\n    \n#     validation_loss, score = eval_model(model, l_dataloader_test, loss_fct)\n    \n#     print(f\"train loss: {train_loss}, validation loss: {validation_loss}, score: {score}\")\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.902739Z","iopub.execute_input":"2022-04-18T06:35:28.903203Z","iopub.status.idle":"2022-04-18T06:35:28.919036Z","shell.execute_reply.started":"2022-04-18T06:35:28.903118Z","shell.execute_reply":"2022-04-18T06:35:28.918252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"T1 = 1\nT2 = 5\naf = 3\ndef alpha_weight(step):\n    if step < T1:\n        return 0.0\n    elif step > T2:\n        return af\n    else:\n         return ((step-T1) / (T2-T1))*af","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.92028Z","iopub.execute_input":"2022-04-18T06:35:28.92081Z","iopub.status.idle":"2022-04-18T06:35:28.928308Z","shell.execute_reply.started":"2022-04-18T06:35:28.920764Z","shell.execute_reply":"2022-04-18T06:35:28.927651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for batch in l_dataloader_train:\n#     batch['input_ids'] = batch['input_ids'].to(device)\n#     batch['attention_mask'] = batch[\"attention_mask\"].to(device)\n#     batch['token_type_ids'] = batch['token_type_ids'].to(device)\n#     res = model(batch)\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.930969Z","iopub.execute_input":"2022-04-18T06:35:28.931414Z","iopub.status.idle":"2022-04-18T06:35:28.936579Z","shell.execute_reply.started":"2022-04-18T06:35:28.931377Z","shell.execute_reply":"2022-04-18T06:35:28.93593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(50):\n    print(alpha_weight(i))\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.938283Z","iopub.execute_input":"2022-04-18T06:35:28.939863Z","iopub.status.idle":"2022-04-18T06:35:28.946332Z","shell.execute_reply.started":"2022-04-18T06:35:28.939827Z","shell.execute_reply":"2022-04-18T06:35:28.945488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install wandb --upgrade -qq","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:28.947635Z","iopub.execute_input":"2022-04-18T06:35:28.948061Z","iopub.status.idle":"2022-04-18T06:35:41.879284Z","shell.execute_reply.started":"2022-04-18T06:35:28.948026Z","shell.execute_reply":"2022-04-18T06:35:41.878089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key = secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:41.881098Z","iopub.execute_input":"2022-04-18T06:35:41.881357Z","iopub.status.idle":"2022-04-18T06:35:44.465233Z","shell.execute_reply.started":"2022-04-18T06:35:41.881326Z","shell.execute_reply":"2022-04-18T06:35:44.464454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.environ[\"WANDB_PROJECT\"] = \"nbme_pseudo_labelling\"\n# os.environ[\"WANDB_RUN_GROUP\"] = \"DEBERTA_\" + datetime.now().strftime(\n# \"%Y-%m-%d %H:%M\"\n# )","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:44.466879Z","iopub.execute_input":"2022-04-18T06:35:44.467131Z","iopub.status.idle":"2022-04-18T06:35:44.472831Z","shell.execute_reply.started":"2022-04-18T06:35:44.467094Z","shell.execute_reply":"2022-04-18T06:35:44.471706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project = \"pseudo-labelling\")\n# wandb.watch(model, loss_fct, log='all', log_freq=100)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:44.474233Z","iopub.execute_input":"2022-04-18T06:35:44.474827Z","iopub.status.idle":"2022-04-18T06:35:51.362572Z","shell.execute_reply.started":"2022-04-18T06:35:44.474788Z","shell.execute_reply":"2022-04-18T06:35:51.361795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def semisuper_train(model, train_loader, unlabelled_loader, test_loader, epochs, optimizer, loss_fct):\n#     epochs = epochs\n    step = 1\n    eval_loss = []\n    eval_score = []\n    ul_loss = []\n    alpha_list = []\n    for epoch in range(epochs):\n        ul_batch_loss = []\n        for batch_idx, ul in enumerate(unlabelled_loader):\n            model.eval()\n            ul = ul.to(device)\n            ul_output = model(ul)\n            ul_output = (ul_output.sigmoid() > 0.5).type(torch.float)\n            \n            model.train()\n            output = model(ul)\n#             pdb.set_trace()\n\n            unlabelled_loss = (alpha_weight(step) * loss_fct(output, ul_output)).mean()\n#             print(unlabelled_loss, ul_output.shape, output.shape)\n            ul_batch_loss.append(unlabelled_loss)\n            optimizer.zero_grad()\n            unlabelled_loss.backward()\n            optimizer.step()\n            \n            if batch_idx%(int(ul_dataloader.__len__()/3)) == 0 and batch_idx != 0:\n                train_loss = train_model(model, train_loader, optimizer, loss_fct)\n                step += 1\n                wandb.log({\"train_loss\": train_loss})\n        loss, score = eval_model(model, test_loader, loss_fct)\n        wandb.log({\"test_f1_score\":score['f1']})\n        wandb.log({\"test_accuracy_score\":score['Accuracy']})\n        wandb.log({\"test_loss\": loss})\n        wandb.log({\"unlabelled_loss\": unlabelled_loss.mean()})\n        print('Epoch: {} : Alpha Weight : {:.5f} | Test Loss : {:.3f} '.format(epoch, alpha_weight(step), loss))\n        print(f\"Epoch score : {score}\")\n        eval_loss.append(loss)\n        eval_score.append(score)\n        alpha_list.append(alpha_weight(step))\n        ul_loss.append(unlabelled_loss.mean())\n#             model.trai","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:35:51.366925Z","iopub.execute_input":"2022-04-18T06:35:51.368912Z","iopub.status.idle":"2022-04-18T06:35:51.387449Z","shell.execute_reply.started":"2022-04-18T06:35:51.368867Z","shell.execute_reply":"2022-04-18T06:35:51.386645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"semisuper_train(model, l_dataloader_train, ul_dataloader, l_dataloader_test, config['epochs'], optimizer, loss_fct)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:36:02.49195Z","iopub.execute_input":"2022-04-18T06:36:02.492677Z","iopub.status.idle":"2022-04-18T06:37:20.499695Z","shell.execute_reply.started":"2022-04-18T06:36:02.492639Z","shell.execute_reply":"2022-04-18T06:37:20.49877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:37:20.504337Z","iopub.execute_input":"2022-04-18T06:37:20.506304Z","iopub.status.idle":"2022-04-18T06:37:20.513898Z","shell.execute_reply.started":"2022-04-18T06:37:20.50626Z","shell.execute_reply":"2022-04-18T06:37:20.51306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"fine-tunned-weights\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:40:54.43925Z","iopub.execute_input":"2022-04-18T06:40:54.439694Z","iopub.status.idle":"2022-04-18T06:40:56.006078Z","shell.execute_reply.started":"2022-04-18T06:40:54.439628Z","shell.execute_reply":"2022-04-18T06:40:56.005238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_dataset = convert_to_dataset(ul_final_df, predict_tokenize)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:40:56.007717Z","iopub.execute_input":"2022-04-18T06:40:56.007972Z","iopub.status.idle":"2022-04-18T06:40:58.152221Z","shell.execute_reply.started":"2022-04-18T06:40:56.007937Z","shell.execute_reply":"2022-04-18T06:40:58.151388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_dataloder = DataLoader(score_dataset, batch_size=config['train_batch_size'], shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:40:58.156484Z","iopub.execute_input":"2022-04-18T06:40:58.158757Z","iopub.status.idle":"2022-04-18T06:40:58.167192Z","shell.execute_reply.started":"2022-04-18T06:40:58.158713Z","shell.execute_reply":"2022-04-18T06:40:58.166445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor batch in score_dataloder:\n    batch['input_ids'] = batch['input_ids'].to(device)\n    batch['attention_mask'] = batch[\"attention_mask\"].to(device)\n    batch['token_type_ids'] = batch['token_type_ids'].to(device)\n    res = model(batch)\n    predictions.extend(get_location_predictions(res.detach().cpu().numpy(), batch['offset_mapping'].numpy(), batch['sequence_ids'].numpy(), batch['input_ids'], test=True))\n#     print(res)\n#     break","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:42:52.399716Z","iopub.execute_input":"2022-04-18T06:42:52.400013Z","iopub.status.idle":"2022-04-18T06:43:01.49372Z","shell.execute_reply.started":"2022-04-18T06:42:52.399981Z","shell.execute_reply":"2022-04-18T06:43:01.492839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_final_df['predictions'] = predictions","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:01.495389Z","iopub.execute_input":"2022-04-18T06:43:01.495652Z","iopub.status.idle":"2022-04-18T06:43:01.502464Z","shell.execute_reply.started":"2022-04-18T06:43:01.495617Z","shell.execute_reply":"2022-04-18T06:43:01.501576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_final_df.to_csv(\"ul_labelled.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:01.504216Z","iopub.execute_input":"2022-04-18T06:43:01.504533Z","iopub.status.idle":"2022-04-18T06:43:01.519275Z","shell.execute_reply.started":"2022-04-18T06:43:01.50447Z","shell.execute_reply":"2022-04-18T06:43:01.518405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ul_final_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T06:43:01.520863Z","iopub.execute_input":"2022-04-18T06:43:01.521055Z","iopub.status.idle":"2022-04-18T06:43:01.535221Z","shell.execute_reply.started":"2022-04-18T06:43:01.521031Z","shell.execute_reply":"2022-04-18T06:43:01.534374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}