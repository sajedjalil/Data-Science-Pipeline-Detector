{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/28009/logos/header.png?)","metadata":{"papermill":{"duration":0.010494,"end_time":"2021-09-04T19:30:52.804664","exception":false,"start_time":"2021-09-04T19:30:52.79417","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# **This is a updated version of the public notebook. I have updated it with full explanation. I have also added all the refereneces to each line, so t will be easier for evryone to modify it.**","metadata":{}},{"cell_type":"markdown","source":"# Your task should be,\n# **Focus on the hyperparameters of the neural network. Try to change the values and check whether your auc value is improving or not.**","metadata":{}},{"cell_type":"markdown","source":"# If you find the notebook useful, you can thumbs it up.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os \n#The functions that the OS module provides allows you to interface with the operating system that Python is running on\nfrom sklearn.pipeline import Pipeline \n#Pipeline is used to assemble several steps that can be cross-validated together while setting different parameters. \n\n\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers \n#for building up the different layers of neural network\n#Keras tensor flow deep learning library to create a deep learning model for both regression and classification problems.\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Embedding,  Flatten\n#importing necessary dunctions for building the network\nfrom tensorflow.keras.models import Model, Sequential\n#Sequential model allows us to create a deep learning model by adding layers to it. \n#Here, every unit in a layer is connected to every unit in the previous layer. \nfrom keras.callbacks import ReduceLROnPlateau\n#ReduceLROnPlateau is used to Reduce learning rate when a metric has stopped improving.\nfrom keras.optimizers import RMSprop\n#Optimizer that implements the RMSprop algorithm.\n\nfrom tensorflow.data import Dataset\n#It handles downloading and preparing the data deterministically and constructing a tf.data.Dataset \nfrom sklearn.model_selection import train_test_split\n#From splitting the dataset for training and testing\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\n#QuantileTransformer method transforms the features to follow a uniform or a normal distribution\n#KBinsDiscretizer bins continuous data into intervals.\nfrom tensorflow import keras\n\nfrom sklearn.impute import SimpleImputer\n#Imputation transformer for completing missing values.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n#For quantifying the quality of predictions","metadata":{"_kg_hide-input":false,"papermill":{"duration":6.469406,"end_time":"2021-09-04T19:31:07.792466","exception":false,"start_time":"2021-09-04T19:31:01.32306","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-17T03:21:38.584821Z","iopub.execute_input":"2021-09-17T03:21:38.585175Z","iopub.status.idle":"2021-09-17T03:21:38.592811Z","shell.execute_reply.started":"2021-09-17T03:21:38.585148Z","shell.execute_reply":"2021-09-17T03:21:38.591728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#You need to first install datatable with pip command\n#pip install datatable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datatable package is like pandas which can read data up to 10 times faster than pandas.\n#import datatable as dt ","metadata":{"execution":{"iopub.status.busy":"2021-09-16T13:31:47.041539Z","iopub.execute_input":"2021-09-16T13:31:47.042077Z","iopub.status.idle":"2021-09-16T13:31:47.074827Z","shell.execute_reply.started":"2021-09-16T13:31:47.042013Z","shell.execute_reply":"2021-09-16T13:31:47.07363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n\n1)https://www.tensorflow.org/datasets/overview\n\n2)https://scikit-learn.org/stable/modules/model_evaluation.html\n\n3)https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html\n\n4)https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html\n\n5)https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n\n6)https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\n7)https://keras.io/api/optimizers/learning_rate_schedules/\n\n8)https://keras.io/api/optimizers/\n\n9)https://www.tutorialspoint.com/keras/keras_model_compilation.htm\n\n10)https://www.kdnuggets.com/2019/08/overview-python-datatable-package.html#:~:text=Modern%20machine%20learning%20applications%20need%20to%20process%20a,a%20single-node%20machine%2C%20at%20the%20maximum%20possible%20speed.","metadata":{}},{"cell_type":"markdown","source":"# Load Dataset","metadata":{"papermill":{"duration":0.011117,"end_time":"2021-09-04T19:31:07.814981","exception":false,"start_time":"2021-09-04T19:31:07.803864","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\n# %%time prints the wall time for the entire cell whereas %time gives you the time for first line only\n\ntrain = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\ntest  = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsub   = pd.read_csv('../input/tabular-playground-series-sep-2021/sample_solution.csv')\n#We are again coverting it to pandas.\n#Check refrence (10) for details","metadata":{"papermill":{"duration":43.590539,"end_time":"2021-09-04T19:31:51.416663","exception":false,"start_time":"2021-09-04T19:31:07.826124","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-17T03:21:43.023399Z","iopub.execute_input":"2021-09-17T03:21:43.023783Z","iopub.status.idle":"2021-09-17T03:22:15.268425Z","shell.execute_reply.started":"2021-09-17T03:21:43.023749Z","shell.execute_reply":"2021-09-17T03:22:15.267497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.019305,"end_time":"2021-09-04T19:31:51.456252","exception":false,"start_time":"2021-09-04T19:31:51.436947","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:22:15.270098Z","iopub.execute_input":"2021-09-17T03:22:15.270503Z","iopub.status.idle":"2021-09-17T03:22:15.302866Z","shell.execute_reply.started":"2021-09-17T03:22:15.27046Z","shell.execute_reply":"2021-09-17T03:22:15.302024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain['n_missing'] = train.isna().sum(axis=1) \n#Checking the total null value row wise and storing it to a new column\n\ntest['n_missing'] = test.isna().sum(axis=1)\n#Checking the total null value row wise and storing it to a new column\ntrain['claim'] = train['claim'].astype(str)\n#Converting the int datatypes to string datatype\n\nfeatures = [col for col in train.columns if col not in ['claim', 'id']]\n#Here we are taking only the features . We are not invluding the output(claim) and id.\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=128,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=128, encode='ordinal',strategy='uniform'))\n        ])\n#Now we are ready to create a pipeline object by providing with the list of steps. \n#Our steps are — SimpleImputer,QuantileTransformer and KBinsDiscretizer\n#These steps are list of tuples consisting of name and an instance of the transformer or estimator. \n#For imputing the missing value, we have used median.\n\ntrain[features] = pipe.fit_transform(train[features])\ntest[features] = pipe.transform(test[features])\n#transforming the features with pipeline","metadata":{"papermill":{"duration":67.394763,"end_time":"2021-09-04T19:32:58.870564","exception":false,"start_time":"2021-09-04T19:31:51.475801","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-09-17T03:22:15.304511Z","iopub.execute_input":"2021-09-17T03:22:15.30478Z","iopub.status.idle":"2021-09-17T03:24:11.439927Z","shell.execute_reply.started":"2021-09-17T03:22:15.304756Z","shell.execute_reply":"2021-09-17T03:24:11.438943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{"papermill":{"duration":0.011257,"end_time":"2021-09-04T19:32:58.893801","exception":false,"start_time":"2021-09-04T19:32:58.882544","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:24:11.441435Z","iopub.execute_input":"2021-09-17T03:24:11.441722Z","iopub.status.idle":"2021-09-17T03:24:11.743145Z","shell.execute_reply.started":"2021-09-17T03:24:11.441694Z","shell.execute_reply":"2021-09-17T03:24:11.742246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    Input(train[features].shape[1:]), #train[features].shape[1:] is used to get the input shape\n    #The model needs to know what input shape it should expect. \n    #For this reason, the first layer in a Sequential model needs to receive information about its input shape\n    Embedding(input_dim=512, output_dim=4),\n    #we use an embedding layer to compress the input feature space into a smaller one.\n    #There are three parameters to the embedding layer\n    #input_dim : Size of the vocabulary\n    #output_dim : Length of the vector for each word\n    #input_length : Maximum length of a sequence\n    \n    Flatten(),\n    Dense(64,  activation='relu'),\n    Dropout(0.5),\n    Dense(32,  activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid'),\n])\n\nauc = tf.keras.metrics.AUC(name='aucroc') #Defining how we want to evaluate our model\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=5e-2,\n        decay_steps = 3000,\n        decay_rate= 0.8)\n#We are using a learning rate schedule to modulate how the learning rate of your optimizer changes over time\n\noptimizer = RMSprop(lr=1e-3, rho=0.9, epsilon=1e-08, decay=0.0) \n#This optimizer is usually a good choice for recurrent neural networks.\n#\"rho\" is the decay factor or the exponentially weighted average over the square of the gradients.\n#\"decay\" decays the learning rate over time, so we can move even closer to the local minimum in the end of training.\n#Check reference (8) for details\nmodel.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=[auc]) \n#Keras model provides a method, compile() to compile the model.\n#The important arguments are as follows −loss function,Optimizer and metrics\n#Check reference (9) for details","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:24:11.744377Z","iopub.execute_input":"2021-09-17T03:24:11.744715Z","iopub.status.idle":"2021-09-17T03:24:12.686287Z","shell.execute_reply.started":"2021-09-17T03:24:11.744686Z","shell.execute_reply":"2021-09-17T03:24:12.685384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x = np.float32(train[features]), y = np.float32(train.claim),\n          batch_size = 512, shuffle = True, epochs = 10) \n#Models are trained by NumPy arrays using fit(). \n#The main purpose of this fit function is used to evaluate your model on training.\n#Check reference (9) for details","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:24:12.687637Z","iopub.execute_input":"2021-09-17T03:24:12.688211Z","iopub.status.idle":"2021-09-17T03:25:48.345685Z","shell.execute_reply.started":"2021-09-17T03:24:12.688165Z","shell.execute_reply":"2021-09-17T03:25:48.344515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['claim'] = model.predict(np.float32(test[features]))\nsub=sub.set_index('id')\nsub.to_csv('submission.csv')\n#and lastly we are creating our submission model","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:26:01.499772Z","iopub.execute_input":"2021-09-17T03:26:01.500152Z","iopub.status.idle":"2021-09-17T03:26:12.466416Z","shell.execute_reply.started":"2021-09-17T03:26:01.500121Z","shell.execute_reply":"2021-09-17T03:26:12.465234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2021-09-17T03:03:47.015922Z","iopub.execute_input":"2021-09-17T03:03:47.016402Z","iopub.status.idle":"2021-09-17T03:03:47.028251Z","shell.execute_reply.started":"2021-09-17T03:03:47.016327Z","shell.execute_reply":"2021-09-17T03:03:47.027341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}