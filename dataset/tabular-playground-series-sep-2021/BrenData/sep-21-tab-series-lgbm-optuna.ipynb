{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import roc_auc_score\n\nimport catboost\nimport lightgbm as lgb\nimport sklearn\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T15:47:58.401237Z","iopub.execute_input":"2021-09-30T15:47:58.401585Z","iopub.status.idle":"2021-09-30T15:48:02.01444Z","shell.execute_reply.started":"2021-09-30T15:47:58.401488Z","shell.execute_reply":"2021-09-30T15:48:02.013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notes\n\nFor tabular data, I believe that two of the main reasons for an increase in model accuracy are feature engineering, and creating additional forms of data that your model can not find on its own.\n\nFor example, one column may correspond to group_id. And maybe some groups are more likely to have a claim value of 1 versus 0. Given that there are so many different IDs and they are in no particular order, it would be difficult for an ML model to determine this by itself. If we created an additional column with the probability that each particular group_id was to have a claim value of 1, then this would be much easier for the model to interpret. \n\nGiven that we do not know what each column corresponds to in this dataset, it will be difficult to perform\nfeature engineering on this set.\n\n### Versions\n\n- v1: Baseline - 0.790\n- v3: Added Optuna Hyperparameter Optimizer - 0.791\n- v4: Training on all data + increased range of hyperparameters(overfitting) - 0.776\n- v5: Reduced hyperparameter range(still overfitting) - 0.783\n- v6: Added hold-out set - 0.791\n- v7: Setting new seed + Feature Importance - 0.793\n- v11: Leave empty values as NAN/null - 0.802\n- v12: Added NAN count feature - 0.812\n- v13: 5-fold cross validation - 0.813\n- v14: 10-fold cross validation - 0.813\n- v15: Added skew/std of row - 0.813\n- v16: Added \"top-mean\" - 0.814\n- v17: Tested some features - 0.813\n- v19: Change seed - 0.814\n- v21: Cat-Boost + LGBM 10-fold - 0.814\n- v22: Just Cat-Boost - 0.814\n- v23: Removed Scalar + (Cat-Boost/LGBM 10-fold) - 0.817\n- v25: Changed params + Seed - 0.816\n- v26: 20-fold v25 - 0.817\n- v27: added mad feature - 0.817\n- V28: Changed how feats are calculated + Added pct_change() - 0.816\n- V29: Removed PCT change - 0.817\n- V30: Added Quantile + STD NAN's - 0.817\n- V31: Seed + Removed 1 Feature - \n\n### Changes\n\nAt this point I am looking for a strong combination of parameters for a 10-fold catboost and 10-fold Lightgbm combined.","metadata":{}},{"cell_type":"markdown","source":"### Read and Train_Test_Split Data\n\nMake the train_test_split before filling in for missing values, as we do not want any data leakage into the validation set!","metadata":{}},{"cell_type":"code","source":"#setting a seed for reproducability\nseed = 1380\n\n#reading training data\ntrain_df = pd.read_csv('../input/tabular-playground-series-sep-2021/train.csv')\ntrain_df_labels = train_df['claim']\ntrain_df = train_df.drop(columns=['claim','id'],axis=1)\n\n#reading test_data\ntest_df = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\nsubmission_df = test_df['id'].to_frame()\ntest_df = test_df.drop(columns=['id'], axis=1)\n\nprint(train_df.shape)\nprint(train_df_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:48:02.020676Z","iopub.execute_input":"2021-09-30T15:48:02.02101Z","iopub.status.idle":"2021-09-30T15:48:52.410945Z","shell.execute_reply.started":"2021-09-30T15:48:02.020964Z","shell.execute_reply":"2021-09-30T15:48:52.408357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Filling in Missing Values\n\nAs we can see from the first cell below there are plenty of missing values across the columns. It would be wise to analyze each column individually and find out what the best value the replace missing values would be, or to even replace the missing values at all.\n\nIn some cases this would be the mean, median, mode, or even an value associated with another column. \n\nAfter some testing, leaving the null values as null worked best. ","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum(axis = 0).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:48:52.416842Z","iopub.execute_input":"2021-09-30T15:48:52.417611Z","iopub.status.idle":"2021-09-30T15:48:52.652633Z","shell.execute_reply.started":"2021-09-30T15:48:52.417549Z","shell.execute_reply":"2021-09-30T15:48:52.652042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature engineering\n\nTesting out adding a NAN count for each row as per reccomendation from this discussion thread --> [Add Number of Nans in A Row as a Feature](https://www.kaggle.com/c/tabular-playground-series-sep-2021/discussion/270206)","metadata":{}},{"cell_type":"code","source":"#train_df features\na = train_df.abs().sum(axis=1)\nb = train_df.sem(axis=1)\nc = train_df.max(axis=1)\nd = train_df.min(axis=1)\ne = train_df.std(axis=1)\nf = train_df.skew(axis=1)\ng = train_df.mean(axis=1)\nh = train_df[[\"f40\",\"f70\",\"f47\",\"f34\",\"f35\",\"f45\"]].mean(axis=1)\ni = train_df.isnull().sum(axis=1)\nj = train_df.mad(axis=1)\nk = train_df.quantile(axis=1)\nl = train_df.isnull().std(axis=1)\n\ntrain_df['abs_sum'] = a\ntrain_df['sem'] = b\ntrain_df['max'] = c\ntrain_df['min'] = d\ntrain_df['std'] = e\ntrain_df['skew'] = f\ntrain_df['mean'] = g\ntrain_df[\"top_mean\"] = h\ntrain_df['NANs'] = i\ntrain_df['mad'] = j\ntrain_df['quantile'] = k\ntrain_df['NANs_std'] = l\n\n#test_df features\na = test_df.abs().sum(axis=1)\nb = test_df.sem(axis=1)\nc = test_df.max(axis=1)\nd = test_df.min(axis=1)\ne = test_df.std(axis=1)\nf = test_df.skew(axis=1)\ng = test_df.mean(axis=1)\nh = test_df[[\"f40\",\"f70\",\"f47\",\"f34\",\"f35\",\"f45\"]].mean(axis=1)\ni = test_df.isnull().sum(axis=1)\nj = test_df.mad(axis=1)\nk = test_df.quantile(axis=1)\nl = test_df.isnull().std(axis=1)\n\ntest_df['abs_sum'] = a\ntest_df['sem'] = b\ntest_df['max'] = c\ntest_df['min'] = d\ntest_df['std'] = e\ntest_df['skew'] = f\ntest_df['mean'] = g\ntest_df[\"top_mean\"] = h\ntest_df['NANs'] = i\ntest_df['mad'] = j\ntest_df['quantile'] = k\ntest_df['NANs_std'] = l\n\ndel a,b,c,d,e,f,g,h,i,j,k,l","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:48:52.653617Z","iopub.execute_input":"2021-09-30T15:48:52.654305Z","iopub.status.idle":"2021-09-30T15:52:08.998867Z","shell.execute_reply.started":"2021-09-30T15:48:52.654262Z","shell.execute_reply":"2021-09-30T15:52:08.997431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scaling Value between 0-1\n\nMinMaxScaler() - Transforms all of the columns into a range between 0-1\n\nWe are fitting the range of this on the training data, and then using that scaler to scale both the training and test datasets.\n\nWhen we do fit it to the test data, we may see values that are above and below the max/min range of the training data and so we are going to map any value below zero to zero, and any values above one to one.","metadata":{}},{"cell_type":"code","source":"#Defining and fitting minmaxscaler on train_data\n# scaler = MinMaxScaler() \n# scaler.fit(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.184117Z","iopub.status.idle":"2021-09-30T15:52:09.184986Z","shell.execute_reply.started":"2021-09-30T15:52:09.184756Z","shell.execute_reply":"2021-09-30T15:52:09.184778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting scaler and mapping outliers in test data to 0 or 1\n#keeps as pd.dataframe after scaler applied\n# train_df = pd.DataFrame(scaler.fit_transform(train_df),columns = train_df.columns)\n\n# valid_df = pd.DataFrame(scaler.fit_transform(valid_df),columns = valid_df.columns)\n\n# valid_df[valid_df < 0] = 0\n# valid_df[valid_df > 1] = 1","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.186074Z","iopub.status.idle":"2021-09-30T15:52:09.186462Z","shell.execute_reply.started":"2021-09-30T15:52:09.186272Z","shell.execute_reply":"2021-09-30T15:52:09.1863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training with Optuna\n\nFound a great article here on hyperparameter optimization with Optuna. Really helps break down the functionality of each parameter.\n\nhttps://neptune.ai/blog/lightgbm-parameters-guide\n\nWe are also using predict_proba to predict the probability of each label. We take the probability predicted that the label in equal to one, and test this against the true label to calculate roc_auc_score. See a relevant stack_overflow topic regarding this below.\n\n[Predict_Proba StackOverflow Question](https://stackoverflow.com/questions/55882873/how-predict-proba-in-sklearn-produces-two-columns-what-are-their-significance)","metadata":{}},{"cell_type":"code","source":"def create_model(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 20, 40)\n    n_estimators = trial.suggest_int(\"n_estimators\", 200, 450)\n    max_depth = trial.suggest_int('max_depth', 3, 8)\n    min_child_samples = trial.suggest_int('min_child_samples', 200, 750)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.10, 0.30)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.50, 1.0)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.50, 1.0)\n    \n        \n    model = lgb.LGBMClassifier(\n        objective='binary',\n        metric='binary_logloss',\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        learning_rate=learning_rate,\n        colsample_bytree=colsample_bytree,\n        random_state=seed\n    )\n    \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(train_df, train_df_labels)\n    #see link in markdown above for this next line\n    score = sklearn.metrics.roc_auc_score(valid_df_labels, model.predict_proba(valid_df)[:,1])\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.188934Z","iopub.status.idle":"2021-09-30T15:52:09.189841Z","shell.execute_reply.started":"2021-09-30T15:52:09.189481Z","shell.execute_reply":"2021-09-30T15:52:09.189517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It would be beneficial to test other samplers to see if they reach different model accuracies, but I have found the TPE Sampler to consistently perform well. \n\nTPESampler is based on the (Tree-structured Parzen Estimator) algorithm.\n\nIn the following cell we are wanting to find the params that perform the best on the validation set, and then we will restore the best parameters and make predictions.","metadata":{}},{"cell_type":"code","source":"# sampler = TPESampler(seed=seed)\n\n# study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n# study.optimize(objective, n_trials=1)\n# params = study.best_params #getting best params from study","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.191205Z","iopub.status.idle":"2021-09-30T15:52:09.191718Z","shell.execute_reply.started":"2021-09-30T15:52:09.191427Z","shell.execute_reply":"2021-09-30T15:52:09.191453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Restoring the model with the best model parameters.","metadata":{}},{"cell_type":"code","source":"# params = {'num_leaves': 28,\n#           'n_estimators': 376,\n#           'max_depth': 8,\n#           'min_child_samples': 202,\n#           'learning_rate': 0.11682677767413432,\n#           'bagging_fraction': 0.5036513634677549,\n#           'colsample_bytree': 0.7519268943195143\n#          }\n\n# model = lgb.LGBMClassifier(**params)\n# model.fit(train_df, train_df_labels)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.19366Z","iopub.status.idle":"2021-09-30T15:52:09.194228Z","shell.execute_reply.started":"2021-09-30T15:52:09.193907Z","shell.execute_reply":"2021-09-30T15:52:09.193935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training with Cross-Fold Validation\n\nThis is the third option I have for training an LGBM in this notebook. I would like to see if training either a 5-fold or 10-fold model will increase the accuracy of the model. \n\nI am using Sckit-Learn to create the folds, and then training based on the rows included in that specific fold. Cross-validation often makes models perform better on new data so fingers crossed!\n\nUPDATE:\n\nI have added a catboost model to pair with the lgb model, and I am now using a 10-fold cross-validation strategy paired with a catboost and lgbm model trained on each fold.","metadata":{}},{"cell_type":"code","source":"fold_models = []\nn_folds = 20\nfolds = KFold(n_splits = n_folds, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.195628Z","iopub.status.idle":"2021-09-30T15:52:09.196241Z","shell.execute_reply.started":"2021-09-30T15:52:09.195871Z","shell.execute_reply":"2021-09-30T15:52:09.195941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor train_index, test_index in folds.split(train_df):\n    fold_train_df, fold_valid_df = train_df.iloc[train_index,:], train_df.iloc[test_index,:]\n    #ravel creates a flattened array\n    fold_train_labels, fold_test_labels = train_df_labels.iloc[train_index].values.ravel(), \\\n                                          train_df_labels.iloc[test_index].values.ravel()\n        \n    lgb_params = {'num_leaves': 28,\n                  'n_estimators': 376,\n                  'max_depth': 8,\n                  'min_child_samples': 202,\n                  'learning_rate': 0.11682677767413432,\n                  'bagging_fraction': 0.5036513634677549,\n                  'colsample_bytree': 0.7519268943195143\n                 }\n\n    cat_params = {'iterations': 15585, \n                  'objective': 'CrossEntropy', \n                  'bootstrap_type': 'Bernoulli', \n                  'od_wait': 1144, \n                  'learning_rate': 0.023575206684596582, \n                  'reg_lambda': 36.30433203563295, \n                  'random_strength': 43.75597655616195, \n                  'depth': 7, \n                  'min_data_in_leaf': 11, \n                  'leaf_estimation_iterations': 1, \n                  'subsample': 0.8227911142845009,\n                  'task_type' : 'GPU',\n                  'devices' : '0',\n                  'verbose' : 0\n                 }\n    \n    print(\"Starting Fold: \", i)\n    i+=1\n        \n    lgbm_model = lgb.LGBMClassifier(**lgb_params)\n    lgbm_model.fit(fold_train_df, fold_train_labels)\n    \n    cat_model = catboost.CatBoostClassifier(**cat_params)\n    cat_model.fit(fold_train_df, fold_train_labels)\n    \n    fold_models.append(lgbm_model)\n    fold_models.append(cat_model)","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.198468Z","iopub.status.idle":"2021-09-30T15:52:09.19965Z","shell.execute_reply.started":"2021-09-30T15:52:09.19933Z","shell.execute_reply":"2021-09-30T15:52:09.19936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance\n\nWe can use a built-in method from lgbm to plot the importance of each feature. The distribution of features in terms of importance for this dataset is relatively evenly distributed. It is often the case that a few features are highly correlated with the data and are highly weighted when determining predictions.","metadata":{}},{"cell_type":"code","source":"# lgb.plot_importance(model,figsize=(14,30))\nlgb.plot_importance(fold_models[0],figsize=(14,30))","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.200801Z","iopub.status.idle":"2021-09-30T15:52:09.201889Z","shell.execute_reply.started":"2021-09-30T15:52:09.201596Z","shell.execute_reply":"2021-09-30T15:52:09.201627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Predictions\n\nFirst I am ging to read in the sample submission to make sure I am correctly formatting my true submission. I am then preproccessing the test data, making predictions, and then submitting them to an output file.","metadata":{}},{"cell_type":"code","source":"#preprocessing test_data\n# test_df = pd.read_csv('../input/tabular-playground-series-sep-2021/test.csv')\n# submission_df = test_df['id'].to_frame()\n# test_df = test_df.drop(columns=['id'], axis=0)\n\n#adding feats\n# test_df['NANs'] = test_df.isnull().sum(axis=1)\n# test_df['std'] = test_df.std(axis=1)\n# test_df['skew'] = test_df.skew(axis=1)\n# test_df['mean'] = test_df.mean(axis=1)\n# test_df[\"top_mean\"] = test_df[[\"f40\",\"f70\",\"f47\",\"f34\",\"f35\",\"f45\"]].mean(axis=1)\n\n#fitting scaler\n# test_df = pd.DataFrame(scaler.fit_transform(test_df),columns = test_df.columns)\n# test_df[test_df < 0] = 0\n# test_df[test_df > 1] = 1","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.203319Z","iopub.status.idle":"2021-09-30T15:52:09.203813Z","shell.execute_reply.started":"2021-09-30T15:52:09.203533Z","shell.execute_reply":"2021-09-30T15:52:09.203557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again note we are taking the second column of predict_proba, as this is the probability that we think the case is of label 1.","metadata":{}},{"cell_type":"code","source":"# #Making predictions and creating csv\n# submission_df['claim'] = model.predict_proba(test_df)[:,1]\n# submission_df.to_csv(\"submission.csv\", index=False)\n\n# submission_df","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.205382Z","iopub.status.idle":"2021-09-30T15:52:09.205883Z","shell.execute_reply.started":"2021-09-30T15:52:09.205622Z","shell.execute_reply":"2021-09-30T15:52:09.205647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multi-Fold Model Predictions\n\nPredictions for unseen data with multi-fold model.","metadata":{}},{"cell_type":"code","source":"all_preds = np.zeros(len(test_df))\n\nfor i in range(len(fold_models)):\n    all_preds += fold_models[i].predict_proba(test_df)[:,1]\n    \nall_preds /= len(fold_models)\nsubmission_df['claim'] = all_preds\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-09-30T15:52:09.207142Z","iopub.status.idle":"2021-09-30T15:52:09.207645Z","shell.execute_reply.started":"2021-09-30T15:52:09.207364Z","shell.execute_reply":"2021-09-30T15:52:09.207388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final Thoughts\n\nI noticed a few of the models parameters in the test data were mapped directly to the max/min of 0 or 1. I wonder what these variables are? More exploration needed.\n\nHopefully I find some time to improve upon this notebook. Constructive Criticism welcomed!","metadata":{}}]}