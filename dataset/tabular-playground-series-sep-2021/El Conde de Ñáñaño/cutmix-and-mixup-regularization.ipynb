{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sys\nimport os\nfrom time import time\nimport tensorflow as tf\nimport gc\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:56:58.4261Z","iopub.execute_input":"2021-09-21T14:56:58.426492Z","iopub.status.idle":"2021-09-21T14:56:58.449464Z","shell.execute_reply.started":"2021-09-21T14:56:58.426445Z","shell.execute_reply":"2021-09-21T14:56:58.448329Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's New?\n\nI read [this paper](https://arxiv.org/pdf/2106.01342.pdf) and found an interesting way to add noise. In the paper, the authors created noise as a two stage process. First, they used CutMix (also known as swapnoise) on the raw data. After an embedding layer, they used MixUp to blend samples together. This paper used the noise to help create embeddings, then fine tunes the embeddings on the data through vanilla supervised training.  My network only uses them for supervised training, however.  \n\n## But aren't Cutmix and Mixup were vision techniques?\n\nWhile both of those regularization methods are very popular in vision, they can be adapted to tabular relatively easily.  Michael Jahrer used CutMix (which he called swapnoise) in his wonderful [first place finish](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629) to the Porto-Seguro competition.  Instead of replacing boxes in an image, he just replaced column values with values from the same column but different rows.  I implement CutMix as a tf.keras.layer whose replaces values at random from other values in the **minibatch**.\n\nMixUp blends 2 images with some mixing parameter.  While this is pretty straight forward to implement, I have not seen this implemented in tabular data aside from the Somepalli's paper I cited at the start of this block.  Granted, I am not the world's best reader in the field of Neural Networks for tabular data, so I would not be surprised if other authors have done this before!  Like CutMix, for MixUp I blend samples with other samples in the same **minibatch**.  This is implemented as a tf.keras.layer.\n\nGiven those 2 interesting regularization techniques, I have created this basic notebook to test their viablility on this dataset.  ","metadata":{}},{"cell_type":"code","source":"##################################################################\n#Special layers\n##################################################################\nclass CutMix(tf.keras.layers.Layer):\n    '''\n    Implementation of CutMix\n    Args\n    _____\n    noise: (R in [0,1)) probability that a value is not sampled from distribution\n    Application\n    ____________\n    CM = CutMix(.2)\n    x = tf.reshape(tf.range(0,10, dtype=tf.float32), (5,2))\n    print(x.numpy())\n    y = CM(x,True)\n    print(y.numpy())\n    '''\n    def __init__(self, noise, **kwargs):\n        super(CutMix, self).__init__(**kwargs)\n        self.noise = noise\n\n    def get_config(self):\n        config = super(CutMix, self).get_config()\n        config.update({\"noise\": self.noise})\n        return config\n\n    def call(self, inputs, training=None):\n        if training:\n            shuffled = tf.stop_gradient(tf.random.shuffle(inputs))\n            #print(shuffled.numpy())\n\n            msk = tf.keras.backend.random_bernoulli(tf.shape(inputs), p=1 - self.noise, dtype=tf.float32)\n            #print(msk)\n            return msk * inputs + (tf.ones_like(msk) - msk) * shuffled\n        return inputs\n\nclass MixUp(tf.keras.layers.Layer):\n    '''\n    Implementation of MixUp\n    Args\n    _____\n    alpha: (R in [0,1)) percentage of random sample to input  used\n    Application\n    ____________\n    MU = MixUp(.1)\n    x = tf.reshape(tf.range(0,10, dtype=tf.float32), (5,2))\n    y = MU(x)\n    print(x.numpy())\n    print(y.numpy())\n    '''\n    def __init__(self, alpha, **kwargs):\n        super(MixUp, self).__init__(**kwargs)\n        self.alpha = alpha\n        self.alpha_constant = tf.constant(self.alpha)\n        self.one_minus_alpha = tf.constant(1.) - self.alpha\n\n    def get_config(self):\n        config = super(MixUp, self).get_config()\n        config.update({\"alpha\": self.alpha})\n        return config\n\n    def call(self, inputs, training=None):\n        if training:\n            shuffled = tf.stop_gradient(tf.random.shuffle(inputs))\n            #print(shuffled.numpy())\n            return self.alpha_constant * inputs + self.one_minus_alpha * shuffled\n        return inputs\n    \nclass ResnetBlockTabular(tf.keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        '''\n        output_dim: (int) dimension of output dense layer. \n        NOTE: if output_dim == input_dim, this is a ResNetIdentityBlock\n        '''\n        super(ResnetBlockTabular, self).__init__(**kwargs)\n        self.output_dim = output_dim\n    \n    def build(self, input_shape):\n        if self.output_dim == input_shape[-1]:\n            self.Dense1 = None\n        else:\n            self.Dense1 = tf.keras.layers.Dense(output_dim)\n\n        self.bn1 = tf.keras.layers.BatchNormalization()\n        self.relu1 = tf.keras.layers.ReLU()\n        self.dense2 = tf.keras.layers.Dense(self.output_dim)\n        self.bn2 = tf.keras.layers.BatchNormalization()\n        self.relu2 = tf.keras.layers.ReLU()\n        self.dense3 = tf.keras.layers.Dense(self.output_dim)\n    \n    def call(self, input_tensor, training=False):\n        if self.Dense1 is not None:\n            input_tensor = self.Dense1(input_tensor)\n        \n        x = self.bn1(input_tensor)\n        x = self.relu1(x)\n        x = self.dense2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.dense3(x)\n        \n        return x + input_tensor","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:44:03.537637Z","iopub.execute_input":"2021-09-21T14:44:03.538Z","iopub.status.idle":"2021-09-21T14:44:03.55632Z","shell.execute_reply.started":"2021-09-21T14:44:03.537972Z","shell.execute_reply":"2021-09-21T14:44:03.554701Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The Network\n\nNothing too fancy really.  After input, a CutMix layer, then a resnet block, then a Mixup layer.  The rest of the network is just resnet blocks with dropout regularization.  Feel free to changed this up to your own preference!  ","metadata":{}},{"cell_type":"code","source":"def ff(num_input_columns, BLOCKS, drop_rate, cutmix_noise, mixup_alpha, optimizer, block_sizes =None):\n    \n    if block_sizes is None:\n        block_sizes = [num_input_columns for _ in range(BLOCKS)]\n    else:\n        if len(block_sizes) !=BLOCKS:\n            print(f'block_sizes has {len(block_sizes)} blocks.  Needs {BLOCKS}.')\n    \n    #Input\n    inp = tf.keras.layers.Input(num_input_columns)\n    x = CutMix(noise = cutmix_noise)(inp)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = ResnetBlockTabular(output_dim = block_sizes[0], name=f'Resnet_0')(x)\n    x = MixUp(alpha= mixup_alpha)(x)\n    \n    for i in range(1,BLOCKS):\n        x = ResnetBlockTabular(output_dim = block_sizes[i], name=f'Resnet_{i}')(x)\n        x = tf.keras.layers.Dropout(drop_rate)(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    \n    \n    model.compile(optimizer=optimizer,\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=[tf.keras.metrics.AUC()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:44:05.031634Z","iopub.execute_input":"2021-09-21T14:44:05.031984Z","iopub.status.idle":"2021-09-21T14:44:05.042886Z","shell.execute_reply.started":"2021-09-21T14:44:05.031952Z","shell.execute_reply":"2021-09-21T14:44:05.041709Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Data\n\nI used Rank Gaussed data from [this notebook](https://www.kaggle.com/ottpocket/feather-creator) to train the network.  This data has a column counting the number of `np.nans` each row and has imputed each `np.nan` with -6.  Why -6 you ask?  Rank Gauss makes each column resemble a normal distribution.  By replacing the `nans` with -6, we tell the network that those values are 6 standard distributions below from the mean.  The next lowest value in the data is -5, for reference.  ","metadata":{}},{"cell_type":"code","source":"train = pd.read_feather('/kaggle/input/september-feather/train_rg_min')\ntest =  pd.read_feather('/kaggle/input/september-feather/test_rg_min')\nss = pd.read_csv('/kaggle/input/tabular-playground-series-sep-2021/sample_solution.csv')\nFEATURES = [feat for feat in train.columns if 'f' in feat]\nTARGET = 'claim'","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:44:08.124472Z","iopub.execute_input":"2021-09-21T14:44:08.124781Z","iopub.status.idle":"2021-09-21T14:44:15.350738Z","shell.execute_reply.started":"2021-09-21T14:44:08.124754Z","shell.execute_reply":"2021-09-21T14:44:15.349404Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n\nTake row wise statistics.  Also use crude mean encodings and count on the nan_count variable","metadata":{}},{"cell_type":"code","source":"#Row wise stats\nSTATS = ['min','max','sum','var','mean']\nfor df in [train, test]:\n    df['min'] = df[FEATURES].min(axis=1)\n    df['max'] = df[FEATURES].max(axis=1)\n    df['sum'] = df[FEATURES].sum(axis=1)\n    df['var'] = df[FEATURES].var(axis=1)\n    df['mean'] = df[FEATURES].mean(axis=1)\n    \n#Adding Encoding based on nan_count\nagg = train.groupby('nan_count').agg({'id':'count', TARGET:'mean'}).rename(columns={'id':'count', 'claim':'encoding'})\ncount_max = agg['count'].max()\nfor df in [train, test]:\n    #df['encoding'] = df.nan_count.map(agg['encoding'])\n    df['count'] = df.nan_count.map(agg['count']) / count_max\n\nFEATURES = FEATURES + ['nan_count', 'count'] + STATS #+ ['encoding']","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Hyperparameters\n\nAll parameters were obtained via optuna in [this notebook](https://www.kaggle.com/ottpocket/fork-of-fork-of-fork-of-nn-starter).  In that notebook I trained for 4.5 hours.  This was the first time using optuna for me, so maybe you can make some improvements on your own!  ","metadata":{}},{"cell_type":"code","source":"#####################\n#Model Params\n##################### \nbatch_size = 1024\nBLOCKS = 7\ndrop_rate = 0.19980551223829823 #Dropout rate for body of resnet\ncutmix_noise = 0.11104093311728253 #Probability that a value will be randomly swapped\nmixup_alpha = 0.2312874504067844 #How much weight the mixup mixing parameter has\nadam_learning_rate = 0.0014281456754098325\noptimizer = tf.keras.optimizers.Adam(learning_rate = adam_learning_rate)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:54:34.227559Z","iopub.execute_input":"2021-09-21T14:54:34.227928Z","iopub.status.idle":"2021-09-21T14:54:34.24088Z","shell.execute_reply.started":"2021-09-21T14:54:34.227878Z","shell.execute_reply":"2021-09-21T14:54:34.239623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross Validation\n\nWe will get a 5 fold cross val here.  We train a model to early stopping, then evaluate.","metadata":{}},{"cell_type":"code","source":"#######################\n#Cross Val Params\n#######################\nNUM_FOLDS = 5 # the number of folds in the KFold validation\nNUM_STARTS = 1 #Number of random starts to train per fold\nNUM_SPLITS = 1 #Number of times to repeat the KFold validation","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:54:36.023871Z","iopub.execute_input":"2021-09-21T14:54:36.024283Z","iopub.status.idle":"2021-09-21T14:54:36.028859Z","shell.execute_reply.started":"2021-09-21T14:54:36.024255Z","shell.execute_reply":"2021-09-21T14:54:36.027603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof = pd.DataFrame()\npreds = pd.DataFrame()\nES = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0, patience=20, verbose=0, mode='max')\n\n#Number of times to do KFold cross val\nfor random_state in tqdm(range(NUM_SPLITS)):\n    skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=random_state)\n    oof[random_state] = np.zeros(train.shape[0])\n    \n    for f, (t_idx, v_idx) in enumerate(skf.split(X=train, y=train[TARGET])):\n        tr_temp = train[FEATURES].iloc[t_idx]\n        tr_temp_target = train[TARGET].iloc[t_idx]\n        val_temp = train[FEATURES].iloc[v_idx]\n        val_temp_target = train[TARGET].iloc[v_idx]\n        Repeat_start = time()\n        \n        #Number of random starts per fold\n        for repeat in range(NUM_STARTS):\n            tf.keras.backend.clear_session()\n            start = time()\n            model = ff(num_input_columns = len(FEATURES), BLOCKS = BLOCKS, drop_rate = drop_rate, \n                       cutmix_noise = cutmix_noise, mixup_alpha = mixup_alpha, optimizer = optimizer)\n            \n            model.fit(tr_temp, tr_temp_target, batch_size=batch_size, callbacks=[ES], epochs=200,\n                      validation_data=(val_temp, val_temp_target))\n            oof[random_state].iloc[v_idx] = np.squeeze(model.predict(val_temp, batch_size=100000)) / NUM_STARTS\n            preds[f'{random_state}_{f}'] = np.squeeze(model.predict(test[FEATURES], batch_size=100000)) / NUM_STARTS\n            print(f'{time() - start :.2f}', end=', ')\n        print(f'Repeat total: {time() - Repeat_start :.2f}')","metadata":{"execution":{"iopub.status.busy":"2021-09-21T14:57:54.735175Z","iopub.execute_input":"2021-09-21T14:57:54.735564Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = [roc_auc_score(train[TARGET].values, oof[col].values) for col in oof.columns]\nscore = np.mean(scores)\nprint(f'Scores on all Cross validation splits: {scores}')\nprint(f'Mean AUC from splits: {score}')","metadata":{"execution":{"iopub.status.busy":"2021-09-10T00:41:38.511193Z","iopub.execute_input":"2021-09-10T00:41:38.511549Z","iopub.status.idle":"2021-09-10T00:41:38.821029Z","shell.execute_reply.started":"2021-09-10T00:41:38.511521Z","shell.execute_reply":"2021-09-10T00:41:38.820001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds.to_csv('predictions.csv', index=False)\noof.to_csv('oof.csv', index=False)\nss[TARGET] = np.mean(preds, axis=1)\nss.to_csv('ss.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-02T18:47:19.332525Z","iopub.execute_input":"2021-08-02T18:47:19.332864Z","iopub.status.idle":"2021-08-02T18:47:20.976774Z","shell.execute_reply.started":"2021-08-02T18:47:19.33282Z","shell.execute_reply":"2021-08-02T18:47:20.975501Z"},"trusted":true},"execution_count":null,"outputs":[]}]}