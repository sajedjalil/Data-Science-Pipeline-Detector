{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-16T22:46:08.302688Z","iopub.execute_input":"2021-10-16T22:46:08.303378Z","iopub.status.idle":"2021-10-16T22:46:08.422925Z","shell.execute_reply.started":"2021-10-16T22:46:08.303268Z","shell.execute_reply":"2021-10-16T22:46:08.421762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:46:08.552289Z","iopub.execute_input":"2021-10-16T22:46:08.552534Z","iopub.status.idle":"2021-10-16T22:46:08.556301Z","shell.execute_reply.started":"2021-10-16T22:46:08.552507Z","shell.execute_reply":"2021-10-16T22:46:08.555388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-learn  -U","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:46:16.40488Z","iopub.execute_input":"2021-10-16T22:46:16.40524Z","iopub.status.idle":"2021-10-16T22:46:23.56289Z","shell.execute_reply.started":"2021-10-16T22:46:16.405208Z","shell.execute_reply":"2021-10-16T22:46:23.562047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport sklearn\nimport gc\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:46:23.566241Z","iopub.execute_input":"2021-10-16T22:46:23.56648Z","iopub.status.idle":"2021-10-16T22:46:26.834675Z","shell.execute_reply.started":"2021-10-16T22:46:23.566453Z","shell.execute_reply":"2021-10-16T22:46:26.833928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 47","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:46:26.836568Z","iopub.execute_input":"2021-10-16T22:46:26.836829Z","iopub.status.idle":"2021-10-16T22:46:26.84287Z","shell.execute_reply.started":"2021-10-16T22:46:26.836798Z","shell.execute_reply":"2021-10-16T22:46:26.841333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabular Playground Series - Oct 2021\n\nThe tabular series on kaggle are meant to help novices in data science field like me get acquainted with kaggle competitions.\n\nThe dataset created for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties.\n\nThe first step in almost every data science project is to perfom some exploratory data analysis, which is already done in a previous notebook [1]. Here we will try to use some assumption based on that analysis to\nverify whether we can obtaion better performance. To begin with, we will use only 100k samples of the original data. Here we try some baseline models and also do some grid search on some data transformation methods.\n\n[1] https://www.kaggle.com/peressim/tabular-playground-series-oct-2021-eda","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, x, y):\n    y_pred = model.predict(x)\n    y_pred_prob = model.predict_proba(x)[:, 1]\n    auc_roc = roc_auc_score(y, y_pred_prob)\n    return {'auc_roc_curve' : auc_roc}","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:46:26.844322Z","iopub.execute_input":"2021-10-16T22:46:26.844833Z","iopub.status.idle":"2021-10-16T22:46:26.851594Z","shell.execute_reply.started":"2021-10-16T22:46:26.844797Z","shell.execute_reply":"2021-10-16T22:46:26.850909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(seed)\nn = 1000000\ns = 100000\nskip = sorted(random.sample(range(n),n-s))\n\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-oct-2021/train.csv', sep=',', skiprows=skip)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:46:26.852895Z","iopub.execute_input":"2021-10-16T22:46:26.853194Z","iopub.status.idle":"2021-10-16T22:47:01.479329Z","shell.execute_reply.started":"2021-10-16T22:46:26.85316Z","shell.execute_reply":"2021-10-16T22:47:01.478121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:01.484057Z","iopub.execute_input":"2021-10-16T22:47:01.484578Z","iopub.status.idle":"2021-10-16T22:47:01.543373Z","shell.execute_reply.started":"2021-10-16T22:47:01.484539Z","shell.execute_reply":"2021-10-16T22:47:01.542747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Dataset shape: ', train_df.shape )","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:01.54705Z","iopub.execute_input":"2021-10-16T22:47:01.548922Z","iopub.status.idle":"2021-10-16T22:47:01.557408Z","shell.execute_reply.started":"2021-10-16T22:47:01.548887Z","shell.execute_reply":"2021-10-16T22:47:01.556581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:01.561835Z","iopub.execute_input":"2021-10-16T22:47:01.563698Z","iopub.status.idle":"2021-10-16T22:47:01.59837Z","shell.execute_reply.started":"2021-10-16T22:47:01.563664Z","shell.execute_reply":"2021-10-16T22:47:01.597746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity check for balanced number of classes in the target variable\n\nsns.countplot(train_df['target'])\nplt.title('Distribution of classes in target variable (target) \\n')\nplt.xlabel('Target')\nplt.ylabel('Count')","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:01.603002Z","iopub.execute_input":"2021-10-16T22:47:01.60484Z","iopub.status.idle":"2021-10-16T22:47:01.929196Z","shell.execute_reply.started":"2021-10-16T22:47:01.604804Z","shell.execute_reply":"2021-10-16T22:47:01.928072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data splitting\n\nHere we split the data into train and test sets","metadata":{}},{"cell_type":"code","source":"def get_train_test_split(test_size=0.2):\n    x_train = train_df.drop(['id', 'target'], axis=1).values\n    y_train = train_df['target'].values \n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = test_size, random_state = seed)\n\n    print('x_train', x_train.shape, 'y_train', y_train.shape)\n    print('x_test', x_test.shape, 'y_test', y_test.shape)\n    return x_train, x_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:01.93107Z","iopub.execute_input":"2021-10-16T22:47:01.931275Z","iopub.status.idle":"2021-10-16T22:47:01.942554Z","shell.execute_reply.started":"2021-10-16T22:47:01.931251Z","shell.execute_reply":"2021-10-16T22:47:01.941584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = get_train_test_split()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:01.965191Z","iopub.execute_input":"2021-10-16T22:47:01.96834Z","iopub.status.idle":"2021-10-16T22:47:02.762211Z","shell.execute_reply.started":"2021-10-16T22:47:01.9683Z","shell.execute_reply":"2021-10-16T22:47:02.761351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline\n\nBaseline models to choose the methods that will allow the model to achieve a good performance.\n\n1 - Logistic Regression\n\n2 - XGBoost\n\nAll the tests will be based on a fraction of 10% of all available data","metadata":{}},{"cell_type":"markdown","source":"# Experiment - 1\n\nHere we just test the performance of logistic regression and an xgboost without in the dataset as it is. No preprocessing in the data is performed.","metadata":{}},{"cell_type":"code","source":"print(\"Fitting a simple Logistic Regression model\")\nmodel = LogisticRegression(random_state=seed, solver='liblinear')\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:08:22.780737Z","iopub.execute_input":"2021-10-09T16:08:22.781418Z","iopub.status.idle":"2021-10-09T16:08:34.516557Z","shell.execute_reply.started":"2021-10-09T16:08:22.781373Z","shell.execute_reply":"2021-10-09T16:08:34.515759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Fitting XGBoost Classifier\")\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:08:34.518229Z","iopub.execute_input":"2021-10-09T16:08:34.518832Z","iopub.status.idle":"2021-10-09T16:08:40.23674Z","shell.execute_reply.started":"2021-10-09T16:08:34.518777Z","shell.execute_reply":"2021-10-09T16:08:40.235932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GridCV on Data Preparation\n\nBefore doing anything else with the baseline models, we will perform a grid search on data preprocessing techniques.\n\nReferences:\n\n    [1]  https://machinelearningmastery.com/grid-search-data-preparation-techniques/","metadata":{}},{"cell_type":"code","source":"# get modeling pipelines to evaluate\ndef get_pipelines(model):\n    pipelines = list()\n    # normalize\n    p = Pipeline([('s',MinMaxScaler()), ('m',model)])\n    pipelines.append(('norm', p))\n    # standardize\n    p = Pipeline([('s',StandardScaler()), ('m',model)])\n    pipelines.append(('std', p))\n    # quantile\n    p = Pipeline([('s',QuantileTransformer(n_quantiles=100, output_distribution='normal')), ('m',model)])\n    pipelines.append(('quan', p))\n    # pca\n    p = Pipeline([('s',PCA()), ('m',model)])\n    pipelines.append(('pca', p))\n    # svd\n    p = Pipeline([('s',TruncatedSVD()), ('m',model)])\n    pipelines.append(('svd', p))\n    \n    p = Pipeline([('s',StandardScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('std-power', p))\n    # scale and power\n    p = Pipeline([('s',MinMaxScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('min-max-power', p))\n    \n    p = Pipeline([('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('power', p))\n    \n    return pipelines","metadata":{"execution":{"iopub.status.busy":"2021-10-11T17:10:21.824426Z","iopub.execute_input":"2021-10-11T17:10:21.824724Z","iopub.status.idle":"2021-10-11T17:10:21.834854Z","shell.execute_reply.started":"2021-10-11T17:10:21.824693Z","shell.execute_reply":"2021-10-11T17:10:21.834121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_model(x, y, model):\n    # define the cross-validation procedure\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # evaluate model\n    scores = cross_val_score(model, x, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    return scores","metadata":{"execution":{"iopub.status.busy":"2021-10-11T17:10:23.503281Z","iopub.execute_input":"2021-10-11T17:10:23.503865Z","iopub.status.idle":"2021-10-11T17:10:23.50903Z","shell.execute_reply.started":"2021-10-11T17:10:23.503825Z","shell.execute_reply":"2021-10-11T17:10:23.508271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression - Pipelines","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression(random_state=seed, solver='liblinear')\npipelines = get_pipelines(model)\nx_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \n\n# evaluate each pipeline\nresults, names = list(), list()\nfor name, pipeline in pipelines:\n\t# evaluate\n\tscores = score_model(x_train, y_train, pipeline)\n\t# summarize\n\tprint('>%s: %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n\t# store\n\tresults.append(scores)\n\tnames.append(name)\n\n# No Transform\nscores = score_model(x_train, y_train, model)\nprint('>%s: %.3f (%.3f)' % ('No-transform', np.mean(scores), np.std(scores)))\nresults.append(scores)\nnames.append('No-transform')","metadata":{"execution":{"iopub.status.busy":"2021-10-09T16:15:31.888475Z","iopub.execute_input":"2021-10-09T16:15:31.88896Z","iopub.status.idle":"2021-10-09T17:33:49.465569Z","shell.execute_reply.started":"2021-10-09T16:15:31.88891Z","shell.execute_reply":"2021-10-09T17:33:49.46444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-09T17:33:49.467345Z","iopub.execute_input":"2021-10-09T17:33:49.46766Z","iopub.status.idle":"2021-10-09T17:33:49.851268Z","shell.execute_reply.started":"2021-10-09T17:33:49.467621Z","shell.execute_reply":"2021-10-09T17:33:49.850441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost - Pipelines","metadata":{}},{"cell_type":"code","source":"# model = XGBClassifier(random_state=seed, verbosity=0)\nmodel = XGBClassifier(random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\npipelines = get_pipelines(model)\nx_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \n\n# evaluate each pipeline\nresults, names = list(), list()\nfor name, pipeline in pipelines:\n\t# evaluate\n\tscores = score_model(x_train, y_train, pipeline)\n\t# summarize\n\tprint('>%s: %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n\t# store\n\tresults.append(scores)\n\tnames.append(name)\n\n# No Transform\nscores = score_model(x_train, y_train, model)\nprint('>%s: %.3f (%.3f)' % ('No-transform', np.mean(scores), np.std(scores)))\nresults.append(scores)\nnames.append('No-transform')","metadata":{"execution":{"iopub.status.busy":"2021-10-11T17:10:37.102418Z","iopub.execute_input":"2021-10-11T17:10:37.102676Z","iopub.status.idle":"2021-10-11T18:13:59.886183Z","shell.execute_reply.started":"2021-10-11T17:10:37.10265Z","shell.execute_reply":"2021-10-11T18:13:59.885203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = dict(zip(names, np.mean(results, axis=1).tolist()))\nn = max(r, key=r.get)\nprint(n, r[n])\ndict(zip(names, np.mean(results, axis=1).tolist()))","metadata":{"execution":{"iopub.status.busy":"2021-10-11T18:35:22.793575Z","iopub.execute_input":"2021-10-11T18:35:22.794238Z","iopub.status.idle":"2021-10-11T18:35:22.803486Z","shell.execute_reply.started":"2021-10-11T18:35:22.794196Z","shell.execute_reply":"2021-10-11T18:35:22.802643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T18:27:51.966657Z","iopub.execute_input":"2021-10-11T18:27:51.967246Z","iopub.status.idle":"2021-10-11T18:27:52.263972Z","shell.execute_reply.started":"2021-10-11T18:27:51.967213Z","shell.execute_reply":"2021-10-11T18:27:52.263202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\n<h3>Best results</h3>\n\nLogistic Regression when used with PowerTransformer overcome all the XGBoost.\nXGBoost - No transform (any other transformation gives the same result, except when it is used with svd)\n","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering - Creating some synthetic features","metadata":{}},{"cell_type":"markdown","source":"# Logistic regression\n\nWe create synthetic features and use power transform as well","metadata":{}},{"cell_type":"code","source":"geomean = lambda x, axis : np.exp(np.mean(np.log(x), axis=axis))\nharmonic_mean = lambda x, axis : len(x) / np.sum(1.0/x, axis=axis) \n\nfuncs = {'mean' : np.mean, \n         'std' : np.std, \n         'var' : np.var, \n         'geo_mean' : geomean, \n         'harmonic_mean' : harmonic_mean, \n         'median' : np.median}","metadata":{"execution":{"iopub.status.busy":"2021-10-10T03:15:32.38879Z","iopub.execute_input":"2021-10-10T03:15:32.389362Z","iopub.status.idle":"2021-10-10T03:15:32.394895Z","shell.execute_reply.started":"2021-10-10T03:15:32.389325Z","shell.execute_reply":"2021-10-10T03:15:32.394023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results, names = list(), list()\np = PowerTransformer()\n\nfor key in funcs.keys():\n    x = train_df.drop(['id', 'target'], axis=1)\n    x[key] = funcs[key](x, axis=1)\n    y = train_df['target']\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = seed)\n    x_train = p.fit_transform(x_train)\n    x_test = p.transform(x_test)\n    model = LogisticRegression(random_state=seed, solver='liblinear')\n    \n    model.fit(x_train, y_train)\n    model.score(x_test, y_test)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['auc_roc_curve'])\n\nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T03:16:39.037145Z","iopub.execute_input":"2021-10-10T03:16:39.037947Z","iopub.status.idle":"2021-10-10T03:20:36.260196Z","shell.execute_reply.started":"2021-10-10T03:16:39.037901Z","shell.execute_reply":"2021-10-10T03:20:36.259408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"results, names = list(), list()\n\nfor key in funcs.keys():\n    x = train_df.drop(['id', 'target'], axis=1)\n    x[key] = funcs[key](x, axis=1)\n    y = train_df['target']\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = seed)\n    model = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    \n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['auc_roc_curve'])\n\nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T03:23:37.193483Z","iopub.execute_input":"2021-10-10T03:23:37.194073Z","iopub.status.idle":"2021-10-10T03:23:56.073292Z","shell.execute_reply.started":"2021-10-10T03:23:37.194037Z","shell.execute_reply":"2021-10-10T03:23:56.072533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic regression showed the best results. It seems to work well with mean or std, however std was almost identical it was a little better than the mean. XGboost present the best result with the var as synthetic feature. \n\nLogistic - std\n\nXGboost - var","metadata":{}},{"cell_type":"code","source":"del x, y\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grid Search\n\nReferences:\n\n    [1] https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"p = PowerTransformer()\nx_train = train_df.drop(['id', 'target'], axis=1)\nx_train['std'] = np.std(x_train, axis=1)\ny_train = train_df['target']\nx_train = p.fit_transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:09:50.779527Z","iopub.execute_input":"2021-10-11T16:09:50.779825Z","iopub.status.idle":"2021-10-11T16:10:48.17192Z","shell.execute_reply.started":"2021-10-11T16:09:50.779795Z","shell.execute_reply":"2021-10-11T16:10:48.171021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"space = dict()\nspace['solver'] = ['liblinear', 'newton-cg', 'lbfgs']\nspace['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nspace['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\nmodel = LogisticRegression(random_state=seed, verbose=0)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\nsearch = GridSearchCV(model, space, scoring='roc_auc', n_jobs=-1, cv=cv)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:21:54.286217Z","iopub.execute_input":"2021-10-11T16:21:54.287211Z","iopub.status.idle":"2021-10-11T16:21:54.294729Z","shell.execute_reply.started":"2021-10-11T16:21:54.28712Z","shell.execute_reply":"2021-10-11T16:21:54.293876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = search.fit(x_train, y_train)\n# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:21:58.156937Z","iopub.execute_input":"2021-10-11T16:21:58.157595Z","iopub.status.idle":"2021-10-11T16:28:45.808741Z","shell.execute_reply.started":"2021-10-11T16:21:58.157552Z","shell.execute_reply":"2021-10-11T16:28:45.807317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-10-11T16:41:24.60863Z","iopub.execute_input":"2021-10-11T16:41:24.608984Z","iopub.status.idle":"2021-10-11T16:41:24.616673Z","shell.execute_reply.started":"2021-10-11T16:41:24.608945Z","shell.execute_reply":"2021-10-11T16:41:24.615892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"p = PowerTransformer()\nx_train = train_df.drop(['id', 'target'], axis=1)\nx_train['var'] = np.var(x_train, axis=1)\ny_train = train_df['target']\nx_train = p.fit_transform(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:02.763713Z","iopub.execute_input":"2021-10-16T22:47:02.764157Z","iopub.status.idle":"2021-10-16T22:47:47.019561Z","shell.execute_reply.started":"2021-10-16T22:47:02.76412Z","shell.execute_reply":"2021-10-16T22:47:47.018782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators' : [1000, 1290, 1295, 1300, 1305, 1310, 1315, 1325],\n          'max_depth' : [3, 4],\n          'subsample' : [0.8, 0.9, 1.0],\n          'eta' : [0.12],\n          'colsample_bytree' : [0.3, 0.4],\n          'min_child_weight': [5],\n          'gamma': [5],\n         }\n\nmodel = XGBClassifier(random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\nsearch = GridSearchCV(model, param_grid=params, scoring='roc_auc', refit='roc_auc', n_jobs=-1, cv=cv)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:47.022664Z","iopub.execute_input":"2021-10-16T22:47:47.022891Z","iopub.status.idle":"2021-10-16T22:47:47.028906Z","shell.execute_reply.started":"2021-10-16T22:47:47.022865Z","shell.execute_reply":"2021-10-16T22:47:47.028148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = search.fit(x_train, y_train)\n# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T22:47:47.030514Z","iopub.execute_input":"2021-10-16T22:47:47.031035Z","iopub.status.idle":"2021-10-17T00:02:07.111795Z","shell.execute_reply.started":"2021-10-16T22:47:47.030999Z","shell.execute_reply":"2021-10-17T00:02:07.110896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Individual Search on Full data set\n\nNow with best params, we will do an individual search on the full dataset since we were using only 10% in the grid search.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-oct-2021/train.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:07:09.189472Z","iopub.execute_input":"2021-10-17T00:07:09.190166Z","iopub.status.idle":"2021-10-17T00:07:46.76961Z","shell.execute_reply.started":"2021-10-17T00:07:09.190127Z","shell.execute_reply":"2021-10-17T00:07:46.768682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = PowerTransformer()\nx_train = train_df.drop(['id', 'target'], axis=1)\nx_train['var'] = np.var(x_train, axis=1)\ny_train = train_df['target']\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)\nx_train = p.fit_transform(x_train)\nx_test = p.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:07:46.771843Z","iopub.execute_input":"2021-10-17T00:07:46.772316Z","iopub.status.idle":"2021-10-17T00:13:42.274393Z","shell.execute_reply.started":"2021-10-17T00:07:46.772278Z","shell.execute_reply":"2021-10-17T00:13:42.273433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = result.best_params_.copy()\nprint(params)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:13:42.277491Z","iopub.execute_input":"2021-10-17T00:13:42.277837Z","iopub.status.idle":"2021-10-17T00:13:42.283302Z","shell.execute_reply.started":"2021-10-17T00:13:42.2778Z","shell.execute_reply":"2021-10-17T00:13:42.282216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>1 - Testing different number of estimators</h3>","metadata":{}},{"cell_type":"code","source":"results_trees = {}\ntrees = [100, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 950, 975, 1000, 1025, 1050, 1100, 1150, 1290, 1295, 1300, 1305, 1310, 1315, 1325, 2000]\nfor n in trees:\n    params['n_estimators'] = n\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_trees[n] = result['auc_roc_curve']\n    print('n_estimators:', n, 'auc_roc_curve:', results_trees[n])\n\nbest_nestimator = max(results_trees, key=results_trees.get)\nprint('\\nBest n_estimators:', best_nestimator, 'AUCROC score:', results_trees[best_nestimator])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:13:42.286628Z","iopub.execute_input":"2021-10-17T00:13:42.287346Z","iopub.status.idle":"2021-10-17T00:32:28.366257Z","shell.execute_reply.started":"2021-10-17T00:13:42.287306Z","shell.execute_reply":"2021-10-17T00:32:28.365441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>2 - Testing different max_depth</h3>","metadata":{}},{"cell_type":"code","source":"results_max_depths = {}\nparams['n_estimators'] = best_nestimator\nmax_depths = [i for i in range(1,5)]\n\nfor max_depth in max_depths:\n    params['max_depth'] = max_depth\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_max_depths[max_depth] = result['auc_roc_curve']\n    print('max_depth:', max_depth, 'auc_roc_curve:', results_max_depths[max_depth])\n\nbest_max_depth = max(results_max_depths, key=results_max_depths.get)\nprint('\\nBest max_depth:', best_max_depth, 'AUCROC score:', results_max_depths[best_max_depth])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:33:10.598316Z","iopub.execute_input":"2021-10-17T00:33:10.598596Z","iopub.status.idle":"2021-10-17T00:36:18.626858Z","shell.execute_reply.started":"2021-10-17T00:33:10.598565Z","shell.execute_reply":"2021-10-17T00:36:18.626031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Testing different subsamples</h3>","metadata":{}},{"cell_type":"code","source":"results_subsamples = {}\nparams['max_depth'] = best_max_depth\nsubsamples = [i for i in np.arange(0.1, 1.1, 0.1)]\n\nfor subsample in subsamples:\n    params['subsample'] = subsample\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_subsamples[subsample] = result['auc_roc_curve']\n    print('subsample:', subsample, 'auc_roc_curve:', results_subsamples[subsample])\n\nbest_subsample = max(results_subsamples, key=results_subsamples.get)\nprint('\\nBest subsample:', best_subsample, 'AUCROC score:', results_subsamples[best_subsample])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:42:42.869158Z","iopub.execute_input":"2021-10-17T00:42:42.869889Z","iopub.status.idle":"2021-10-17T00:50:33.342087Z","shell.execute_reply.started":"2021-10-17T00:42:42.869841Z","shell.execute_reply":"2021-10-17T00:50:33.340599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>4 - Testing different learning rates</h3>","metadata":{}},{"cell_type":"code","source":"results_etas = {}\nparams['subsample'] = best_subsample\netas = [0.0001, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.12, 0.13, 0.3, 0.5, 1.0]\n\nfor eta in etas:\n    params['eta'] = eta\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_etas[eta] = result['auc_roc_curve']\n    print('eta:', eta, 'auc_roc_curve:', results_etas[eta])\n\nbest_eta = max(results_etas, key=results_etas.get)\nprint('\\nBest eta:', best_eta, 'AUCROC score:', results_etas[best_eta])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T00:51:13.977557Z","iopub.execute_input":"2021-10-17T00:51:13.978145Z","iopub.status.idle":"2021-10-17T01:02:28.265654Z","shell.execute_reply.started":"2021-10-17T00:51:13.978107Z","shell.execute_reply":"2021-10-17T01:02:28.264203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>5 - Testing different number of features</h3>","metadata":{}},{"cell_type":"code","source":"results_colsample_bytrees = {}\nparams['eta'] = best_eta\ncolsample_bytrees = [i for i in np.arange(0.1, 1.1, 0.1)]\n\nfor colsample_bytree in colsample_bytrees:\n    params['colsample_bytree'] = colsample_bytree\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_colsample_bytrees[colsample_bytree] = result['auc_roc_curve']\n    print('colsample_bytree:', colsample_bytree, 'auc_roc_curve:', results_colsample_bytrees[colsample_bytree])\n\nbest_colsample_bytree = max(results_colsample_bytrees, key=results_colsample_bytrees.get)\nprint('\\nBest colsample_bytree:', best_colsample_bytree, 'AUCROC score:', results_colsample_bytrees[best_colsample_bytree])\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:02:28.267541Z","iopub.execute_input":"2021-10-17T01:02:28.26791Z","iopub.status.idle":"2021-10-17T01:11:03.027534Z","shell.execute_reply.started":"2021-10-17T01:02:28.267871Z","shell.execute_reply":"2021-10-17T01:11:03.026073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> 6 - Testing different values for min_child_weight</h3>","metadata":{}},{"cell_type":"code","source":"results_min_child_weight = {}\nparams['colsample_bytree'] = best_colsample_bytree\nmin_child_weights = [i for i in range(1,10)]\n\nfor min_child_weight in min_child_weights:\n    params['min_child_weight'] = min_child_weight\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_min_child_weight[min_child_weight] = result['auc_roc_curve']\n    print('min_child_weight:', min_child_weight, 'auc_roc_curve:', results_min_child_weight[min_child_weight])\n\nbest_min_child_weight = max(results_min_child_weight, key=results_min_child_weight.get)\nprint('\\nBest min_child_weight:', best_min_child_weight, 'AUCROC score:', results_min_child_weight[best_min_child_weight])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:11:03.02908Z","iopub.execute_input":"2021-10-17T01:11:03.029348Z","iopub.status.idle":"2021-10-17T01:18:45.300923Z","shell.execute_reply.started":"2021-10-17T01:11:03.029313Z","shell.execute_reply":"2021-10-17T01:18:45.300137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> 7 - Testing different values for gamma</h3>","metadata":{}},{"cell_type":"code","source":"results_gamma = {}\nparams['min_child_weight'] = best_min_child_weight\ngammas = [0.01, 0.02, 0.03, 0.1, 0.3, 0.5, 1, 1.1, 1.5, 2, 5, 7, 9, 10]\n\nfor gamma in gammas:\n    params['gamma'] = gamma\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_gamma[gamma] = result['auc_roc_curve']\n    print('gamma:', gamma, 'auc_roc_curve:', results_gamma[gamma])\n\nbest_gamma = max(results_gamma, key=results_gamma.get)\nprint('\\nBest gamma:', best_gamma, 'AUCROC score:', results_gamma[best_gamma])","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:18:45.303037Z","iopub.execute_input":"2021-10-17T01:18:45.303338Z","iopub.status.idle":"2021-10-17T01:30:41.448559Z","shell.execute_reply.started":"2021-10-17T01:18:45.303301Z","shell.execute_reply":"2021-10-17T01:30:41.447746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['gamma'] = best_gamma\nprint('Best Score', results_gamma[best_gamma])\nprint('Best Hyperparameters:', params)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:52:57.83356Z","iopub.execute_input":"2021-10-17T01:52:57.834176Z","iopub.status.idle":"2021-10-17T01:52:57.841602Z","shell.execute_reply.started":"2021-10-17T01:52:57.834135Z","shell.execute_reply":"2021-10-17T01:52:57.839985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:57:40.576565Z","iopub.execute_input":"2021-10-17T01:57:40.576927Z","iopub.status.idle":"2021-10-17T01:58:32.47851Z","shell.execute_reply.started":"2021-10-17T01:57:40.57689Z","shell.execute_reply":"2021-10-17T01:58:32.477718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del x_train, x_test, y_train, y_test, train_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:59:51.945468Z","iopub.execute_input":"2021-10-17T01:59:51.946092Z","iopub.status.idle":"2021-10-17T01:59:52.177512Z","shell.execute_reply.started":"2021-10-17T01:59:51.946054Z","shell.execute_reply":"2021-10-17T01:59:52.176766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tabular-playground-series-oct-2021/test.csv', sep=',')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T01:59:55.379412Z","iopub.execute_input":"2021-10-17T01:59:55.379737Z","iopub.status.idle":"2021-10-17T02:00:22.178266Z","shell.execute_reply.started":"2021-10-17T01:59:55.379703Z","shell.execute_reply":"2021-10-17T02:00:22.177287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = test_df.drop(['id'], axis=1)\nx_test['var'] = np.var(x_test, axis=1)\nx_test = p.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T02:00:22.180078Z","iopub.execute_input":"2021-10-17T02:00:22.180345Z","iopub.status.idle":"2021-10-17T02:00:34.197982Z","shell.execute_reply.started":"2021-10-17T02:00:22.180312Z","shell.execute_reply":"2021-10-17T02:00:34.197167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = model.predict_proba(x_test)[:, 1]\nids = test_df['id'].values\nsubmission = pd.DataFrame({'id' : ids, 'target' : target})","metadata":{"execution":{"iopub.status.busy":"2021-10-17T02:00:34.199342Z","iopub.execute_input":"2021-10-17T02:00:34.199604Z","iopub.status.idle":"2021-10-17T02:00:37.38083Z","shell.execute_reply.started":"2021-10-17T02:00:34.199572Z","shell.execute_reply":"2021-10-17T02:00:37.380038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T02:00:37.382736Z","iopub.execute_input":"2021-10-17T02:00:37.382999Z","iopub.status.idle":"2021-10-17T02:00:37.394103Z","shell.execute_reply.started":"2021-10-17T02:00:37.382966Z","shell.execute_reply":"2021-10-17T02:00:37.393144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T02:00:37.395863Z","iopub.execute_input":"2021-10-17T02:00:37.396343Z","iopub.status.idle":"2021-10-17T02:00:38.881942Z","shell.execute_reply.started":"2021-10-17T02:00:37.396307Z","shell.execute_reply":"2021-10-17T02:00:38.881096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Next Steps\n\nThe next step is try to improve the results by training both Logistic Regression and XGBoost and then averaging their results. We are going to try it in a new notebook [1] using the best parameters found here so far.\n\n[1] https://www.kaggle.com/peressim/tabular-playground-series-oct-2021-final-models","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}