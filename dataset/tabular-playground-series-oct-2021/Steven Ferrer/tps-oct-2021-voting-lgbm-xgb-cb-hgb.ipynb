{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## TPS Oct. 2021 - Voting LGBM/XGB/CB/Ada/GB/HGB","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport os\nimport logging\nimport sys\nimport time\nfrom datetime import timedelta\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import (HistGradientBoostingClassifier, VotingClassifier)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load datasets","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Refer to https://www.kaggle.com/bextuychiev/how-to-work-w-million-row-datasets-like-a-pro\n\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\". \n              format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndata_dir = \"../input/tabular-playground-series-oct-2021/\"\n\ntrain  = reduce_memory_usage(pd.read_csv(data_dir  + \"train.csv\"))\ntest = reduce_memory_usage(pd.read_csv(data_dir + \"test.csv\"))\nsubmission = reduce_memory_usage(pd.read_csv(data_dir + \"sample_submission.csv\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"code","source":"%%time\n\nTARGET = \"target\"\n\nfeatures = [col for col in train.columns if col not in [\"id\", TARGET]]\nprint(f\"All features {len(features)}:\")\nfor feat in features:\n    print(feat, end=\", \")\nprint(\"\\n\\n\")\n\ncont_features = []\ncat_features = []\nfor feat in features:\n    if \"float\" in str(train[feat].dtype):\n        cont_features.append(feat)\n    else:\n        cat_features.append(feat)\n\nprint(f\"Continuous features {len(cont_features)}\")\nfor feat in cont_features:\n    print(feat, end=\", \")\nprint(\"\\n\\n\")\n\nprint(f\"Categorical (binary) features {len(cat_features)}\")\nfor feat in cat_features:\n    print(feat, end=\", \")\nprint(\"\\n\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ny = train.pop(\"target\")\nX = train.copy()\nX_test = test.drop(\"id\", axis=1).copy()\n\ndel train\ndel test","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"%%time\n\nlgb1_params = {\n    \"random_state\": 42,\n    \"n_estimators\": 1000,\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n}\n\nxgb1_params = {\n    \"random_state\": 42,\n    \"n_estimators\": 1000,\n    \"eval_metric\": \"auc\",\n    \"objective\":\"binary:logistic\",\n    \"booster\": \"gbtree\",\n    ## cpu\n    #\"tree_method\": \"hist\",\n    #\"n_jobs\": -1,\n    ## gpu\n    \"gpu_id\": 0,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\"\n}\n\ncb1_params = {\n    \"random_seed\": 42,\n    \"iterations\": 1000,\n    \"eval_metric\" : \"AUC\",\n    \"verbose\": 0,\n    # gpu\n    \"task_type\" : \"GPU\",\n    \"devices\" : \"0\",\n}\n\nhgb1_params = {\n    \"random_state\": 42,\n    \"max_iter\": 1500,\n    \"scoring\": \"roc_auc\",\n    \"early_stopping\": True,\n}\n\nmeta_models = [\n    (\"lgb1\", LGBMClassifier(**lgb1_params)),\n    (\"xgb1\", XGBClassifier(**xgb1_params)),\n    (\"cb1\", CatBoostClassifier(**cb1_params)),\n    (\"hgb1\", HistGradientBoostingClassifier(**hgb1_params)),\n]\n\nvote1_params = {\n    \"estimators\": meta_models,\n    \"weights\": [3, 2, 2, 1],\n    \"n_jobs\": -1,\n    \"voting\": \"soft\",\n    \"verbose\": True,\n}\n\nmodels = [\n    (\"vote1\", VotingClassifier(**vote1_params))\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef predict_with_model(model, simple_fit=False, splits=5):\n    test_preds = []\n    valid_preds = {}\n    scores = []\n    \n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n    for fold, (idx_train, idx_valid) in enumerate(skf.split(X, y)):\n        start_time = time.monotonic()\n        \n        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n        \n        valid_ids = X_valid.id.values.tolist()\n\n        X_train = X_train[features]\n        X_valid = X_valid[features]\n\n        if simple_fit:\n            model.fit(X_train, y_train)\n        else:\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_valid, y_valid)],\n                early_stopping_rounds=180,\n                verbose=1000\n            )\n        \n        valid_pred = model.predict_proba(X_valid)[:, 1]\n        test_pred = model.predict_proba(X_test)[:, 1]\n        \n        test_preds.append(test_pred)\n        valid_preds.update(dict(zip(valid_ids, valid_pred)))\n\n        score = roc_auc_score(y_valid, valid_pred)\n        \n        end_time = time.monotonic()\n        dur = timedelta(seconds=end_time - start_time)\n        print(f\"Fold {fold} | AUC: {score} | Took: {dur}\")\n        scores.append(score)\n    \n    test_preds = np.mean(np.column_stack(test_preds), axis=1)\n    valid_preds = pd.DataFrame.from_dict(valid_preds, orient=\"index\").reset_index()\n    \n    return test_preds, valid_preds, scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ndef predict_with_models(models):\n    print(f\"Predicting with {len(models)} models...\", end=\"\\n\\n\")\n    for model_name, model in models:\n        start_time = time.monotonic()\n        \n        # Simple fit for voting\n        simple_fit = True\n        \n        print(\"-\" * 50)\n        print(f\"Using {model_name} model...\")\n        test_preds, valid_preds, scores = predict_with_model(model, simple_fit=simple_fit)\n        print(f\"Score: {np.mean(scores)}, Std: {np.std(scores)}\", end=\"\\n\\n\")\n\n        print(\"Saving predictions...\")\n        valid_preds.columns = [\"id\", model_name]\n        valid_preds.to_csv(f\"{model_name}_train.csv\", index=False)\n\n        test_preds_df = pd.DataFrame({\"id\": submission.id, model_name: test_preds})\n        test_preds_df.to_csv(f\"{model_name}_test.csv\", index=False)\n\n        sub = pd.DataFrame({\"id\": submission.id, TARGET: test_preds})\n        sub.to_csv(f\"{model_name}_submission.csv\", index=False)\n        \n        end_time = time.monotonic()\n        dur = timedelta(seconds=end_time - start_time)\n        print(f\"Took: {dur}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npredict_with_models(models)","metadata":{},"execution_count":null,"outputs":[]}]}