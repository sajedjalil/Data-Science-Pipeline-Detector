{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About Tabular Playground Series - Oct 2021\n\nThe dataset used for this competition is synthetic, but based on a real dataset and generated using a CTGAN.\n\nThe dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features.\n\n## Previous notebooks\n\nMy first notebook on this competition explored the data in detail. At this point that notebook has had 38 upvotes and 15 comments, so might be worth a look before moving on to look at this notebook if you have not yet explored the data fully.\nhttps://www.kaggle.com/davidcoxon/first-look-at-october-data\n\nThe second notebook concentrated on feature engineering and tevaluated a number of basic models creating a baseline for parameter tuning. https://www.kaggle.com/davidcoxon/20-model-comparison-oct-tabular-playground\n\n## About this notebook\n\nThis notebook is a cuuenty a work in progress and will be regularly updated, it will concentrate on evaluating the performance of a several hyper tuned models.\n\nThis is a beginner level notebook meant mainly for my own use. Some of the code will be taken from other public notebooks, sources will be creditted at the bottom of the notebook.\n\n## First thoughts on this months project\n\n* This months Tabular Playground Dataset is once again quite large, so managing both cpu usage and ram is going to be an important element of the project.\n* It looks like another classification problem.\n* There is no missing data, so imputing values will not be required.\n* There a both categorical and continuous features. The categorical data is all binary and some of the continuous data appears to be category like. It may be possible to reduce the memory requirements by redefining data types in order to minimize memory use without lossing any meaningful information.\n* Data engineering and feature importance may be important.\n* Its likely that model selection and hyper parameter tuning will be important.\n* Staking, blending and ensambles are likely to be important to get higher scores.\n\n## Exploring the data\n\nYou can find a complete exploration of the data this notebook: https://www.kaggle.com/davidcoxon/first-look-at-october-data/notebook\n\nThe summary of the data exploration is:\n\n* The test dataset is approx 1/2 the size of the training dataset\n* The training dataset is highly representative of the test dataset\n* There is no missing data\n* Approx 1/6th of features are binary features\n* Approx 5/6th of features are continuous features\n* There is relatively low correlation between features\n* There appears to be a relatively high correlation between f22 and target value.\n* The majority of categorical features have a negative correlation to target classification.\n* Continuous feature have show both positive and negative correlations to target classification.\n* feature importance indicates that there are a number of both categorical and continuous features of importance.\n* feature importance doesn't indicate f22 as an important feature.\n\n## Model performance\n\n* The ROC AUC scores fell into 2 groups clustered around scores of 76 and 50.\n* The Boosted classifiers generally produced the best results with Catboost coming top by a small margin. Ridge, Linear Discriminant Analysis and Random forest were also in the higher scoring model.\n* Low scoring models included Gausian naive bayes, k nearest neighbour, linear svc, logistic regression, Stochastic Gradient Descent and passive aggressive models.\n\n* If we take just the categorical features and run the same models we get a range af ROC_AUC scores of between 61.93 and 75.77 in fact 12 of the models produce a score of 75.77.\n\n* If we take just the continuous features and run the models we get a range of ROC_AUC scores of between 49.5 and 63.4, meaning that despite 85% of the features being contimuous on only 15% binary categogical features.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Set up environment","metadata":{}},{"cell_type":"code","source":"%%time\n\nimport os, psutil\nimport gc\n\nimport numpy as np \nimport pandas as pd \nfrom statsmodels.graphics.mosaicplot import mosaic\nfrom scipy.stats import randint\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import ensemble, linear_model,metrics,model_selection,neighbors,preprocessing, svm, tree\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import (AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier,StackingClassifier,VotingClassifier)\nfrom sklearn.feature_selection import mutual_info_regression, SelectKBest, f_classif\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score, mean_squared_error\nfrom sklearn.model_selection import cross_validate,cross_val_score, GridSearchCV,KFold,train_test_split,StratifiedKFold\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder, OneHotEncoder,MinMaxScaler,StandardScaler, RobustScaler\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble, linear_model,neighbors, svm, tree\n\nimport lightgbm as lgb\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom optuna.integration import lightgbm as lgb\nfrom xgboost import XGBClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import (HistGradientBoostingClassifier)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create functions","metadata":{"execution":{"iopub.status.busy":"2021-10-07T02:17:26.652061Z","iopub.execute_input":"2021-10-07T02:17:26.652443Z","iopub.status.idle":"2021-10-07T02:17:26.656859Z","shell.execute_reply.started":"2021-10-07T02:17:26.652405Z","shell.execute_reply":"2021-10-07T02:17:26.65586Z"}}},{"cell_type":"code","source":"%%time\n# taken from https://www.kaggle.com/ryanholbrook/getting-started-september-2021-tabular-playground\n\ndef cpu_stats():\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memory_use = py.memory_info()[0] / 2. ** 30\n    return 'memory GB:' + str(np.round(memory_use, 2))\n\ndef score(X, y, model, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model, X_train, y_train, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\n## from: https://www.kaggle.com/bextuychiev/how-to-work-w-million-row-datasets-like-a-pro\ndef reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df\n\ndef SelectKBestFeatures(features, target, threshold):\n    kbest = SelectKBest(score_func = f_classif, k = len(features.columns))\n    X = kbest.fit_transform(features, target.values.ravel())\n    print('Before the SelectKBest =',features.shape)\n    \n    selected_features = []\n    \n    for i in range(len(features.columns)):\n        if kbest.pvalues_[i]<=threshold:\n            selected_features.append(features.columns[i])\n            \n    X_selected =  pd.DataFrame(X)\n    X_selected.columns = features.columns\n    X_selected = X_selected[selected_features]\n    \n    print('After the SelectKBest = ', X_selected.shape)\n    \n    return X_selected, selected_features\n\nprint('Function built')\n\n# Create an empty dataframe for performance results\nAdvancedModelPerformanced_df = pd.DataFrame(columns=['Score'])\n# Create an empty list for outputs\nOutputs=[]\n\nprint('Dataframe created')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Data","metadata":{}},{"cell_type":"code","source":"%%time\n# Get data\ntrain=pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\ntest=pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\ntest_id=test.id\nprint(\"Data imported\")\n\ntrain = reduce_memory_usage(train, verbose=True)\ntest = reduce_memory_usage(test, verbose=True)\nprint(cpu_stats())\nprint('Memory reduced')\n\nplt.pie([len(train), len(test)], \n        labels=['train', 'test'],\n        colors=['skyblue', 'blue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()\n\n# Merge training and test\ncombine=[train,test]\ncombined=pd.concat(combine)\ncombined = reduce_memory_usage(combined, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add statistical features\n\nWe can add the standard deviation, variance,absolute sum, minimum and maximum values of each row as additional features. ","metadata":{}},{"cell_type":"code","source":"# create lists\nfeatures=[]\ncat_features=[]\ncont_features=[]\n\n# get initial features\nfor item in combined.columns:\n    features.append(item)    \nfeatures.remove('target')\nfor feature in features:\n    if combined.dtypes[feature]=='float16':\n        cont_features.append(feature)\n\n# add std field\nif \"Std\" in combined.columns:\n    print('Std training feature exists')\nelse:\n    combined['std'] = combined[cont_features].std(axis=1)\n    print('Std training feature added')\n    \n# add abs_sum field\nif \"abs_sum\" in combined.columns:\n    print('Abs_sum training feature exists')\nelse:\n    combined['abs_sum'] = combined[cont_features].abs().sum(axis=1)\n    print('Abs_sum training feature added')\n        \n# add var field\nif \"var\" in combined.columns:\n    print('var training feature exists')\nelse:\n    combined['var'] = combined[cont_features].var(axis=1)\n    print('var training feature added')   \n    \n# add min field\nif \"min\" in combined.columns:\n    print('min training feature exists')\nelse:\n    combined['min'] = combined[cont_features].min(axis=1)\n    print('min training feature added') \n    \n# add max field\nif \"max\" in combined.columns:\n    print('max training feature exists')\nelse:\n    combined['max'] = combined[cont_features].max(axis=1)\n    print('max training feature added') \n    \n# Add features to lists\nnew_features=[\"std\",\"abs_sum\",\"var\",\"min\",\"max\"]\nfor item in new_features:\n    features.append(item) \n    cat_features.append(item)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get features","metadata":{}},{"cell_type":"code","source":"# create lists\ncat_features=[]\ncont_features=[]\n\n# get features\nfor feature in features:\n    if combined.dtypes[feature]=='int8':\n        cat_features.append(feature)\n    if combined.dtypes[feature]=='float16':\n        cont_features.append(feature)\n    #print(test.dtypes[feature])\nprint('features obtained')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuous'],\n        colors=['skyblue', 'blue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize data\n\nRobustScaler - Scale features using statistics that are robust to outliers. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). \n\nMinMaxScaler - Scale features between the highest and lowest values making the lowest 0 and the highest 1. Use this option if you are going to use convert the script to convert category like continuous features later on. ","metadata":{}},{"cell_type":"code","source":"%%time\n\n#select scaler (remark out the options you don't want to use)\n#scale = StandardScaler()\nscale = RobustScaler()\n#scale = MinMaxScaler()\n\ncombined[cont_features]=scale.fit_transform(combined[cont_features]) \nprint('Data scaled using : ', scale)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data for modelling\n\nIn this notebook we are really only exploring how different models might perform to identify some models that we'll go on to look at in more detail later. I have therefore split only 10% of the data out for training the models and 5% for testing. This allows the full notebook to run in about an hour. You can increase this to 80% / 20% if you want to test individual models more robustly but its a relatively large dataset and the session will most likely time out if you try and run all cell with 80% of the data.","metadata":{}},{"cell_type":"code","source":"%%time\n# split training and test\ntry:\n    train = combined.iloc[:1000000,:]\n    print(train.shape)\n    test=combined.iloc[1000000:,:]\n    print(test.shape)\n    # delete dataframe\n    del combined\n    print('test/train datasets separated')\nexcept:\n    print('test/train datasets already separated')\n\n#train = reduce_memory_usage(train, verbose=True)\n#test = reduce_memory_usage(test, verbose=True)\n\ntry:\n    y = train.target\n    train.drop(['target'], axis=1, inplace=True)\n    test.drop(['target'], axis=1, inplace=True)\nexcept:\n    print('target already separated')\nX=train#[cat_features] #slice training here to explore feature reduction\nprint('Target data separated')\n# Create data sets for training (80%) and validation (20%)\nX_train, X_val, y_train, y_val = train_test_split(X, y,train_size=0.8,test_size = 0.2,random_state = 0)\nprint('Model data split')\n\nplt.pie([len(X_train), len(X_val),len(X)-((len(X_train)+len(X_val)))], \n        labels=['X-train', 'y-train','usused data'],\n        colors=['skyblue', 'blue','cornflowerblue'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target distribution\n\nWe can chart the distribution of the target values in the training dataset, on the assumption that the test dataset will have a similar distribution this gives us an idea of what the distribution might look like for our predicted results. ","metadata":{}},{"cell_type":"code","source":"sns.distplot(y, kde=True, hist=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observations on feature engineering and data processing\n\n* The test and training data were combined before being normalised and were then separated to ensure that the values were not shifted comparative to each other.\n\n* Normalising the data with robustscaler seemed to produce the best result.\n\n* Initial test showed that removing some features yielded better results but doing this with kbestfeatures proved more effective than doing it manually.\n\n* Converting some numeric category like features to a number of binary features produced better results but look a lot of time / memory and the difference in results was very small.\n\n* The amount of time / processing taken to produce some models was so great that memory reduction was needed, converting some features into object types that look less memory without loosing any meaningful data.\n\n* Good housekeeping, ie deleting temporary data that was no longer required as you went was also necessary to ensure that the notebook did not run out of resources before completing.\n\n* When trying to run several models in the same notebook for comparison it sometimes became necessary to reduce the amount the size of the data used in the train|test split in order to minimize resource use, 60|15 produced similar results to 80\\20.\n\n* Adding features for statistical values std,var, abs sum, min and max for each row seems to improve performance.\n\n## Observations on memory use\n\n* Loading all of the libraries and the functions required in the entire notebook took up 0.3 gb\n\n* Loading the data increase memory usage to 5gb, which came down to 2.5 after the data was compressed.\n\n* producing feature correlations added 0.65gb to memory usage.\n\n* Normalising the data did not affect over all memory usage.\n\n* splitting the data added 0.5gb to memory usage.\n\n* LgbModel added 0.15gb to memory usage.","metadata":{}},{"cell_type":"markdown","source":"# Optimized Models\n\nThe best performing models in the previous notebook https://www.kaggle.com/davidcoxon/20-model-comparison-oct-tabular-playground#Model-Comparison were all boosting models. This notebook will take the 3 main boosting models and build on the default parameters. As well as just running each model with enhanced parameters we will run them with some form of cross fold validation to further improve performance.\n\nGradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.","metadata":{}},{"cell_type":"markdown","source":"## Catboost\nCatboost is a categorizer that used gradient boosting on decision trees.\n\nWe can use CatBoost without any explicit pre-processing to convert categories into numbers. CatBoost converts categorical values into numbers using various statistics on combinations of categorical features and combinations of categorical and numerical features.\n\nIt reduces the need for extensive hyper-parameter tuning and lowers the chances of overfitting also which leads to more generalized models. ","metadata":{}},{"cell_type":"code","source":"%%time\n# tuned catboost model\nmodelname =\"tuned_CatBoost\"\ny_train=y_train.astype(float)\n# set parameters\ncat_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli',\n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 8, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'eval_metric' : 'AUC',\n    'verbose' : 1000,\n    'early_stopping_rounds' : 500,\n}\n# instanciate model\ncat = CatBoostClassifier(**cat_params)\n# fit model\ncat.fit(X_train, y_train)\n\n# evaluate performance\ny_pred = cat.predict(X_val)\ny_pred_proba = cat.predict_proba(X_val)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_val,  y_pred_proba)\nauc = metrics.roc_auc_score(y_val, y_pred_proba)\nprint(auc)\nmetrics.plot_confusion_matrix(cat, X_val, y_val)\nplt.title('Confusion matrix for catboost model')\nplt.grid(False)\nplt.show()\n\nplt.plot(fpr,tpr,label=\"Catboost, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n# Use the model to generate predictions\npredictions2 = cat.predict(test)\n\n# Save the predictions to a CSV file\noutput1 = pd.DataFrame({'Id': test_id,\n                       'target': predictions2})\noutput1.to_csv('tuned_cat_submission.csv', index=False)\nprint('tuned cat submission completed')\n\n# plot distribution of target\nsns.distplot(output1['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output1')\n    \ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=auc\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({'index':modelname,'Score':acc_xgb})\n\n# tidy up\ndel auc,predictions2 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:07:31.514963Z","iopub.execute_input":"2021-10-26T23:07:31.515217Z","iopub.status.idle":"2021-10-26T23:07:31.521472Z","shell.execute_reply.started":"2021-10-26T23:07:31.515186Z","shell.execute_reply":"2021-10-26T23:07:31.520597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Light Gradient Boosting","metadata":{}},{"cell_type":"code","source":"%%time\n## lgb\nmodelname =\"Light_Gradient_Boosting\"\n# set parameters\n\nlgb_params = {'reg_alpha': 8.158768860412389,\n        'reg_lambda': 8.793022151019823,\n        'colsample_bytree': 0.2,\n        'subsample': 0.4,\n        'learning_rate': 0.02,\n        'max_depth': 100,\n        'num_leaves': 12,\n        'min_child_samples': 68,\n        'cat_smooth': 91,\n        'objective': 'binary',  \n        'random_state': 48,\n        'n_estimators': 20000,\n        'n_jobs': -1}\n\nlgbmmodel = LGBMClassifier(**lgb_params)\nlgbmmodel.fit(X_train, y_train)\nprint('model fit')\ny_pred = lgbmmodel.predict(X_val)\nacc_lgbmmodel = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_lgbmmodel)\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(lgbmmodel, X_val, y_val)\nplt.title('Confusion matrix for light gradient boosting')\nplt.grid(False)\nplt.show()\n\n# Use the model to generate predictions\nlgbpredictions = lgbmmodel.predict(test)\n\n# Save the predictions to a CSV file\noutput2 = pd.DataFrame({'Id': test_id,\n                       'target': lgbpredictions})\noutput2.to_csv('tuned_lgbmodel_submission.csv', index=False)\nprint('tuned lgbmodel submission completed')\n\n# plot distribution of target\nsns.distplot(output2['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output2')\n\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=acc_lgbmmodel\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':acc_lgbmmodel})\n    \n# tidy up\ndel acc_lgbmmodel ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:32.378958Z","iopub.execute_input":"2021-10-26T23:08:32.379193Z","iopub.status.idle":"2021-10-26T23:08:32.385478Z","shell.execute_reply.started":"2021-10-26T23:08:32.379166Z","shell.execute_reply":"2021-10-26T23:08:32.384481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## xgboost\n\n\nXGBoost (or Extreme Gradient Boosting) is an implementation of gradient boosted decision trees designed for speed and performance.","metadata":{}},{"cell_type":"code","source":"%%time\n# Taken from https://www.kaggle.com/pallavisinha12/october-playground-series\n\n# tuned xgboost\nmodelname =\"tuned_XGBoost\"\n# set parameters\nxgb_params = {\n    \"subsample\": 0.65,\n    \"colsample_bytree\": 0.4,\n    \"max_depth\": 7,\n    \"learning_rate\": 0.01,\n    \"objective\": \"binary:logistic\",\n    'eval_metric': 'auc',\n    \"nthread\": -1,\n    \"max_bin\": 192, \n    'min_child_weight': 2,\n    'reg_lambda': 0.003,\n    'reg_alpha': 0.02, \n    'seed' : 42,\n    }\n\n# instanciate model\nxgb = XGBClassifier(**xgb_params)\nxgb.fit(X_train, y_train)\nprint('model fit')\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)\n\nfpr, tpr, _ = metrics.roc_curve(y_val,  y_pred)\nauc = metrics.roc_auc_score(y_val, y_pred)\nprint(auc)\nplt.plot(fpr,tpr,label=\"xgb, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(xgb, X_val, y_val)\nplt.title('Confusion matrix for xgb model')\nplt.grid(False)\nplt.show()\n\n# Use the model to generate predictions\nxgbpredictions = xgb.predict(test)\n\n# Save the predictions to a CSV file\noutput3 = pd.DataFrame({'Id': test_id,\n                       'target': xgbpredictions})\noutput3.to_csv('tuned_xgb_submission.csv', index=False)\nprint('tuned xgb submission completed')\n\n# plot distribution of target\nsns.distplot(output3['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output3')\n\n# Add to comparison file\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=auc\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':acc_xgb})\n    \n# tidy up\ndel acc_xgb ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:36.360463Z","iopub.execute_input":"2021-10-26T23:08:36.361442Z","iopub.status.idle":"2021-10-26T23:08:36.366157Z","shell.execute_reply.started":"2021-10-26T23:08:36.3614Z","shell.execute_reply":"2021-10-26T23:08:36.365302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## catboost with kbestfeatures","metadata":{}},{"cell_type":"code","source":"#%%time\n# based on https://www.kaggle.com/pallavisinha12/october-playground-series\n\n# kbestfeatures\nmodelname =\"catboost_kbest\"\np_feature = 0.0001\ntrain_numerical, selected_numerical = SelectKBestFeatures(train[cont_features], y, p_feature)\ntrain_categorical, selected_categorical = SelectKBestFeatures(train[cat_features], y, p_feature)\n\ncatboost_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli',\n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 8, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'eval_metric' : 'AUC',\n    'verbose' : 1000,\n    'early_stopping_rounds' : 500,\n}\n\npreds = np.zeros((test.astype('float32')).shape[0])\n\nkf = StratifiedKFold(n_splits = 5, random_state=42,shuffle=True)\n\nauc = []\nn = 0\n\ntry:\n    X.drop(['kfold'], axis=1, inplace=True) # remove columns\n    X.drop('target', axis=1, inplace=True) # remove columns\nexcept:\n    a=\"do nothing\"# do nothing\n\nfor train_idx, test_idx in kf.split(X,y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n    cat_model = CatBoostClassifier(**catboost_params)\n    y_train=y_train.astype('float32')\n    X_train=X_train.astype('float32')\n    y_val=y_val.astype('float32')\n    X_val=X_val.astype('float32')\n    test=test.astype('float32')\n    cat_model.fit(X_train, y_train, eval_set = [(X_val,y_val)],early_stopping_rounds = 100,verbose=False)\n    preds += cat_model.predict_proba(test)[:,1]/kf.n_splits\n    auc.append(roc_auc_score(y_val, cat_model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1  \n    \nprint(np.mean(auc))\n\n# Save the predictions to a CSV file\noutput4 = pd.DataFrame({'Id': test_id,\n                       'target': preds})\noutput4.to_csv('tuned_catboost_kbest_submission.csv', index=False)\nprint('tuned catboost kbest submission completed')\n\n# plot distribution of target\nsns.distplot(output4['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output4')\n\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=(np.mean(auc))\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':(np.mean(auc))})\n    \n# tidy up\ndel auc ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.525982Z","iopub.status.idle":"2021-10-26T23:08:39.526375Z","shell.execute_reply.started":"2021-10-26T23:08:39.526181Z","shell.execute_reply":"2021-10-26T23:08:39.526201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Light Gradient Boosting with kbestfeatures","metadata":{}},{"cell_type":"code","source":"#%%time\n# kbestfeatures\nmodelname =\"Light_Gradient_Boosting_kbest\"\np_feature = 0.001\ntrain_numerical, selected_numerical = SelectKBestFeatures(train[cont_features], y, p_feature)\ntrain_categorical, selected_categorical = SelectKBestFeatures(train[cat_features], y, p_feature)\n\n#set parameters\nparams={'reg_alpha': 8.158768860412389,\n        'reg_lambda': 8.793022151019823,\n        'colsample_bytree': 0.2,\n        'subsample': 0.4,\n        'learning_rate': 0.02,\n        'max_depth': 100,\n        'num_leaves': 12,\n        'min_child_samples': 68,\n        'cat_smooth': 91,\n        'objective': 'binary',  \n        'random_state': 48,\n        'n_estimators': 20000,\n        'n_jobs': -1}\n\n#create preds\npreds = np.zeros(test.shape[0])\n\nkf = StratifiedKFold(n_splits = 5, random_state=42,shuffle=True)\nauc = []\nn = 0\n\ntry:\n    X.drop(['kfold'], axis=1, inplace=True) # remove columns\n    X.drop('target', axis=1, inplace=True) # remove columns\nexcept:\n    a=\"do nothing\"# do nothing\n\nfor train_idx, test_idx in kf.split(X,y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n    lgbm_model = LGBMClassifier(**params)\n    lgbm_model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 100, eval_metric = \"auc\", verbose = \"False\")\n    preds += lgbm_model.predict_proba(test)[:,1]/kf.n_splits\n    auc.append(roc_auc_score(y_val, lgbm_model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1  \n    \nprint(np.mean(auc))\n\n# Save the predictions to a CSV file\noutput5 = pd.DataFrame({'Id': test_id,\n                       'target': preds})\noutput5.to_csv('tuned_lgbm_kbest_submission.csv', index=False)\nprint('tuned LGBM kbest submission completed')\n\n# plot distribution of target\nsns.distplot(output5['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output5')\n\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=np.mean(auc)\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':np.mean(auc)})\n    \n# tidy up\ndel auc ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## xgboost with kbestfeatures","metadata":{}},{"cell_type":"code","source":"#%%time\n# based on https://www.kaggle.com/pallavisinha12/october-playground-series\n\n# kbestfeatures\nmodelname =\"xgBoosting_kbest\"\np_feature = 0.0001\ntrain_numerical, selected_numerical = SelectKBestFeatures(train[cont_features], y, p_feature)\ntrain_categorical, selected_categorical = SelectKBestFeatures(train[cat_features], y, p_feature)\n\nxgb_params = {\n    \"subsample\": 0.65,\n    \"colsample_bytree\": 0.4,\n    \"max_depth\": 7,\n    \"learning_rate\": 0.01,\n    \"objective\": \"binary:logistic\",\n    'eval_metric': 'auc',\n    \"nthread\": -1,\n    \"max_bin\": 192, \n    'min_child_weight': 2,\n    'reg_lambda': 0.003,\n    'reg_alpha': 0.02, \n    'seed' : 42,\n    }\n\npreds = np.zeros((test.astype('float32')).shape[0])\n\nkf = StratifiedKFold(n_splits = 5, random_state=42,shuffle=True)\n\nauc = []\nn = 0\n\ntry:\n    X.drop(['kfold'], axis=1, inplace=True) # remove columns\n    X.drop('target', axis=1, inplace=True) # remove columns\nexcept:\n    a=\"do nothing\"# do nothing\n\nfor train_idx, test_idx in kf.split(X,y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(X_train, y_train, eval_set = [(X_val,y_val)],early_stopping_rounds = 100,verbose=0)\n    test=test.astype('float32')\n    y_val=y_val.astype('float32')\n    X_val=X_val.astype('float32')\n    preds += xgb_model.predict_proba(test)[:,1]/kf.n_splits\n    auc.append(roc_auc_score(y_val, xgb_model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1  \n    \nprint(np.mean(auc))\n\n# Save the predictions to a CSV file\noutput6 = pd.DataFrame({'Id': test_id,\n                       'target': preds})\noutput6.to_csv('tuned_xgboost_kbest_submission.csv', index=False)\nprint('tuned xgboost kbest submission completed')\n\n# plot distribution of target\nsns.distplot(output6['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output6')\n\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=(np.mean(auc))\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':(np.mean(auc))})\n    \n# tidy up\ndel auc ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Power averaging\n\nPower average simply takes the average of a number of models, this can reduce problems associated with over fitting but flip side of this is that if some of the better performing models are finding genuinne patterns that the others arn't that averaging several poor models will reduce the score. ","metadata":{}},{"cell_type":"code","source":"# based on https://www.kaggle.com/vamsikrishnab/exploring-submissions-and-power-averaging\n\n# plot models\nhist_data = [output4.target, output5.target, output6.target]\ngroup_labels = ['Catboost', 'lgbm', 'xgboost']\nsns.distplot(hist_data)\nplt.show()\n\noutput7 = output4.copy()\noutput7.loc[:,'target'] = (output4**2 + output5**2 + output6**2)/3\noutput7.to_csv('poweraverage.csv', index=False)\n\n# plot distribution of target\nsns.distplot(output4['target'], kde=True, hist=False)\nsns.distplot(output5['target'], kde=True, hist=False)\nsns.distplot(output6['target'], kde=True, hist=False)\nsns.distplot(output7['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output7')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.535932Z","iopub.status.idle":"2021-10-26T23:08:39.537032Z","shell.execute_reply.started":"2021-10-26T23:08:39.536725Z","shell.execute_reply":"2021-10-26T23:08:39.536786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.53805Z","iopub.status.idle":"2021-10-26T23:08:39.53895Z","shell.execute_reply.started":"2021-10-26T23:08:39.538711Z","shell.execute_reply":"2021-10-26T23:08:39.538733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blending models","metadata":{}},{"cell_type":"code","source":"modelname =\"blending\"\n\n# create test and training df\ntest_df=output5.copy()\ntest_df=train_df.append(output6['target'])\ntest_df.rename(columns=[\"id\",\"pred_1\",\"pred_2\"])\n\ntrain_df=train.insert(1,\"pred_1\",output6['target'],True)\ntrain_df=train_df.insert(1,\"pred_2\",output6['target'],True)\nuseful_features = [\"pred_1\", \"pred_2\"]\n\nNFOLDS = 5\nSEED = 42\n\nfinal_predictions = []\nscores = []\n\nkfold = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, valid_idx) in enumerate(kfold.split(train_df)):\n    xtrain =  train_df.iloc[train_idx].reset_index(drop=True)\n    xvalid = train_df.iloc[valid_idx].reset_index(drop=True)\n    \n    xtest = test_df.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    model = LinearRegression()\n    model.fit(xtrain, ytrain)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\n\noutput12 = pd.DataFrame({'Id': test_id,\n                       'target': np.mean(scores)})\n\n# plot distribution of target\nsns.distplot(output12['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output12')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.542603Z","iopub.status.idle":"2021-10-26T23:08:39.543002Z","shell.execute_reply.started":"2021-10-26T23:08:39.542795Z","shell.execute_reply":"2021-10-26T23:08:39.542816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacking model","metadata":{}},{"cell_type":"code","source":"modelname =\"stacking\"\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# define first layer estamators\nestimators = [\n    (\"catboost\", CatBoostClassifier(n_estimators=10, random_state=42)),\n    (\"lgbm\", LGBMClassifier(random_state=42)),\n    (\"xgboost\", XGBClassifier(random_state=42)),\n]\n\n# create model \nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\n# fit model\nclf.fit(X_train, y_train).score(X_test, y_test)\nprint('model fit')\n\n# evaluate model\ny_pred = clf.predict(X_test)\nacc_clf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_clf)\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nprint(auc)\n\n# plot results\nplt.plot(fpr,tpr,label=\"stacking, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(clf, X_test, y_test)\nplt.title('Confusion matrix for stacking model')\nplt.grid(False)\nplt.show()\n\n# Use the model to generate predictions\nclfpredictions = clf.predict(test)\n\n# Save the predictions to a CSV file\noutput10 = pd.DataFrame({'Id': test_id,\n                       'target': clfpredictions})\noutput10.to_csv('Stacking_submission.csv', index=False)\nprint('Stacking submission completed')\n\n# plot distribution of target\nsns.distplot(output10['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output10')\n\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=acc_clf\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':acc_clf})","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.544406Z","iopub.status.idle":"2021-10-26T23:08:39.544768Z","shell.execute_reply.started":"2021-10-26T23:08:39.544577Z","shell.execute_reply":"2021-10-26T23:08:39.544594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.545701Z","iopub.status.idle":"2021-10-26T23:08:39.546506Z","shell.execute_reply.started":"2021-10-26T23:08:39.546292Z","shell.execute_reply":"2021-10-26T23:08:39.546317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble model\n\nEnsemble models, take results of several models as a first layer and then feed these in as the inputs to secondary model that votes on which of the first layer predictions to use. ","metadata":{}},{"cell_type":"code","source":"modelname =\"ensemble\"\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# define first layer estamators\nestimators = [\n    (\"catoost\", CatBoostClassifier(n_estimators=10, random_state=42)),\n    (\"lgbm\", LGBMClassifier(random_state=42)),\n    (\"xgboost\", XGBClassifier(random_state=42)),\n]\n\n# create model\nvotingC = VotingClassifier(estimators=estimators, voting='soft', n_jobs=4)\n\n# fit model\nvotingC = votingC.fit(X_train, y_train)\nprint('model fit')\n\n# evaluate model\ny_pred = votingC.predict(X_test)\nacc_clf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_clf)\n\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred)\nauc = metrics.roc_auc_score(y_test, y_pred)\nprint(auc)\n\n# plot results\nplt.plot(fpr,tpr,label=\"stacking, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n# create confusion matrix\nmetrics.plot_confusion_matrix(votingC, X_test, y_test)\nplt.title('Confusion matrix for stacking model')\nplt.grid(False)\nplt.show()\n\n# evaluate model\ny_pred = votingC.predict(X_test)\nprint('predictions completed')\n\n# Save the predictions to a CSV file\noutput9 = pd.DataFrame({'Id': test_id,\n                       'target': votingC.predict(test)})\noutput9.to_csv('ensemble_submission.csv', index=False)\nprint('ensemble submission completed')\n\n# plot distribution of target\nsns.distplot(output9['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output9')","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.547817Z","iopub.status.idle":"2021-10-26T23:08:39.548146Z","shell.execute_reply.started":"2021-10-26T23:08:39.547974Z","shell.execute_reply":"2021-10-26T23:08:39.547991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelname =\"folded ensemble\"\n\n# tidy up before modelling\ntry:\n    X.drop(['kfold'], axis=1, inplace=True) # remove columns\n    X.drop('target', axis=1, inplace=True) # remove columns\nexcept:\n    a=\"do nothing\"# do nothing\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n\n# define first layer estamators\nestimators = [\n    (\"catoost\", CatBoostClassifier(n_estimators=10, random_state=42)),\n    (\"lgbm\", LGBMClassifier(random_state=42)),\n    (\"xgboost\", XGBClassifier(random_state=42)),\n]\n\n# kbestfeatures\np_feature = 0.0001\ntrain_numerical, selected_numerical = SelectKBestFeatures(train[cont_features], y, p_feature)\ntrain_categorical, selected_categorical = SelectKBestFeatures(train[cat_features], y, p_feature)\n\n# prepare for folds\npreds = np.zeros((test.astype('float32')).shape[0])\nkf = StratifiedKFold(n_splits = 5, random_state=42,shuffle=True)\nauc = []\nn = 0\n\nfor train_idx, test_idx in kf.split(X,y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n    # create model\n    model = VotingClassifier(estimators=estimators, voting='soft', n_jobs=4)\n    model.fit(X_train, y_train)\n    test=test.astype('float32')\n    y_val=y_val.astype('float32')\n    X_val=X_val.astype('float32')\n    preds += model.predict_proba(test)[:,1]/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1  \n    \nprint(np.mean(auc))\n\n# Save the predictions to a CSV file\noutput11 = pd.DataFrame({'Id': test_id,\n                       'target': preds})\noutput11.to_csv('ensemble_kbest_submission.csv', index=False)\nprint('ensemble kbest submission completed')\n\n# plot distribution of target\nsns.distplot(output11['target'], kde=True, hist=False)\nsns.distplot(y, kde=True, hist=False)\nOutputs.append('output11')\n\ntry:\n    AdvancedModelPerformanced_df.at[modelname,'Score']=(np.mean(auc))\nexcept:\n    AdvancedModelPerformanced_df = AdvancedModelPerformanced_df.append({index:modelname,'Score':(np.mean(auc))})\n    \n# tidy up\ndel auc ","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.549161Z","iopub.status.idle":"2021-10-26T23:08:39.549484Z","shell.execute_reply.started":"2021-10-26T23:08:39.549316Z","shell.execute_reply":"2021-10-26T23:08:39.549332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.550329Z","iopub.status.idle":"2021-10-26T23:08:39.550637Z","shell.execute_reply.started":"2021-10-26T23:08:39.550474Z","shell.execute_reply":"2021-10-26T23:08:39.550491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison","metadata":{}},{"cell_type":"code","source":"%%time\n#order and print model comparison\nAdvancedModelPerformanced_df=AdvancedModelPerformanced_df.sort_values(by='Score', ascending=False)\nprint(AdvancedModelPerformanced_df)\n\n# plot results\nx_pos = [i for i, _ in enumerate(AdvancedModelPerformanced_df.index)]\nplt.bar(x_pos,AdvancedModelPerformanced_df.Score)\nplt.xlabel(\"Model\")\nplt.ylabel(\"Score\")\nplt.title(\"Advanced Model Comparison\")\nplt.xticks(x_pos, AdvancedModelPerformanced_df.index,rotation='vertical')\nplt.show()\n\nAdvancedModelPerformanced_df.to_csv(\"Advanced_model_comparison.csv\")\nprint('Comparison saved as csv')\n\n# plot distribution of target for all models\nsns.distplot(y, kde=True, hist=False, color=\"Red\")\n#sns.distplot(output1['target'], kde=True, hist=False)\n#sns.distplot(output2['target'], kde=True, hist=False)\n#sns.distplot(output3['target'], kde=True, hist=False)\n#sns.distplot(output4['target'], kde=True, hist=False)\n#sns.distplot(output5['target'], kde=True, hist=False)\n#sns.distplot(output6['target'], kde=True, hist=False)\n#sns.distplot(output7['target'], kde=True, hist=False)\n#sns.distplot(output8['target'], kde=True, hist=False)\n#sns.distplot(output9['target'], kde=True, hist=False)\n#sns.distplot(output10['target'], kde=True, hist=False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.551955Z","iopub.status.idle":"2021-10-26T23:08:39.552292Z","shell.execute_reply.started":"2021-10-26T23:08:39.552117Z","shell.execute_reply":"2021-10-26T23:08:39.552135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cpu_stats())","metadata":{"execution":{"iopub.status.busy":"2021-10-26T23:08:39.554139Z","iopub.status.idle":"2021-10-26T23:08:39.554517Z","shell.execute_reply.started":"2021-10-26T23:08:39.554299Z","shell.execute_reply":"2021-10-26T23:08:39.554356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Observations on model comparison\n\n* The boosted classifiers produced the best results in our initial tests so we have gone with light gradient boosting, xgboost and catboost as candidates for parameter turning. I should really have performed my own parameter tuning but at this stage i'm just exploring performance differences and have taken the parameters from the highest scoring public models for now. \n\n* The boosted models with the default parameters score between 75 and 76 in our previous test.\n\n* The boosted models with parameters tuned scored between 76 and 77.\n\n* Using the default setting from the boosting models a stacking model achieved a score of 76.6\n\n* The boost model with kbestfeatures scored between 84 and 85.63\n\n* Power averaging produced a score of 85.55 coming in just between the 2nd and third place booster models on the public score.  \n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"#### NOTE:\n\nThis notebook is possibly more complicated that it needed to be because i wanted to do all of the tuned models, then the kfold versions and then all of the kbestfeatures version in that order, which meant that extra columns were sometimes added only to be removed later. Similarly some functions require particular object types for example as 'float32' while other wanted something else, so some columns had to be reformed several times. This could have been avoided by re-importing the data each time or copying it for each model however because of the size of the dataset this was complicated. These changes make it slighly harder to follow - sorry about that! ","metadata":{}},{"cell_type":"markdown","source":"## Credit were credits due\n\nFirst up thanks to the Kaggle team for the tireless work putting the tabular plaground together. Many thanks to the kaggle and stackoverflow communities and the folks that contitbutor to the various documents for the various python modules, without whom finding solutions to these problems would be so much tougher.\n\nSelectkbest feature selection and lgb parameters from : https://www.kaggle.com/pallavisinha12/october-playground-series by PALLAVI SINHA\n\nCatboost tuning parameters from : https://www.kaggle.com/shenurisumanasekara/tabular-october-catboost by SHENURI SUMANASEKARA\n\nBlending from : https://www.kaggle.com/mmellinger66/tps-oct-2021-the-melling-blend by mmellinger66\n\nIf you found this notebook useful or you have comments please upvote / comment here. Also please do upvote any of the notebooks above if you use them or find them helpful. ","metadata":{}}]}