{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"AutoGluon is an auto-ml package, developed by J Mueller, X Shi, A Smola:\n\nMueller, Jonas, Xingjian Shi, and Alexander Smola. \"Faster, Simpler, More Accurate: Practical Automated Machine Learning with Tabular, Text, and Image Data.\" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n\nFor tabular data, AutoGluon can produce models to predict the values in one column based on the values in the other columns. With just a single call to fit(), you can achieve high accuracy in standard supervised learning tasks (both classification and regression).\n\nIn the economy of a competition it can help you to create benchmarks, get insights on models' workings and accelerate your experimentations.","metadata":{}},{"cell_type":"markdown","source":"Installing the latest Scikit-learn","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn -U","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:44:34.296873Z","iopub.execute_input":"2021-10-02T19:44:34.297639Z","iopub.status.idle":"2021-10-02T19:44:47.832819Z","shell.execute_reply.started":"2021-10-02T19:44:34.297459Z","shell.execute_reply":"2021-10-02T19:44:47.832011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Installing LightGBM for GPU","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall --install-option=--gpu lightgbm","metadata":{"execution":{"iopub.status.busy":"2021-10-02T19:49:01.049687Z","iopub.execute_input":"2021-10-02T19:49:01.049969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Installing AutoGluon","metadata":{}},{"cell_type":"code","source":"!pip install autogluon ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport gc\n\n# Importing AutoGluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# Scikit Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Derived from the original script https://www.kaggle.com/gemartin/load-data-reduce-memory-usage \n# by Guillaume Martin\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data \nX_train = pd.read_csv(\"../input/tabular-playground-series-oct-2021/train.csv\").set_index('id')\nX_test = pd.read_csv(\"../input/tabular-playground-series-oct-2021/test.csv\").set_index('id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering\nunique_values = X_train.iloc[:1000].nunique()\ncategoricals = [col for col in  unique_values.index[unique_values < 10] if col!='target']\nnumeric = [col for col in X_test.columns if col not in categoricals]\n\nX_train['mean_numeric'] = X_train[numeric].mean(axis=1)\nX_train['std_numeric'] = X_train[numeric].std(axis=1)\nX_train['min_numeric'] = X_train[numeric].min(axis=1)\nX_train['max_numeric'] = X_train[numeric].max(axis=1)\nX_train['sum_categoricals'] = X_train[categoricals].sum(axis=1)\n\nX_test['mean_numeric'] = X_test[numeric].mean(axis=1)\nX_test['std_numeric'] = X_test[numeric].std(axis=1)\nX_test['min_numeric'] = X_test[numeric].min(axis=1)\nX_test['max_numeric'] = X_test[numeric].max(axis=1)\nX_test['sum_categoricals'] = X_test[categoricals].sum(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature selection\nfeatures = ['f1', 'f103', 'f107', 'f112', 'f119', 'f12', 'f125', 'f127', 'f13', \n            'f130', 'f134', 'f136', 'f138', 'f139', 'f14', 'f141', 'f143', 'f144', \n            'f150', 'f152', 'f154', 'f156', 'f16', 'f163', 'f169', 'f17', 'f173', \n            'f179', 'f18', 'f187', 'f19', 'f192', 'f195', 'f198', 'f2', 'f20', 'f200', \n            'f201', 'f211', 'f213', 'f214', 'f22', 'f222', 'f227', 'f231', 'f239', 'f241', \n            'f243', 'f247', 'f252', 'f258', 'f26', 'f266', 'f27', 'f29', 'f3', 'f33', 'f4', \n            'f40', 'f42', 'f43', 'f44', 'f48', 'f5', 'f52', 'f53', 'f56', 'f58', 'f6', 'f60', \n            'f62', 'f63', 'f64', 'f65', 'f69', 'f7', 'f71', 'f72', 'f73', 'f74', 'f75', 'f77', \n            'f78', 'f8', 'f82', 'f83', 'f85', 'f86', 'f90', 'f92', 'f93', 'f95', 'f96', 'f98', \n            'f99', 'mean_numeric', 'std_numeric', 'sum_categoricals']\n\nX_train = X_train[features + ['target']]\nX_test = X_test[features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### REDUCE MEMORY USAGE\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALIDATION = False\nif VALIDATION is True:\n    X_train, X_val = train_test_split(X_train, test_size=int(len(X_train) * 0.2), random_state=42)\n    train_data = TabularDataset(X_train)\n    val_data = TabularDataset(X_val)\nelse:\n    train_data = TabularDataset(X_train)\n    val_data = TabularDataset(X_train.iloc[:100_000, :])\n\nSUBSAMPLE = False\nRANDOM_STATE = 0\nif SUBSAMPLE is True:\n    subsample_size = 100_000  # subsample subset of data for faster demo, try setting this to much larger values\n    train_data = train_data.sample(n=subsample_size, random_state=RANDOM_STATE)\n    \ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label = 'target'\nprint(\"Summary of target variable: \\n\", train_data[label].describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir agModels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can actually use the optimized parameters that you can find on public kernels to boost your AutoGluon performances.\n\nFor instance these parameters are from the high scoring notebook https://www.kaggle.com/dlaststark/tps-1021-la-dee-da by DLASTSTARK","metadata":{}},{"cell_type":"code","source":"xgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'use_label_encoder': False,\n    'n_estimators': 10000,\n    'max_depth': 3,\n    'subsample': 0.5,\n    'colsample_bytree': 0.5,\n    'learning_rate': 0.01187,\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor'\n}\n\ncb_params = {\n    'loss_function' : 'CrossEntropy',\n    'eval_metric' : 'AUC',\n    'iterations' : 10000,\n    'grow_policy' : 'SymmetricTree',\n    'use_best_model' : True,\n    'depth' : 5,\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nlgb_params = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'device' : 'gpu'\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_path = 'agModels'  # specifies folder to store trained models\npresets='best_quality'\nmetric = 'roc_auc'\nhours = 5.0\npredictor = (TabularPredictor(label=label, eval_metric=metric, path=save_path)\n             .fit(train_data,\n                  excluded_model_types = ['KNN', 'XT' ,'RF', 'NN', 'FASTAI'],\n                  hyperparameters = {'GBM': lgb_params, \n                                     'CAT': cb_params,\n                                     'XGB': xgb_params\n                                    },\n                  presets=presets,\n                  time_limit= int(60 * 60 * hours))\n            )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = predictor.fit_summary(show_plot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leaderboard = predictor.leaderboard(val_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = TabularDataset(X_test)\ntest_preds = predictor.predict_proba(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting and submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': test_preds.iloc[:,1].ravel()})\n\nsubmission.to_csv(\"./submission.csv\", index=False)\nsubmission.to_csv(\"./agModels/submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}