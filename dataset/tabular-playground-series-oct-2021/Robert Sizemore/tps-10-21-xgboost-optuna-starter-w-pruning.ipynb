{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost Hyperparameter Search\n\nIn this notebook we optimize an XGBoost model using the optuna library along with a [pruner](https://optuna.readthedocs.io/en/stable/reference/pruners.html). For each set of parameters, we perform k-fold cross-validation and our pruner references past models trained on the same data and ends unpromising trials early (i.e. if the AUC on a given fold is too low).","metadata":{}},{"cell_type":"code","source":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 3\nMAX_TREES = 20000\nEARLY_STOP = 150\nNUM_TRIALS = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport time\nimport gc\n\n# Model and evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n# Optuna\nimport optuna\nfrom optuna.visualization import plot_param_importances, plot_parallel_coordinate\nfrom optuna.pruners import PercentilePruner\n\n# Hide warnings (makes optuna output easier to parse)\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Data\n\n1. Load data with `datatable` and convert to `pandas`\n2. Reduce memory usage by downcasting datatypes\n3. Get holdout set from training data using a stratified scheme","metadata":{}},{"cell_type":"code","source":"# Helper function for downcasting \ndef reduce_memory_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col, dtype in df.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            df[col] = pd.to_numeric(df[col], downcast ='integer')\n        elif dtype.name == 'bool':\n            df[col] = df[col].astype('int8')\n        elif dtype.name.startswith('float'):\n            df[col] = pd.to_numeric(df[col], downcast ='float')\n        \n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Load training data\ntrain = dt.fread(r'../input/tabular-playground-series-oct-2021/train.csv').to_pandas()\ntrain = reduce_memory_usage(train)\n\n# Holdout set for testing our models\ntrain, holdout = train_test_split(\n    train,\n    test_size = 0.5,\n    shuffle = True,\n    stratify = train['target'],\n    random_state = RANDOM_SEED,\n)\n\ntrain.reset_index(drop = True, inplace = True)\nholdout.reset_index(drop = True, inplace = True)\n\n# Get features\nfeatures = [x for x in train.columns if x not in ['id','target']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost\n\nWe create a function to train an XGBoost model and return the holdout AUC.","metadata":{}},{"cell_type":"markdown","source":"## 1. Default Parameters","metadata":{}},{"cell_type":"code","source":"# Default XGBoost params, used for ALL models considered\ndefault_params = dict(            \n    random_state = RANDOM_SEED,\n    n_estimators = MAX_TREES,\n    tree_method = 'gpu_hist',\n    predictor = \"gpu_predictor\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Scoring Function\n\n* `model_params` - parameters passed to `XGBClassifier`\n* `fit_params` - parameters passed to the `fit` method","metadata":{}},{"cell_type":"code","source":"def score_xgboost(trial = None, model_params = {}, fit_params = {}):\n    \n    # Store the holdout predictions\n    holdout_preds = np.zeros((holdout.shape[0],))\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        start = time.time()\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        \n        # Define Model\n        model = XGBClassifier(**default_params, **model_params)\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            eval_metric = \"logloss\",\n            early_stopping_rounds = EARLY_STOP,\n            **fit_params\n        )\n        \n        # validation/holdout predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        holdout_preds += model.predict_proba(holdout[features])[:, 1] / NUM_FOLDS\n        valid_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        \n        print(f'Fold {fold} AUC: {round(valid_auc, 6)} in {round((end-start) / 60, 2)} minutes.')\n        \n        time.sleep(0.5)\n        if trial:\n            # Use pruning on fold AUC\n            trial.report(\n                value = valid_auc,\n                step = fold\n            )\n            # prune slow trials and bad fold AUCs\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        \n        \n    return roc_auc_score(holdout['target'], holdout_preds)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Search\n\nTo tweak the pruner consider adding/adjusting the following keyword arguments:\n\n* `percentile` - prunes trial if in lower percentile of trials at a given step\n* `n_startup_trials` - number of trials (models trained) before pruning starts\n* `n_warmup_steps` - number of iterations before pruning checks\n* `interval_steps` - number of iterations between pruning checks\n* `n_min_trials` - skip pruning check if too few trials","metadata":{}},{"cell_type":"code","source":"# Percentile Pruner settings\npruner = PercentilePruner(\n    percentile = 66,\n    n_startup_trials = 5,\n    n_warmup_steps = 0,\n    interval_steps = 1,\n    n_min_trials = 5,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parameter_search(trials):\n    \n    # Optuna objective function\n    def objective(trial):\n        model_params = dict( \n            # default 6\n            max_depth = trial.suggest_int(\n                \"max_depth\", 2, 12\n            ), \n            # default 0.3\n            learning_rate = trial.suggest_loguniform(\n                \"learning_rate\", 0.01, 0.3\n            ),\n            # default 0\n            gamma = trial.suggest_loguniform(\n                \"gamma\", 1e-10, 100\n            ), \n            # default 1\n            min_child_weight = trial.suggest_loguniform(\n                \"min_child_weight\", 1e-2, 1e2\n            ),\n            # default 1\n            subsample = trial.suggest_discrete_uniform(\n                \"subsample\", 0.2, 1.0, 0.01\n            ),\n            # default 1\n            colsample_bytree = trial.suggest_discrete_uniform(\n                \"colsample_bytree\",  0.2, 1.0, 0.01\n            ),\n            # default 1\n            colsample_bylevel = trial.suggest_discrete_uniform(\n                \"colsample_bylevel\",  0.2, 1.0, 0.01\n            ),\n            # default 1\n            reg_lambda = trial.suggest_loguniform(\n                \"reg_lambda\", 1e-10, 100\n            ),\n            # default 0\n            reg_alpha = trial.suggest_loguniform(\n                \"reg_alpha\", 1e-10, 100\n            ),\n        )\n        \n        return score_xgboost(trial, model_params)\n    \n    \n    optuna.logging.set_verbosity(optuna.logging.DEBUG)\n    study = optuna.create_study(pruner = pruner,direction = \"maximize\")\n    \n    # (nearly) defaults\n    study.enqueue_trial({\n        \"max_depth\": 6,\n        'learning_rate': 0.3, \n        'gamma': 1e-10, \n        'min_child_weight': 1.0, \n        'subsample': 1.0,\n        'colsample_bytree': 1.0,\n        'colsample_bylevel': 1.0,\n        'reg_alpha': 1e-10,\n        'reg_lambda': 1.0,\n    })\n    # high auc from previous run\n    study.enqueue_trial({\n        'max_depth': 4, \n        'learning_rate': 0.010283092300598066, \n        'gamma': 0.03506917176837801,\n        'min_child_weight': 0.3878531236460043, \n        'subsample': 0.8900000000000001, \n        'colsample_bytree': 0.69, \n        'colsample_bylevel': 0.24000000000000002, \n        'reg_lambda': 5.051637651463356e-07,\n        'reg_alpha': 30.170712609605435\n    })\n    study.optimize(objective, n_trials=trials)\n    return study","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hide output\nstudy = parameter_search(NUM_TRIALS)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"markdown","source":"## 1. Best Parameters","metadata":{}},{"cell_type":"code","source":"print(\"Best Parameters:\", study.best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Parameter Importances","metadata":{}},{"cell_type":"code","source":"plot_param_importances(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Parallel Coordinate Plot\n\nClick on the vertical axes to see how certain parameter ranges affected the scores","metadata":{}},{"cell_type":"code","source":"# Likely broken on GitHub, view on Kaggle for interactive version\nplot_parallel_coordinate(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Submission","metadata":{}},{"cell_type":"code","source":"%%time\ntrain = dt.fread(r'../input/tabular-playground-series-oct-2021/train.csv').to_pandas()\ntest = dt.fread(r'../input/tabular-playground-series-oct-2021/test.csv').to_pandas()\nsubmission = dt.fread(r'../input/tabular-playground-series-oct-2021/sample_submission.csv').to_pandas()\n\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Similar to scoring function but trains on full data and predicts on test\ndef train_xgboost(folds, model_params = {}, fit_params = {}):\n    \n    # Store the holdout predictions\n    test_preds = np.zeros((test.shape[0],))\n    print('')\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = folds, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['target'])):\n        \n        # Training and Validation Sets\n        start = time.time()\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        \n        # Define Model\n        model = XGBClassifier(**default_params, **model_params)\n        gc.collect()\n        \n        model.fit(\n            X_train, y_train,\n            verbose = False,\n            eval_set = [(X_valid, y_valid)],\n            eval_metric = \"logloss\",\n            early_stopping_rounds = EARLY_STOP,\n            **fit_params\n        )\n        \n        # validation and test predictions\n        valid_preds = model.predict_proba(X_valid)[:, 1]\n        test_preds += model.predict_proba(test[features])[:, 1] / folds\n        \n        # fold auc score\n        fold_auc = roc_auc_score(y_valid, valid_preds)\n        end = time.time()\n        print(f'Fold {fold} AUC: {round(fold_auc, 6)} in {round((end-start) / 60, 2)} minutes.')\n\n        \n    return test_preds","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make submission\nsubmission['target'] = train_xgboost(6, model_params = study.best_params)\nsubmission.to_csv('xgboost_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hope you found this notebook useful, feel free to fork it and adapt it to your own uses.","metadata":{}}]}