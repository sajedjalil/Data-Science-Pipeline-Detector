{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using dataset sandpit_data which is basically taken from the sandpit series of models, in particular I only used the f_pred_* csv's.\n# https://www.kaggle.com/joecooper/tps-oct-joes-sandpit\n# The v prefix shows which version of the notebook generated that output.\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nThis notebook is by no means a tutorial on kmean. I'm just trying out a few ideas and hoping that I can learn a little from it. In particular I enjoyed the notebook https://www.kaggle.com/motchan/tps-oct-2021-kmeans\nIt got me thinking and that is always a good thing. \n\nI was concerned that the basic idea wouldn't work too well since the 280 features in the training data are just not well correlatated to the tartget. Clustering wouldn't change that but could it help. I was scepticle.\nI copied the notebook and ran some tests. I concluded (or at least believed) that it wasn't doing anything helpful. As soon as the sample size increases the value of the cluster features diminihses. Which means that a decent model will find out that info without the need to pre run kmeans. Also the idea of basing the cluster on just a few features didn't feel right. I feel all 280 odd features sit within the same category. Week corrlation with target.\n\nBut now I've trained a few models and would hope that they do have a correlation with the target. Can kmeans generate clusters from these predictions. Can it help spot the circumstances that LightGBM is better than Catboost and XGBoost? \n\nCertainly this concept wasn't included in the list that Raahul had for his experiments in stacking https://www.kaggle.com/raahulsaxena/tps-oct-21-some-experiments-with-stacking/notebook\n\n\nTo set expectations with this notebook - My experience with kmeans would be measured in days (decimal points allowed) rather than years. \nAlso - I've not cleaned this notebook - so it largely follows my though patterns and occassionally goes off tangent into deadends.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom datetime import datetime\nimport gc\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\n\nimport json\nimport optuna\n\nimport lightgbm as lgbm\nimport optuna.integration.lightgbm as lgbo\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport datatable as dt\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom warnings import resetwarnings\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n## Better check what version we are using as I may want to make use of init_model\nimport lightgbm as lgbm\nprint (lgbm.__version__)\n\nfrom sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_train = reduce_mem_usage(pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv'))\ndf_test = reduce_mem_usage (dt.fread('../input/tabular-playground-series-oct-2021/test.csv').to_pandas())\n\nsample_submission = pd.read_csv(\"../input/tabular-playground-series-oct-2021/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_kmeans_train\ndf_kmeans_train = pd.DataFrame(np.zeros(df_train.shape[0])).copy()\ndf_kmeans_test = pd.DataFrame(np.zeros(df_test.shape[0])).copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf_kmeans_train['v6_lgbm1'] = pd.read_csv(\"../input/sandpit-data/v6_f_pred_train.csv\")['pred']\ndf_kmeans_train['v7_xgb0'] = pd.read_csv(\"../input/sandpit-data/v7_f_pred_train.csv\")['pred']\ndf_kmeans_train['v8_xgb1'] = pd.read_csv(\"../input/sandpit-data/v8_f_pred_train.csv\")['pred']\ndf_kmeans_train['v10_catb0'] = pd.read_csv(\"../input/sandpit-data/v10_f_pred_train.csv\")['pred']\ndf_kmeans_train['v11_lgbm0'] = pd.read_csv(\"../input/sandpit-data/v11_f_pred_train.csv\")['pred']\n\n\ndf_kmeans_test['v6_lgbm1'] = pd.read_csv(\"../input/sandpit-data/v6_f_pred_test.csv\")['pred']\ndf_kmeans_test['v7_xgb0'] = pd.read_csv(\"../input/sandpit-data/v7_f_pred_test.csv\")['pred']\ndf_kmeans_test['v8_xgb1'] = pd.read_csv(\"../input/sandpit-data/v8_f_pred_test.csv\")['pred']\ndf_kmeans_test['v10_catb0'] = pd.read_csv(\"../input/sandpit-data/v10_f_pred_test.csv\")['pred']\ndf_kmeans_test['v11_lgbm0'] = pd.read_csv(\"../input/sandpit-data/v11_f_pred_test.csv\")['pred']\n\n\njoe_features = ['v6_lgbm1', 'v7_xgb0', 'v8_xgb1', 'v10_catb0', 'v11_lgbm0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nfor col in joe_features:\n    score = roc_auc_score(y, df_kmeans_train[col])\n    print (\"Joes overall computed score for \", col, \" is \", score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Joes overall computed score for  v10_catb0  is  0.8566436011399643   0.85597\n# Joes overall computed score for  v8_xgb1    is  0.8568621253215738   0.85621\n# Joes overall computed score for  v11_lgbm0  is  0.8569640619014858   0.85638\n# Joes overall computed score for  v7_xgb0    is  0.8570048237818386   0.85645\n# Joes overall computed score for  v6_lgbm1   is  0.8571127311293687   0.85633\n\n# How well does that correlation between my CV and LB score fit\n\n## Not fantastic, but it is what it is\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def diff_checker (col):\n\n    print (\"Working on : \", col)\n\n    diffs_df = df_kmeans_train.copy()\n    diffs_df ['diff'] = (diffs_df[col]-y).abs()\n\n    print (\"Correct prediction\")\n    print (diffs_df['diff'].between(0, 0.5, inclusive=False).value_counts())\n    print ()\n    print ()\n    \n    \n# diff_checker ('v6_lgbm1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in joe_features:\n    diff_checker (col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Stealing some code from https://www.kaggle.com/motchan/tps-oct-2021-kmeans\n\n\nuseful_features = joe_features\ndistortions=[]\nfor i in range(1,14):\n    print (\"woring on : \", i)\n    km = KMeans(n_clusters=i,\n              init=\"k-means++\",\n              max_iter=500,\n              random_state=42)\n    km.fit(df_kmeans_train[useful_features])\n    distortions.append(km.inertia_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(range(1,14),distortions,marker=\"o\")\nplt.xticks(range(1,14))\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Distortion\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## Excatly what is this graph supposed to be telling me ?\n\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = [col for col in df_kmeans_test.columns.tolist()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# useful_features = [\"f22\",\"f179\",\"f69\",\"f156\",\"f58\",\"f136\",\"f214\"]\n\n## okay lets drop the number of cluster to 6 \nn_clusters_1 = 6\ncd_feature = True # cluster distance instead of cluster number\ncluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters_1)]\nkmeans = KMeans(n_clusters=n_clusters_1, init=\"k-means++\", max_iter=500, random_state=42)\n\nif cd_feature:\n    # train\n    X_cd = kmeans.fit_transform(df_kmeans_train[useful_features])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=df_kmeans_train.index)\n    df_kmeans_train = df_kmeans_train.join(X_cd)\n    # test\n    X_cd = kmeans.transform(df_kmeans_test[useful_features])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=df_kmeans_test.index)\n    df_kmeans_test = df_kmeans_test.join(X_cd)\n    \nelse:\n    print (\"don't bother\")\n#     # train\n#     train[\"cluster\"] = kmeans.fit_predict(train[useful_features])\n#     # test\n#     test[\"cluster\"] = kmeans.predict(test[useful_features])\n    \n#     # one-hot encode\n#     ohe = OneHotEncoder()\n#     X_ohe = ohe.fit_transform(np.array(train[\"cluster\"]).reshape(-1,1)).toarray()\n#     T_ohe = ohe.transform(np.array(test[\"cluster\"]).reshape(-1,1)).toarray()\n\n#     X_ohe = pd.DataFrame(X_ohe, columns=cluster_cols, index=train.index)\n#     T_ohe = pd.DataFrame(T_ohe, columns=cluster_cols, index=test.index)\n\n#     train = pd.concat([train, X_ohe],axis=1)\n#     test = pd.concat([test, T_ohe],axis=1)\n\nfeature_cols += cluster_cols\ndf_kmeans_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature_cols = [col for col in df_kmeans_test.columns.tolist()]\n# feature_cols += cluster_cols\ndf_kmeans_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfig = plt.figure(figsize = (20,5))\n\nif cd_feature:\n    sns.kdeplot(data=df_kmeans_train[cluster_cols])\nelse:\n    ax = sns.countplot(data=df_kmeans_train, x='cluster', hue=\"target\")\n    for p in ax.patches:\n        ax.annotate(f'\\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=5)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# okay - that IS a cool graph. I can see some patterns, but still don't know what it is showing me. \n# To be more precise : I'm not clear on exactly how it is supossed to help me move forward or what alterations would need to be made to improve things. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_kmeans_train['target'] = df_train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_selection import mutual_info_regression\n### how dangerous is it to sample on only 5000 records\nx = df_kmeans_train.iloc[:5000,:][feature_cols].copy()\ny = df_kmeans_train.iloc[:5000,:]['target'].copy()\nmi_scores = mutual_info_regression(x, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.figure_factory as ff\nimport plotly.express as px\ntop = 20\nfig = px.bar(mi_scores, x=mi_scores.values[:top], y=mi_scores.index[:top])\nfig.update_layout(\n    title=f\"Top {top} Strong Relationships Between Feature Columns and Target Column\",\n    xaxis_title=\"Relationship with Target\",\n    yaxis_title=\"Feature Columns\",\n    yaxis={'categoryorder':'total ascending'},\n    colorway=[\"blue\"]\n)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.feature_selection import mutual_info_regression\n\n### how dangerous is it to sample on only 5000 records?\n### Try with a value of 50K failed with ram issues\n\nx = df_kmeans_train.iloc[:30000,:][feature_cols].copy()\ny = df_kmeans_train.iloc[:30000,:]['target'].copy()\nmi_scores = mutual_info_regression(x, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.figure_factory as ff\nimport plotly.express as px\ntop = 20\nfig = px.bar(mi_scores, x=mi_scores.values[:top], y=mi_scores.index[:top])\nfig.update_layout(\n    title=f\"Top {top} Strong Relationships Between Feature Columns and Target Column\",\n    xaxis_title=\"Relationship with Target\",\n    yaxis_title=\"Feature Columns\",\n    yaxis={'categoryorder':'total ascending'},\n    colorway=[\"blue\"]\n)\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# So as I increase the sample size (from 5k to 10k to 15k to 40k - 50k gives ram issues) the importance of clusters drops and instead we have my base models becoming more important. \n# At least that gives me some hope that my base models have some correlation with the traget and I haven't been wasting time over the last few weeks.\n# However I'm thinking that maybe my own CV scores or even the LB scores are a much simpler way of working that all out.\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Anyway my DF's are now populated with some new features that I don't understand very well. At least not the ones that match \"cluster+i\"\n# So I should use this opportunity to do some modelling against them and see what the output looks like\n\nprint (df_kmeans_train.columns)\nprint (df_kmeans_test.columns)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_kmeans_train.drop(columns=[ \"target\"]).copy()\ny = df_kmeans_train[\"target\"].copy()\n\ntest_data = df_kmeans_test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {\n   'eval_metric': 'auc',     'n_estimators': 1000,  'learning_rate' : 0.01       }\n\n# I have no idea what params would be good for this dataset. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\nname = 'defXGB'\n\noof_pred_tmp[name] = list()\noof_pred_tmp[\"y_valid\"] = list()\ntest_pred_tmp[name] = list()\nscores_tmp[name] = list()\n\n\n### I need a basic model\n## nomally logisticRegression is a good choice, but the addition of kmeans might change that. \nmodel = XGBClassifier(**xgb_params)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    print (\"Fold : \", fold, \" Start : \", datetime.now())\n    start_time = datetime.now()\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid,y_valid)],\n        early_stopping_rounds=80,\n        verbose=100\n    )\n\n    pred_valid = model.predict_proba(X_valid)[:, -1]\n    score = roc_auc_score(y_valid, pred_valid)\n\n    scores_tmp[name].append(score)\n    oof_pred_tmp[name].extend(pred_valid)\n\n    end_time = datetime.now()\n    duration = end_time - start_time\n\n#     f_pred.loc[idx_valid, 'pred'] += model.predict_proba(X_valid)[:, 1]/len(SEEDS)\n#     f_test.loc[f_test.index, 'pred'] += model.predict_proba(test_data)[:, 1]/len(SEEDS)/N_FOLDS  ## 5 being number of folds\n\n    print(f\"Fold: {fold} Model: {name} Score: {score}\", \"Duration :\", duration)\n\n    ### Time to get the predictions for competition test data (df_test descendandts)\n    y_hat = model.predict_proba(test_data)[:,1]\n    test_pred_tmp[name].append(y_hat)\n\n    oof_pred_tmp[\"y_valid\"].extend(y_valid)\n\n\nprint(f\"Average Validation Score | {name}: {np.mean(scores_tmp[name])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to unravel those dictionaries of models and predictions \ntest_predictions = pd.DataFrame(  \n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) for name in test_pred_tmp.keys()}\n)\n\n# Always handy to save a copy for use later\ntest_predictions.to_csv(\"./test_predictions.csv\", index=False)\n\n# get the average of all predictions\ntest_predictions[\"avg\"] = test_predictions.mean(axis=1)\n\n# overwrite target in sample_submission with our new and improved predictions \navg_submission = sample_submission.copy()\navg_submission[\"target\"] = test_predictions[\"avg\"]\navg_submission.to_csv(\"./avg_submission1.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and before we even think about submitting - we should check what out oof cv looks like\n\n\noof_predictions = pd.DataFrame({name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()})\n\n# Again Always handy so save a copy for revisiting\noof_predictions.to_csv(\"./oof_preds.csv\", index=False)\n\n# And now we get the all important validation score\ny_valid = oof_predictions[\"y_valid\"].copy()\ny_hat_blend = oof_predictions.drop(columns=[\"y_valid\"]).mean(axis=1)\nscore = roc_auc_score(y_valid, y_hat_blend)\n\nprint(f\"Overall Training Validation Score | Blend: {score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### At this stage we don't have a BAD CV score. Is it worth running the last few sections again after dropping the cluster features. We can compare the results and see if Kmeans clustering had any influence.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster_features =  ['cluster1',  'cluster2',  'cluster3',  'cluster4', 'cluster5',  'cluster6']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for clusterf in cluster_features:\n    df_kmeans_train = df_kmeans_train.drop(clusterf, 1)\n    df_kmeans_test = df_kmeans_test.drop(clusterf, 1)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_kmeans_train.drop(columns=[ \"target\"]).copy()\ny = df_kmeans_train[\"target\"].copy()\ntest_data = df_kmeans_test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\nname = 'defXGB'\n\noof_pred_tmp[name] = list()\noof_pred_tmp[\"y_valid\"] = list()\ntest_pred_tmp[name] = list()\nscores_tmp[name] = list()\n\n\n### I need a basic model\nmodel = XGBClassifier(**xgb_params)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    print (\"Fold : \", fold, \" Start : \", datetime.now())\n    start_time = datetime.now()\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model.fit(\n        X_train, y_train,\n        eval_set=[(X_valid,y_valid)],\n        early_stopping_rounds=80,\n        verbose=100\n    )\n\n    pred_valid = model.predict_proba(X_valid)[:, -1]\n    score = roc_auc_score(y_valid, pred_valid)\n\n    scores_tmp[name].append(score)\n    oof_pred_tmp[name].extend(pred_valid)\n\n    end_time = datetime.now()\n    duration = end_time - start_time\n\n#     f_pred.loc[idx_valid, 'pred'] += model.predict_proba(X_valid)[:, 1]/len(SEEDS)\n#     f_test.loc[f_test.index, 'pred'] += model.predict_proba(test_data)[:, 1]/len(SEEDS)/N_FOLDS  ## 5 being number of folds\n\n    print(f\"Fold: {fold} Model: {name} Score: {score}\", \"Duration :\", duration)\n\n    ### Time to get the predictions for competition test data (df_test descendandts)\n    y_hat = model.predict_proba(test_data)[:,1]\n    test_pred_tmp[name].append(y_hat)\n\n    oof_pred_tmp[\"y_valid\"].extend(y_valid)\n\n\nprint(f\"Average Validation Score | {name}: {np.mean(scores_tmp[name])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = pd.DataFrame(      {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) for name in test_pred_tmp.keys()})\n\ntest_predictions[\"avg\"] = test_predictions.mean(axis=1)\n\navg_submission = sample_submission.copy()\navg_submission[\"target\"] = test_predictions[\"avg\"]\navg_submission.to_csv(\"./avg_submission2.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and before we even think about submitting - we should check what out oof cv looks like\n\n\noof_predictions = pd.DataFrame({name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()})\n\n# Again Always handy so save a copy for revisiting\noof_predictions.to_csv(\"./oof_preds.csv\", index=False)\n\n# And now we get the all important validation score\ny_valid = oof_predictions[\"y_valid\"].copy()\ny_hat_blend = oof_predictions.drop(columns=[\"y_valid\"]).mean(axis=1)\nscore = roc_auc_score(y_valid, y_hat_blend)\n\nprint(f\"Overall Training Validation Score | Blend: {score}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n## Initail run suggests a difference \n## Maybe tuning the final model params instead would make a difference\n## Or running a differnt model.  XGBoost was not that good an idea?\n## Maybe tuning the kmeans model might yield improvments as clearly I didn't understand what it was doing and choosing 6 clusters was maybe not a good idea.\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# A last quick test of the logisticRegression code from original sandpit series\n## https://www.kaggle.com/joecooper/tps-oct-joes-sandpit\n\nfrom sklearn.linear_model import LogisticRegression\n\n# get the set of predictions for oof data (training descendants)\nX_meta = oof_predictions.drop(columns=[\"y_valid\"]).copy()\ny_meta = oof_predictions[\"y_valid\"].copy()\ntest_meta = test_predictions.drop(columns=[\"avg\"]).copy()\n\n\n### Notice how we can simplify this when only working with one model\n### we no longer need to create (and then later unravel) dictionaries\nmeta_pred_tmp = []\nscores_tmp = []\n\n## This modelling has so few features that it costs nothing to increase folds\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_meta, y_meta)):\n    X_train, y_train = X_meta.iloc[idx_train], y_meta.iloc[idx_train]\n    X_valid, y_valid = X_meta.iloc[idx_valid], y_meta.iloc[idx_valid]\n\n    ### Obviously params could be added to tune the model, training is quick but \n    ## benefits might not be significant. The effort shoudl be expended before here \n    ## such that you have better data to work with\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # validation prediction\n    pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, pred_valid)\n    scores_tmp.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    \n    y_hat = model.predict_proba(test_meta)[:,1]\n    meta_pred_tmp.append(y_hat)\n    \n\nprint(f\"Meta Validation Score | Meta: {np.mean(scores_tmp)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For perspective we started with\n# Joes overall computed score for  v10_catb0  is  0.8566436011399643   0.85597\n# Joes overall computed score for  v8_xgb1    is  0.8568621253215738   0.85621\n# Joes overall computed score for  v11_lgbm0  is  0.8569640619014858   0.85638\n# Joes overall computed score for  v7_xgb0    is  0.8570048237818386   0.85645\n# Joes overall computed score for  v6_lgbm1   is  0.8571127311293687   0.85633\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the average prediction for each fold of meta data\nmeta_preds = np.mean(np.column_stack(meta_pred_tmp), axis=1)\n\n# create submission file\nmeta_submission = sample_submission.copy()\nmeta_submission[\"target\"] = meta_preds\nmeta_submission.to_csv(\"./meta_submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}