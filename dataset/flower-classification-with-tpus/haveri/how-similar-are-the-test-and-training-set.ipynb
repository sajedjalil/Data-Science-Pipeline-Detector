{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description\n\n### The Reason for these experiments\nI had one submission where an ensemble of 6 models had resulted in a submitted score of 0.96087 while out of the 3712 images used for validation 116 had failed validation. These 6 models used the default training images and images from the Oxford image set.<p>\nI had another submission where an ensemble of just 2 models had resulted in a submitted score of 0.97208 while out of the 3712 images used for validation 180 had failed validation. These 2 models used the entire default training images and images from the 5 datasets (ImageNet, Oxford 102, TF Flowers, Open Images, and iNaturalist) available as a shared dataset.\nPoor validation score but a higher test score led to further investigation using these experiments.\n\n### The Conclusion\nLooking at these experiments I am sure different individuals may have different conclusions. For me, I would still rely on the validation score but maybe now be careful in comparing the validation score with all the parameters that changed. And training-data biased towards validation-data and/or test-data being just one of them. And of course always get more data in real life projects for better performance along with a model.\n\n### The Experiments and the Observations\nChecking a test set of 7382 images with a combined training set of 68094 would have required 500 million image comparisons. This was too high and had my own doubts of utility of this exercise. Only recently I had also come across some one using Structural Similarity Index used to compare images. Finally decided to compare based on the class id of the image. For test images I just used one of the submission files to get its class id. This comparison would still be valid only a little less accurate. For the structural similarity index I used 0.9 as the cutoff meaning if the index was more than 0.9 then the images would be flagged as similar.<p>\n\n* Default Training images - 16465<br>\n* Default Validation images - 3712<br>\n* Default Testing images - 7382<br>\n* Combined Training images using 5 additional sources of images - 68094<p>\n\nBelow when we say two images were similar, it would mean that the structural similarity index of the two images was above 0.9.<br>\n* A total of 3065 test images out of 7382 test images were similar to at least one image in the 68094 images. Data in version 2,3,4,5.<br>\n* A total of 20 test images out of 7382 test images were similar to atleast one image in the default 16465 training images. Data in version 6.<br>\n* A total of 4 validation images out of 3712 validation images were similar to atleast one image in the default 16465 training images. Data in version 7.<br>\n* A total of 116 validation images out of 3712 validation images were similar to at least one image in the 68094 images. Data in version 8,10.<p>\n\nFrom a percentage perspective the number of validation images similar to training images jumped from 0.11% to 3.13% while the number of testing images similar to training images jumped from 0.27% to 41.52%.<p>\n\nAn image pair being an image from the testing image set and a similar image in the training set.<br>\n* 12 images pairs had SSIM value of 1.0<br>\n* 121 images pairs had SSIM value above 0.95<p>\n\n##### It will be fun if we can have more than a 100 folks getting above 0.9825.\nAs a newbie I definitely learnt a lot from this competition. Maybe I should just\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math, re, gc\nimport numpy as np # linear algebra\nimport pickle\nfrom datetime import datetime, timedelta\nfrom multiprocessing import Pool\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nprint('TensorFlow version', tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\n\nimport scipy.io\nimport random\nimport os\nfrom datetime import datetime, timedelta\nimport tensorflow as tf\n\nimport skimage\nfrom skimage.io import imread as SKImageRead\nfrom skimage.io import imsave as SKImageSave\nfrom skimage.util import crop as SKImageCrop\nfrom skimage.transform import resize as SKImageResize\nfrom skimage.metrics import structural_similarity as ssim\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up the path"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint('Replicas:', strategy.num_replicas_in_sync)\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus')\nMORE_IMAGES_GCS_DS_PATH = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')\nprint(GCS_DS_PATH, '\\n', MORE_IMAGES_GCS_DS_PATH)\n!ls -ltr $GCS_DS_PATH $MORE_IMAGES_GCS_DS_PATH\nGCS_DS_PATH = '/kaggle/input/flower-classification-with-tpus'\nMORE_IMAGES_GCS_DS_PATH = '/kaggle/input/tf-flower-photo-tfrec'\nprint(GCS_DS_PATH, '\\n', MORE_IMAGES_GCS_DS_PATH)\n!cp /kaggle/input/efficientnet-with-all-5-imagesets-s1/submission.csv ./\n#!ls -ltr ./\n#!ls -l /kaggle/input/tf-flower-photo-tfrec/*/tfrecords-jpeg-224x224/*.tfrec\n#!ls -l /kaggle/input/tf-flower-photo-tfrec/imagenet/tfrecords-jpeg-224x224/*.tfrec\n#!ls -l /kaggle/input/tf-flower-photo-tfrec/inaturalist/tfrecords-jpeg-224x224/*.tfrec\n#!ls -l /kaggle/input/tf-flower-photo-tfrec/openimage/tfrecords-jpeg-224x224/*.tfrec\n#!ls -l /kaggle/input/tf-flower-photo-tfrec/oxford_102/tfrecords-jpeg-224x224/*.tfrec\n#!ls -l /kaggle/input/tf-flower-photo-tfrec/tf_flowers/tfrecords-jpeg-224x224/*.tfrec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = datetime.now()\nprint('Time now is', start_time)\n\nIMAGE_SIZE = [224, 224] # [512, 512]\n\nEPOCHS = 12\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\nGCS_PATH_SELECT = {\n    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n}\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec')\n\nMOREIMAGES_PATH_SELECT = {\n    192: '/tfrecords-jpeg-192x192',\n    224: '/tfrecords-jpeg-224x224',\n    331: '/tfrecords-jpeg-331x331',\n    512: '/tfrecords-jpeg-512x512'\n}\nMOREIMAGES_PATH = MOREIMAGES_PATH_SELECT[IMAGE_SIZE[0]]\n\nIMAGENET_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '/imagenet' + MOREIMAGES_PATH + '/*.tfrec')\nINATURELIST_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '/inaturalist' + MOREIMAGES_PATH + '/*.tfrec')\nOPENIMAGE_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '/openimage' + MOREIMAGES_PATH + '/*.tfrec')\nOXFORD_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '/oxford_102' + MOREIMAGES_PATH + '/*.tfrec')\nTENSORFLOW_FILES = tf.io.gfile.glob(MORE_IMAGES_GCS_DS_PATH + '/tf_flowers' + MOREIMAGES_PATH + '/*.tfrec')\nADDITIONAL_TRAINING_FILENAMES = IMAGENET_FILES + INATURELIST_FILES + OPENIMAGE_FILES + OXFORD_FILES + TENSORFLOW_FILES\n#print(TEST_FILENAMES)\nprint('----')\nTRAINING_FILENAMES = TRAINING_FILENAMES + ADDITIONAL_TRAINING_FILENAMES\n#print(TRAINING_FILENAMES)\n\n# This is so awkward. Everyone is doing this for an extra few points.\n# TRAINING_FILENAMES = TRAINING_FILENAMES + VALIDATION_FILENAMES\n# VALIDATION_FILENAMES = TRAINING_FILENAMES\n\nCLASSES = ['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'wild geranium', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', # 00 - 09\n           'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', # 10 - 19\n           'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', # 20 - 29\n           'carnation', 'garden phlox', 'love in the mist', 'cosmos', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', # 30 - 39\n           'barberton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'daisy', 'common dandelion', # 40 - 49\n           'petunia', 'wild pansy', 'primula', 'sunflower', 'lilac hibiscus', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', # 50 - 59\n           'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'iris', 'windflower', 'tree poppy', # 60 - 69\n           'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', # 70 - 79\n           'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen ', 'watercress', 'canna lily', # 80 - 89\n           'hippeastrum ', 'bee balm', 'pink quill', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', # 90 - 99\n           'trumpet creeper', 'blackberry lily', 'common tulip', 'wild rose'] # 100 - 102","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n#\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint('Dataset: {} training images, {} unlabeled test images. Total possible ssim comparisions {}'.format(NUM_TRAINING_IMAGES, NUM_TEST_IMAGES, (NUM_TRAINING_IMAGES * NUM_TEST_IMAGES)))\nprint('Dataset: {} training images, {} labeled validation images. Total possible ssim comparisions {}'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, (NUM_TRAINING_IMAGES * NUM_VALIDATION_IMAGES)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(TRAINING_FILENAMES), len(VALIDATION_FILENAMES), len(TEST_FILENAMES))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading the submitted file\n\nThe file 'submission.csv' is an output from one the kernel used. The image predictions are stored using a map with the testids as the key and the prediction value as the value."},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = './submission.csv'\n\nmy_submission = np.loadtxt(filename, dtype=str, skiprows=1, unpack=True)\nimage_predictions = {}\nfor aline in my_submission:\n    splitstring = aline.split(',')\n    image_predictions[splitstring[0]] = int(splitstring[1])\nprint(type(my_submission), my_submission.shape)\nprint(type(image_predictions))\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process the images finding similarity\n\nThe method find_similar_flowers_in_test_train() runs in a seperate process for different test image files. Only way I could to speed up the experiments.<p>\nWhen checking the similarity of testing images with training images we pass the class ids of the testing images.<p>\nWhen checking the similarity of the validation images with the training images we read the class ids of the images from the tfrec file itself."},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_similar_flowers_in_test_train(id, image_predictions, flowers_test_files, flowers_training_files):\n#    no_of_test_images = 0\n    failure_threshold = 0.9\n    total_comparisons = 0\n    image_by_cls_id = {} # For each image we store a list of tuples (idnum, training_images_exists_in_test, image)\n    for i in range(104):\n        image_by_cls_id[i] = []\n#\n    def read_labeled_tfrecord(example):\n        LABELED_TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'class': tf.io.FixedLenFeature([], tf.int64),\n        }\n        example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n        image = tf.image.decode_jpeg(example['image'])\n        label = tf.cast(example['class'], tf.int32)\n        return image, label\n#\n    def read_unlabeled_tfrecord(example):\n        UNLABELED_TFREC_FORMAT = {\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'id': tf.io.FixedLenFeature([], tf.string),\n        }\n        example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n        image = tf.image.decode_jpeg(example['image'])\n        idnum = example['id']\n        return image, idnum\n#\n    if image_predictions == None:\n        for filename in flowers_test_files:\n            raw_image_dataset = tf.data.TFRecordDataset(filename)\n            image_dataset = raw_image_dataset.map(read_labeled_tfrecord)\n            idnum_prefix = re.compile(r\"\\/([0-9].*)\\.tfrec\").search(filename).group(1) + '_img_'\n#            print('filename is {}, idnum_prefix is {}'.format(filename, idnum_prefix))\n            no_of_test_images = 0\n            for image_features in image_dataset:\n                idnum = idnum_prefix + str(no_of_test_images)\n                no_of_test_images = no_of_test_images + 1\n                image, cls_id = image_features\n                image = image.numpy()\n                cls_id = cls_id.numpy()\n                curr_image_data = (idnum, 0, image)\n                image_by_cls_id[cls_id].append(curr_image_data)\n    else:\n        for filename in flowers_test_files:\n            raw_image_dataset = tf.data.TFRecordDataset(filename)\n            image_dataset = raw_image_dataset.map(read_unlabeled_tfrecord)\n            for image_features in image_dataset:\n#                no_of_test_images = no_of_test_images + 1\n                image, idnum = image_features\n                image = image.numpy()\n                idnum = idnum.numpy().decode('UTF-8')\n                cls_id = image_predictions[idnum]\n                curr_image_data = (idnum, 0, image)\n                image_by_cls_id[cls_id].append(curr_image_data)\n#\n#    print(no_of_test_images)\n#\n    def check_ssim_with_test_images(filename, img_id_within_tfrec, trn_label, image):\n        comparisons = 0 # TODO\n        messages = []\n        compare_test_images = image_by_cls_id[trn_label]\n        no_of_compares = len(compare_test_images)\n        for j in range(no_of_compares):\n#            if j >= 20:\n#                break\n            curr_image_data = image_by_cls_id[trn_label][j]\n            idnum, training_images_exists_in_test, test_image = curr_image_data\n            if training_images_exists_in_test:\n                continue\n            comparisons = comparisons + 1\n            ssim_val = ssim(test_image, image, multichannel=True)\n            if ssim_val > failure_threshold:\n                curr_image_data = (idnum, 1, test_image)\n                image_by_cls_id[trn_label][j] = curr_image_data\n                this_message = 'Image {} similar with image in training {} img number {}. similarity index is {}'.format(idnum, filename, img_id_within_tfrec, ssim_val)\n                messages.append(this_message)\n        return comparisons, messages\n#\n    print('Start {}: {}'.format(id, datetime.now()))\n    all_messages = []\n    for filename in flowers_training_files:\n        raw_image_dataset = tf.data.TFRecordDataset(filename)\n        image_dataset = raw_image_dataset.map(read_labeled_tfrecord)\n        img_id_within_tfrec = 0\n        for image_features in image_dataset:\n#            if img_id_within_tfrec >= 20:\n#                break\n            image, label = image_features\n            image = image.numpy()\n            label = label.numpy()\n            comparisons, messages = check_ssim_with_test_images(filename, img_id_within_tfrec, label, image)\n            total_comparisons = total_comparisons + comparisons\n            all_messages.extend(messages)\n            img_id_within_tfrec = img_id_within_tfrec + 1\n    #\n    print('End {}: Total Comparisons {}: Images matched {}: {}'.format(id, total_comparisons, len(all_messages), datetime.now()))\n    filename = str(id) + '_in_case_i_crash_timeout_similar_images.csv'\n    np.savetxt(filename, np.rec.fromarrays([all_messages]), fmt = ['%s'], delimiter = ',', header = 'messages', comments = '')\n    return all_messages\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set up the kernel to prevent failure."},{"metadata":{"trusted":true},"cell_type":"code","source":"#flowers_test_files = TEST_FILENAMES\n#flowers_training_files = TRAINING_FILENAMES\n#find_similar_flowers_in_test_train(0, image_predictions, flowers_test_files, flowers_training_files)\nall_messages = [] # setting up to prevent failing of this kernel to be published.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the images\n\nThe following cells no longer run the actual image comparisons and code that starts the multiple processes have been commented. These were executed in versions 2,3,4,5,6,7,8,10 of this kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"mul_proc_params = []\nno_test_tfrec_files = len(TEST_FILENAMES)\nshift_1 = len(TRAINING_FILENAMES) // no_test_tfrec_files\nfor i in range(no_test_tfrec_files):\n    test_file = [TEST_FILENAMES[i]]\n    shift = i * shift_1\n    train_files = TRAINING_FILENAMES[shift:] + TRAINING_FILENAMES[:shift]\n    mul_proc_params.append((i, image_predictions, test_file, train_files))\n#\n# mul_proc_params = mul_proc_params[12:]\n#\ndef is_flower_similar():\n    with Pool(6) as p:\n        all_messages = p.starmap(find_similar_flowers_in_test_train, mul_proc_params)\n        print(len(all_messages))\n    return all_messages\n#\n#if __name__ == '__main__':\n#    all_messages = is_flower_similar()\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mul_proc_params = []\nno_val_tfrec_files = len(VALIDATION_FILENAMES)\nshift_1 = len(TRAINING_FILENAMES) // no_val_tfrec_files\nfor i in range(no_val_tfrec_files):\n    val_file = [VALIDATION_FILENAMES[i]]\n    shift = i * shift_1\n    train_files = TRAINING_FILENAMES[shift:] + TRAINING_FILENAMES[:shift]\n    mul_proc_params.append((i, None, val_file, train_files))\n#\nprint(len(mul_proc_params))\nmul_proc_params = mul_proc_params[8:]\n#\ndef is_flower_similar():\n    with Pool(5) as p:\n        all_messages = p.starmap(find_similar_flowers_in_test_train, mul_proc_params)\n        print(len(all_messages))\n    return all_messages\n#\n#if __name__ == '__main__':\n#    all_messages = is_flower_similar()\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_list_messages = []\nfor i in range(len(all_messages)):\n    single_list_messages.extend(all_messages[i])\n#\nprint('Number of images similar with training images', len(single_list_messages))\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'similar_images.csv'\nnp.savetxt(filename, np.rec.fromarrays([single_list_messages]), fmt = ['%s'], delimiter = ',', header = 'messages', comments = '')\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize(image1, image1_title, image2, image2_title):\n    image1_title = 'test id - ' + image1_title\n    image2_title = 'ssi - ' + image2_title\n    fig = plt.figure()\n    plt.subplot(1,2,1)\n    plt.title(image1_title)\n    plt.imshow(image1)\n\n    plt.subplot(1,2,2)\n    plt.title(image2_title)\n    plt.imshow(image2)\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'class': tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = tf.image.decode_jpeg(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label\n#\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'id': tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = tf.image.decode_jpeg(example['image'])\n    idnum = example['id']\n    return image, idnum\n#\ndef get_training_images(filename):\n    training_image = []\n    raw_image_dataset = tf.data.TFRecordDataset(filename)\n    image_dataset = raw_image_dataset.map(read_labeled_tfrecord)\n    for image_features in image_dataset:\n        image, label = image_features\n        image = image.numpy()\n        training_image.append(image)\n    return training_image\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = {}\nno_of_test_images = 0\n#\nfor filename in TEST_FILENAMES:\n    raw_image_dataset = tf.data.TFRecordDataset(filename)\n    image_dataset = raw_image_dataset.map(read_unlabeled_tfrecord)\n    for image_features in image_dataset:\n        no_of_test_images = no_of_test_images + 1\n#        if no_of_test_images > 2:\n#            break\n        image, idnum = image_features\n        image = image.numpy()\n        idnum = idnum.numpy().decode('UTF-8')\n        test_images[idnum] = image\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\ntestids = test_images.keys()\nprint('Test', len(TEST_FILENAMES), no_of_test_images, len(testids))\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Visual Evidence"},{"metadata":{},"cell_type":"markdown","source":"Images with a perfect match"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image 61f976a63 similar to /tfrecords-jpeg-224x224/train/06-224x224-798.tfrec img number 353. similarity index is 1.0\n#Image 9eac41e56 similar to /tfrecords-jpeg-224x224/train/06-224x224-798.tfrec img number 546. similarity index is 1.0\n#Image 805888e57 similar to /tfrecords-jpeg-224x224/train/06-224x224-798.tfrec img number 627. similarity index is 1.0\n#\ntest_ids_to_show = ['61f976a63', '9eac41e56', '805888e57']\nids_similar_to = [353, 546, 627]\nssi_vals = [1.0, 1.0, 1.0]\nfilename = GCS_DS_PATH + '/tfrecords-jpeg-224x224/train/06-224x224-798.tfrec'\ntraining_images = get_training_images(filename)\n\nvisualize(test_images[test_ids_to_show[0]], test_ids_to_show[0], training_images[ids_similar_to[0]], str(ssi_vals[0]))\nvisualize(test_images[test_ids_to_show[1]], test_ids_to_show[1], training_images[ids_similar_to[1]], str(ssi_vals[1]))\nvisualize(test_images[test_ids_to_show[2]], test_ids_to_show[2], training_images[ids_similar_to[2]], str(ssi_vals[2]))\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images matched but less than 1.0"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image 3bef19347 similar to /tf_flowers/tfrecords-jpeg-224x224/7-224x224-99.tfrec img number 79. similarity index is 0.9631336470049914\n#Image c9e27d0e3 similar to /tf_flowers/tfrecords-jpeg-224x224/7-224x224-99.tfrec img number 88. similarity index is 0.9579509019834677\n#Image 49292d94c similar to /tf_flowers/tfrecords-jpeg-224x224/7-224x224-99.tfrec img number 91. similarity index is 0.963339089650537\n#\ntest_ids_to_show = ['3bef19347', 'c9e27d0e3', '49292d94c']\nids_similar_to = [79, 88, 91]\nssi_vals = [0.9631, 0.9579, 0.9633]\nfilename = MORE_IMAGES_GCS_DS_PATH + '/tf_flowers/tfrecords-jpeg-224x224/7-224x224-99.tfrec'\ntraining_images = get_training_images(filename)\n\nvisualize(test_images[test_ids_to_show[0]], test_ids_to_show[0], training_images[ids_similar_to[0]], str(ssi_vals[0]))\nvisualize(test_images[test_ids_to_show[1]], test_ids_to_show[1], training_images[ids_similar_to[1]], str(ssi_vals[1]))\nvisualize(test_images[test_ids_to_show[2]], test_ids_to_show[2], training_images[ids_similar_to[2]], str(ssi_vals[2]))\n#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image a0925df64 similar to /imagenet/tfrecords-jpeg-224x224/0-224x224-1852.tfrec img number 142. similarity index is 0.9171391656692433\n#Image 360538bb1 similar to /imagenet/tfrecords-jpeg-224x224/0-224x224-1852.tfrec img number 809. similarity index is 0.9009493950335353\n#Image 8482cd4e5 similar to /imagenet/tfrecords-jpeg-224x224/0-224x224-1852.tfrec img number 1268. similarity index is 0.9017363495863923\n#\ntest_ids_to_show = ['a0925df64', '360538bb1', '8482cd4e5']\nids_similar_to = [142, 809, 1268]\nssi_vals = [0.9171, 0.9009, 0.9017]\nfilename = MORE_IMAGES_GCS_DS_PATH + '/imagenet/tfrecords-jpeg-224x224/0-224x224-1852.tfrec'\ntraining_images = get_training_images(filename)\n\nvisualize(test_images[test_ids_to_show[0]], test_ids_to_show[0], training_images[ids_similar_to[0]], str(ssi_vals[0]))\nvisualize(test_images[test_ids_to_show[1]], test_ids_to_show[1], training_images[ids_similar_to[1]], str(ssi_vals[1]))\nvisualize(test_images[test_ids_to_show[2]], test_ids_to_show[2], training_images[ids_similar_to[2]], str(ssi_vals[2]))\n#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## References\n\n* Dataset by Kirill Blinov. [tf_flower_photo_tfrec](https://www.kaggle.com/kirillblinov/tf-flower-photo-tfrec)\n* Idea for using Structural Similarity Index was from a [comment in Kaggle](https://www.kaggle.com/c/flower-classification-with-tpus/discussion/140866#797825)\n* Hope I haven't missed any others"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}