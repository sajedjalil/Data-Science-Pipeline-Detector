{"nbformat_minor":1,"nbformat":4,"cells":[{"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"419948f1de60686ace56c21f07734edfab62d973","_cell_guid":"4f7ca8e0-6af7-40a5-ae36-c5fdccd6ec5b"}},{"outputs":[],"source":"def getRawFeatures(picture):\n    red = []\n    green = []\n    blue = []\n    for row in range(picture.shape[0]):\n        for col in range(picture.shape[1]):\n            red.append(picture[row][col][0])\n            green.append(picture[row][col][1])\n            blue.append(picture[row][col][2])\n    feature = red\n    feature.extend(green)\n    feature.extend(blue)\n    return feature","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7fc1b38e38da8ebee2d499b4cc627a723a07addf","_cell_guid":"cd799485-93e0-4d76-9624-0b628504bf88"}},{"outputs":[],"source":"import io\nimport bson # this is installed with the pymongo package\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom skimage.data import imread   # or, whatever image library you prefer\nimport multiprocessing as mp      # will come in handy due to the size of the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"89d8ce2e6149ea1a8bf7db009e7db5f8b10adcec","_cell_guid":"f88cfc36-a17c-4eda-b5f2-fe567201441a"}},{"outputs":[],"source":"# Simple data processing\ncount_images = 0\nimage_names_array = []\ncategory_id_array = []\ndata = bson.decode_file_iter(open('../input/train_example.bson', 'rb'))\npictures = []\ncount = 0\nprod_to_category = dict()\n\nfor c, d in enumerate(data):\n    #for each product_id\n    product_id = d['_id']\n    category_id = d['category_id'] # This won't be in Test data\n    prod_to_category[product_id] = category_id\n    \n    for e, pic in enumerate(d['imgs']):\n        #for each image\n        picture = imread(io.BytesIO(pic['picture']))\n        pictures.append(picture)\n        count = count + 1\n        # do something with the picture, etc\n#         image_name = \"prod_id-\" + str(product_id) + \"-\" + \"image-\" + str(e)\n#         print(\"PRODUCT ID:\", product_id, \"NUMBER\", e)\n#         plt.imshow(picture)\n#         fig1 = plt.gcf()\n#         plt.show()\n#         plt.draw()\n        count_images = count_images + 1\n#         image_names_array.append(image_name)\n        category_id_array.append(str(category_id))\n        #fig1.savefig(\"img/\" + str(image_name), dpi=100)\n    print(\"done\")\n#     break\n\nprod_to_category = pd.DataFrame.from_dict(prod_to_category, orient='index')\nprod_to_category.index.name = '_id'\nprod_to_category.rename(columns={0: 'category_id'}, inplace=True)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"bdfffde30a8d59257d649a0379e3a33a44744d8b","_cell_guid":"7a754b19-e70c-4f08-98cc-66b549a4a280"}},{"outputs":[],"source":"X_train = np.asarray(pictures)\ny_train = np.asarray(category_id_array)\ny_train.shape","execution_count":null,"cell_type":"code","metadata":{"_uuid":"dd9c5d9b3078672abc57612f7302cd40c207b2d4","_cell_guid":"5c9ff9fb-0aa2-4646-b270-cfb503e59088"}},{"outputs":[],"source":"X_train.shape","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"212c7bd7cc970c523f145c07f4b7f54371af9897","_cell_guid":"edf1ee26-b25a-4ad0-8b47-8644ed2dc1ae"}},{"outputs":[],"source":"X_train = X_train.reshape(X_train.shape[0], 3, 180, 180).astype('float32')\nX_train = X_train - np.mean(X_train) / X_train.std()","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"a598bc27fa63b16b02cf53df434c8c4f295e0f4d","_cell_guid":"4e70c847-e53e-464a-970e-204502ecf584"}},{"outputs":[],"source":"# listFeatureVector = []\n\n# for picture in pictures:\n#     featureVector = getRawFeatures(picture)\n#     listFeatureVector.append(featureVector)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"501608b6ad3a38cb5c0bffeb673f82c99d8fa227","_cell_guid":"eef8decd-6eb1-486f-ac06-74a4d3724777"}},{"outputs":[],"source":"# print(len(listFeatureVector[0]))\n\n# X = listFeatureVector\n# y = category_id_array","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"7b6b9552045767aaac42d1cd2ab08e84121349ee","_cell_guid":"b53e1cd6-fa71-4cb2-8b56-b5c2cc6c075a"}},{"outputs":[],"source":"# len(list(set(y)))","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"aa61f62f16119f9aad826f4eeab5c0186929fa93","_cell_guid":"88dd3296-b648-48ee-b88d-36225659e2c3"}},{"outputs":[],"source":"y_train","execution_count":null,"cell_type":"code","metadata":{"_uuid":"d39d93483789299c7b0682b6fc6ad9304a791160","_cell_guid":"c264809c-7c93-4710-adc6-20fd5e451218"}},{"outputs":[],"source":"b,c = np.unique(y_train, return_inverse=True)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"6074f841a1e47c2799c0b86555d0aaea8ca186eb","_cell_guid":"8bbe7c31-295f-437f-9736-b041afbf9179"}},{"outputs":[],"source":"from collections import Counter\nd = Counter(c)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"218cfbf405c95753628ee70780d513c87dabb1a7","_cell_guid":"03ff71db-9254-47c8-80a3-73ed1b00e34e"}},{"outputs":[],"source":"y_train = c","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c849c91309c0900788639c1af03b75e7a72a7a89","_cell_guid":"37ed0c0f-e378-4e98-9906-331b5f1be3a3"}},{"outputs":[],"source":"y_train","execution_count":null,"cell_type":"code","metadata":{"scrolled":true,"_uuid":"1fdfd5277811f1db662c3c7384026d683eba2a41","_cell_guid":"1dd12042-468d-4ce2-80ef-53f6eaddee5e"}},{"outputs":[],"source":"from keras.utils import np_utils\nfrom tflearn.data_utils import to_categorical\ny_train = np_utils.to_categorical(y_train)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"56077036cbc3da6fcd24e653370547de30054cf3","_cell_guid":"4fb8235c-c68d-48c0-8903-a09920738418"}},{"outputs":[],"source":"y_train","execution_count":null,"cell_type":"code","metadata":{"_uuid":"9e45d340b3690317fcc7a19123fda9af3d16d206","_cell_guid":"e6b4be89-e7dd-4287-932f-1333404b34ac"}},{"outputs":[],"source":"# X_train = X_train.reshape(X_train.shape[0], 180, 180, 3)\n# X_train = X_train.astype('float32')\n# X_train = X_train/255","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"62edb20bcd5fba16188b40dd5147632641b511a1","_cell_guid":"6011c80b-8df6-4522-bb65-5fb774ad6fa3"}},{"outputs":[],"source":"from tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.estimator import regression\nmodel = input_data(shape=[None,3,180,180])\nmodel = conv_2d(model,32,5,activation='elu')\nmodel = max_pool_2d(model,2)\nmodel = conv_2d(model,64,3,activation='relu')\nmodel = max_pool_2d(model,2)\nmodel = dropout(model,0.3)\nmodel = conv_2d(model,64,3, activation='elu')\nmodel = max_pool_2d(model,2)\nmodel = fully_connected(model,512,activation='sigmoid')\nmodel = dropout(model,0.3)\nmodel = fully_connected(model,36,activation='softmax')\nmodel = regression(model,optimizer='adagrad',loss='categorical_crossentropy',learning_rate=0.05)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"38d44e5ae3ba1742fbf0267e5509caad9373522c","_cell_guid":"29f10004-3395-4b54-9d13-dc00e4aab544"}},{"outputs":[],"source":"import tensorflow as tf\nimport tflearn\nwith tf.device('cpu:0'):\n   model = tflearn.DNN(model)\n   model.fit(X_train , y_train)","execution_count":null,"cell_type":"code","metadata":{"_uuid":"890699033ca957a5ad5f2a79c714bf34f95eeb79","_cell_guid":"be9ccb57-01a5-4ba2-b68d-6634264db821"}},{"outputs":[],"source":"pred = model.predict(X_train)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"3402208c3c102151463dc1783251d244870f406d","_cell_guid":"275ceb64-3c80-4f20-868d-a260758550be"}},{"outputs":[],"source":"model.evaluate(X_train, y_train)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"85c8aac0d7f8db3ebf266094b0aa232c02cfdbf3","_cell_guid":"97071bf2-6b7d-42b6-9fa9-54eb74481554"}},{"outputs":[],"source":"# from keras.preprocessing import image\n# img = image.load_img('',target_size=(180,180))\n# img = image.img_to_array(img)\n# img = np.expand_dims(img, axis=0)\nimg = X_train[0].reshape(1,3,180,180)\n# img.shape\npreds = model.predict(img)\npreds","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"daa7ba8dafc15699c2d26dc7a4d3a43f8fa418ef","_cell_guid":"3ca68b97-d3e5-4b1e-a2ab-0c917e79515d"}},{"outputs":[],"source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), input_shape=(180,180,3)))\nBatchNormalization(axis=-1)\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nBatchNormalization(axis=-1)\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(64,(3, 3)))\nBatchNormalization(axis=-1)\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nBatchNormalization(axis=-1)\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\n\n# Fully connected layer\nmodel.add(Dense(512))\n\nBatchNormalization()\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(110))\n\nmodel.add(Activation('softmax'))","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"c9861455245ff2a1fcf1c87d66f19ea3a6a0847b","_cell_guid":"b6a608a3-fd30-4d7f-a0d7-31f7fd667835"}},{"outputs":[],"source":"model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"cab0255b6ce12bdaaac303d6910f0dde086cb463","_cell_guid":"1a4b4cec-cfd0-44d6-b5f1-75f895d6c4d7"}},{"outputs":[],"source":"from keras.preprocessing.image import ImageDataGenerator\ngen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,\n                         height_shift_range=0.08, zoom_range=0.08)\n","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"1e9e468e4b2498cbb46e355a41ff71f05a254ce2","_cell_guid":"f951046f-ca3f-4e26-b8b9-329a705d8271"}},{"outputs":[],"source":"train_generator = gen.flow(X_train, y_train)\n","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"bd4b30c4a178e66ba3f4f8610545bd46d540eb09","_cell_guid":"c5dda7c6-f4c5-405f-a654-bfefa97cf5d7"}},{"outputs":[],"source":"model.fit_generator(train_generator, steps_per_epoch=110//64, epochs=2)","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"e54196eeef6a6ae9bcafa14c648bf011dff8e471","_cell_guid":"28d5580d-ec37-4d1a-ad46-bccd2170afff"}},{"outputs":[],"source":"","execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"_uuid":"07f0cf2599944d3052be02e407db3a5d18373dd9","_cell_guid":"fe765f16-a5b7-478d-8855-a03241fedc39"}}],"metadata":{"language_info":{"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","nbconvert_exporter":"python","version":"3.6.3"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}}}