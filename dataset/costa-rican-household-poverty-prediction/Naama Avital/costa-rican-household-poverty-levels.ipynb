{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Costa Rican Household Poverty Level Prediction"},{"metadata":{},"cell_type":"markdown","source":"### Overview"},{"metadata":{},"cell_type":"markdown","source":"The data for this project is taken from a Kaggle competition for poverty levels prediction of Costa Rican housholds, hosted by the The Inter-American Development Bank: <br>\nhttps://www.kaggle.com/c/costa-rican-household-poverty-prediction\n<br>\n<br>\nThe training data conatains 9558 rows, each row represting a person. There are 143 columns, including the person ID, its household identifer and the target columns (our labels). Some of the features refer to the person and some are aggregated per houshold. \n<br>\nFor example: \nAge, is male/female if owns a tablet and number of years of education refer to person. <br>\nNumber people under 12, rent, the matirial the house is made of and the region -  refer to a household. <br>\n<br>\nThe prediction should be on the houshold level, i.e. all the persons under the same household should have the same poverty level.\n<br>\nThe poverty level are:<br>\n1 = extreme poverty <br>\n2 = moderate poverty <br>\n3 = vulnerable households <br>\n4 = non vulnerable households <br>\n"},{"metadata":{},"cell_type":"markdown","source":"### Exploration and Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import confusion_matrix, SCORERS, classification_report, accuracy_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport warnings\nimport time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As mentioned, the data should be aggregated to houshold level, since the prediction should be per household.<br>\nSo we explored each column to understand the data range and understand how to aggregate."},{"metadata":{"trusted":false},"cell_type":"code","source":"val_exsplore=(data.agg(['min','max','dtype',lambda df: df.nunique(),pd.unique])\n                  .transpose()\n                  .rename(index=str, columns={'<lambda>':'nunique'})\n             )","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 100)\nval_exsplore.sort_values(by=['nunique'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Check for NaN values\nnan_sum=pd.DataFrame(data.isnull().sum())\nnan_sum[nan_sum[0]>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After consideration we decided to replace nan values with 0. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace nan values with 0\ndata_clean = data.copy()\ndata_clean.fillna(value=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We note some inconsistencies with houshole lables. <br>\nSome households have more than one lable. In those cases the lable of the head of the household label for the houshold"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"house_labels=pd.DataFrame(data_clean\n                                    .groupby(['idhogar'])\n                                    .Target\n                                    .nunique()\n                         )\nhouse_labels[house_labels['Target']>1].size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will find the labels of the heads of houshold:"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"data_clean[data_clean['parentesco1']==1][['idhogar','Target']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#size of targets of heads of households\ndata_clean[data_clean['parentesco1']==1][['idhogar','Target']].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#number of unique housholds\ndata_clean['idhogar'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_clean['idhogar'].nunique()-data_clean[data_clean['parentesco1']==1][['idhogar','Target']].shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So there are 15 households with no head of household in the data. We will finde their lables and concatenate to what we have"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_household_ishead=pd.pivot_table(data_clean, index='idhogar',aggfunc = sum, values = ['parentesco1'])\ndata_household_nohead=data_household_ishead[data_household_ishead['parentesco1']==0].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"missing_labels=data_clean[data_clean['idhogar'].isin(data_household_nohead)][['idhogar','Target']].drop_duplicates()\nall_house_labels=pd.concat([data_clean[data_clean['parentesco1']==1][['idhogar','Target']],missing_labels], axis=0)\nall_house_labels.rename(index=str, columns={'Target': 'Target_new'}, inplace = True)\nall_house_labels.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"house_labels_new=pd.DataFrame(all_house_labels\n                                    .groupby(['idhogar'])\n                                    .Target_new\n                                    .nunique()\n                         )\nhouse_labels_new[house_labels_new['Target_new']>1].size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join the new target labels into the data_clean table"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_clean = pd.merge(data_clean,all_house_labels, on='idhogar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_clean.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How the target lables are split in the data:"},{"metadata":{"trusted":false},"cell_type":"code","source":"labels = (pd\n            .DataFrame(data_clean\n                            .Target_new\n                            .value_counts()\n                      )\n            .sort_index()\n         )\nlabels['Target_all_%']=((labels['Target_new']/(labels['Target_new'] .sum()))\n                                                            .round(3)\n                       )\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we need to lable in the household label, we look at the target labels groupd by houshold"},{"metadata":{"trusted":false},"cell_type":"code","source":"labels['house_target']=all_house_labels['Target_new'].value_counts().sort_index()\nlabels['house_target_%']=(labels['house_target']/labels['house_target'].sum()).round(3)\n#  .plot.bar())\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"names = pd.Series(['extreme', 'moderate', 'vulnerable', 'non vulnerable'])\nlabels.set_index(names,inplace=True)\nlabels['house_target'].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"(pd\n     .concat([labels,pd.DataFrame(labels.sum()).transpose()], axis=0)\n     .rename(index={0: 'Total'})\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the label ratio is similar on the person and on the household level."},{"metadata":{},"cell_type":"markdown","source":"We note issues in calculating dependency. The dependency in the data was not according to the promised calculations, so we calculated our own dependecy columns, based on the age data of the members of each housholds."},{"metadata":{"trusted":false},"cell_type":"code","source":"def is_adult(s):\n    if (s<=64) & (s>=19):\n        return 1\n    return 0\n    \ndef is_minor(s):\n    if s<19:\n        return 1\n    return 0\n    \ndef is_senior(s):\n    if s>64:\n        return 1\n    return 0\n    \ndata['is_adult']=data['age'].apply(is_adult)\ndata['is_minor']=data['age'].apply(is_minor)\ndata['is_senior']=data['age'].apply(is_senior)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agg_ages=pd.pivot_table(data, index='idhogar', values = ['is_minor','is_adult','is_senior'], aggfunc = sum) \nagg_ages['all_ages']=agg_ages[['is_minor','is_adult','is_senior']].sum(axis=1)\nagg_ages.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agg_ages['dependency_our']=agg_ages[['is_minor','is_senior']].sum(axis=1)/agg_ages['is_adult']\n# agg_ages.head(20)\n\nagg_ages['dependency_our'].replace(np.inf, 10, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"agg_ages.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop columns that don't contibute to the household data"},{"metadata":{"trusted":false},"cell_type":"code","source":"(data_clean.drop(\n                    axis=1, \n                    columns=['Id','hogar_nin','hogar_adul','hogar_mayor',\n                        'hogar_total','dependency','qmobilephone','age','agesq','Target'],\n                    inplace =True\n                )\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# replace string values with yes with 1 and no with 0\ndata_clean.edjefe=data_clean.edjefe.replace(['yes'], 1, inplace=True)\ndata_clean.edjefe=data_clean.edjefe.replace(['no'], 0, inplace=True)\ndata_clean.edjefa=data_clean.edjefa.replace(['yes'], 1, inplace=True)\ndata_clean.edjefa=data_clean.edjefa.replace(['no'], 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aggregate the data by houshold"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_household_max=pd.pivot_table(data_clean, index='idhogar',aggfunc = max, values = ['v2a1','hacdor','rooms','hacapo',\n                    'v14a',\n                    'refrig',\n                    'v18q',\n                    'v18q1',\n                    'r4h1',\n                    'r4h2',\n                    'r4h3',\n                    'r4m1',\n                    'r4m2',\n                    'r4m3',\n                    'r4t1',\n                    'r4t2',\n                    'r4t3',\n                    'tamhog',\n                    'tamviv',\n                    'hhsize',\n                    'paredblolad',\n                    'paredzocalo',\n                    'paredpreb',\n                    'pareddes',\n                    'paredmad',\n                    'paredzinc',\n                    'paredfibras',\n                    'paredother',\n                    'pisomoscer',\n                    'pisocemento',\n                    'pisoother',\n                    'pisonatur',\n                    'pisonotiene',\n                    'pisomadera',\n                    'techozinc',\n                    'techoentrepiso',\n                    'techocane',\n                    'techootro',\n                    'cielorazo',\n                    'abastaguadentro',\n                    'abastaguafuera',\n                    'abastaguano',\n                    'public',\n                    'planpri',\n                    'noelec',\n                    'coopele',\n                    'sanitario1',\n                    'sanitario2',\n                    'sanitario3',\n                    'sanitario5',\n                    'sanitario6',\n                    'energcocinar1',\n                    'energcocinar2',\n                    'energcocinar3',\n                    'energcocinar4',\n                    'elimbasu1',\n                    'elimbasu2',\n                    'elimbasu3',\n                    'elimbasu4',\n                    'elimbasu5',\n                    'elimbasu6',\n                    'epared1',\n                    'epared2',\n                    'epared3',\n                    'etecho1',\n                    'etecho2',\n                    'etecho3',\n                    'eviv1',\n                    'eviv2',\n                    'eviv3',\n                    'dis',\n                    'male',\n                    'female',\n                    'estadocivil1',\n                    'estadocivil2',\n                    'estadocivil3',\n                    'estadocivil4',\n                    'estadocivil5',\n                    'estadocivil6',\n                    'estadocivil7',\n                    'parentesco1',\n                    'parentesco2',\n                    'parentesco3',\n                    'parentesco4',\n                    'parentesco5',\n                    'parentesco6',\n                    'parentesco7',\n                    'parentesco8',\n                    'parentesco9',\n                    'parentesco10',\n                    'parentesco11',\n                    'parentesco12',\n                    'edjefe',\n                    'edjefa',\n                    'meaneduc',\n                    'bedrooms',\n                    'overcrowding',\n                    'tipovivi1',\n                    'tipovivi2',\n                    'tipovivi3',\n                    'tipovivi4',\n                    'tipovivi5',\n                    'computer',\n                    'television',\n                    'mobilephone',\n                    'lugar1',\n                    'lugar2',\n                    'lugar3',\n                    'lugar4',\n                    'lugar5',\n                    'lugar6',\n                    'area1',\n                    'area2',\n                    'SQBescolari',\n                    'SQBage',\n                    'SQBhogar_total',\n                    'SQBedjefe',\n                    'SQBhogar_nin',\n                    'SQBovercrowding',\n                    'SQBdependency',\n                    'SQBmeaned','Target_new']\n                                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_household_sum=pd.pivot_table(data_clean, index='idhogar',aggfunc = sum, values = ['escolari',\n                                                                                        'rez_esc',\n                                                                                        'instlevel1',\n                                                                                        'instlevel2',\n                                                                                        'instlevel3',\n                                                                                        'instlevel4',\n                                                                                        'instlevel5',\n                                                                                        'instlevel6',\n                                                                                        'instlevel7',\n                                                                                        'instlevel8',\n                                                                                        'instlevel9']\n                                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold=(data_household_max\n                                .join(data_household_sum)\n                                .join(agg_ages)\n              )\ndata_houshold.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold = pd.concat([data_houshold,(pd.DataFrame(agg_ages['dependency_our']))], axis=1, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_names = ['v18q','mobilephone','refrig','computer','television']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for feature in feature_names:\n    data_houshold.groupby(['Target_new',feature]).size().unstack().plot(kind='bar', stacked=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"region = data_houshold[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6','Target_new']].copy()\nregion['lugar2'] = region['lugar2'].replace(1,2)\nregion['lugar3'] = region['lugar3'].replace(1,3)\nregion['lugar4'] = region['lugar4'].replace(1,4)\nregion['lugar5'] = region['lugar5'].replace(1,5)\nregion['lugar6'] = region['lugar6'].replace(1,6)\nregion['Region']= (region[['lugar1','lugar2','lugar3','lugar4','lugar5','lugar6']].max(axis =1)\n      .replace([1,2,3,4,5,6],['Central','Chorotega','PacÃƒÂ­fico central','Brunca','Huetar AtlÃƒÂ¡ntica','Huetar Norte']))\nregion['Target_new']=region['Target_new'].replace([1,2,3,4],['extreme', 'moderate', 'vulnerable', 'non vulnerable'])\nregion.groupby(['Region','Target_new']).size().groupby(level=0).apply(lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting the models"},{"metadata":{},"cell_type":"markdown","source":"Now our data is clean and aggregated by household.<br>\nWe will create 3 datasets for predicting different labels.<br>\n\n1. **data_houshold_2 - 2 Labels of balanced data:** <br>\n    1 = extreme poverty ==> 1<br>\n    2 = moderate poverty ==> 1 <br>\n    3 = vulnerable households ==> 1 <br>\n    4 = non vulnerable households ==> 0 <br>\n    <br>\n2. **data_houshold_2_i - 2 Labels of imbalanced data:** <br>\n    1 = extreme poverty ==> 1<br>\n    2 = moderate poverty ==> 1 <br>\n    3 = vulnerable households ==> 0 <br>\n    4 = non vulnerable households ==> 0<br>\n    <br>\n3. **data_houshold - 4 Labels of imbalanced data:** <br>\n    1 = extreme poverty <br>\n    2 = moderate poverty <br>\n    3 = vulnerable households <br>\n    4 = non vulnerable households <br>"},{"metadata":{},"cell_type":"markdown","source":"Most features consist of binary data but some features have int/float type data that is much larger than 1, so data will be scaled with min-max scaler.<br> \n\nFor each dataset we will run 2 methods of dimensionality reduction:\n* Feature selection with lasso logistic regression\n* Feature extraction with PCA\n  \nand 2 classifiers: \n* Logistic regression\n* Random forest\n \nWe will run a grid search cross validation (CV=10, since the data size is fairly small, using accuracy score for multiclass and roc-auc for the 2 class datasets), with diffrent parameters to find the best classifires for each dataset.<br>\n<br>\n<br>\nWe will also look at logistic regression and random forest calssifiers, with all the features"},{"metadata":{},"cell_type":"markdown","source":"**The hyper-parameter grids for each configuration:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid_log_reg_L1 = {'Feature_selection__estimator__C': [0.1, 1, 10],\n                          'clf__C' : [0.1, 1, 10], \n                          'Feature_selection__threshold': [0.05, 0.1, 0.2]\n                         }\n\nparam_grid_log_reg_PCA = {'Feature_extraction__n_components': [30, 60, 90, 120],\n                          'clf__C' : [0.1, 1, 10]\n                         }\n\nparam_grid_log_reg = {'clf__C' : [0.01, 0.1, 1, 10, 100]}\n\nparam_grid_RF_L1 = {'Feature_selection__estimator__C': [0.1, 1, 10],\n                    'Feature_selection__threshold': [0.05, 0.1, 0.2], \n                    'clf__min_samples_split': [2,8,15,20]\n                   }\n\nparam_grid_RF_PCA = {'Feature_extraction__n_components': [30, 60, 90, 120],\n                     'clf__min_samples_split': [2,8,15,20]\n                    }\n\nparam_grid_RF = {'clf__min_samples_split': [2,8,15,20]}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define scaler, feature extraction and selection models and classifiers:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"max_iter_param=100\n\n#Scaler\nminmax_scaler = MinMaxScaler()\n\n#Feature selection - 2 labels\nLasso_log_reg2 = LogisticRegression(penalty='l1',\n                                    class_weight='balanced',\n                                    solver='liblinear'\n                                   )\n                                    \nFeature_selection2=SelectFromModel(Lasso_log_reg2)                \n\n#Feature selection - 4 labels\nLasso_log_reg4 =LogisticRegression(penalty='l1' ,\n                                   max_iter = max_iter_param,\n                                   multi_class='multinomial',\n                                   class_weight='balanced', \n                                   solver='saga'\n                                  )\n\nFeature_selection4=SelectFromModel(Lasso_log_reg4)     \n\n#Feature Extraction\nPCA_features=PCA()\n\n#Logistic regression classifier - 2 labels\nlog_reg_clf2=LogisticRegression(solver='lbfgs',\n                                class_weight='balanced'\n                               )\n\n#Logistic regression classifier - 4 labels\nlog_reg_clf4=LogisticRegression(solver='saga',\n                                max_iter = max_iter_param,\n                                multi_class='multinomial',\n                                class_weight='balanced'\n                               )\n\n#Random forest classifier\nRF_clf = RandomForestClassifier(class_weight='balanced' ,\n                                n_estimators=100,\n                                random_state=123\n                               )\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Define the piplines:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Pipelines - 2 labels\n\nLog_reg_pipe2_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                                   ('Feature_selection', Feature_selection2),\n                                   ('clf',log_reg_clf2)\n                                  ]\n                            )\n\nLog_reg_pipe2_PCA = Pipeline(steps=[('Scaler', minmax_scaler),\n                                     ('Feature_extraction', PCA_features),\n                                     ('clf',log_reg_clf2)\n                                   ]\n                            ) \n\nLog_reg_pipe2= Pipeline (steps=[('Scaler', minmax_scaler),\n                                ('clf',log_reg_clf2)\n                               ]\n                        )\n\nRF_pipe_2_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                               ('Feature_selection', Feature_selection2),\n                               ('clf',RF_clf)\n                                  ]\n                            )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Pipelines - 4 labels\nLog_reg_pipe4_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                                   ('Feature_selection', Feature_selection4),\n                                   ('clf',log_reg_clf4)\n                                  ]\n                            )\n\nLog_reg_pipe4_PCA = Pipeline(steps=[('Scaler', minmax_scaler),\n                                    ('Feature_extraction', PCA_features),\n                                    ('clf',log_reg_clf4)\n                                   ]\n                            ) \n\nLog_reg_pipe4= Pipeline (steps=[('Scaler', minmax_scaler),\n                                ('clf',log_reg_clf4)\n                               ]\n                        )\n\nRF_pipe_4_L1 = Pipeline(steps=[('Scaler', minmax_scaler),\n                               ('Feature_selection', Feature_selection4),\n                               ('clf',RF_clf)\n                                  ]\n                            )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#pipelines for 2 or 4 labels\nRF_pipe_PCA = Pipeline(steps=[('Scaler', minmax_scaler),\n                              ('Feature_extraction', PCA_features),\n                              ('clf',RF_clf)\n                             ]\n                      )\n                                   \n\nRF_pipe = Pipeline(steps=[('Scaler', minmax_scaler),\n                          ('clf',RF_clf)\n                         ]\n                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Helpful functions:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Runs the GridSearchCV fit and finds the best classifer:\ndef fit_model (pipe, grid, X_train, y_train, num_cv, scoring_param):\n    best_clf=GridSearchCV(pipe, grid, cv=num_cv,scoring = scoring_param)\n    print ('Begin GridSearchCV fit')\n    t0 = time.time()\n    best_clf.fit(X_train, y_train)\n    t1 = time.time()\n    h, m ,s=time_convert(t1-t0)\n    print('GridSearchCV ended. Elapsed time: {0:.0f} hours, {1:.0f} minutes and {2:.0f} seconds'.format(h,m,s))\n    return best_clf\n\n#Returns DF with relevant columns (parameter, mean_test_score and rank_test_score)\n#from cv_results_ of the after GridSearchCV fit, sorted by test score rank\ndef cv_results (clf) :\n    df_cv_results = pd.DataFrame(clf.cv_results_)\n    param_list=[i for i in list(df_cv_results.columns) if 'param_' in i]\n    df_cv_results_filter=df_cv_results[param_list+['mean_test_score','rank_test_score']].sort_values(by=['rank_test_score'])\n    return df_cv_results_filter\n\n\n#Returns DF with best classifer params and train/test scores of for each pipeline:\ndef results_test_df(rdf,name,clf,X_test,y_test,scoring_param):\n    if scoring_param == 'roc_auc':\n        rdf=rdf.append ({ 'Model':name,\n                          'Best_params':clf.best_params_,\n                          'Best_Train_Score':clf.best_score_.round(5),\n                          'Best_Test_Score':roc_auc_score(y_true=y_test, y_score=clf.predict(X_test)).round(5)}\n                          ,ignore_index = True\n                        )\n    elif scoring_param == 'accuracy':\n        rdf=rdf.append ({ 'Model':name,\n                          'Best_params': clf.best_params_,\n                          'Best_Train_Score':clf.best_score_.round(5),\n                          'Best_Test_Score':accuracy_score(y_true=y_test, y_pred=clf.predict(X_test)).round(5)}\n                          ,ignore_index = True\n                       )\n    return rdf\n\ndef time_convert (t):\n    h,m1=divmod(t, 3600)\n    m,s=divmod(m1, 60) \n    return h, m ,s\n\ndef find_best(model_dict, X_train,y_train,X_test,y_test,num_cv,scoring_param,df_init,df_all_best_results):\n    for name, (pipe,grid) in model_dict.items():\n        print ('Model name:',name)\n        best_clf = fit_model (pipe, grid, X_train,y_train, num_cv,scoring_param)\n        best_clf_cv_results=cv_results(best_clf)\n        display(best_clf_cv_results)\n        df_all_best_results = df_all_best_results.append(results_test_df(df_init,name,best_clf,X_test,y_test, scoring_param))\n        print('======================================================')\n    return df_all_best_results\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define some helpful parameters:"},{"metadata":{"trusted":false},"cell_type":"code","source":"#feature dictionary - 2 labels\nmodel_dict2={'Feature selection and logistic regression':(Log_reg_pipe2_L1, param_grid_log_reg_L1),\n            'PCA and logistic regression':               (Log_reg_pipe2_PCA, param_grid_log_reg_PCA),\n            'All features and logistic regression':      (Log_reg_pipe2, param_grid_log_reg),\n            'Feature selection and random forest':       (RF_pipe_2_L1,param_grid_RF_L1),\n            'PCA and random forest':                     (RF_pipe_PCA,param_grid_RF_PCA),\n            'All features and random forest':            (RF_pipe , param_grid_RF)\n            }\n\n#feature dictionary - 4 labels\nmodel_dict4={'Feature selection and logistic regression':(Log_reg_pipe4_L1, param_grid_log_reg_L1),\n            'PCA and logistic regression':               (Log_reg_pipe4_PCA, param_grid_log_reg_PCA),\n            'All features and logistic regression':      (Log_reg_pipe4, param_grid_log_reg),\n            'Feature selection and random forest':       (RF_pipe_4_L1,param_grid_RF_L1),\n            'PCA and random forest':                     (RF_pipe_PCA,param_grid_RF_PCA),\n            'All features and random forest':            (RF_pipe , param_grid_RF)\n            }\n\n#init dataframe for best scores of each pipline\ndf_init=pd.DataFrame(columns=['Model','Best_params','Best_Train_Score','Best_Test_Score'])\n\n#number of cross valisation folds\nnum_cv = 10\n\n#scoring parameter - 2 labels\nscoring_param2 = 'roc_auc'\n\n#scoring parameter - 4 labels\nscoring_param4 = 'accuracy'\n\n#Columns display definition\npd.set_option('display.max_colwidth', 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=red>2 Labels - balanced data</font> "},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold_2=data_houshold.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Replace the label values to create 2 labels\ndata_houshold_2['Target_new']=data_houshold_2['Target_new'].replace([2,3,4], [1,1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold_2['Target_new'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels of this data are more balanced and they represent non_vulnerable (0) and vulnerable/poor (1) households. <br>\nWe'll split the data to train and test:"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train2, X_test2, y_train2, y_test2 = split(data_houshold_2.drop(axis=1, columns=['Target_new']), \n                                             data_houshold_2['Target_new'], \n                                             test_size =0.3, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = X_train2, X_test2, y_train2, y_test2\ndf_all_best_results=pd.DataFrame()\n\n\ndf_all_best_results2=find_best(model_dict2, X_train,y_train,X_test,y_test,num_cv,scoring_param2,df_init,df_all_best_results)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=green>2 Labels - imbalanced data</font> "},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold_2_i=data_houshold.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold_2_i['Target_new']=data_houshold_2_i['Target_new'].replace([2,3,4], [1,0,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold_2_i['Target_new'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels of this data are not balanced, but the split makes more sense, since the lables now represent poor (1) and not-poor (0) households <br>\nWe'll split the data to train and test"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train2_i, X_test2_i, y_train2_i, y_test2_i = split(data_houshold_2_i.drop(axis=1, columns=['Target_new']), \n                                                     data_houshold_2_i['Target_new'], \n                                                     test_size =0.3, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"\ndf_all_best_results=pd.DataFrame()\nX_train, X_test, y_train, y_test = X_train2_i, X_test2_i, y_train2_i, y_test2_i\n\ndf_all_best_results2_i=find_best(model_dict2, X_train,y_train,X_test,y_test,num_cv,scoring_param2,df_init,df_all_best_results)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color=blue>4 labels of imbalanced data</font>"},{"metadata":{"trusted":false},"cell_type":"code","source":"data_houshold['Target_new'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data to train and test"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train4, X_test4, y_train4, y_test4 = split(data_houshold.drop(axis=1, columns=['Target_new']), \n                                             data_houshold['Target_new'], \n                                             test_size =0.3, random_state=123\n                                            )","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df_all_best_results=pd.DataFrame()\nX_train, X_test, y_train, y_test = X_train4, X_test4, y_train4, y_test4\n\ndf_all_best_results4=find_best(model_dict4, X_train,y_train,X_test,y_test,num_cv,scoring_param4,df_init,df_all_best_results)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"For Each dataset we have a all the best estimators from each type of pipeline. we will compare them to each other on the test data."},{"metadata":{},"cell_type":"markdown","source":"## <font color=red>2 Labels - balanced data</font> "},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_best_results2.sort_values(['Best_Test_Score','Best_Train_Score'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic regression looks like the better method (with C=1). Seems like keeping all the features is be as good as PCA."},{"metadata":{},"cell_type":"markdown","source":"## <font color=green>2 Labels - imbalanced data</font> "},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_best_results2_i.sort_values(['Best_Test_Score','Best_Train_Score'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, using all features or extracting with PCA seem like equal methods, combined with logistic regression as classifier, with C=1, and they preform the best on the test data. "},{"metadata":{},"cell_type":"markdown","source":"## <font color=blue>4 labels of imbalanced data</font>"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_all_best_results4.sort_values(['Best_Test_Score','Best_Train_Score'],ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"for multiclass, random forest classifier performs better than logistic regresstion, with feature selection (Threshold = 0.2) having the best test score."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}