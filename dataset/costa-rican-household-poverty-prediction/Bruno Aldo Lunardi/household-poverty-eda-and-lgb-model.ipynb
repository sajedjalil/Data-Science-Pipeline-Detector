{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Python â‰¥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn â‰¥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npalette = sns.color_palette('Paired', 10)\n\nimport numpy as np\nimport pandas as pd\n# Pandas display options\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\n#setting fontsize and style for all the plots\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['figure.figsize'] = (16,5)\n\n%matplotlib inline \n#plotting directly without requering the plot()\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\") #ignoring most of warnings, cleaning up the notebook for better visualization\n\npd.set_option('display.max_columns', 500) #fixing the number of rows and columns to be displayed\npd.set_option('display.max_rows', 500)\n\nprint(os.listdir(\"../input\")) #showing all the files in the ../input directory\n\n# Set random seed \nrandomseed = 42\n\n# Any results you write to the current directory are saved as output. Kaggle message :D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading all dataframes"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  printing the shapes of our sales and test dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**peeking at the training 5 entries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Merging the train and test dataset in order to have more data to train our model.\n\ntrain['source']='train' #creating a label for the training and testing set\ntest['source']='test'\n\ndata = pd.concat([train, test],ignore_index=True)\nprint (train.shape, test.shape, data.shape) #printing the shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's create a function to check for null values, calculate the percentage relative to the total size\n#only shows null values.\ndef missing_values_calculate(trainset): \n    nulldata = (trainset.isnull().sum() / len(trainset)) * 100\n    nulldata = nulldata.drop(nulldata[nulldata == 0].index).sort_values(ascending=False)\n    ratio_missing_data = pd.DataFrame({'Ratio' : nulldata})\n    return ratio_missing_data.head(30)\n\nmissing_values_calculate(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Null columns** \n* target - nan values comes from the test dataset,so, ignoring it.\n* SQBmeaned - square of the mean years of education of adults (>=18) in the household\n* meaneduc - average years of education for adults (18+)\n* rez_esc - Years behind in school\n* v18q1 - number of tablets household owns\n* v2a1 - Monthly rent payment"},{"metadata":{},"cell_type":"markdown","source":"**Dealing manually with null values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v18q1'] = data['v18q1'].fillna(0)\n\n# Fill in households that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n# Create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\n\n# If individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# Add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()\n\ndata.loc[data['rez_esc'] > 5, 'rez_esc'] = 5\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/willkoehrsen/featuretools-for-good\n"},{"metadata":{},"cell_type":"markdown","source":"**Feature engineering domain specific knowledge credits to willkoehrsen**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Difference between people living in house and household size\ndata['hhsize-diff'] = data['tamviv'] - data['hhsize']\n\nelec = []\n\n# Assign values\nfor i, row in data.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\ndata['elec'] = elec\ndata['elec-missing'] = data['elec'].isnull()\n\n# Remove the electricity columns\n# data = data.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])\n\n# Wall ordinal variable\ndata['walls'] = np.argmax(np.array(data[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\n\n# data = data.drop(columns = ['epared1', 'epared2', 'epared3'])\n\n# Roof ordinal variable\ndata['roof'] = np.argmax(np.array(data[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\n# data = data.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\ndata['floor'] = np.argmax(np.array(data[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\n# data = data.drop(columns = ['eviv1', 'eviv2', 'eviv3'])\n\n# Create new feature\ndata['walls+roof+floor'] = data['walls'] + data['roof'] + data['floor']\n\n# No toilet, no electricity, no floor, no water service, no ceiling\ndata['warning'] = 1 * (data['sanitario1'] + \n                         (data['elec'] == 0) + \n                         data['pisonotiene'] + \n                         data['abastaguano'] + \n                         (data['cielorazo'] == 0))\n\n# Owns a refrigerator, computer, tablet, and television\ndata['bonus'] = 1 * (data['refrig'] + \n                      data['computer'] + \n                      (data['v18q1'] > 0) + \n                      data['television'])\n\n# Per capita features\ndata['phones-per-capita'] = data['qmobilephone'] / data['tamviv']\ndata['tablets-per-capita'] = data['v18q1'] / data['tamviv']\ndata['rooms-per-capita'] = data['rooms'] / data['tamviv']\ndata['rent-per-capita'] = data['v2a1'] / data['tamviv']\n\n# Create one feature from the `instlevel` columns\ndata['inst'] = np.argmax(np.array(data[[c for c in data if c.startswith('instl')]]), axis = 1)\n# data = data.drop(columns = [c for c in data if c.startswith('instlevel')])\n\ndata['escolari/age'] = data['escolari'] / data['age']\ndata['inst/age'] = data['inst'] / data['age']\ndata['tech'] = data['v18q'] + data['mobilephone']\n\nprint('Data shape: ', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {\"yes\": 1, \"no\": 0}\n\n# Fill in the values with the correct mapping\ndata['dependency'] = data['dependency'].replace(mapping).astype(np.float64)\ndata['edjefa'] = data['edjefa'].replace(mapping).astype(np.float64)\ndata['edjefe'] = data['edjefe'].replace(mapping).astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove Highly Correlated Columns**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = data.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')\nprint(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import featuretools.variable_types as vtypes\n\n#Household variables boolean type\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing', 'elec-missing']\n\n#household ordered type\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin','hhsize-diff',\n              'elec',  'walls', 'roof', 'floor', 'walls+roof+floor', 'warning', 'bonus',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\n#household continuous type\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding',\n          'phones-per-capita', 'tablets-per-capita', 'rooms-per-capita', 'rent-per-capita']\n\n#individual boolean type\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\n#individual ordered type\nind_ordered = ['age', 'escolari', 'rez_esc', 'inst', 'tech']\n\n#individual continuous type\nind_cont = ['escolari/age', 'inst/age']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_remove = []\nfor l in [hh_ordered, hh_bool, hh_cont, ind_bool, ind_ordered, ind_cont]:\n    for c in l:\n        if c not in data:\n            to_remove.append(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in [hh_ordered, hh_bool, hh_cont, ind_bool, ind_ordered, ind_cont]:\n    for c in to_remove:\n        if c in l:\n            l.remove(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for variable in (hh_bool + ind_bool):\n    data[variable] = data[variable].astype('bool')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for variable in (hh_cont + ind_cont):\n    data[variable] = data[variable].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for variable in (hh_ordered + ind_ordered):\n    try:\n        data[variable] = data[variable].astype(int)\n    except Exception as e:\n        print(f'Could not convert {variable} because of missing values.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"},{"metadata":{},"cell_type":"markdown","source":"### Target - the target is an ordinal variable indicating groups of income levels. \n* 1 extreme poverty \n* 2 moderate poverty \n* 3 vulnerable households \n* 4 non vulnerable households"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data[data['source'] == 'train']['Target'])\nplt.title(\"Target distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**parentesco1**\n- indicates if this person is the head of the household."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\nsns.countplot(data['parentesco1'])\nplt.title(\"Head of the household distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,5))\nplt.subplot(2,1,1)\nsns.distplot(data['v2a1'].dropna())\nplt.title(\"Monthly rent payment distribution\")\nplt.subplot(2,1,2)\nsns.distplot(np.log1p(data['v2a1'].dropna()))\nplt.title(\"Monthly rent payment distribution in log + 1\")\nplt.tight_layout(h_pad=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['SQBdependency'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,10))\nfor i,col in enumerate(['agesq','SQBmeaned','SQBdependency','SQBovercrowding']):\n    plt.subplot(4,1,i+1)\n    sns.distplot(np.log1p(data[col].dropna()))\nplt.tight_layout(h_pad=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.dropna().corr() #Let's take a look at the pearson's corr, just to have an overall view of how the attributes influence the price.\ncorr = corr[(corr >=0.2) | (corr <=-0.1)]\ncorr['Target'].dropna().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['Target'].corr(df[var_name])\n        \n    # Plot the distribution\n    sns.kdeplot(df.ix[df['Target'] == 1.0, var_name], label = 'Target == 1')\n    sns.kdeplot(df.ix[df['Target'] == 2.0, var_name], label = 'Target == 2')\n    sns.kdeplot(df.ix[df['Target'] == 3.0, var_name], label = 'Target == 3')\n    sns.kdeplot(df.ix[df['Target'] == 4.0, var_name], label = 'Target == 4')\n\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the Target: %0.4f' % (var_name, corr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting negativaly correlated distributions\nfig = plt.figure(figsize=(16,10))\nfor i,col in enumerate(['eviv2','dependency','etecho2']):\n    plt.subplot(3,1,i+1)\n    kde_target(col,data)\nplt.tight_layout(h_pad=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing Squared Variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[[x for x in data if not x.startswith('SQB')]]\ndata = data.drop(columns = ['agesq'])\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data[data['source'] == 'train']\ntest = data[data['source'] == 'test']\ntrain_labels = train['Target'].copy()\ntrain.drop([\"source\",\"Id\",'idhogar',\"Target\"], axis=1, inplace=True) #this is really important, separate our target Y from our X\ntest.drop([\"source\",\"Id\",'idhogar',\"Target\"], axis=1, inplace=True)\nprint(train.shape, train_labels.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attribs = train.select_dtypes(exclude=['object', 'bool']) #selecting all the numerical data to use in our function DataFrameSelector\ncat_attribs = train.select_dtypes(exclude=['int64','float64']) #selecting non numerical data to use in our function DataFrameSelector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inspired from stackoverflow.com/questions/25239958\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder #gonna try this one later\n\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)\n    \n\n#this pipeline is gonna be use for numerical atributes and standard scaler    \nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        #('std_scaler', StandardScaler()),\n        #('robust_scaler', RobustScaler()),\n        ('minmaxscaler', MinMaxScaler()),\n    ])\n\n#this is gonna be used to imput categorical values\ncat_pipeline = Pipeline([\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OrdinalEncoder()),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, list(num_attribs)),\n        (\"cat\", cat_pipeline, list(cat_attribs)),\n    ])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_final,train_validation, train_y, train_val_y  = train_test_split(train, train_labels,test_size=0.2, shuffle = True, random_state=randomseed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_final.shape, train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, accuracy_score,confusion_matrix,classification_report,f1_score, roc_auc_score\nimport time #implementing in this function the time spent on training the model\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV,cross_val_score,train_test_split, KFold\nfrom catboost import CatBoostClassifier, Pool\nimport lightgbm as lgb\nimport xgboost as xgb\n\n#Generic function for making a classification model and accessing performance:\ndef classification_model(X_train,x_val, y_train,y_val,test_set, model_type='lgb',\n                         params={},plot_confusion_matrix=False,\n                         plot_features_importances=False):\n    \n    time_start = time.perf_counter() #start counting the time\n    \n    test_sub = np.zeros(test_set.shape[0])\n    \n    predict_val = np.zeros(X_train.shape[0])\n    score = {}\n\n     \n    if model_type == 'lgb':\n        \n        model = lgb.LGBMClassifier(**params, n_estimators = 1500,class_weight = 'balanced', nthread = 4, n_jobs = -1)\n        \n        model.fit(X_train, y_train, eval_set=[(X_train, y_train), (x_val, y_val)], eval_metric='multiclass', \n                  early_stopping_rounds=200,\n                    verbose=50)\n            \n        predict_val = model.predict(x_val, num_iteration=model.best_iteration_)\n        \n        #predicting using the model that has been trained above\n        \n        predict_val = model.predict(x_val)\n        score['Accuracy'] = (accuracy_score(y_val, predict_val))\n        score['Precision'] = (precision_score(y_val, predict_val,average='micro'))\n        score['F1 score'] = (f1_score(y_val, predict_val,average='micro'))\n        \n        print(\"Model Report\")\n\n        print(\"Accuracy: \"+ str(score[\"Accuracy\"]))\n        print(\"Precision: \"+ str(score[\"Precision\"]))\n        print(\"F1 score: \"+ str(score[\"F1 score\"]))\n        print('\\n')\n        \n        print(\"         -------Classification Report----------\")\n        print(classification_report(y_val, predict_val))\n    \n        test_sub = model.predict(test_set) \n            \n    if model_type == 'xgb':\n\n        model = xgb.XGBClassifier(**params, nthread = 4, n_jobs = -1)\n\n        model.fit(X_train, y_train, \n                      eval_set=[(X_train, y_train), (x_val, y_val)],\n                          early_stopping_rounds=100,\n                             verbose=50)\n        \n        #predicting using the model that has been trained above\n        \n        predict_val = model.predict(x_val, ntree_limit=model.best_ntree_limit)\n        score['Accuracy'] = (accuracy_score(y_val, predict_val))\n        score['Precision'] = (precision_score(y_val, predict_val,average='micro'))\n        score['F1 score'] = (f1_score(y_val, predict_val,average='micro'))\n        \n        print(\"Model Report\")\n\n        print(\"Accuracy: \"+ str(score[\"Accuracy\"]))\n        print(\"Precision: \"+ str(score[\"Precision\"]))\n        print(\"F1 score: \"+ str(score[\"F1 score\"]))\n        print('\\n')\n        \n        print(\"         -------Classification Report----------\")\n        print(classification_report(y_val, predict_val))\n    \n        test_sub = model.predict(test_set) \n\n        \n        if plot_features_importances:\n            # feature importance\n            fig, ax = plt.subplots(figsize=(12,10))\n            xgb.plot_importance(model, max_num_features=50, height=0.8,color='c', ax=ax)\n            ax.grid(False)\n            plt.title(\"XGBoost - Feature Importance\", fontsize=15)\n            \n    if model_type == 'cat':\n        model = CatBoostClassifier(**params)\n        model.fit(X_train, y_train, eval_set=(x_val, y_val), \n                  cat_features=[], use_best_model=True)\n\n        #predicting using the model that has been trained above\n        \n        predict_val = model.predict(x_val)\n        score['Accuracy'] = (accuracy_score(y_val, predict_val))\n        score['Precision'] = (precision_score(y_val, predict_val,average='micro'))\n        score['F1 score'] = (f1_score(y_val, predict_val,average='micro'))\n        \n        print(\"Model Report\")\n\n        print(\"Accuracy: \"+ str(score[\"Accuracy\"]))\n        print(\"Precision: \"+ str(score[\"Precision\"]))\n        print(\"F1 score: \"+ str(score[\"F1 score\"]))\n        print('\\n')\n        \n        print(\"         -------Classification Report----------\")\n        print(classification_report(y_val, predict_val))\n    \n        test_sub = model.predict(test_set)\n    #################### PLOTTING FEATURES IMPORTANCE #################### \n    if plot_features_importances:\n        plt.figure(figsize=(20, 20)) #figure size\n        feature_importance = model.feature_importances_[:30]\n        # make importances relative to max importance\n        feature_importance = 100.0 * (feature_importance / feature_importance.max()) #making it a percentage relative to the max value\n        sorted_idx = np.argsort(feature_importance)\n        pos = np.arange(sorted_idx.shape[0]) + .5\n        plt.barh(pos, feature_importance[sorted_idx], align='center')\n        plt.yticks(pos, train.columns[sorted_idx], fontsize=15) #used train_drop here to show the name of each feature instead of our train_prepared \n        plt.xlabel('Relative Importance', fontsize=20)\n        plt.ylabel('Features', fontsize=20)\n        plt.title('Variable Importance', fontsize=30)\n\n    #################### PLOTTING CONFUSION MATRIX #######################\n    \n    if plot_confusion_matrix:\n        fig, ax = plt.subplots(figsize=(8,8)) #setting the figure size and ax\n        mtx = confusion_matrix(y_val, predict_val)\n        sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  cbar=True, ax=ax) #create a heatmap with the values of our confusion matrix\n        plt.ylabel('true label')\n        plt.xlabel('predicted label')\n\n    \n    time_end = time.perf_counter() #end of counting the time\n    \n    total_time = time_end-time_start #total time spent during training and cross_validation\n    \n    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))\n    \n    # Clean up memory\n    gc.enable()\n    del model,score,total_time, time_end, time_start,predict_val,test_set\n    gc.collect()\n                        \n    return test_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prepared = full_pipeline.fit_transform(train_final)\ntrain_validation_prepared = full_pipeline.fit_transform(train_validation)\ntest_prepared = full_pipeline.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {\n        \"metric\" : \"multiclass\",\n        \"num_leaves\" : 30,\n        \"min_child_weight\" : 50,\n        \"learning_rate\" : 0.05,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.7,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        'reg_aplha': 1,\n        'reg_lambda': 0.001\n}\n\npreds_lgb = classification_model(X_train=train_prepared,x_val=train_validation_prepared,y_train=train_y,y_val=train_val_y, \n                                 test_set=test_prepared,params=params_lgb, plot_features_importances=True,plot_confusion_matrix=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['Target'] = np.array(preds_lgb).astype(int)\nsub.to_csv('lgb.csv', index = False)\nsns.countplot(sub['Target'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}