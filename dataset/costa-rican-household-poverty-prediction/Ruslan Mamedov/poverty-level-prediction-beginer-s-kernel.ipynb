{"cells":[{"metadata":{"_uuid":"2c207e76bece57f6a99439c61229a1a68d4db600"},"cell_type":"markdown","source":"## 1. **Importing the libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #for plotting\nimport seaborn as sea #for visualization\n\n# Set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 15\nplt.rcParams['patch.edgecolor'] = 'k'\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n# Any results you write to the current directory are saved as output.p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look at all available files:\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2b0b82dcbe0197903a9d4f60668c64118822f22"},"cell_type":"markdown","source":"## **2. Importing/exploring the train/test datasets and converting to numeric form**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nprint (\"Train Dataset: Rows, Columns: \", train_df.shape)\nprint (\"Test Dataset: Rows, Columns: \", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the prediction will be based on the head of the household\nsubmit = test_df[['Id','idhogar']]\n#https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d13717877e6756b66160b0a2616c27eabfc28c"},"cell_type":"code","source":"# a glimpse at train_df\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e69f8013e8f3cc0944731569a705689ab74aeacc"},"cell_type":"code","source":"#First, let's deal with non-numeric columns\ntrain_df.select_dtypes(['object']).head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bbfbe85e22de74f84839b33c85eb72a24c907be"},"cell_type":"code","source":"#Id and idhogar won't be used for training so we'll take care of them later\n#1. 'dependency'\ntrain_df['dependency'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Notice there is a column containing squared values for dependency, 'SQBdependency'. \n#see what are its analogs to 'yes' and 'no' of 'dependency':\nprint (train_df.loc[train_df['dependency']=='no',['SQBdependency']]['SQBdependency'].value_counts())\nprint (train_df.loc[train_df['dependency']=='yes',['SQBdependency']]['SQBdependency'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8968b2773fc2593c33c5623ebfe6959a98499f8"},"cell_type":"code","source":"#Convert 'yes' to 1 and 'no' to 0\ntrain_df['dependency'] = train_df['dependency'].replace(('yes', 'no'), (1, 0))\ntest_df['dependency'] = test_df['dependency'].replace(('yes', 'no'), (1, 0))\ntrain_df['dependency']=train_df['dependency'].astype(float)\ntest_df['dependency']=test_df['dependency'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0d6240817d1799b3f7161f30fb1b5407521fe50"},"cell_type":"code","source":"#2 and #3 'edjefe'/'edjefa'\ntrain_df['edjefe'].value_counts()\n#edjefe, years of education of male head of household, \n#based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e60fbf4c863d92c90802988ad54430dfd86386"},"cell_type":"code","source":"train_df['edjefa'].value_counts()\n#edjefa, years of education of female head of household, \n#based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Again, correlate 'edjefe' with 'SQBedjefe'(squared value)\nprint (train_df.loc[train_df['edjefe']=='no',['SQBedjefe']]['SQBedjefe'].value_counts())\nprint (train_df.loc[train_df['edjefe']=='yes',['SQBedjefe']]['SQBedjefe'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a5e392e1cf9168bd00b47b8bb8e94a42aef1bee"},"cell_type":"code","source":"#Based on 'SQBedjefe' column, convert 'no' to 0 and 'yes' to 1 to make the rows of 'edjefa'/'edjefe' numeric\ntrain_df['edjefa'] = train_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntrain_df['edjefe'] = train_df['edjefe'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefa'] = test_df['edjefa'].replace(('yes', 'no'), (1, 0))\ntest_df['edjefe'] = test_df['edjefe'].replace(('yes', 'no'), (1, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f20ac4e9a3dce9191a6f0a678d07e624b1f366f"},"cell_type":"code","source":"#converting these object type columns to floats\ntrain_df['edjefa']=train_df['edjefa'].astype(float)\ntrain_df['edjefe']=train_df['edjefe'].astype(float)\ntest_df['edjefa']=test_df['edjefa'].astype(float)\ntest_df['edjefe']=test_df['edjefe'].astype(float)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04d7e0afd3e377616674d5ac30a3e92c9c932ed7"},"cell_type":"code","source":"#double checking that all columns are now numeric - except for Id and idhogar\nprint (train_df.select_dtypes(['object']).describe(), '\\n')\nprint (test_df.select_dtypes(['object']).describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"105225123b5b2f727d1362e518bc9e6e54e5b0ae"},"cell_type":"markdown","source":"\n## 3. Taking care of the missing values"},{"metadata":{"trusted":true,"_uuid":"8530be791a45ac9bddc39261c33bfcd511f99f20"},"cell_type":"code","source":"#Now let's take care of the missing columns\nprint (\"Top Training Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Top Testing Columns having missing values:\")\nmissing_df = test_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nmissing_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad53a035a42b7bf86141673f9a9026519915f163"},"cell_type":"code","source":"#1 'v18q1' - number of tablets household owns\ntrain_df.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0dd17a64f1b036e67a7256df79828c42467a31"},"cell_type":"code","source":"#Every family that has nan for v18q1 does not own a tablet. \n#Therefore, we can fill in this missing value with zero.\ntrain_df['v18q1'] = train_df['v18q1'].fillna(0)\ntest_df['v18q1'] = test_df['v18q1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2 'rez_esc' - Years behind in school \n#let's see if high percentage of missing values in 'rez_esc' accounts for minors and people without education\nprint (train_df.loc[train_df['rez_esc'].isnull()]['age'].value_counts().head(6))\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel1'].value_counts())\nprint (train_df.loc[train_df['rez_esc'].isnull()]['instlevel2'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#another theory is that those 'na' are for individuals outside of school age\nprint (train_df.loc[train_df['rez_esc'].notnull()]['age'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9a4b5b903c4266510e14f20788bb0ae6c35b08"},"cell_type":"code","source":"#which is actually true: min age - 7, max age - 17. Assigning '0' to those people\ntrain_df['rez_esc'] = train_df['rez_esc'].fillna(0)\ntest_df['rez_esc'] = test_df['rez_esc'].fillna(0)\ntrain_df.loc[train_df['rez_esc'] > 5, 'rez_esc'] = 5\ntest_df.loc[test_df['rez_esc'] > 5, 'rez_esc'] = 5 #5 is a maximum value per competition's discussion, so here we're accounting for the outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcb4cb406b71872a922ba4bb8cb299ff76d992bf"},"cell_type":"code","source":"#3 v2a1, Monthly rent payment\nprint(train_df['v2a1'].isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7649ea2fccf0530cb9a3c5ee81f3dd36dffd5a98"},"cell_type":"code","source":"#Let's correlate it with tipovivi1, =1 own and fully paid house\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())\nprint(train_df['tipovivi1'].value_counts())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing with '0' na for fully paid house \ntrain_df.loc[(train_df['v2a1'].isnull() & train_df['tipovivi1'] == 1), 'v2a1'] = 0\ntest_df.loc[(test_df['v2a1'].isnull() & test_df['tipovivi1'] == 1), 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df.loc[train_df['v2a1'].isnull()]['tipovivi1'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tipovivi2, \"=1 own,  paying in installments\"\n#tipovivi3, =1 rented\n#tipovivi4, =1 precarious\n#tipovivi5, \"=1 other(assigned,  borrowed)\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi2'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi3'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi4'].value_counts())\nprint (train_df.loc[train_df['v2a1'].isnull()]['tipovivi5'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's replace na for precarious with '0' as well\ntrain_df.loc[(train_df['v2a1'].isnull() & train_df['tipovivi4'] == 1), 'v2a1'] = 0\ntest_df.loc[(test_df['v2a1'].isnull() & test_df['tipovivi4'] == 1), 'v2a1'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df.loc[train_df['v2a1'].isnull()]['Target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#see if we can find a feature to correlate with those remaining missing values\nv2a1_na_corr = train_df\nv2a1_na_corr.v2a1.where(v2a1_na_corr.v2a1.isnull(), 1, inplace=True)\nv2a1_na_corr['v2a1'].fillna(0, inplace = True)\nprint (v2a1_na_corr.corr()['v2a1'].sort_values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1539b3f2afaadfdb8d48baef748db94af21c5d9b"},"cell_type":"code","source":"#No luck. But since the property is 'assigned, borrowed', let's assume there's no monthly rent associated with it\ntrain_df['v2a1'].fillna(train_df['v2a1'].mean(), inplace = True)\ntest_df['v2a1'].fillna(test_df['v2a1'].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Top Training Columns having missing values:\")\nmissing_df = train_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nprint (missing_df.head())\nprint (\"Top Testing Columns having missing values:\")\nmissing_df = test_df.isnull().sum().to_frame()\nmissing_df = missing_df.sort_values(0, ascending = False)\nprint (missing_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9457a4ef21a6cebf1711e97d880d393f019e61e0"},"cell_type":"code","source":"#the rest of the missing values can be replaced with mean as their percentage towards total number of entries is insignificant\ntrain_df.fillna (train_df.mean(), inplace = True)\ntest_df.fillna(test_df.mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"861ada1e42b53afa995ada3af1d2d431a5ab207c"},"cell_type":"code","source":"print ('Columns having missing values:')\nprint (train_df.columns[train_df.isnull().any()])\nprint (test_df.columns[test_df.isnull().any()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Removing outliers with box-plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"#top 30 features with best correlation to 'Target'\nbest_correlations = train_df.corr()['Target'].abs().sort_values().tail(30)\ntype(best_correlations)\nbest_correlations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_correlation = best_correlations.index\nbest_correlation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'dependency':'dependency, Dependency rate', 'v18q1':'v18q1, number of tablets household owns', 'epared1':'epared1, if walls are bad', 'qmobilephone':'qmobilephone, # of mobile phones', \n     'pisocemento':'pisocemento, =1 if predominant material on the floor is cement',\n       'eviv1':'eviv1, =1 if floor are bad', 'instlevel8':'instlevel8, =1 undergraduate and higher education', 'rooms':'rooms,  number of all rooms in the house', 'r4h1':'r4h1, Males younger than 12 years of age', \n        'v18q': 'v18q, owns a tablet:', 'edjefe':'edjefe, years of education of male head of household', 'SQBedjefe':'SQBedjefe, years of education of male head of household squared',\n       'etecho3':'etecho3, =1 if roof are good', 'r4m1':'r4m1, Females younger than 12 years of age', 'SQBovercrowding':'SQBovercrowding, overcrowding squared', \n       'paredblolad':'paredblolad, =1 if predominant material on the outside wall is block or brick', 'SQBmeaned':'SQBmeaned, square of the mean years of education of adults (>=18) in the household',\n       'pisomoscer':'pisomoscer, \"=1 if predominant material on the floor is mosaic,  ceramic,  terrazo\"', 'overcrowding':'overcrowding, # persons per room', 'epared3':'epared3, =1 if walls are good',\n        'eviv3':'eviv3, =1 if floor are good', 'SQBescolari' :'SQBescolari, years of schooling squared',\n       'escolari':'escolari, years of schooling', 'cielorazo':'cielorazo, =1 if the house has ceiling', 'SQBhogar_nin':'SQBhogar_nin, Number of children 0 to 19 in household, squared',\n        'r4t1':'r4t1, persons younger than 12 years of age', 'hogar_nin':'hogar_nin, Number of children 0 to 19 in household',\n       'meaneduc':'meaneduc,average years of education for adults (18+)', 'Target':'Target', 'elimbasu5':'elimbasu5, \"=1 if rubbish disposal mainly by throwing in river,  creek or sea\"'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in best_correlation:\n    if len(train_df[i].unique())>2:\n        sea.boxplot(train_df[i])\n        plt.xlabel(d.get(i))\n        plt.show()\n        sea.distplot(train_df[i])\n        plt.xlabel(d.get(i))\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we'll drop only the ones with less than 100 outliers\nprint(len(train_df.loc[(train_df['SQBmeaned']>900)]))\nprint(train_df['SQBmeaned'].value_counts(sort = True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = train_df.loc[(train_df['rooms']>9)|(train_df['r4m1']>3)|\n                       (train_df['r4t1']>5)|(train_df['hogar_nin']>6)|\n                       (train_df['meaneduc']>25)|\n                       (train_df['qmobilephone']>8)|(train_df['r4h1']>2)|\n                       (train_df['SQBedjefe']>300)|(train_df['SQBescolari']>300)|\n                       (train_df['SQBhogar_nin']>70)|(train_df['SQBovercrowding']>25)|\n                       (train_df['SQBmeaned']>900)].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(to_drop, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('Target').mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ed8a34d958fe0c74d9b813cf0ab0e7b73d6c241"},"cell_type":"markdown","source":"## **5. Data visuzalisation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Target'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features with <5 possible values\nfor j in best_correlation:\n    if len(train_df[j].unique())<5:\n        sea.countplot(x=j, hue='Target', data=train_df)\n        plt.xlabel(d.get(j))\n        plt.ylabel(\"Count\")\n        #plt.title(str(j),' vs Target') \n        plt.figure()\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if target distribution in each feature cathegory is similar to overall target distribution, \n#then the chances are that the feature will have a better correlation to Target\n#I tried to combine a couple of features to produce better distributions/correlations\ntrain_df['v18q+etecho3'] = train_df['v18q']+train_df['etecho3']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['etecho3'])\nprint (train_df.corr()['Target']['v18q+etecho3'])\ntest_df['v18q+etecho3'] = test_df['v18q']+test_df['etecho3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['v18q+paredblolad'] = train_df['v18q']+train_df['paredblolad']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['paredblolad'])\nprint (train_df.corr()['Target']['v18q+paredblolad'])\ntest_df['v18q+paredblolad'] = test_df['v18q']+test_df['paredblolad']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['v18q+pisomoscer'] = train_df['v18q']+train_df['pisomoscer']\nprint (train_df.corr()['Target']['v18q'])\nprint (train_df.corr()['Target']['pisomoscer'])\nprint (train_df.corr()['Target']['v18q+pisomoscer'])\ntest_df['v18q+pisomoscer'] = test_df['v18q']+test_df['pisomoscer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['pisomoscer+instlevel8'] = train_df['pisomoscer']+train_df['instlevel8']\nprint (train_df.corr()['Target']['pisomoscer'])\nprint (train_df.corr()['Target']['instlevel8'])\nprint (train_df.corr()['Target']['pisomoscer+instlevel8'])\ntest_df['pisomoscer+instlevel8'] = test_df['pisomoscer']+test_df['instlevel8']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distribution(df, var, target, **kwargs):\n    row = kwargs.get('row', None)\n    col = kwargs.get('col', None)\n    facet = sea.FacetGrid(df, hue = target, size=4.0, aspect=1.3, sharex=False, sharey=False)\n    facet.map(sea.kdeplot, var)\n    facet.set(xlim = (0, df[var].max()))\n    facet.add_legend()\n    plt.xlabel(d.get(j))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features with >=5 possible values\nfor j in best_correlation:\n    if len(train_df[j].unique())>5:\n        plot_distribution(train_df, j, 'Target')\n\n#In the first graph instead of 0's should be nulls(we changed these before). So there is no info about monthly rate payment for non vulnerable households ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#following the same logic, let's try to combine features to get a better distribution\ntrain_df['edjefe+escolari'] = train_df['edjefe']+train_df['escolari']\nprint (train_df.corr()['Target']['edjefe'])\nprint (train_df.corr()['Target']['escolari'])\nprint (train_df.corr()['Target']['edjefe+escolari'])\ntest_df['edjefe+escolari'] = test_df['edjefe']+test_df['escolari']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can also do some pairwise feature comparison for various target cathegories with jointplots:\ndict={4: \"NonVulnerable\", 3: \"Moderate Poverty\", 2: \"Vulnerable\", 1: \"Extereme Poverty\"}\nfor i in range(1, 5):\n    sea.set(font_scale=1, style=\"white\")\n    sea_jointplot = sea.jointplot('hogar_nin', 'age', data=train_df[train_df['Target'] == i], size=6,color = sea.color_palette(\"deep\")[i], kind='kde', stat_func=None)\n    plt.title(dict.get(i))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finally, let's have some interactive plots as well - using plotly\n# Standard plotly imports\npip install plotly chart-studio\nimport chart_studio.plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Target1'] = train_df.Target\ntrain_df.Target1 = train_df.Target1.apply(str)\ntrain_df.iplot(\n    x='hogar_nin',\n    y='meaneduc',\n    # Specify the category\n    categories=\"Target1\",\n    xTitle = d.get('hogar_nin'),\n    yTitle = d.get('meaneduc'),\n    title=\"Number of children vs. Average education by Poverty level\")\ntrain_df = train_df.drop(['Target1'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.pivot(columns='Target', values='meaneduc').iplot(\n        kind='box',\n        xTitle = d.get('meaneduc'),\n        yTitle= 'Target',\n        title='Education level per different poverty groups')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = go.Bar(\n    x=train_df['Target'],\n    y=train_df['meaneduc'],\n    name=d.get('meaneduc')\n)\ntrace2 = go.Bar(\n    x=train_df['Target'],\n    y=train_df['hogar_nin'],\n    name=d.get('hogar_nin')\n)\ntrace3 = go.Bar(\n    x=train_df['Target'],\n    y=train_df['rooms'],\n    name=d.get('rooms')\n)\ndata = [trace2, trace3,trace1]\n\n\nlayout = go.Layout(\n    barmode=\"group\",\n    hovermode= 'closest',\n    showlegend= True,\n    xaxis ={\"title\":\"Target\"},\n    yaxis ={\"title\":\"Count\"}\n    \n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_correlation_df = train_df[['Target']]\nfor i in best_correlation:\n    if len(train_df[i].unique())>2:\n        best_correlation_df[i] = train_df[i]\n#Correlation Heatmap\ncorrs = best_correlation_df.corr()\ncorrs.style.background_gradient(cmap='coolwarm').set_precision(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sea.clustermap(corrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c1 = corrs.abs().unstack().drop_duplicates()\nc1.sort_values(ascending = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_sample = train_df.sample(500) #sampling for better graph\nsea.set(rc={'figure.figsize':(12,8)})\nsea.swarmplot(x='rooms', y = 'hogar_nin', hue='Target', dodge = True, data=train_df_sample, size = 4)\n#sea.violinplot(x='r4t1', y = 'hogar_nin', hue='Target', dodge = True, data=train_df_sample, size = 4)\nplt.xlabel(d.get('rooms'))\nplt.ylabel(d.get('hogar_nin'))\nplt.figure()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18ca1f43e2f5c4d180c92c5b12c2c48eaa523fe5"},"cell_type":"markdown","source":"## **6. Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"### Manual feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#some of it has already been done in visualization part above\n#poor materials used\ntrain_df[\"Poor_materials\"]=train_df['pareddes']+train_df['paredfibras']+train_df['pisonatur']+train_df['pisonotiene']+train_df['techocane']+train_df['epared1']+train_df['etecho1']+train_df['eviv1']\ntest_df[\"Poor_materials\"]=test_df['pareddes']+test_df['paredfibras']+test_df['pisonatur']+test_df['pisonotiene']+test_df['techocane']+test_df['epared1']+test_df['etecho1']+test_df['eviv1']\nprint ('Pearson correlation coefficients:')\nprint ('Poor Materials (training set): ',train_df['Poor_materials'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rich materials used\ntrain_df[\"Rich_Materials\"]=train_df['paredblolad']+train_df['pisomoscer']+train_df['techoentrepiso']+train_df['techootro']+train_df['cielorazo']+train_df['epared3']+train_df['etecho3']+train_df['eviv3']\ntest_df[\"Rich_Materials\"]=test_df['paredblolad']+test_df['pisomoscer']+test_df['techoentrepiso']+test_df['techootro']+test_df['cielorazo']+test_df['epared3']+test_df['etecho3']+test_df['eviv3']\nprint ('Pearson correlation coefficients:')\nprint ('Materials (training set): ',train_df['Rich_Materials'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Poor_Infrastructure\"]=train_df['abastaguano']+train_df['noelec']+train_df['epared1']+train_df['etecho1']+train_df['eviv1']+train_df['lugar3']+train_df['sanitario1']+train_df['energcocinar1']+train_df['elimbasu3']\ntest_df[\"Poor_Infrastructure\"]=test_df['abastaguano']+test_df['noelec']+test_df['epared1']+test_df['etecho1']+test_df['eviv1']+test_df['lugar3']+test_df['sanitario1']+test_df['energcocinar1']+test_df['elimbasu3']\nprint ('Pearson correlation coefficients:')\nprint ('Materials (training set): ',train_df['Poor_Infrastructure'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Good_Infrastructure\"]=train_df['sanitario2']+train_df['energcocinar2']+train_df['elimbasu1']+train_df['abastaguadentro']+train_df['planpri']+train_df['epared3']+train_df['etecho3']*(3)+train_df['eviv3']+train_df['lugar1']+train_df['lugar2']+train_df['lugar6']\ntest_df[\"Good_Infrastructure\"]=test_df['sanitario2']+test_df['energcocinar2']+test_df['elimbasu1']+test_df['abastaguadentro']+test_df['planpri']+test_df['epared3']+test_df['etecho3']*(3)+test_df['eviv3']+test_df['lugar1']+test_df['lugar2']+test_df['lugar6']\nprint ('Pearson correlation coefficients:')\nprint ('Infrastructure (training set): ',train_df['Good_Infrastructure'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#overcrowding + total of persons younger than 12 years of age + no level of education + zona rural\ntrain_df[\"overcrowding_total\"] = train_df[\"hacdor\"]+train_df[\"r4t1\"] +train_df[\"instlevel1\"] + train_df[\"area2\"]\ntest_df[\"overcrowding_total\"] = test_df[\"hacdor\"]+ test_df[\"r4t1\"] + test_df[\"instlevel1\"] + test_df[\"area2\"]\nprint ('overcrowding_total: ',train_df['overcrowding_total'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#years of schooling + overcdrowding\ntrain_df[\"escolari+hacapo\"] = train_df[\"escolari\"]+train_df[\"hacapo\"]\ntest_df[\"escolari+hacapo\"] = test_df[\"escolari\"]+test_df[\"hacapo\"]\nprint (train_df['escolari+hacapo'].corr( train_df['Target']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.columns[-30:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Automated Feature Engineering with Featuretools"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install --upgrade https://github.com/featuretools/featuretools/zipball/master","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#credits to Will Koehrsen for his excellent kernel: https://www.kaggle.com/willkoehrsen/featuretools-for-good#Deep-Feature-Synthesis\n#first we need to define variable types in the dataframe (boolean vs. ordered vs. continuous) \n#and group them into individual vs. household categories\nind_bool = list()\nind_ordered = list()\nind_cont = list()\nhh_bool = list()\nhh_ordered = list()\nhh_cont = list()\nprint(train_df.drop(['age', 'SQBescolari','SQBage', 'agesq'], axis = 1).columns.get_loc('Target'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_list_hh = list(train_df.drop(['escolari', 'rez_esc'], axis = 1).loc[:,'v2a1':'eviv3'].columns)+list(train_df.loc[:,'idhogar':'meaneduc'].columns)+list(train_df.drop(['age', 'SQBescolari','SQBage', 'agesq','Target'], axis = 1).loc[:,'bedrooms':].columns)\ntrain_list_ind = [column for column in list(train_df.columns) if column not in set (train_list_hh)]\nprint (len(train_list_hh)+len(train_list_ind)-len(list(train_df.columns)))\nfor i in train_list_hh:\n    if len(train_df[i].unique())<=2:\n        hh_bool.append(i)\n    elif train_df[i].dtypes == 'int':\n        hh_ordered.append(i)\n    elif train_df[i].dtypes == 'float':\n        hh_cont.append(i)\n\nfor i in train_list_ind:\n    if len(train_df[i].unique())==2:\n        ind_bool.append(i)\n    elif train_df[i].dtypes == 'int':\n        ind_ordered.append(i)\n    elif train_df[i].dtypes == 'float':\n        ind_cont.append(i) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Target'] = np.nan\n\ndata = train_df.append(test_df, sort = True)\nfor variable in (hh_bool + ind_bool):\n    data[variable] = data[variable].astype('bool')\nfor variable in (hh_cont + ind_cont):\n    data[variable] = data[variable].astype(float)\nfor variable in (hh_ordered + ind_ordered):\n    try:\n        data[variable] = data[variable].astype(int)\n    except Exception as e:\n        print(f'Could not convert {variable} because of missing values.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import featuretools as ft\nes = ft.EntitySet(id = 'households') #creating the entity set\nes.entity_from_dataframe(entity_id = 'data',    #adding first entity (table) to the entity set\n                         dataframe = data, \n                         index = 'Id')\n\n'''hh = hh_bool+hh_ordered+hh_cont+[\"Target\"]+[\"idhogar\"]\nhousehold = data.loc[data['parentesco1']==1, hh]\nes.entity_from_dataframe(entity_id = 'household',    #adding second entity (table) to the entity set\n                         dataframe = household, \n                         index = \"idhogar\")\nhousehold_rl = ft.Relationship(es[\"household\"][\"idhogar\"],\n                              es[\"data\"][\"idhogar\"])\nes = es.add_relationship(household_rl)'''\n#we'll make a new entity by normalization of the original table \nes.normalize_entity(base_entity_id='data', \n                    new_entity_id='household', #adding household table to the entity set \n                    index = 'idhogar',\n                   additional_variables = hh_bool + hh_ordered + hh_cont+[\"Target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_matrix, feature_names = ft.dfs(entityset=es, \n                                       target_entity = 'household', \n                                       max_depth = 2, \n                                       verbose = 1, \n                                       n_jobs = -1,\n                                       chunk_size = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = [str(x.get_name()) for x in feature_names]\nfeature_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_cols = []\nfor col in feature_matrix:\n    if col == 'Target':\n        pass\n    else:\n        if 'Target' in col:\n            drop_cols.append(col)\n            \nprint(drop_cols)            \nfeature_matrix = feature_matrix[[x for x in feature_matrix if x not in drop_cols]]         \nfeature_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = feature_matrix[feature_matrix['Target'].notnull()].reset_index()\ntest_df = feature_matrix[feature_matrix['Target'].isnull()].reset_index()\ntest_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idhogar = test_df['idhogar']\ntrain_df = train_df.select_dtypes(exclude=['object'])\ntest_df = test_df.select_dtypes(exclude=['object'])\ntrain_df = train_df.dropna(axis='columns')\ntest_df = test_df.dropna(axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## #7. Dimension reduction."},{"metadata":{"trusted":true,"_uuid":"bc32aebe3a9ad6b011b5a06bcd87dd6fb3f055a1"},"cell_type":"code","source":"#Removing columns with greater than 99% correlation as redundant\n# Create correlation matrix\ncorr_matrix = train_df.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.99\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.99)]\n\nprint(f'There are {len(to_drop)} correlated columns to remove.')\nprint(to_drop)\n\ntrain_df = train_df.drop(columns = to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be873f80a42f842fa0a5bf0918bb616c235b1cf3"},"cell_type":"code","source":"#let's compare all the correlation coefficients now\nprint (train_df.corr()['Target'].abs().sort_values().tail(30))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5e48fa636e55260a20ccdae0ad90bd24b144dcb"},"cell_type":"code","source":"#realligning two datasets based on the features selected in training\ntrain_df_H20 = train_df # for use with autoML\ny_df = train_df['Target']\ntrain_df, test_df = train_df.align(test_df, join = 'inner', axis = 1)\nprint(f\"Training set shape:{train_df.shape}, testing set shape:{test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9de734fdf88dfb328d6cb6b9f28894b9c8b57bd"},"cell_type":"markdown","source":"## **8. Converting to numpy arrays and scaling**"},{"metadata":{"trusted":true,"_uuid":"38d13dcc45b892f0b612c44a9381f8994585c028"},"cell_type":"code","source":"#converting to numpy array\nX = train_df.values\ny = y_df.values\ny = y.reshape(-1, 1)\ntest_np = test_df.values\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"140ceb74f21ac4a1fc76a517eef046c513c68359"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split (X, y,test_size = 0.1, random_state = 123)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8acd29fb35fe3d3b3db0000d9c141668f70b1016"},"cell_type":"code","source":"#Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX = sc.transform(X)\nprint (X)\ntest_np = sc.transform (test_np)\nprint (test_np)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c568ea4fe4b96d46ddd2f2f681124524c844221"},"cell_type":"markdown","source":"## **9. Building the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#First, we'll try some autoML tool to generate a model\nimport h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\n\nhtrain = h2o.H2OFrame(train_df_H20)\nhtest = h2o.H2OFrame(test_df)\nx = htrain.columns\ny =\"Target\"\nx.remove(y)\n# This line is added in the case of classification\nhtrain[y] = htrain[y].asfactor()\n\naml = H2OAutoML(max_runtime_secs = 400)\naml.train(x=x, y =y, training_frame=htrain)\nlb = aml.leaderboard\nprint (lb)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''print(\"Generate predictionsâ€¦\")\ntest_y = aml.leader.predict(htest)\ntest_y = test_y.as_data_frame()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"050c6f76339e7d34cd3f895ca76f069223a2ed6b"},"cell_type":"code","source":"#AutoML can make a decent prediction, but not as good as the manually tuned model yet. For now we'll use LGBoost with early stopping for our final prediction\n#credits to https://www.kaggle.com/mlisovyi/lighgbm-hyperoptimisation-with-f1-macro for the parameters values\nimport lightgbm as lgb\nclassifier = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdb80d84da7432163a14e9c4373fe4c001e45cb3"},"cell_type":"code","source":"eval_set = [(X_train, y_train), (X_test, y_test)]\nclassifier.fit(X_train, y_train, eval_metric=\"multiclass\", eval_set=eval_set, verbose=True, early_stopping_rounds=400) #LGBoost model model\ny_pred = classifier.predict(X_test) \ny_pred = y_pred.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3367900d7a6b568b7882f1f9b232fbc884ec539"},"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_pred)\nprint (cm1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04b7f08a1eb8009a127025b7652b4ab9e6a32b94"},"cell_type":"code","source":"from sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_pred, average ='macro')\nprint ('f1 score for LGBoost model:',f1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Making prediction "},{"metadata":{"trusted":true,"_uuid":"261298d8d91eceaf6eb5464934f70fefc796d48e"},"cell_type":"code","source":"y_pred = classifier.predict(test_np)\ny_pred = y_pred.reshape(-1, 1)\ny_pred = y_pred.astype(int)\nprint(plt.hist(y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 11. Visualizing/explaining the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise with a barplot\nimport seaborn as sns\nindices = np.argsort(classifier.feature_importances_)[::-1]\nindices = indices[:30]\n\n\nplt.subplots(figsize=(40, 40))\ng = sea.barplot(y=train_df.columns[indices], x = classifier.feature_importances_[indices], orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=40)\ng.set_ylabel(\"Features\",fontsize=40)\ng.tick_params(labelsize=40)\ng.set_title(\"Feature importance\", fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 12.Submitting to the competition"},{"metadata":{"trusted":true,"_uuid":"7606fa8969e3bfab5facd20d2cb9a3ce77635d0d"},"cell_type":"code","source":"#Submitting the prediction\ntest_df['Target'] = y_pred.astype(int)\ntest_df['idhogar'] = idhogar\nsubmit = submit.merge(test_df[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n#submit['TARGET'] = test_y['predict'].values - for autoML  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['Target'] = submit['Target'].fillna(4) #there is no head of the household, assigning '4' to those\nsubmit['Target'] = submit['Target'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['Target'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('LGBClassification.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('The prediction was based on LGBoost model with early stopping, trained on ', train_df.shape[1],' features. F1 score for the the training dataset was ',f1,'.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}