{"cells":[{"execution_count":null,"outputs":[],"metadata":{"_uuid":"218a5ed660e50098d58b0b1d4692c0da574cd30c","_cell_guid":"fe894dd5-c2c9-4e99-bc0d-f6846b3464eb"},"cell_type":"code","source":"#imports\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport matplotlib.image as mpimg\nimport random\nfrom PIL import Image\nimport collections as co\nimport cv2\nimport scipy as sp\nimport copy\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfigWidth = figHeight = 10\nwhoAmI = 24601\nrandom.seed(whoAmI)\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nsns.set_style(\"dark\")\n# Any results you write to the current directory are saved as output."},{"metadata":{"_uuid":"e73cce5035f6304eb35ba778ef3aa688507dc889","_cell_guid":"a7e6fff4-d28b-45bd-8291-2e0b8f54b3dd"},"source":"# Whales\n\nWhales are cool! Mostly because I think animals are generally awesome, but Whales are truly badasses of the great blue sea. Let's try to identify them!\n\n## Summary Statistics","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"df3ee50933449f84d9af4631f8a393e7044c1885","_cell_guid":"7f0755da-aec4-410e-bad0-fd82450f6919"},"cell_type":"code","source":"print(len(os.listdir(\"../input/train\")))\nprint(len(os.listdir(\"../input/test\")))"},{"metadata":{"_uuid":"26740b75b764b9cf2d9c3074108477dfc08306ca","_cell_guid":"e9570ccc-9c43-4d2b-b815-58735c02fcb4"},"source":"In this case, we have more test data than we have training data. This means we have to especially optimize our models for out-of-sample prediction.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"114bad61c8f46463d7dab7a6827533578d2a3b8d","_cell_guid":"75cd3574-1b6a-4f8b-b59a-8135e766b408","collapsed":true},"cell_type":"code","source":"trainFrame = pd.read_csv(\"../input/train.csv\")"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"378a6d3cd1a95b818b20a67b83884d452495a6e9","_cell_guid":"ba76ccea-d3aa-48bb-aae2-f2ffbda2bd83"},"cell_type":"code","source":"trainFrame.shape"},{"metadata":{"_uuid":"1dd1e87b9cf7ba1efee9247d244414dc6301da5e","_cell_guid":"c0fbc8b2-cf18-4858-9f28-2b0b8c16f3be"},"source":"Just to confirm, we do have the same number of training IDs as images in `../input/train`.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"72954932ee05f727e5db126bb6b051356f9107af","_cell_guid":"8285ba26-a86b-4e73-ada1-d524d1a7189c"},"cell_type":"code","source":"len(trainFrame[\"Id\"].unique())"},{"metadata":{"_uuid":"eef0b91a639ccf2279baf87a962c592877409585","_cell_guid":"6b135c8a-8c68-417f-8419-63b5f8928aab"},"source":"We have a ton of unique labels! This is already shaping up to be a hard problem.\n\n## Distribution of Labels","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"142f650f0a2bdb0b950b90d2d2b73c9e503a7d46","_cell_guid":"33f4dd7a-2acd-42d8-a2ac-4f4a247f42e5","collapsed":true},"cell_type":"code","source":"idCountFrame = trainFrame.groupby(\"Id\",as_index = False)[\"Image\"].count()"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"0a4b74e54f9edce088c0a19d25fc501665086f68","_cell_guid":"697c549f-7d0c-4e99-8373-35d48b90fbc5","collapsed":true},"cell_type":"code","source":"idCountFrame = idCountFrame.rename(columns = {\"Image\":\"numImages\"})"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"e1f0c77bcdd5497cc1dc6d9a8d716d04412e3d84","_cell_guid":"e6bf01da-8058-42e3-8233-94cbb22fa971","collapsed":true},"cell_type":"code","source":"idCountFrame[\"density\"] = idCountFrame[\"numImages\"] / np.sum(idCountFrame[\"numImages\"])"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"b8eb50bbbc7d6465b85fc991ae94bf8e1591234d","_cell_guid":"853db42e-3f99-487f-acb9-26706f67cd4b","collapsed":true},"cell_type":"code","source":"idCountFrame = idCountFrame.sort_values(\"density\",ascending = False)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"19c8a55a4c07c7283049e96786e9dd58db70802a","_cell_guid":"85dae00e-5586-487b-91da-615ba6c729fd","collapsed":true},"cell_type":"code","source":"#rank them\nidCountFrame[\"rank\"] = range(idCountFrame.shape[0])\nidCountFrame[\"logRank\"] = np.log(idCountFrame[\"rank\"] + 1)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"94a75188aa9fc73f1bb9ad222858a91b4d38b33c","_cell_guid":"cdefec10-2490-4e63-8259-2ca42673717c"},"cell_type":"code","source":"plt.plot(idCountFrame[\"logRank\"],idCountFrame[\"density\"])\nplt.xlabel(\"$\\log(Rank)$\")\nplt.ylabel(\"Density\")\nplt.title(\"$\\log(Rank)$-Density Plot for our Labels\")"},{"metadata":{"_uuid":"734075d84b963a1e12a725bef4d44b17d750b358","_cell_guid":"33e16625-69c2-4aed-b7c0-d7f3e1156bfc"},"source":"_Figure 1: $\\log(Rank)$-Density Plot for our labels._\n\nWe see an extreme distribution forming, where we have many IDs with very few observations. This might suggest a severe imbalanced classes problem; We may in this case want to do some form of SMOTE for effectively accounting for imbalanced classes. Other than that, we will likely need to do many image transformations in order to bump up the observation numbers for some of our classes.\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"ce805cec855a0a972a051a7c5f8ca11cf0717548","_cell_guid":"304ae03d-3969-49a3-b4f2-2ddef74d4bfa"},"cell_type":"code","source":"topLev = 10\nidCountFrame.iloc[0:topLev,:]"},{"metadata":{"_uuid":"2198b675997bd1d101d3fc5d383f592497301980","_cell_guid":"a02a7c18-d0ae-4dea-9513-c54f5a6c9460"},"source":"_Table 1: Top 10 Most common labels._\n\nWe see that the largest class by a longshot is the `new_whale` label.","cell_type":"markdown"},{"metadata":{"_uuid":"fc5ba7e60a4811ab5b5d8679b6ef7a4086d1f63b","_cell_guid":"46096cd5-00a2-4aba-b006-ae8bbbc856d7","collapsed":true},"source":"## Let's watch some whales\n\nLet's open up some of the images themselves.\n","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"0689808cd91bb94f8e9481993513dcfcb0b9167a","_cell_guid":"08ad1e2b-75b4-47c9-abae-5ef76185b3cc","collapsed":true},"cell_type":"code","source":"trainDir = \"../input/train\"\ntestDir = \"../input/test\""},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"1976b7ec76ec40c47f99292fda6d088805cd10f3","_cell_guid":"73c5dc16-ec41-434b-92f5-343ce76ad33b"},"cell_type":"code","source":"sampleImageFilename = \"../input/train/00022e1a.jpg\"\nsampleImage = mpimg.imread(sampleImageFilename)\nplt.imshow(sampleImage)"},{"metadata":{"_uuid":"fafb6f73681ceacd7ed19a7fc72a4ab204cb64c3","_cell_guid":"f0b219a7-f7e3-4ae8-9e61-96ddcafd5945"},"source":"_Figure 1: One of the images present in the training set._\n\nA few things stand out about this image:\n\n1. It's already been altered some degree, seemingly emphasizes the R of the RGB spectrum present in the data. This might suggest that we will need to consider a classifier that is somewhat RGB-agnostic.\n\n2. It has a few parts of the image that aren't relevant to classification (see IDing at the bottom). I'm not sure whether this is supposed to correlate to a particular label in our training set, so it might be worthwhile exploring.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"ed36ca5ebe622d5807bc517ef205f0b567f80019","_cell_guid":"2ad24608-4164-4c42-855a-8c3d5b188d66"},"cell_type":"code","source":"iiwcInfo = idCountFrame[idCountFrame[\"Id\"].str.contains(\"iiwc\")]\nnumberInfo = idCountFrame[idCountFrame[\"Id\"].str.contains(\"1034\")]\nprint(iiwcInfo)\nprint(numberInfo)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"12d0d20fa095755d94894d621277c4807e27faed","_cell_guid":"8a5be183-1094-4fbb-a0a4-31329ababb8a"},"cell_type":"code","source":"trainFrame[trainFrame[\"Id\"] == \"w_103488f\"]"},{"metadata":{"_uuid":"0b6e8795929a37836ea0a39f133d99573e9f3c85","_cell_guid":"f31e9ff8-a317-4556-ac4c-4a8836d291f5"},"source":"Doesn't look to be correlated with ID. This is probably filler information from the camera capture.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"73c096058c361c5ba8cab08eef46cb6b56f8742f","_cell_guid":"4f59faa7-b591-49f4-ad61-d7a8c4ec396d"},"cell_type":"code","source":"#sample a couple of pictures\nnumSampled = 4\nsampledPicNames = random.sample(os.listdir(trainDir),numSampled)\n#then read the images\nreadImages = [mpimg.imread(trainDir + os.sep + sampledPicNames[i])\n             for i in range(len(sampledPicNames))]\n#then plot\nfig, subplots = plt.subplots(2,2)\nfig.set_size_inches(figWidth,figHeight)\nfor i in range(len(readImages)):\n    subplots[int(i / 2),i % 2].imshow(readImages[i])"},{"metadata":{"_uuid":"d8d7fa5084b2909ab9cac1610201ae8fb3a0f631","_cell_guid":"4984c0cd-f1ec-4470-ba0e-5bd6df8a0a01"},"source":"_Figure 2: A couple of sample images from the training set._\n\nA couple of interesting things are going on here:\n\n1. They are not all the same shape. This means we either need a shape-agnostic model. Or we will have to collapse dimensions of the pictures into the smallest recommended size.\n\n2. They are not all the same color spectrum with respect to RGB. We see the bottom two are in black-and-wihte, and the top two are in full color. This suggests again that we should be looking for a model or a feature extraction pipeline that is flexible to different coloring schemes in the pictures.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"173a250d44a68eeaa8a2cf855d1290672d00cca3","_cell_guid":"18fff981-6d06-48be-8445-ebf4eec8be07"},"cell_type":"code","source":"#then sample the test set\nnumSampled = 4\nsampledPicNames = random.sample(os.listdir(testDir),numSampled)\n#then read the images\nreadImages = [mpimg.imread(testDir + os.sep + sampledPicNames[i])\n             for i in range(len(sampledPicNames))]\n#then plot\nfig, subplots = plt.subplots(2,2)\nfig.set_size_inches(figWidth,figHeight)\nfor i in range(len(readImages)):\n    subplots[int(i / 2),i % 2].imshow(readImages[i])"},{"metadata":{"_uuid":"afd3892679af7e57db6840899d069f14e4315dc7","_cell_guid":"d080ec17-4a02-4d96-8e9f-9bc83f5ab4f3"},"source":"_Figure 3: Some whales from the test set._\n\nSimilar situation where we have some images that are greyscaled (bottom-left), redscaled (top-left and bottom-right), and some that are in full color (top-right). At the very least, it does suggest a similar image quality issues in the test set as there is in the training set.\n\n### What is the scale of our image scaling issue?\n\nI'm interesting in figuring out how much that scaling is an issue in our dataset. Let's check the distribution of image sizes.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"b620117cc4f895ceb510d1b485125029a9ffa589","_cell_guid":"072000ff-718c-43b8-8e21-2b2cacc21af2","collapsed":true},"cell_type":"code","source":"imageSizes = co.Counter([Image.open(f'../input/train/{filename}').size\n                        for filename in os.listdir(\"../input/train\")])"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"2f2cbc627eccefd6c7925dc55a2a2882c35baf4a","_cell_guid":"79508cad-b428-4a6d-89d1-71e30a3df39d","collapsed":true},"cell_type":"code","source":"imageSizeFrame = pd.DataFrame(list(imageSizes.most_common()),columns = [\"imageDim\",\"count\"])"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"c811717f564d97ee6c6ed313cb8422d46fd5a5f2","_cell_guid":"2e7b299d-597f-4bb6-9d9d-21f4118798c6","collapsed":true},"cell_type":"code","source":"#get density\nimageSizeFrame[\"density\"] = imageSizeFrame[\"count\"] / np.sum(imageSizeFrame[\"count\"])\n#get rank\nimageSizeFrame[\"rank\"] = range(imageSizeFrame.shape[0])\nimageSizeFrame[\"logRank\"] = np.log(imageSizeFrame[\"rank\"] + 1)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"a3418da7d9e7b248af3395bf55e32b2aa27ecbf9","_cell_guid":"625ccc7e-1344-4419-a4f5-78ac13d7b9d9"},"cell_type":"code","source":"#then plot\nplt.plot(imageSizeFrame[\"logRank\"],imageSizeFrame[\"density\"])\nplt.xlabel(\"$\\log(Rank)$\")\nplt.ylabel(\"Density\")\nplt.title(\"$\\log(Rank)$-Density Plot for image sizes in the training set\")"},{"metadata":{"_uuid":"a1f1a8bd157e5bc06fce5999ab8a84835e1d33ea","_cell_guid":"18cd2195-ae85-4b8a-85f4-2e46f279acfd"},"source":"_Figure 4: $\\log(Rank)$-Density PLot for image sizes in the training set._\n\nWe see that we have over $e^7 \\approx 1100$ different image sizes. This means we will need to do some substantial resizing to do.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"77dae28a4e3b7fd2bfbee6e915771aa080207eeb","_cell_guid":"7b709527-1381-441d-860d-d0f592349665"},"cell_type":"code","source":"topLev = 10\nimageSizeFrame.iloc[0:topLev,:]"},{"metadata":{"_uuid":"ece0248144319f31ceb37203ade5c30818b7858c","_cell_guid":"c53786ad-8b7d-4ff5-8ebe-96c3c46e71c8"},"source":"_Table 10: Top 10 Most Common Image Sizes in the training set._\n\nWe see that around $11\\%$ of our training observations are of size $(1050,600)$, with around $9.65\\%$ being of size $(1050,700)$. These would be good options for starting to standardize image sizes, but let's confirm that these are also the highest rank image sizes in the test set as well.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"7417ef16dd087e36f2fdaea8a218a619cbb804c2","_cell_guid":"ed40cc55-e244-4587-bd48-c1cc4951fbc0","collapsed":true},"cell_type":"code","source":"testImageSizesCounter = co.Counter([Image.open(f'../input/test/{filename}').size\n                                    for filename in os.listdir(\"../input/test\")])"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"fbab9a28e6a85a4286f2f670c4cdfca82e7fab89","_cell_guid":"fd306c52-f44f-49d4-a430-6c83a7ae8147","collapsed":true},"cell_type":"code","source":"testImageSizeFrame = pd.DataFrame(list(testImageSizesCounter.most_common()),\n                                  columns = [\"imageDim\",\"count\"])"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"b23ab098fd299ed133f14fbe60a02d5d77030331","_cell_guid":"72080dc7-3cc9-40c5-986c-8cc27921d8be","collapsed":true},"cell_type":"code","source":"#get density\ntestImageSizeFrame[\"density\"] = testImageSizeFrame[\"count\"] / np.sum(testImageSizeFrame[\"count\"])\n#get rank\ntestImageSizeFrame[\"rank\"] = range(testImageSizeFrame.shape[0])\ntestImageSizeFrame[\"logRank\"] = np.log(testImageSizeFrame[\"rank\"] + 1)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"0d3a11d7cb8276f7dc86c150e12190cea3f17277","_cell_guid":"e15cfd33-683b-4e2e-8de0-1a7f29d288fc"},"cell_type":"code","source":"topLev = 10\ntestImageSizeFrame.iloc[0:topLev,:]"},{"metadata":{"_uuid":"abd0e074d8615b8d685cbe55331b25b42ceac25c","_cell_guid":"51d2f29b-3a12-49e0-adf0-6ba3bbfa71ff"},"source":"_Table 3: Top 10 Image Sizes for the test set._\n\nThe top two in the case are also $(1050,600)$ and $(1050,700)$. This suggests that if we pick one of the top image sizes to standardize the training set, it'll be a reasonabe choice in the test set as well.","cell_type":"markdown"},{"metadata":{"_uuid":"22b677a4d018c4ebe1133e0804a49448646ce341","_cell_guid":"7ff8839b-b5e2-4c95-ae5b-ba5728cb390c"},"source":"### How many images are on different color scales?\n\nWe saw from our early EDA that some of the images are either on a greyscale or redscale format, which is different from typical RGB pictures. One of the questions we have is, how many images are on these different color sclaes?\n\n#### Grayscale\n\nWe will use the following function for testing whether an image is grayscaled.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"2c86c792ffacb1a67e2b6bb9b04e31f7d74de750","_cell_guid":"d1dc4cea-2f0e-47cc-bd91-a6d1d7b24cb2","collapsed":true},"cell_type":"code","source":"def is_grey_scale(givenImage):\n    \"\"\"Adopted from \n    https://www.kaggle.com/lextoumbourou/humpback-whale-id-data-and-aug-exploration\"\"\"\n    w,h = givenImage.size\n    for i in range(w):\n        for j in range(h):\n            r,g,b = givenImage.getpixel((i,j))\n            if r != g != b: return False\n    return True"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"80be9b98901242cd88b0b5b12bd1a013254966e2","_cell_guid":"f6d0be92-7ebd-4b1d-bc5b-9d7399c1e5ca","collapsed":true},"cell_type":"code","source":"sampleFrac = 0.1\n#get our sampled images\nimageList = [Image.open(f'../input/train/{imageName}').convert('RGB')\n            for imageName in trainFrame['Image'].sample(frac=sampleFrac)]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"fd96c40429e3674ae7f5ae7131a9eab3f3130fff","_cell_guid":"b0676df0-2415-4920-a183-0a7a4f508eba","collapsed":true},"cell_type":"code","source":"isGreyList = [is_grey_scale(givenImage) for givenImage in imageList]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"4e9763e031ff895ab82fd605f464b4a08512a067","_cell_guid":"3e0da271-9d08-445c-94e6-41efb373d6f5"},"cell_type":"code","source":"#then get proportion greyscale\nnp.sum(isGreyList) / len(isGreyList)"},{"metadata":{"_uuid":"212061c575a946ece90a6767fa018e71c58ec273","_cell_guid":"9789c6ad-016b-4567-9eec-d28339ec0f6f"},"source":"We see that around half of the images in the training set are greyscale. This suggests to me that we need to create image transformations that are very agnostic to the RGB spectrum (i.e. bump up the number of greyscaled images in the smaller classes).\n\n#### Redscale","cell_type":"markdown"},{"metadata":{"_uuid":"6257c33c0ccd2b8dc845890821ddd0fac10e75a8","_cell_guid":"50331831-8e20-43cc-9966-ae8fbb7b374d"},"source":"Unfortunately, I haven't found a function yet to identify whether there is redscaling in a picture. As indiciative of the bottom-right of Figure 3, there are in fact instances of redscaling in the dataset. If someone has a recommendation on how to identify redscaling in an image, that would be useful knowledge for this EDA.","cell_type":"markdown"},{"metadata":{"_uuid":"100d9ed84da10b34e3d90ce79bc9156d6ba79098","_cell_guid":"c08a3be1-7060-4967-a03b-97f766c281da"},"source":"### How different is our training set from our test set?\n\nOne of the questions that will be essential for out of-sample prediction optimization is to check how different our training set is in aggregate to our test set. Here's my approach. Take $D_{test}$ to be the test set and $D_{train}$ to be the training set.\n\n1. Sample $Y_1,...,Y_{1000} \\sim D_{test}$ and $X_1,...,X_{1000} \\sim D_{train}.$ Create pairs $\\{(X_1,Y_1),...,(X_{1000},Y_{1000})\\}.$\n\n2. Convert all $X_1,...,X_{1000},Y_1,...,Y_{1000}$ to black and white.\n\n3. Get the pixel value distribution for $X_1,...,X_{1000},Y_1,...,Y_{1000}$. Compute [Wasserstein Distance](https://en.wikipedia.org/wiki/Wasserstein_metric) for the pixel distributions for pairs $\\{(X_1,Y_1),...,(X_{1000},Y_{1000})\\}.$\n\n4. Compute the mean Wasserstein Distance $\\hat{WS}$ from this sample.\n\n5. Bootstrap mean null Wasserstein Distance by performing $1000$ simulations of $1000$ samples each from $D = D_{test} \\cup D_{train}.$\n\n6. Compute $p$-value for $\\hat{WS}.$","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"5f5aa6dc6de4cedfd3884b1d858833148ce9daf8","_cell_guid":"069880cd-c992-420f-8230-ab0a53509e6f","collapsed":true},"cell_type":"code","source":"#first get filenames\ntrainImageFilenames = os.listdir(\"../input/train\")\ntestImageFilenames = os.listdir(\"../input/test\")"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"8b0df4b27546a4edc8da8e6a9ae229bcc794035e","_cell_guid":"c94423ba-ed45-4402-9a7b-9e5bb42fcff6","collapsed":true},"cell_type":"code","source":"#sample 1000 from each\nsampleSize = 1000\ntrainImageFilenamesSample = random.sample(trainImageFilenames,sampleSize)\ntestImageFilenamesSample = random.sample(testImageFilenames,sampleSize)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"4a5d60804b8b28fca94ae5397f5197c977e9bb62","_cell_guid":"80384377-b800-4ddf-97ca-59ada7368fd8","collapsed":true},"cell_type":"code","source":"#then get images\ntrainImageSample = [cv2.imread(f'../input/train/{trainImageFilename}',0)\n                    for trainImageFilename in trainImageFilenamesSample]\ntestImageSample = [cv2.imread(f'../input/test/{testImageFilename}',0)\n                    for testImageFilename in testImageFilenamesSample]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"bde399358108ff79d0a6db5417a223ca65dc5194","_cell_guid":"6678841c-a6df-4e15-850f-6d307eb74e51","collapsed":true},"cell_type":"code","source":"#then get histograms for each\ncolorMax = 256\ntrainImageHists =  [cv2.calcHist([trainImage],[0],None,[colorMax],[0,colorMax]).squeeze()\n                    for trainImage in trainImageSample]\ntestImageHists =  [cv2.calcHist([testImage],[0],None,[colorMax],[0,colorMax]).squeeze()\n                    for testImage in testImageSample]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"0b7acbdde55587b37c4674704e9d20a773174307","_cell_guid":"13daaccc-f308-408d-9758-6efdb25206fa","collapsed":true},"cell_type":"code","source":"#normalize each\ntrainImageHists = [trainImageHist / np.sum(trainImageHist) for trainImageHist in trainImageHists]\ntestImageHists = [testImageHist / np.sum(testImageHist) for testImageHist in testImageHists]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"4350ca697653b9b96ec6cb431398e6b515b8ffd7","_cell_guid":"96b0edd3-ab0c-4009-9ae8-e51eebf58e93","collapsed":true},"cell_type":"code","source":"#then get wasserstein distances\nwassersteinDistances = [sp.stats.energy_distance(trainImageHists[i],testImageHists[i])\n                        for i in range(len(trainImageHists))]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"920b096fb383d91d10b5fc0ce38a6c6ae9553666","_cell_guid":"e64a10a3-d6f6-42f2-8f40-abae842cce6c","collapsed":true},"cell_type":"code","source":"testStatistic = np.mean(wassersteinDistances)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"dfa48e3affe8027ef3e2fd2acde8648d370ea00b","_cell_guid":"d5459fda-f24f-4337-881e-7e0aaf56007a"},"cell_type":"code","source":"testStatistic"},{"metadata":{"_uuid":"c1c4dc4959cfcf0e4bc96560b5f36e38d77bdcbd","_cell_guid":"f72a49d9-f1ca-4a35-9d9e-74ed8c8de475"},"source":"This is relatively small for Wasserstein Distances. Let's see if this is anything significant based on our train-test split.","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"2b1deb90ad374cde98312d92ae1b1c50d037fcd5","_cell_guid":"9f33b884-d21d-4e58-ad76-5d0a407f980c","collapsed":true},"cell_type":"code","source":"def bootstrapMeanWassersteinDistance(imageList,numSamples):\n    \"\"\"Helper for bootstrapping the mean wasserstein distance from a given filename list\"\"\"\n    #first get full sample\n    fullSampleImages = random.sample(imageList,numSamples * 2)\n    #then get train-test split by indices\n    fullSampleImageIndices = [i for i in range(len(fullSampleImages))]\n    trainImageSampleIndices = random.sample(fullSampleImageIndices,numSamples)\n    testImageSampleIndices = list(set(fullSampleImageIndices) - set(trainImageSampleIndices))\n    #then actually get said images\n    trainImageSample = [fullSampleImages[i] for i in trainImageSampleIndices]\n    testImageSample = [fullSampleImages[i] for i in testImageSampleIndices]\n    #then get histograms\n    colorMax = 256\n    trainImageHists =  [cv2.calcHist([trainImage],[0],None,[colorMax],[0,colorMax]).squeeze()\n                        for trainImage in trainImageSample]\n    testImageHists =  [cv2.calcHist([testImage],[0],None,[colorMax],[0,colorMax]).squeeze()\n                        for testImage in testImageSample]\n    #normalize each\n    trainImageHists = [trainImageHist / np.sum(trainImageHist) \n                       for trainImageHist in trainImageHists]\n    testImageHists = [testImageHist / np.sum(testImageHist) \n                      for testImageHist in testImageHists]\n    #then get wasserstein distances\n    wassersteinDistances = [sp.stats.energy_distance(trainImageHists[i],testImageHists[i])\n                        for i in range(len(trainImageHists))]\n    #then get test statistic\n    return np.mean(wassersteinDistances)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"5bf080099918fbba406ccbe6bebc3149f2a34b28","_cell_guid":"f2674cfd-71b3-4c1d-9aa0-43a46456ca0e","collapsed":true},"cell_type":"code","source":"def runSimulations(imageList,numSims,numSamples):\n    \"\"\"Helper that bootstraps our full distribution of mean wasserstein distances\"\"\"\n    wdDist = [bootstrapMeanWassersteinDistance(imageList,numSamples) for i in range(numSims)]\n    return wdDist"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"1316e4ad9ce52008de0317df6951291629f52369","_cell_guid":"e01e59c5-9d08-4717-a169-a74a0431e3b5","collapsed":true},"cell_type":"code","source":"#then form filename list\ntrainImageFilenames = [f'../input/train/{trainImageFilename}'\n                       for trainImageFilename in trainImageFilenames]\ntestImageFilenames = [f'../input/test/{testImageFilename}'\n                       for testImageFilename in testImageFilenames]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"844af4293f898f747b936284f70ebed687588242","_cell_guid":"e4114b1e-1d8f-45db-ad80-071070a8801b","collapsed":true},"cell_type":"code","source":"filenameList = copy.deepcopy(trainImageFilenames)\nfilenameList.extend(testImageFilenames)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"93519e6c1eb28dfa806744b7217234fa73e3116b","_cell_guid":"91b66657-4502-4617-9f57-ef4d35ba5607","collapsed":true},"cell_type":"code","source":"#and because we would crash Kaggle if we loaded in all the images, let's just load in 8000\nmetaSampleSize = 8000\nfilenameSample = random.sample(filenameList,metaSampleSize)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"bb5119bc9da731d7fadf8920addec926a810c95d","_cell_guid":"9da1a883-980b-4a3f-92c6-4f44a43aeaf3","collapsed":true},"cell_type":"code","source":"imageList = [cv2.imread(filename,0) for filename in filenameSample]"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"f51337019b1620489513062b4c4577b0add44fde","_cell_guid":"d3c2a2ac-57cd-4fa1-ab69-2e2c948a77de","collapsed":true},"cell_type":"code","source":"numSims = 100\nnumSamples = 1000\nwdDist = runSimulations(imageList,numSims,numSamples)"},{"execution_count":null,"outputs":[],"metadata":{"_uuid":"fb1843f7bce5bde4029e2a043b2739afeb4eba38","_cell_guid":"3dd36f00-31d0-44d0-9843-69bd7b528f86"},"cell_type":"code","source":"wdDistVec = np.array(wdDist)\nnp.mean(wdDistVec > testStatistic)"},{"metadata":{"_uuid":"8305c17e0d225a49adae4b1856e1cf8079d946f0","_cell_guid":"ad25e17d-d09c-4d14-a287-92534275ee93"},"source":"Not significant! This suggests that our training samples are generally similar to our test samples, so there isn't systematic bias in this context.\n\n# Conclusion\n\nWe have a couple takeaways from the EDA:\n\n* There are many classes that are underrepresented in the dataset. This suggests that we will need to make a large number of transformations to our underrepresented classes in order to give them a re-balance this classification problem.\n\n* We see that there are thousands of different image sizes in this dataset. We may need to find a way to crop and pad thes images until they fit the largest image size class of $(1050,600)$ or the second largest, which is $(1050,700).$\n\n* We see that we have a mixture of different color schemes to the pictures. Many of them are greyscale, some of them are redscale, and a portion of them are genuine color pictures. We need to ensure that our classifier is color scheme agnostic by greyscaling and redscaling the color pictures we can find.\n\n* We see that by our Wasserstein Distance exercise, the pixel makeup of the training set is insignificantly different from the pixel makeup of the test set. Thus, we would argue that the test set is not that systematically different than the training set, which puts us in a good place for out-of-sample prediction.\n\nNext step: transformations! Coming soon...","cell_type":"markdown"},{"execution_count":null,"outputs":[],"metadata":{"collapsed":true},"cell_type":"code","source":""}],"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"file_extension":".py","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.4","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"}}}