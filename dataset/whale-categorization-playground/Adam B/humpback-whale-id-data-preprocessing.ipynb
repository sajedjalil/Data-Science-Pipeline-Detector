{"cells":[{"metadata":{"_cell_guid":"68e3007d-74fa-4eca-855d-49c810e20009","_uuid":"ca2c0018a6c9f16fadca8d1c131821a73978f318"},"cell_type":"markdown","source":"# Acknowledgments\nThis notebook is a fork of *\"Humpback Whale ID: Data and Aug Exploration\"* by** Lex Toumbourou**, and contains also some parts coppied from other kernels/notebooks. Unfortunatelly, I did not written them down, so I cannot provide you a list with proper citations. Sorry!\n\nIf you happen to identify part of your work in this notebook, please, let me know, and I will add a note about it to this section.\n\n# Intro\nThis notebook provides/showcases a preprocessing of the input data for the Humpback Whale Identification Challenge. It firstly examine the data, and then it provide utilities to ease the augumentation."},{"metadata":{"_uuid":"475daca21ca7d0cf241f43bde192260aecc13d57"},"cell_type":"markdown","source":"## Important imports"},{"metadata":{"_cell_guid":"7814d047-2889-4011-8979-bd806aa7c43a","_uuid":"2280846462c23119ef8dd2d4e3d8b285c28c6bfc","trusted":true},"cell_type":"code","source":"import sys\nfrom collections import Counter\nimport random\nimport itertools\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport skimage.filters\n\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.image import (\n    random_rotation, random_shift, random_shear, random_zoom,\n    random_channel_shift, img_to_array)\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ebf109a9-6afe-49e3-8f92-dbb77a76f04c","_uuid":"8ed38085d2a61a9df3b7f72346cbfc89bcd19a1a","trusted":true},"cell_type":"code","source":"np.random.seed(42) # here, this is set to a constant for a reproducible results. You may drop this line to gain proper randomness on every run.\nINPUT_DIR = '../input/whale-categorization-playground/'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a3a38bae0feb96e479f5ca637b772a3b4e02d84"},"cell_type":"markdown","source":"## General utility functions"},{"metadata":{"_cell_guid":"173fc1a5-0518-420d-bc51-e87685767274","_uuid":"07fd08f0718cb78959a5c43b3560a818c5d3f490","trusted":true},"cell_type":"code","source":"def plot_images_for_filenames(filenames, labels, rows=4):\n    '''\n    Loads the images from the file paths provided in filenames,\n    and prints them with the provided labels.\n    '''\n    imgs = [plt.imread(f'{INPUT_DIR}/train/{filename}') for filename in filenames]\n    \n    return plot_images(imgs, labels, rows)\n    \n        \ndef plot_images(imgs, labels, rows=4):\n    '''\n    Plots the provided images with the labels in a grid with the provided number of rows.\n    '''\n    # Set figure to 13 inches x 8 inches\n    figure = plt.figure(figsize=(13, 8))\n\n    cols = len(imgs) // rows + 1\n\n    for i in range(len(imgs)):\n        subplot = figure.add_subplot(rows, cols, i + 1)\n        subplot.axis('Off')\n        if labels:\n            subplot.set_title(labels[i], fontsize=16)\n        plt.imshow(imgs[i], cmap='gray')\n        \ndef is_grey_scale(img_path):\n    \"\"\"\n    This checks whether the image is percievably grayscale = it returns true not only for\n    single channel images, but also for multichannel (RGB) grayscale images.\n    Thanks to https://stackoverflow.com/questions/23660929/how-to-check-whether-a-jpeg-image-is-color-or-gray-scale-using-only-python-stdli\n    \"\"\"\n    im = Image.open(img_path)\n    if len(im.getbands()) == 1:\n        return True\n    im = im.convert('RGB')\n    w,h = im.size\n    for i in range(w):\n        for j in range(h):\n            r,g,b = im.getpixel((i,j))\n            if r != g != b: return False\n    return True\n\ndef random_greyscale(img, p):\n    '''\n    Converts image to grayscale with the given probability p.\n    The returned image has always three channels.\n    '''\n    # check whether image is not grayscale already\n    if len(img.shape) == 2 or img.shape[2] == 1:\n        return np.repeat(img[:, :, np.newaxis], 3, axis=2)\n    \n    # colour image - convert it with the given probability.\n    if random.random() < p:\n        return np.dot(img[...,:3], [0.299, 0.587, 0.114])\n    # otherwise just return original image\n    return img\n\n\ndef random_flip(img, p):\n    '''\n    Randomly flips the image from left-to-right, given the probability p.\n    '''\n    if random.random() < p:\n        return np.flip(img, 1)\n    return img\n\ndef random_blur(img, p, sigma=1.37):\n    '''\n    Randomly blurs the image with gaussian kernel with sigma, given the probability p. \n    '''\n    if random.random() < p:\n        return skimage.filters.gaussian(img / 255.0, sigma=sigma, multichannel=len(img.shape) == 3) * 255\n    return img\n\ndef augmentation_pipeline(img_arr):\n    '''All augumentations together'''\n    img_arr = random_rotation(img_arr, 18, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n    img_arr = random_shear(img_arr, intensity=0.4, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n    img_arr = random_zoom(img_arr, zoom_range=(0.7, 1.4), row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') # we do not want to zoom out, as the net may learn the artifacts caused by the filled areas.\n    img_arr = random_greyscale(img_arr, 0.4)\n    img_arr = random_blur(img_arr, 0.33)\n    img_arr = random_flip(img_arr, 0.5)\n\n    return img_arr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5910c3e4905358eaca94c95928ed8a94dab859fe"},"cell_type":"markdown","source":"## Dataset-specific utility functions"},{"metadata":{"trusted":true,"_uuid":"304529529e12db65d7c90d7488ae849ea1611b02"},"cell_type":"code","source":"IMAGE_SIZE = (160,120) # width, height\nNUM_OF_AUGMENTATIONS = 5\n\ndef makeNewWhalesUnique(trainDataFrame):\n    '''\n    Renames all new_whale whales to new_whale_<Id>, so that every\n    new whale is unique.\n    Note: I am not sure wheter every new whale is unique.\n    '''\n    for i in range(len(trainDataFrame['Id'])):\n        if trainDataFrame['Id'][i] == 'new_whale':\n            trainDataFrame['Id'][i] = ('new_whale_%d' % i)\n    return trainDataFrame\n            \ndef loadImageAndAugument(imgPath, imageSize=IMAGE_SIZE, numOfAugumentations=NUM_OF_AUGMENTATIONS, inputDir=INPUT_DIR):\n    '''Loads image, randomly auguments it and returns list of images suitable for use in Resnet50'''\n    if not imgPath.startswith(inputDir):\n        imgPath = inputDir + '/train/' + imgPath\n    inputImage = Image.open(imgPath).resize(imageSize).convert('RGB')\n    inputImageArray = img_to_array(inputImage)\n    result = [inputImageArray / 255]\n    for i in range(numOfAugumentations):\n        result.append(augmentation_pipeline(inputImageArray) / 255)\n    return result\n\ndef loadBatch(imagePaths, labels, startIdx=0, numOfImages=None):\n    if numOfImages is None:\n        numOfImages = imagePaths.count() - startIdx\n    loadedLabels = []\n    loadedImages = []\n    #for index, imgPath in imagePaths.iteritems()[startIdx:min(startIdx+numOfImages, imagePaths.count())]:\n    for index, imgPath in itertools.islice(imagePaths.iteritems(), startIdx, min(startIdx+numOfImages, imagePaths.count())):\n        print(index, imgPath)\n        imgArrays = loadImageAndAugument(imgPath)\n        for img in imgArrays:\n            loadedLabels.append(labels[index])\n            loadedImages.append(img)\n    return (loadedImages, loadedLabels)\n\n\ndef laodAndPrepareDataset(datasetPath=f'{INPUT_DIR}/train.csv', train_size=0.8, random_state=None):\n    '''\n    Loads the dataset and preprocess it so that it can be used to load batches for training.\n    '''\n    if random_state is None:\n        random_state = random.randrange(2**32 - 1)\n    # load data to Panda DataFrame\n    train_df = pd.read_csv(datasetPath)\n    # Make things unique\n    train_df = makeNewWhalesUnique(train_df)\n    # split to train/validation subsets in the following order:\n    # trainInputImagePaths, validationInputImagePaths, trainInputLabels, validationInputLabels\n    return  train_test_split(train_df['Image'], \n                             train_df['Id'],\n                             train_size=train_size, \n                             test_size=1.0 - train_size,\n                             random_state=random_state)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a500ad3e-5845-4f42-8a90-20a9a78d2119","_uuid":"e4997792aa81648de39276449fb0047fa730b7cb"},"cell_type":"markdown","source":"## Exploring the dataset"},{"metadata":{"_cell_guid":"aebeb9b5-205b-4d74-8e48-326767b9cbed","_uuid":"d16ff05d95bcb532e90609d249fedd5e998001b2","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/whale-categorization-playground/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a78faf22-9a34-4ff2-a86b-5b8aa42a71dd","_uuid":"03ba5502241770a53367e16df870e2278232aa5b"},"cell_type":"markdown","source":"Let's plot a couple of images at random."},{"metadata":{"_cell_guid":"bc189b65-5aa9-4d1a-a8cd-9b314027abc0","_uuid":"eb84095fd97943cb98cb83d468d70636f1e3596e","trusted":true},"cell_type":"code","source":"rand_rows = train_df.sample(frac=1.)[:20]\nimgs = list(rand_rows['Image'])\nlabels = list(rand_rows['Id'])\n\nplot_images_for_filenames(imgs, labels)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdf8c7e3-8f9d-4399-a187-c72add92b8c1","_uuid":"c4a48b849c090d32da291b54028319973331d3bd"},"cell_type":"markdown","source":"The competition states that it's hard because: \"there are only a few examples for each of 3,000+ whale ids\", so let's take a look at the breakdown of number of image per category."},{"metadata":{"_cell_guid":"0de4233b-919e-472d-b203-661030310276","_uuid":"b1bc00295ffb2e4601c950aef9add3bb23d8029d","trusted":true},"cell_type":"code","source":"num_categories = len(train_df['Id'].unique())\n     \nprint(f'Number of categories: {num_categories}')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"02c2a524-5c28-4023-a0aa-8df9482a7a46","_uuid":"cffd2dd1e02a992d6c544fa724008bdeca785b0e"},"cell_type":"markdown","source":"There appear to be too many categories to graph count by category, so let's instead graph the number of categories by the number of images in the category."},{"metadata":{"_cell_guid":"86a8d12b-1e7f-4bae-abf4-302061efdf9a","_uuid":"083753550c9db61d481e750e85faf744289da6a7","trusted":true},"cell_type":"code","source":"size_buckets = Counter(train_df['Id'].value_counts().values)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"36634bb3-ee2f-44ca-b9ee-bed3ce6a3c72","_uuid":"041dd6d3e2398133b7639d2e84b89ce6ec077690","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n\nplt.bar(range(len(size_buckets)), list(size_buckets.values())[::-1], align='center')\nplt.xticks(range(len(size_buckets)), list(size_buckets.keys())[::-1])\nplt.title(\"Num of categories by images in the training set\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a06bd89f-f99f-4640-b7ac-35324bf470dc","_uuid":"7ef86491f073f35d740e96988d87238097329bbe"},"cell_type":"markdown","source":"As we can see, the vast majority of classes only have a single image in them. This is going to make predictions very difficult for most conventional image classification models."},{"metadata":{"_cell_guid":"bbdd85b6-0c80-434b-bf1f-b46c835b5daa","_uuid":"9887c3f282a81dc26f5c2091e7f293a45c923f61","trusted":true},"cell_type":"code","source":"train_df['Id'].value_counts().head(3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b823f46-390f-482b-97cc-6c3cf21a470a","_uuid":"79867633da0e28dcc3083cfdd96e4165e70387d6","trusted":true},"cell_type":"code","source":"total = len(train_df['Id'])\nprint(f'Total images in training set {total}')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2bf7d22-d501-4c2d-b703-d1548e058751","_uuid":"33595e5c0cab6486821b17b5e3467c1c6e0b1136"},"cell_type":"markdown","source":"New whale is the biggest category with 810, followed by `w_1287fbc`. New whale, I believe, is any whale that isn't in scientist's database. Since we can pick 5 potential labels per id, it's probably going to make sense to always include new_whale in our prediction set, since there's always an 8.2% change that's the right one. But, to have training nice, we should rename each of its instance to be unique:"},{"metadata":{"trusted":true,"_uuid":"e0a5ce9d5a31411a808ecea4c02fc7425b06cf8c"},"cell_type":"code","source":"train_df = makeNewWhalesUnique(train_df)\n# check whether the counts are still that bad...\ntrain_df['Id'].value_counts().head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db8663b1e4301985580de46105496fffb601e911"},"cell_type":"markdown","source":"\nLet's take a look at one of the classes, to get a sense what flute looks like from the same whale."},{"metadata":{"_cell_guid":"48750e6f-9318-4e3c-86f6-172847c65df6","_uuid":"537e3c1402758539298b2b940cb770a99e1b3773","trusted":true},"cell_type":"code","source":"w_1287fbc = train_df[train_df['Id'] == 'w_1287fbc']\nplot_images_for_filenames(list(w_1287fbc['Image']), None, rows=9)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed338ffd-25a6-4ab1-b8fb-748bb13f11c3","_uuid":"be40e8b6223994263f584e8707f76aeecb7f9cbf","trusted":true},"cell_type":"code","source":"w_98baff9 = train_df[train_df['Id'] == 'w_98baff9']\nplot_images_for_filenames(list(w_98baff9['Image']), None, rows=9)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39f82e4b-ee88-4b14-b5d5-0ed7c32c5fe3","_uuid":"1083103782ff0b9f484887b121c56f0cfe5642cf"},"cell_type":"markdown","source":"It's very difficult to build a validation set when most classes only have 1 image, so my thinking is to perform some aggressive data augmentation on the classes with < 10 images before creating a train/validation split. Let's take a look at a few examples of whales with only one example."},{"metadata":{"_cell_guid":"fbf4007f-dcdb-4f14-8668-17fca227bd49","_uuid":"4f2a784bfc07b4245540d3dd1454e106595012fd","trusted":true},"cell_type":"code","source":"one_image_ids = train_df['Id'].value_counts().tail(8).keys()\none_image_filenames = []\nlabels = []\nfor i in one_image_ids:\n    one_image_filenames.extend(list(train_df[train_df['Id'] == i]['Image']))\n    labels.append(i)\n    \nplot_images_for_filenames(one_image_filenames, labels, rows=3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0579d91-ec02-4f99-a0bd-8041d9752ad7","_uuid":"b893356f7e397e38970fe4bdf2bcea18f8559690"},"cell_type":"markdown","source":"From these small sample sizes, it seems like > 50% of images are black and white, suggesting that a good initial augementation might be to just convert colour images to greyscale and add to the training set. Let's confirm that by looking at a sample of the images."},{"metadata":{"_cell_guid":"2e21b5d4-d359-44c6-adff-2f6d2f2ef3df","_uuid":"f58e03b107e675bfb710656d3025deee1985fb05","trusted":true},"cell_type":"code","source":"is_grey = [is_grey_scale(f'{INPUT_DIR}/train/{i}') for i in train_df['Image'].sample(frac=0.1)]\ngrey_perc = round(sum([i for i in is_grey]) / len([i for i in is_grey]) * 100, 2)\nprint(f\"% of grey images: {grey_perc}\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4af57953-8c0d-4a12-97e0-0b7962110499","_uuid":"1dbd886cada07fd6a29cb04f04289e7b9077f20b"},"cell_type":"markdown","source":"It might also be worth capturing the size of the images so we can get a sense of what we're dealing with."},{"metadata":{"_cell_guid":"30bf3795-ae23-49e5-94c2-8e1489b47eb6","_uuid":"f81f0dbe8bbe9e16a56dd9dc96235e726beaf5da","trusted":true},"cell_type":"code","source":"img_sizes = Counter([Image.open(f'{INPUT_DIR}/train/{i}').size for i in train_df['Image']])\n\nsize, freq = zip(*Counter({i: v for i, v in img_sizes.items() if v > 1}).most_common(20))\n\nplt.figure(figsize=(10, 6))\n\nplt.bar(range(len(freq)), list(freq), align='center')\nplt.xticks(range(len(size)), list(size), rotation=70)\nplt.title(\"Image size frequencies (where freq > 1)\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0011e408-e67c-4865-9a0b-fa5f8c9fc107","_uuid":"0eacf56cebac3cce1d69e62f7fd4d937946a5322"},"cell_type":"markdown","source":"## Data Augmentation"},{"metadata":{"_cell_guid":"7fcf6d52-184e-4267-91fe-d03fd4c186dd","_uuid":"bbc9c306f4683b8aac774a4d7e6a8054516b63f5","trusted":true},"cell_type":"code","source":"img = Image.open(f'{INPUT_DIR}/train/ff38054f.jpg')\nimg_arr = img_to_array(img)\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46527baf-bf7a-4711-988d-51a9466ebe84","_uuid":"cb4644fffdcd499661bcfffad13a065003b86ce2"},"cell_type":"markdown","source":"### Random rotation"},{"metadata":{"_cell_guid":"fff831e2-c6cc-42bb-8491-42f10212d02f","_uuid":"84f4cfbe5cfa3a91b966b26c4a8e34254a831851","trusted":true},"cell_type":"code","source":"imgs = [\n    random_rotation(img_arr, 30, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') / 255\n    for _ in range(5)]\nplot_images(imgs, None, rows=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c44ea228-8515-49e3-9e95-85aa49c06889","_uuid":"edc31413fb6eb1c7c516d2213f1810c29af0a454"},"cell_type":"markdown","source":"### Random shift"},{"metadata":{"_cell_guid":"64fe5ad9-a875-4ceb-bf09-44a735afbc5c","_uuid":"e3a05219b500ab559d39889f41620063e233868c","trusted":true},"cell_type":"code","source":"imgs = [\n    random_shift(img_arr, wrg=0.1, hrg=0.3, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') / 255\n    for _ in range(5)]\nplot_images(imgs, None, rows=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9cda0bc7-9db8-4362-898b-3d4f5a96fe71","_uuid":"e73cfed7901c96b7edb5bb7210a312bf5f91402f"},"cell_type":"markdown","source":"### Random shear"},{"metadata":{"_cell_guid":"38dcf7db-1759-4f57-8d12-0dff459a69e4","_uuid":"480a6983beca719cd8b86ec152d08df2928d8036","trusted":true},"cell_type":"code","source":"imgs = [\n    random_shear(img_arr, intensity=0.4, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') / 255\n    for _ in range(5)]\nplot_images(imgs, None, rows=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c211075-0a69-407a-badc-e5ae36e31542","_uuid":"9dd3f4f1da2d61488e2d2010f634ebf0836984f9"},"cell_type":"markdown","source":"### Random zoom"},{"metadata":{"_cell_guid":"d01ceafa-9390-4112-bae1-290a3faa3531","_uuid":"9b97aad967b67a245a210d4a40bff0fd1d369682","trusted":true},"cell_type":"code","source":"imgs = [\n    random_zoom(img_arr, zoom_range=(1.5, 0.7), row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest') / 255\n    for _ in range(5)]\nplot_images(imgs, None, rows=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"211543b3-2821-48b1-b0e4-4b4d65ee5969","_uuid":"dcfdc3f6a473f8ff2a885ddedbe9de0a1b8e8c31"},"cell_type":"markdown","source":"### Grey scale\n\nWe want to ensure that all colour images also have a grey scale version."},{"metadata":{"_cell_guid":"92867723-df40-4bb1-97c5-b2e8bcbe537a","_uuid":"df6228eb8000546adf514fc5f6037fc576343fcd","trusted":true},"cell_type":"code","source":"imgs = [random_greyscale(img_arr / 255, 0.5)\n        for _ in range(5)]\n\nplot_images(imgs, None, rows=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35a6d25a4492034ec2bffa61d7b9ca3ad8ea7615"},"cell_type":"markdown","source":"### All together\n\nGoing to create an augmentation pipeline which will combine all the augs for *a* single predictions. We are not giving zoom too huge range to reduce weid lines at the side of the image."},{"metadata":{"_uuid":"b4bfc12475d0b3c94033f8555e377cab0c9f6ae9","trusted":true},"cell_type":"code","source":"imgs = [augmentation_pipeline(img_arr) / 255 for _ in range(20)]\nplot_images(imgs, None, rows=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01c3408e2aeda93b0a60609cafd7ebf075383e65"},"cell_type":"code","source":"imgs = loadImageAndAugument('70238365.jpg')\nplot_images(imgs, None, rows=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f111ffac29afcd68e2200097316779f6f26b7f5"},"cell_type":"markdown","source":"### Prepare the data\n\nPrepare the data for training (and validation) process."},{"metadata":{"trusted":true,"_uuid":"14a1d5be95388748958e70bb866bc98c0409ce08"},"cell_type":"code","source":"trainInputImagePaths, validationInputImagePaths, trainInputLabels, validationInputLabels = laodAndPrepareDataset()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1c816b59110036724b2cf3a2bb64fde91cd0e2e"},"cell_type":"markdown","source":"We have loaded and split the data. Seems ok... Now, lets generate some of  the data, and store it in array."},{"metadata":{"trusted":true,"_uuid":"f3409853dda17ed8433324ce55bb0a0a7629619e"},"cell_type":"code","source":"#example:\ntrainAugumentedImages, trainAugumentedLabels = loadBatch(trainInputImagePaths, trainInputLabels, 0, 4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b5e42a9d4f76851299c03698036c354346b842c"},"cell_type":"markdown","source":"...and see how they look like:"},{"metadata":{"trusted":true,"_uuid":"d7f0d54540333c10bc3ce32383cdfc7b2600944b"},"cell_type":"code","source":"plot_images(trainAugumentedImages, trainAugumentedLabels, rows=4)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}