{"nbformat":4,"nbformat_minor":1,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","name":"python","nbconvert_exporter":"python","version":"3.6.4","mimetype":"text/x-python"}},"cells":[{"source":"from collections import defaultdict\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nfrom sklearn.neighbors import NearestNeighbors\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.layers import Embedding, Flatten, Input, merge\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, GlobalMaxPooling2D\nfrom keras.models import Model\nimport glob\nimport os\nfrom PIL import Image\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom keras import optimizers, losses, activations, models\nfrom keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n    GlobalMaxPool2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda\nfrom keras.applications.resnet50 import ResNet50\nimport pandas as pd\nimport numpy as np\nimport os\nimport glob\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten, Dropout\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom tqdm import tqdm","outputs":[],"metadata":{"_cell_guid":"60c25a40-ba1b-4b41-9e54-704125e07f34","_uuid":"a6f154c5e53a700fbf626e69d73f80ed41266074","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"training_dir = '../input/train/'\ndata = pd.read_csv('../input/train.csv')\ntrain, test = train_test_split(data, test_size=0.3, shuffle=True, random_state=1337)\nprint('Checking training data head')\nprint(train.head())\nprint('Checking test data head')\nprint(test.head())","outputs":[],"metadata":{"_cell_guid":"06d7e527-48ae-4fa2-9cbf-d20c6d38311f","_uuid":"dfc428bdeb89ebf110c549f0661c23c522b21c63","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"sampleImageFile1 = train.Image[2]\nsampleImage = mpimg.imread(training_dir + sampleImageFile1)\nplt.imshow(sampleImage)\nplt.show()","outputs":[],"metadata":{"_cell_guid":"0b6b768b-fd75-4570-8518-2077d2f93b41","_uuid":"6ac67882fe65c307e5b4a1dc792c1fbde6af0186","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"sampleImageFile2 = train.Image[89]\nsampleImage2 = mpimg.imread(training_dir + sampleImageFile2)\nplt.imshow(sampleImage2)","outputs":[],"metadata":{"_cell_guid":"7e6589ac-0289-4173-976f-ffe511227821","_uuid":"2a3c40df923aa2ef1b2799e070aeebbfa0dd5af0","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"## Read training CSV\n\n1. Checking duplicate IDs","metadata":{"_cell_guid":"b311f2dc-4915-45f7-83fe-2228ae0c6682","_uuid":"b9d3331cabf3aadab3f94a1403a198ba15d7fc06","collapsed":true},"cell_type":"markdown"},{"source":"vc = train.Id.value_counts().sort_values(ascending=False)\nvc[:50].plot(kind='bar')\nplt.show()","outputs":[],"metadata":{"_cell_guid":"d3dc1c72-6a67-48f8-a0ae-59f6e64a1ce2","_uuid":"79725c45e2b9b7f7feffe3f6a890f8b2eca60c69","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"# Train model","metadata":{"_cell_guid":"9954cf9c-e7b4-434b-8398-01019ce247ba","_uuid":"82c1b5ab17f54fc633d5801dbbe4dd03cc6cc79b","collapsed":true},"cell_type":"markdown"},{"source":"# PARAMETERS\n# The parameters are not the final parameters and will be changed later.\nk_size = (4,4)\ndrop_probability = 0.5\nhidden_size = 256\nbatch_size = 64\ninput_shape = (batch_size, 128, 128)\npool_size = (2,2)\nlearning_rate = 0.07\nnum_of_epochs = 10\nnum_of_classes = 4251","outputs":[],"metadata":{"_cell_guid":"69bb1219-28ad-45e4-ae9a-336790eac84b","_uuid":"91d6f85da47d3578c08e96cd8df2a9cc89e85ff7","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"# NETWORK\nmodel = Sequential()\nmodel.add(Convolution2D(32, kernel_size=k_size, activation=\"relu\", input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=pool_size, strides=(2,2)))\nmodel.add(Convolution2D(64, kernel_size=k_size, activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=pool_size, strides=(1,1)))\nmodel.add(Convolution2D(512, kernel_size=k_size, activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=pool_size, strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation=\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(512, activation=\"relu\"))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(num_of_classes, activation=\"softmax\"))","outputs":[],"metadata":{"_cell_guid":"51f009f2-b074-4b5f-9b6f-a22ae4f3f142","_uuid":"8a966516d4d07f908812d33a8510101f598c31f8","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"# COST AND OPTIMIZER\nmodel.compile(loss=categorical_crossentropy,\n              optimizer=Adam(lr=0.01),\n              metrics=['accuracy'])","outputs":[],"metadata":{"_cell_guid":"89e1a8f5-a437-4518-894b-926b8cd3fc90","_uuid":"30f3372ee3c92227500a575dd1cfcd2d4d6ef0e2","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"# Data preparation and training","metadata":{"_cell_guid":"7558761d-d6d2-4949-a678-033744c5a93b","_uuid":"8ed37157bbb3c2b20d3c27f44f8521e0fc6d4f9c"},"cell_type":"markdown"},{"source":"def process(image):\n    # resize\n    image = np.resize(image, [128, 128])\n    \n    # convert to grayscale\n    if image.shape == 3:\n        image = np.dot([image[:,:,0],image[:,:,1],image[:,:,2]],[0.299,0.587,0.114])\n    \n    # return normalized\n    return image / 255\n    ","outputs":[],"metadata":{"_cell_guid":"bf15ea61-4c12-42e0-a1c9-a1024383a4e3","_uuid":"79b1591100e55d132902747bd591fc64a26ac4c0","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"# Convert target variable to a one hot coded value","metadata":{"_cell_guid":"a9e64efc-9acd-4108-87a0-db11a6f4c62c","_uuid":"7fa77a96e3c4a4b260871b5557fd8e5cff3279d0"},"cell_type":"markdown"},{"source":"x = []\ny = []","outputs":[],"metadata":{"_cell_guid":"49b6e159-5d4a-427d-b302-bec4a6fd4f8c","_uuid":"7a64a341441ae6a0f3feae6eb7d6ad92ba6e64b3","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"for path in tqdm(train.Image):\n    image = mpimg.imread(training_dir + path)\n    image = process(image)\n    x.append(image)\n    \n    cod = 'Id_' + train[train.Image == path]['Id']\n    y.append(cod)\n","outputs":[],"metadata":{"_cell_guid":"94ecd7f0-687c-42c6-9de7-3f6f6fad623d","_uuid":"9ae377c09d721dd480d5f02dd8e6aeca44453b1b","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"model.fit(np.array(x), np.array(y), batch_size=batch_size, epochs=num_of_epochs, verbose=1)","outputs":[],"metadata":{"_cell_guid":"6b7bba88-1a2c-4269-b19d-6f9389a9d76e","_uuid":"aebac2454bc7a1e90e082b0042d979af2cef6bd9","collapsed":true},"execution_count":null,"cell_type":"code"},{"source":"test_preds = []\ntest_file_names = []\ni = 1\ntest_files = glob.glob(\"../input/test/*.jpg\")\nfor fnames, imgs in data_generator(test_files, batch=32):\n    print(i * 32 / len(test_files) * 100)\n    i += 1\n    predicts = inference_model.predict(imgs)\n    predicts = predicts.tolist()\n    test_preds += predicts\n    test_file_names += fnames\n\ntest_preds = np.array(test_preds)","outputs":[],"metadata":{"_cell_guid":"03b593cd-160f-4813-903c-4baf71470d3f","_uuid":"10f48462e29a0dbfd3eabeee6f3a6b3baa902011","collapsed":true},"execution_count":null,"cell_type":"code"}]}