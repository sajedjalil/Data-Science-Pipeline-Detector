{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile XLMROBERTA_TPU.py\n\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\n#from apex import amp\nimport random\nimport re\nimport json\nfrom transformers import ( \n    BertTokenizer, \n    AdamW, \n    BertModel, \n    BertForPreTraining,\n    BertConfig,\n    get_linear_schedule_with_warmup,\n    BertTokenizerFast,\n    RobertaModel,\n    RobertaTokenizerFast,\n    RobertaConfig,\n    AlbertTokenizer,\n    AlbertConfig,\n    AlbertModel\n)\nimport transformers\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport sentencepiece as spm\n#import sentencepiece_pb2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.path.insert(0, \"../input/sentencepiece-pb2/\")\nimport sentencepiece_pb2\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current values\"\"\"\n    def __init__(self):\n        self.reset()\n    \n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef get_position_accuracy(logits, labels):\n    predictions = np.argmax(F.softmax(logits, dim=1).cpu().data.numpy(), axis=1)\n    labels = labels.cpu().data.numpy()\n    total_num = 0\n    sum_correct = 0\n    for i in range(len(labels)):\n        if labels[i] >= 0:\n            total_num += 1\n            if predictions[i] == labels[i]:\n                sum_correct += 1\n    if total_num == 0:\n        total_num = 1e-7\n    return np.float32(sum_correct) / total_num, total_num\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / ((len(a) + len(b)) - len(c))\n\ndef calculate_jaccard_score(features_dict, start_logits, end_logits, tokenizer):\n\n    input_ids = to_list(features_dict[\"input_ids\"])\n    #start_position = to_list(features_dict[\"start_position\"])\n    #end_position = to_list(features_dict[\"end_position\"])\n    tweet = features_dict[\"tweet\"]\n    selected_text = features_dict[\"selected_text\"]\n    sentiment = features_dict[\"sentiment\"]\n    #offsets = features_dict[\"offsets\"]\n\n    start_logits = np.argmax(F.softmax(start_logits, dim=1).cpu().data.numpy(), axis=1)\n    end_logits = np.argmax(F.softmax(end_logits, dim=1).cpu().data.numpy(), axis=1)\n\n    jac_list = []\n\n    for i in range(len(tweet)):\n\n        idx_start = start_logits[i]\n        idx_end = end_logits[i]\n        #offset = offsets[i]\n        input_id = input_ids[i]\n        tw = tweet[i]\n        target_st = selected_text[i]\n\n        if idx_end < idx_start:\n            idx_end = idx_start\n\n        \"\"\"\n        filtered_output = \"\"\n        for ix in range(idx_start, idx_end):\n            filtered_output += tw[offset[ix][0]: offset[ix][1]]\n            if (ix+1) < len(offset) and offset[ix][1] < offset[ix+1][0]:\n                filtered_output += \" \"\n        \"\"\"\n        filtered_output = tokenizer.decode(input_id[idx_start:idx_end+1], skip_special_tokens=True)\n\n        if sentiment[i] == \"neutral\" or len(tw.split()) < 2:\n            filtered_output = tw\n        \n        \n        jac = jaccard(target_st.strip(), filtered_output.strip())\n\n        jac_list.append(jac)\n\n    return np.mean(jac_list)\n\n########## Datapreparation & Dataset ###########\n\ndef find_start_and_end(tweet, selected_text):\n    len_st = len(selected_text)\n    start = None\n    end = None\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            start = ind\n            end = ind + len_st - 1\n            break\n    return start, end\n\ndef _is_whitespace(c):\n    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n        return True\n    return False\n\ndef whitespace_tokenizer(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    #print(text)\n    if not text:\n         return []\n    tokens = text.split()\n    return tokens\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n     \n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n    return (input_start, input_end)\n\nclass SentencePieceTokenizer:\n    def __init__(self, model_path):\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(os.path.join(model_path))\n    \n    def encode(self, sentence):\n        spt = sentencepiece_pb2.SentencePieceText()\n        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n        offsets = []\n        tokens = []\n        for piece in spt.pieces:\n            tokens.append(piece.id)\n            offsets.append((piece.begin, piece.end))\n        return tokens, offsets\n\ndef find_start_end_offsets(tweet, selected_text, start_position_character, tokenizer):\n    \n    start_position = 0\n    end_position = 0\n    \n    tweet_tokens = []\n    \n    char_to_word_offset = []\n    prev_is_whitespace = True\n    \n    # Split on whitespace so that different tokens may be attributed to their original position.\n    for c in tweet:\n        if _is_whitespace(c):\n            prev_is_whitespace = True\n        else:\n            if prev_is_whitespace:\n                tweet_tokens.append(c)\n            else:\n                tweet_tokens[-1] += c\n            prev_is_whitespace = False\n        char_to_word_offset.append(len(tweet_tokens) - 1)\n    \n    # Start and end positons only has a value during evalution.\n    if start_position_character is not None:\n        start_position = char_to_word_offset[start_position_character]\n        end_position = char_to_word_offset[\n                min(start_position_character + len(selected_text) - 1, len(char_to_word_offset) -1)                                  \n        ]\n        \n    tok_to_orig_index = []\n    orig_to_tok_index = []\n    all_tweet_tokens = []\n    \n    for (i, token) in enumerate(tweet_tokens):\n        orig_to_tok_index.append(len(all_tweet_tokens))\n        sub_tokens = tokenizer.tokenize(token)\n        for sub_token in sub_tokens:\n            tok_to_orig_index.append(i)\n            all_tweet_tokens.append(sub_token)\n    #print(orig_to_tok_index)\n    tok_start_position = orig_to_tok_index[start_position]\n    if end_position < len(tweet_tokens) - 1:\n        tok_end_position = orig_to_tok_index[end_position + 1] - 1\n    else:\n        tok_end_position = len(all_tweet_tokens) - 1\n    \n     \n    \n    (tok_start_position, tok_end_position) = _improve_answer_span(\n            all_tweet_tokens, tok_start_position, tok_end_position, tokenizer, selected_text\n        )\n    \n    return tok_start_position, tok_end_position\n\ndef process_with_offsets(args, tweet, selected_text, sentiment, tokenizer, spt):\n\n    start_index, end_index = find_start_and_end(tweet, selected_text)\n\n    char_targets = [0]*len(tweet)\n    if start_index != None and end_index != None:\n        for ct in range(start_index, end_index+1):\n            char_targets[ct] = 1\n    \n    encoded = tokenizer.encode_plus(\n                    sentiment,\n                    tweet,\n                    max_length=args.max_seq_len,\n                    pad_to_max_length=True,\n                    return_token_type_ids=True,\n                    #return_offsets_mapping=True\n                )\n    \n    len_sentences_pair_tokens = tokenizer.max_len - tokenizer.max_len_sentences_pair + 2\n\n    offsets = spt.encode(tweet)[-1]\n    encoded[\"offset_mapping\"] = [(0,0)]*4 + offsets\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(encoded[\"offset_mapping\"]):\n        if j > 3:#(len_sentences_pair_tokens + 2) - 1 :\n            if sum(char_targets[offset1:offset2]) > 0:\n                target_idx.append(j)\n\n    #sp , ep = find_start_end_offsets(tweet, selected_text, start_index, tokenizer)\n    #print(sp, ep)\n    \n    \n    encoded[\"start_position\"] = target_idx[0]\n    encoded[\"end_position\"] = target_idx[-1]\n    encoded[\"tweet\"] = tweet\n    encoded[\"selected_text\"] = selected_text\n    encoded[\"sentiment\"] = sentiment\n\n    return encoded\n\nclass TweetDataset:\n    def __init__(self, args, tokenizer, spt , df, mode=\"train\", fold=0):\n        \n        self.mode = mode\n\n        if self.mode == \"train\":\n            df = df[~df.kfold.isin([fold])].dropna()\n            self.tweet = df.text.values\n            self.sentiment = df.sentiment.values\n            self.selected_text = df.selected_text.values\n        \n        elif self.mode == \"valid\":\n            df = df[df.kfold.isin([fold])].dropna()\n            self.tweet = df.text.values\n            self.sentiment = df.sentiment.values\n            self.selected_text = df.selected_text.values\n        \n        self.tokenizer = tokenizer\n        self.args = args\n        self.spt = spt\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n\n        tweet = str(self.tweet[item])\n        selected_text = str(self.selected_text[item])\n        sentiment = str(self.sentiment[item])\n        \n        features = process_with_offsets(\n                        args=self.args, \n                        tweet=tweet, \n                        selected_text=selected_text, \n                        sentiment=sentiment, \n                        tokenizer=self.tokenizer,\n                        spt=self.spt\n                    )\n        \n        return {\n            \"input_ids\":torch.tensor(features[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\":torch.tensor(features[\"token_type_ids\"], dtype=torch.long),\n            \"attention_mask\":torch.tensor(features[\"attention_mask\"], dtype=torch.long),\n            #\"offsets\":features[\"offset_mapping\"],\n            \"start_position\":torch.tensor(features[\"start_position\"],dtype=torch.long),\n            \"end_position\":torch.tensor(features[\"end_position\"], dtype=torch.long),\n\n            \"tweet\":features[\"tweet\"],\n            \"selected_text\":features[\"selected_text\"],\n            \"sentiment\":features[\"sentiment\"]\n\n        }\n\nclass BertForQuestionAnswering(BertForPreTraining):\n    \"\"\"\n    BERT model for QA\n\n    Parameters\n    ----------\n    config : transformers.BertConfig. Configuration class for BERT.\n\n    Returns\n    -------\n    start_logits : torch.Tensor with shape (batch_size, sequence_size)\n        Starting scores of each tokens.\n    end_logits : torch.Tenosr with shape (batch_size, sequence_size).\n        Ending scores of each tokens.\n    \"\"\"\n\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = RobertaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            #token_type_ids=token_type_ids,\n                            #position_ids=position_ids,\n                            #head_mask=head_mask\n                        )\n        sequence_output = outputs[0]\n        #pooled_output = outputs[1]\n\n        # predict start & end position\n        qa_logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n\nclass TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, model_path, conf):\n        super(TweetModel, self).__init__(conf)\n        self.xlmroberta = transformers.XLMRobertaModel.from_pretrained(model_path, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2) #768\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n\n\n    def forward(self,input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        _,_, out = self.xlmroberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        #print(out.shape)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n\ndef reduce_fn(vals):\n    return sum(vals) / len(vals)\n\ndef loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n\n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    return start_loss, end_loss\n\ndef train(args, train_loader, model, optimizer,scheduler, epoch, f):\n    total_loss = AverageMeter()\n    losses1 = AverageMeter() # start\n    losses2 = AverageMeter() # end\n    accuracies1 = AverageMeter() # start\n    accuracies2 = AverageMeter() # end\n\n    model.train()\n\n    t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n    for step, d in enumerate(t):\n        \n        input_ids = d[\"input_ids\"].to(args.device)\n        attention_mask = d[\"attention_mask\"].to(args.device)\n        token_type_ids = d[\"token_type_ids\"].to(args.device)\n        start_position = d[\"start_position\"].to(args.device)\n        end_position = d[\"end_position\"].to(args.device)\n\n        model.zero_grad()\n\n        logits1, logits2 = model(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids, \n            position_ids=None, \n            head_mask=None\n        )\n\n        y_true = (start_position, end_position)\n        loss1, loss2 = loss_fn((logits1, logits2), (start_position, end_position))\n        loss = loss1 + loss2\n\n        acc1, n_position1 = get_position_accuracy(logits1, start_position)\n        acc2, n_position2 = get_position_accuracy(logits2, end_position)\n\n        total_loss.update(loss.item(), n_position1)\n        losses1.update(loss1.item(), n_position1)\n        losses2.update(loss2.item(), n_position2)\n        accuracies1.update(acc1, n_position1)\n        accuracies2.update(acc2, n_position2)\n\n        \n        #optimizer.zero_grad()\n        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n        #    scaled_loss.backward()\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        scheduler.step()\n        print_loss = xm.mesh_reduce(\"loss_reduce\", total_loss.avg, reduce_fn)\n        print_acc1 = xm.mesh_reduce(\"acc1_reduce\", accuracies1.avg, reduce_fn)\n        print_acc2 = xm.mesh_reduce(\"acc2_reduce\", accuracies2.avg, reduce_fn)\n        t.set_description(f\"Train E:{epoch+1} - Loss:{print_loss:0.2f} - acc1:{print_acc1:0.2f} - acc2:{print_acc2:0.2f}\")\n\n\n    log_ = f\"Epoch : {epoch+1} - train_loss : {total_loss.avg} - \\n \\\n    train_loss1 : {losses1.avg} - train_loss2 : {losses2.avg} - \\n \\\n    train_acc1 : {accuracies1.avg} - train_acc2 : {accuracies2.avg}\"\n\n    f.write(log_ + \"\\n\\n\")\n    f.flush()\n    \n    return total_loss.avg\n\ndef valid(args, valid_loader, model, tokenizer, epoch, f):\n    total_loss = AverageMeter()\n    losses1 = AverageMeter() # start\n    losses2 = AverageMeter() # end\n    accuracies1 = AverageMeter() # start\n    accuracies2 = AverageMeter() # end\n\n    jaccard_scores = AverageMeter()\n\n    model.eval()\n\n    with torch.no_grad():\n        t = tqdm(valid_loader, disable=not xm.is_master_ordinal())\n        for step, d in enumerate(t):\n            \n            input_ids = d[\"input_ids\"].to(args.device)\n            attention_mask = d[\"attention_mask\"].to(args.device)\n            token_type_ids = d[\"token_type_ids\"].to(args.device)\n            start_position = d[\"start_position\"].to(args.device)\n            end_position = d[\"end_position\"].to(args.device)\n\n            logits1, logits2 = model(\n                input_ids=input_ids, \n                attention_mask=attention_mask, \n                token_type_ids=None, \n                position_ids=None, \n                head_mask=None\n            )\n\n            y_true = (start_position, end_position)\n            loss1, loss2 = loss_fn((logits1, logits2), (start_position, end_position))\n            loss = loss1 + loss2\n\n            acc1, n_position1 = get_position_accuracy(logits1, start_position)\n            acc2, n_position2 = get_position_accuracy(logits2, end_position)\n\n            total_loss.update(loss.item(), n_position1)\n            losses1.update(loss1.item(), n_position1)\n            losses2.update(loss2.item(), n_position2)\n            accuracies1.update(acc1, n_position1)\n            accuracies2.update(acc2, n_position2)\n\n            jac_score = calculate_jaccard_score(features_dict=d, start_logits=logits1, end_logits=logits2, tokenizer=tokenizer)\n\n            jaccard_scores.update(jac_score)\n\n            print_loss = xm.mesh_reduce(\"vloss_reduce\", total_loss.avg, reduce_fn)\n            print_jac = xm.mesh_reduce(\"jac_reduce\", jaccard_scores.avg, reduce_fn)\n\n            t.set_description(f\"Eval E:{epoch+1} - Loss:{print_loss:0.2f} - Jac:{print_jac:0.2f}\")\n\n    #print(\"Valid Jaccard Score : \", jaccard_scores.avg)\n    log_ = f\"Epoch : {epoch+1} - valid_loss : {total_loss.avg} - \\n\\\n    valid_loss1 : {losses1.avg} - \\valid_loss2 : {losses2.avg} - \\n\\\n    valid_acc1 : {accuracies1.avg} - \\valid_acc2 : {accuracies2.avg} \"\n\n    f.write(log_ + \"\\n\\n\")\n    f.flush()\n    \n    return jaccard_scores.avg\n\n\ndef main():\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n    parser.add_argument(\"--max_seq_len\", type=int, default=192)\n    parser.add_argument(\"--fold_index\", type=int, default=0)\n    parser.add_argument(\"--learning_rate\", type=float, default=0.00002)\n    parser.add_argument(\"--epochs\", type=int, default=5)\n    parser.add_argument(\"--batch_size\", type=int, default=16)\n    parser.add_argument(\"--model_path\", type=str, default=\"roberta-base\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"\")\n    parser.add_argument(\"--exp_name\", type=str, default=\"\")\n    parser.add_argument(\"--spt_path\", type=str, default=\"\")\n\n    args = parser.parse_args()\n\n    # Setting seed\n    seed = 42\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n     # hperparameters\n    #args.max_seq_len = 192\n    #args.learning_rate = 0.00002\n    #args.batch_size = 16\n    #args.epochs = 5\n    #args.fold_index = 0\n\n    #model_path = \"../huggingface_pretrained/bert-base-uncased/\"\n    model_path = args.model_path\n    config = transformers.XLMRobertaConfig.from_pretrained(model_path)\n    config.output_hidden_states = True\n    tokenizer = transformers.XLMRobertaTokenizer.from_pretrained(model_path, do_lower_case=True)\n    #model = BertForQuestionAnswering.from_pretrained(model_path, config=config)\n    MX = TweetModel(model_path, config)\n\n    spt = SentencePieceTokenizer(args.spt_path)\n\n    train_df = pd.read_csv(\"../input/tweet-create-folds/train_3folds.csv\")\n    #train_df = train_df[train_df.sentiment != \"neutral\"]\n    print(len(train_df))\n\n    args.save_path = os.path.join(args.output_dir, args.exp_name)\n\n    if not os.path.exists(args.save_path):\n        os.makedirs(args.save_path)\n\n    f = open(os.path.join(args.save_path, f\"log_f_{args.fold_index}.txt\"), \"w\")\n\n    def run():\n\n        args.device = xm.xla_device()\n        model = MX.to(args.device)\n\n\n        # DataLoaders\n        train_dataset = TweetDataset(\n            args=args,\n            df=train_df,\n            mode=\"train\",\n            fold=args.fold_index,\n            tokenizer=tokenizer,\n            spt=spt\n        )\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=args.batch_size,\n            sampler=train_sampler,\n            drop_last=True,\n            num_workers=2\n        )\n\n\n        valid_dataset = TweetDataset(\n            args=args,\n            df=train_df,\n            mode=\"valid\",\n            fold=args.fold_index,\n            tokenizer=tokenizer,\n            spt=spt\n        )\n        valid_sampler = torch.utils.data.distributed.DistributedSampler(\n            valid_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=False\n        )\n        valid_loader = DataLoader(\n            valid_dataset,\n            batch_size=args.batch_size,\n            sampler=valid_sampler,\n            num_workers=1,\n            drop_last=False\n        )\n\n        #optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n        #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n\n        num_train_steps = int(len(train_df) / args.batch_size * args.epochs)\n    \n        param_optimizer = list(model.named_parameters())\n        no_decay = [\n            \"bias\",\n            \"LayerNorm.bias\",\n            \"LayerNorm.weight\"\n        ]\n        optimizer_parameters = [\n            {\n                'params': [\n                    p for n, p in param_optimizer if not any(\n                        nd in n for nd in no_decay\n                    )\n                ], \n            'weight_decay': 0.001\n            },\n            {\n                'params': [\n                    p for n, p in param_optimizer if any(\n                        nd in n for nd in no_decay\n                    )\n                ], \n                'weight_decay': 0.0\n            },\n        ]\n\n        num_train_steps = int(\n            len(train_df) / args.batch_size / xm.xrt_world_size() * args.epochs\n        )\n\n        optimizer = AdamW(\n            optimizer_parameters,\n            lr=args.learning_rate * xm.xrt_world_size()\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_train_steps\n        )\n\n        xm.master_print(\"Training is Starting.....\")\n        best_jac = -1000\n\n        for epoch in range(args.epochs):\n            para_loader = pl.ParallelLoader(train_loader, [args.device])\n            train_loss = train(\n                args, \n                para_loader.per_device_loader(args.device),\n                model, \n                optimizer,\n                scheduler,\n                epoch,\n                f\n            )\n            para_loader = pl.ParallelLoader(valid_loader, [args.device])\n            valid_jac = valid(\n                args, \n                para_loader.per_device_loader(args.device),\n                model, \n                tokenizer,\n                epoch,\n                f\n            )\n\n            jac = xm.mesh_reduce(\"jac_reduce\", valid_jac, reduce_fn)\n            xm.master_print(f\"**** Epoch {epoch+1} **==>** Jaccard = {jac}\")\n\n            log_ = f\"**** Epoch {epoch+1} **==>** Jaccard = {jac}\"\n\n            f.write(log_ + \"\\n\\n\")\n\n            \n\n            if jac > best_jac:\n                xm.master_print(\"**** Model Improved !!!! Saving Model\")\n                xm.save(model.state_dict(), os.path.join(args.save_path, f\"fold_{args.fold_index}\"))\n                best_jac = jac\n\n\n    \n    def _mp_fn(rank, flags):\n        torch.set_default_tensor_type('torch.FloatTensor')\n        a = run()\n    \n    FLAGS={}\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')\n\n\nif __name__ == \"__main__\":\n    main()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://s3.amazonaws.com/models.huggingface.co/bert/xlm-roberta-base-sentencepiece.bpe.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python XLMROBERTA_TPU.py --fold_index=0 \\\n                  --model_path=\"xlm-roberta-base\" \\\n                  --spt_path=\"xlm-roberta-base-sentencepiece.bpe.model\" \\\n                  --output_dir=\"xlm-roberta-base\" \\\n                  --exp_name=\"xlm-roberta-base\" \\\n                  --batch_size=64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python XLMROBERTA_TPU.py --fold_index=1 \\\n                  --model_path=\"xlm-roberta-base\" \\\n                  --spt_path=\"albert-xlarge-v2-spiece.model\" \\\n                  --output_dir=\"xlm-roberta-base\" \\\n                  --exp_name=\"xlm-roberta-base\" \\\n                  --batch_size=64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python XLMROBERTA_TPU.py --fold_index=2 \\\n                  --model_path=\"xlm-roberta-base\" \\\n                  --spt_path=\"albert-xlarge-v2-spiece.model\" \\\n                  --output_dir=\"xlm-roberta-base\" \\\n                  --exp_name=\"xlm-roberta-base\" \\\n                  --batch_size=64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}