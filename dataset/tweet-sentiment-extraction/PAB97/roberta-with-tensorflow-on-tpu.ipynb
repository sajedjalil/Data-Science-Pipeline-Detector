{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Running a Keras kernel on TPU for fast training\n\nHello everyone, I have been wondering for days how to speed up the training process with Keras. I have been using the Tensorflow Dataset API for a while and have been struggling it to pass multiple inputs and outputs.\n\nI finally come up with the solution that I share with you. \n\nPlease let's note that most of this notebook is Kiram's work: https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712\n\nI only added the TPU part ;) \n\n** If you like this notebook, feel free to upvote it ;) **"},{"metadata":{},"cell_type":"markdown","source":"Note that for the inference part, you need to do it on a GPU since per competition rules, we must not enable Internet on a notebook.\n\n**v3: I noticed that increasing the batch size from 8 to 16 was lowering the performance. Hence I lowered batch size back down at 8.**"},{"metadata":{},"cell_type":"markdown","source":"# Load  data and libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom transformers import *\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text'] = train['text'].astype(str)\n    train['selected_text'] = train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text'] = test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preproccesing"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\nMAX_LEN = 96\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nLEARNING_RATE = 3e-5 * strategy.num_replicas_in_sync \nEPOCHS = 5\n\nPATH = '../input/tf-roberta/'\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file = PATH + 'vocab-roberta-base.json', \n    merges_file = PATH + 'merges-roberta-base.txt', \n    lowercase = True,\n    add_prefix_space=True\n)\n\nsentiment_id = {'positive': 1313, \n                'negative': 2430, \n                'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train_df.shape[0]\n\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\n\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train_df.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+ \" \".join(train_df.loc[k,'text'].split())\n    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)] = 1\n    \n    if text1[idx-1] == ' ': \n        chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm > 0: \n            toks.append(i) \n        \n    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids) + 5] = 1\n    \n    if len(toks) > 0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_dataset(idxT, idxV):\n    \n    # Trainset\n    trn_input_ids = input_ids[idxT,]\n    trn_att_mask = attention_mask[idxT,]\n    trn_token_type_ids = token_type_ids[idxT,]\n    \n    trn_start_tokens = start_tokens[idxT,]\n    trn_end_tokens = end_tokens[idxT,]\n    \n    # Validation set\n    val_input_ids = input_ids[idxV,]\n    val_att_mask = attention_mask[idxV,]\n    val_token_type_ids = token_type_ids[idxV,]\n    \n    val_start_tokens = start_tokens[idxV,]\n    val_end_tokens = end_tokens[idxV,]\n    \n    # Generating tf.data object\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices(({'input_ids':trn_input_ids, 'attention_mask': trn_att_mask, 'token_type_ids': trn_token_type_ids}, \n                             {'start_tokens': trn_start_tokens, 'end_tokens': trn_end_tokens}))\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices(({'input_ids':val_input_ids, 'attention_mask': val_att_mask, 'token_type_ids': val_token_type_ids}, \n                             {'start_tokens': val_start_tokens, 'end_tokens': val_end_tokens}))\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n    \n    return trn_input_ids.shape[0]//BATCH_SIZE, train_dataset, valid_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return LEARNING_RATE * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='input_ids')\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='attention_mask')\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32, name='token_type_ids')\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5', config=config)\n    x = bert_model(ids,\n                   attention_mask=att,\n                   token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2, padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax', name='start_tokens')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax', name='end_tokens')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train\nWe will skip this stage and load already trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"jac = []\nVER='v4'\nDISPLAY=1 # USE display=1 FOR INTERACTIVE\n\noof_start = np.zeros((input_ids.shape[0], MAX_LEN))\noof_end = np.zeros((input_ids.shape[0], MAX_LEN))\n\nskf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n\nfor fold, (idxT, idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    # Cleaning everything\n    K.clear_session()\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # Building model\n    with strategy.scope():\n        model = build_model()\n    \n    n_steps, trn_dataset, val_dataset = generate_dataset(idxT, idxV)\n        \n    reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    hist = model.fit(trn_dataset, \n                     epochs=EPOCHS, \n                     verbose=DISPLAY, \n                     callbacks=[sv, reduce_lr],\n                     validation_data=val_dataset)\n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict(val_dataset, verbose=DISPLAY)\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train_df.loc[k,'text']\n        else:\n            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train_df.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nDISPLAY=1\nfor i in range(5):\n    print('#'*25)\n    print('### MODEL %i'%(i+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    model.load_weights('../input/model4/v4-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nall = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_df.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df['selected_text'] = all\n#test_df[['textID','selected_text']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}