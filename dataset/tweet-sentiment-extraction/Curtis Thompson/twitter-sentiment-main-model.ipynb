{"cells":[{"metadata":{},"cell_type":"markdown","source":"This was my working notebook for the [Twitter Sentiment Extraction competition](https://www.kaggle.com/c/tweet-sentiment-extraction/), my first major competition on Kaggle. The aim of the competition was to create a model that could extract the part of a tweet that highlighted a particular sentiment (positive, negative, neutral), given the tweet and its sentiment.\n\nI used this notebook throughout the competition to try out different techniques and methods which would help to improve my model. The techniques that I tried in this notebook include:\n - **Pseudo labelling**, *which involves predicting on another dataset, and then using those predictions as further training data in the model*\n - **Post-processing**, *which involves applying rules afterwards to modify predicted text*\n - **Getting best logits**, *an apparently useful function for start-end token predicting mentioned in the [discussion pages](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/147115) by [Kerem Turgutlu](https://www.kaggle.com/keremt)*\n - **Predicting the training dataset**, *which allows us to see exactly how the model performs by comparing actual selected text to predicted dataset, exported to another notebook for further analysis*\n - **URL substitution**, *a pre-processing technique that replaces URLs in text with a common word*\n - **Data augmentation by word synonyms**, *augmenting more data by swapping words with their synonyms*\n - **Adding extra sentiment word tokens**, *add extra tokens to the beginning of each data point (similar to how sentiment is prepended) to denote whether positive or negative words can be detected by NLTK's VADER*\n - **Jaccard expectation maximisation**, *an alternative way of selecting to predicted text, as proposed by [Dmitrij Kozachuk](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/158613)*\n \nMany of these techniques did not appear helpful in improving the cross-validation and public scores of my model, but I did find some success with synonym augmentation and adding extra sentiment word tokens. I finished 558th out of 2,227 teams with a private leaderboard score of 0.71572. My best submitted model from this notebook actually scored 0.71770 which would have earned me a silver medal but I ultimately did not choose that submission for the final evaluation due to the low cross-validation and public leaderboard score.\n\nOverall, this competition was useful to me as it helped me to consolidate my Keras skills while also learning about some machine learning and natural language processing techniques such as transformers (e.g roBERTa), pseudo labelling, and synonym augmentation. I would say that I am happy with my overall performance in the competition. Hopefully I can use this new knowledge to help me earn a medal in my next competition.\n\nThis notebook was originally forked from the [Outlier Analysis Notebook](https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis) by [Vitalii Mokin](https://www.kaggle.com/vbmokin). My work takes heavy inspiration from [Kiram Al-Kharba's kernel](https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712), which is itself based upon [Chris Deotte's starter kernel](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705). Chris has also given some [explanation on roBERTa here](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281), although a lot of this can also be found in his kernel.\n\n# Table of Contents\n1. [Notebook Set-Up](#Notebook-Set-Up)\n2. [Data Import and Augmentation](#Data-Import-and-Augmentation)\n3. [Model Creation](#Model-Creation)\n4. [Data Preparation (Pre-Processing)](#Data-Preparation-(Pre-Processing))\n5. [Model Training](#Model-Training)\n6. [Submission](#Submission)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Notebook Set-Up and Data Import","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import *\nimport tokenizers\nfrom sklearn.model_selection import StratifiedKFold\n\npd.set_option('max_colwidth', 40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following variables can be set to modify the performance of the notebook.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"IS_PSEUDO_LABELLING = False\nIS_USING_PSEUDO_LABELS = False # currently cannot be used with IS_ADDING_SENTIMENT_WORDS\nIS_SUBSTITUTING_URLS = False\nIS_GETTING_BEST_LOGITS = False\nIS_PREDICTING_TRAIN = False\nIS_POSTPROCESSING = False\nIS_AUGMENT_SYNONYM = False\nIS_ADDING_SENTIMENT_WORDS = True # currently cannot be used with IS_USING_PSEUDO_LABELS\nIS_INCLUDING_EXTRA_DATA = False\nIS_EXPECTATION_MAXIMISE = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following parameters are also required for the model creation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 192\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nPAD_ID = 1\nSEED = 88888\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Import and Augmentation\nWe can now import the training dataset for the competition.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_INCLUDING_EXTRA_DATA:\n    extra_data = pd.read_csv('../input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv').fillna('')\n    extra_data = extra_data[(~extra_data.aux_id.isin(train['textID'].values)) & (extra_data.selected_text != '')].reset_index(drop=True)\n    extra_data.textID = extra_data.aux_id\n    extra_data.sentiment = extra_data.new_sentiment\n    extra_data = extra_data[['textID', 'text', 'selected_text', 'sentiment']]\n    train = pd.concat([train, extra_data]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have the option to perform data augmentation by replacing words with their synonyms in the cell below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_AUGMENT_SYNONYM:\n    import json\n    import random\n    random.seed(SEED)\n    with open('../input/englishengen-synonyms-json-thesaurus/eng_synonyms.json') as json_file:  \n        synonyms_dict = json.load(json_file)\n\n    def get_synonym_word(text, first_selected, last_selected):\n        attempts = 0\n        old_word = ''\n        new_word = ''\n        possible_words = text.split()\n        while len(possible_words) > 0 and attempts < 5:\n            word_choice = random.choice(possible_words)\n            if word_choice in synonyms_dict and len(synonyms_dict[word_choice]) > 0:\n                if not ((word_choice in first_selected and len(word_choice) != len(first_selected)) and (word_choice in last_selected and len(word_choice) != len(last_selected))):\n                    old_word = word_choice\n                    new_word = random.choice(synonyms_dict[old_word])\n                    break\n            attempts += 1\n        return old_word, new_word\n\n    def get_synonym_row(row, changes):\n        text = row['text']\n        selected_text = row['selected_text']\n        sentiment = row['sentiment']\n        point_id = row['textID'] + 'aug' + str(changes)\n\n        if len(text) > 0:\n            for i in range(changes):\n                # Get a synonym\n                word_to_replace, replacement_word = get_synonym_word(text, selected_text.split()[0], selected_text.split()[-1])\n                # Make the replacement\n                if word_to_replace in selected_text.split():\n                    old_selected_text = selected_text\n                    selected_text = selected_text.replace(word_to_replace, replacement_word)\n                    text = text.replace(old_selected_text, selected_text, 1)\n                else:\n                    text = text.replace(word_to_replace, replacement_word)\n\n        return text, selected_text, sentiment, point_id\n\n    aug_rows = [train]\n    for changes in [1]:\n        new_rows = []\n        for index, row in train.iterrows():\n            text, selected_text, sentiment, point_id = get_synonym_row(row, changes)\n            new_rows.append([point_id, text, selected_text, sentiment])\n        aug_train = pd.DataFrame(new_rows, columns=['textID', 'text', 'selected_text', 'sentiment'])\n        aug_rows.append(aug_train)\n\n    train = pd.concat(aug_rows).sort_index(kind='merge').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are also going to include a pseudo-labelled dataset. This dataset has been created by myself, but the principle is similar to this [pseudo-labelled dataset](https://www.kaggle.com/thanatoz/tweetsentiment-pseudo-labelled). The creation and use of this additional dataset is described [here](https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/156556).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_USING_PSEUDO_LABELS:\n    pseudo_labelled_set = pd.read_csv('../input/tweetsentimentextraction2020completepseudo/extra_data.csv').fillna('')[['textID', 'text', 'selected_text', 'sentiment']]\n    pseudo_labelled_folds = []\n    for k in range(5):\n        pseudo_labelled_folds.append(pseudo_labelled_set.iloc[k::5].reset_index(drop=True))\n    pseudo_labelled_folds[4].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also need to import the test data. The test data we import depends on whether or not we are psuedo labelling. If we are, we should import the dataset we are pseudo labelling, otherwise we will import the test data for the competition.","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"if IS_PSEUDO_LABELLING:\n    test = pd.read_csv('../input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv').fillna('')\n    test = test[~test.aux_id.isin(train['textID'].values)].reset_index(drop=True)\n    test['textID'] = test['aux_id']\n    sent_con = {'empty' : 'neutral', 'sadness' : 'negative', 'worry' : 'negative', 'neutral' : 'neutral', 'fun' : 'positive', 'happiness' : 'positive', 'hate' : 'negative', 'surprise' : 'neutral', 'relief' : 'positive', 'enthusiasm' : 'neutral', 'anger' : 'negative', 'boredom' : 'negative', 'love' : 'positive'}\n    test['sentiment'] = test.apply(lambda x: x.new_sentiment if len(x.new_sentiment) > 0 else sent_con[x.sentiment], axis=1)\n    test[['textID', 'text', 'sentiment']]\nelse:\n    test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DROPOUT = 0.1 # 0.1\nN_SPLIT = 5 # 5\nLEARNING_RATE = 3e-5 # 3e-5\nLEAKY_RELU_ALPHA = 0.1 # 0.3\nLABEL_SMOOTHING = 0.1 # 0\nEPOCHS = 3 # 3\nBATCH_SIZE = 32 # 32","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    # The first output (for the start token)\n    x1 = tf.keras.layers.Dropout(DROPOUT)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x1)\n    x1 = tf.keras.layers.Conv1D(128, 2, padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x1)\n    x1 = tf.keras.layers.Dense(32)(x1)\n    x1 = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    # The second output (for the end token)\n    x2 = tf.keras.layers.Dropout(DROPOUT)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x2)\n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x2)\n    x2 = tf.keras.layers.Dense(32)(x2)\n    x2 = tf.keras.layers.LeakyReLU(alpha=LEAKY_RELU_ALPHA)(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    # Create model\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_POSTPROCESSING:\n    def post_processing(text):\n        if len(text.split()) > 0 and 'http' in text.split()[-1]:\n            return  ' '.join(text.split()[:-1])\n        else:\n            return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The metric used for the competition is Jaccard. The following cell calculates the Jaccard index metric for two strings. [More information on the Jaccard index can be found here](https://en.wikipedia.org/wiki/Jaccard_index), it is essentially the intersection over union.","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation (Pre-Processing)\nFirstly, we are going to perform some preprocessing. We are going to replace URLs with a common word (website0, website1, etc.). The purpose of this is firstly to ensure that the model doesn't cut off part of the URL, and secondly so that the model treats each URL more equally. The deprocess method is used later on when we need to retrieve the mappings of URL to common word.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_SUBSTITUTING_URLS:\n    import re\n\n    def preprocess_urls(replacements, full_text, sub_text, k):\n        text_split = full_text.split()\n        urls_done = 0\n        for word in text_split:\n            urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', word)\n            if len(urls) > 0:\n                replacements.append((k, urls[0], 'website' + str(urls_done)))\n                full_text = full_text.replace(urls[0], 'website' + str(urls_done), 1)\n                sub_text = sub_text.replace(urls[0], 'website' + str(urls_done), 1)\n                urls_done += 1\n        return replacements, full_text, sub_text\n\n    def deprocess_urls(replacements, full_text, sub_text, k):\n        replaces = [x for x in replacements if x[0] == k]\n        for replacer in replaces:\n            full_text = full_text.replace(replacer[2], replacer[1], 1)\n            sub_text = sub_text.replace(replacer[2], replacer[1], 1)\n        return full_text, sub_text\n\n    replacements, test, sub_test = preprocess_urls(replacements, test, sub_test, k)\n\n    test, sub_test = deprocess_urls(replacements, test, sub_test, k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_SUBSTITUTING_URLS:\n    train_web_replacements = []\n    for k in range(train.shape[0]):\n        text = train.loc[k, 'text']\n        sub_text = train.loc[k, 'selected_text']\n        train_web_replacements, train.loc[k, 'text'], train.loc[k, 'selected_text'] = preprocess_urls(train_web_replacements, text, sub_text, k)\n    \n    if IS_USING_PSEUDO_LABELS:\n        pseudo_web_replacements = []\n        for fold in range(5):\n            pseudo_replacements = []\n            for k in range(train.shape[0]):\n                text = pseudo_labelled_folds[fold].loc[k, 'text']\n                sub_text = pseudo_labelled_folds[fold].loc[k, 'selected_text']\n                pseudo_replacements, pseudo_labelled_folds[fold].loc[k, 'text'], pseudo_labelled_folds[fold].loc[k, 'selected_text'] = preprocess_urls(pseudo_replacements, text, sub_text, k)\n            pseudo_web_replacements.append(pseudo_replacements)\n\n    test_web_replacements = []\n    for k in range(test.shape[0]):\n        text = test.loc[k, 'text']\n        sub_text = ''\n        test_web_replacements, test.loc[k, 'text'], throwaway = preprocess_urls(train_web_replacements, text, sub_text, k)\n        del(throwaway)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use [NLTK's VADER](https://www.nltk.org/howto/sentiment.html) to detect the positive and negative words in the tweets. We can then add extra tokens to the tweets to specify whether there are positive or negative words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_ADDING_SENTIMENT_WORDS:\n    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n    polarity_threshold = 0.3\n    pos_exist_token = 8000\n    npos_exist_token = 8001\n    neg_exist_token = 9000\n    nneg_exist_token = 9001\n\n    sid = SentimentIntensityAnalyzer()\n\n    # Find the highly positive and negative words in the dataset\n    def get_high_polarity_words(data):\n        high_polarity_words = []\n        for index, row in data.iterrows():\n            row_polarising = []\n            for word in row['text'].split():\n                if (sid.polarity_scores(word)['compound'] >= polarity_threshold) or (sid.polarity_scores(word)['compound'] <= -polarity_threshold):\n                    row_polarising += tokenizer.encode(word).ids\n                    #row_polarising.append(word)\n            high_polarity_words.append(row_polarising)\n        return np.array(high_polarity_words)\n\n    # Create tokens to specify whether highly positive and negative words exist\n    def get_high_polarity_tokens(data):\n        high_polarity_tokens = []\n        for index, row in data.iterrows():\n            pos_words = [word for word in row['text'].split() if sid.polarity_scores(word)['compound'] >= polarity_threshold]\n            neg_words = [word for word in row['text'].split() if sid.polarity_scores(word)['compound'] <= -polarity_threshold]\n            high_polarity_tokens.append([pos_exist_token if len(pos_words) > 0 else npos_exist_token,\n                                         neg_exist_token if len(neg_words) > 0 else nneg_exist_token])\n        return np.array(high_polarity_tokens)\n\n    train_sentiment_words = get_high_polarity_tokens(train)\n    test_sentiment_words = get_high_polarity_tokens(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data needs to be tokenised into arrays so that it can be understood by roBERTa.[](http://)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    if IS_ADDING_SENTIMENT_WORDS:\n        input_ids[k,:len(enc.ids)+len(train_sentiment_words[k])+3] = [0, s_tok] + list(train_sentiment_words[k]) + enc.ids + [2]\n        attention_mask[k,:len(enc.ids)+len(train_sentiment_words[k])+3] = 1\n    else:\n        input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n        attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will perform similar tokenisation on the pseudo-labelled data. The only difference is that the data for each fold is done separately.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_USING_PSEUDO_LABELS:\n    pseudo_labelled_folds_tokens = []\n    for fold in range(5):\n        pseudo_labels = pseudo_labelled_folds[fold]\n        ct = pseudo_labels.shape[0]\n        input_ids_ps = np.ones((ct,MAX_LEN),dtype='int32')\n        attention_mask_ps = np.zeros((ct,MAX_LEN),dtype='int32')\n        token_type_ids_ps = np.zeros((ct,MAX_LEN),dtype='int32')\n        start_tokens_ps = np.zeros((ct,MAX_LEN),dtype='int32')\n        end_tokens_ps = np.zeros((ct,MAX_LEN),dtype='int32')\n\n        for k in range(pseudo_labels.shape[0]):\n\n            # FIND OVERLAP\n            text1 = \" \"+\" \".join(pseudo_labels.loc[k,'text'].split())\n            text2 = \" \".join(pseudo_labels.loc[k,'selected_text'].split())\n            idx = text1.find(text2)\n            chars = np.zeros((len(text1)))\n            chars[idx:idx+len(text2)]=1\n            if text1[idx-1]==' ': chars[idx-1] = 1 \n            enc = tokenizer.encode(text1) \n\n            # ID_OFFSETS\n            offsets = []; idx=0\n            for t in enc.ids:\n                w = tokenizer.decode([t])\n                offsets.append((idx,idx+len(w)))\n                idx += len(w)\n\n            # START END TOKENS\n            toks = []\n            for i,(a,b) in enumerate(offsets):\n                sm = np.sum(chars[a:b])\n                if sm>0: toks.append(i) \n\n            s_tok = sentiment_id[pseudo_labels.loc[k,'sentiment']]\n            input_ids_ps[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n            attention_mask_ps[k,:len(enc.ids)+3] = 1\n            if len(toks)>0:\n                start_tokens_ps[k,toks[0]+2] = 1\n                end_tokens_ps[k,toks[-1]+2] = 1\n\n            pseudo_labelled_folds_tokens.append((input_ids_ps, attention_mask_ps, token_type_ids_ps, start_tokens_ps, end_tokens_ps))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will tokenise the test data, but we do not have the answers so cannot store the start and end tokens too (this is what we will be predicting).","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"ct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    if IS_ADDING_SENTIMENT_WORDS:\n        input_ids_t[k,:len(enc.ids)+len(test_sentiment_words[k])+3] = [0, s_tok] + list(test_sentiment_words[k]) + enc.ids + [2]\n        attention_mask_t[k,:len(enc.ids)+len(test_sentiment_words[k])+3] = 1\n    else:\n        input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n        attention_mask_t[k,:len(enc.ids)+3] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training\nHere we will perform cross validation. Each fold will create a separate model, and we will also use these models for the final predictions. Therefore it also makes sense to make the test predictions on each fold.\n\nIf we are using the jaccard expectation maximisation method to modify the final predictions then we will also have to create a dataframe to store each fold prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_EXPECTATION_MAXIMISE:\n    jem_preds = np.zeros((input_ids_t.shape[0], N_SPLIT * 3))\n    jem_preds = pd.DataFrame(jem_preds, columns = ['start0', 'end0', 'string0', 'start1', 'end1', 'string1', 'start2', 'end2', 'string2', 'start3', 'end3', 'string3', 'start4', 'end4', 'string4'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n\n# The start and end tokens will be stored in these\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n#preds_start_train = np.zeros((input_ids.shape[0],MAX_LEN))\n#preds_end_train = np.zeros((input_ids.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=N_SPLIT,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n    # Output the current fold\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    # Build the model\n    K.clear_session()\n    model, padded_model = build_model()\n    \n    # Add pseudo labels to the fold\n    if IS_USING_PSEUDO_LABELS:\n        input_ids_ps, attention_mask_ps, token_type_ids_ps, start_tokens_ps, end_tokens_ps = pseudo_labelled_folds_tokens[fold]\n    input_ids_fold = np.concatenate((input_ids[idxT,], input_ids_ps)) if IS_USING_PSEUDO_LABELS else input_ids[idxT,]\n    attention_mask_fold = np.concatenate((attention_mask[idxT,], attention_mask_ps)) if IS_USING_PSEUDO_LABELS else attention_mask[idxT,]\n    token_type_ids_fold = np.concatenate((token_type_ids[idxT,], token_type_ids_ps)) if IS_USING_PSEUDO_LABELS else token_type_ids[idxT,]\n    start_tokens_fold = np.concatenate((start_tokens[idxT,], start_tokens_ps)) if IS_USING_PSEUDO_LABELS else start_tokens[idxT,]\n    end_tokens_fold = np.concatenate((end_tokens[idxT,], end_tokens_ps)) if IS_USING_PSEUDO_LABELS else end_tokens[idxT,]\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids_fold, attention_mask_fold, token_type_ids_fold]\n    targetT = [start_tokens_fold, end_tokens_fold]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    \n    # Sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # Sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        \n        # Shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        \n        # Reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n    \n    # Load weights\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n    \n    # Make fold predictions\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    # Predict on the test set (which will only be 1/fold of the prediction)\n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    if IS_PSEUDO_LABELLING:\n        preds[0][fold::5] = 0\n        preds[1][fold::5] = 0\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # JEM prediction storage\n    if IS_EXPECTATION_MAXIMISE:\n        jem_preds['start' + str(fold)] = [preds[0][index][token] for index, token in enumerate(np.argmax(preds[0], axis=1))]\n        jem_preds['end' + str(fold)] = [preds[1][index][token] for index, token in enumerate(np.argmax(preds[1], axis=1))]\n        jem_preds['string' + str(fold)] = [tokenizer.decode(tokenizer.encode(\" \"+\" \".join(test.loc[i,'text'].split())).ids[np.argmax(preds[0][i])-2:np.argmax(preds[1][i])-1]) for i in range(input_ids_t.shape[0])]\n    \n    if IS_PREDICTING_TRAIN:\n        print('Predicting Train...')\n        preds_train = padded_model.predict([input_ids,attention_mask,token_type_ids],verbose=DISPLAY)\n        preds_start_train += preds_train[0]/skf.n_splits\n        preds_end_train += preds_train[1]/skf.n_splits\n    \n    # Convert the predicted start and end tokens into strings\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        if IS_SUBSTITUTING_URLS:\n            throwaway, st = deprocess_urls(train_web_replacements, '', st, k)\n        if IS_POSTPROCESSING:\n            st = post_processing(st)\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    \n    # Output fold score\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output the final CV score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))\nfor j in jac:\n    print('>>', j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nConvert the predicted start and end tokens into strings.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"jem_all = []\n\nif IS_EXPECTATION_MAXIMISE:\n    def jem_expectation_maximisation(confidences, strings, check_string):\n        res = 0\n        for i in range(len(confidences)):\n            res += confidences[i] * jaccard(strings[i], check_string)\n        return res\n\n    for k in range(input_ids_t.shape[0]):\n        # Calculate the prediction for each fold\n        #for fold in range(N_SPLIT):\n        #    jem_preds.loc[k,'string' + str(fold)] = tokenizer.decode(enc.ids[jem_preds.loc[k,'start' + str(fold)]-2:jem_preds.loc[k,'end' + str(fold)]-1])\n        \n        # Get predictions and confidences for each fold\n        jem_strings = [jem_preds.loc[k,'string' + str(x)] for x in range(N_SPLIT)]\n        jem_confidences = [0.5 * (jem_preds.loc[k, 'start' + str(x)] + jem_preds.loc[k, 'end' + str(x)]) for x in range(N_SPLIT)]\n        \n        # Jaccard Expectation Maximisation\n        jem_best = (0, test.loc[k, 'text'])\n        for fold in range(N_SPLIT):\n            jem_fold_conf = jem_expectation_maximisation(jem_confidences, jem_strings, jem_preds.loc[k, 'string' + str(fold)])\n            if jem_fold_conf > jem_best[0]:\n                jem_best = (jem_fold_conf, jem_strings[fold])\n        \n        # Add the best prediction to the list\n        jem_all.append(jem_best[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_GETTING_BEST_LOGITS:\n    def get_best_start_end_idxs(_start_logits, _end_logits):\n        best_logit = -1000\n        best_idxs = None\n        for start_idx, start_logit in enumerate(_start_logits):\n            for end_idx, end_logit in enumerate(_end_logits[start_idx:]):\n                logit_sum = (start_logit + end_logit).item()\n                if logit_sum > best_logit:\n                    best_logit = logit_sum\n                    best_idxs = (start_idx, start_idx+end_idx)\n        return best_idxs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    # Get the best start and end tokens\n    if IS_GETTING_BEST_LOGITS:\n        a, b = get_best_start_end_idxs(preds_start[k,], preds_end[k,])\n    else:\n        a = np.argmax(preds_start[k,])\n        b = np.argmax(preds_end[k,])\n    \n    # Extract the selected text using the start and end tokens\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    \n    # Substitute URL if required\n    if IS_SUBSTITUTING_URLS:\n        throwaway, st = deprocess_urls(test_web_replacements, '', st, k)\n    \n    # Perform postprocessing\n    if IS_POSTPROCESSING:\n        st = post_processing(st)\n    \n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output the predictions to file, and then show a sample of the predictions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_EXPECTATION_MAXIMISE:\n    test['selected_text'] = jem_all\nelse:\n    test['selected_text'] = all\n\nif IS_PSEUDO_LABELLING:\n    test.to_csv('extra_data.csv', index=False)\nelse:\n    test[['textID','selected_text']].to_csv('submission.csv',index=False)\n    \ntest.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we have been predicting on the training dataset then we also need to output the results of that.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if IS_PREDICTING_TRAIN:\n    all = []\n    start = []\n    end = []\n    start_pred = []\n    end_pred = []\n    for k in range(input_ids.shape[0]):\n        # Get the best start and end tokens\n        if IS_GETTING_BEST_LOGITS:\n            a, b = get_best_start_end_idxs(preds_start_train[k,], preds_end_train[k,])\n        else:\n            a = np.argmax(preds_start_train[k,])\n            b = np.argmax(preds_end_train[k,])\n        \n        start.append(np.argmax(start_tokens[k]))\n        end.append(np.argmax(end_tokens[k]))\n        \n        # Extract the selected text using the start and end tokens\n        if a>b:\n            st = train.loc[k,'text']\n            start_pred.append(0)\n            end_pred.append(len(st))\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n            start_pred.append(a)\n            end_pred.append(b)\n            \n        # Substitute URL if required\n        if IS_SUBSTITUTING_URLS:\n            throwaway, st = deprocess_urls(test_web_replacements, '', st, k)\n\n        # Perform postprocessing\n        if IS_POSTPROCESSING:\n            st = post_processing(st)\n            \n        all.append(st)\n        \n    train['start'] = start\n    train['end'] = end\n    train['start_pred'] = start_pred\n    train['end_pred'] = end_pred\n    train['selected_text_pred'] = all\n    train.to_csv('outliers.csv', index=False)\n    train.sample(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}