{"cells":[{"metadata":{},"cell_type":"markdown","source":"<br><br>\n\n<center><font size=\"5\">üìù Question-Answering Starter pack with ü§ótransformers</font></center>\n   \n<br>\n\n<center>\n<font size=\"3\">\n  In this notebook, by making use of  <a href=\"http://https://github.com/huggingface/transformers\">transformers</a> we express the learning problem as a <strong>question-answering system</strong>.\n  \n  <br><br>\n  \n  The code and the notebook-format have been designed to be easy-to-understand for beginners but hopefully also useful for advanced Kagglers.\n  \n  <br><br>\n  \n  Any comment/feedback is very appreciated. Disclaimer: work in progress, I will add new resources and comments soon. \n  \n    \n</font>\n</center>\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Problem formulation\n\nWe formulate the task as question answering problem: given a question and a context, we train a transformer model to find the **answer** in the `text` column (the context).\n\nWe have:\n 1. Question: `sentiment` column (`positive` or `negative`)\n 2. Context:  `text` column\n 3. Answer: `selected_text` column\n\n\n### 2. Getting started with QA\n\nA great resource to quickly recap question answering is this great amazing Stanford Lecture: [Question Answering](https://web.stanford.edu/class/cs124/lec/watsonqa.pdf).\n\n#### 2.1 Other free online resources:\n\n - [Youtube: Stanford CS224N - Question Answering](https://www.youtube.com/watch?v=yIdF-17HwSk)\n - [Medium: Building a Question-Answering System from Scratch‚Äî Part 1](https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507)\n - [Github: awesome question answering](https://github.com/seriousran/awesome-qa)\n\n\n### 3. Learning QA from scratch\n\nThe final project of the Stanford course CS224n, **Natural Language Processing with Deep Learning** consist of creating (almost) from scratch a Question-Ansering system using deep neural nets and transformers. [Here](https://web.stanford.edu/class/cs224n/project/default-final-project-handout.pdf) you can find the handout of 24 pages. For the most enthusiast out there: you may want to do this project and implement your Question-Answering system. It's probably the best way to fully understand and learn what QA is about.\n\n### 4. Model: DistilBERT + SQuAD\n\nThe current version of the notebook makes use of the `distilbert-base-uncased-distilled-squad` model.\n\nDistilBERT paper: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n\n> As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\n\nThe distilBERT model has already been fine-tuned on a question-answering challenge: SQuAD, the [Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/). This is the main reason why it performs already well out-of-the-box (0.666 score in the LB).\n\n#### 4.1 Training time\n\nThanks to the limited size of the transformer model, the notebook runs quite fast, training time is about 20 minutes with _GPU_.\n\n\n### 5. Dataset publicly available\n\n#### 5.1 DistilBERT + SQuAD model\nBecause Tweet Sentiment Extraction's notebooks must have internet switched off, I already downloaded and stored the transformer model in a public Kaggle dataset: [Transformers pre-trained distilBERT models](https://www.kaggle.com/jonathanbesomi/transformers-pretrained-distilbert). In future, I plan to upload all [distilBERT pre-trained models](https://huggingface.co/transformers/pretrained_models.html) to the same dataset so that we can easily play around with many models and configuration.\n\n#### 5.2 Simple Transformers PyPI\n\nTo keep the code to-the-point, this notebook makes use of an external python package: [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers). For your convenience, the wheel files to install the package have already been stored in this database: [Simple Transformers PyPI](https://www.kaggle.com/jonathanbesomi/simple-transformers-pypi).\n\n\n### 6. Acknowledgement\n\n- [RoBERTa Baseline Starter (+ simple postprocessing)](https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nLOAD DATA\n\"\"\"\n\nimport numpy as np \nimport pandas as pd \nimport json\n\n\ntrain_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsub_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n\ntrain = np.array(train_df)\ntest = np.array(test_df)\n\n!mkdir -p data\n\n\"\"\"\nSETTINGS\n\"\"\"\n\nuse_cuda = True # whether to use GPU or not","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare data in QA format\n\nExample-format:\n\n```\ntrain_data = [\n    {\n        'context': \"This tweet sentiment extraction challenge is great\",\n        'qas': [\n            {\n                'id': \"00001\",\n                'question': \"positive\",\n                'answers': [\n                    {\n                        'text': \"is great\",\n                        'answer_start': 43\n                    }\n                ]\n            }\n        ]\n    }\n    ]\n```"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%time\n\n\"\"\"\nPrepare training data in QA-compatible format\n\"\"\"\n\n# Adpated from https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        output.append({'context': context.lower(), 'qas': qas})\n        \n    return output\n\nqa_train = do_qa_train(train)\n\nwith open('data/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n\n\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\ndef do_qa_test(test):\n    output = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n        output.append({'context': context.lower(), 'qas': qas})\n    return output\n\nqa_test = do_qa_test(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Install [simple-transformers](https://github.com/ThilinaRajapakse/simpletransformers), a tool to train and test transformers model easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install '/kaggle/input/simple-transformers-pypi/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '/kaggle/input/simple-transformers-pypi/simpletransformers-0.22.1-py3-none-any.whl' -q","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train model\n\nTrain the `distilbert-base-uncased-distilled-squad` model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n\nfrom simpletransformers.question_answering import QuestionAnsweringModel\n\nMODEL_PATH = '/kaggle/input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad/'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 3,\n                                     'max_seq_length': 192,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data/train.json')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\nsub_df['selected_text'] = predictions_df['answer']\n\nsub_df.to_csv('submission.csv', index=False)\n\nprint(\"File submitted successfully.\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}