{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"<font size=\"+3\" color=purple ><b> <center><u>Tweet Sentiment Extraction</u></center></b></font>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of content\n* [1. Objective](#1)\n* [2. Evaluation Metric](#2)\n* [3. Data](#3)\n    - [3.1 Libraries](#3.1)\n    - [3.2 Load Data](#3.2)\n    - [3.3 Shape](#3.3)\n    - [3.4 Proportion](#3.4)\n    - [3.5 NAN](#3.5)\n* [4. Features](#4)\n    - [4.1 Target-Selected Text](#4.1)\n        - [4.1.1 Find URLs](#4.1.1)\n        - [4.1.2 Punctuations - Selected Text](#4.1.2)\n        - [4.1.3 Length - Punctuation](#4.1.3)\n        - [4.1.4 Length - Selected Text](#4.1.4)\n        - [4.1.5 Average Length - Selected Text](#4.1.5)\n        - [4.1.6 Most Words - Selected Text](#4.1.6)\n    - [4.2 Sentiment](#4.2)\n        - [4.2.1 Sentiment - Train](#4.2.1)\n        - [4.2.2 Sentiment - Test](#4.2.2)\n    - [4.3 Text](#4.3)\n        - [4.3.1 Punctuation - Text](#4.3.1)\n        - [4.3.2 Length - Text](#4.3.2)\n        - [4.3.3 Average Length - Text](#4.3.3)\n        - [4.3.4 Most Words - Text](#4.3.4)\n        - [4.3.5 Stopwords](#4.3.5)\n* [5. Comparison](#5)\n    - [5.1 N-grams](#5.1)\n        - [5.1.1 Train - Selected text](#5.1.1)\n        - [5.1.2 Text - Train vs Test](#5.1.2)\n    - [5.2 Venn](#5.2)\n        - [5.2.1 Venn - Text vs Selected Text](#5.2.1)\n    - [5.3 Wordcloud](#5.3)\n        - [5.3.1 Word Cloud - Selected Text](#5.3.1)\n        - [5.3.2 Word Cloud Train vs Text](#5.3.2)\n    - [5.4 Jaccard](#5.4)\n        - [5.4.1 Text vs Selected Text](#5.4.1)\n        - [5.4.2 Jaccard - Sentiment](#5.4.2)\n        - [5.4.3 Violin - Jaccard Score](#5.4.3)\n        - [5.4.4 Difference - Text vs Selected Text](#5.4.4)\n        - [5.4.5 Difference vs Jaccard](#5.4.5)\n* [6. roBERTa](#6)\n    - [6.1 Basic Setup](#6.1)\n    - [6.2 Mask - Train](#6.2)\n    - [6.3 Mask - Test](#6.3)\n    - [6.4 Model](#6.4)\n    - [6.5 Run Model](#6.5)\n    - [6.6 Submission](#6.6)\n    \n    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"blue\"><b>1. Objective</b></font><a id=\"1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Competiton**     : [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)\n* **Predict**    : Support phrases from given tweet text\n* **Evaluation** : [Jaccard Score](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50). We will come to know more about this in below sections.\n* **Last Date to Join this competition in kaggle** :June 9, 2020 - Entry Deadline.So dont get late to join.\n* **Stages of this kernel** : Data >> Features(EDA) >> Comparison(EDA) >> Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\">Please show patience to go through my kernel and appreciate me with an <font color=\"red\"><b>UPVOTE</b></font> which will encourage me to make more kernels","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Note:**<br>\n* I have hidden helper function code input and plot code input for providing more readabilty view.\n* I have another kernel that has basic level of helper functions required for text processing.Please read it as well. https://www.kaggle.com/raenish/cheatsheet-text-helper-functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"blue\"><b>2. Evaluation Metric</b></font><a id=\"2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Jaccard Score** is more about how exactly the predicted words match against actual words in a sentence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n# First set of sentences\nActual_1 = 'My life is totally awesome'\nPredict_1 = 'awesome life'\n\n# First set of sentences\nActual_2 = 'We are active kagglers'\nPredict_2 = 'We are active kagglers'\n    \nprint(\"Jaccard score for first set of scentences: {}\".format(jaccard(Actual_1,Predict_1)))\nprint(\"Jaccard score for second set of scentences: {}\".format(jaccard(Actual_2,Predict_2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"blue\"><b>3. Data</b></font><a id=\"3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The data is collected from twitter.And we have four columns of data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"| Columns       |      Description          | \n|---------------|:-------------------------:|\n| ID            |  Unique ID for each tweet |       \n| Text          |  Whole content of tweet   |   \n| Selected Text |  Selected Text of tweet   |    \n| Sentiment     |  Sentiment of tweet       |","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>Key Things before start</b></font><br>\n\nThere are few factors that can play a major part in terms of getting good score in competition.We may not consider all of them for EDA.\n\n* **Do consider raw data**.As we have selected text in our data which is **completely filled with raw text**,we cannot avoid them.It is our target variable(Selected text).(I may call it as target variable in most part of my kernel)\n* **Sentiments are very important.** Sentiments play a vital part because it will identify few words for target.So this variable can always play a cameo in EDA and modelling too.(I have completely utilized Sentiments for my EDA presentation.I could gain some insights from them)\n* **Do not correct spell** Our metric is so strict that even a punctuation can ruin your predicted word.\n\n\n*My primary focus would be to perform deep EDA on **text,selected_text,sentiment** to get pattern of train and test data.Understanding data with these features is very important*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>3.1 Libraries</b></font><br><a id=\"3.1\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib_venn as venn\nimport seaborn as sns\n\n\nfrom tqdm import tqdm\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom collections import defaultdict\nfrom collections import  Counter\n\n\n# sklearn \nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n#nltk\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nstop=set(stopwords.words('english'))\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\n#Avoid warning messages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#plotly libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\n\nfrom datetime import datetime as dt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>3.2 Load Data</b></font><br><a id=\"3.2\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntrain.sample(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>3.3 Shape</b></font><br><a id=\"3.3\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"There are {} rows and {} columns in train file\".format(train.shape[0],train.shape[1]))\nprint(\"There are {} rows and {} columns in test file\".format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>3.4 Proportion</b></font><br><a id=\"3.4\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"There are {} percentage of test data proportion compared to train data\".format(round(test.shape[0]/train.shape[0]*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>3.5 NAN</b></font><br><a id=\"3.5\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Function for missing value\ndef miss_val(df):\n    total=df.isnull().sum()\n    return pd.concat([total],axis=1,keys=['Total'])\nprint(\"Missing values for train dataset \\n\")\nprint(miss_val(train))\nprint(\"---------------------------------------------------------------------\")\nprint(\"Missing values for test dataset \\n\")\nprint(miss_val(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observations:</b></font><br>\n\n* Data proportion between train and test is almost **88% - 12%**\n* **4** object type variables.\n* Only one row in train data has null value in text and selected_text columns\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<font size=\"+3\" color=\"blue\"><b>4. Features</b></font><br><a id=\"4\"></a>\n\n#### What are we going to do in this section?\n\n* Data cleaning\n* EDA\n* Feature generation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Since we have 1 NULL row,we will remove it from train data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.dropna()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>4.1 Target-Selected Text</b></font><a id=\"4.1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>EDA on Selected text</b></font> <br>\n\nWe will undergo some basic text prepocessing and EDA on our target field- **Selected Text**.This is to understand how this feature is distributed in train data.\n\n* Find URLs\n* Punctuations\n* Length of tweets\n* Average of tweets\n* Most words ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.1.1 Find URLs - Target</b></font> <a id=\"4.1.1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Why to consider URL?\n\nURLs makes no sense for extreme sentiments.There are chances that they stay on neutral side.Lets check how they are spread in selected text\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Convert to lower\ntrain['target']=train['selected_text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Find URL\ndef find_link(string): \n    url = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string)\n    return \"\".join(url) \ntrain['target_url']=train['target'].apply(lambda x: find_link(x))\ndf=pd.DataFrame(train.loc[train['target_url']!=\"\"]['sentiment'].value_counts()).reset_index()\ndf.rename(columns={\"index\": \"sentiment\", \"sentiment\": \"url_count\"})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* As expected,our target variables that has urls are **neutral** tweets.Only few urls are found in positive and negative.\n* Our model would easily judge that if the text has URL it will be along neutral side.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b> 4.1.2 Punctuations - Selected Text</b></font> <a id=\"4.1.2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Can punctutations/symbols play a part in modelling?\n\nSince we are analysing sentimental tweets,people describe their emotions in symbols.Say symbols like continuous stars **( * )** is considered to be extreme emotions(happy,angry,delight etc).Other symbols like **(# - tagging)** or **(@ - mention)** are also used very often in tweets.\n\nLets analyse all of them including other punctuations","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Function to find punctuation\ndef find_punct(text):\n    line = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*', text)\n    string=\"\".join(line)\n    return list(string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# New Features with punctuation and punctuation length\ntrain['target_punct']=train['target'].apply(lambda x:find_punct(x))\ntrain['target_punct_len']=train['target'].apply(lambda x:len(find_punct(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"punc_df=pd.DataFrame(train,columns=['target_punct','sentiment'])\npunc_df=punc_df[punc_df['target_punct'].map(lambda d: len(d)) > 0]\npunc_df=punc_df.explode('target_punct')\n\npositive_df=pd.DataFrame(punc_df.loc[punc_df['sentiment']==\"positive\"]['target_punct'].value_counts()).reset_index().rename(columns={'index': 'punct','target_punct':'pos_punct'})\nnegative_df=pd.DataFrame(punc_df.loc[punc_df['sentiment']==\"negative\"]['target_punct'].value_counts()).reset_index().rename(columns={'index': 'punct','target_punct':'neg_punct'})\nneutral_df=pd.DataFrame(punc_df.loc[punc_df['sentiment']==\"neutral\"]['target_punct'].value_counts()).reset_index().rename(columns={'index': 'punct','target_punct':'neut_punct'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=3)\n\nfig.append_trace(go.Bar(x=positive_df.punct[:10],y=positive_df.pos_punct[:10],name='Positive',marker_color='green'), row=1, col=1)\nfig.append_trace(go.Bar(x=negative_df.punct[:10],y=negative_df.neg_punct[:10],name='Negative',marker_color='red'), row=1, col=2)\nfig.append_trace(go.Bar(x=neutral_df.punct[:10],y=neutral_df.neut_punct[:10],name='Neutral',marker_color='orange'), row=1, col=3)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text=\"Selected Text - Sentiment vs Punctuation\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### How much impact does ( * ) have?","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def find_star(text):\n   # if len(text.split())<1:\n    line=re.findall(r'[*]{2,5}',text)\n    return len(line)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['star']=train['target'].apply(lambda x:find_star(x))\ntrain.loc[train['star']!=0]['sentiment'].value_counts().to_frame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eventhough negative shows high counts.Still it describes about neutral tweets dependency.Let us analyse the tweet with only ( * ) in tweet.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def find_only_star(text):\n    if len(text.split())==1:\n        line=re.findall(r'[*]{2,5}',text)\n        return len(line)\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# grt column value that has only * in its tweet\ntrain['only_star']=train['target'].apply(lambda x:find_only_star(x))\ntrain.loc[train['only_star']==1]['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is interesting.Selected text with only **star** symbol has negative sentiments.\nReplace those star with word \"**abusive**\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target']= np.where(train['only_star']==1,\"abusive\",train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* All sentiment tweets have full stop and quotes on top of list which is expected.\n* Tweets with ( * ) only has negative sentiments \n* ( # ) and ( @ ) are no where in top lists.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.1.3 Length - Punctuation</b></font> <a id=\"4.1.3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### How will punctuations play part in this competition?\n\nYes we know that the jaccard score matches string with punctuations too.\n\n<u>For example:</u> <br>\na=good.<br>\nb=good <br>\nSince there is a punctuation in a,length of a is not equal to length of b.<br>\n\nSo punctuations needs more attention as well.We cant remove it from our text without convincing reason.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(\n    x=train[train['sentiment']=='positive']['target_punct_len'],\n    #histnorm='percent',\n    name='Positive', # name used in legend and hover labels\n    xbins=dict( # bins used for histogram\n        start=1,\n        end=20,\n        size=1\n    ),\n    marker_color='green',\n    opacity=0.75\n))\nfig.add_trace(go.Histogram(\n    x=train[train['sentiment']=='negative']['target_punct_len'],\n   # histnorm='percent',\n    name='Negative',\n    xbins=dict(\n        start=1,\n        end=20,\n        size=1\n    ),\n    marker_color='red',\n    opacity=0.75\n))\n\nfig.add_trace(go.Histogram(\n    x=train[train['sentiment']=='neutral']['target_punct_len'],\n   # histnorm='percent',\n    name='Neutral',\n    xbins=dict(\n        start=1,\n        end=20,\n        size=1\n    ),\n    marker_color='orange',\n    opacity=0.75\n))\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(\n    title_text='Distribution - Selected Text Punctuation length', # title of plot\n    xaxis_title_text='Length', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    title_x=0.5,\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.1.4 Length - Selected Text</b></font> <br><a id=\"4.1.4\"></a>\n\nIn this section we will analyse more on words of *selected_text*. Before extracting length of selected_text words,let us remove urls and punctuation\n\n**Note**: We need to keep in mind that while modelling we cant strip them.They play part in jaccard score.Now we will do this check the distribution of words only.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Remove URLs & Punctuation","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def remove_link(string): \n    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \",string)\n    return \" \".join(text.split())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]+',\" \",text)\n    return \" \".join(line.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['target']=train['target'].apply(lambda x:remove_link(x))\ntrain['target']=train['target'].apply(lambda x:remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['target_tweet_length']=train['target'].str.split().map(lambda x: len(x))\ntrain['target_tweet_length'].describe().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train[train['sentiment']=='positive']['target_tweet_length'],name=\"Positive\",marker_color='green'))\nfig.add_trace(go.Histogram(x=train[train['sentiment']=='negative']['target_tweet_length'],name=\"Negative\",marker_color='red'))\nfig.add_trace(go.Histogram(x=train[train['sentiment']=='neutral']['target_tweet_length'],name=\"Neutral\",marker_color='orange'))\n\n# Overlay both histograms\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(\n    title_text='Length of each Selected Text tweet', # title of plot\n    xaxis_title_text='Length', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    title_x=0.5,\n    barmode='overlay'\n)\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Average length of word in selected text is around 7 which seems bit high.This says we need to** predict more words per tweet**\n* Most of positive and negative tweets have selected text length **less than 5**.This is good significance as we can add exclude conditons for predicting words.\n* Neutral sentiments are distributed across all length.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.1.5 Average Length - Selected Text</b></font><br><a id=\"4.1.5\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Averaging selected text can determine us how long the train data has accepted strings from whole tweet.We will find the distribution of the selected text or target variable","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['target_average_word_len']=train['target'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\ntrain['target_average_word_len'].describe().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Histogram(x=train[train['sentiment']=='positive']['target_average_word_len'], xbins=dict(\n        start=1,\n        end=30,\n        size=1\n    ),name=\"Positive\",marker_color='green'))\nfig.add_trace(go.Histogram(x=train[train['sentiment']=='negative']['target_average_word_len'], xbins=dict(\n        start=1,\n        end=30,\n        size=1\n    ),name=\"Negative\",marker_color='red'))\nfig.add_trace(go.Histogram(x=train[train['sentiment']=='neutral']['target_average_word_len'], xbins=dict(\n        start=1,\n        end=30,\n        size=1\n    ),name=\"Neutral\",marker_color='orange'))\n\n# Overlay both histograms\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(\n    title_text='Target - Average length of Selected Text', # title of plot\n    xaxis_title_text='Length', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    title_x=0.5,\n    barmode='overlay'\n)\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Positive and negative are averaging around 4-5 per sentence\n* Neutral keeps their high density.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.1.6 Most Words - Selected Text</b></font> <br><a id=\"4.1.6\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is essential to know before we perform modelling.Before we perform GLOVE or any word embedding,it is good to know which all are the words often used.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_corpus(data,feature,sentiment):\n    corpus=[]\n    for x in data[data['sentiment']==sentiment][feature].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def corpus_sentiment(data,feature,sentiment):\n    corpus=create_corpus(data,feature,sentiment)\n    dic=defaultdict(int)\n    for word in corpus:\n        if word not in stop:\n            dic[word]+=1\n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)\n    x,y=zip(*top)\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,3,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=list(corpus_sentiment(train,'target','positive')[0])[:15], x= list(corpus_sentiment(train,'target','positive')[1])[:15],color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\nsns.barplot(y=list(corpus_sentiment(train,'target','negative')[0])[:15], x=list(corpus_sentiment(train,'target','negative')[1])[:15],color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\nsns.barplot(y=list(corpus_sentiment(train,'target','neutral')[0])[:15], x=list(corpus_sentiment(train,'target','neutral')[1])[:15],color=\"orange\", ax=axes[2]).set_title(\"Neutral\",color=\"orange\")\n\nplt.suptitle(\"Most Common Words in Selected Text\" ,fontsize=25,color=\"blue\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Positive produces words like **good,happy,thanks** which is quite expected\n* Negative produces words like **miss,sad,sorry,bad** which indicates negative emotions\n* Neutral produces words like **get,going,work** which is sort of common words\n* We could see \"day\" comes in all three sentiments.(These tweets might be indicating a event day.Hopefully we may come to know after competition)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>4.2 Sentiment</b></font><a id=\"4.2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Sentiment variable is the theme of our data.Let us know how it is distributed accross whole data.As of now most of participants in this competition have done modeling based on sentiments.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# count unique values present in each column\ndef count_values(df,feature):\n    total=df.loc[:,feature].value_counts(dropna=False)\n    percent=round(df.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percent],axis=1,keys=['Total','Percent'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.2.1 Sentiment - Train</b></font><a id=\"4.2.1\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"count_values(train,'sentiment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.2.2 Sentiment - Test</b></font><a id=\"4.2.2\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"count_values(train,'sentiment')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sent_train=count_values(train,'sentiment')\nsent_test=count_values(test,'sentiment')\n\ncolors = ['orange','green','red']\n\nfig = make_subplots(rows=1, cols=2,specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\nfig.add_trace(go.Pie(labels=list(sent_train.index), values=list(sent_train.Total.values), hoverinfo='label+percent', \n               textinfo='value+percent',marker=dict(colors=colors)),row=1,col=1)\nfig.add_trace(go.Pie(labels=list(sent_test.index), values=list(sent_test.Total.values), hoverinfo='label+percent', \n               textinfo='value+percent', marker=dict(colors=colors)),row=1,col=2)\nfig.update_layout( title_text=\"Sentiment - Train vs Test\",title_x=0.5)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* We could see the distribution of sentiment is spread equally in both train and test data.  \n* This indicates that the data is **randomly shuffled**.\n* Both data has **high number of neutral reviews**.Positive and negative reviews have just **2.9% difference** between them in terms of spread.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>4.3 Text</b></font><br><a id=\"4.3\"></a>\n\nThis is the variable which we are going to train and predict\n\nBefore doing analysis,let us merge train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data=pd.concat([train,test])\nfull_data['text']=full_data['text'].str.lower()\nfull_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.3.1 Punctuation - Text</b></font><a id=\"4.3.1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Already we analysed punctuations of selected text,now lets have a peek into whole text.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# New Features with punctuation and punctuation length\nfull_data['text_punct']=full_data['text'].apply(lambda x:find_punct(x))\nfull_data['text_punct_len']=full_data['text'].apply(lambda x:len(find_punct(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"punc_text_df=pd.DataFrame(full_data,columns=['text_punct','sentiment'])\npunc_text_df=punc_text_df[punc_text_df['text_punct'].map(lambda d: len(d)) > 0]\npunc_text_df=punc_text_df.explode('text_punct')\n\npositive_text_df=pd.DataFrame(punc_text_df.loc[punc_text_df['sentiment']==\"positive\"]['text_punct'].value_counts()).reset_index().rename(columns={'index': 'punct','text_punct':'pos_punct'})\nnegative_text_df=pd.DataFrame(punc_text_df.loc[punc_text_df['sentiment']==\"negative\"]['text_punct'].value_counts()).reset_index().rename(columns={'index': 'punct','text_punct':'neg_punct'})\nneutral_text_df=pd.DataFrame(punc_text_df.loc[punc_text_df['sentiment']==\"neutral\"]['text_punct'].value_counts()).reset_index().rename(columns={'index': 'punct','text_punct':'neut_punct'})\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = make_subplots(rows=1, cols=3)\nfig.append_trace(go.Bar(x=positive_text_df.punct[:15],y=positive_text_df.pos_punct[:10],name='Positive',marker_color='green'), row=1, col=1)\nfig.append_trace(go.Bar(x=negative_text_df.punct[:15],y=negative_text_df.neg_punct[:15],name='Negative',marker_color='red'), row=1, col=2)\nfig.append_trace(go.Bar(x=neutral_text_df.punct[:15],y=neutral_text_df.neut_punct[:15],name='Neutral',marker_color='orange'), row=1, col=3)\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text=\"Text - Sentiment vs Punctuation\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Interestingly,we have ( @ ) and ( # ) in top positive and neutral tweets.\n* As expected ( * ) are found more in negative than rest two sentiments.May consider them as abusive language.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.3.2 Length - Text</b></font><a id=\"4.3.2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We would know the proportion between text and selected text.Now lets know the length of whole text.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"full_data['text']=full_data['text'].apply(lambda x:remove_link(x))\nfull_data['text']=full_data['text'].apply(lambda x:remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two records were found to be null after removal of punctuations.They are filled with values","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"full_data.loc[full_data['text']==\"\",['text']]=\"nothing\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"full_data['text_tweet_length']=full_data['text'].str.split().map(lambda x: len(x))\nfull_data['text_tweet_length'].describe().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(x=full_data[full_data['sentiment']=='neutral']['text_tweet_length'],name=\"Neutral\",marker_color='orange',boxmean='sd'))\nfig.add_trace(go.Box(x=full_data[full_data['sentiment']=='negative']['text_tweet_length'],name=\"Negative\",marker_color='red',boxmean='sd'))\nfig.add_trace(go.Box(x=full_data[full_data['sentiment']=='positive']['text_tweet_length'],name=\"Positive\",marker_color='green',boxmean='sd'))\n\n# Overlay both histograms\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(\n    title_text='Distribution of Text Length', # title of plot\n    xaxis_title_text='Length', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    title_x=0.5\n)\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* The average length is around.Selected text had only 5.One thing to notice that we merged whole train and test data.So this is more or less expected.\n* We observe almost similar kind of distribution among all sentiments.This describes that words are diversified with length among all sentiments. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.3.3 Average Length - Text</b></font><a id=\"4.3.3\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"full_data['text_average_word_len']=full_data['text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\nfull_data['text_average_word_len'].describe().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Box(x=full_data[full_data['sentiment']=='positive']['text_average_word_len'],name=\"Positive\",marker_color='green'))\nfig.add_trace(go.Box(x=full_data[full_data['sentiment']=='negative']['text_average_word_len'],name=\"Negative\",marker_color='red'))\nfig.add_trace(go.Box(x=full_data[full_data['sentiment']=='neutral']['text_average_word_len'],name=\"Neutral\",marker_color='orange'))\n\n# Overlay both histograms\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(\n    title_text='Average Length of Tweet', # title of plot\n    xaxis_title_text='Length', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    title_x=0.5,\n)\n\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* As we saw earlier,some sort of uniform distribution with more outlier are found.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.3.4 Most Words - Text</b></font><a id=\"4.3.4\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We need to what sort of words are place on top for each sentiments","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,3,figsize=(20,12))\nsns.set(font_scale=2)\nsns.barplot(y=list(corpus_sentiment(full_data,'text','positive')[0])[:15], x= list(corpus_sentiment(full_data,'text','positive')[1])[:15],color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\nsns.barplot(y=list(corpus_sentiment(full_data,'text','negative')[0])[:15], x=list(corpus_sentiment(full_data,'text','negative')[1])[:15],color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\nsns.barplot(y=list(corpus_sentiment(full_data,'text','neutral')[0])[:15], x=list(corpus_sentiment(full_data,'text','neutral')[1])[:15],color=\"orange\", ax=axes[2]).set_title(\"Neutral\",color=\"orange\")\n\nplt.suptitle(\"Most Common Words - Text\" ,fontsize=25,color=\"blue\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Positive Words like **\"good\",\"love\",\"happy\"** are in top of the list which is expected.\n* Neutral Words like **\"get\",\"go\",\"day\",\"lol\"** are also sort of neutral words.\n* But we could see some difference in negative,some non negative words like **\"get\",\"miss\",\"like\",\"work\"** are edging to top.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>4.3.5 Stopwords</b></font><a id=\"4.3.5\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We cannot remove stopwords from this variable.They are also connsidered as per our metric.Still it is good to know how is distributed to each sentiments","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def corpus_sentiment_stop(data,feature,sentiment):\n    corpus=create_corpus(data,feature,sentiment)\n    dic=defaultdict(int)\n    for word in corpus:\n        if word in stop:\n            dic[word]+=1\n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)\n    x,y=zip(*top)\n    return x,y\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,3,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=list(corpus_sentiment_stop(full_data,'text','positive')[0])[:15], x= list(corpus_sentiment_stop(full_data,'text','positive')[1])[:15],color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\nsns.barplot(y=list(corpus_sentiment_stop(full_data,'text','negative')[0])[:15], x=list(corpus_sentiment_stop(full_data,'text','negative')[1])[:15],color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\nsns.barplot(y=list(corpus_sentiment_stop(full_data,'text','neutral')[0])[:15], x=list(corpus_sentiment_stop(full_data,'text','neutral')[1])[:15],color=\"orange\", ax=axes[2]).set_title(\"Neutral\",color=\"orange\")\n\nplt.suptitle(\"Most Common Stop Words - Text\" ,fontsize=25,color=\"blue\")\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* **\"I\" ,\"the\" ,\"too\" ,\"a\" ** are leading on all sentiments.This is very much expected. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Blue\"><b>5. Comparison</b></font><a id=\"5\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Why do we need comparison?\n\nWe are facing information extraction from a sentence/tweet.And we have selected text from train data too.So We have to kind of matching these two variables in all expects.(Train data -Text will compete with Test data - Text)\n\nBut for now ,I think extracting information from these two variables will be a key insight to predict selected text for test data\n\n\nThis section holds :\n\n* **(N)grams     - Train Selected text** \n* **(N)grams     - Train text vs Test text** \n* **Venn Diagram - Train text vs Selected text**\n* **Word cloud   - Train selected Text**\n* **Word cloud   - Train text vs Test Text**\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_word=full_data[:train.shape[0]]\ntest_word=full_data[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>5.1 N-grams</b></font><br><a id=\"5.1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.1.1 Train - Selected text</b></font><br><a id=\"5.1.1\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def ngrams_plot(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, axes = plt.subplots(1,3,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['target'],(1,1),10)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['target'],(1,1),10)['count'],\n            color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['target'],(1,1),10)['text'],\n            x=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['target'],(1,1),10)['count'],\n            color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['target'],(1,1),10)['text'], \n            x=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['target'],(1,1),10)['count'],\n            color=\"orange\", ax=axes[2]).set_title(\"Neutral\",color=\"orange\")\naxes[2].set(ylabel=\" \",xlabel=\" \")\nf.suptitle(\"Top 10 Selected Text - Unigram\" ,fontsize=25,color=\"blue\")\nf.show() \n\nf, axes = plt.subplots(1,3,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['target'],(2,2),10)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['target'],(2,2),10)['count'],\n            color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['target'],(2,2),10)['text'],\n            x=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['target'],(2,2),10)['count'],\n            color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['target'],(2,2),10)['text'], \n            x=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['target'],(2,2),10)['count'],\n            color=\"orange\", ax=axes[2]).set_title(\"Neutral\",color=\"orange\")\naxes[2].set(ylabel=\" \",xlabel=\" \")\nf.suptitle(\"Top 10 Selected Text - Bigram\" ,fontsize=25,color=\"blue\")\nf.show() \n\nf, axes = plt.subplots(1,3,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['target'],(3,3),10)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['target'],(3,3),10)['count'],\n            color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['target'],(3,3),10)['text'],\n            x=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['target'],(3,3),10)['count'],\n            color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['target'],(3,3),10)['text'], \n            x=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['target'],(3,3),10)['count'],\n            color=\"orange\", ax=axes[2]).set_title(\"Neutral\",color=\"orange\")\naxes[2].set(ylabel=\" \",xlabel=\" \")\nf.suptitle(\"Top 10 Selected Text - Trigram\" ,fontsize=25,color=\"blue\")\nf.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* \"Day\" was always a part among all sentiment tweets and thus confirmed the event.Well,<b><i>\"Happy Mothers day\"</i></b>.All these tweets were collected around **May 2nd Week**.\n* Positive bigrams and trigrams words are more biased towards mothers day.\n* Negative ngrams are displaying negative emotional words.\n* Neutral shows common words.Nothing much inference from this sentiment.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.1.2 Text - Train vs Test</b></font><br> <a id=\"5.1.2\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['text'],(1,1),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['text'],(1,1),15)['count'],\n            color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nf.suptitle(\"Tweet Unigrams - Train vs Test\" ,fontsize=30,color=\"blue\")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='positive']['text'],(1,1),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='positive']['text'],(1,1),15)['count'],\n            color=\"green\", ax=axes[1]).set_title(\"Positive\",color=\"green\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n\nf, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['text'],(1,1),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['text'],(1,1),15)['count'],\n            color=\"red\",ax=axes[0]).set_title(\"Negative\",color=\"red\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='negative']['text'],(1,1),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='negative']['text'],(1,1),15)['count'],\n            color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n\nf, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['text'],(1,1),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['text'],(1,1),15)['count'],\n            color=\"orange\",ax=axes[0]).set_title(\"Neutral\",color=\"orange\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='neutral']['text'],(1,1),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='neutral']['text'],(1,1),15)['count'],\n            color=\"orange\", ax=axes[1]).set_title(\"Neutral\",color=\"orange\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['text'],(2,2),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['text'],(2,2),15)['count'],\n            color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nf.suptitle(\"Tweet Bigrams - Train vs Test\" ,fontsize=30,color=\"blue\")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='positive']['text'],(2,2),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='positive']['text'],(2,2),15)['count'],\n            color=\"green\", ax=axes[1]).set_title(\"Positive\",color=\"green\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n\nf, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['text'],(2,2),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['text'],(2,2),15)['count'],\n            color=\"red\",ax=axes[0]).set_title(\"Negative\",color=\"red\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='negative']['text'],(2,2),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='negative']['text'],(2,2),15)['count'],\n            color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n\nf, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['text'],(2,2),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['text'],(2,2),15)['count'],\n            color=\"orange\",ax=axes[0]).set_title(\"Neutral\",color=\"orange\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='neutral']['text'],(2,2),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='neutral']['text'],(2,2),15)['count'],\n            color=\"orange\", ax=axes[1]).set_title(\"Neutral\",color=\"orange\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['text'],(3,3),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='positive']['text'],(3,3),15)['count'],\n            color=\"green\",ax=axes[0]).set_title(\"Positive\",color=\"green\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nf.suptitle(\"Tweet Trigrams - Train vs Test\" ,fontsize=30,color=\"blue\")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='positive']['text'],(3,3),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='positive']['text'],(3,3),15)['count'],\n            color=\"green\", ax=axes[1]).set_title(\"Positive\",color=\"green\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n\nf, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['text'],(3,3),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='negative']['text'],(3,3),15)['count'],\n            color=\"red\",ax=axes[0]).set_title(\"Negative\",color=\"red\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='negative']['text'],(3,3),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='negative']['text'],(3,3),15)['count'],\n            color=\"red\", ax=axes[1]).set_title(\"Negative\",color=\"red\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n\nf, axes = plt.subplots(1,2,figsize=(20,12))\nsns.set(font_scale =2)\nsns.barplot(y=ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['text'],(3,3),15)['text'],\n            x= ngrams_plot(train_word.loc[train_word['sentiment']=='neutral']['text'],(3,3),15)['count'],\n            color=\"orange\",ax=axes[0]).set_title(\"Neutral\",color=\"orange\")\naxes[0].set(ylabel=\" \",xlabel=\" \")\nsns.barplot(y=ngrams_plot(test_word.loc[test_word['sentiment']=='neutral']['text'],(3,3),15)['text'],\n            x=ngrams_plot(test_word.loc[test_word['sentiment']=='neutral']['text'],(3,3),15)['count'],\n            color=\"orange\", ax=axes[1]).set_title(\"Neutral\",color=\"orange\")\naxes[1].set(ylabel=\" \",xlabel=\" \")\nf.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Just like above,we see same sort of words.But there are few new words found from test data.\n* We saw mostly words are resembling between train and test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>5.2 Venn</b></font><br><a id=\"5.2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.2.1 Venn - Text vs Selected Text</b></font><br><a id=\"5.2.1\"></a>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"pos_text_list=list(corpus_sentiment(train_word,'text','positive')[0])\npos_target_list=list(corpus_sentiment(train_word,'target','positive')[0])\nneg_text_list=list(corpus_sentiment(train_word,'text','negative')[0])\nneg_target_list=list(corpus_sentiment(train_word,'target','negative')[0])\nneutral_text_list=list(corpus_sentiment(train_word,'text','neutral')[0])\nneutral_target_list=list(corpus_sentiment(train_word,'target','neutral')[0])\n\npos_common_words_list=list(set(pos_text_list).intersection(pos_target_list))\nneg_common_words_list=list(set(neg_text_list).intersection(neg_target_list))\nneutral_common_words_list=list(set(neutral_text_list).intersection(neutral_target_list))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def venn_plot(x,y,common_list,title):\n    plt.title(title)\n    venn.venn2(subsets=(x,y,common_list), alpha = 0.5,set_labels=(\"# of unique words in Text\",\"# of unique words in Selected text\"))\n    return plt.show()\n\nvenn_plot(len(pos_text_list),len(pos_target_list),len(pos_common_words_list),\"# of Common Positive Words\")\nvenn_plot(len(neg_text_list),len(neg_target_list),len(neg_common_words_list),\"# of Common Negative Words\")\nvenn_plot(len(neutral_text_list),len(neutral_target_list),len(neutral_common_words_list),\"# of Common Neutral Words\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Neutral have most shared words between text and selected text.This is true because unseperable words are pushed in neutral section.\n* Positive and Negative have almost same sort of allocation according to their own length.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>5.3 Wordcloud</b></font><br><a id=\"5.3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.3.1 Word Cloud - Selected Text</b></font><br><a id=\"5.3.1\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"d = '../input/twitter/'\nbird = np.array(Image.open(d + 'twitter_mask.png'))\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(train_word.loc[train_word['sentiment']==\"positive\"]['target']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive Selected Text',fontsize=35);\n\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(train_word.loc[train_word['sentiment']==\"negative\"]['target']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Selected Text',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Blues\",\n                        width=600,\n                        height=400).generate(\" \".join(train_word.loc[train_word['sentiment']==\"neutral\"]['target']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral Selected Text',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.3.2 Word Cloud Train vs Text</b></font><br><a id=\"5.3.2\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ((ax1,ax3,ax5),(ax2, ax4,ax6)) = plt.subplots(2, 3, figsize=[30, 15])\n\nwordcloud1 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(train_word.loc[train_word['sentiment']==\"positive\"]['text']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Train - Positive text',fontsize=35);\n\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(test_word.loc[test_word['sentiment']==\"positive\"]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Test - Positive text',fontsize=35);\n\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(train_word.loc[train_word['sentiment']==\"negative\"]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Train - Negative text',fontsize=35);\n\n\nwordcloud4 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(test_word.loc[test_word['sentiment']==\"negative\"]['text']))\nax4.imshow(wordcloud4)\nax4.axis('off')\nax4.set_title('Test - Negative text',fontsize=35);\n\n\n\nwordcloud5 = WordCloud( background_color='white',mask=bird,colormap=\"Blues\",\n                        width=600,\n                        height=400).generate(\" \".join(train_word.loc[train_word['sentiment']==\"neutral\"]['text']))\nax5.imshow(wordcloud5)\nax5.axis('off')\nax5.set_title('Train - Neutral text',fontsize=35);\n\n\nwordcloud6 = WordCloud( background_color='white',mask=bird,colormap=\"Blues\",\n                        width=600,\n                        height=400).generate(\" \".join(test_word.loc[test_word['sentiment']==\"neutral\"]['text']))\nax6.imshow(wordcloud6)\nax6.axis('off')\nax6.set_title('Test - Neutral text',fontsize=35);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>5.4 Jaccard</b></font><br><a id=\"5.4\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.4.1 Text vs Selected Text</b></font><br><a id=\"5.4.1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### How can we utilize jaccard metric here?\n\nThe next upcoming plots will not display predicted vs actual values.In fact we need to understand that **how much selected words are pulled out from original text**.\n\n* If values is high or near to 1 ,the whole text is similar to selected text\n* If values is low or near to 0 ,only few selected text are taken from whole text.\n\n**Note:** I have removed URLs and punctuations already.Eventhough they play a small part,i have reduced the noise from them.Let it be only text vs text.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"results_jaccard=[]\n\nfor ind,row in train_word.iterrows():\n    sentence1 = row.text\n    sentence2 = row.target\n\n    jaccard_score = jaccard(sentence1,sentence2) # Jaccard function is defined at top of kernel\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"target\",\"jaccard_score\"])\ntrain_word = train_word.merge(jaccard,how='outer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_word['jaccard_score'].describe().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = train_word['text_tweet_length']\ny = train_word['jaccard_score']\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'blue',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"Jaccard score equals 1 : {}%\".format(round(train_word.loc[train_word['jaccard_score']==1].shape[0]/train_word.shape[0]*100,2)))\nprint(\"Jaccard score less than 0.3 : {}%\".format(round(train_word.loc[train_word['jaccard_score']<0.3].shape[0]/train_word.shape[0]*100,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Almost 80% of train data has jaccard score of 1 which means words in text equals seleted text\n* From above plot,we could gain agreat insight that **text less than 8 are almost equal to selected text(top left dense area)**.\n* **Text Words between 10 & 20 have low jaccard score.(bottom middle dense area around jacard score 0.1)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.4.2 Jaccard - Sentiment</b></font><br><a id=\"5.4.2\"></a>","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = train_word.loc[train_word['sentiment']==\"positive\"]['text_tweet_length']\ny = train_word.loc[train_word['sentiment']==\"positive\"]['jaccard_score']\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'green',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Postive Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"x = train_word.loc[train_word['sentiment']==\"negative\"]['text_tweet_length']\ny = train_word.loc[train_word['sentiment']==\"negative\"]['jaccard_score']\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'red',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Negative Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"x = train_word.loc[train_word['sentiment']==\"neutral\"]['text_tweet_length']\ny = train_word.loc[train_word['sentiment']==\"neutral\"]['jaccard_score']\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'orange',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Neutral Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.4.3 Violin - Jaccard Score</b></font><br><a id=\"5.4.3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"These plots can explain the distribution of jaccard score.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=train_word.loc[train_word['sentiment']==\"positive\"]['jaccard_score'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='green', opacity=0.6,name=\"Positive\",\n                               x0='Positive')\n             )\n\nfig.add_trace(go.Violin(y=train_word.loc[train_word['sentiment']==\"negative\"]['jaccard_score'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Negative\",\n                               x0='Negative')\n             )\n\nfig.add_trace(go.Violin(y=train_word.loc[train_word['sentiment']==\"neutral\"]['jaccard_score'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='orange', opacity=0.6,name=\"Neutral\",\n                               x0='Neutral')\n             )\n\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - Jaccard score\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* Positive and Negative tweets have almost similar sort of distribution.\n* Positive and Negative -text with less than 8 words have jaccard score near 1\n* Positive and Negative -Text around 10 - 20 are biased to low jaccard score\n* Despite the length of text most of them are similar to selected text.(Thereby jaccard score is high).Train data text are extracted as a whole and passed into selected text for most of tweets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.4.4 Difference - Text vs Selected Text</b></font><br><a id=\"5.4.4\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Difference variable would be difference between length of selected text and length of whole text.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_word['difference']=abs(train_word['text_tweet_length']-train_word['target_tweet_length'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=train_word.loc[train_word['sentiment']==\"positive\"]['difference'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='green', opacity=0.6,name=\"Positive\",\n                               x0='Positive')\n             )\n\nfig.add_trace(go.Violin(y=train_word.loc[train_word['sentiment']==\"negative\"]['difference'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Negative\",\n                               x0='Negative')\n             )\n\nfig.add_trace(go.Violin(y=train_word.loc[train_word['sentiment']==\"neutral\"]['difference'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='orange', opacity=0.6,name=\"Neutral\",\n                               x0='Neutral')\n             )\n\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - Difference\",title_x=0.5)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* As we saw from jaccard distribution,the same result can be observed on this plot as well","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"chocolate\"><b>5.4.5 Difference vs Jaccard</b></font><br><a id=\"5.4.5\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The difference between length of words and jaccard in same plot would tell us how it is baised.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=train_word['difference'],\n    y=train_word['jaccard_score'],\n    marker=dict(color=\"blue\", size=12),\n    mode=\"markers\"\n))\n\nfig.update_layout(title=\"Difference vs Jaccard\",\n                  xaxis_title=\"Difference\",\n                  yaxis_title=\"Jaccard Score\",title_x=0.5)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+1\" color=\"green\"><b>Observation:</b></font><br>\n\n* As difference decreases,jaccard score must go high.Then plot will look linear.\n* We have robust data in positive and negative tweets and so we could see our plot noisy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"violet\"><b>Findings:</b></font><br><a id=\"Findings\"></a>\n\n* Sentiments are **splitted uniformly** between train and test set.But neutral has covered more data.\n* Neutral tweets Selected Text(target) words are spread across different length till **30 words**.But positive and negative are **almost less than 5 words**.\n* Sentiments distribution for positive,negative,neutral - **approx(31%,28%,49%)**\n* Most of tweets were around **Mothers Day** Celebration\n* Neutral have **most shared words between text and selected text**.This is true because unseperable words are pushed in neutral section.\n* Positive and Negative - Text with **less than 8 words have jaccard score near 1** whereas tweet length **around 10 - 20 are biased to low jaccard score**.\n* Neutral - Despite the length of text most of them are similar to selected text.(Thereby jaccard score is high).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Blue\"><b>6. roBERTa</b></font><a id=\"6\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I would like to thank Chris Deotte for his wonderful [kernel](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705]) .This gave me inspirational to understand roberta.Also i referenced [Kiram Al Karba kernel](https://www.kaggle.com/al0kharba/tensorflow-roberta-0-712) where he reduced the training time of the model.\n\nI will try to explain codes as simple as possible.I tried working on most of above EDA outcomes on this model.But i could not find progress in CV.So I will stick with basic stuff for now done by Chris and Kiram.I will be updating this model once any of the EDA or prepocessing gets succeed with the score/CV.\n\n**Note:** You can fork my kernel to get pretrained data.Else you can fetch them from below links <br> \nhttps://www.kaggle.com/cdeotte/tf-roberta<br>\nhttps://www.kaggle.com/al0kharba/model4","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>6.1 Basic Setup</b></font><br><a id=\"6.1\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we dont have length larger than 96\nMAX_LEN = 96\n\n# Pretrained model of roberta\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\n\n# Sentiment ID value is encoded from tokenizer\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>6.2 Mask - Train</b></font><br><a id=\"6.2\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\nct=train.shape[0] #27481\n\n# Initialising training inputs\ninput_ids=np.ones((ct,MAX_LEN),dtype=\"int32\")          # Array with value 1 of shape(27481,96)\nattention_mask=np.zeros((ct,MAX_LEN),dtype=\"int32\")    # Array with value 0 of shape(27481,96)\ntoken_type_ids=np.zeros((ct,MAX_LEN),dtype=\"int32\")    # Array with value 0 of shape(27481,96)\nstart_tokens=np.zeros((ct,MAX_LEN),dtype=\"int32\")      # Array with value 0 of shape(27481,96)\nend_tokens=np.zeros((ct,MAX_LEN),dtype=\"int32\")        # Array with value 0 of shape(27481,96)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In below code ,please go through comments which i have mentioned between codes to identify variables progress line by line.I have added a sample row from train data for explanation.\n\n> text1 = my boss is bullying me <br>\n> text2 = bullying me","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in range(train.shape[0]):\n#1 FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    \n    # idx - position where the selected text are placed. \n    idx = text1.find(text2)   # we get [12] position\n    \n    # all character position as 0 and then places 1 for selected text position  \n    chars = np.zeros((len(text1))) \n    chars[idx:idx+len(text2)]=1    # [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] \n    \n    #tokenize id of text \n    if text1[idx-1]==' ': chars[idx-1] = 1    \n    enc = tokenizer.encode(text1)  #  [127, 3504, 16, 11902, 162]\n        \n#2. ID_OFFSETS - start and end index of text\n    offsets = []\n    idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))     #  [(0, 3), (3, 8), (8, 11), (11, 20), (20, 23)]\n        idx += len(w) \n    \n#3  START-END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b]) # number of characters in selected text - [0.0,0.0,0.0,9.0,3.0] - bullying me\n        if sm>0: \n            toks.append(i)  # token position - selected text - [3, 4]\n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']] # Encoded values by tokenizer\n    \n    #Formating input for roberta model\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]   #[ 0   127  3504    16 11902   162     2     2  2430     2]\n    attention_mask[k,:len(enc.ids)+5] = 1                                  # [1 1 1 1 1 1 1 1 1 1]\n    \n    if len(toks)>0:\n        # this will produce (27481, 96) & (27481, 96) arrays where tokens are placed\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1 \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>6.3 Mask - Test</b></font><br><a id=\"6.3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct_test = test.shape[0]\n\n# Initialize inputs\ninput_ids_t = np.ones((ct_test,MAX_LEN),dtype='int32')        # array with value 1 for shape (3534, 96)\nattention_mask_t = np.zeros((ct_test,MAX_LEN),dtype='int32')  # array with value 0 for shape (3534, 96)\ntoken_type_ids_t = np.zeros((ct_test,MAX_LEN),dtype='int32')  # array with value 0 for shape (3534, 96)\n\n# Set Inputs attention \nfor k in range(test.shape[0]):\n        \n#1. INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n     \n    # Encoded value of tokenizer\n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    \n    #setting up of input ids - same as we did for train\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>6.4 Model</b></font><br><a id=\"6.4\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef scheduler(epoch):\n    return 3e-5 * 0.2**epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef build_model():\n    \n    # Initialize keras layers\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    # Fetching pretrained models \n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    # Setting up layers\n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    # Initializing input,output for model.THis will be trained in next code\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    \n    #Adam optimizer for stochastic gradient descent. if you are unware of it - https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>6.5 Run Model</b></font><br><a id=\"6.5\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time=dt.now()\n\nn_splits=5 # Number of splits\n\n# INitialize start and end token\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nDISPLAY=1\nfor i in range(5):\n    print('#'*40)\n    print('### MODEL %i'%(i+1))\n    print('#'*40)\n    \n    K.clear_session()\n    model = build_model()\n    # Pretrained model\n    model.load_weights('../input/model4/v4-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits\n    \nend_time=dt.now()\nprint(\"   \")\nprint(\"   \")\nprint(\"Time Taken to run above code :\",(end_time-start_time).total_seconds()/60,\" minutes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"indigo\"><b>6.6 Submission</b></font><br><a id=\"6.6\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\n\nfor k in range(input_ids_t.shape[0]):\n    # Argmax - Returns the indices of the maximum values along axis\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = all\nsubmission=test[['textID','selected_text']]\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}