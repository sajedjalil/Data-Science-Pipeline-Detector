{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig, RobertaTokenizer\nfrom twittersentimentextactionmodule import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"preprocess<br>\ndata augmentation<br>\nmodel architecture, loss<br>\ntraining schedule, optimizer<br>\npostprocess","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"online = False\ne_cuda = True\n\nconfig_vocab_file = '../input/roberta-base/vocab.json'\nconfig_merges_file = '../input/roberta-base/merges.txt'\nconfig_roberta_config = '../input/roberta-base/config.json'\nconfig_roberta_model = '../input/roberta-base/pytorch_model.bin'\n\nif online:\n    rtokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    rtokenizer.save_vocabulary(\".\")\n    config_vocab_file = 'vocab.json'\n    config_merges_file = 'merges.txt'\n    config_roberta_config = 'roberta-base'\n    config_roberta_model = 'roberta-base'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"huggingface recommends training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more <unk> tokens!).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file = config_vocab_file, \n            merges_file = config_merges_file, \n            lowercase = True,\n            add_prefix_space = True)\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_val_loaders(df, train_idx, val_idx, batch_size):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\ndef get_test_loader(df, batch_size):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs):\n    \n    if e_cuda:\n        model.cuda()\n\n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            for data in (dataloaders_dict[phase]):\n                ids = data['ids'] #.cuda()\n                masks = data['masks'] #.cuda()\n                tweet = data['tweet']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_idx'] #.cuda()\n                end_idx = data['end_idx'] #.cuda()\n\n                if e_cuda:\n                    ids = ids.cuda()\n                    masks = masks.cuda()\n                    start_idx = start_idx.cuda()\n                    end_idx = end_idx.cuda()\n\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    start_logits, end_logits = model(ids, masks)\n\n                    loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                    epoch_loss += loss.item() * len(ids)\n                    \n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    for i in range(len(ids)):                        \n                        jaccard_score = compute_jaccard_score(\n                            tweet[i],\n                            start_idx[i],\n                            end_idx[i],\n                            start_logits[i], \n                            end_logits[i], \n                            offsets[i])\n                        epoch_jaccard += jaccard_score\n                    \n            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n            \n            print('|  {}/{}  | {:^5} | {:.4f} |  {:.4f} |'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n            #print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n            #    epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_logits(test_loader):\n    s_logits = {}\n    e_logits = {}\n    for i, data in enumerate(test_loader):\n        s_logits[i] = []\n        e_logits[i] = []      \n    return s_logits, e_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tf.keras.layers.Conv1D(\n#     filters, kernel_size, strides=1, padding='valid', data_format='channels_last',\n#     dilation_rate=1, activation=None, use_bias=True,\n#     kernel_initializer='glorot_uniform', bias_initializer='zeros',\n#     kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n#     kernel_constraint=None, bias_constraint=None, **kwargs\n# )        \n# 768 -> filters     -> Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n# 2   -> kernel_size -> An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n# tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n\n# in_channels (int)  – Number of channels in the input image\n# out_channels (int) – Number of channels produced by the convolution\n# kernel_size (int or tuple) – Size of the convolving kernel\n# torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n\n# torch.nn.Linear(in_features, out_features, bias=True)\n\n# x1 = tf.keras.layers.Dropout(0.1)(x[0])\n# x1 = tf.keras.layers.Conv1D(768, 2, padding='same')(x1)\n# x1 = tf.keras.layers.LeakyReLU()(x1)\n# x1 = tf.keras.layers.Dense(1)(x1)\n# x1 = tf.keras.layers.Flatten()(x1)\n# x1 = tf.keras.layers.Activation('softmax')(x1)\n\n# hs[-1] torch.Size([32, 96, 768])\n## torch.Size([3, 32, 96, 768])\n#x = x.permute(1, 2, 3, 0) # channels to last dim\n#print(x.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass TweetModel(nn.Module):\n    def __init__(self, p_drop, l_std, m_size):\n        super(TweetModel, self).__init__()\n       \n        config = RobertaConfig.from_pretrained(\n            config_roberta_config, output_hidden_states=True)    \n        self.roberta = RobertaModel.from_pretrained(\n            config_roberta_model, config=config)\n        \n        # (N, C, H, W) format. N is the number of samples/batch_size. C is the channels. H and W are height and width resp.\n        # For conv1D, input should be (N,C,L) see documentation at\n        # N is a batch size, C denotes a number of channels, L is a length of signal sequence.    \n        \n        self.dropout = nn.Dropout(p_drop)\n        self.conv = nn.Conv1d(config.hidden_size, m_size , kernel_size = 2) # padding='same'\n        self.conv2 = nn.Conv1d(config.hidden_size, m_size, kernel_size = 2) # padding='same'\n        \n        #self.dropout2 = nn.Dropout(p_drop)\n        \n        self.fc = nn.Linear(m_size, 1)\n        nn.init.normal_(self.fc.weight, std=l_std)\n        nn.init.normal_(self.fc.bias, 0)\n\n        self.fc2 = nn.Linear(m_size, 1)\n        nn.init.normal_(self.fc2.weight, std=l_std)\n        nn.init.normal_(self.fc2.bias, 0)\n\n    def forward(self, input_ids, attention_mask):\n\n        _, _, hs = self.roberta(input_ids, attention_mask)\n        x = torch.stack([hs[-1], hs[-2], hs[-3]]) \n        x = torch.mean(x, 0)                      \n        x = self.dropout(x).transpose(1,2)\n        \n        #print(x.size()) # torch.Size([32, 768, 96])\n        \n        x1 = F.leaky_relu(self.conv(x)).transpose(1,2)                  \n        x2 = F.leaky_relu(self.conv2(x)).transpose(1,2)\n        \n        #start_logits, end_logits = x.split(1, dim=-1)\n\n        #print(x1.size()) # torch.Size([32, 95, 128])\n        #print(x2.size()) # torch.Size([32, 95, 128])\n        \n        start_logits = self.fc(x1).squeeze(-1)\n        end_logits = self.fc2(x2).squeeze(-1)\n        \n        #print(start_logits.size()) # torch.Size([32, 95, 2])\n        #print(end_logits.size())   # torch.Size([32, 95, 2])\n        \n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 93\nnum_epochs = 3\nbatch_size = 32\nn_splits = 10\np_drop = 0.1\nlearning_rate=3e-5\nl_std=0.03\nm_size=128\n\nprint(f'| param  |        |')\nprint(f'|--------|--------|')\nprint(f'| seed   |  {seed}    |')\nprint(f'| epochs |   {num_epochs}    |')\nprint(f'| batch  |  {batch_size}    |')\nprint(f'| splits |  {n_splits}    |')\nprint(f'| p_drop |   {p_drop}  |')\nprint(f'| lr     | {learning_rate}  |')\nprint(f'| std    |  {l_std}  |')\nprint(f'| m_size | {m_size}    |')\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n# %%time\n\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntrain_df['text'] = train_df['text'].astype(str)\ntrain_df['selected_text'] = train_df['selected_text'].astype(str)\n\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df, batch_size)\n\ns_logits, e_logits = init_logits(test_loader)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment)): \n    print_and_log(f'Fold: {fold+1}')\n    print('| Epoch |       |  Loss  | Jaccard |')\n    print('|-------|-------|--------|---------|')\n\n    model = TweetModel(p_drop, l_std, m_size)\n    # https://www.fast.ai/2018/07/02/adam-weight-decay/\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n    criterion = loss_fn    \n    dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n\n    train_model(\n        model, \n        dataloaders_dict,\n        criterion, \n        optimizer, \n        num_epochs)\n\n    for i, data in enumerate(test_loader):\n        ids = data['ids'] #.cuda()\n        masks = data['masks'] #.cuda()\n        \n        if e_cuda:\n            ids = ids.cuda()\n            masks = masks.cuda()\n\n        with torch.no_grad():\n            output = model(ids, masks)\n            s_logits[i].append(torch.softmax(output[0], dim=1).cpu().detach().numpy())\n            e_logits[i].append(torch.softmax(output[1], dim=1).cpu().detach().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save('s_logits.npy', s_logits)\nnp.save('e_logits.npy', e_logits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\npredictions = []\n\nfor i, data in enumerate(test_loader):\n    ids = data['ids'].cuda()\n    masks = data['masks'].cuda()\n    tweet = data['tweet']\n    offsets = data['offsets'].numpy()\n    start_logits = np.mean(s_logits[i], axis=0)\n    end_logits = np.mean(e_logits[i], axis=0)\n    for i in range(len(ids)):    \n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred > end_pred:\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n        predictions.append(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsub_df['selected_text'] = predictions\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}