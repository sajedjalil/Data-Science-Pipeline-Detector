{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os \nimport re\nimport emoji\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\ndf=pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_value_of_data(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    precent=round(total/data.shape[0]*100,2)\n    return pd.concat([total,precent],axis=1,keys=['Total','Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_value_of_data(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_values_in_columns(data,feature):\n    total=data.loc[:,feature].value_counts()\n    percentage = round(data.loc[:,feature].value_counts(normalize=True)*100,2)\n    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_values_in_columns(df,'sentiment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique_value_in_column(data,feature):\n    unique_value=pd.Series(data.loc[:,feature].unique())\n    return pd.concat([unique_value],axis=1,keys=['Unique_values'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_value_in_column(df,'sentiment')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def duplicated_values_data(data):\n    dup=[]\n    columns=data.columns\n    for i in columns:\n        dup.append(sum(data[i].duplicated()))\n    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated_values_data(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_url(string):\n    try:\n        text = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n    except:\n        text=[]\n    return \"\".join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence=\"Hello this is https://www.google.com\"\nfind_url(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['url'] = df['text'].apply(lambda x : find_url(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_emoji(text):\n    emo_text=emoji.demojize(text)\n    line=re.findall(r'\\:(.*?)\\:',emo_text)\n    return line","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence=\"I love âš½ very much ðŸ˜\"\nfind_emoji(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['emoji'] = df['text'].apply(lambda x: find_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence=\"Its all about \\U0001F600 face\"\nprint(sentence)\nremove_emoji(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stop_word_fn(text):\n    stop_words=set(stopwords.words('english'))\n    word_tokens= word_tokenize(text)\n    non_stop_word=[w for w in word_tokens if not w in stop_words ]\n    stop_words = [w for w in word_tokens if w in stop_words]\n    return stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\nstop_word_fn(example_sent)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['stop_words']=df['text'].apply(lambda x : stop_word_fn(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**start**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1,str2):\n    a=set(str1.lower().split())\n    b=set(str2.lower().split())\n    c=a.intersection(b)\n    return float(len(c))/(len(a)+ len(b)+ len(c))\ns1=\"Lets Play with the jaccard metrics whether its working\"\ns2=\"jaccard metrics\"\ns3 = \"play working with metrics\"\nprint(jaccard(s1,s2))\nprint(jaccard(s1,s3))\nprint(jaccard(s2,s3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib_venn as venn\nimport seaborn as sns\nfrom tqdm import tqdm\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nfrom collections import defaultdict,Counter\nfrom sklearn import preprocessing,model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nstop = set(stopwords.words('english'))\nfrom wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n#plotly libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nfrom datetime import datetime as dt\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntrain.sample(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna()\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'] = train['selected_text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target_url'] =train['target'].apply(lambda x : find_url(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_star(text):\n    try:\n        line=re.findall(r'[*]{2,5}',text)\n    except:\n        line=[]\n    return len(line)\ntrain['star']=train['target'].apply(lambda x:find_star(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_only_star(text):\n    try:\n        if len(text.split())==1:\n            line=re.findall(r'[*]{2,5}',text)\n            return len(line)\n        else:\n            return 0\n    except:\n        return 0\ntrain['only_star']=train['target'].apply(lambda x:find_only_star(x))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target']= np.where(train['only_star']==1,\"abusive\",train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_link(string):\n    try:\n        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \",string)\n    except:\n        text=''\n    return \" \".join(text.split())\ndef remove_punct(text):\n    try:\n        line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]+',\" \",text)\n    except:\n        line=''\n    return \" \".join(line.split())\ntrain['target']=train['target'].apply(lambda x:remove_link(x))\ntrain['target']=train['target'].apply(lambda x:remove_punct(x))\ntrain['target_tweet_length']=train['target'].str.split().map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target_average_word_len']=train['target'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data=pd.concat([train,test])\nfull_data['text']=full_data['text'].str.lower()\nfull_data.shape\nfull_data['text']=full_data['text'].apply(lambda x:remove_link(x))\nfull_data['text']=full_data['text'].apply(lambda x:remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data.loc[full_data['text']==\"\",['text']]=\"nothing\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_data['text_tweet_length']=full_data['text'].str.split().map(lambda x: len(x))\nfull_data['text_average_word_len']=full_data['text'].str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def corpus_sentiment_stop(data,feature,sentiment):\n    corpus=create_corpus(data,feature,sentiment)\n    dic=defaultdict(int)\n    for word in corpus:\n        if word in stop:\n            dic[word]+=1\n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)\n    x,y=zip(*top)\n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we dont have length larger than 96\nMAX_LEN = 96\n\n# Pretrained model of roberta\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\n\n# Sentiment ID value is encoded from tokenizer\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\nct=train.shape[0] #27481\n\n\ninput_ids=np.ones((ct,MAX_LEN),dtype=\"int32\")          \nattention_mask=np.zeros((ct,MAX_LEN),dtype=\"int32\")    \ntoken_type_ids=np.zeros((ct,MAX_LEN),dtype=\"int32\")    \nstart_tokens=np.zeros((ct,MAX_LEN),dtype=\"int32\")  \nend_tokens=np.zeros((ct,MAX_LEN),dtype=\"int32\")\nfor k in range(train.shape[0]):\n#1 FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    \n    # idx - position where the selected text are placed. \n    idx = text1.find(text2)   # we get [12] position\n    \n    # all character position as 0 and then places 1 for selected text position  \n    chars = np.zeros((len(text1))) \n    chars[idx:idx+len(text2)]=1    # [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] \n    \n    #tokenize id of text \n    if text1[idx-1]==' ': chars[idx-1] = 1    \n    enc = tokenizer.encode(text1)  #  [127, 3504, 16, 11902, 162]\n        \n#2. ID_OFFSETS - start and end index of text\n    offsets = []\n    idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))     #  [(0, 3), (3, 8), (8, 11), (11, 20), (20, 23)]\n        idx += len(w) \n    \n#3  START-END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b]) # number of characters in selected text - [0.0,0.0,0.0,9.0,3.0] - bullying me\n        if sm>0: \n            toks.append(i)  # token position - selected text - [3, 4]\n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']] # Encoded values by tokenizer\n    \n    #Formating input for roberta model\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]   #[ 0   127  3504    16 11902   162     2     2  2430     2]\n    attention_mask[k,:len(enc.ids)+5] = 1                                  # [1 1 1 1 1 1 1 1 1 1]\n    \n    if len(toks)>0:\n        # this will produce (27481, 96) & (27481, 96) arrays where tokens are placed\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct_test = test.shape[0]\n\n# Initialize inputs\ninput_ids_t = np.ones((ct_test,MAX_LEN),dtype='int32')        # array with value 1 for shape (3534, 96)\nattention_mask_t = np.zeros((ct_test,MAX_LEN),dtype='int32')  # array with value 0 for shape (3534, 96)\ntoken_type_ids_t = np.zeros((ct_test,MAX_LEN),dtype='int32')  # array with value 0 for shape (3534, 96)\n\n# Set Inputs attention \nfor k in range(test.shape[0]):\n        \n#1. INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n     \n    # Encoded value of tokenizer\n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    \n    #setting up of input ids - same as we did for train\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    return 3e-5 * 0.2**epoch\ndef build_model():\n    \n    # Initialize keras layers\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    # Fetching pretrained models \n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    # Setting up layers\n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    # Initializing input,output for model.THis will be trained in next code\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    \n    #Adam optimizer for stochastic gradient descent. if you are unware of it - https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits=5 # Number of splits\n\n# INitialize start and end token\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nDISPLAY=1\nfor i in range(5):\n    print('#'*40)\n    print('### MODEL %i'%(i+1))\n    print('#'*40)\n    \n    K.clear_session()\n    model = build_model()\n    # Pretrained model\n    model.load_weights('../input/model4/v4-roberta-%i.h5'%i)\n\n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/n_splits\n    preds_end += preds[1]/n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\n\nfor k in range(input_ids_t.shape[0]):\n    # Argmax - Returns the indices of the maximum values along axis\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\ntest['selected_text'] = all\nsubmission=test[['textID','selected_text']]\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}