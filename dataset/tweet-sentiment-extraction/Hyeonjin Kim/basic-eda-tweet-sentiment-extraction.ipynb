{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport string\nimport operator\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import STOPWORDS\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom wordcloud import WordCloud\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n#plt.style.use('fivethirtyeight')\\","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Check Missing value","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.dropna()\ntest_df = test_df.dropna() # Although there is no missing value in test dataset, perform the dropna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Number of Labels","metadata":{}},{"cell_type":"code","source":"x=train_df.sentiment.value_counts()\nax = sns.barplot(x.index,x)\nfor i, v in enumerate(x.iteritems()):        \n    ax.text(i ,v[1], \"{:,}\".format(v[1]), ha='center', va ='bottom', fontsize=10, color='black', rotation=0)\nplt.gca().set_ylabel('tweets')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Number of characters","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\ntrain_len = train_df[train_df['sentiment']=='neutral']['text'].str.len()\nax1.hist(train_len,color='red')\nax1.set_title('Neutral tweets')\n\ntrain_len = train_df[train_df['sentiment']=='positive']['text'].str.len()\nax2.hist(train_len,color='blue')\nax2.set_title('Positive tweets')\n\ntrain_len = train_df[train_df['sentiment']=='negative']['text'].str.len()\nax3.hist(train_len, color='green')\nax3.set_title('Negative tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\nsns.kdeplot(train_df[train_df['sentiment']=='neutral']['text'].str.len())\nsns.kdeplot(train_df[train_df['sentiment']=='positive']['text'].str.len())\nsns.kdeplot(train_df[train_df['sentiment']=='negative']['text'].str.len())\nplt.title(\"Distribution of Tweets\")\nax.legend(labels=[\"Neutral\",\"Positive\",\"Negative\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* #### Number of words","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\ntrain_len = train_df[train_df['sentiment']=='neutral']['text'].str.split().map(lambda x: len(x))\nax1.hist(train_len,color='red')\nax1.set_title('Neutral tweets')\n\ntrain_len = train_df[train_df['sentiment']=='positive']['text'].str.split().map(lambda x: len(x))\nax2.hist(train_len,color='blue')\nax2.set_title('Positive tweets')\n\ntrain_len = train_df[train_df['sentiment']=='negative']['text'].str.split().map(lambda x: len(x))\nax3.hist(train_len, color='green')\nax3.set_title('Negative tweets')\nfig.suptitle('Words in tweets')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,5))\nsns.kdeplot(train_df[train_df['sentiment']=='neutral']['text'].str.split().map(lambda x: len(x)))\nsns.kdeplot(train_df[train_df['sentiment']=='positive']['text'].str.split().map(lambda x: len(x)))\nsns.kdeplot(train_df[train_df['sentiment']=='negative']['text'].str.split().map(lambda x: len(x)))\nplt.title(\"Distribution of Tweets\")\nax.legend(labels=[\"Neutral\",\"Positive\",\"Negative\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Meta data analysis\nThe dataset is needed to be cleaned. To check the effect of cleaning, we perform the data analysis before.<br>\n[Reference: NLP with Disaster Tweets](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)","metadata":{}},{"cell_type":"markdown","source":"* Meta data","metadata":{}},{"cell_type":"code","source":"# word_count\ntrain_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['word_count'] = test_df['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ntrain_df['stop_word_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df['stop_word_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ntrain_df['url_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest_df['url_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ntrain_df['mean_word_length'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df['mean_word_length'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ntrain_df['char_count'] = train_df['text'].apply(lambda x: len(str(x)))\ntest_df['char_count'] = test_df['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ntrain_df['punctuation_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df['punctuation_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ntest_df['hashtag_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ntrain_df['mention_count'] = train_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ntest_df['mention_count'] = test_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n\nneu = train_df['sentiment'] == 'neutral'\npos = train_df['sentiment'] == 'positive'\nneg = train_df['sentiment'] == 'negative'\n\nfig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(30, 50))\n\nfor i, feature in enumerate(METAFEATURES):\n    sns.distplot(train_df.loc[neu][feature], label='Neutral', ax=axes[i][0], color='green')\n    sns.distplot(train_df.loc[pos][feature], label='Positive', ax=axes[i][0], color='blue')\n    sns.distplot(train_df.loc[neg][feature], label='Negative', ax=axes[i][0], color='red')\n\n    sns.distplot(train_df[feature], label='Training', ax=axes[i][1], color='blue')\n    sns.distplot(test_df[feature], label='Test', ax=axes[i][1], color='yellow')\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=10)\n        axes[i][j].tick_params(axis='y', labelsize=10)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* N-gram","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ngram_cal(df, n_gram):\n    neutral_ngrams = defaultdict(int)\n    positive_ngrams = defaultdict(int)\n    negative_ngrams = defaultdict(int)\n    \n    for tweet in df[neu]['text']:\n        for word in generate_ngrams(tweet, n_gram=n_gram):\n            neutral_ngrams[word] += 1\n        \n    for tweet in df[pos]['text']:\n        for word in generate_ngrams(tweet, n_gram=n_gram):\n            positive_ngrams[word] += 1\n            \n    for tweet in df[neg]['text']:\n        for word in generate_ngrams(tweet, n_gram=n_gram):\n            negative_ngrams[word] += 1\n    \n    df_neutral_ngrams = pd.DataFrame(sorted(neutral_ngrams.items(), key=lambda x: x[1])[::-1])\n    df_positive_ngrams = pd.DataFrame(sorted(positive_ngrams.items(), key=lambda x: x[1])[::-1])\n    df_negative_ngrams = pd.DataFrame(sorted(negative_ngrams.items(), key=lambda x: x[1])[::-1])\n    \n    return df_neutral_ngrams, df_positive_ngrams, df_negative_ngrams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ngram_plot(df_gram1, df_gram2, df_gram3, U):\n    fig, axes = plt.subplots(ncols=3, figsize=(30, 10))\n    plt.tight_layout()\n\n    sns.barplot(y=df_gram1[0].values[:N], x=df_gram1[1].values[:N], ax=axes[0], color='green')\n    sns.barplot(y=df_gram2[0].values[:N], x=df_gram2[1].values[:N], ax=axes[1], color='blue')\n    sns.barplot(y=df_gram3[0].values[:N], x=df_gram3[1].values[:N], ax=axes[2], color='red')\n\n    for i in range(3):\n        axes[i].spines['right'].set_visible(False)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n        axes[i].tick_params(axis='x', labelsize=10)\n        axes[i].tick_params(axis='y', labelsize=10)\n\n    axes[0].set_title(f'Top {N} most common {U}-grams in Neutral Tweets', fontsize=15)\n    axes[1].set_title(f'Top {N} most common {U}-grams in Positive Tweets', fontsize=15)\n    axes[2].set_title(f'Top {N} most common {U}-grams in Negative Tweets', fontsize=15)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"N-grams in Train dataset","metadata":{}},{"cell_type":"code","source":"a, b, c = ngram_cal(train_df, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(train_df, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(train_df, 3)\nngram_plot(a, b, c, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"N-grams in Test dataset","metadata":{}},{"cell_type":"code","source":"neu = test_df['sentiment'] == 'neutral'\npos = test_df['sentiment'] == 'positive'\nneg = test_df['sentiment'] == 'negative'\n\na, b, c = ngram_cal(test_df, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(test_df, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(test_df, 3)\nngram_plot(a, b, c, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data cleaning","metadata":{}},{"cell_type":"markdown","source":"* Combine Dataset <br>\nBefore data cleaning, to perform at once, train dataset and test dataset are combined.<br>\nThe label in train dataset is seperated to <code>label</code>.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperate Label\nlabel = train_df['selected_text'].values\ntrain_df = train_df.drop(['selected_text'], axis=1)\n\n# Drop useless data from ngrams\n#METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n#                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\ntrain_df = train_df.drop(METAFEATURES, axis=1)\ntest_df = test_df.drop(METAFEATURES, axis=1)\n\n# Drop the irrevelant parameter\ntrain_df = train_df.drop(['textID'], axis=1)\ntest_df = test_df.drop(['textID'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine dataset\ndf = pd.concat([train_df, test_df], axis=0)\nprint('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))\nprint('There are {} rows and {} columns in total'.format(df.shape[0],df.shape[1]))\n\n# Save the size of train and test dataset\ntrain_size = train_df.shape[0]\ntest_size = test_df.shape[0]\n\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Remove Url / Html / Emoji / Punct","metadata":{}},{"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : remove_URL(x))\ndf['text'] = df['text'].apply(lambda x : remove_html(x))\ndf['text'] = df['text'].apply(lambda x : remove_emoji(x))\ndf['text'] = df['text'].apply(lambda x : remove_punct(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Remove Tags","metadata":{}},{"cell_type":"code","source":"def remove_tags(text):\n    tag_pattern = re.compile(r'[@|#][^\\s]+')\n    return tag_pattern.sub(r'',text)\n#    return text + ' ' + ' '.join(tags) + ' '+ ' '.join(tags) + ' ' + ' '.join(tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : remove_tags(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Remove Stopwords","metadata":{}},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nstemmer  = SnowballStemmer('english')\nstopword = stopwords.words('english')\n\ndef Remove_StopAndStem(text):\n    string_list = text.split()\n    return ' '.join([stemmer.stem(i) for i in string_list if i not in stopword])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(Remove_StopAndStem)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Word Cloud","metadata":{}},{"cell_type":"code","source":"dict_of_words = {}\nfor row in  df.itertuples():\n    for i in row[1].split():\n        try:\n            dict_of_words[i] += 1\n        except:\n            dict_of_words[i] = 1\n\n#Initializing  WordCloud\nwordcloud = WordCloud(background_color = 'black', width=1000, height=500).generate_from_frequencies(dict_of_words)\nfig = plt.figure(figsize=(10,5))\nplt.imshow(wordcloud)\nplt.tight_layout(pad=1)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Cleaning result","metadata":{}},{"cell_type":"code","source":"re_train = df[:train_size]\nre_test = df[train_size:]\n\nprint('There are {} rows and {} columns in train'.format(re_train.shape[0],re_train.shape[1]))\nprint('There are {} rows and {} columns in test'.format(re_test.shape[0],re_test.shape[1]))\nprint('Original train dataset : {} \\nOriginal test dataset: {}'.format(train_size, test_size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Train dataset","metadata":{}},{"cell_type":"code","source":"neu = re_train['sentiment'] == 'neutral'\npos = re_train['sentiment'] == 'positive'\nneg = re_train['sentiment'] == 'negative'\n\na, b, c = ngram_cal(re_train, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(re_train, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(re_train, 3)\nngram_plot(a, b, c, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Test dataset","metadata":{}},{"cell_type":"code","source":"neu = re_test['sentiment'] == 'neutral'\npos = re_test['sentiment'] == 'positive'\nneg = re_test['sentiment'] == 'negative'\n\na, b, c = ngram_cal(re_test, 1)\nngram_plot(a, b, c, 1)\na, b, c = ngram_cal(re_test, 2)\nngram_plot(a, b, c, 2)\na, b, c = ngram_cal(re_test, 3)\nngram_plot(a, b, c, 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Split Train and Validation dataset","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(re_train, label, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(min_df = 0.0005, \n                             max_features = 100000, \n                             tokenizer = lambda x: x.split(),\n                             ngram_range = (1,4))\n\n\nX_train = vectorizer.fit_transform(X_train['text'])\nX_valid = vectorizer.transform(X_valid['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training Points: \", len(X_train.toarray()),\"| Training Features:\" , len(X_train.toarray()[0]))\nprint(\"Testing Points: \", len(X_valid.toarray()),\"| Testing Features:\" , len(X_valid.toarray()[0]))\nprint()\nprint(\"Training Points: \", len(y_train))\nprint(\"Testing Points: \", len(y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}