{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RoBERTa - Modelo HuggingFace preentrenado TF "},{"metadata":{},"cell_type":"markdown","source":"Tratando de mejorar la primera implementación de BERT con DistilBERT, usaremos el modelo entrenado de RoBERTa y la librería transformers. El modelo pre-entrenado de RoBERTa elegido será dla implementación de HuggingFace preentrenado en tensorflow.."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS \nimport re \nimport string\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Input,Flatten,Embedding,Lambda,Dropout,LSTM,Conv1D,Concatenate,Add\nfrom tensorflow.keras.models import Model\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nimport keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import  RobertaConfig, TFRobertaModel\nfrom tokenizers import Tokenizer\nfrom tokenizers.decoders import ByteLevel as ByteLevelDecoder\nfrom tokenizers.models import BPE\nfrom tokenizers.normalizers import Lowercase, NFKC, Sequence\nfrom tokenizers.pre_tokenizers import ByteLevel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Referencia https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert\n\ndef clean(tweet):\n    tweet = str(tweet)\n\n    tweet=tweet.lower()\n\n    #Remove html tags\n    tweet=re.sub('<.*?>','',tweet)\n\n    #Remove text in square brackets\n    tweet=re.sub('\\[.*?\\]','',tweet)\n\n    #Remove hyperlinks\n    tweet=re.sub('https?://\\S+|www\\.\\S+','',tweet)\n\n\n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace = True)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : x.strip())\ntrain[\"selected_text\"] = train[\"selected_text\"].apply(lambda x : x.strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,Y_train,Y_val=train_test_split(train,train['sentiment'],\n                                              test_size=0.2,random_state=42,stratify=train['sentiment'])\nX_train,X_test,Y_train,Y_test=train_test_split(X_train,Y_train,\n                                               test_size=0.2,random_state=42,stratify=X_train['sentiment'])\n\nX_train.reset_index(inplace=True,drop=True)\nX_val.reset_index(inplace=True,drop=True)\nX_test.reset_index(inplace=True,drop=True)\n\nY_train=Y_train.reset_index(drop=True)\nY_val=Y_val.reset_index(drop=True)\nY_test=Y_test.reset_index(drop=True)\n\nprint('X_train shape',X_train.shape,' Y_train shape ',Y_train.shape)\nprint('X_val shape',X_val.shape,' Y_val shape ',Y_val.shape)\nprint('X_test shape',X_test.shape,' Y_test shape ',Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Definición de variables generales\nMAX_LEN = 128\ntokenizer = Tokenizer(BPE.from_file('../input/tf-roberta/vocab-roberta-base.json', '../input/tf-roberta/merges-roberta-base.txt'))\ntokenizer.pre_tokenizer = ByteLevel()\ntokenizer.decoder = ByteLevelDecoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resPos = tokenizer.encode('How I new that thing')\nresNeg = tokenizer.encode('negative')\nresNeu = tokenizer.encode('neutral')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Representación ID positive: {resPos.ids}')\nprint(f'Representación ID negative: {resNeg.ids}')\nprint(f'Representación ID neutral: {resNeu.ids}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.decode(resPos.ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definición de sentimientos basados en el diccionario de RoBERTa\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Referencia: https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705\n\ndef createInputData(data,tokenizer):\n\n    row = data.shape[0]\n    input_ids = np.ones((row,MAX_LEN),dtype='int32')\n    attention_mask = np.zeros((row,MAX_LEN),dtype='int32')\n    token_type_ids = np.zeros((row,MAX_LEN),dtype='int32')\n    start_tokens = np.zeros((row,MAX_LEN),dtype='int32')\n    end_tokens = np.zeros((row,MAX_LEN),dtype='int32')\n\n    for k in range(data.shape[0]):\n        # Búsqueda del indice inicial\n        text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n        text2 = \" \".join(data.loc[k,'selected_text'].split())\n        idx = text1.find(text2)\n        # Selección de las posiciones que ocupan los textos\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n\n        if text1[idx-1]==' ': \n            chars[idx-1] = 1 \n\n        # Codificación del texto completo\n        enc = tokenizer.encode(text1) \n\n        # Encuentro de offsets\n        token_offsets=[]\n        idx=0\n        for i in enc.ids:\n            word=tokenizer.decode([i])\n            token_offsets.append((idx,idx+len(word)))\n            idx+=len(word)\n\n        # Definición de tokens de inicio y finalización\n        target_idx = []\n        for i,(o1,o2) in enumerate(token_offsets):\n            if(sum(chars[o1:o2])>0):\n                target_idx.append(i)  \n        s_tok = sentiment_id[data.loc[k,'sentiment']]\n\n        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n        attention_mask[k,:len(enc.ids)+5] = 1\n\n        #Se adiciona 1 a los tokens\n        if len(target_idx)>0:\n            start_tokens[k,target_idx[0]+1] = 1\n            end_tokens[k,target_idx[-1]+1] = 1\n\n    return (input_ids,attention_mask,token_type_ids,start_tokens,end_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Convertir los datos de validación y pruebas en la entrada de RoBERTa\nX_tr1,X_tr2,X_tr3,Y_tr1,Y_tr2=createInputData(X_train,tokenizer)\nX_val1,X_val2,X_val3,Y_val1,Y_val2=createInputData(X_val,tokenizer)\nX_te1,X_te2,X_te3,Y_te1,Y_te2=createInputData(X_test,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Arquitectura de red reunal con RoBERTa\ndef build_model():\n    '''Builds the model'''\n\n    ids=Input((MAX_LEN),name='ids',dtype='int32')\n    att_mask=Input((MAX_LEN),name='att_mask',dtype='int32')\n    type_ids=Input((MAX_LEN),name='type_ids',dtype='int32')\n\n    roberta_conf = RobertaConfig.from_pretrained('roberta-base')\n    roberta_model = TFRobertaModel.from_pretrained('roberta-base',config=roberta_conf)\n\n    bert_output=roberta_model([ids,att_mask,type_ids])\n\n    dropout1=Dropout(0.1,name='dropout1')(bert_output[0])\n    conv1d_1 = Conv1D(1,1,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=20),name='conv1d_1')(dropout1)\n    flatten_1 = Flatten(name='flatten_1')(conv1d_1)\n    out_1 = tf.keras.layers.Activation('softmax',name='activation1')(flatten_1)\n\n    dropout2=Dropout(0.1,name='dropout2')(bert_output[0])\n    conv1d_2 = Conv1D(1,1,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=20),name='conv1d_2')(dropout2)\n    flatten_2 = Flatten(name='flatten_2')(conv1d_2)\n    out_2 = tf.keras.layers.Activation('softmax',name='activation2')(flatten_2)\n\n    model1 = Model(inputs=[ids, att_mask, type_ids], outputs=[out_1,out_2])\n\n    return model1\n  \nmodel=build_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, './ModeloRobertaRN.png',show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nif not os.path.exists('./model-roberta'):\n    os.makedirs('./model-roberta')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parámetros de RoBERTA para tensorboard\nfrom tensorflow.keras.callbacks import TensorBoard\n%load_ext tensorboard\n!rm -rf ./logs/ \n\nlog_dir='./model-roberta'\ntensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint=ModelCheckpoint('./model-roberta/roberta.h5', monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compilación del modelo con optimizador Adam\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entrenamiento del modelo\ncallback=[tensorboard_callback,checkpoint]\nmodel.fit([X_tr1,X_tr2,X_tr3],[Y_tr1,Y_tr2],\n           validation_data=([X_val1,X_val2,X_val3],[Y_val1,Y_val2]),\n           batch_size=32,epochs=4,callbacks=callback)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predecir con los datos de texto\nstart,end=model.predict([X_te1,X_te2,X_te3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef find_selected_text(data,tokenizer,start,end):\n    '''Finds the selected text for the given tweet'''\n    selected_text_list=[]\n    for i in range(data.shape[0]):\n\n        # Búsqueda de index\n        start_idx=np.argmax(start[i])\n        end_idx=np.argmax(end[i])\n\n\n        # Encuentra el texto de la predicción a partir de los indices\n        if (start_idx>end_idx):\n            predicted_text=data.loc[i,'text']\n\n        else:\n            text1 = \" \"+\" \".join(data.loc[i,'text'].split())\n            tokens=tokenizer.encode(text1)\n            predicted_text=tokenizer.decode(tokens.ids[start_idx-1:end_idx])        \n\n        selected_text_list.append(predicted_text)\n\n    return selected_text_list\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_text=find_selected_text(X_test,tokenizer,start,end)\nX_test['predicted_text']=selected_text\n\nfor i,(_,row) in enumerate(X_test.iterrows()):\n    X_test.loc[i,'jaccard']=jaccard(row.selected_text,row.predicted_text)\n\nX_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Promedio del indice de Jaccard\n\npos_average=np.mean(X_test['jaccard'][X_test['sentiment']=='positive'])\nprint('Promedio del indice de jaccard para los sentimientos positivos  ',pos_average)\n\nneg_average=np.mean(X_test['jaccard'][X_test['sentiment']=='negative'])\nprint('Promedio del indice de jaccard para los sentimientos negativos  ',neg_average)\n\nneu_average=np.mean(X_test['jaccard'][X_test['sentiment']=='neutral'])\nprint('Promedio del indice de jaccard para los sentimientos neutrales  ',neu_average)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rendimiento general para el modelo de RoBERTa\nprint(np.mean(X_test['jaccard']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}