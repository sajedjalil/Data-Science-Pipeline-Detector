{"cells":[{"metadata":{},"cell_type":"markdown","source":"# T5 text-to-text transformer\n\nHey guys I made this tutorial for all NLP lovers who are intrested to try current state of the art T5 transformer. For those who don't know what T5 is, here is the\noriginal paper link https://arxiv.org/abs/1910.10683. If you don't want to go through the whole paper check this [TowardDataScience article](https://towardsdatascience.com/t5-text-to-text-transformer-a-brief-paper-analysis-e4bba797bd68).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"T5 is text to text transformer aimed to take text input and text output.What's special about it is you can solve any NLP problem using it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this tutorial I will try how to use T5 for text extraction.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers\n!pip install pytorch_lightning","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# T5 fine-tuning\nThis notebook is to showcase how to fine-tune T5 model with Huggigface's Transformers to solve different NLP tasks using text-2-text approach proposed in the T5 paper. For demo I chose 3 non text-2-text problems just to reiterate the fact from the paper that how widely applicable this text-2-text framework is and how it can be used for different tasks without changing the model at all.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libraries\nimport argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\n\n\nfrom transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_linear_schedule_with_warmup\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_seed(seed):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\nWe'll be using the awesome pytorch-lightning library for training. Most of the below code is adapted from here https://github.com/huggingface/transformers/blob/master/examples/lightning_base.py\n\nThe trainer is generic and can be used for any text-2-text task. You'll just need to change the dataset. Rest of the code will stay unchanged for all the tasks.\n\nThis is the most intresting and powrfull thing about the text-2-text format. You can fine-tune the model on variety of NLP tasks by just formulating the problem in text-2-text setting. No need to change hyperparameters, learning rate, optimizer or loss function. Just plug in your dataset and you are ready to go!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class T5FineTuner(pl.LightningModule):\n  def __init__(self, hparams):\n    super(T5FineTuner, self).__init__()\n    self.hparams = hparams\n    \n    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n  \n  def is_logger(self):\n    return self.trainer.proc_rank <= 0\n  \n  def forward(\n      self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n  ):\n    return self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        lm_labels=lm_labels,\n    )\n\n  def _step(self, batch):\n    lm_labels = batch[\"target_ids\"]\n    lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n    outputs = self(\n        input_ids=batch[\"source_ids\"],\n        attention_mask=batch[\"source_mask\"],\n        lm_labels=lm_labels,\n        decoder_attention_mask=batch['target_mask']\n    )\n\n    loss = outputs[0]\n\n    return loss\n\n  def training_step(self, batch, batch_idx):\n    loss = self._step(batch)\n\n    tensorboard_logs = {\"train_loss\": loss}\n    return {\"loss\": loss, \"log\": tensorboard_logs}\n  \n  def training_epoch_end(self, outputs):\n    avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n    tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n    return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n  def validation_step(self, batch, batch_idx):\n    loss = self._step(batch)\n    return {\"val_loss\": loss}\n  \n  def validation_epoch_end(self, outputs):\n    avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n    tensorboard_logs = {\"val_loss\": avg_loss}\n    return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n  def configure_optimizers(self):\n    \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n    model = self.model\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": self.hparams.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n    self.opt = optimizer\n    return [optimizer]\n  \n  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n    if self.trainer.use_tpu:\n      xm.optimizer_step(optimizer)\n    else:\n      optimizer.step()\n    optimizer.zero_grad()\n    self.lr_scheduler.step()\n  \n  def get_tqdm_dict(self):\n    tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n\n    return tqdm_dict\n\n  def train_dataloader(self):\n    train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n    dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n    t_total = (\n        (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n        // self.hparams.gradient_accumulation_steps\n        * float(self.hparams.num_train_epochs)\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n    )\n    self.lr_scheduler = scheduler\n    return dataloader\n\n  def val_dataloader(self):\n    val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n    return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n  def on_validation_end(self, trainer, pl_module):\n    logger.info(\"***** Validation results *****\")\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n      # Log results\n      for key in sorted(metrics):\n        if key not in [\"log\", \"progress_bar\"]:\n          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n  def on_test_end(self, trainer, pl_module):\n    logger.info(\"***** Test results *****\")\n\n    if pl_module.is_logger():\n      metrics = trainer.callback_metrics\n\n      # Log and save results to file\n      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n      with open(output_test_results_file, \"w\") as writer:\n        for key in sorted(metrics):\n          if key not in [\"log\", \"progress_bar\"]:\n            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define the hyperparameters and other arguments. You can overide this dict for specific task as needed. While in most of cases you'll only need to change the data_dirand output_dir.\n\nHere the batch size is 8 and gradient_accumulation_steps are 16 so the effective batch size is 128","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"args_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='t5-base',\n    tokenizer_name_or_path='t5-base',\n    max_seq_length=512,\n    learning_rate=3e-4,\n    weight_decay=0.0,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=8,\n    eval_batch_size=8,\n    num_train_epochs=2,\n    gradient_accumulation_steps=16,\n    n_gpu=1,\n    early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=42,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\n/kaggle/input/tweetextract/val.csv\n\n\n/kaggle/input/tweetextract/test.csv\n\n\n/kaggle/input/tweetextract/train.csv ```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I divided the training data into train and val with 22k and 5k rows and uploaded it on tweetextract folder.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nfrom dataclasses import dataclass\n\nfrom enum import Enum\nfrom typing import List, Optional\nfrom transformers import PreTrainedTokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The only thing you need to understand is how to dataset as input.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@dataclass(frozen=True)\nclass InputExample:\n  example_id : str\n  text : str\n  sentiment : str\n  label : str\n  \"\"\"\n    A single training/test example for multiple choice\n    Args:\n        example_id: Unique id for the example.\n        contexts: list of str. The untokenized text of the first sequence (context of corresponding question).\n        answer : str containing answer for which we need to generate question\n        label: string containg questions\n    \"\"\"\n\n\nclass Split(Enum):\n    train = \"train\"\n    dev = \"dev\"\n    test = \"test\"\n\nclass DataProcessor:\n    \"\"\"Base class for data converters for multiple choice data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\nclass SwagProcessor(DataProcessor):\n    \"\"\"Processor for the SWAG data set.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        logger.info(\"LOOKING AT {} train\".format(data_dir))\n        return self._create_examples(self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n        return self._create_examples(self._read_csv(os.path.join(data_dir, \"val.csv\")), \"dev\")\n\n    def get_test_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n        raise ValueError(\n            \"For swag testing, the input file does not contain a label column. It can not be tested in current code\"\n            \"setting!\"\n        )\n        return self._create_examples(self._read_csv(os.path.join(data_dir, \"test.csv\")), \"test\")\n\n    \n\n    def _read_csv(self, input_file):\n        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n            return list(csv.reader(f))\n\n    def _create_examples(self, lines: List[List[str]], type: str):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        if type == \"train\" and lines[0][2] != \"selected_text\":\n            raise ValueError(\"For training, the input file must contain a label column.\")\n\n        examples = [\n            InputExample(\n                example_id=line[0],\n                # common beginning of each\n                # choice is stored in \"sent2\".\n                text=line[1],\n                sentiment=line[3],\n                label=line[2]\n\n            )\n            for line in lines[1:]  # we skip the line with the column names\n        ]\n\n        return examples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(Dataset):\n  def __init__(self, tokenizer, data_dir, type_path,  max_len=512):\n    self.data_dir = data_dir\n    self.type_path = type_path\n    self.max_len = max_len\n    self.tokenizer = tokenizer\n    self.inputs = []\n    self.targets = []\n\n    self.proc = SwagProcessor()\n\n    self._build()\n  \n  def __getitem__(self, index):\n    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n    target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n  \n  def __len__(self):\n    return len(self.inputs)\n  \n  def _build(self):\n    if self.type_path == 'train':\n      examples = self.proc.get_train_examples(self.data_dir)\n    else:\n      examples = self.proc.get_dev_examples(self.data_dir)\n    \n    for example in examples:\n      self._create_features(example)\n  \n  def _create_features(self, example):\n    input_ = example.text\n    answer = example.sentiment\n    input_ = \"text: %s  sentiment: %s </s>\" % (input_, answer)\n    target = example.label\n    target = \"%s </s>\" % (str(target))\n\n    # tokenize inputs\n    tokenized_inputs = self.tokenizer.batch_encode_plus(\n        [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n    )\n    # tokenize targets\n    tokenized_targets = self.tokenizer.batch_encode_plus(\n        [target], max_length=150, pad_to_max_length=True, return_tensors=\"pt\"\n    )\n\n    self.inputs.append(tokenized_inputs)\n    self.targets.append(tokenized_targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained('t5-base')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = TweetDataset(tokenizer, data_dir='/kaggle/input/tweetextract/', type_path='val')\nlen(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = dataset[69]\nprint(tokenizer.decode(data['source_ids']))\nprint(tokenizer.decode(data['target_ids']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir /kaggle/working/t5_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /kaggle/working/t5_tweet\nargs_dict.update({'data_dir': '/kaggle/input/tweetextract/', 'output_dir': '/kaggle/working/t5_tweet/', 'num_train_epochs': 1})\nargs = argparse.Namespace(**args_dict)\nprint(args_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n)\n\ntrain_params = dict(\n    accumulate_grad_batches=args.gradient_accumulation_steps,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    early_stop_callback=False,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=checkpoint_callback,\n    callbacks=[LoggingCallback()],\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dataset(tokenizer, type_path, args):\n  return TweetDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = T5FineTuner(args)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(**train_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%env JOBLIB_TEMP_FOLDER=/tmp\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import textwrap\nfrom tqdm.auto import tqdm\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset =  TweetDataset(tokenizer, data_dir='/kaggle/input/tweetextract/', type_path='val')\nloader = DataLoader(dataset, batch_size=32, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.model.eval()\noutputs = []\ntargets = []\nfor batch in tqdm(loader):\n  outs = model.model.generate(input_ids=batch['source_ids'].cuda(), \n                              attention_mask=batch['source_mask'].cuda(), \n                              max_length=200)\n\n  dec = [tokenizer.decode(ids) for ids in outs]\n  target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n  \n  outputs.extend(dec)\n  targets.extend(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df = pd.read_csv(\"/kaggle/input/tweetextract/val.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df[\"new_selected_text\"] = outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}