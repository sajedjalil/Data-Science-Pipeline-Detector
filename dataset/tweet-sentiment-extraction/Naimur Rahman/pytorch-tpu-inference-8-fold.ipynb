{"cells":[{"metadata":{},"cell_type":"markdown","source":"I'm new in NLP and intending to create my notebooks with \n1. [PyTorch + GPU](https://www.kaggle.com/naimur978/pytorch-gpu-inference-5-fold)\n2. [PyTorch + TPU](https://www.kaggle.com/naimur978/pytorch-tpu-inference-8-fold)\n3. Tensorflow + GPU\n4. Tensorflow + TPU\n\nIn every notebook, Training and Inference sections are handled separetely. I've known a lot from notebooks shared by the kagglers especially from Abhishek and Cdeotte. Comments are attached so that it might helpful for the beginners. This notebook is separated into 3 sections -\n\n* [Labelling by stratified k-fold](https://www.kaggle.com/naimur978/pytorch-tpu-inference-8-fold?scriptVersionId=39501982)\n* [Training](https://www.kaggle.com/naimur978/pytorch-tpu-inference-8-fold)\n* Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport utils\nfrom joblib import Parallel, delayed\nimport torch_xla.core.xla_model as xm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To download RoBERTa base model -\n* go to https://huggingface.co/roberta-base#\n* Click on \"list all files in model\"\n* Download config, pytorch_model, merges, vocab files\n* and save it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class config:\n    LEARNING_RATE = 4e-5\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 50\n    VALID_BATCH_SIZE = 32\n    EPOCHS = 5\n    TRAINING_FILE = \"../input/tweet-8fold/train_8_folds.csv\"\n    ROBERTA_PATH = \"../input/roberta-base\"\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n        lowercase=True,\n        add_prefix_space=True\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Process Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Processes the tweet and outputs the features necessary for model training and inference","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    # We add a space in front for the Roberata tokenizer. Same thing fot the selected text. \n    # As turns out, doing this processing step could be improved. Check the many top solutions \n    # for better approaches.\n    tweet = \" \" + \" \".join(str(tweet).split()) # each word is spearated with space through join method\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None # start of the selected text\n    idx1 = None # ending of the selected text\n\n    # Find the start and end indices of the span\n    # assert 1 in the tweet whether selected_text remains\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]): # 0th position holds space\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    # Assign a positive label for the characters within the selected span \n    # (based on the start and end indices)\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    # Tokenize the tweet text and get ids and offsets\n    # One detail here: we need to use the tokenizer from the tokenizers\n    # library since the one from transformers doesn't provide offsets\n    # (or maybe I am wrong, please correct me in the comments if that is the case).\n    tok_tweet = tokenizer.encode(tweet)\n    \n    # an instance of id and offset : \n    # 0 [0,3]: Four\n    # here first 0 is the id and two braced numbers are the offsets\n    # for more : http://morphadorner.northwestern.edu/morphadorner/techtalk/sentenceandtokenoffsets/\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    \n    # the tokenized word is appended when it has at least one character\n    # The indices of the \"positive\" tokens are stored in `target_idx`.\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    # Ommit the first and last tokens, which should be the [CLS] and [SEP] tokens\n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    # id's are stored in roberta's pretrained token, which is shown at the beginning of this notebook\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    # Configuration of tokenizer has given earlier, check it out.\n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    \n    \n    '''\n    RoBERTa doesnâ€™t have token_type_ids, you donâ€™t need to indicate which token \n    belongs to which segment.\n    \n    before input_ids_orig, we can see 4 individual segment, as there is no token_type_ids\n    in roberta, 4 positions are added with 0 value.\n    '''\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n    # How much to pad the text to have the same sequence lengths. \n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    # Return processed tweet as a dictionary\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loader","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Dataset which stores the tweets and returns them as processed features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        \n        # from another class\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        # Return the processed data where the lists are converted to `torch.tensor`s\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Model class that combines a pretrained bert model with a linear layer","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        # Load the pretrained BERT model\n        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)\n        # Set 10% dropout to be applied to the BERT backbone's output\n        self.drop_out = nn.Dropout(0.1)\n        \n        '''\n        768 is the dimensionality of roberta_base's hidden representations\n        Multiplied by 2 since the forward pass concatenates the last two hidden representation layers\n        The output will have two dimensions (\"start_logits\", and \"end_logits\")\n        '''\n        self.l0 = nn.Linear(768 * 2, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    \n    \n    # Return the hidden states from the BERT backbone\n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        ) # bert_layers x bs x SL x (768)\n        \n        \n        '''\n        Concatenate the last two hidden states\n        This is done since experiments have shown that just getting the last layer\n        gives out vectors that may be too taylored to the original BERT training objectives (MLM + NSP)\n        Sample explanation: https://bert-as-service.readthedocs.io/en/latest/section/faq.html\n        why-not-the-last-hidden-layer-why-second-to-last\n        '''\n        out = torch.cat((out[-1], out[-2]), dim=-1) # bs x SL x (768 * 2)\n        # Apply 10% dropout to the last 2 hidden states\n        out = self.drop_out(out) # bs x SL x (768 * 2)\n        # The \"dropped out\" hidden vectors are now fed into the linear layer to output two scores\n        logits = self.l0(out) # bs x SL x 2\n\n        # Splits the tensor into start_logits and end_logits\n        # (bs x SL x 2) -> (bs x SL x 1), (bs x SL x 1)\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1) # (bs x SL)\n        end_logits = end_logits.squeeze(-1) # (bs x SL)\n\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Calculate the jaccard score from the predicted span and the actual span for a batch of tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    # A span's end index has to be greater than or equal to the start index\n    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    # Combine into a string the tokens that belong to the predicted span\n    filtered_output  = \"\"\n    \n    \n    '''\n    If the token is not the last token in the tweet, and the ending offset of the current token is less\n    than the beginning offset of the following token, add a space.\n    Basically, add a space when the next token (word piece) corresponds to a new word\n    '''\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    # Set the predicted output as the original tweet when the tweet's sentiment is \n    # \"neutral\", or the tweet only contains one word\n    if len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n        \n    # Calculate the jaccard score between the predicted span, and the actual span\n    # The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:\n    # https://www.kaggle.com/abhishek/utils\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = xm.xla_device()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROBERTA_PATH = \"../input/roberta-base\"\nmodel_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\nmodel_config.output_hidden_states = True\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Load each of the trained models and move to GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmodel1 = TweetModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(\"../input/tweet-8fold/model_0.bin\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(\"../input/tweet-8fold/model_1.bin\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(\"../input/tweet-8fold/model_2.bin\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(\"../input/tweet-8fold/model_3.bin\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(\"../input/tweet-8fold/model_4.bin\"))\nmodel5.eval()\n\nmodel6 = TweetModel(conf=model_config)\nmodel6.to(device)\nmodel6.load_state_dict(torch.load(\"../input/tweet-8fold/model_5.bin\"))\nmodel6.eval()\n\nmodel7 = TweetModel(conf=model_config)\nmodel7.to(device)\nmodel7.load_state_dict(torch.load(\"../input/tweet-8fold/model_6.bin\"))\nmodel7.eval()\n\nmodel8 = TweetModel(conf=model_config)\nmodel8.to(device)\nmodel8.load_state_dict(torch.load(\"../input/tweet-8fold/model_7.bin\"))\nmodel8.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_output = []\n\n# Instantiate TweetDataset with the test data\nTEST_BATCH_SIZE = 32\n\ntest_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n    )\n\n# Instantiate DataLoader with `test_dataset`\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=TEST_BATCH_SIZE,\n    num_workers=1\n)\n\n# Turn off gradient calculations\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    # Predict the span containing the sentiment for each batch\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        # Predict start and end logits for each of the models\n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start3, outputs_end3 = model3(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start4, outputs_end4 = model4(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start5, outputs_end5 = model5(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start6, outputs_end6 = model6(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start7, outputs_end7 = model7(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start8, outputs_end8 = model8(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        # Get the average start and end logits across the five models and use these as predictions\n        # This is a form of \"ensembling\"\n        outputs_start = (outputs_start1 + outputs_start2 + outputs_start3 + outputs_start4 + outputs_start5 \n                         + outputs_start6 + outputs_start7 + outputs_start8) / 8\n        outputs_end = (outputs_end1 + outputs_end2 + outputs_end3 + outputs_end4 \n                       + outputs_end5 + outputs_end6 + outputs_end7 + outputs_end8) / 8\n        \n        \n        # Apply softmax to the predicted start and end logits\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        \n        # Convert the start and end scores to actual predicted spans (in string form)\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            _, output_sentence = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            final_output.append(output_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.to_csv(\"submission.csv\", index=False)\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References \n\n- https://www.kaggle.com/abhishek/roberta-inference-5-folds/data\n- https://medium.com/@YassineAlouini/roberta-meets-tpus-af839ce7c070\n- About hugging face : https://medium.com/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a\n- For zero gradient : https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n- Ground truth is a term used to refer to information provided by direct observation (i.e. empirical evidence) as opposed to information provided by inference.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <center><font color = #174A65>Thank you for your time ðŸ˜ƒ </font></center>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}