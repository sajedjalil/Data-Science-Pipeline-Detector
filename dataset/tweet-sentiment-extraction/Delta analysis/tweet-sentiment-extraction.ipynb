{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers","metadata":{"id":"Xv7qx2iXQZ6J","outputId":"d4dcc57c-5ea2-4a6c-abaa-25b06869657d","execution":{"iopub.status.busy":"2021-07-28T05:30:18.572683Z","iopub.execute_input":"2021-07-28T05:30:18.573068Z","iopub.status.idle":"2021-07-28T05:30:18.577257Z","shell.execute_reply.started":"2021-07-28T05:30:18.572986Z","shell.execute_reply":"2021-07-28T05:30:18.576534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"szASIwcMRdB3"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nfrom tqdm import tqdm\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaForQuestionAnswering, RobertaConfig, RobertaTokenizer\n\nwarnings.filterwarnings('ignore')\nTRAIN = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"O5LtsA-lRdB5","execution":{"iopub.status.busy":"2021-07-28T05:30:18.578506Z","iopub.execute_input":"2021-07-28T05:30:18.578924Z","iopub.status.idle":"2021-07-28T05:30:27.407026Z","shell.execute_reply.started":"2021-07-28T05:30:18.578882Z","shell.execute_reply":"2021-07-28T05:30:27.405897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test out basics of roberta  \n# https://huggingface.co/transformers/model_doc/roberta.html#robertaforquestionanswering\nTest =False\nif Test:\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n    text = \"Jim is happy, but not me\" \n    sent_text = 'negative ' + \"Jim is happy, but not me\"\n    selected_text =  \"happy, but not me\"\n\n    text = [\"I win\"]\n    selected_text =  \"I win\"\n\n    inputs = tokenizer(text, return_tensors='pt', pad_to_max_length=True, truncation=True, max_length=10)\n    start_positions = torch.tensor([1])\n    end_positions = torch.tensor([2])\n\n    model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n    outputs = model(**inputs)\n    # outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n    loss = outputs.loss\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n    print(inputs, '\\n', outputs)\n# outputs\n# tokenizer.encode('negative'+\"Jim is happy, but not me\", return_tensors='pt')\n# inputs\n# outputs.start_logits.squeeze(0)","metadata":{"id":"DlvEp4cM2KBA","execution":{"iopub.status.busy":"2021-07-28T05:30:27.409228Z","iopub.execute_input":"2021-07-28T05:30:27.409659Z","iopub.status.idle":"2021-07-28T05:30:27.418083Z","shell.execute_reply.started":"2021-07-28T05:30:27.409613Z","shell.execute_reply":"2021-07-28T05:30:27.417047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seed","metadata":{"id":"CRnT19flRdCA"}},{"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","id":"M6MtpB7RRdCA","execution":{"iopub.status.busy":"2021-07-28T05:30:27.419907Z","iopub.execute_input":"2021-07-28T05:30:27.420524Z","iopub.status.idle":"2021-07-28T05:30:27.437646Z","shell.execute_reply.started":"2021-07-28T05:30:27.42047Z","shell.execute_reply":"2021-07-28T05:30:27.436297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader","metadata":{"id":"0ShzS94DRdCH"}},{"cell_type":"markdown","source":"### Add  token_len, start_idx, end_idx to training data\n","metadata":{"id":"ScW55-ijFLJ4"}},{"cell_type":"code","source":"if TRAIN:\n    cleaned = True\n    if cleaned:\n        train_df = pd.read_csv('input/tweet-sentiment-extraction/clean_train.csv')\n    else:\n        train_df = pd.read_csv('input/tweet-sentiment-extraction/train.csv')\n    train_df['text'] = train_df['text'].astype(str)\n    train_df['selected_text'] = train_df['selected_text'].astype(str)","metadata":{"id":"-Z-yibjHdCCz","execution":{"iopub.status.busy":"2021-07-28T05:30:27.439395Z","iopub.execute_input":"2021-07-28T05:30:27.440065Z","iopub.status.idle":"2021-07-28T05:30:27.449013Z","shell.execute_reply.started":"2021-07-28T05:30:27.440019Z","shell.execute_reply":"2021-07-28T05:30:27.448014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    if 'token_len' not in train_df:  \n        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        def token_length(row):\n            texto = \" \" + \" \".join(row.text.lower().split())\n            text = tokenizer(texto)['input_ids']\n            return len(text)\n        train_df['token_len'] = train_df.apply(token_length, axis=1)\n        print('max train token length: ', train_df.token_len.max())","metadata":{"id":"j_v7_fZC4Xjx","execution":{"iopub.status.busy":"2021-07-28T05:30:27.450744Z","iopub.execute_input":"2021-07-28T05:30:27.451198Z","iopub.status.idle":"2021-07-28T05:30:27.473396Z","shell.execute_reply.started":"2021-07-28T05:30:27.451158Z","shell.execute_reply":"2021-07-28T05:30:27.472576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    if 'start_idx' not in train_df:   \n        # token level index \n        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        def find_idx(row, p_token=False):\n            # tokenizer should not use padding since actual length is used\n            texto = \" \" + \" \".join(row.text.lower().split())\n            sel_to = \" \" + \" \".join(row.selected_text.lower().split())\n            text = tokenizer(texto)['input_ids']\n            sel_t = tokenizer(sel_to)['input_ids']\n            if p_token:\n                print(text, '\\n', sel_t)\n            # for very long sublist finding \n            # see https://stackoverflow.com/questions/7100242/python-numpy-first-occurrence-of-subarray\n            # we will just use rolling windows for tweet data\n            i = 1\n            while i<=len(text)-len(sel_t)+1:\n                if text[i] == sel_t[1]:\n                    # print(i, text[i:i+len(sel_t)-2], sel_t[1:len(sel_t)-1])\n                    if text[i:i+len(sel_t)-2] == sel_t[1:len(sel_t)-1]:\n                        start_idx = i\n                        end_idx = i+len(sel_t)-3\n                        return start_idx, end_idx\n\n                i+=1\n            # Error in selected_text, this should be corrected using character level index\n            # idea 1: remove incomplete words in selected_text\n            # idea 2: complete the words\n            # idea 3: remove these rows\n            return 0, 0 \n\n        train_df['start_idx'] = train_df.apply(lambda x: find_idx(x)[0], axis=1)\n        train_df['end_idx'] = train_df.apply(lambda x: find_idx(x)[1], axis=1)\n\n    #=============================================================\n    # character level index\n    # def find_start(row):\n    #     return row.text.find(row.selected_text)\n    # def find_end(row):\n    #     return  row.start_idx + len(row.selected_text)\n    # if 'start_idx' not in train_df:\n    #     train_df['start_idx'] = train_df.apply(lambda row: row.text.find(row.selected_text), axis=1)  # along column\n    #     train_df['end_idx'] = train_df.apply(find_end, axis=1)\n\n","metadata":{"id":"-7gj6gx8cV6a","execution":{"iopub.status.busy":"2021-07-28T05:30:27.475844Z","iopub.execute_input":"2021-07-28T05:30:27.476422Z","iopub.status.idle":"2021-07-28T05:30:27.489829Z","shell.execute_reply.started":"2021-07-28T05:30:27.476371Z","shell.execute_reply":"2021-07-28T05:30:27.488725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Error in training labels** \n?? **convert_tokens_to_string** might solve the subwords error ??","metadata":{"id":"qXyJYzERQh_2"}},{"cell_type":"code","source":"Test =False\nif Test:\n    error_train_df = train_df[train_df.start_idx ==0]\n    error_train_df\n# error_train_df.to_csv('input/tweet-sentiment-extraction/error_train.csv')\n    print(error_train_df.iloc[0].text, '\\n', error_train_df.iloc[0].selected_text)\n    find_idx(error_train_df.iloc[0], p_token=True)\n    print('**----- selected text wrong -----**')\n\n\n    print(error_train_df.iloc[2593].text, '\\n', error_train_df.iloc[2593].selected_text)\n    find_idx(error_train_df.iloc[2593], p_token=True)\n    print('**----- selected text missing a parenthesis -----**')","metadata":{"id":"k5MKx-6zUp78","execution":{"iopub.status.busy":"2021-07-28T05:30:27.491684Z","iopub.execute_input":"2021-07-28T05:30:27.492288Z","iopub.status.idle":"2021-07-28T05:30:27.507903Z","shell.execute_reply.started":"2021-07-28T05:30:27.492225Z","shell.execute_reply":"2021-07-28T05:30:27.50691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The error data is droped because\n* the error is not a structured and there is no easy fix\n* drop 2594/27481 <10% is not hurting too much","metadata":{"id":"Lyy62tI8dr_K"}},{"cell_type":"code","source":"if TRAIN:\n    train_df_clean = train_df[train_df.start_idx !=0]\n    train_df_clean.reset_index(drop=True, inplace=True)\n    del train_df","metadata":{"id":"_CZTWnxSUh8l","execution":{"iopub.status.busy":"2021-07-28T05:30:27.509264Z","iopub.execute_input":"2021-07-28T05:30:27.509857Z","iopub.status.idle":"2021-07-28T05:30:27.520028Z","shell.execute_reply.started":"2021-07-28T05:30:27.509812Z","shell.execute_reply":"2021-07-28T05:30:27.519317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Torch data class","metadata":{"id":"a9_C7QLBfJbN"}},{"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        # use internet\n        # self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")       \n        # no internet\n        self.tokenizer = RobertaTokenizer(vocab_file='../input/roberta-base/vocab.json', \n                                          merges_file='../input/roberta-base/merges.txt')\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        # tokenizer should not use padding since actual length is used\n        text_o =  \" \" + \" \".join((row.text + ' '+ row.sentiment).lower().split())\n        data = self.tokenizer(text_o, \n                         return_tensors='pt', \n                         pad_to_max_length=True, \n                         truncation=True, \n                         max_length=self.max_len)\n        # \n        # since return_tensors='pt' will produce batched result but\n        # dataloaders only feed in one row at a time. so we should remove \n        # batch dimension In order to have auto batching working properly\n        for key in data.keys():\n            data[key]= data[key].squeeze()\n\n        # if we do not require  return_tensors='pt', tokenizer produce list; we need\n        #for key in data.keys():\n        #    data[key] = torch.tensor(data[key]) \n        \n        if self.labeled:\n            data['token_len'] = row.token_len\n            data['start_idx'] = row.start_idx\n            data['end_idx'] = row.end_idx\n\n            \"\"\"\n            compute start_idx and end_idx is time consuming, so we move it \n            to operate after loading df, and saving as columns in df\n            \"\"\"\n            # ## old code\n            # sel_o = \" \" + \" \".join(row.selected_text.lower().split())\n            # sel_token = self.tokenizer(sel_o, \n            #             truncation=True, \n            #             max_length=self.max_len)['input_ids']\n            # print(sel_o, '\\n', sel_token)\n            # data['start_idx'], data['end_idx'] = self.find_idx(text_token, sel_token)\n           \n        return data\n    # def find_idx(self, text, sel_t):\n    #   # for very long sublist finding \n    #     # see https://stackoverflow.com/questions/7100242/python-numpy-first-occurrence-of-subarray\n    #     # we will just use rolling windows for tweet data\n    #     i = 1\n    #     while i<=len(text)-len(sel_t)+1:\n    #         if text[i] == sel_t[1]:\n    #             if text[i:i+len(sel_t)-2]== sel_t[1:len(sel_t)-1]:\n    #                 start_idx = i\n    #                 print(i)\n    #                 end_idx = i+len(sel_t)-3\n    #                 return start_idx, end_idx\n    #         i+=1\n    \n    def __len__(self):\n        return len(self.df)\n\n#==============================================================  \n# auto batching is tricky when data are in different format, we could write a\n# function to replace default collate_fn\ndef customer_batch(data):\n    pass\n\n#==============================================================    \n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        #collate_fn= customer_batch,\n        shuffle=True, \n        num_workers=1,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        #collate_fn= customer_batch,\n        shuffle=False, \n        num_workers=1)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\n#==============================================================    \n\ndef get_test_loader(df, batch_size=32):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        #collate_fn= customer_batch,\n        shuffle=False, \n        num_workers=1)    \n    return loader","metadata":{"id":"y6xRY4b9RdCI","execution":{"iopub.status.busy":"2021-07-28T05:30:27.521334Z","iopub.execute_input":"2021-07-28T05:30:27.521822Z","iopub.status.idle":"2021-07-28T05:30:27.538311Z","shell.execute_reply.started":"2021-07-28T05:30:27.521786Z","shell.execute_reply":"2021-07-28T05:30:27.537272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Test the dataloaders\n\"\"\"\nTest = False\ni=1\nif Test:\n    sss = StratifiedShuffleSplit(n_splits=1, train_size=777*32, random_state=seed)\n    for train_idx, val_idx in sss.split(train_df_clean, train_df_clean.sentiment):\n        # print(train_idx, val_idx)\n        \n        data_loader = get_train_val_loaders(train_df_clean, train_idx, val_idx, batch_size=2)['train']\n        for data in data_loader:\n            if i < 2:\n                #print(data)\n                i += 1\n                \n            # decode convert token ids to text\n            tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  \n            print( tokenizer.decode(data['input_ids'][0][1:5]) )\n            break\n","metadata":{"id":"7lkc6pBt0JC_","outputId":"c90d8732-7430-4314-b330-9aeda6818e8a","execution":{"iopub.status.busy":"2021-07-28T05:30:27.539638Z","iopub.execute_input":"2021-07-28T05:30:27.540174Z","iopub.status.idle":"2021-07-28T05:30:27.556061Z","shell.execute_reply.started":"2021-07-28T05:30:27.540129Z","shell.execute_reply":"2021-07-28T05:30:27.55455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"24887/32","metadata":{"id":"WZep2SM2LANu","outputId":"b8d753ed-e60a-469c-81a7-23f19cb3fdf6","execution":{"iopub.status.busy":"2021-07-28T05:30:27.558058Z","iopub.execute_input":"2021-07-28T05:30:27.558511Z","iopub.status.idle":"2021-07-28T05:30:27.579052Z","shell.execute_reply.started":"2021-07-28T05:30:27.558458Z","shell.execute_reply":"2021-07-28T05:30:27.578264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"oix6ns76RdCL"}},{"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        # use internet\n        # self.roberta = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n        # no internet \n        config = RobertaConfig.from_pretrained(\n            '../input/roberta-base/config.json')    \n        self.roberta = RobertaForQuestionAnswering.from_pretrained(\n            '../input/roberta-base/pytorch_model.bin', config=config)\n\n        # self.dropout = nn.Dropout(0.2)\n        # self.fc = nn.Linear(config.hidden_size, 2)\n        # nn.init.normal_(self.fc.weight, std=0.02)\n        # nn.init.normal_(self.fc.bias, 0)\n\n    def forward(self, inputs):\n        outputs = self.roberta(**inputs)\n         \n        # x = torch.stack([hs[-1], hs[-2], hs[-3], hs[-4]])\n        # x = torch.mean(x, 0)\n        # x = self.dropout(x)\n        # x = self.fc(x)\n        # start_logits, end_logits = x.split(1, dim=-1)\n        # start_logits = start_logits.squeeze(-1)\n        # end_logits = end_logits.squeeze(-1)\n                \n        return outputs.start_logits, outputs.end_logits\n","metadata":{"id":"MvpKKwrlRdCM","execution":{"iopub.status.busy":"2021-07-28T05:30:27.580328Z","iopub.execute_input":"2021-07-28T05:30:27.580971Z","iopub.status.idle":"2021-07-28T05:30:27.59426Z","shell.execute_reply.started":"2021-07-28T05:30:27.580926Z","shell.execute_reply":"2021-07-28T05:30:27.593139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss Function","metadata":{"id":"2Pg2wsE0RdCP"}},{"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    # start_logits/end_logits has dimension: batch * text_length\n    # start_positions/end_positions : batch * 1\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss\n\ndef loss_fn1(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    # start_logits/end_logits has dimension: batch * text_length\n    # start_positions/end_positions : batch * 1\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)   \n    length =  (end_positions - start_positions).abs().float()\n    # when length is large, we do not really care so much on every position, take average \n    total_loss = (start_loss + end_loss)/length # + 0.1* length\n    return total_loss","metadata":{"id":"TVirnKouRdCQ","execution":{"iopub.status.busy":"2021-07-28T05:30:27.595861Z","iopub.execute_input":"2021-07-28T05:30:27.596393Z","iopub.status.idle":"2021-07-28T05:30:27.609771Z","shell.execute_reply.started":"2021-07-28T05:30:27.596357Z","shell.execute_reply":"2021-07-28T05:30:27.608952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Jaccard distance and Binary Cross Entropy are similar","metadata":{"id":"RYZf3tbeHsVu"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\n\ndef jaccard_distance_loss(y_true, y_pred, smooth=1):\n    \"\"\"\n    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n    Jaccard_smoothed = \n    \n    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n    \n    \"\"\"\n    intersection= (y_true * y_pred).abs().sum(dim=1)\n    union = torch.sum(y_true.abs() + y_pred.abs(), dim=1) -intersection\n    jac = (intersection + smooth) / (union + smooth)\n    return (1 - jac) * smooth\n\n\n# Test and plot\ny_pred = torch.from_numpy(np.array([np.arange(-10, 10+0.1, 0.1)]).T)\ny_true = torch.from_numpy(np.zeros(y_pred.shape))\nname='jaccard_distance_loss'\nloss = jaccard_distance_loss(y_true,y_pred).numpy()\nplt.title(name)\nplt.plot(y_pred.numpy(),loss)\nplt.xlabel('abs prediction error')\nplt.ylabel('loss')\nplt.show()\n    \nname='binary cross entropy'\nloss = torch.nn.functional.binary_cross_entropy(\n       y_true,y_pred, reduction='none').mean(-1).numpy()\nplt.title(name)\nplt.plot(y_pred.numpy(),loss)\nplt.xlabel('abs prediction error')\nplt.ylabel('loss')\nplt.show()\n    \n# Test\nprint(\"TYPE                 |Almost_right |half right |extra selected |all_wrong\")\ny_true = torch.from_numpy(np.array([[0,0,1,0],[0,0,1,0],[0,0,1,0],[0,0,1.,0.]]))\ny_pred = torch.from_numpy(np.array([[0,0,0.9,0],[0,0,0.1,0],[1,1,1,1],[1,1,0,1]]))\n\ny_true = torch.from_numpy(np.array([[0,0,1],[0,0,1],[0,0,1],[0,0,1.]]))\ny_pred = torch.from_numpy(np.array([[0,0,0.9],[0,0,0.1],[1,1,1],[1,1,0]]))\nr1 = jaccard_distance_loss(\n    y_true,\n    y_pred,).numpy()\nprint('jaccard_distance_loss',r1)\nprint('jaccard_distance_loss scaled',r1/r1.max())\nassert r1[0]<r1[1]\nassert r1[1]<r1[2]\n\nr2 = torch.nn.functional.binary_cross_entropy(\n    y_true,\n    y_pred,\n    reduction='none').mean(-1).numpy()\nprint('binary_crossentropy',r2)\nprint('binary_crossentropy_scaled',r2/r2.max())\nassert r2[0]<r2[1]\nassert r2[1]<r2[2]\n","metadata":{"id":"F31jz_hldfv5","outputId":"b1221a6d-8693-4145-9101-6efc71194409","execution":{"iopub.status.busy":"2021-07-28T05:30:27.611093Z","iopub.execute_input":"2021-07-28T05:30:27.61162Z","iopub.status.idle":"2021-07-28T05:30:28.078329Z","shell.execute_reply.started":"2021-07-28T05:30:27.611579Z","shell.execute_reply":"2021-07-28T05:30:28.077299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation Function","metadata":{"id":"44UqLqzxRdCU"}},{"cell_type":"markdown","source":"- If start_idx pred > end_idx pred: we will take the entire text as selected_text","metadata":{"id":"mW82CyQfkmK0"}},{"cell_type":"code","source":"def jaccard_score(text_token_nopadding_len, start_idx, end_idx, start_pred, end_pred):\n    # start_logits, end_logits are logits output of model\n    # start_pred = np.argmax(start_logits)\n    # end_pred = np.argmax(end_logits)\n    text_len = text_token_nopadding_len\n    if start_pred > end_pred: # taking the whole text as selected_text\n        start_pred = 1\n        end_pred = text_len-1\n\n    if end_idx < start_pred or end_pred < start_idx: # intersection = 0\n        return 0\n    else: \n        union = max(end_pred, end_idx) - min(start_pred, start_idx)+1\n        intersection = min(end_pred, end_idx) - max(start_pred, start_idx)+1\n        return intersection/union\nTest =False\nif Test:\n    jaccard_score(5,1,1,4,2) # 0.25\n    # jaccard_score(96,1,1,4,2) # 0.0105\n\n    start_logits = torch.tensor([[0,0,0,0,1]]).float() \n    start_idx =torch.tensor([1])\n    #start_pred = torch.cat((start_pred, torch.zeros(1,91)),axis=1)\n\n    ce = torch.nn.CrossEntropyLoss()\n    ce(start_logits, start_idx)\n# when len=5, loss = 1.9048; when len=96, loss = 4.5718\n","metadata":{"id":"UnDbnZ6ERdCV","execution":{"iopub.status.busy":"2021-07-28T05:30:28.079828Z","iopub.execute_input":"2021-07-28T05:30:28.080182Z","iopub.status.idle":"2021-07-28T05:30:28.088261Z","shell.execute_reply.started":"2021-07-28T05:30:28.080149Z","shell.execute_reply":"2021-07-28T05:30:28.087152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Note**: \n1. jaccard_score is sensitive to total length, CrossEntropy is not sensitive.\n2. our jaccard_score function is a fast and close approximation of the true Jaccard score (character level) used in this competetion. There would be a bit more computation if we want character level Jaccard.","metadata":{"id":"lG4CRaE4etdd"}},{"cell_type":"markdown","source":"# Training Function","metadata":{"id":"Ka9RrsaLRdCZ"}},{"cell_type":"code","source":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, batch_size, filename):\n    if torch.cuda.is_available():\n        model.cuda()\n\n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            epoch_loss = 0.0\n            epoch_jaccard = 0.0\n            \n            with tqdm(dataloaders_dict[phase], unit=\"batch\") as tepoch:\n                tepoch.set_description(f\"Epoch {epoch+1}\")\n                for data in tepoch:\n                    # reserve token_len, start_idx, end_idx for later loss computation\n                    token_len = data['token_len'].numpy()\n                    start_idx = data['start_idx']\n                    end_idx = data['end_idx']\n                    for key in ['token_len', 'start_idx', 'end_idx']:\n                        data.pop(key)\n                    \n                    # put data in GPU\n                    if torch.cuda.is_available():\n                        start_idx = start_idx.cuda()\n                        end_idx = end_idx.cuda()\n                        for key in data.keys():\n                            data[key]= data[key].cuda()\n\n                    # training \n                    optimizer.zero_grad()\n                    with torch.set_grad_enabled(phase == 'train'):\n\n                        start_logits, end_logits = model.forward(data)\n\n                        loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                        if phase == 'train':\n                            loss.backward()\n                            optimizer.step()\n                        epoch_loss += loss.item() \n                        \n                        # Jaccard score\n                        #torch.argmax(torch.tensor([[0,0,0,0,1],[0,0,0,1.5,1]]), dim=1)\n                        start_pred = torch.argmax(start_logits, dim=1).cpu().detach().numpy()\n                        end_pred = torch.argmax(end_logits, dim=1).cpu().detach().numpy()\n                        \n                        start_idx = start_idx.cpu().detach().numpy()\n                        end_idx = end_idx.cpu().detach().numpy()\n\n                        for i in range(batch_size):  # or range(token_len.shape[0])                      \n                            jaccard = jaccard_score(token_len[i], start_idx[i], end_idx[i], start_pred[i], end_pred[i])\n                            epoch_jaccard += jaccard\n                    tepoch.set_postfix(loss=loss.item()/batch_size)\n                    \n            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n            epoch_jaccard = epoch_jaccard / len(dataloaders_dict[phase].dataset)\n            \n            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss, epoch_jaccard))\n    \n    torch.save(model.state_dict(), filename)","metadata":{"id":"tyoEEYX1RdCa","execution":{"iopub.status.busy":"2021-07-28T05:30:28.090013Z","iopub.execute_input":"2021-07-28T05:30:28.090424Z","iopub.status.idle":"2021-07-28T05:30:28.109294Z","shell.execute_reply.started":"2021-07-28T05:30:28.090383Z","shell.execute_reply":"2021-07-28T05:30:28.108366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"Czn97pW9RdCm"}},{"cell_type":"code","source":"num_epochs = 5\nbatch_size = 32\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n","metadata":{"id":"WK_2s1XFRdCn","execution":{"iopub.status.busy":"2021-07-28T05:30:28.110794Z","iopub.execute_input":"2021-07-28T05:30:28.111565Z","iopub.status.idle":"2021-07-28T05:30:28.12794Z","shell.execute_reply.started":"2021-07-28T05:30:28.111518Z","shell.execute_reply":"2021-07-28T05:30:28.126634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"id":"W5zbrcCFdM3b","execution":{"iopub.status.busy":"2021-07-28T05:30:28.132304Z","iopub.execute_input":"2021-07-28T05:30:28.132671Z","iopub.status.idle":"2021-07-28T05:30:28.141977Z","shell.execute_reply.started":"2021-07-28T05:30:28.132636Z","shell.execute_reply":"2021-07-28T05:30:28.140998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    %%time\n    # Each fold takes 7* epochs = 35 mins,\n    split_fold = False\n    if split_fold:\n        for fold, (train_idx, val_idx) in enumerate(skf.split(train_df_clean, train_df_clean.sentiment), start=1): \n\n            print(f'Fold: {fold}')\n\n            model = TweetModel()\n            optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n            criterion = loss_fn    \n            dataloaders_dict = get_train_val_loaders(train_df_clean, train_idx, val_idx, batch_size)\n            train_model(\n                model, \n                dataloaders_dict,\n                criterion, \n                optimizer, \n                num_epochs,\n                batch_size,\n                f'roberta_fold{fold}.pth')","metadata":{"id":"6Hz6pRq_RdCq","outputId":"6439f861-648c-4811-fac5-22c8d5d3d0b7","execution":{"iopub.status.busy":"2021-07-28T05:30:28.143851Z","iopub.execute_input":"2021-07-28T05:30:28.144373Z","iopub.status.idle":"2021-07-28T05:30:28.154789Z","shell.execute_reply.started":"2021-07-28T05:30:28.144329Z","shell.execute_reply":"2021-07-28T05:30:28.153556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see a increase in validation loss after 2 epochs. So we only train 2 epochs on the full data","metadata":{"id":"fRQgMtKtJqMV"}},{"cell_type":"markdown","source":"### run on the full training data","metadata":{"id":"9s4_Cj-nFJC9"}},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"id":"Vp97LFy9GP5A","execution":{"iopub.status.busy":"2021-07-28T05:30:28.156419Z","iopub.execute_input":"2021-07-28T05:30:28.156829Z","iopub.status.idle":"2021-07-28T05:30:28.167689Z","shell.execute_reply.started":"2021-07-28T05:30:28.156786Z","shell.execute_reply":"2021-07-28T05:30:28.166654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    %%time\n    num_epochs = 2\n    batch_size = 32\n    split_fold = False\n    if not split_fold:\n        sss = StratifiedShuffleSplit(n_splits=1, train_size=776*32, random_state=seed)\n        for train_idx, val_idx in sss.split(train_df_clean, train_df_clean.sentiment):\n            dataloaders_dict = get_train_val_loaders(train_df_clean, train_idx, val_idx, batch_size)\n\n        model = TweetModel()\n        optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n        criterion = loss_fn    \n\n        train_model(\n            model, \n            dataloaders_dict,\n            criterion, \n            optimizer, \n            num_epochs,\n            batch_size,\n            f'roberta_whole.pth')","metadata":{"id":"G6Scb1AkFHuQ","outputId":"9d84bd97-9f52-4d76-a258-e104cc3290ba","execution":{"iopub.status.busy":"2021-07-28T05:30:28.169355Z","iopub.execute_input":"2021-07-28T05:30:28.169908Z","iopub.status.idle":"2021-07-28T05:30:28.181946Z","shell.execute_reply.started":"2021-07-28T05:30:28.169857Z","shell.execute_reply":"2021-07-28T05:30:28.180358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    del model\n    # train one more epoch with lower learning rate\n    sss = StratifiedShuffleSplit(n_splits=1, train_size=775*32, random_state=seed)\n    for train_idx, val_idx in sss.split(train_df_clean, train_df_clean.sentiment):\n        dataloaders_dict = get_train_val_loaders(train_df_clean, train_idx, val_idx, batch_size)\n    num_epochs = 1\n    model = TweetModel()\n    model.cuda()\n    model.load_state_dict(torch.load('../input/tweetextraction/roberta_whole.pth'))\n\n\n    optimizer = optim.AdamW(model.parameters(), lr=3e-6, betas=(0.9, 0.999)) \n\n    train_model(\n        model, \n        dataloaders_dict,\n        criterion, \n        optimizer, \n        num_epochs,\n        batch_size,\n        f'roberta_whole2.pth')","metadata":{"id":"_EX6RD_HdBxt","outputId":"e346bb11-f207-40a5-d764-8ef446e80d01","execution":{"iopub.status.busy":"2021-07-28T05:30:28.183694Z","iopub.execute_input":"2021-07-28T05:30:28.184287Z","iopub.status.idle":"2021-07-28T05:30:28.19529Z","shell.execute_reply.started":"2021-07-28T05:30:28.184225Z","shell.execute_reply":"2021-07-28T05:30:28.194526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"YXSMwFLTRdCt"}},{"cell_type":"code","source":"# For Inference only\n# https://huggingface.co/transformers/internal/tokenization_utils.html\n\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df)\n\n\nmodel = TweetModel()\nif torch.cuda.is_available():\n    model.cuda()\n    model.load_state_dict(torch.load('../input/tweetextraction/roberta_whole1.pth'))\nelse:\n    model.load_state_dict(torch.load('../input/tweetextraction/roberta_whole.pth', map_location=torch.device('cpu') ))\nmodel.eval()","metadata":{"id":"VSoLMeps6xqr","outputId":"a1955059-3c42-4e63-b860-117406687296","execution":{"iopub.status.busy":"2021-07-28T05:30:28.19686Z","iopub.execute_input":"2021-07-28T05:30:28.197322Z","iopub.status.idle":"2021-07-28T05:30:39.841858Z","shell.execute_reply.started":"2021-07-28T05:30:28.197281Z","shell.execute_reply":"2021-07-28T05:30:39.840975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npredictions = []\n# decode convert token ids to text\ntokenizer = RobertaTokenizer(vocab_file='../input/roberta-base/vocab.json', \n                             merges_file='../input/roberta-base/merges.txt') \nwith tqdm(test_loader, unit=\"batch\") as tepoch:\n    tepoch.set_description(\"Test:\")\n    for data in tepoch:\n        \n        # put data in GPU\n        if torch.cuda.is_available():\n            for key in data.keys():\n                data[key]= data[key].cuda()\n\n        # testing \n        with torch.no_grad():\n\n            start_logits, end_logits = model(data)\n            start_pred = torch.argmax(start_logits, dim=1)\n            end_pred = torch.argmax(end_logits, dim=1)\n            \n            for i in range(start_pred.shape[0]): # number of rows in a batch\n                if start_pred[i] > end_pred[i]:\n                    predictions.append(' ') # those will be replace by text after we build the dataframe\n                else:\n                    sel_t = tokenizer.decode(data['input_ids'][i][start_pred[i]:end_pred[i]+1])\n                    predictions.append(sel_t)\n","metadata":{"id":"RWiPjmA_RdCu","outputId":"ef0deb5f-9b3d-4896-92e3-e06abf4ca7b5","execution":{"iopub.status.busy":"2021-07-28T05:30:39.842917Z","iopub.execute_input":"2021-07-28T05:30:39.843212Z","iopub.status.idle":"2021-07-28T05:31:58.627269Z","shell.execute_reply.started":"2021-07-28T05:30:39.843184Z","shell.execute_reply":"2021-07-28T05:31:58.626475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"id":"EPlUWdEKRdCw"}},{"cell_type":"code","source":"sub_df = test_df # [['textID','text','sentiment']]\nsub_df['selected_text'] = predictions\ndef rep_text(row):\n    rst= row.selected_text\n    if (rst is ' ') or (len(rst)> len(row.text)):\n        return row.text\n    if len(rst.split())==1:\n        rst = rst.replace('!!!!', '!')\n        rst = rst.replace('..', '.')\n        rst = rst.replace('...', '.')\n        return rst\n    if row.sentiment == 'neutral':\n        rst = row.text\n    return rst\n\nsub_df['selected_text'] = sub_df.apply(rep_text, axis=1)\nsub_df.drop(['text','sentiment'], axis=1, inplace=True)\nsub_df.to_csv('submission.csv', index=False)\nsub_df","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:31:58.628655Z","iopub.execute_input":"2021-07-28T05:31:58.629129Z","iopub.status.idle":"2021-07-28T05:31:58.677287Z","shell.execute_reply.started":"2021-07-28T05:31:58.629095Z","shell.execute_reply":"2021-07-28T05:31:58.675731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}