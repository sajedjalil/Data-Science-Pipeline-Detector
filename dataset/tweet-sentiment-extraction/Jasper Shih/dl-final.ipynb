{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tokenizers, math, pickle\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as kfold\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold as stratkfold\nfrom transformers import *","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-21T05:33:16.269704Z","iopub.execute_input":"2021-12-21T05:33:16.270656Z","iopub.status.idle":"2021-12-21T05:33:23.646579Z","shell.execute_reply.started":"2021-12-21T05:33:16.270607Z","shell.execute_reply":"2021-12-21T05:33:23.645433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constants\nLEN = 96\nEPOCHS = 3\nBATCH_SIZE = 32\nSEED = 88888\nFOLDS = 5\nMAP_SENTI_TO_ID = {'positive': 1313,\n                   'negative': 2430,\n                   'neutral': 7974}\n\npath_vocab = '../input/tf-roberta/vocab-roberta-base.json'\npath_merge = '../input/tf-roberta/merges-roberta-base.txt'\npath_pretrained = '../input/tf-roberta/pretrained-roberta-base.h5'\npath_train = '../input/tweet-sentiment-extraction/train.csv'\npath_test = '../input/tweet-sentiment-extraction/test.csv'\npath_config = '../input/tf-roberta/config-roberta-base.json'","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:33:23.650547Z","iopub.execute_input":"2021-12-21T05:33:23.651388Z","iopub.status.idle":"2021-12-21T05:33:23.664679Z","shell.execute_reply.started":"2021-12-21T05:33:23.651333Z","shell.execute_reply":"2021-12-21T05:33:23.663427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utils\ndef jaccard_idx(str1, str2):\n    set1 = set(str1.lower().split())\n    set2 = set(str2.lower().split())\n    if (len(set1)==0) & (len(set2)==0):\n        return 0.5\n    else:\n        set3 = set1.intersection(set2)\n        return float(len(set3)) / (len(set1) + len(set2) - len(set3))\n\ndef dump_data(mod, path):\n    wei = mod.get_weights()\n    with open(path, 'wb') as fd:\n        pickle.dump(wei, fd)\n\ndef load_data(mod, path):\n    with open(path, 'rb') as fd:\n        wei = pickle.load(fd)\n    mod.set_weights(wei)\n    return mod\n\ndef cal_loss(yt, yhat):\n    pos = tf.shape(yhat)[1]\n    yt = yt[:, :pos]\n    ret = tf.keras.losses.categorical_crossentropy(yt, yhat, from_logits=False, label_smoothing=0.1)\n    ret = tf.reduce_mean(ret)\n    return ret","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-12-21T05:33:23.666805Z","iopub.execute_input":"2021-12-21T05:33:23.668275Z","iopub.status.idle":"2021-12-21T05:33:23.684709Z","shell.execute_reply.started":"2021-12-21T05:33:23.66821Z","shell.execute_reply":"2021-12-21T05:33:23.683577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize\ntrain = pd.read_csv(path_train).fillna('')\ntest = pd.read_csv(path_test).fillna('')\nconfig = RobertaConfig.from_pretrained(path_config)\ntknzer = tokenizers.ByteLevelBPETokenizer(vocab_file=path_vocab, merges_file=path_merge, lowercase=True, add_prefix_space=True)\nstrat = stratkfold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nin_ids = np.ones((train.shape[0],LEN),dtype='int32')\nin_ids_t = np.ones((test.shape[0],LEN),dtype='int32')\nmask = np.zeros((train.shape[0],LEN),dtype='int32')\nmask_t = np.zeros((test.shape[0],LEN),dtype='int32')\ntypes = np.zeros((train.shape[0],LEN),dtype='int32')\ntypes_t = np.zeros((test.shape[0],LEN),dtype='int32')\nstart = np.zeros((train.shape[0],LEN),dtype='int32')\nend = np.zeros((train.shape[0],LEN),dtype='int32')","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:33:23.688714Z","iopub.execute_input":"2021-12-21T05:33:23.689151Z","iopub.status.idle":"2021-12-21T05:33:24.003236Z","shell.execute_reply.started":"2021-12-21T05:33:23.689041Z","shell.execute_reply":"2021-12-21T05:33:24.00223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convertTrainData():\n    for cur in range(train.shape[0]):\n        str1 = \" \"+\" \".join(train.loc[cur,'text'].split())\n        str2 = \" \".join(train.loc[cur,'selected_text'].split())\n\n        vec = np.zeros((len(str1)))\n        pos = str1.find(str2)\n        vec[pos:pos+len(str2)]=1\n\n        if str1[pos-1]==' ': vec[pos-1] = 1\n        enc = tknzer.encode(str1)\n\n        gather = []\n        pos=0\n        for i in enc.ids:\n            w = tknzer.decode([i])\n            gather.append((pos, pos+len(w)))\n            pos += len(w)\n\n\n        tokens = []\n        for i,(a,b) in enumerate(gather):\n            sm = np.sum(vec[a:b])\n            if sm>0:\n                tokens.append(i)\n\n        stok = MAP_SENTI_TO_ID[train.loc[cur, 'sentiment']]\n        in_ids[cur, :len(enc.ids) + 3] = [0, stok] + enc.ids + [2]\n        mask[cur, :len(enc.ids) + 3] = 1\n        if len(tokens)>0:\n            start[cur, tokens[0] + 2] = 1\n            end[cur, tokens[-1] + 2] = 1\n\ndef convertTestData():\n    for cur in range(test.shape[0]):\n        str1 = \" \"+\" \".join(test.loc[cur,'text'].split())\n        enc = tknzer.encode(str1)\n        stok = MAP_SENTI_TO_ID[test.loc[cur,'sentiment']]\n        in_ids_t[cur,:len(enc.ids)+3] = [0, stok] + enc.ids + [2]\n        mask_t[cur,:len(enc.ids)+3] = 1\n\ndef build_model():\n    _ids = tf.keras.layers.Input((LEN,), dtype=tf.int32)\n    _mask = tf.keras.layers.Input((LEN,), dtype=tf.int32)\n    _types = tf.keras.layers.Input((LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(_ids, 1), tf.int32)\n\n    cur_len = tf.reduce_max(LEN - tf.reduce_sum(padding, -1))\n    new_ids = _ids[:, :cur_len]\n    new_mask = _mask[:, :cur_len]\n    new_types = _types[:, :cur_len]\n\n\n    x_val = TFRobertaModel.from_pretrained(path_pretrained, config=config)(\n        new_ids,\n        attention_mask=new_mask,\n        token_type_ids=new_types\n    )\n\n    x_val1 = tf.keras.layers.Dropout(0.1)(x_val[0])\n    x_val1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x_val1)\n    x_val1 = tf.keras.layers.LeakyReLU()(x_val1)\n    x_val1 = tf.keras.layers.Dense(1)(x_val1)\n    x_val1 = tf.keras.layers.Flatten()(x_val1)\n    x_val1 = tf.keras.layers.Activation('softmax')(x_val1)\n\n    x_val2 = tf.keras.layers.Dropout(0.1)(x_val[0])\n    x_val2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x_val2)\n    x_val2 = tf.keras.layers.LeakyReLU()(x_val2)\n    x_val2 = tf.keras.layers.Dense(1)(x_val2)\n    x_val2 = tf.keras.layers.Flatten()(x_val2)\n    x_val2 = tf.keras.layers.Activation('softmax')(x_val2)\n\n    new_model = tf.keras.models.Model(inputs=[_ids, _mask, _types], outputs=[x_val1,x_val2])\n    new_model.compile(loss=cal_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5))\n    new_pad_model = tf.keras.models.Model(inputs=[_ids, _mask, _types], outputs=[tf.pad(x_val1, [[0, 0], [0, LEN - cur_len]], constant_values=0.),\n                                                                                 tf.pad(x_val2, [[0, 0], [0, LEN - cur_len]], constant_values=0.)])\n    return new_model, new_pad_model","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:33:24.006512Z","iopub.execute_input":"2021-12-21T05:33:24.006819Z","iopub.status.idle":"2021-12-21T05:33:24.046457Z","shell.execute_reply.started":"2021-12-21T05:33:24.006786Z","shell.execute_reply":"2021-12-21T05:33:24.045093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"convertTrainData()\nconvertTestData()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:33:24.048824Z","iopub.execute_input":"2021-12-21T05:33:24.049778Z","iopub.status.idle":"2021-12-21T05:33:42.952403Z","shell.execute_reply.started":"2021-12-21T05:33:24.049724Z","shell.execute_reply":"2021-12-21T05:33:42.95138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rets = []\n\npstart = np.zeros((in_ids.shape[0],LEN))\npend = np.zeros((in_ids.shape[0],LEN))\n\npSsum = np.zeros((in_ids_t.shape[0],LEN))\npEsum = np.zeros((in_ids_t.shape[0],LEN))\nhistories = []\n\nfor curFold, (idx1, idx2) in enumerate(strat.split(in_ids,train.sentiment.values)):\n    print(\"Fold \" + str(curFold+1) + \":\")\n    fname = \"fold\" + str(curFold+1) + \".h5\"\n\n    kfold.clear_session()\n    model, pad_model = build_model()\n\n    inp1 = [in_ids[idx1,], mask[idx1,], types[idx1,]]\n    target1 = [start[idx1,], end[idx1,]]\n\n    inp2 = [in_ids[idx2,], mask[idx2,], types[idx2,]]\n    target2 = [start[idx2,], end[idx2,]]\n\n    sortedData2 = np.int32(sorted(range(len(inp2[0])),\n                               key=lambda i: (inp2[0][i] == 1).sum(),\n                               reverse=True))\n\n    inp2 = [arr[sortedData2] for arr in inp2]\n    target2 = [arr[sortedData2] for arr in target2]\n\n    for epo in range(1, EPOCHS + 1):\n        sortedData1 = np.int32(sorted(range(len(inp1[0])),\n                                   key=lambda i: (inp1[0][i] == 1).sum() + np.random.randint(-3, 3),\n                                   reverse=True))\n        idxs = np.random.permutation(math.ceil(len(sortedData1) / BATCH_SIZE))\n        tmp = []\n        for idx in idxs:\n            tmp.append(sortedData1[idx * BATCH_SIZE: (idx + 1) * BATCH_SIZE])\n        sortedData1 = np.concatenate(tmp)\n\n        inp1 = [arr[sortedData1] for arr in inp1]\n        target1 = [arr[sortedData1] for arr in target1]\n\n        histories.append(model.fit(inp1, target1, epochs=epo, initial_epoch=epo - 1,\n                  batch_size=BATCH_SIZE, verbose=1, callbacks=[],\n                  validation_data=(inp2, target2), shuffle=False))\n        dump_data(model, fname)\n\n    load_data(model, fname)\n    pstart[idx2,], pend[idx2,] = pad_model.predict([in_ids[idx2,], mask[idx2,], types[idx2,]], verbose=1)\n\n    pSE = pad_model.predict([in_ids_t, mask_t, types_t], verbose=1)\n    pSsum += pSE[0]/strat.n_splits\n    pEsum += pSE[1]/strat.n_splits\n\n    jacs = []\n    for idxCur in idx2:\n        maxS = np.argmax(pstart[idxCur,])\n        maxE = np.argmax(pend[idxCur,])\n        if maxS>maxE:\n            st = train.loc[idxCur,'text']\n        else:\n            enc = tknzer.encode(\" \"+\" \".join(train.loc[idxCur,'text'].split()))\n            st = tknzer.decode(enc.ids[maxS-2: maxE-1])\n        jacs.append(jaccard_idx(st, train.loc[idxCur, 'selected_text']))\n    rets.append(np.mean(jacs))\n    print(\"Fold \" + str(curFold+1) + \" Jaccard = \" + str(np.mean(jacs)) + \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:33:42.955163Z","iopub.execute_input":"2021-12-21T05:33:42.955522Z","iopub.status.idle":"2021-12-21T06:08:31.094912Z","shell.execute_reply.started":"2021-12-21T05:33:42.955489Z","shell.execute_reply":"2021-12-21T06:08:31.093918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Mean Jaccard of Folds = ',np.mean(rets))","metadata":{"pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2021-12-21T06:08:31.097491Z","iopub.execute_input":"2021-12-21T06:08:31.098151Z","iopub.status.idle":"2021-12-21T06:08:31.10725Z","shell.execute_reply.started":"2021-12-21T06:08:31.098105Z","shell.execute_reply":"2021-12-21T06:08:31.105863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rets)","metadata":{"collapsed":false,"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-12-21T06:08:31.109516Z","iopub.execute_input":"2021-12-21T06:08:31.110107Z","iopub.status.idle":"2021-12-21T06:08:31.119568Z","shell.execute_reply.started":"2021-12-21T06:08:31.110055Z","shell.execute_reply":"2021-12-21T06:08:31.117898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show = []\nshow2 = []\ncnt = 0\none = []\none2 = []\n\nfor his in histories:\n    one.append(his.history['loss'])\n    one2.append(his.history['val_loss'])\n#     print(his.history['val_loss'])\n    if len(one) >= EPOCHS:\n        show.append(one)\n        show2.append(one2)\n        one = []\n        one2 = []\n\nf = 1\nfor i in range(len(show)):\n    plt.plot(list(range(1, EPOCHS+1)), show[i])\n    plt.plot(list(range(1, EPOCHS+1)), show2[i])\n    plt.title(\"Fold \" + str(f) + \": model loss\")\n    f+=1\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","metadata":{"pycharm":{"name":"#%%\n"},"jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2021-12-21T06:08:31.122297Z","iopub.execute_input":"2021-12-21T06:08:31.123061Z","iopub.status.idle":"2021-12-21T06:08:32.14213Z","shell.execute_reply.started":"2021-12-21T06:08:31.123002Z","shell.execute_reply":"2021-12-21T06:08:32.140793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}