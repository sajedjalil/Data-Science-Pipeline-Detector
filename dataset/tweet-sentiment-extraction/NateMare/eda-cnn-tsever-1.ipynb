{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://breakingtech.it/wp-content/uploads/2018/04/twitter-moments-1.jpg)\n\n### Several procedures come from my previous notebook (I suggest to take a look also there!): https://www.kaggle.com/doomdiskday/full-tutoria-eda-to-ensembles-embeddings-zoo\n\n## Introduction\n\"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\nHelp build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?\n\nIn this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n## What's in the notebook?\n- Data Cleaning\n- Full Exploratory Data Analysis (EDA)\n- Evaluation\n    - BL Model\n    - DNN (coming soon!)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport matplotlib\nfrom tqdm import tqdm\nfrom statistics import *\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport re\nfrom nltk.tokenize import word_tokenize\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nfrom statistics import *\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom copy import deepcopy\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\n    test['text']=test['text'].astype(str)\n    return test\n\ndef read_submission():\n    test=pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\n    return test\n    \ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning\nHere we are gonna clean the DF.\nSpecifically, we clean:\n- stopwords \n- URL \n- HTML \n- emoji \n- punctuation\n- multiple spaces\n\nIn addition we also lower all the tokens, so that we have a better-sized vocabulary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n        if text is not None:\n            tokens = [x for x in word_tokenize(text) if x not in stop]\n            return \" \".join(tokens)\n        else:\n            return None\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_punct(text):\n    exclude = set(string.punctuation)\n    s = ''.join(ch for ch in text if ch not in exclude)\n    return s\n\ndef clean_df(df, train=True):\n    df[\"dirty_text\"] = df['text']\n    \n    \n    df[\"text\"] = df['text'].apply(lambda x : x.lower())\n    \n    df['text']=df['text'].apply(lambda x: remove_emoji(x))\n        \n    df['text']=df['text'].apply(lambda x : remove_URL(x))\n        \n    df['text']=df['text'].apply(lambda x : remove_html(x))\n        \n    df['text'] =df['text'].apply(lambda x : remove_stopwords(x)) \n    \n    df['text']=df['text'].apply(lambda x : remove_punct(x))\n    \n    df.text = df.text.replace('\\s+', ' ', regex=True)\n    \n    if train:\n        df[\"selected_text\"] = df['selected_text'].apply(lambda x : x.lower())\n        df['selected_text']=df['selected_text'].apply(lambda x: remove_emoji(x))\n        df['selected_text']=df['selected_text'].apply(lambda x : remove_URL(x))\n        df['selected_text']=df['selected_text'].apply(lambda x : remove_html(x))\n        df['selected_text'] =df['selected_text'].apply(lambda x : remove_stopwords(x))\n        df['selected_text']=df['selected_text'].apply(lambda x : remove_punct(x))\n        df.selected_text = df.selected_text.replace('\\s+', ' ', regex=True)\n    \n    return df\n\n#train_df = clean_df(train_df)\n#test_df = clean_df(test_df, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analisys\nIn the following we're gonna see some data analysis on the corpus. \n\nSpecifically:\n- General dataset infos\n    - Number of samples\n    - Class Label Distributiom\n- Text analysis (Done both on 'text' and 'selected_text' for trainin and on 'text' for the test set)\n    - Number of characters in tweets \n    - Number of words in a tweet\n    - Average word lenght in a tweet\n    - Word distribution\n    - Number of unique words\n    - Top Bi-grams and Tri-grams\n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_distrib_train_test(train, test):\n    fig=make_subplots(1,2,subplot_titles=('Training set','Test set'))\n    x=train.sentiment.value_counts()\n    fig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['orange','green','red'],name=''),row=1,col=1)\n    x=test.sentiment.value_counts()\n    fig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['orange','green','red'],name=''),row=1,col=2)\n    fig.show()\n\ndef show_word_distrib(df, target=\"positive\", field=\"text\", top_N=10, selected=True):\n    fig = plt.figure()\n    \n    txt = df[df['sentiment']==target][field].str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n    words = nltk.tokenize.word_tokenize(txt)\n    words_except_stop_dist = nltk.FreqDist(words) \n    \n    rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                        columns=['Word', 'Frequency Text']).set_index('Word')\n    print(rslt)\n    #matplotlib.style.use('ggplot')\n    ax1 = fig.add_subplot()\n    rslt.plot.bar(rot=0, ax=ax1)\n    \n    if selected:\n        txt = df[df['sentiment']==target][\"selected_text\"].str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n        words = nltk.tokenize.word_tokenize(txt)\n        words_except_stop_dist = nltk.FreqDist(words) \n\n        rslt = pd.DataFrame(words_except_stop_dist.most_common(top_N),\n                            columns=['Word', 'Frequency Selected Text']).set_index('Word')\n        \n        print(rslt)\n        #matplotlib.style.use('ggplot')\n        ax2 = fig.add_subplot()\n        rslt.plot.bar(rot=0, ax=ax2)\n    plt.show()\n    \n    \ndef general_stats(train, test):\n    n_train = len(train)\n    n_test = len(test)\n    \n    print(\"Number of train samples: {}\".format(n_train))\n    print(\"Number of test samples: {}\".format(n_test))\n    plot_distrib_train_test(train, test)\n    \n    print(\"Word distribution of 'text' and 'selected_text' in Training set for positive samples\")\n    show_word_distrib(train, target=\"positive\")\n    \n    print(\"Word distribution of 'text' and 'selected_text' in Training set for neutral samples\")\n    show_word_distrib(train, target=\"neutral\")\n    \n    print(\"Word distribution of 'text' and 'selected_text' in Training set for negative samples\")\n    show_word_distrib(train, target=\"negative\")\n    \n    \n    print(\"Word distribution of 'text' in Test set for positive samples\")\n    show_word_distrib(test, target=\"positive\", selected=False)\n    \n    print(\"Word distribution of 'text' in Test set for neutral samples\")\n    show_word_distrib(test, target=\"neutral\", selected=False)\n    \n    print(\"Word distribution of 'text' in Test set for negative samples\")\n    show_word_distrib(test, target=\"negative\", selected=False)\n    \n\ndef plot_hist_classes(df, to_plot, _header, col=\"text\"):\n    fig,(ax1,ax2, ax3)=plt.subplots(1,3,figsize=(10,5))\n    \n    df_len = to_plot(df, \"negative\", col=col)\n    ax1.hist(df_len,color='red')\n    ax1.set_title('Negative Tweets')\n    \n    df_len = to_plot(df, \"positive\", col=col)\n    ax2.hist(df_len,color='green')\n    ax2.set_title('Positive Tweets')\n    \n    df_len = to_plot(df, \"neutral\", col=col)\n    ax3.hist(df_len,color='orange')\n    ax3.set_title('Neutral Tweets')\n    \n    \n    fig.suptitle(_header)\n    plt.show()\n    plt.close()\n    \n\ndef average_word_lenght(df,col=\"text\"):\n    fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(10,5))\n    \n    word=df[df['sentiment']==\"negative\"][col].str.split().apply(lambda x : [len(i) for i in x])\n    sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n    ax1.set_title('Negative')\n    \n    word=df[df['sentiment']==\"positive\"][col].str.split().apply(lambda x : [len(i) for i in x])\n    sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n    ax2.set_title('Positive')\n    \n    word=df[df['sentiment']==\"neutral\"][col].str.split().apply(lambda x : [len(i) for i in x])\n    sns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='orange')\n    ax2.set_title('Neutral')\n    \n    fig.suptitle('Average word length in each tweet')\n    plt.show()\n    \n\ndef unique_words(df, col=\"text\", title=\"Distribution of number of unique words\"):\n    fig,ax=plt.subplots(1,3,figsize=(12,7))\n    colors = {\n        \"positive\": \"green\",\n        \"negative\": \"red\",\n        \"neutral\": \"orange\"\n    }\n    for _, i in enumerate([\"positive\", \"negative\", \"neutral\"]):\n        new=df[df['sentiment']==i][col].map(lambda x: len(set(x.split())))\n        sns.distplot(new.values,ax=ax[_],color=colors[i])\n        ax[_].set_title(i)\n    fig.suptitle(title)\n    fig.show()\n    \ndef get_top_ngram(corpus, n=None):\n        vec = CountVectorizer(ngram_range=(n, n),stop_words=stop).fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0) \n        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n        return words_freq[:20]\n    \ndef plot_n_grams(df, size=2, \n                 title=\"Common bigrams in selected text\",\n                 column=\"text\"):\n    colors = {\n        \"positive\": \"green\",\n        \"negative\": \"red\",\n        \"neutral\": \"orange\"\n    }\n    \n    fig,ax=plt.subplots(1,3,figsize=(15,10))\n    for _, i in enumerate([\"positive\", \"negative\", \"neutral\"]):\n        new=df[df['sentiment']==i][column]\n        top_n_bigrams=get_top_ngram(new,size)[:20]\n        x,y=map(list,zip(*top_n_bigrams))\n        sns.barplot(x=y,y=x,ax=ax[_],color=colors[i])\n        ax[_].set_title(i)\n    \n    fig.suptitle(title)\n    fig.show()\n   \n\ngeneral_stats(train_df, test_df)\n\ndef to_plot_chars(df, _target, col=\"text\"):\n    return df[df['sentiment']==_target][col].str.len()\nplot_hist_classes(train_df, to_plot_chars, _header='Characters Lenght Distribution in Training Tweets \"text\"')\nplot_hist_classes(test_df, to_plot_chars, _header='Characters Lenght Distribution in Test Tweets \"text\"')\nplot_hist_classes(train_df, to_plot_chars, _header='Characters Lenght Distribution in Training Tweets \"selected_text\"', col='selected_text')\n\ndef to_plot_word(df, _target, col=\"text\"):\n    return df[df['sentiment']==_target][col].str.split().map(lambda x: len(x))\nplot_hist_classes(train_df, to_plot_word, _header='Sentence Lenght Distribution in Training Tweets \"text\" column')\nplot_hist_classes(train_df, to_plot_word, _header='Sentence Lenght Distribution in Training Tweets \"selected_text\" column', col='selected_text')\nplot_hist_classes(test_df, to_plot_word, _header='Sentence Lenght Distribution in Test Tweets \"text\" column')\n\nprint(\"Average word lenght in Training Tweets 'text' column\")\naverage_word_lenght(train_df)\nprint(\"Average word lenght in Training Tweets 'selected_text' column\")\naverage_word_lenght(train_df, col=\"selected_text\")\nprint(\"Average word lenght in Test Tweets 'text' column\")\naverage_word_lenght(test_df)\n\nunique_words(train_df, title=\"Distribution of number of unique words in Training samples for 'text' column\")\nunique_words(train_df, col=\"selected_text\",title=\"Distribution of number of unique words in Training samples for 'selected_text' column\")\nunique_words(test_df, title=\"Distribution of number of unique words in Test samples for 'text' column\")\n\n\n\nplot_n_grams(train_df, size=2, \n                 title=\"Common bigrams in text for the training set\",\n                 column=\"text\")\n\nplot_n_grams(train_df, size=2, \n                 title=\"Common bigrams in selected text for the training set\",\n                 column=\"selected_text\")\n\nplot_n_grams(test_df, size=2, \n                 title=\"Common bigrams in text for the test set\",\n                 column=\"text\")\n\n\nplot_n_grams(train_df, size=3, \n                 title=\"Common tri-grams in text for the training set\",\n                 column=\"text\")\n\nplot_n_grams(train_df, size=3, \n                 title=\"Common tri-grams in selected_text for the training set\",\n                 column=\"selected_text\")\n\nplot_n_grams(test_df, size=3, \n                 title=\"Common tri-grams in text for the test set\",\n                 column=\"text\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Insights\n\nFrom the above analysis, we can say the following:\n1. Both training and test set have a majority of **netrual samples** and a (more or less) equal amount of positive and negative samples. The sentiment distribution of the test set follows the one of the training set.\n2. As expected, positive words like \"good\", \"love\", \"happy\" are in the top of the **most frequent words** for both text and selected_text for the positive class, also the test set seems to follow the same line.\n3. **Negative and Neutral posts** seems to be pretty **high corelated** w.r.t to word, bi-gram and tri-gram analysis. \n4. The **selected_text** seems to have a very good bi-gram, tri-gram **corelation** w.r.t the **text**. Maybe a simple approach based on these features can work.\n5. Neutral posts seem to be longer.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# More insights!\n\nAnalyzing the output of my best model I realized that the neutral tweets prediction are really short an uncoherent. \n\nSo I proceeded analyzing the training set and I discovered that:\n1. A lot (>50%) of the selected text for the neutral post replicate exactly the text. \n2. The same insight applies for all the tweets having a lenght of at most 3 words.\n\nSo it could be useful to post-process the model outputs using these 2 reasonings.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def analyze_neutral_lenght(train_df):\n    neutral_df = train_df[train_df[\"sentiment\"] == \"neutral\"]\n    equal_selected_with_text = neutral_df[neutral_df['selected_text'] == neutral_df[\"text\"]]\n    \n    print(\"Total number of neutral: {}\".format(len(neutral_df)))\n    print(\"Neutral with text equal to selected text: {}\".format(len(equal_selected_with_text)))\n    print(\"Ratio: {}\".format(len(equal_selected_with_text)/len(neutral_df)))\n    \n    count = train_df.text.str.count(' ') <= 2\n    less_than_T = train_df.loc[count]\n    equal_selected_with_text = less_than_T[less_than_T['selected_text'] == less_than_T[\"text\"]]\n    print(\"Total number of tweets with less than 3 words per text: {}\".format(len(neutral_df)))\n    print(\"Neutral with text equal to selected text: {}\".format(len(equal_selected_with_text)))\n    print(\"Ratio: {}\".format(len(equal_selected_with_text)/len(less_than_T)))\n   \n\n\ndef post_process(submission_df, test_df):\n\n    index_to_selected_text = {}\n    for i, row in test_df.iterrows():\n        _id = row[0]\n        text = row[1]\n        sentiment = row[2]\n        if len(text.split(\" \")) <= 3 or sentiment == \"neutral\":\n            index_to_selected_text[_id] = text\n    \n    submission_rows = submission_df.to_dict(\"records\")\n    new_rows = []\n    for row in submission_rows:\n        _id = row['textID']\n        if _id in index_to_selected_text:\n            new_row = deepcopy(row)\n            new_row['selected_text'] = index_to_selected_text[_id]\n        else:\n            new_row = row\n        \n        new_rows.append(new_row)\n\n    return pd.DataFrame(new_rows)\n\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()\nanalyze_neutral_lenght(train_df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Why NOT TO CLEAN!\n\nLooking at the training data and Challenge Discussions I realized that the \"selected_text\" column is pretty dirty and, most probably, it has been derived in an automatic or semi-automatic fashion.\n\nIn order to check how dirty it is I decided to run all the cleaning functions I previously wrote, one by one, checking the differences (in terms of words) w.r.t the cleaned selected text and the original one.\n\nHow it is possible to see by the below analysis both **stopwords** and **punctuations** play an important role in the selected text. \n\nSo, instead of cleaning them before training a model, we should just use **lowercasing** (which does not affect our evaluation setting) and a **good post-processing** technique in order to get better selected_texts\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def analyze_cleaning(df, cleaning_lambda, field=\"selected_text\"):\n    def diff_strings(row):\n        count = {}\n        A = row[0]\n        B = row[1]\n\n        if A is None:\n            A = \"TEMP\"\n        \n        if B is None:\n            B = \"TEMP\"\n        \n        for word in A.split(): \n            count[word] = count.get(word, 0) + 1\n\n        for word in B.split(): \n            count[word] = count.get(word, 0) + 1\n\n\n        diff = [word for word in count if count[word] == 1]\n        _max_len = max(len(A), len(B))\n        return len(diff)/_max_len\n        \n    to_analyze = df[[field]]\n    to_analyze['cleaned_selected_text'] = to_analyze[field]\n    to_analyze[\"diff_ratio\"] = None\n    to_analyze['cleaned_selected_text']=to_analyze['cleaned_selected_text'].apply(lambda x: cleaning_lambda(x))\n    \n    to_analyze[\"diff_ratio\"] = to_analyze.apply(lambda x: diff_strings(x), axis=1)\n    print(\"Cleaning function used: {}\".format(cleaning_lambda.__name__))\n    print(\"Average difference ratio: {}\".format(to_analyze[\"diff_ratio\"].mean()))\n    print(\"\")\n\n\ntrain_df = read_train()\ntest_df = read_test()\ncleaning_functions = [remove_stopwords, remove_URL, remove_html, remove_emoji, remove_punct]\nfor func in cleaning_functions:\n    analyze_cleaning(train_df, func, field=\"selected_text\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom tqdm import tqdm\n\n\ndef expand(text, selected, steps=2):\n    _text = text.split()\n    _selected = selected.split()\n    \n    _start = 0\n    for i,_t in enumerate(_text):\n        to_check = \" \".join(_text[i: i + len(_selected)])\n        if to_check == selected:\n            break\n        _start += 1\n        \n    #_start = text.find(selected)\n    _end = _start + len(_selected)\n    \n    substrings = set([text])\n    _low = _start\n    _high = _end\n    for i in range(steps):\n        _low = _low - i if _low - i > 0 else 0\n        _high = _high + i if _high + 1 < (len(_text) - 1) else len(_text) - 1\n        \n        to_add = \" \".join(_text[_low:_high])\n        if to_add != selected:\n            substrings.add(to_add)\n    \n    return list(substrings)\n\n\ntrain_df = read_train()\n\nrows = train_df.to_dict(\"records\")\nnew_rows = []\ntot_text = set ()\nfor row in tqdm(rows):\n    if row[\"sentiment\"] != \"neutral\" and len(row[\"text\"].split()) > 3:\n        _text = row['text']\n        _sel = row[\"selected_text\"]\n        add = expand(_text, _sel)\n        for a in add:\n            if _sel in a:\n                if a != _text:\n                    new_row = deepcopy(row)\n                    new_row[\"text\"] = a\n                    new_rows.append(new_row)\n    new_rows.append(row)\n\n\ntrain_df = pd.DataFrame(new_rows)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Models\n\nEvery project needs a bunch of very very basic approaches as first trial, for this one we're gonna try the following:\n- Using whole text as target\n- N-gram selector (coming soon)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef whole_text_classifier(test):\n    test[\"selected_text\"] = test['text']\n    test = test[[\"textID\", \"selected_text\"]]\n    test.to_csv('whole_text_submission.csv',index=False)\n    \nwhole_text_classifier(test_df)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The neural way!\nHere we are gonna explore some neural approach:\n- BERT Lstm: 0.33 (with cleaned text, meaning that cleaning is not a good idea here)\n- Enhanced DistilBERT + SQuAD (0.664)\n- Enhanced Albert (0.666)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### BERT Lstm\n\nHere a multi-input model using BERT embedding for predicting the target selected text.\n\n\n- We start preparing the data, using distilbert tokenizer.\n- Then the Distilbert pretrained tokenized (uncased) is loaded and saved\n- After, we reload and use BertWordPieceTokenizer.\n- The comment text is prepared and encoded using this tokenizer easily.\n- We set the maxlen=128\n- We load the pretrained bert ('uncased') transformer layer,  used for creating the representations and training our corpus.\n- We then create the representation for the selected text from tweet text (create_targets function). This representation is created such that the positions of tokens which is selected from text is represented with 1 and others with 0.\n- We then create a multi-input model (comment + sentiment label). In our case is a simple LSTM model where we concatenate the inputs\n- Finally, we train the model, output predictions and re-alling those predictions using tokens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport numpy as np \nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom transformers import BertTokenizer,BertConfig,TFBertModel\nfrom tqdm import tqdm\ntqdm.pandas()\nBERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\ndef find_offset(x,y):\n    \"\"\"find offset in fail scenarios (only handles the start fail as of now)\"\"\"\n    x_str = ' '.join(x)\n    y_str = ' '.join(y)\n    idx0=0\n    ## code snippet from this https://www.kaggle.com/abhishek/text-extraction-using-bert-w-sentiment-inference\n    for ind in (i for i, e in enumerate(x_str) if e == y_str[0]):\n        if (x_str[ind: ind+len(y_str)] == y_str) or (x_str[ind: ind+len(y_str.replace(' ##',''))] == y_str.replace(' ##','')):\n            idx0 = ind\n            idx1 = ind + len(y_str) - 1\n            break\n    t = 0\n    for offset,i in enumerate(x):\n        if t +len(i)+1>idx0:\n            break\n        t = t+len(i)+1\n    return offset\n\ndef create_targets(df, tokenizer):\n    df['t_text'] = df['text'].apply(lambda x: tokenizer.tokenize(str(x)))\n    df['t_selected_text'] = df['selected_text'].apply(lambda x: tokenizer.tokenize(str(x)))\n    def func(row):\n        x,y = row['t_text'],row['t_selected_text'][:]\n        _offset = 0\n        for offset in range(len(x)):\n            _offset = offset\n            d = dict(zip(x[offset:],y))\n            #when k = v that means we found the offset\n            check = [k==v for k,v in d.items()]\n            if all(check)== True:\n                break \n        targets = [0]*_offset + [1]*len(y) + [0]* (len(x)-_offset-len(y))\n        \n        ## should be same if not its a fail scenario because of  start or end issue \n        if len(targets) != len(x):\n            offset = find_offset(x,y)\n            targets = [0]*offset + [1]*len(y) + [0] * (len(x)-offset-len(y))\n        return targets\n    df['targets'] = df.apply(func,axis=1)\n    return df\n\ndef _convert_to_transformer_inputs(text, tokenizer, max_sequence_length):\n    def return_id(str1, str2, truncation_strategy, length):\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids, input_masks, input_segments = return_id(text, None, 'longest_first', max_sequence_length)\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arrays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df.iterrows()):\n        t = str(instance.text)\n\n        ids, masks, segments= _convert_to_transformer_inputs(t,tokenizer, max_sequence_length)\n        \n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns].values.tolist())\n\ndef create_model():\n    id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    attn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    config = BertConfig() \n    config.output_hidden_states = True \n    \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    _,_, hidden_states = bert_model(id, attention_mask=mask, token_type_ids=attn)\n\n    h12 = tf.reshape(hidden_states[-1][:,0],(-1,1,768))\n    h11 = tf.reshape(hidden_states[-2][:,0],(-1,1,768))\n    h10 = tf.reshape(hidden_states[-3][:,0],(-1,1,768))\n    h09 = tf.reshape(hidden_states[-4][:,0],(-1,1,768))\n\n    concat_hidden = tf.keras.layers.Concatenate(axis=2)([h12, h11, h10, h09])\n    x = tf.keras.layers.GlobalAveragePooling1D()(concat_hidden)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(MAX_TARGET_LEN, activation='sigmoid')(x)\n    model = tf.keras.models.Model(inputs=[id, mask, attn], outputs=x)\n    return model\n\ndef convert_pred_to_text(df,pred):\n    temp_output = []\n    for idx,p in enumerate(pred):\n        indexes = np.where(p>0.5)\n        current_text = df['t_text'][idx]\n        if len(indexes[0])>0:\n            start = indexes[0][0]\n            end = indexes[0][-1]\n        else:\n            start = 0\n            end = len(current_text)\n\n        ### < was written previously but it should be > \n        ### (means model goes into padding tokens then restrict till the end of text)\n        ### Thanks Davide Romano for pointing this out\n        if end >= len(current_text):\n            end = len(current_text)\n        temp_output.append(' '.join(current_text[start:end+1]))\n    return temp_output\n\ndef correct_op(row):\n    placeholder = row['temp_output']\n    for original_token in str(row['text']).split():\n        token_str = ' '.join(tokenizer.tokenize(original_token))\n        placeholder = placeholder.replace(token_str,original_token,1)\n    return placeholder\n\ndef replacer(row):\n    if row['sentiment'] == 'neutral':\n        return row['text']\n    else:\n        return row['temp_output2']\n\"\"\" \ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\nMAX_TARGET_LEN = MAX_SEQUENCE_LENGTH = 108\ntrain_df = create_targets(train_df, tokenizer)\ntest_df['t_text'] = test_df['text'].apply(lambda x: tokenizer.tokenize(str(x)))\ntrain_df['targets'] = train_df['targets'].apply(lambda x :x + [0] * (MAX_TARGET_LEN-len(x)))\noutputs = compute_output_arrays(train_df,'targets')\ninputs = compute_input_arrays(train_df, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(test_df, tokenizer, MAX_SEQUENCE_LENGTH)\n\ntrain_inputs = inputs\ntrain_outputs = outputs\n\ndel inputs,outputs\nK.clear_session()\nmodel = create_model()\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer)\nif not os.path.exists('/kaggle/input/tweet-finetuned-bert-v2/finetuned_bert.h5'):\n    # Training done in another private kernel\n    model.fit(train_inputs, train_outputs, epochs=10, batch_size=32)\n    model.save_weights(f'finetuned_bert.h5')\nelse:\n    model.load_weights('/kaggle/input/tweet-finetuned-bert-v2/finetuned_bert.h5')\n    \nthreshold = 0.5\npredictions = model.predict(test_inputs, batch_size=32, verbose=1)\npred = np.where(predictions>threshold,1,0)\ntest_df['temp_output'] = convert_pred_to_text(test_df,pred)\ngc.collect()\ntest_df['temp_output2'] = test_df.progress_apply(correct_op,axis=1)\nsubmission_df['selected_text'] = test_df['temp_output2']\nsubmission_df['selected_text'] = submission_df['selected_text'].str.replace(' ##','')\nsubmission_df.to_csv('submission.csv',index=False)\nsubmission_df.head(10)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DistilBert + Squad\n\nHere we are gonna use an implementation from simpletransformers, really easy to setup and to use.\nIn this version we also try to convert sentiment into more meaningful questions for the model, since it has been pre-trained on squad it could make sense.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The model\nThe current version of the notebook makes use of the distilbert-base-uncased-distilled-squad model.\n\n\nDistilBERT paper: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\n\n\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\n\nThe distilBERT model has already been fine-tuned on a question-answering challenge: SQuAD, the Stanford Question Answering Dataset. \n\n\n### Simpletransformers\n\nTo keep the code to-the-point, this notebook makes use of an external python package: simpletransformers. For your convenience, the wheel files to install the package have already been stored in this database: Simple Transformers PyPI.\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n!mkdir -p data\n!pip install '/kaggle/input/simple-transformers-pypi/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '/kaggle/input/simple-transformers-pypi/simpletransformers-0.22.1-py3-none-any.whl' -q\nfrom simpletransformers.question_answering import QuestionAnsweringModel\nfrom copy import deepcopy\n\nuse_cuda = True\n\n    \ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n        \n    return paragraphs\n\ndef do_qa_test(test):\n    paragraphs = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return paragraphs\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df_distil = read_submission()\n\n\ntrain = np.array(train_df)\ntest = np.array(test_df)\nqa_train = do_qa_train(train)\n\n\nwith open('data/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\nqa_test = do_qa_test(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n    \nMODEL_PATH = '/kaggle/input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad/'\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                              args={\"reprocess_input_data\": True,\n                               \"overwrite_output_dir\": True,\n                               \"learning_rate\": 8e-05,\n                               \"num_train_epochs\": 3,\n                               \"max_seq_length\": 192,\n                               \"weight_decay\": 0.001,\n                               \"doc_stride\": 64,\n                               \"save_model_every_epoch\": False,\n                               \"fp16\": False,\n                               \"do_lower_case\": True,\n                                 'max_query_length': 8,\n                               'max_answer_length': 150,\n                                    \"use_cuda\":True\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data/train.json')\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\nsubmission_df_distil['selected_text'] = predictions_df['answer']\nsubmission_df_distil = post_process(submission_df_distil, test_df)\n\n\n\n#submission_df_distil.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper parameter tuning\n\nHere we gonna run a hyper-parameter selection procedure, in order to try different settings for our models, which fits best.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef do_qa_test2(test):\n    paragraphs = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-2]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return paragraphs\n\n\ntrain_df = read_train()\n\ntrain_df.dropna(inplace=True)\n\ntrain_size = int(0.70 * len(train_df))\n\nsub_train_df = train_df[:train_size]\nsub_test_df = train_df[train_size:]\n\nsub_test_df[\"predicted\"] = None\n\ntrain = np.array(sub_train_df)\ntest = np.array(sub_test_df)\nqa_train = do_qa_train(train)\n\n\nwith open('data/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\nqa_test = do_qa_test2(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n    \nMODEL_PATH = '/kaggle/input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad/'\nuse_cuda = True\n\n\nparameters = {\n    \"1\": {\n                   \"reprocess_input_data\": True,\n                   \"overwrite_output_dir\": True,\n                   \"learning_rate\": 8e-05,\n                   \"num_train_epochs\": 1,\n                   \"adam_epsilon\": 1e-08,\n                   \"max_seq_length\": 192,\n                   \"weight_decay\": 0.01,\n                   \"doc_stride\": 64,\n                   \"save_model_every_epoch\": False,\n                   \"fp16\": False,\n                   \"do_lower_case\": True,\n                    \"warmup_steps\": 200\n                },\n    \"2\": {\n                   \"reprocess_input_data\": True,\n                   \"overwrite_output_dir\": True,\n                   \"learning_rate\": 8e-05,\n                   \"num_train_epochs\": 1,\n                   \"adam_epsilon\": 1e-08,\n                   \"max_seq_length\": 192,\n                   \"weight_decay\": 0.01,\n                   \"doc_stride\": 64,\n                   \"save_model_every_epoch\": False,\n                   \"fp16\": False,\n                   \"do_lower_case\": True,\n                    \"warmup_steps\": 200,\n                    \"warmup_ratio\": 0.01\n                },\n    \"3\": {\n                   \"reprocess_input_data\": True,\n                   \"overwrite_output_dir\": True,\n                   \"learning_rate\": 8e-05,\n                   \"num_train_epochs\": 1,\n                   \"adam_epsilon\": 1e-08,\n                   \"max_seq_length\": 192,\n                   \"weight_decay\": 0.01,\n                   \"doc_stride\": 64,\n                   \"save_model_every_epoch\": False,\n                   \"fp16\": False,\n                   \"do_lower_case\": True,\n                    \"warmup_steps\": 200,\n                    \"warmup_ratio\": 0.1\n                },\n    \"4\": {\n                   \"reprocess_input_data\": True,\n                   \"overwrite_output_dir\": True,\n                   \"learning_rate\": 8e-05,\n                   \"num_train_epochs\": 1,\n                   \"adam_epsilon\": 1e-08,\n                   \"max_seq_length\": 192,\n                   \"weight_decay\": 0.01,\n                   \"doc_stride\": 64,\n                   \"save_model_every_epoch\": False,\n                   \"fp16\": False,\n                   \"do_lower_case\": True,\n                    \"warmup_steps\": 200,\n                    \"warmup_ratio\": 0.5\n                }\n    }\ndef jaccard(row): \n    a = set(row[1].lower().split()) \n    b = set(row[2].lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\nscores = {k: None for k in list(parameters.keys())}\n\ndef post_process2(submission_df, test_df):\n\n    index_to_selected_text = {}\n    for i, row in test_df.iterrows():\n        _id = row[0]\n        text = row[1]\n        sentiment = row[3]\n        if len(text.split(\" \")) <= 3 or sentiment == \"neutral\":\n            index_to_selected_text[_id] = text\n    \n    submission_rows = submission_df.to_dict(\"records\")\n    new_rows = []\n    for row in submission_rows:\n        _id = row['textID']\n        if _id in index_to_selected_text:\n            new_row = deepcopy(row)\n            new_row['selected_text'] = index_to_selected_text[_id]\n        else:\n            new_row = row\n        \n        new_rows.append(new_row)\n\n    return pd.DataFrame(new_rows)\n\n\ndef align_test_pred(predictions_df, test_df):\n    \n    pred_text = {}\n    for i, row in predictions_df.iterrows():\n        _id = row[0]\n        _pred = row[1]\n        pred_text[_id] = _pred\n        \n    test_rows = test_df.to_dict(\"records\")\n    new_rows = []\n    for t in test_rows:\n        _id = t['textID']\n        if _id in pred_text:\n            new_row = t\n            new_row['predicted'] = pred_text[_id]\n            new_rows.append(new_row)\n    \n    new_df = pd.DataFrame(new_rows)\n    return new_df\n    \nfor key, param in parameters.items():\n    sub_test_df = train_df[train_size:]\n\n    sub_test_df[\"predicted\"] = None\n    \n    model = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                              args=param,\n                              use_cuda=use_cuda)\n\n    model.train_model('data/train.json')\n    predictions = model.predict(qa_test)\n    predictions_df = pd.DataFrame.from_dict(predictions)\n    \n    sub_test_df = align_test_pred(predictions_df, sub_test_df)\n    \n    sub_test_df = sub_test_df[[\"textID\",\"selected_text\", \"predicted\", \"sentiment\"]]\n    \n    sub_test_df = post_process2(sub_test_df, sub_test_df)\n    \n    ## Based on discussion https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140235\n    def f(selected):\n         return \" \".join(set(selected.lower().split()))\n\n    sub_test_df.predicted = sub_test_df.predicted.map(f)\n    sub_test_df['jaccard'] = sub_test_df.apply(lambda x: jaccard(x), axis=1)\n    average_jaccard = sub_test_df['jaccard'].mean()\n    scores[key] = average_jaccard\n    print(\"Params tested: {} - {}\".format(key, json.dumps(param, indent=3)))\n    print(\"Jaccard: {}\".format(average_jaccard))\n    print(\":::::\")\n\n\nprint(scores)\n\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying ALBERT (SOTA for QA)\n\nThis experiment replicates the previous one but using Albert","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n!mkdir -p data\n!pip install '/kaggle/input/simple-transformers-pypi/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '/kaggle/input/simple-transformers-pypi/simpletransformers-0.22.1-py3-none-any.whl' -q\nfrom simpletransformers.question_answering import QuestionAnsweringModel\nfrom copy import deepcopy\n\nuse_cuda = True\n    \ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n        \n    return paragraphs\n\ndef do_qa_test(test):\n    paragraphs = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return paragraphs\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df_albert = read_submission()\n\n\ntrain = np.array(train_df)\ntest = np.array(test_df)\nqa_train = do_qa_train(train)\n\n\nwith open('data/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\nqa_test = do_qa_test(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n    \nMODEL_PATH = '/kaggle/input/pretrained-albert-pytorch/albert-base-v2/'\nmodel = QuestionAnsweringModel('albert', \n                               MODEL_PATH, \n                               args={\"reprocess_input_data\": True,\n                               \"overwrite_output_dir\": True,\n                               \"learning_rate\": 8e-05,\n                               \"num_train_epochs\": 3,\n                               \"max_seq_length\": 192,\n                               \"weight_decay\": 0.001,\n                               \"doc_stride\": 64,\n                               \"save_model_every_epoch\": False,\n                               \"fp16\": False,\n                               \"do_lower_case\": True,\n                                 'max_query_length': 8,\n                               'max_answer_length': 150\n                                },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data/train.json')\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\nsubmission_df_albert['selected_text'] = predictions_df['answer']\nsubmission_df_albert = post_process(submission_df_albert, test_df)\n\n#submission_df_albert.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n!mkdir -p data\n!pip install '/kaggle/input/simple-transformers-pypi/seqeval-0.0.12-py3-none-any.whl' -q\n!pip install '/kaggle/input/simple-transformers-pypi/simpletransformers-0.22.1-py3-none-any.whl' -q\nfrom simpletransformers.question_answering import QuestionAnsweringModel\nfrom copy import deepcopy\n\nuse_cuda = True\n    \ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n        \n    return paragraphs\n\ndef do_qa_test(test):\n    paragraphs = []\n    for line in test:\n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return paragraphs\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df_bert = read_submission()\n\n\ntrain = np.array(train_df)\ntest = np.array(test_df)\nqa_train = do_qa_train(train)\n\n\nwith open('data/train.json', 'w') as outfile:\n    json.dump(qa_train, outfile)\n\noutput = {}\noutput['version'] = 'v1.0'\noutput['data'] = []\n\nqa_test = do_qa_test(test)\n\nwith open('data/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n    \nMODEL_PATH = '/kaggle/input/bert-base-uncased/'\nmodel = QuestionAnsweringModel('bert', \n                               MODEL_PATH, \n                               args={\"reprocess_input_data\": True,\n                               \"overwrite_output_dir\": True,\n                               \"learning_rate\": 8e-05,\n                               \"num_train_epochs\": 3,\n                               \"max_seq_length\": 192,\n                               \"weight_decay\": 0.001,\n                               \"doc_stride\": 64,\n                               \"save_model_every_epoch\": False,\n                               \"fp16\": False,\n                               \"do_lower_case\": True,\n                                 'max_query_length': 8,\n                               'max_answer_length': 150\n                                },\n                              use_cuda=use_cuda)\n\nmodel.train_model('data/train.json')\npredictions = model.predict(qa_test)\npredictions_df = pd.DataFrame.from_dict(predictions)\n\nsubmission_df_bert['selected_text'] = predictions_df['answer']\nsubmission_df_bert = post_process(submission_df_bert, test_df)\n\n#submission_df_albert.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)# Merge predictions!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsentiment_analyzer = SentimentIntensityAnalyzer()\n\ndef merge_predictions(submission_distil, sumbission_albert, submission_bert):\n    def merge(row):\n        pred_1 = set(str(row[1]).split())\n        pred_2 = set(str(row[2]).split())\n        pred_3 = set(str(row[3]).split())\n        res = []\n        \n        res = list(pred_1.intersection(pred_2, pred_3))\n        if len(res) == 0:\n            res = list(pred_1.union(pred_2, pred_3))\n        \n        return \" \".join(res)\n    \n    def merge_compound(row):\n        pred_1 = str(row[1])\n        pred_2 = str(row[2])\n        pred_3 = str(row[3])\n        \n        preds = [pred_1, pred_2, pred_3]\n        \n        comp_1 = abs(sentiment_analyzer.polarity_scores(pred_1).get(\"compound\",0))\n        comp_2 = abs(sentiment_analyzer.polarity_scores(pred_2).get(\"compound\",0))\n        comp_3 = abs(sentiment_analyzer.polarity_scores(pred_3).get(\"compound\",0))\n        \n        comps = [comp_1, comp_2, comp_3]\n        \n        best_comp = comps.index(max(comps))\n        return preds[best_comp]\n    \n    def capitalize_selected(df):\n        rows = df.to_dict(\"records\")\n        new_rows = []\n        \n        for r in rows:\n            text = r[\"text\"]\n            lower_text = text.lower()\n            pred_distil = r[\"selected_text_distil\"]\n            pred_albert = r[\"selected_text_albert\"]\n            pred_bert = r[\"selected_text_bert\"]\n            \n            new_pred_distil = text[lower_text.find(pred_distil):len(pred_distil)]\n            new_pred_albert = text[lower_text.find(pred_albert):len(pred_albert)]\n            new_pred_bert = text[lower_text.find(pred_bert):len(pred_bert)]\n            \n            new_row = {\n                \"textID\": r['textID'],\n                \"selected_text_distil\": new_pred_distil,\n                \"selected_text_albert\": new_pred_albert,\n                \"selected_text_bert\": new_pred_bert\n            }\n            new_rows.append(new_row)\n        \n        return pd.DataFrame(new_rows)\n    \n    sumbission_albert = sumbission_albert.rename({'selected_text': 'selected_text_albert'}, axis=1) \n    del sumbission_albert[\"textID\"]\n    submission_distil = submission_distil.rename({'selected_text': 'selected_text_distil'}, axis=1) \n    \n    submission_bert = submission_bert.rename({'selected_text': 'selected_text_bert'}, axis=1) \n    del submission_bert[\"textID\"]\n    \n    final_sub = pd.concat([submission_distil, sumbission_albert, submission_bert], axis=1)\n    \n    #final_sub = capitalize_selected(final_sub)\n    final_sub[\"selected_text\"] = final_sub.apply(lambda x: merge_compound(x), axis=1)\n    \n    del final_sub[\"selected_text_albert\"]\n    del final_sub[\"selected_text_distil\"]\n    del final_sub[\"selected_text_bert\"]\n    \n    return final_sub\n\n\"\"\"\nsubmission_df_albert = read_submission()\nsubmission_df_distil = read_submission()\nsubmission_df_bert = read_submission()\ntest_df = read_test()\n\nsubmission_df_albert.selected_text = [x.lower() for x in test_df.text]\nsubmission_df_distil.selected_text = [x.lower() for x in test_df.text]\nsubmission_df_bert.selected_text = [x.lower() for x in test_df.text]\n\n\"\"\"\n\nsubmission = merge_predictions(submission_df_distil, submission_df_albert, submission_df_bert)\n\n\nsubmission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}