{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Jaccard Expectation Maximization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"That's notebook with implementation for some post process method : Jaccard Expectation Maximization (JEM). \n\nTopic with explantions: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/158613.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\nimport tokenizers\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n\nI just take one of my prediction for testing strategy, you can fill this part of notebook with the same data. We need the next ones:\n- oof start/end/selected_text prediction (+ oof tweet text)\n- tokenizer, that you used in training time\n- [optional]: I also use splitter for recover correct indexes for oof prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_train():\n    train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n    \ndef read_submission():\n    test=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n    return test\n\ntrain_df = read_train()\ntest_df = read_test()\nsubmission_df = read_submission()\n\ntrain_df = read_train()\ntest_df = read_test()\n\n# there was one NaN value inside tweets in train_df\nassert train_df[\"text\"].isna().sum() <= 1\ntrain_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\nsplits = list(skf.split(np.arange(len(train_df)), train_df.sentiment.values))\nval_inds_arr = [val_inds for tr_inds, val_inds in splits]\nval_inds_arr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 5\n\ndef get_union_df(name=\"train_prediction\", inds_arr=None, agg_f=None):\n    \"\"\" function for gathering results from each fold (for test with aggregation (agg_f) and for oof without one) \"\"\"\n    df = DataFrame()\n    for n_fold in range(N_FOLDS):\n        fold_df = (\n            pd\n            .read_csv(\"../input/bestoofprediction/{}_{}.csv\".format(name, n_fold + 1))\n            .drop(\"Unnamed: 0\", axis=1)\n        )\n\n        if inds_arr is not None:\n            fold_df.index = inds_arr[n_fold]\n\n        df = pd.concat([df, fold_df])\n\n    if agg_f:\n        df = df.astype(np.float32)\n        df = df.groupby(df.index).agg(agg_f)\n        \n    return df.sort_index()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"oof_start_proba = get_union_df(name=\"validation_start_prediction\", inds_arr=val_inds_arr)\noof_end_proba = get_union_df(name=\"validation_end_prediction\", inds_arr=val_inds_arr)\n\noof_start_proba.shape, oof_end_proba.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_start_proba.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load tokenizer and get the oof prediction with (oof_start_prediction, oof_end_prediction) tuple:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef get_pred(start_proba, end_proba, df, tokenizer):\n    pred = []\n    n_samples = len(start_proba)\n    for i in range(n_samples):\n        text = df['text'][df.index[i]]\n        a, b = np.argmax(start_proba[i]), np.argmax(end_proba[i])\n        if a > b: \n            pred_ = text # IMPROVE CV/LB with better choice here\n        else:\n            cleaned_text = \" \" + \" \".join(text.split())\n            encoded_text = tokenizer.encode(cleaned_text)\n            pred_ids = encoded_text.ids[a - 2: b - 1]\n            pred_ = tokenizer.decode(pred_ids)\n        pred += [pred_]\n\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt',\n    lowercase=True,\n    add_prefix_space=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"pred_selected_text\"] = get_pred(oof_start_proba.values, oof_end_proba.values, train_df, tokenizer)\ntrain_df[\"jaccard\"] = train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"pred_selected_text\"]), axis=1)\ntrain_df = train_df.sort_values(\"jaccard\")\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also compute the model confidence:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.sort_index()\ntrain_df[\"confidence\"] = 0.5 * (oof_start_proba.max(1) + oof_end_proba.max(1))\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_score = train_df[\"jaccard\"].mean()\nprint(f'oof score before optimization: {oof_score:.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# JEP Implementation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now, let's implement new approach. At first, implement getting **hypo_df** table with next fields:\n- **start** - start index prediction\n- **end** - end index prediction\n- **proba** - prediction probability : $0.5 \\cdot (start\\_proba + end\\_proba)$ (but can use other functions)\n\nsorted by **proba** in decreasing oreder. For compuatinal effectiveness we can compute only first **beam_size** rows (**beam_size** = 100 by default).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_hypo_df(start_proba, end_proba, beam_size=100):\n    start2top_proba = Series(start_proba).sort_values(ascending=False)[:beam_size]\n    end2top_proba   = Series(end_proba  ).sort_values(ascending=False)[:beam_size]\n\n    hypos = []\n    for start, start_proba in start2top_proba.items():\n        for end, end_proba in end2top_proba.items():\n            proba = 0.5 * (start_proba + end_proba)\n            hypos += [(start, end, proba)]\n\n    return DataFrame(hypos, columns=[\"start\", \"end\", \"proba\"]).sort_values(\"proba\")[::-1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Example of **hypo_df** computation for some sample:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = 0\nstart_proba = oof_start_proba.values[ind,]\nend_proba   = oof_end_proba.values  [ind,]\n\ntext_len = max((start_proba != 0).sum(), (end_proba != 0).sum())\nstart_proba = start_proba[:text_len + 5]\nend_proba   =   end_proba[:text_len + 5]\n\nstart_proba.shape, end_proba.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hypo_df = get_hypo_df(start_proba, end_proba, beam_size=10)\n\nhypo_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now implement JEPP with next algorithm (example in topic https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/158613):\n1. Compute **hypo_df**.\n2. With **hypo_df** for each row compute jaccard expectation with assumption, that correct selected_text is the one, that corresponds this row.\n3. Find row that maximize jaccard expectation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_jaccard_expectation(start0, end0, hypo_df, encoded_text, tokenizer):\n    jaccard_expectation_logit, logit_sum = 0, 0\n    selected_text0 = tokenizer.decode(encoded_text[start0 - 2: end0 - 1])\n    for start, end, logit in zip(hypo_df[\"start\"], hypo_df[\"end\"], hypo_df[\"proba\"]):\n        selected_text  = tokenizer.decode(encoded_text[start - 2: end - 1])\n        if (start <= end) and (start >= 2) and (selected_text.strip() != ''):\n            jaccard_val = jaccard(selected_text0, selected_text)\n            jaccard_expectation_logit += logit * jaccard_val\n            logit_sum += logit\n\n    return jaccard_expectation_logit / logit_sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_selected_text(ind, oof_start_prediction, oof_end_prediction, train_df, tokenizer, beam_size=1):\n    text = \" \" + \" \".join(train_df[\"text\"][ind].split())\n    encoded_text = tokenizer.encode(text).ids\n    text_len = len(encoded_text)\n    \n    start_proba = oof_start_prediction.values[ind,][:text_len + 5]\n    end_proba   = oof_end_prediction.values  [ind,][:text_len + 5]\n    hypo_df = get_hypo_df(start_proba, end_proba, beam_size=100)\n\n    max_jaccard_expectation, best_selected_text = 0, \"\"\n    for start0, end0 in zip(hypo_df[\"start\"][:beam_size], hypo_df[\"end\"][:beam_size]):\n        selected_text0  = tokenizer.decode(encoded_text[start0 - 2: end0 - 1])\n        if (start0 <= end0) and (start0 >= 2) and (selected_text0.strip() != ''):\n            jaccard_expectation = get_jaccard_expectation(start0, end0, hypo_df, encoded_text, tokenizer)\n            if jaccard_expectation > max_jaccard_expectation:\n                max_jaccard_expectation = jaccard_expectation\n                best_selected_text = tokenizer.decode(encoded_text[start0 - 2: end0 - 1])\n\n    return max_jaccard_expectation, best_selected_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compute all predictions (**pred_selected_text2**) and corresponding JEM (**confidence2**):","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"N_SAMPLES = 30000\n\nmax_jaccard_expectations, selected_texts = [], []\nfor ind in tqdm(train_df[:N_SAMPLES].index):\n    max_jaccard_expectation, selected_text =  get_best_selected_text(ind, oof_start_proba, oof_end_proba, train_df, tokenizer, beam_size=2)\n    max_jaccard_expectations += [max_jaccard_expectation]\n    selected_texts += [selected_text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"used_train_df = train_df[:N_SAMPLES].copy()\nused_train_df[\"pred_selected_text2\"] = selected_texts\nused_train_df[\"confidence2\"] = max_jaccard_expectations\nused_train_df[\"jaccard2\"] = used_train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"pred_selected_text2\"]), axis=1)\nused_train_df.sort_values(\"confidence2\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can use differen strategies to apply JEM:\n- for samples with small confidence\n- for samples with large confidence2\n- for samples with small confidence2\n- for samples with confidence < **thresh** * confidence2 for some threshold **thresh**\n\nYou can use each of them, but for my case 3rd strategy is the best. Find best treshold:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"old_jaccards = {}\nnew_jaccards = {}\n\nfor thresh in np.arange(0.1, 1.1, 0.1):\n    old_jaccard = used_train_df[used_train_df[\"confidence2\"] < thresh][\"jaccard\" ].mean()\n    new_jaccard = used_train_df[used_train_df[\"confidence2\"] < thresh][\"jaccard2\"].mean()\n    old_jaccards[thresh] = old_jaccard\n    new_jaccards[thresh] = new_jaccard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nplt.title(\"Jaccard Curves\")\n\nplt.plot(list(old_jaccards.keys()), list(old_jaccards.values()), label=\"old jaccard\")\nplt.plot(list(new_jaccards.keys()), list(new_jaccards.values()), label=\"new jaccard\")\n_ = plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_thresh = (Series(new_jaccards) - Series(old_jaccards)).idxmax()\nbest_thresh","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And compute the boost:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_prediction(row):\n    if row[\"confidence2\"] < best_thresh:\n        return jaccard(row[\"selected_text\"], row[\"pred_selected_text2\"])\n    return jaccard(row[\"selected_text\"], row[\"pred_selected_text\"])   \n\nused_train_df[\"best_jaccard\"] = used_train_df.apply(lambda row: get_best_prediction(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"old_oof_score = used_train_df[\"jaccard\"].mean()\nnew_oof_score = used_train_df[\"best_jaccard\"].mean()\nprint(f'oof score before optimization: {old_oof_score:.5f}')\nprint(f'oof score after optimization: {new_oof_score:.5f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Boost is not so high, but it's diffrenet for different probability predictions. Hope, it helps.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}