{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Description**\n* Added NER [0-X,1-Positive,2-Negative,3-Neutral],[0- not part of selected sentence],[1,2,3- part of pos,neg,neu,sentences respectively] to train both tasks simultaneously \n* Extracted Sentence using Span Extraction (I failed using NER predictions during Inference) \n* Got a lower score Compared to reference Kernel, I'm not sure if doing two tasks at once is helpful [Pls share your insights on this]\n* Reference Code: [Tweet Sentiment RoBERTa PyTorch](https://www.kaggle.com/shoheiazuma/tweet-sentiment-roberta-pytorch)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking CUDA Avail","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda_yes = torch.cuda.is_available()\n# cuda_yes = False\nprint('Cuda is available?', cuda_yes)\ndevice = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\nprint('Device:', device)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seed","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed = 42\nseed_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file='../input/roberta-base/vocab.json', \n            merges_file='../input/roberta-base/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n#         self._label_types = ['X', 'positive', 'negative','neutral']\n#         self._label_map = {label: i for i,\n#                            label in enumerate(self._label_types)}\n\n#         self.num_labels=num_labels\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        data['sentiment'] = row.sentiment\n        if self.labeled:\n            start_idx, end_idx,labels = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n            data['labels']=labels\n            data['selected_text']=row.selected_text\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        # [0,1,2,3]\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n        if row.sentiment==\"positive\":\n            sent=1\n        elif row.sentiment==\"negative\":\n            sent=2\n        else:\n            sent=3\n            \n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 is not None and idx1 is not None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        labels = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n                labels.append(sent)\n            elif j==1: # Sentiment in input\n                labels.append(sent)\n            else:\n                labels.append(0)\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        labels = torch.tensor(labels)\n        return start_idx, end_idx, labels\n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\ndef get_test_loader(df, batch_size=32):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)    \n    return loader","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        \n        config = RobertaConfig.from_pretrained(\n            '../input/roberta-base/config.json', output_hidden_states=True,num_labels=NUM_LABELS)    \n        self.roberta = RobertaModel.from_pretrained(\n            '../input/roberta-base/pytorch_model.bin', config=config)\n        self.num_labels = config.num_labels\n        self.dropout = nn.Dropout(0.5)\n        self.fc = nn.Linear(config.hidden_size, 2)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        nn.init.normal_(self.fc.weight, std=0.02)\n        nn.init.normal_(self.fc.bias, 0)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        sequence_output, _, hs = self.roberta(input_ids, attention_mask)\n         \n        x = torch.stack([hs[-1], hs[-2], hs[-3],hs[-4]])\n        x = torch.mean(x, 0)\n        x_drop = self.dropout(x)\n        x = self.fc(x_drop)\n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n        logits = self.classifier(x_drop)\n        \n        outputs= (logits,start_logits, end_logits)\n        \n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n                \n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n        \n        return outputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ndef compute_jaccard_score(text,selected_text,start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    jaccard_approx = jaccard(true, pred)\n    jaccard_true = jaccard(selected_text,pred)\n    return jaccard_approx,jaccard_true\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, dataloaders_dict, criterion, optimizer, num_epochs, filename):\n    model.to(device)\n\n    for epoch in range(num_epochs):\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            epoch_loss = 0.0\n            epoch_approx_jaccard = 0.0\n            epoch_true_jaccard = 0.0\n            epoch_start_end_loss=0.0\n            epoch_ner_loss=0.0\n            tk0 = tqdm(dataloaders_dict[phase], total=len(dataloaders_dict[phase]))\n            for data in tk0:\n                ids = data['ids'].to(device)\n                masks = data['masks'].to(device)\n                tweet = data['tweet']\n                selected_text=data['selected_text']\n                sentiment = data['sentiment']\n                offsets = data['offsets'].numpy()\n                start_idx = data['start_idx'].to(device)\n                end_idx = data['end_idx'].to(device)\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    ner_loss,logits,start_logits, end_logits = model(ids, masks,labels)\n\n                    start_end_loss = criterion(start_logits, end_logits, start_idx, end_idx)\n                    loss=ner_loss+start_end_loss\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        scheduler.step()\n\n                    epoch_loss += loss.item() * len(ids)\n                    epoch_start_end_loss += start_end_loss.item() * len(ids)\n                    epoch_ner_loss += ner_loss.item() * len(ids)\n                    start_idx = start_idx.cpu().detach().numpy()\n                    end_idx = end_idx.cpu().detach().numpy()\n                    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n                    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n                    \n                    for i in range(len(ids)):                        \n                        jaccard_approx_score,jaccard_true_score = compute_jaccard_score(\n                            tweet[i],\n                            selected_text[i],\n                            start_idx[i],\n                            end_idx[i],\n                            start_logits[i], \n                            end_logits[i], \n                            offsets[i])\n                        \n                        epoch_approx_jaccard += jaccard_approx_score\n                        epoch_true_jaccard += jaccard_true_score\n                        \n                    \n            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n            epoch_ner_loss = epoch_ner_loss / len(dataloaders_dict[phase].dataset)\n            epoch_start_end_loss = epoch_start_end_loss / len(dataloaders_dict[phase].dataset)\n            epoch_approx_jaccard = epoch_approx_jaccard / len(dataloaders_dict[phase].dataset)\n            epoch_true_jaccard = epoch_true_jaccard / len(dataloaders_dict[phase].dataset)\n            \n            print('Epoch {}/{} | {:^5} | Loss: {:.4f} | NER_Loss: {:.4f} | Start_End_Loss: {:.4f} | Approx_Jaccard: {:.4f} | True_Jaccard: {:.4f}'.format(\n                epoch + 1, num_epochs, phase, epoch_loss,epoch_ner_loss,epoch_start_end_loss,epoch_approx_jaccard,epoch_true_jaccard))\n    \n    torch.save(model.state_dict(), filename)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training [Skipped]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 3\nbatch_size = 32\ngradient_accumulation_steps = 1\nwarmup_proportion=0.1\nNUM_LABELS=4\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n\n# train_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n# train_df['text'] = train_df['text'].astype(str)\n# train_df['selected_text'] = train_df['selected_text'].astype(str)\n\n# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df.sentiment), start=1): \n#     print(f'Fold: {fold}')\n\n#     model = TweetModel()\n#     optimizer = optim.AdamW(model.parameters(), lr=3e-5, betas=(0.9, 0.999))\n#     total_train_steps = int(len(train_df) / batch_size / gradient_accumulation_steps * num_epochs)\n\n#     criterion = loss_fn    \n#     dataloaders_dict = get_train_val_loaders(train_df, train_idx, val_idx, batch_size)\n#     scheduler = get_linear_schedule_with_warmup(optimizer,warmup_proportion*total_train_steps,total_train_steps)\n\n#     train_model(\n#         model, \n#         dataloaders_dict,\n#         criterion, \n#         optimizer, \n#         num_epochs,\n#         f'roberta_fold{fold}.pth')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df)\npredictions = []\nmodels = []\nmodel_dir = \"../input/tweet-sentiment-roberta-pytorch/\"\nfor fold in range(skf.n_splits):\n    model = TweetModel()\n    model.to(device)\n    model.load_state_dict(torch.load(model_dir+f'roberta_fold{fold+1}.pth',map_location=torch.device(device)))\n    model.eval()\n    models.append(model)\n\nfor data in tqdm(test_loader):\n    ids = data['ids'].to(device)\n    masks = data['masks'].to(device)\n    tweet = data['tweet']\n    sentiment = data['sentiment']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(torch.softmax(output[1], dim=1).cpu().detach().numpy())\n            end_logits.append(torch.softmax(output[2], dim=1).cpu().detach().numpy())\n\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    for i in range(len(ids)):    \n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        \n        if (start_pred > end_pred):\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n        predictions.append(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\nsub_df['selected_text'] = predictions\n\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}