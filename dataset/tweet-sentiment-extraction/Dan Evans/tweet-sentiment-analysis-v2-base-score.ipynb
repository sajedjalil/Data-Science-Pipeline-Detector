{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Choosing the text that supports tweet sentiment classification\n\nThis is my second attempt at an NLP approach. My first one is not great, and times out when I try to submit answers. So, attempt the second!\n\nThe EDA I did in my v1 notebook won't be recapitulated here beyond the fact that neutral tweets usually have the whole phrase as \"selected_text\" and positive/negative tweets have about 20% +- 30% of their text as selected_text.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    \"\"\"the Jaccard score of two strings. Provided by the competition rules\"\"\"\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n# stopword set\nsw = set(stopwords.words('english'))\ncommon = 100\n\ndef filter_stopwords(tokens, sws):\n    \"\"\"return a sublist of tokens that are not stopwords\"\"\"\n    return [x for x in tokens if not x in sws]\n\nimport re\n\n#read in the training data and drop the one row an empty string\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv', header=0)\ntrain_df.dropna(inplace=True)\n\n#preprocessing to fix the funkiness - taken from https://www.kaggle.com/dhananjay3/investigating-html\ndef remove_html_char_ref(i):\n    i = i.replace(\"&quot;\", '\"')\n    i = i.replace(\"&lt;\", '<')\n    i = i.replace(\"&gt;\", '>')\n    i = i.replace(\"&amp;\", '&')\n    return i\n\ntrain_df['text'] = train_df.apply(lambda x: remove_html_char_ref(x['text']), axis=1)\ntrain_df['selected_text'] = train_df.apply(lambda x: remove_html_char_ref(x['selected_text']), axis=1)\n\n#train_df['selected_text'] = train_df['selected_text'].fillna(\"blank\")\n\ntrain_df['text_no_punc'] = train_df.apply(lambda x: re.sub(\"[^\\w\\s]\",\"\",x['text']), axis=1)\ntrain_df['selected_text_no_punc'] = train_df.apply(lambda x: re.sub(\"[^\\w\\s]\",\"\", x['selected_text']), axis=1)\ntrain_df['text_tokens'] = train_df.apply(lambda x: x['text_no_punc'].lower().split(), axis=1)\ntrain_df['selected_text_tokens'] = train_df.apply(lambda x: x['selected_text_no_punc'].lower().split(), axis=1)\ntrain_df['text_tokens_no_stop'] = train_df.apply(lambda x: filter_stopwords(x['text_tokens'], sw), axis=1)\ntrain_df['selected_text_tokens_no_stop'] = train_df.apply(lambda x: filter_stopwords(x['selected_text_tokens'], sw), axis=1)\n\npos = train_df.loc[train_df['sentiment'] == 'positive']\nneg = train_df.loc[train_df['sentiment'] == 'negative']\n\nprint(pos.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Base Expectation Score\nThis notebook will create a submission based on the following:\n- for positive/negative tweet:\n  - selected_text is most-occuring non-stopword single token from the text\n- for netural tweet:\n  - selected_text is entire tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\n\npwc = collections.Counter()\n#pos.apply(lambda x: pwc.update(filter_stopwords(x['selected_text_tokens'], sw)), axis=1)\npos.apply(lambda x: pwc.update(filter_stopwords(x['selected_text_tokens_no_stop'], sw)), axis=1)\n#print(pwc.most_common(common))\npwc_dict = dict(pwc)\n#print(\"---\")\nnwc = collections.Counter()\n#neg.apply(lambda x: nwc.update(filter_stopwords(x['selected_text_tokens'], sw)), axis=1)\nneg.apply(lambda x: nwc.update(filter_stopwords(x['selected_text_tokens_no_stop'], sw)), axis=1)\n#print(nwc.most_common(common))\nnwc_dict = dict(nwc)\n\ndef strip_punc(text):\n    \"\"\"return text stripped of punctuation\"\"\"\n    return re.sub(\"[^\\w\\s]\",\"\", text)\n\ndef tokenize(text):\n    \"\"\"tokenize the passed-in text\"\"\"\n    return text.lower().split()\n    \ndef choose_selected_text(text, sentiment):\n    \"\"\"choose the selected text for the passed-in text and sentiment\"\"\"\n    if sentiment == 'neutral':\n        return text\n    elif sentiment == 'positive':\n        ctr = pwc\n    else:\n        ctr = nwc\n        \n    text_tokens_no_stop = filter_stopwords(tokenize(strip_punc(text)), sw)\n    best_count = 0\n    best_word = ''\n    \n    for t in text_tokens_no_stop:\n        if t in ctr:\n            if ctr[t] > best_count:\n                best_count = ctr[t]\n                best_word = t\n                    \n    if best_count > 0:\n        #print(\"returning \", best_word)\n        return best_word\n    else:\n        #print(\"returning whole text\")\n        return text\n\ntrain_df['model_text'] = train_df.apply(lambda x: choose_selected_text(x['text'], x['sentiment']), axis=1)\ntrain_df['jaccard'] = train_df.apply(lambda x: jaccard(x['selected_text'], x['model_text']), axis=1)\nprint('training jaccard score: {0:.3f}'.format(np.mean(train_df['jaccard'])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv', header=0)\ntest_df['text'] = test_df['text'].fillna('blank')\ntest_df['selected_text'] = test_df.apply(lambda x: choose_selected_text(x['text'], x['sentiment']), axis=1)\n\n#test_df.loc[314,'selected_text'] = ' '\nprint(test_df.loc[314])\n#print(test_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output the correct data for scoring\nimport csv\n\nsub_df = test_df.loc[:,['textID','selected_text']]\nprint(sub_df.isnull().sum())\nprint(sub_df.loc[sub_df['selected_text'] == ''])\n#sub_df.to_csv('submission.csv', index=False)\n\nf = open('submission.csv','w')\nf.write('textID,selected_text\\n')\nfor index, row in sub_df.iterrows():\n    f.write('%s,\"%s\"\\n'%(row.textID,row.selected_text))\nf.close()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}