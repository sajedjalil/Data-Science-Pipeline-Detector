{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Tweet Sentiment Extraction**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this project, I take a my first dive into NLP tasks and sentiment work. Here, I have to extract the parts of tweets that gives it its sentiment: Positive, Negative, or Neutral. Although the approach I took didn't really incorporate machine learning models, it still performed mediocrely well. \n\nNote: I was well unprepared for this level of task and competition, therefore some of my code was copied and adopted from other models and notebooks posted already.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport re\nimport nltk.corpus\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk import ngrams\nfrom nltk import word_tokenize \nimport string\nimport operator\nfrom collections import Counter\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I load in the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I define two functions. The first one removes all punctuation from the tweets while the other removes all stop words from the tweets. This is my \"data cleaning\". I make a backup of my data just in case.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"string.punctuation\ndef remove_punc (string1):\n    string1 = string1.lower()\n    translation_table = dict.fromkeys(map(ord,string.punctuation),' ')\n    string2 = string1.translate(translation_table)\n    return string2\ndef remove_stop (string1):\n    pattern = re.compile(r'\\b(' +r'|'.join(stopwords.words('english'))+r')\\b\\s*')\n    string2 = pattern.sub('', string1)\n    return string2\n\ntrain['text_backup'] = train['text']\ntrain['text_select_backup'] = train['selected_text']\ntest['text_backup'] = test['text']\n\ntrain['text'] = train['text'].astype(str)\ntrain['selected_text'] = train['selected_text'].astype(str)\ntest['text'] = test['text'].astype(str)\n\ntrain['text'] = train['text'].apply(lambda x:remove_stop(x))\ntrain['text'] = train['text'].apply(lambda x:remove_punc(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:remove_stop(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:remove_punc(x))\ntest['text'] = test['text'].apply(lambda x:remove_stop(x))\ntest['text'] = test['text'].apply(lambda x:remove_punc(x))\n\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I do some feature engineering. These parts were adopted from other scripts and notebooks, helping me learn the process of using lambda functions in applying actions to entire columns. The features I've made are word count, average length of words, number of stopwords, most words, least words, punctuation, and most popular words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features\n#length of input or count of words\ntrain['Feature_1'] = train['text_backup'].apply(lambda x: len(str(x).split()))\ntrain['Feature_1a'] = train['text_select_backup'].apply(lambda x: len(str(x).split()))\ntest['Feature_1'] = test['text_backup'].apply(lambda x: len(str(x).split()))\n\n#avg length of word\ntrain['Feature_a'] = train[\"text_select_backup\"].apply(lambda x: len(str(x)))\ntrain['Feature_a'] = train['Feature_a'] / train['Feature_1a']\n\n#number of stopwords\nstop_words = set(stopwords.words('english'))\ntrain['Feature_2'] = train['text_backup'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\ntrain['Feature_2a'] = train['text_select_backup'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\ntest['Feature_2'] = test['text_backup'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n\n#most words\nall_text_without_sw = ''\nfor i in train.itertuples():\n    all_text_without_sw = all_text_without_sw + str(i.text)\ncounts = Counter(re.findall(r\"[\\w']+\",all_text_without_sw))\ndel counts [\"'\"]\nsorted_x = dict(sorted(counts.items(),key=operator.itemgetter(1),reverse=True)[:50])\ntrain['Feature_3'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split()if w in sorted_x]))\ntrain['Feature_3a'] = train['selected_text'].apply(lambda x: len([w for w in str(x).lower().split()if w in sorted_x]))\ntest['Feature_3'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split()if w in sorted_x]))\n\n#least words\nreverted_x = dict(sorted(counts.items(),key=operator.itemgetter(1))[:10000])\ntrain['Feature_4'] = train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]))\ntrain['Feature_4a'] = train['selected_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]))\ntest['Feature_4'] = test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in reverted_x]))\n\n#punctuation\ntrain['Feature_5'] = train['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]))\ntrain['Feature_5a'] = train['text_select_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]))\ntest['Feature_5'] = test['text_backup'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]))\n\n#Most popular word\np = train[\"text_select_backup\"]\nfirst = []\nfor sent in p:\n    sent = str(sent)\n    wordsss = sent.split()\n    first.append(wordsss[0])\nCounter = Counter(first)\nmost_occur= Counter.most_common(4)\n\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I considered implementing a spell checker, to correct spelling checks that would be important for my evaluation and model later on, but I wasn't able to do it efficiently and effectively. This could be a portion where I improve my code.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here, I begin to script my model. I will be relying on the nltk sentiment libraries to do my code. First, though, I calculate some average values that may be useful later on, as well as splitting my data into the \"x\" and \"y\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\navg_length = round(np.average(train['Feature_1a']), 0) + round(np.std(train['Feature_1a'])/2)\navg_stop = round(np.average(train['Feature_2a']),0) + round(np.std(train['Feature_2a'])/2 + 1)\navg_punc = round(np.average(train['Feature_5a']),0)\navg_word = round(np.average(train['Feature_a']),0)\n\nsubset = test['text_backup']\nsentiment = test['sentiment']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I then define a couple functions that will be important for later on in my code, mostly to get indicies and positions of certain words inside a string in order to aid in indexing and creating substrings to formulate my final submission. The final function is used to replace spaces whenever needed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_1st_pos(lst):\n    index = []\n    j=1\n    for i in lst:\n        if i <= 0:\n            index.append(100000000)\n        elif i > 0:\n            index.append(j)\n        j=j+1\n    return np.argmin(index)\n\ndef get_last_pos (lst):\n    index = []\n    j = 1\n    for i in lst:\n        if i <= 0:\n            index.append(-10000000)\n        elif i > 0:\n            index.append(j)\n        j = j+1\n    return np.argmax(index)\n\ndef get_1st_neg (lst):\n    index = []\n    j=1\n    for i in lst:\n        if i >= 0:\n            index.append(100000000)\n        elif i < 0:\n            index.append(j)\n        j=j+1\n    return np.argmin(index)\n\ndef get_last_neg (lst):\n    index = []\n    j = 1\n    for i in lst:\n        if i >= 0:\n            index.append(-10000000)\n        elif i < 0:\n            index.append(j)\n        j = j+1\n    return np.argmax(index)\n\ndef remove(string):\n    return string.replace (\" \",\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I also defined a couple of \"bad words\" that reverse the connotation of certain words. For example, when detecting positive sentiments, the function will give the phrase \"not bad\" a negative connotation, since bad is scored as negative, but including \"not\" makes it a positive sentiment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_words = ['not','no','oh']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is the bulk of my model. My \"model\" is built up of extended if statements, that help score the sentiments of the words of the tweet, picking out the parts that give it its sentiments. In each function, I first split the tweet into its individual wods and remove punctuation again. I score each word, recording its score in a list. If the opposite score is recorded (ie A negative score for a positive comment or vice versa), I add it to a count. I then find the first and second occurence of the desired sentiment. Next, I go through the if statements:\n1. If the number of sentiment words minus the opposite sentiment words is 0, then return the string from either end of the tweet to the least value in the tweet, depending which side of the average that value falls.\n2. If the difference between the words is 1, return the string between the max index and the least index.\n3. If the difference between the words is 2, return the string between the two highest indicies (Or least for negative).\n4. Otherwise, find the first and last positive (or negative) words, and return that string. If those do not exist, return the entire string.\n\nFor neutral tweets, I basically used a similar model as above, but tried to maximize words with sentiment scores of 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def positive (sent):\n    sent=re.sub('http[s]?://\\S+','',str(sent))\n    words = sent.split()\n    score_list=[]\n    s = ' '\n    a=0\n    for w in words:\n        w = w.translate(str.maketrans('','',string.punctuation))\n        w = remove(w)\n        score = sid.polarity_scores(w)['compound']\n        score_list.append(score)\n        if score < 0:\n            a = a+1\n    if words[np.argmin(score_list)-1] in bad_words:\n        b = np.argmin(score_list)\n        j = np.argmin(score_list)-1\n        score_list.remove(min(score_list))\n        score_list.insert(b,0)\n        del score_list[j]\n        score_list.insert(j,0)\n        a = a-1 \n    if a < 0:\n        a = 0\n    #First Max\n    max_index = np.argmax(score_list)\n    max_val = score_list[max_index]\n    \n    #Second Max\n    if len(words)>1 and np.count_nonzero(score_list) != 0:\n        score_list.remove(score_list[max_index])\n        if max_index > np.argmax(score_list):\n            max2_index = np.argmax(score_list) \n            max_val2 = score_list[max2_index]\n        elif max_index <= np.argmax(score_list):\n            max2_index = np.argmax(score_list)+1\n            max_val2 = score_list[max2_index-1]\n        score_list.insert(max_index,max_val)\n    else:\n        max2_index = 0\n    \n    #Cycling Thorugh\n    if np.count_nonzero(score_list)-a == 0:\n        if a > 0 and np.argmin(score_list)>len(score_list)/2:\n            ans = words[0:np.argmin(score_list)]\n            return s.join(ans)\n        elif a>0 and np.argmin(score_list)<len(score_list)/2:\n            ans = words[np.argmin(score_list)+1: len(score_list)]\n            return s.join(ans)\n        else:\n            return sent\n    elif np.count_nonzero(score_list) - a == 1 :\n        if a == 1 and np.argmin(score_list) < max_index:\n            ans = words[np.argmin(score_list)+1:max_index+1]\n            return s.join(ans)\n        elif a==1 and np.argmin(score_list) > max_index:\n            ans = words[max_index:np.argmin(score_list)]\n            return s.join(ans)\n        else:\n            return words[max_index]\n    elif np.count_nonzero(score_list) - a== 2:\n        if abs(max_val - max_val2) < 0.36:\n            if max_index > max2_index:\n                ans =  words [max2_index:max_index+1]\n                return s.join(ans)\n            else:\n                ans= words[max_index:max2_index+1]\n                return s.join(ans)\n        elif min(score_list) < 0: \n            if max_index < max2_index and np.argmin(score_list) < max2_index and np.argmin(score_list) > max_index:\n                ans =  words [max_index:np.argmin(score_list)]\n                return s.join(ans)\n            elif max_index > max2_index and np.argmin(score_list) > max2_index and np.argmin(score_list) < max_index:\n                ans =  words [max2_index:np.argmin(score_list)]\n                return s.join(ans)\n    else:\n        first_max = get_1st_pos(score_list)\n        last_max = get_last_pos(score_list)\n        if min(score_list) < 0 and np.argmin(score_list) > first_max and np.argmin(score_list) < last_max:\n            if max_index > np.argmin(score_list):\n                ans =  words[np.argmin(score_list)+1:max_index+1]\n                return s.join(ans)\n            else:\n                ans = words[max_index:np.argmin(score_list)]\n                return s.join(ans)\n        else:\n            ans =  words[first_max:last_max+1]\n            return s.join(ans)\n            \ndef negative (sent):\n    sent=re.sub('http[s]?://\\S+','',str(sent))\n    words = sent.split()\n    score_list=[]\n    s=' '\n    a = 0\n    for w in words:\n        w = w.translate(str.maketrans('','',string.punctuation))\n        w = remove(w)\n        score = sid.polarity_scores(w)['compound']\n        score_list.append(score)\n        if score > 0:\n            a = a+1\n    if words[np.argmax(score_list)-1] in bad_words:\n        b = np.argmax(score_list)\n        j = np.argmax(score_list)-1\n        score_list.remove(max(score_list))\n        score_list.insert(b,0)\n        del score_list[j]\n        score_list.insert(j,0)\n        a = a-1 \n    if a < 0:\n        a = 0\n    #First Min\n    min_index = np.argmin(score_list)\n    min_val = score_list[min_index]\n    \n    #Second Min\n    if len(words)>1 and np.count_nonzero(score_list) != 0:\n        score_list.remove(score_list[min_index])\n        if min_index > np.argmin(score_list):\n            min2_index = np.argmin(score_list) \n            min_val2 = score_list[min2_index]\n        elif min_index <= np.argmin(score_list):\n            min2_index = np.argmin(score_list)+1\n            min_val2 = score_list[min2_index-1]\n        score_list.insert(min_index,min_val)\n    else:\n        min2_index = 0\n    \n    \n    #Cycling Thorugh\n    if np.count_nonzero(score_list)-a == 0:\n        if a>0 and np.argmax(score_list) > len(score_list)/2:\n            ans = words[0:np.argmax(score_list)]\n            return s.join(ans)\n        elif a>0 and np.argmax(score_list) < len(score_list)/2:\n            ans = words[np.argmax(score_list)+1:len(score_list)]\n            return s.join(ans)\n        else:\n            return sent\n    elif np.count_nonzero(score_list)-a == 1:  \n        if a == 1 and np.argmax(score_list) < min_index:\n            ans = words[np.argmax(score_list)+1:min_index+1]\n            return s.join(ans)\n        elif a==1 and np.argmax(score_list) > min_index:\n            ans =  words[min_index:np.argmax(score_list)]\n            return s.join(ans)\n        else:\n            return words[min_index]\n    elif np.count_nonzero(score_list)-a== 2:\n        if abs(min_val - min_val2) < 0.375:\n            if min_index > min2_index:\n                ans = words [min2_index:min_index+1]\n                return s.join(ans)\n            else:\n                ans =  words[min_index:min2_index+1]\n                return s.join(ans)\n        elif max(score_list) > 0: \n            if min_index < min2_index and np.argmax(score_list) < min2_index and np.argmax(score_list) > min_index:\n                ans =  words [min_index:np.argmax(score_list)]\n                return s.join(ans)\n            elif min_index > min2_index and np.argmax(score_list) > min2_index and np.argmax(score_list) < min_index:\n                ans =  words [min2_index:np.argmax(score_list)]\n                return s.join(ans)\n    else:\n        first_min = get_1st_neg(score_list)\n        last_min = get_last_neg(score_list)\n        if max(score_list) > 0 and np.argmax(score_list) > first_min and np.argmax(score_list) < last_min:\n            if min_index > np.argmax(score_list):\n                ans =  words[np.argmax(score_list)+1:min_index+1]\n                return s.join(ans)\n            else:\n                ans = words[min_index:np.argmax(score_list)]\n                return s.join(ans)\n        else:\n            ans =  words[first_min:last_min+1]    \n            return s.join(ans)\ndef neutral (sent):\n    sent=re.sub('http[s]?://\\S+','',str(sent))\n    words = sent.split()\n    score_list=[]\n    s = ' '\n    for w in words:\n        w = w.translate(str.maketrans('','',string.punctuation))\n        w = remove(w)\n        score = sid.polarity_scores(w)['compound']\n        score_list.append(score)\n    if np.count_nonzero(score_list)==0:\n        return sent\n    elif np.count_nonzero(score_list) > 0 and (max(score_list) < 0.36 or abs(min(score_list)<0.4)):\n        return sent\n    elif np.count_nonzero(score_list)==1 and np.argmax(score_list) > 0:\n        if len(words)/2 > np.argmax(score_list):\n            ans =  words[np.argmax(score_list)+1:len(score_list)]\n            return s.join(ans)\n        else:\n            ans =  words[0:np.argmax(score_list)]\n            return s.join(ans)\n    elif np.count_nonzero(score_list)==1 and np.argmin(score_list) < 0:\n        if len(words)/2 > np.argmin(score_list):\n            ans =  words[0:np.argmin(score_list)]\n            return s.join(ans)\n        else:\n            ans =  words[np.argmin(score_list)+1:len(score_list)]\n            return s.join(ans)\n    else:\n        max_index = np.argmax(score_list)\n        max_val = score_list[max_index]\n        min_index = np.argmin(score_list)\n        min_val = score_list[min_index]\n        if max_val > abs(min_val) and max_index > len(score_list)/2:\n            ans = words[0:max_index]\n            return s.join(ans)\n        elif max_val > abs(min_val) and max_index < len(score_list)/2:\n            ans = words[max_index+1:len(score_list)]\n            return s.join(ans)\n        elif abs(min_val) > max_val and min_index > len(score_list)/2:\n            ans = words[0:min_index]\n            return s.join(ans)\n        else:\n            ans = words[min_index+1:len(score_list)]\n            return s.join(ans)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, I run through the tweets, assigning the appropiate function depending on the sentiment they were given.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_list=[]\ni = 0\nfor sent in subset:\n    if sentiment[i] == 'positive':\n        word_list.append(positive(sent))\n    elif sentiment[i] == 'negative':\n        word_list.append(negative(sent))\n    else:\n        word_list.append(neutral(sent))\n    i=i+1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, technically, I did not have to \"train\" any models, I used the testing data and submitted the results.","execution_count":null},{"metadata":{"_uuid":"407b4150-c2fc-48ac-94d1-730a40e66c14","_cell_guid":"260e6919-2062-4002-a5f0-c448c0deec1f","trusted":true},"cell_type":"code","source":"submission=pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")\nsubmission[\"selected_text\"]=word_list\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In theory, this approach has many flaws, most notably not training any models and disregarding the training data (Although I did use the training data to test my if statement and correct it wherever necessary), disregarding the new features I created, and creating my \"model\" and if statements based on trial and error. There are defenitely better approaches to this challenge, but for my current level, this was the best I could do without copying code I did not understand.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}