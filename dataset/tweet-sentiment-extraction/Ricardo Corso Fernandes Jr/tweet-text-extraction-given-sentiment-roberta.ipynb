{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Shoutouts:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I would like to thank immensely the authors of many kernels which led me to fully understand the concepts needed for this submission:\n* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch#The-Model\n* https://www.kaggle.com/koushiksahu/roberta-extremely-verbosed-for-beginners\n* https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705/data?#Load-Libraries,-Data,-Tokenizer\n* https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34603010\n* https://www.kaggle.com/vbmokin/tse2020-roberta-cnn-outlier-analysis-3chr\n* https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert <- specially rich resource\n\nAlso, check out Abishek's YouTube channel videos, which also helped me a lot: \n* https://www.youtube.com/watch?v=6a6L_9USZxg\n* https://www.youtube.com/watch?v=XaQ0CBlQ4cY\n\nToDo: implement sentencepiece tokenizer (https://www.youtube.com/watch?v=U51ranzJBpY)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Imports","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, LeakyReLU, Activation\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import StratifiedKFold\nimport transformers\nfrom transformers import RobertaConfig, TFRobertaModel\nimport tokenizers\n\nprint('TF version: ', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Config","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximum length\nlengths = train['text'].apply(lambda x: len(x)).tolist()\nmax(lengths)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 141\n    PAD_ID = 1\n    PATH = '../input/tf-roberta/'\n    tokenizer = tokenizers.ByteLevelBPETokenizer(\n        vocab_file = PATH+'vocab-roberta-base.json',\n        merges_file = PATH+'merges-roberta-base.txt',\n        lowercase = True,\n        add_prefix_space = True\n    )\n    sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n    n_splits = 5\n    seed = 42\n    epochs = 3\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    label_smoothing = 0.1\n    batch_size = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct, config.MAX_LEN), dtype='int32')\nattention_mask = np.zeros((ct, config.MAX_LEN), dtype='int32')\ntoken_type_ids = np.zeros((ct, config.MAX_LEN), dtype='int32')\nstart_tokens = np.zeros((ct, config.MAX_LEN), dtype='int32')\nend_tokens = np.zeros((ct, config.MAX_LEN), dtype='int32')\n\nfor k in range(train.shape[0]):\n    # Selected text masking\n    text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n    text2 = \" \".join(train.loc[k, 'selected_text'].split())\n    \n    selected_idx = text1.find(text2)\n    is_selected = np.zeros((len(text1)))\n    is_selected[selected_idx:selected_idx+len(text2)] = 1\n    if text1[selected_idx-1] == \" \":\n        is_selected[selected_idx-1] = 1\n        \n    enc = config.tokenizer.encode(text1)\n    \n    # IDs start and end offsets (A.K.A.: indexes)\n    offsets = []\n    idx = 0\n    for t in enc.ids:\n        w = config.tokenizer.decode([t])\n        offsets.append((idx, idx+len(w)))\n        idx += len(w)\n        \n    # START and END tokens\n    toks = []\n    for i, (a, b) in enumerate(offsets):\n        verification_sum = np.sum(is_selected[a:b])\n        if verification_sum > 0:\n            toks.append(i)\n            \n    sentiment_tok = config.sentiment_id[train.loc[k, 'sentiment']]\n    input_ids[k, :len(enc.ids)+3] = [0, sentiment_tok] + enc.ids + [2]\n    attention_mask[k, :len(enc.ids)+3] = 1\n    if len(toks) > 0:\n        start_tokens[k, toks[0]+2] = 1\n        end_tokens[k, toks[-1]+2]  = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_test = np.ones((ct, config.MAX_LEN), dtype='int32')\nattention_mask_test = np.zeros((ct, config.MAX_LEN), dtype='int32')\ntoken_type_ids_test = np.zeros((ct, config.MAX_LEN), dtype='int32')\n\nfor k in range(ct):\n    \n    # Input IDs\n    text1 = \" \" + \" \".join(test.loc[k, 'text'].split())\n    enc = config.tokenizer.encode(text1)\n    \n    sentiment_tok = config.sentiment_id[test.loc[k, 'sentiment']]\n    input_ids_test[k, :len(enc.ids)+5] = [0] + enc.ids + [2, 2] + [sentiment_tok] + [2]\n    attention_mask_test[k, :len(enc.ids)+5] = 1\n\ntest.info()\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## roBERTa model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n        \ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n        \n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=config.label_smoothing)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n# from https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705/data?#Load-Libraries,-Data,-Tokenizer\n'''def build_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n\n    roberta_config = RobertaConfig.from_pretrained(config.PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(config.PATH+\n            'pretrained-roberta-base.h5',config=roberta_config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Conv1D(1,1)(x[0])\n    print(x1.shape)\n    x1 = tf.keras.layers.Flatten()(x1)\n    print(x1.shape)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    print(x1.shape)\n    \n    x2 = tf.keras.layers.Conv1D(1,1)(x[0])\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model\n'''\n\n# from https://www.kaggle.com/khoongweihao/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34603010\ndef build_model():\n    ids = Input((config.MAX_LEN,), dtype=tf.int32)\n    att = Input((config.MAX_LEN,), dtype=tf.int32)\n    tok = Input((config.MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, config.PAD_ID), tf.int32)\n    \n    lens = config.MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n    \n    roberta_config = RobertaConfig.from_pretrained(config.PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(config.PATH+'pretrained-roberta-base.h5', config=roberta_config)\n    \n    x = bert_model(ids_, attention_mask=att_, token_type_ids=tok_) #for non-padded model: (ids, attention_mask=att, token_type_ids=tok)\n    #print(len(x))\n    #x = tf.convert_to_tensor(x[0])\n    #print(x.shape)\n    #print(type(x))\n    #print(type(x[0]))\n    \n    x1 = Dropout(0.15)(x[0])\n    #print(x1.shape)\n    x1 = Conv1D(768, 2, padding='same')(x1)\n    #print(x1.shape)\n    x1 = LeakyReLU()(x1)\n    #print(x1.shape)\n    x1 = Conv1D(64, 2, padding='same')(x1)\n    #print(x1.shape)\n    x1 = Dense(1)(x1)\n    #print(x1.shape)\n    x1 = Flatten()(x1)\n    #print(x1.shape)\n    x1 = Activation('softmax')(x1)\n    #print(x1.shape)\n    \n    x2 = Dropout(0.15)(x[0])\n    x2 = Conv1D(768, 2, padding='same')(x2)\n    x2 = LeakyReLU()(x2)\n    x2 = Conv1D(64, 2, padding='same')(x2)\n    x2 = Dense(1)(x2)\n    x2 = Flatten()(x2)\n    x2 = Activation('softmax')(x2)\n    \n    model = Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = Adam(learning_rate=3e-5)\n    model.compile(loss=loss_fn,\n                  optimizer=optimizer)\n  \n    x1_padded = tf.pad(x1, [[0, 0], [0, config.MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, config.MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded, x2_padded])\n    \n    return model, padded_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    if(len(a)==0) & (len(b)==0):\n        return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\njac = []\nVER = 'v0'\nDISPLAY = 1\n\noof_start = np.zeros((input_ids.shape[0], config.MAX_LEN))\noof_end = np.zeros((input_ids.shape[0], config.MAX_LEN))\n\npreds_start_train = np.zeros((input_ids.shape[0], config.MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0], config.MAX_LEN))\npreds_start = np.zeros((input_ids_test.shape[0], config.MAX_LEN))\npreds_end = np.zeros((input_ids_test.shape[0], config.MAX_LEN))\n\nskf = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\nfor fold, (idxText, idxSentValue) in enumerate(skf.split(input_ids, train.sentiment.values)):\n    print('\\n')\n    print('Fold', (fold+1))\n    print('\\n')\n\n    K.clear_session()\n    model, padded_model = build_model()\n\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n\n    inputText = [input_ids[idxText,], attention_mask[idxText,], token_type_ids[idxText,]]\n    targetText = [start_tokens[idxText,], end_tokens[idxText,]]\n\n    inputSentValue = [input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]]\n    targetSentValue = [start_tokens[idxSentValue,], end_tokens[idxSentValue,]]\n\n    # Sorting validation data\n    shuffleSentValue = np.int32(sorted(range(len(inputSentValue[0])), key=lambda k: (inputSentValue[0][k] == config.PAD_ID).sum(), reverse=True))\n    inputSentValue = [arr[shuffleSentValue] for arr in inputSentValue]\n    targetSentValue = [arr[shuffleSentValue] for arr in targetSentValue]\n\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    \n    for epoch in range(1, config.epochs + 1):\n        print('\\n')\n        print('Preparing data.')\n        print('\\n')\n        # add random numbers in order to avoid having the same order in each epoch\n        shuffleText = np.int32(sorted(range(len(inputText[0])), key=lambda k: (inputText[0][k] == config.PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        \n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleText) / config.batch_size)\n        batch_idxs = np.random.permutation(num_batches)\n        shuffleText_ = []\n        for batch_idx in batch_idxs:\n            shuffleText_.append(shuffleText[batch_idx * config.batch_size: (batch_idx + 1) * config.batch_size])\n        shuffleText = np.concatenate(shuffleText_)\n        \n        # reorder the input data\n        inputText = [arr[shuffleText] for arr in inputText]\n        targetText = [arr[shuffleText] for arr in targetText]\n        \n        print('\\n')\n        print('Fitting the model')\n        print('\\n')\n        #preds = padded_model.predict([input_ids_test,attention_mask_test,token_type_ids_t],verbose=DISPLAY)\n        model.fit(inputText, targetText, epochs=config.epochs, initial_epoch=epoch - 1, \n                  batch_size=config.batch_size, verbose=DISPLAY, callbacks=[], \n                  validation_data=(inputSentValue, targetSentValue), shuffle=False) #don't shuffle in fit\n        save_weights(model, weight_fn)\n        \n    print('\\n')\n    print('Loading model.')\n    print('\\n')\n    #model.load_weights('%s-roberta-%i.h5'%(VER, fold))\n    load_weights(model, weight_fn)\n\n    print('\\n')\n    print('Predicting OOF.')\n    print('\\n')\n    oof_start[idxSentValue,], oof_end[idxSentValue,] = padded_model.predict([input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]], \n                                                                            verbose=DISPLAY)\n    #oof_start[idxSentValue,], oof_end[idxSentValue,] = model.predict([input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]], \n                                                                     #verbose=DISPLAY)\n    \n    #print('\\n')\n    #print('Predicting all Train for Outlier analysis.')\n    #print('\\n')\n    #preds_train = padded_model.predict([input_ids, attention_mask, token_type_ids], verbose=DISPLAY)\n    #preds_start_train += preds_train[0] / skf.n_splits\n    #preds_end_train += preds_train[1] / skf.n_splits\n\n    print('\\n')\n    print('Predicting test data.')\n    print('\\n')\n    preds = padded_model.predict([input_ids_test, attention_mask_test, token_type_ids_test], verbose=DISPLAY)\n    #preds = model.predict([input_ids_test, attention_mask_test, token_type_ids_test], verbose=DISPLAY)\n    preds_start += preds[0] / skf.n_splits\n    preds_end += preds[1] / skf.n_splits\n\n    # display fold jaccard\n    all = []\n    for k in idxSentValue:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n\n        if a > b:\n            selected_text = train.loc[k, 'text']\n        else:\n            text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n            enc = config.tokenizer.encode(text1)\n            selected_text = config.tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(selected_text, train.loc[k, 'selected_text']))\n    jac_score = np.mean(all)\n    jac.append(jac_score)\n\n    print('\\n')\n    print('>>>>> FOLD', (fold+1), \": \\n\\tJaccard = \", jac_score)\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Overall 5Fold Cross-Validation Jaccard score:', jac_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_dir = '../output/model/'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kaggle submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_test.shape[0]):\n    a = np.argmax(preds_start[k, ])\n    b = np.argmax(preds_end[k, ])\n    \n    if a > b:\n        st = test.loc[k, 'text']\n    else:\n        text1 = \" \" + \" \".join(test.loc[k, 'text'].split())\n        enc = config.tokenizer.encode(text1)\n        st = config.tokenizer.decode(enc.ids[a-2:b-1])\n    \n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\nprint(len(all))\ntest['selected_text'] = all\ntest[['textID', 'selected_text']].to_csv('submission.csv', index=False)\npd.set_option('max_colwidth', 60)\ntest[['textID', 'selected_text']].sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}