{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Twitter Sentiment Classification\n\nData url: https://www.kaggle.com/c/tweet-sentiment-extraction/data  \n\nData summary: We have 22463 tweets labelled as Positive, Neutral or Negative.\n\nThere is an in progress Kaggle competition aimed at extracting phrases that highlight particular sentiment in a tweet. Given a tweet and the sentiment (positive, neutral, negative), participants need to identify the part of the tweet that defines that sentiment. \n\nI have repurposed the dataset for sentiment classification intially. So ignoring the phrase that highlights sentiment, let's build an algorithm that can accurately detect sentiment based on the text in a tweet. \n\nGiven this is a case study for which we do not know the business use, I have chosen to use accuracy as the evaluation metric.\n\nThe final model accuracy on the test set is 71%. \n\nMethod: \n\nThe approach will be to: \n\n1. Load and clean the data\n2. Perform initial analysis to understand, for example distribution training data\n3. Text preprocessing\n4. Build,compile and fit recurrent neural network.\n5. Compare to using pretrained embedded layer from BERT\n6. Test\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, LSTM, GRU, Embedding\nfrom keras.layers import Activation, Bidirectional, GlobalMaxPool1D, GlobalMaxPool2D, Dropout\n#from tensorflow.compat.v1.keras.layers import CuDNNLSTM, Dropout\n#from tensorflow.keras.layers import concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import RMSprop, adam\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nimport seaborn as sns\nimport transformers\nfrom transformers import AutoTokenizer\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.initializers import Constant\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\n\nstop=set(stopwords.words('english'))\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load and Clean the Data\n\nWe drop the NaN values and have 4 helper functions to clean the data. These are:\n1. basic_cleaning - to remove website urls, non-characters and to replace '*****' swear words with the word swear\n2. remove_html\n3. remove_emojis\n4. remove_multiplechars - this is for when there are more than 3 characters in a row in a word e.g. wayyyyy. The function removes all but one of the letters\n\nThe data is then ready for initial exploration.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/shahules/complete-eda-baseline-model-0-708-lb\n\ndef basic_cleaning(text):\n    text=re.sub(r'https?://www\\.\\S+\\.com','',text)\n    text=re.sub(r'[^A-Za-z|\\s]','',text)\n    text=re.sub(r'\\*+','swear',text) #capture swear words that are **** out\n    return text\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_multiplechars(text):\n    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n    return text\n\n\ndef clean(df):\n    for col in ['text']:#,'selected_text']:\n        df[col]=df[col].astype(str).apply(lambda x:basic_cleaning(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_emoji(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_html(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = clean(df)\ndf_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Exploration\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this section we look at:\n1. The distribution of number of tweets by sentiment:\nThe three groups of positive, neutral and negative are fairly well balanced\n\n2. The distribution of length of tweet by sentiment:\nThe length of tweets, on visual inspection does not appear to be very different.\n\n3. The most common words: \nLike appears in the top 10 for all three groups. Given it's in all three groups, we will not remove it as it will still provide information within individual sequences. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"shape = df_clean.shape\nprint(f\"There are {shape[0]} tweets in the dataset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors=['orange','red','green']\nplt.bar(df.sentiment.unique(),df.sentiment.value_counts(), color=colors);\nplt.xlabel('Tweet Sentiment');\nplt.ylabel('Tweet Count');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent=df.sentiment.unique()\nfig,ax= plt.subplots(1,3,figsize=(12,6),sharey=True)\nfor i in range(0,3):\n    lengths = df_clean[df_clean['sentiment']==sent[i]]['text'].str.split().str.len()\n    ax[i].boxplot(lengths)\n    ax[i].set_title(sent[i])\nax[0].set_ylabel('Number of words in Tweet')\nfig.suptitle(\"Distribution of number Words in Tweets\", fontsize=14);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_news(df,stop=stop,n=1,col='text'):\n    '''Function to preprocess and create corpus'''\n    new_corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for text in df[col]:\n        words=[w for w in word_tokenize(text) if (w not in stop)]\n       \n        words=[lem.lemmatize(w) for w in words if(len(w)>n)]\n     \n        new_corpus.append(words)\n        \n    new_corpus=[word for l in new_corpus for word in l]\n    return new_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,3,figsize=(12,6))\nfor i in range(3):\n    new=df_clean[df_clean['sentiment']==sent[i]]\n    corpus_train=preprocess_news(new,n=3)\n    counter=Counter(corpus_train)\n    most=counter.most_common()\n    x=[]\n    y=[]\n    for word,count in most[:10]:\n        if (word not in stop) :\n            x.append(word)\n            y.append(count)\n    sns.barplot(x=y,y=x,ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i],color=colors[i])\nfig.suptitle(\"Common words in tweet text\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the labels, one-hot encoding performed significantly better than LabelEncoder. We also tokenize and covert to sequences. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean_selection = df_clean.sample(frac=0.2)\nX = df_clean_selection.text.values\ny = pd.get_dummies(df_clean_selection.sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(X))\nlist_tokenized_train = tokenizer.texts_to_sequences(X)\nX_t = sequence.pad_sequences(list_tokenized_train, maxlen=128)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building and Training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For the classification model, we build a recurrent neural network (RNN) with two Long, Short Term Memory (LSTM) layers. \n\nInitial performance indicated two LSTM layers performed slightly better than one. We use birectional layers to take advantage of the fact that some further semantic information can be gained from cycle through the tweets backwards. We have two dropout layers to prevent overfitting and a dense layer with L1L2 regularization. \n\nWe then: \n\n1. Perform gridsearch for the best optimizer of gradient descent\n2. Perform gridsearch for the best learning rate\n\nNote: GridSearch CV is not optimized for GPU, so we are using pure Keras and manual gridsearch.   \nNote: In the interests of time, parameter selection was performed on a random sample of 25% of the training data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(): \n    embedding_size = 128\n    input_ = Input(shape=(128,))\n    x = Embedding(20000, embedding_size)(input_)\n    x = Bidirectional(LSTM(50,return_sequences=True))(x)\n    x = Bidirectional(LSTM(25,return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(50, activation='relu', kernel_regularizer='L1L2')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(3, activation='softmax')(x)\n\n    model = Model(inputs=input_, outputs=x)\n    return model\n\n#model = KerasClassifier(create_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizers_names = ['SGD', 'RMSprop', 'Adam']\nmodel = create_model()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for optim in optimizers_names: \n    \n    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['accuracy'])\n    history = model.fit(X_t,y,batch_size=32,epochs=2,validation_split=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = [0.001,0.01,0.1]\n\nfor rate in learning_rate:\n    Adam_name = adam(lr=rate)\n    model.compile(loss='categorical_crossentropy',optimizer=Adam_name,metrics=['accuracy'])\n    history = model.fit(X_t,y,batch_size=32,epochs=2,validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing, Model build and training with DistilBert","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We now turn our attention to pretrained embeddings. In this case we download and use DistilBert instead of training our own Embedding layer. DistilBert, a light version of BERT, google's game-changing NLP model, provides us with a tokenizer and an embedding matrix. BERT base uncased is trained on lower case English text and has around 110 million parameters (768 dimensions for embedding each word, and a vocab of 143,000 words). Distilbert has 60% of this, but maintains 97% performance against BERT. \n\nFor the purposes of this example, we will leave that matrix rather than train it, as it's large and we would have unrealistic training times. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  ## change it to commit\n\n# Save the loaded tokenizer locally\nsave_path = '/kaggle/working/distilbert_base_uncased/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)\nfast_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):    \n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = fast_encode(df_clean_selection.text.astype(str), fast_tokenizer, maxlen=128)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_size = 128\ninput_ = Input(shape=(100,))\n\ninp = Input(shape=(128, ))\n#inp2= Input(shape=(1,))\n\nembedding_matrix=transformer_layer.weights[0].numpy()\n\nx = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],embeddings_initializer=Constant(embedding_matrix),trainable=False)(inp)\nx = Bidirectional(LSTM(50, return_sequences=True))(x)\nx = Bidirectional(LSTM(25, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.5)(x)\nx = Dense(50, activation='relu', kernel_regularizer='L1L2')(x)\nx = Dropout(0.5)(x)\nx = Dense(3, activation='softmax')(x)\n\nmodel_DistilBert = Model(inputs=[inp], outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_DistilBert.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_DistilBert.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_DistilBert.fit(X,y,batch_size=32,epochs=2,validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test the Final DistilBert Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean_final = df_clean.sample(frac=1)\nX_train = fast_encode(df_clean_selection.text.astype(str), fast_tokenizer, maxlen=128)\ny_train = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Adam_name = adam(lr=0.001)\nmodel_DistilBert.compile(loss='categorical_crossentropy',optimizer=Adam_name,metrics=['accuracy'])\nhistory = model_DistilBert.fit(X_train,y_train,batch_size=32,epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ndf_test.dropna(inplace=True)\ndf_clean_test = clean(df_test)\n\nX_test = fast_encode(df_clean_test.text.values.astype(str), fast_tokenizer, maxlen=128)\ny_test = df_clean_test.sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = model_DistilBert.predict(X_test)\ny_predictions = pd.DataFrame(y_preds, columns=['negative','neutral','positive'])\ny_predictions_final = y_predictions.idxmax(axis=1)\naccuracy = accuracy_score(y_test,y_predictions_final)\nprint(f\"The final model shows {accuracy:.2f} accuracy on the test set.\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}