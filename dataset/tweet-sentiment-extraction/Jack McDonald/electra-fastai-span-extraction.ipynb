{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Overview\n\nSharing a notebook on pre-processing Electra with Fastai.  \nI was really excited about fine-tuning Electra for this competition & was my first approach.  \nIt didn't really outperform Roberta for me in the end.\n\nI took a lot inspiration from these posts:\n\nhttps://www.kaggle.com/keremt/tse-transformers-q-a-with-fastai-training  \nhttps://www.kaggle.com/abhishek/roberta-inference-5-folds  \nhttps://www.kaggle.com/c/tweet-sentiment-extraction/discussion/151878\n\nSo, if this notebook is interesting - please look at those too  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Electra\n\n![](https://1.bp.blogspot.com/-CYdmy-CavFQ/XmfA7gPlDxI/AAAAAAAAFaw/QV_m8U6m82wKIJOV6XckhBRrET_DOTqawCEwYBhgLKskDAMBZVoDkAXgBeYLK0UlPs8KTLC000nVXlb_Vf0ViyBhl7daowMj46skeP5iBS15xmM9P2Y7l6GgUXKwATCbwSw5j3fIuStwsfGXY4eDyE_xP-dk4vYEkjn-6EoBR8bj-yTxWRmVLl3QKpt9xQyMVEv4Fq4Lq5xNshTm2UuGOfrntf_83f7CGsEaAQYtY33St2jCXTL-MMUZohpk9AZaIvZsHtvYElDJ8k0JmVJYGukD73BFnibrv-dw4dBolN8aOtcrIBgn7D-IkUW1bul0_Fzzmhf6L410ngtGsA3HR8PGFGUL8Xv0Unl2lW58Sv2bNeJSFLnyyTsp5d63vorjL9NnMPJgFtIRor1t0-_1XJIk9e1NRUDkn63CPlFjkl1LGYjbc_cEgyMhX431SM8RBE-AqTbR1I7tpu-IkbIzpmfVCxqQSYW1rPhDzn9T6dXT3GxI7mHy7AKfuRDj24nT5cj3Fx8opS5ygDDa582vdxmxIR8pz1Ly3lzKKPGdVVqZhhIlXKGJXpOnsEeELFEyPoq95SwmHG-BBK07_tyR0qySY3wI3Ks-r5v4YoYVw88GZ0JW1V5_jgcBQV5TOTZLGR7aZK4NV9TwcZTC2hJ_zBQ/s1600/image4.gif)\n\nElectra's a recent pre-training objective for Transformer models. \n\nIt's a self-supervised pre-training task, which involves training both a generator and discrimator transformer.   \n\nThe discriminator performs **replaced token detection** and is the model you generally fine-tune on downstream tasks.  \n\nThere's an excellent post on Electra and Transformer pre-training [here](https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoModel, AutoConfig, get_linear_schedule_with_warmup\nfrom transformers.optimization import AdamW\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport os\nfrom itertools import compress","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom functools import partial\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom fastai.core import *\nfrom fastai.text import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_dir, electra_dir = [Path(f'/kaggle/input/{i}') for i in ['tweet-sentiment-extraction', 'electrabase']]\n\ntrain_df = pd.read_csv(file_dir/'train.csv')\ntrain_df['text'] = train_df['text'].apply(lambda x: str(x))\ntrain_df['sentiment'] = train_df['sentiment'].apply(lambda x: str(x))\ntrain_df['selected_text'] = train_df['selected_text'].apply(lambda x: str(x))\n\ntest_df = pd.read_csv(file_dir/'test.csv')\ntest_df['text'] = test_df['text'].apply(lambda x: str(x))\ntest_df['sentiment'] = test_df['sentiment'].apply(lambda x: str(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 128\nbs = 64\ntokenizer = BertWordPieceTokenizer(str(electra_dir/'vocab.txt'), lowercase=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I had fun trying to create my 'own' preprocessing pipeline  \n\nI thought this was an 'efficient' way to tokenize - but maybe not in the end  \nWordPiece tokenizing cases like `gonna` vs `onna` will give input ids, which is why there are two start/end idx loops  \n\nElectra uses the same vocab & tokenizer as Bert","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(sentiment, tweet, selected, tokenizer, max_len):\n    _input = tokenizer.encode(sentiment, tweet)\n    _span = tokenizer.encode(selected, add_special_tokens=False)\n    \n    len_span = len(_span.ids)\n    start_idx = None\n    end_idx = None\n    \n    for ind in (i for i, e in enumerate(_input.ids) if e == _span.ids[0]):\n        if _input.ids[ind: ind + len_span] == _span.ids:\n            start_idx = ind\n            end_idx = ind + len_span - 1\n            break\n    \n    # Handles cases where Wordpiece tokenizing input & span separately produces different outputs\n    if not start_idx:\n        idx0 = tweet.find(selected)\n        idx1 = idx0 + len(selected)\n        \n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1):\n                char_targets[ct] = 1\n                \n        tweet_offsets = list(compress(_input.offsets, _input.type_ids))[0:-1]\n        \n        target_idx = []\n        for j, (offset1, offset2) in enumerate(tweet_offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n                \n        start_idx, end_idx = target_idx[0] +3 , target_idx[-1] + 3\n        \n    _input.start_target = start_idx\n    _input.end_target = end_idx\n    _input.tweet = tweet\n    _input.sentiment = sentiment\n    _input.selected = selected\n    \n    _input.pad(max_len)\n    \n    return _input","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LabelSmoothingCrossEntropy by fastai - changed slightly to use `torch.lerp`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_loss(loss, reduction='mean'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss    \n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, ε:float=0.1, reduction='mean'):\n        super().__init__()\n        self.ε,self.reduction = ε,reduction\n    \n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n        return torch.lerp(nll, loss/c, self.ε) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset(Dataset):\n    def __init__(self, dataset, test = None):\n        self.df = dataset\n        self.test = test\n        \n    def __getitem__(self, idx):\n        if not self.test:\n            sentiment, tweet, selected = (self.df[col][idx] for col in ['sentiment', 'text', 'selected_text'])\n            _input = preprocess(sentiment, tweet, selected, tokenizer, max_len)\n            \n            yb = [torch.tensor(_input.start_target), torch.tensor(_input.end_target)]\n            \n        else:\n            _input = tokenizer.encode(self.df.sentiment[idx], self.df.text[idx])\n            _input.pad(max_len)\n            \n            yb = 0\n\n        xb = [torch.LongTensor(_input.ids),\n              torch.LongTensor(_input.attention_mask),\n              torch.LongTensor(_input.type_ids),\n              np.array(_input.offsets)]\n\n        return xb, yb     \n\n    def __len__(self):\n        return len(self.df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pt_model = AutoModel.from_pretrained(electra_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that in the Model's forward I pass an optional `offsets` parameter.  \n\nI did this to experiment with callbacks, but it's not standard/necessary.  \nValidation DataLoaders aren't shuffled by default in Fastai, so you can grab the offsets differently.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpanModel(nn.Module):\n    def __init__(self,pt_model):\n        super().__init__()\n        self.model = pt_model\n        self.drop_out = nn.Dropout(0.5)\n        self.qa_outputs1c = torch.nn.Conv1d(768*2, 128, 2)\n        self.qa_outputs2c = torch.nn.Conv1d(768*2, 128, 2)\n        self.qa_outputs1 = nn.Linear(128, 1)\n        self.qa_outputs2 = nn.Linear(128, 1)\n        \n#         self.qa_outputs = nn.Linear(768 * 2, 2) # update hidden size\n\n    # could pass offsets here and not use - can grab in last_input\n    def forward(self, input_ids, attention_mask, token_type_ids, offsets = None):\n        \n        _, hidden_states = self.model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n        out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)\n        out = self.drop_out(out)\n        out = torch.nn.functional.pad(out.transpose(1,2), (1, 0))\n        \n        out1 = self.qa_outputs1c(out).transpose(1,2)\n        out2 = self.qa_outputs2c(out).transpose(1,2)\n\n        start_logits = self.qa_outputs1(self.drop_out(out1)).squeeze(-1)\n        end_logits = self.qa_outputs2(self.drop_out(out2)).squeeze(-1)\n        return start_logits, end_logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CELoss(Module):\n    def __init__(self, loss_fn = nn.CrossEntropyLoss()): \n        self.loss_fn = loss_fn\n        \n    def forward(self, inputs, start_targets, end_targets):\n        start_logits, end_logits = inputs # assumes tuple input\n        \n        logits = torch.cat([start_logits, end_logits]).contiguous()\n        \n        targets = torch.cat([start_targets, end_targets]).contiguous()\n        \n        return self.loss_fn(logits, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note that validation ds is by default not shuffled in fastai - so indexing like this will work for Callback\n# https://forums.fast.ai/t/how-to-set-shuffle-false-of-train-and-val/33730\n\nclass JaccardScore(Callback):\n    \"Stores predictions and targets to perform calculations on epoch end.\"\n    def __init__(self, valid_ds): \n        self.valid_ds = valid_ds\n        self.context_text = valid_ds.df.text\n        self.answer_text = valid_ds.df.selected_text\n        \n    def on_epoch_begin(self, **kwargs):\n        self.jaccard_scores = []  \n        self.valid_ds_idx = 0\n        \n        \n    def on_batch_end(self, last_input:Tensor, last_output:Tensor, last_target:Tensor, **kwargs):\n              \n        input_ids = last_input[0]\n        attention_masks = last_input[1].bool()\n        token_type_ids = last_input[2].bool()\n        offsets = last_input[3]\n\n        start_logits, end_logits = last_output\n        \n        # for id in batch of ids\n        for i in range(len(input_ids)):\n            \n            _offsets = offsets[i]\n            start_idx, end_idx = torch.argmax(start_logits[i]), torch.argmax(end_logits[i])\n            _answer_text = self.answer_text[self.valid_ds_idx]\n            original_start, original_end = _offsets[start_idx][0], _offsets[end_idx][1]\n            pred_span = self.context_text[self.valid_ds_idx][original_start : original_end]\n                \n            score = jaccard(pred_span, _answer_text)\n            self.jaccard_scores.append(score)\n\n            self.valid_ds_idx += 1\n            \n    def on_epoch_end(self, last_metrics, **kwargs):        \n        res = np.mean(self.jaccard_scores)\n        return add_metrics(last_metrics, res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SpanModel(pt_model)\ntr_df, val_df = train_test_split(train_df, test_size = 0.2, random_state = 42)\ntr_df, val_df  = [df.reset_index(drop=True) for df in [tr_df, val_df]]\ntrain_ds, valid_ds = [TweetDataset(i) for i in [tr_df, val_df]]\n\ntest_ds = TweetDataset(test_df, test = True)\nloss_fn = partial(CELoss, LabelSmoothingCrossEntropy())\n\ndata = DataBunch.create(train_ds, valid_ds, test_ds, path=\".\", bs = bs)\nlearner = Learner(data, model, loss_func = loss_fn(), path = electra_dir/'electra-conv', model_dir=f\".\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\ntest_df_idx = 0\n\nwith torch.no_grad():\n    \n    for xb,yb in tqdm(learner.data.test_dl):\n        model0 = learner.load(f'electra_conv_0').model.eval()\n        start_logits0, end_logits0 = to_cpu(model0(*xb))\n        start_logits0, end_logits0 = start_logits0.float(), end_logits0.float()\n        \n        model1 = learner.load(f'electra_conv_1').model.eval()\n        start_logits1, end_logits1 = to_cpu(model1(*xb))\n        start_logits1, end_logits1 = start_logits1.float(), end_logits1.float()\n        \n        model2 = learner.load(f'electra_conv_2').model.eval()\n        start_logits2, end_logits2 = to_cpu(model2(*xb))\n        start_logits2, end_logits2 = start_logits2.float(), end_logits2.float()\n        \n        model3 = learner.load(f'electra_conv_3').model.eval()\n        start_logits3, end_logits3 = to_cpu(model3(*xb))\n        start_logits3, end_logits3 = start_logits3.float(), end_logits3.float()\n        \n        model4 = learner.load(f'electra_conv_4').model.eval()\n        start_logits4, end_logits4 = to_cpu(model4(*xb))\n        start_logits4, end_logits4 = start_logits4.float(), end_logits4.float()\n        \n        input_ids = to_cpu(xb[0])\n        attention_masks = to_cpu(xb[1].bool())\n        token_type_ids = to_cpu(xb[2].bool())\n        offsets = to_cpu(xb[3])\n        \n        start_logits = (start_logits0 + start_logits1 + start_logits2 + start_logits3 + start_logits4) / 5\n        end_logits = (end_logits0 + end_logits1 + end_logits2 + end_logits3 + end_logits4) / 5\n        \n        for i in range(len(input_ids)):\n            \n            _offsets = offsets[i]\n#             start_idx, end_idx = torch.argmax(start_logits[i]), torch.argmax(end_logits[i])\n\n            start_idx, end_idx = get_best_start_end_idxs(start_logits[i], end_logits[i])\n            original_start, original_end = _offsets[start_idx][0], _offsets[end_idx][1]\n            pred_span = test_ds.df.text[test_df_idx][original_start : original_end]\n            preds.append(pred_span)\n            test_df_idx += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['selected_text'] = preds\ntest_df['selected_text'] = test_df.apply(lambda o: o['text'] if len(o['text']) < 3 else o['selected_text'], 1)\nsubdf = test_df[['textID', 'selected_text']]\nsubdf.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}