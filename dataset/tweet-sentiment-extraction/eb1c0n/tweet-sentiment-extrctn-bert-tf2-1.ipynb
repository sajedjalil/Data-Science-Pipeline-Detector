{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Complete TensorFlow mixed-precision implementation with Bert\n\n*It seems that mixed-precision isn't working in this kernel, but it does locally with suitable graphics card (try it out!), and reduces the runtime by a factor of 2.<br><br>*\n*Update 1: Corrected the implementation, so that it now works as it should.<br><br>*\n*Update 2: Small adjustments, and added hyperparameters to transformer model<br><br>*\n*Update 3: `\" \".join(set(selected.lower().split()))`<br><br>*\n*Update 4: `len(text[i].split()) < 2: decoded_text = text`<br><br>*\n*Update 5: Removing softmax step on predictions<br><br>*\n\nDone in three steps:\n1. define preprocessing procedure and add it to tf.data.Dataset.from_generator\n2. define Bert model together with train, predict and decode_prediction functions\n3. run the K-fold cross-validation including everything defined in step 1. and 2.\n\nNote: A simple **post processing** is used: predicting all 'neutral' sentiments to full text.\n\ncredits to [abhishek](https://www.kaggle.com/abhishek) and all the 'get started' kernels to help/inspire me and many others to get started with this competition. The preprocessing in this notebook follows abhishek's [implementation](https://www.kaggle.com/abhishek/tweet-text-extraction-roberta-infer) with some modifications.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom math import ceil, floor\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom sklearn import model_selection\nfrom transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer, TFBertModel, BertTokenizer, PreTrainedTokenizer, AutoTokenizer\n# from transformers.tokenizers import PreTrainedTokenizerFast\nfrom tokenizers import BertWordPieceTokenizer\n\nimport logging\ntf.get_logger().setLevel(logging.ERROR)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n    \ntf.config.optimizer.set_jit(True)\ntf.config.optimizer.set_experimental_options(\n    {\"auto_mixed_precision\": True})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read csv files\ntrain_df = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntrain_df.dropna(inplace=True)\ntrain_df = train_df.drop(train_df[train_df.sentiment=='neutral'].index, axis=0).reset_index(drop=True)\n\ntest_df = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\ntest_df.loc[:, \"selected_text\"] = test_df.text.values\n\nsubmission_df = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')\n\nprint(\"train shape =\", train_df.shape)\nprint(\"test shape  =\", test_df.shape)\n\n# set some global variables\nPATH = \"../input/bert-base-uncased/\"\nMAX_SEQUENCE_LENGTH = 128\ntokenizer = BertWordPieceTokenizer(f\"{PATH}/vocab.txt\", lowercase=True, add_special_tokens=False)\nMODELNAME = 'bert'\n\n# let's take a look at the data\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n!pip install twython\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, sys, gc\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport string\nimport time\nimport re\nimport nltk\nimport spacy\nfrom spacy import displacy\nfrom spacy.pipeline import Sentencizer\nfrom spacy.lang.en import English\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom textblob import TextBlob\n\n\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# txt = 'text soooooo is,typing.......'\n# tags = [(token.pos_, token.dep_, token.idx, token.text) for token in nlp(txt) if token.pos_ != 'SPACE'] + [(..., ..., len(txt))]\n# enc = tokenizer.encode(txt)\n# tokens = enc.tokens\n# offsets = enc.offsets\n# pos_tags = []\n# dep_tags = []\n# index0 = 0\n# # print(tags)\n# print(tokens)\n# for index, token in enumerate(tokens):\n# #     print(index, offsets[index][0], tags[index0])\n#     if offsets[index][0] == tags[index0+1][2]:\n#         index0 += 1\n#     pos_tags.append(tags[index0][0])\n#     dep_tags.append(tags[index0][1])\n# print(pos_tags, dep_tags)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nI. Set up preprocessing and dataset/datagenerator\n```\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_map = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699,\n    }\n\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove any links\n    text = re.sub('\\s\\s+', ' ', text) # remove any links\n    return text.lower().strip()\n\n\ndef tag_bert_tokens(train, columns):\n    for column in columns:\n        attr_values = []\n        for ind, (tags, bert_tokens_words_bpe_length) in enumerate(train[[column, 'bert_tokens_words_bpe_length']].values):\n            if type(tags) is list:\n                try:\n                    if 'polarity' in column:\n                        attr_values.append([[0]*len(tags[0])]*3 + [tags[index] for index, value in enumerate(bert_tokens_words_bpe_length) for _ in range(value)] + [[0]*len(tags[0])])\n                    else:\n                        attr_values.append([0]*3 + [tags[index] for index, value in enumerate(bert_tokens_words_bpe_length) for _ in range(value)] + [0])\n                except:\n                    print(train.iloc[ind].values)\n                    print(len(bert_tokens_words_bpe_length), len(tags), tags, bert_tokens_words_bpe_length)\n                    assert False\n            else:\n                attr_values.append([[0]]*3 + [[tags] for value in bert_tokens_words_bpe_length for _ in range(value)] + [[0]])\n        train[column] = attr_values\n    \ndef onehot_encode(sequence, set_of_tags, tag2idx):\n    ohe = np.zeros((len(sequence), len(set_of_tags)))\n    for index, tag in enumerate(sequence):\n        if tag in set_of_tags:\n            ohe[index, tag2idx[tag]] = 1\n    return ohe\n\ndef label_encode(sequence, set_of_tags, tag2idx):\n    encoded_seq = []\n    for index, tag in enumerate(sequence):\n        if tag in set_of_tags:\n            encoded_seq.append(tag2idx[tag])\n    return np.array(encoded_seq, dtype=np.int)\n\ndef encode_all(tagged_text, set_of_tags, tag2idx, encode_func=onehot_encode):\n    sequences = []\n    for sequence in tagged_text:\n        sequences.append(encode_func(sequence, set_of_tags, tag2idx))\n    return sequences\n\n\nclass SentimentAnalyzer:\n    \n    def __init__(self, sentiment_analyzer):\n        self.sentiment_analyzer = sentiment_analyzer\n        if type(self.sentiment_analyzer) not in [SentimentIntensityAnalyzer, TextBlob]:\n            raise BaseException('Unknown sentiment analyzer', self.sentiment_analyzer)\n        \n    def __call__(self, text):\n        if isinstance(self.sentiment_analyzer, SentimentIntensityAnalyzer):\n            return self.sentiment_analyzer.polarity_scores(text)['compound']\n        elif isinstance(self.sentiment_analyzer, TextBlob):\n            return TextBlob(text).sentiment.polarity\n            \n\ndef text2tag(text, sentiment, tokens, offsets, data_model_config, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    if not data_model_config['pos_dep_tags'] and not data_model_config['sentiment_calculation']:\n        return [], [], []\n    if data_model_config['pos_dep_tags']:\n        tags = [(token.pos_, token.dep_, token.idx, token.text) for token in nlp(text) if token.pos_ != 'SPACE'] + [(..., ..., len(text), '')]\n        if data_model_config['sentiment_calculation']:\n            spacy_text = ' '.join(list(zip(*tags[:-1]))[-1])\n            polarity_scores = calculate_texts_polarity_score(spacy_text, sentiment, n_gram_window_range, sentiment_analyzers)\n    else:\n        tags = []\n        spacy_text = ''\n        for idx, char in enumerate(text):\n            if char == ' ':\n                continue\n            elif char in string.punctuation or not tags or tags[-1][-1] in string.punctuation:\n                tags.append(('', '', idx, char))\n            else:\n                tags[-1] = tags[-1][:3] + (tags[-1][3] + char,)\n#         print(tags)\n        tags += [(..., ..., len(text), '')]\n                    \n        spacy_text = ' '.join(list(zip(*tags[:-1]))[-1])\n        polarity_scores = calculate_texts_polarity_score(spacy_text, sentiment, n_gram_window_range, sentiment_analyzers)\n    pos_tags = []\n    dep_tags = []\n    polarity_scores_tokens = []\n    index0 = 0\n    # print(tags)\n    for index, token in enumerate(tokens[3:-1]):\n        index += 3\n    #     print(index, offsets[index][0], tags[index0])\n        if offsets[index][0] == tags[index0+1][2]:\n            index0 += 1\n        if data_model_config['pos_dep_tags']:\n            pos_tags.append(tags[index0][0])\n            dep_tags.append(tags[index0][1])\n        if data_model_config['sentiment_calculation']:\n            polarity_scores_tokens.append([])\n            for ind, pol_token_scores in enumerate(polarity_scores):\n                polarity_scores_tokens[-1] += polarity_scores[ind][index0]\n    return pos_tags, dep_tags, polarity_scores_tokens\n\ndef words_polarity_score(words, sentiment, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    \"\"\"\n    n_gram_window: word[i - n_gram_window_range[1]], ..., words[i], ..., word[i + n_gram_window_range[1]]\n    \"\"\"\n    sentiment_kef = 1 if sentiment == 'positive' else -1\n    polarity_scores = []\n    for _ in range(len(sentiment_analyzers)):\n        polarity_scores.append([])\n    for index, word in enumerate(words):\n        for _ in range(len(sentiment_analyzers)):\n            polarity_scores[_].append([])\n        for window in range(*n_gram_window_range):\n            for index_, sentiment_analyzer in enumerate(sentiment_analyzers):\n                polarity_scores[index_][-1].append(sentiment_analyzer(' '.join(words[max(0, index-window):min(len(words), index+window+1)])))\n    return polarity_scores\n\ndef calculate_texts_polarity_score(text, sentiment, n_gram_window_range=(0, 4), sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer()), \n                                                                                            SentimentAnalyzer(TextBlob('123'))]):\n    \n    column_basename = 'words_polarity_scores'\n    polarity_scores = []\n#     for _ in range(len(sentiment_analyzers)):\n#         polarity_scores.append([])\n        \n    words = text.split()\n    scores = words_polarity_score(words, sentiment, n_gram_window_range, sentiment_analyzers)\n    for index, scores_ in enumerate(scores):\n        polarity_scores.append(scores_)\n            \n#     for index, sentiment_analyzer in enumerate(sentiment_analyzers):\n#         if isinstance(sentiment_analyzer.sentiment_analyzer, SentimentIntensityAnalyzer):\n#             train[f'vader_{column_basename}'] = polarity_scores[index]\n#         elif isinstance(sentiment_analyzer.sentiment_analyzer, TextBlob):\n#             train[f'textblob_{column_basename}'] = polarity_scores[index]\n# #         elif # add your own sentiment analyzer\n    return polarity_scores\n\ndef count_sentences(text):\n    return len(re.split('[.!?]+', text))\n\ndef concat_encoded_features(train, columns):\n    concateneted_features = []\n    for row in train[columns].values:\n        concateneted_features.append(np.concatenate(row, axis=1))\n    return concateneted_features\n\nclass Preprocessor:\n    \n    def __init__(self, data_model_config, sentiment_analyzers=[SentimentAnalyzer(SentimentIntensityAnalyzer())]):\n        self.data_model_config = data_model_config\n        self.max_num_sents = 0\n        self.max_num_tokens = 0\n        self.pos2idx = {}\n        self.dep2idx = {}\n        self.sentiment_analyzers = sentiment_analyzers\n        \n        self.build_output_types()\n    \n    def preprocess(self, text, selected_text, sentiment, isTrain):\n        text = text.decode('utf-8').strip()\n        selected_text = selected_text.decode('utf-8').strip()\n        sentiment = sentiment.decode('utf-8').strip()\n#         print(text)\n    #     X.fillna('', inplace=True)\n#         if sentiment == 'neutral':\n#             return ()\n        # clean texts\n        text = clean_text(text)\n        \n        # tokenize with offsets\n        enc = tokenizer.encode(text)\n        input_ids, offsets = enc.ids, enc.offsets\n        \n        if isTrain:\n            selected_text = clean_text(selected_text)\n#             print(selected_text)\n\n            # find the intersection between text and selected text\n            idx_start, idx_end = None, None\n            for index in (i for i, c in enumerate(text) if c == selected_text[0]):\n                if text[index:index+len(selected_text)] == selected_text:\n                    idx_start = index\n                    idx_end = index + len(selected_text)\n                    break\n#             print(idx_start, idx_end)\n\n            intersection = [0] * len(text)\n            if idx_start != None and idx_end != None:\n                for char_idx in range(idx_start, idx_end):\n                    intersection[char_idx] = 1\n\n                # compute targets\n                target_idx = []\n                for i, (o1, o2) in enumerate(offsets):\n                    if sum(intersection[o1: o2]) > 0: # label\n                        target_idx.append(i)\n    #                 if sum(intersection[o1: o2]) == (o2 - o1):\n    #                     target_idx.append(i)\n\n                target_start = target_idx[0] + 3\n                target_end = target_idx[-1] + 3\n            else:\n                target_start = 0\n                target_end = 0\n        else:\n            target_start = 0\n            target_end = 0\n\n        input_ids = [101, sentiment_map[sentiment], 102] + input_ids + [102]\n        input_type_ids = [0, 0, 0] + [1] * (len(input_ids) - 3)\n        attention_mask = [1] * len(input_ids)\n        offsets = [(0, 0), (0, 0), (0, 0)] + offsets + [(0, 0)]\n\n        if self.data_model_config['global_statistics']:\n            # calculate number of sentences\n            text_num_sents = count_sentences(text)\n            # calculate number of tokens (words + punctuation)\n            text_num_tokens = len(text.split(' '))\n\n            # min_max scaling\n            if isTrain:\n                self.max_num_sents = max(self.max_num_sents, text_num_sents)\n                self.max_num_tokens = max(self.max_num_tokens, text_num_tokens)\n            text_num_sents = MAX_SEQUENCE_LENGTH * [[text_num_sents]]\n            text_num_tokens = MAX_SEQUENCE_LENGTH * [[text_num_tokens]]\n        \n        # get pos, dep tags and calculate polarity scores of each word\n        pos_tags, dep_tags, polarity_scores_tokens = text2tag(text, sentiment, input_ids, offsets, self.data_model_config, sentiment_analyzers=self.sentiment_analyzers)\n        \n#         print(polarity_scores_tokens)\n        if self.data_model_config['pos_dep_tags']:\n            for index, tag in enumerate(pos_tags):\n                if tag not in self.pos2idx and isTrain:\n    #                 print('new pos tag', tag)\n                    self.pos2idx[tag] = len(self.pos2idx)# + 1\n                pos_tags[index] = self.pos2idx.get(tag, len(self.pos2idx))\n    #         pos_tags = [0, 0, 0] + pos_tags + [0]\n\n            for index, tag in enumerate(dep_tags):\n                if tag not in self.dep2idx and isTrain:\n    #                 print('new dep tag', tag)\n                    self.dep2idx[tag] = len(self.dep2idx)# + 1\n                dep_tags[index] = self.dep2idx.get(tag, len(self.dep2idx))\n    #         dep_tags = [0, 0, 0] + dep_tags + [0]\n            pos_tag_indices, pos_tag_values, pos_tag_dense_shape = (np.array([np.arange(3, 3 + len(pos_tags)), pos_tags]).T,\n                                                                    np.ones((len(pos_tags),)),\n                                                                    (MAX_SEQUENCE_LENGTH, len(self.pos2idx)))\n            dep_tag_indices, dep_tag_values, dep_tag_dense_shape = (np.array([np.arange(3, 3 + len(dep_tags)), dep_tags]).T,\n                                                                    np.ones((len(dep_tags),)),\n                                                                    (MAX_SEQUENCE_LENGTH, len(self.dep2idx)))\n                \n\n        padding_length = MAX_SEQUENCE_LENGTH - len(input_ids)\n        if padding_length > 0:\n            input_ids += [0] * padding_length\n            attention_mask += [0] * padding_length\n            input_type_ids += [0] * padding_length\n            offsets += [(0, 0)] * padding_length\n            \n            if self.data_model_config['sentiment_calculation']:\n                polarity_scores_tokens = (np.zeros((3, len(polarity_scores_tokens[0]),)).tolist() + \n                                          polarity_scores_tokens + \n                                          (padding_length + 1) * np.zeros((1, len(polarity_scores_tokens[0]),)).tolist())\n        elif padding_length < 0:\n            # not yet implemented\n            # truncates if input length > max_seq_len\n            pass\n    \n#         print(pos_tags)\n#         print('preproc')\n        input_sample = (input_ids, attention_mask, input_type_ids,)\n        if self.data_model_config['global_statistics']:\n            input_sample += (text_num_sents, text_num_tokens,)\n        if self.data_model_config['pos_dep_tags']:\n            input_sample += ((pos_tag_indices, pos_tag_values, pos_tag_dense_shape), \n                             (dep_tag_indices, dep_tag_values, dep_tag_dense_shape),)\n        if self.data_model_config['sentiment_calculation']:\n            input_sample += (polarity_scores_tokens,)\n#         print(len(input_sample))\n        \n        return (\n            input_sample,\n            (target_start, target_end),\n            (text, offsets, selected_text, sentiment)\n        )\n                  \n    def map_fn(self, *sample):\n#         print('map')\n        (\n            input_sample,\n            (target_start, target_end),\n            (text, offsets, selected_text, sentiment)\n        ) = sample\n        \n        if self.data_model_config['global_statistics']:\n            text_num_sents, text_num_tokens = input_sample[3:5]\n            text_num_sents /= self.max_num_sents\n            text_num_tokens /= self.max_num_tokens\n            input_sample = input_sample[:3] + (text_num_sents, text_num_tokens,) + input_sample[5:]\n        if self.data_model_config['pos_dep_tags']:\n            if self.data_model_config['sentiment_calculation']:\n                ((pos_tag_indices, pos_tag_values, pos_tag_dense_shape), \n                (dep_tag_indices, dep_tag_values, dep_tag_dense_shape)) = input_sample[-3:-1]\n            else:\n                ((pos_tag_indices, pos_tag_values, pos_tag_dense_shape), \n                (dep_tag_indices, dep_tag_values, dep_tag_dense_shape)) = input_sample[-2:]\n            pos_tags = tf.SparseTensor(indices=pos_tag_indices, values=pos_tag_values, dense_shape=(MAX_SEQUENCE_LENGTH, len(self.pos2idx)+1))\n            dep_tags = tf.SparseTensor(indices=dep_tag_indices, values=dep_tag_values, dense_shape=(MAX_SEQUENCE_LENGTH, len(self.dep2idx)+1))\n            if self.data_model_config['sentiment_calculation']:\n                input_sample = input_sample[:-3] + (pos_tags, dep_tags,) + input_sample[-1:]\n            else:\n                input_sample = input_sample[:-2] + (pos_tags, dep_tags,)\n        \n        return (\n            input_sample,\n            (target_start, target_end),\n            (text, offsets, selected_text, sentiment)\n        )\n    \n    def build_output_types(self):\n        input_types = (tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32,) # input_ids, attention_ids, input_type_ids\n        if self.data_model_config['global_statistics']:\n            input_types += (tf.dtypes.int32,   tf.dtypes.int32,) # text_num_sents, text_num_tokens\n        if self.data_model_config['pos_dep_tags']:\n            input_types += ((tf.dtypes.int64,  tf.dtypes.float32,   tf.dtypes.int64),  # pos tags indices, values, shape\n                            (tf.dtypes.int64,  tf.dtypes.float32,   tf.dtypes.int64),) # dep tags indices, values, shape\n        if self.data_model_config['sentiment_calculation']:\n            input_types += (tf.dtypes.float32,) # polarity scores\n            \n        self.OUTPUT_TYPES = (\n            input_types,\n            (tf.dtypes.int32,  tf.dtypes.int32), # target start, target end\n            (tf.dtypes.string, tf.dtypes.int32, tf.dtypes.string, tf.dtypes.string) # text, offsets, selected_text, sentiment\n        )\n    \n    \n#     OUTPUT_SHAPES = (\n#         (\n#             (128,), (128,), (128,), \n#             (128, 1), (128, 1), ((None, 2), (None,), (2)), ((None, 2), (None,), (2)), (128, None)\n#         ),\n#         (\n#             (), ()\n#         ),\n#         (\n#             (), (128, 2), (), ()\n#         )\n#     )\n    \n    # AutoGraph will automatically convert Python code to\n    # Tensorflow graph code. You could also wrap 'preprocess' \n    # in tf.py_function(..) for arbitrary python code\n    def _generator(self, tweet, selected_text, sentiment, isTrain):\n        for tw, st, se in zip(tweet, selected_text, sentiment):\n            outputs = self.preprocess(tw, st, se, isTrain)\n            if (not any(outputs[1]) or se == 'neutral') and isTrain: # target_start and target_end == 0\n                continue\n            yield outputs\n    \n    # This dataset object will return a generator\n    def _create_dataset(self, *args):\n        return tf.data.Dataset.from_generator(\n            self._generator,\n            output_types=self.OUTPUT_TYPES,\n#             output_shapes=cls.OUTPUT_SHAPES,\n            args=args #tweet, selected_text, sentiment, isTrain\n        )\n    \n    def create_dataset(self, dataframe, batch_size, isTrain, shuffle_buffer_size=-1):\n        dataset = self._create_dataset(\n            dataframe.text.values, \n            dataframe.selected_text.values, \n            dataframe.sentiment.values, \n            isTrain\n        )#.map(Preprocessor.map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        \n        if self.data_model_config['global_statistics'] or self.data_model_config['pos_dep_tags']:\n            for sample in dataset:\n                pass\n\n            dataset = dataset.map(self.map_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            \n\n        dataset = dataset.cache()\n        if shuffle_buffer_size != -1:\n            dataset = dataset.shuffle(shuffle_buffer_size)\n        dataset = dataset.batch(batch_size)\n#         dataset = dataset\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n        return dataset\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nII. Set up transformer model and functions\n```","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"class BertQAModel(TFBertPreTrainedModel):\n    \n    DROPOUT_RATE = 0.1\n    NUM_HIDDEN_STATES = 2\n    \n    def __init__(self, config, preprocessor, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        \n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        self.concat = L.Concatenate(axis=-1)\n        self.dropout = L.Dropout(self.DROPOUT_RATE)\n        self.fc = L.Dense(64, activation='relu')\n        self.dropout1 = L.Dropout(self.DROPOUT_RATE)\n        self.qa_outputs = L.Dense(\n            config.num_labels, \n            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n            dtype='float32',\n            name=\"qa_outputs\")\n        \n        self.last_axis_dim = 0\n        if preprocessor.data_model_config['global_statistics']:\n            self.last_axis_dim += 2\n        if preprocessor.data_model_config['pos_dep_tags']:\n            self.last_axis_dim += len(preprocessor.pos2idx) + len(preprocessor.dep2idx) + 2\n        if preprocessor.data_model_config['sentiment_calculation']:\n            self.last_axis_dim += 4\n        \n    @tf.function\n    def call(self, inputs, **kwargs):\n        # outputs: Tuple[sequence, pooled, hidden_states]\n        if type(inputs) is dict:\n            last_out, pooled, hidden_states = self.bert(inputs, **kwargs)\n        else:\n            last_out, pooled, hidden_states = self.bert(*inputs[:3], **kwargs)\n        \n        hidden_states = self.dropout(last_out, training=kwargs.get(\"training\", False))\n#         hidden_states = last_out\n        if self.last_axis_dim:\n            if type(inputs) is not dict: # list or tuple\n                hidden_states = self.concat([hidden_states, *inputs[3:]]) #tf.sparse.to_dense(inputs[3], default_value=0),tf.sparse.to_dense(inputs[4], default_value=0), \n            else:\n                hidden_states = self.concat([hidden_states, tf.ones((3, 5, self.last_axis_dim))])\n        hidden_states = self.fc(hidden_states)\n        hidden_states = self.dropout1(hidden_states, training=kwargs.get(\"training\", False))\n        logits = self.qa_outputs(hidden_states)\n        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n        start_logits = tf.squeeze(start_logits, axis=-1)\n        end_logits = tf.squeeze(end_logits, axis=-1)\n        \n        return start_logits, end_logits\n    \n    \ndef train(model, dataset, loss_fn, optimizer):\n    \n    @tf.function\n    def train_step(model, inputs, y_true, loss_fn, optimizer):\n        with tf.GradientTape() as tape:\n            y_pred = model(inputs, training=True)\n#             print(y_pred, y_true)\n            start_loss  = loss_fn(y_true[0], y_pred[0])\n            end_loss = loss_fn(y_true[1], y_pred[1])\n            loss = start_loss + end_loss\n#             scaled_loss = loss\n            \n            scaled_loss = optimizer.get_scaled_loss(loss)\n    \n        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, y_pred\n\n    epoch_loss = 0.\n    for batch_num, sample in enumerate(dataset):\n#         print(sample)\n        loss, y_pred = train_step(\n            model, sample[0], sample[1], loss_fn, optimizer)\n\n        epoch_loss += loss\n\n        print(\n            f\"training ... batch {batch_num+1:03d} : \"\n            f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n            end='\\r')\n        \n        \ndef predict(model, dataset, loss_fn, optimizer):\n        \n    def to_numpy(*args):\n        out = []\n        for arg in args:\n            if arg.dtype == tf.string:\n                arg = [s.decode('utf-8') for s in arg.numpy()]\n                out.append(arg)\n            else:\n                arg = arg.numpy()\n                out.append(arg)\n        return out\n    \n    # Initialize accumulators\n    offset = tf.zeros([0, 128, 2], dtype=tf.dtypes.int32)\n    text = tf.zeros([0,], dtype=tf.dtypes.string)\n    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n    pred_start = tf.zeros([0, 128], dtype=tf.dtypes.float32)\n    pred_end = tf.zeros([0, 128], dtype=tf.dtypes.float32)\n    \n    for batch_num, sample in enumerate(dataset):\n        \n        print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n        \n        y_pred = model(sample[0], training=False)\n        \n        # add batch to accumulators\n        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n        offset = tf.concat((offset, sample[2][1]), axis=0)\n        text = tf.concat((text, sample[2][0]), axis=0)\n        selected_text = tf.concat((selected_text, sample[2][2]), axis=0)\n        sentiment = tf.concat((sentiment, sample[2][3]), axis=0)\n\n    # pred_start = tf.nn.softmax(pred_start)\n    # pred_end = tf.nn.softmax(pred_end)\n    \n    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n    \n    return pred_start, pred_end, text, selected_text, sentiment, offset\n\n\ndef decode_prediction(pred_start, pred_end, text, offset, sentiment):\n    \n    def decode(pred_start, pred_end, text, offset):\n\n        decoded_text = \"\"\n        for i in range(pred_start, pred_end+1):\n#             print(offset[i])\n            decoded_text += text[offset[i][0]:offset[i][1]]\n            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n                decoded_text += \" \"\n        return decoded_text\n    \n    decoded_predictions = []\n#     print(pred_start.shape, pred_end.shape, offset.shape, text)\n    for i in range(len(text)):\n        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n            decoded_text = text[i]\n        else:\n            idx_start = np.argmax(pred_start[i])\n            idx_end = np.argmax(pred_end[i])\n#             print(idx_start, idx_end, offset[i].tolist())\n            if idx_start > idx_end:\n                idx_end = idx_start \n#             print(idx_start, idx_end, offset[i])\n            decoded_text = decode(idx_start, idx_end, text[i], offset[i])\n#             print(decoded_text)\n#             decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n            if len(decoded_text.strip()) == 0:\n                decoded_text = text[i]\n        decoded_predictions.append(decoded_text.strip())\n    \n    return decoded_predictions\n\ndef decode_prediction1(pred_start, pred_end, text, offset, sentiment):\n    \n    def decode(pred_start, pred_end, text, offset):\n\n        decoded_text = \"\"\n        for i in range(pred_start, pred_end+1):\n#             print(offset[i])\n            decoded_text += text[offset[i][0]:offset[i][1]]\n            if (i+1) < len(offset) and offset[i][1] < offset[i+1][0]:\n                decoded_text += \" \"\n                \n        # post add\n        while (i+1) < len(offset) and offset[i][1] == offset[i+1][0] and offset[i+1][0]:\n            decoded_text += text[offset[i+1][0]:offset[i+1][1]]\n            i += 1\n        \n        # pre add\n        i = pred_start\n        while i - 1 > 0 and offset[i-1][1] == offset[i][0] and offset[i-1][0]:\n            decoded_text = text[offset[i-1][0]:offset[i-1][1]] + decoded_text\n            i -= 1\n        \n        return decoded_text\n    \n    decoded_predictions = []\n#     print(pred_start.shape, pred_end.shape, offset.shape, text)\n    for i in range(len(text)):\n        if sentiment[i] == \"neutral\" or len(text[i].split()) < 2:\n            decoded_text = text[i]\n        else:\n            idx_start = np.argmax(pred_start[i])\n            idx_end = np.argmax(pred_end[i])\n#             print(idx_start, idx_end, offset[i].tolist())\n            if idx_start > idx_end:\n                idx_end = idx_start \n#             print(idx_start, idx_end, offset[i])\n            decoded_text = decode(idx_start, idx_end, text[i], offset[i])\n#             print(decoded_text)\n#             decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n            if len(decoded_text.strip()) == 0:\n                decoded_text = text[i]\n        decoded_predictions.append(decoded_text.strip())\n    \n    return decoded_predictions\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, dataset, loss_fn, optimizer):\n    \n#     @tf.function\n    def train_step(model, inputs, y_true, loss_fn, optimizer): # add weight according to jacard score start-end span matrix\n        with tf.GradientTape() as tape:\n            y_pred = model(inputs, training=True)\n#             print(y_pred, y_true)\n            y_true = (tf.sparse.to_dense(tf.sparse.SparseTensor(tf.cast(tf.stack((np.arange(y_pred[0].shape[0]), y_true[0]), axis=1), tf.int64), \n                                                                np.ones(y_pred[0].shape[0], dtype=np.int64), \n                                                                dense_shape=y_pred[0].shape),\n                                         default_value=0), \n                      tf.sparse.to_dense(tf.sparse.SparseTensor(tf.cast(tf.stack((np.arange(y_pred[1].shape[0]), y_true[1]), axis=1), tf.int64), \n                                                                np.ones(y_pred[1].shape[0], dtype=np.int64), \n                                                                dense_shape=y_pred[1].shape),\n                                         default_value=0))\n#             print(y_pred, y_true[0].shape)\n            start_loss  = tf.nn.softmax(y_pred[0])\n            end_loss = tf.nn.softmax(y_pred[1])\n#             print(start_loss, end_loss, tf.expand_dims(start_loss, axis=-1))\n#             print((tf.expand_dims(start_loss, axis=-1) + tf.expand_dims(end_loss, axis=-2)).shape)\n            start_end_interaction = tf.reshape((tf.expand_dims(start_loss, axis=-1) * tf.expand_dims(end_loss, axis=-2)) \\\n                                               * tf.cast(tf.linalg.band_part(np.ones((MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH)), -1, 0), tf.float32), \n                                               (y_pred[0].shape[0], -1))\n            loss = loss_fn(y_true[0], y_pred[0]) + \\\n                   loss_fn(y_true[1], y_pred[1]) + \\\n                   tf.reduce_sum(start_end_interaction)\n#             print(loss.shape, loss_fn(y_true[0], y_pred[0]).shape, loss_fn(y_true[0], y_pred[0]))\n            scaled_loss = optimizer.get_scaled_loss(loss)\n#         assert False\n    \n        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, y_pred\n\n    epoch_loss = 0.\n    for batch_num, sample in enumerate(dataset):\n#         print(sample)\n        loss, y_pred = train_step(\n            model, sample[0], sample[1], loss_fn, optimizer)\n\n        epoch_loss += loss\n\n        print(\n            f\"training ... batch {batch_num+1:03d} : \"\n            f\"train loss {epoch_loss/(batch_num+1):.3f} \",\n            end='\\r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"```\nIII. Run it all: \n\nmodel.create() -> dataset.create() -> train(train) ->\n       -> predict(val).decode() -> predict(test).decode() -> submit\n```","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_folds = 5\nnum_epochs = 3\nbatch_size = 32\nlearning_rate = 3e-5\n\ndmc = {\n    'global_statistics': False,\n    'pos_dep_tags': False,\n    'sentiment_calculation': False\n}\npreproc = Preprocessor(dmc)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate)\noptimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n    optimizer, 'dynamic')\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nloss_fn1 = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n\nkfold = model_selection.KFold(\n    n_splits=num_folds, shuffle=True, random_state=42)\n\n# initialize test predictions\ntest_preds_start = np.zeros((len(test_df), 128), dtype=np.float32)\ntest_preds_end = np.zeros((len(test_df), 128), dtype=np.float32)\n\nfor fold_num, (train_idx, valid_idx) in enumerate(kfold.split(train_df.text)):\n    print(\"\\nfold %02d\" % (fold_num+1))\n        \n#     break\n    train_dataset = preproc.create_dataset(\n        train_df.iloc[train_idx], batch_size, isTrain=True, shuffle_buffer_size=2048)#.shuffle(2048).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    valid_dataset = preproc.create_dataset(\n        train_df.iloc[valid_idx], batch_size, isTrain=False, shuffle_buffer_size=-1)#.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    \n    test_dataset = preproc.create_dataset(\n        test_df, batch_size, isTrain=False, shuffle_buffer_size=-1)\n    \n    \n    config = BertConfig(output_hidden_states=True, num_labels=2)\n    BertQAModel.DROPOUT_RATE = 0.1\n    BertQAModel.NUM_HIDDEN_STATES = 2\n    model = BertQAModel.from_pretrained(PATH, config=config, preprocessor=preproc)\n#     break\n    for epoch_num in range(num_epochs):\n        print(\"\\nepoch %03d\" % (epoch_num+1))\n        \n        # train for an epoch\n        train(model, train_dataset, loss_fn1, optimizer)\n        \n        # predict validation set and compute jaccardian distances\n        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n            predict(model, valid_dataset, loss_fn1, optimizer)\n        \n        selected_text_pred = decode_prediction(\n            pred_start, pred_end, text, offset, sentiment)\n        jaccards = []\n        for i in range(len(selected_text)):\n            jaccards.append(\n                jaccard(selected_text[i], selected_text_pred[i]))\n        \n        score = np.mean(jaccards)\n        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n        \n        selected_text_pred = decode_prediction1(\n            pred_start, pred_end, text, offset, sentiment)\n        jaccards = []\n        for i in range(len(selected_text)):\n            jaccards.append(\n                jaccard(selected_text[i], selected_text_pred[i]))\n        \n        score = np.mean(jaccards)\n        print(f\"valid jaccard epoch {epoch_num+1:03d}: pre-post-processing {score}\"+\" \"*15)\n        \n#         evaldef(model, valid_dataset, valid_idx, new_train, batch_size=batch_size)\n    break\n        \n#         if score > best_score:\n#             best_score = score\n#             # requires you to have 'fold-{fold_num}' folder in PATH:\n#             # model.save_pretrained(PATH+f'fold-{fold_num}')\n#             # or\n#             # model.save_weights(PATH + f'fold-{fold_num}.h5')\n            \n#             # predict test set\n#             test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n#                 predict(model, test_dataset, loss_fn, optimizer)\n    \n#     # add epoch's best test preds to test preds arrays\n#     test_preds_start += test_pred_start * 0.2\n#     test_preds_end += test_pred_end * 0.2\n    \n#     # reset model, as well as session and graph (to avoid OOM issues?) \n#     session = tf.compat.v1.get_default_session()\n#     graph = tf.compat.v1.get_default_graph()\n#     del session, graph, model\n#     model = BertQAModel.from_pretrained(PATH, config=config)\n    \n# # decode test set and add to submission file\n# selected_text_pred = decode_prediction(\n#     test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\n\n\n# # Update 3 (see https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140942)\n# def f(selected):\n#     return \" \".join(set(selected.lower().split()))\n# submission_df.loc[:, 'selected_text'] = selected_text_pred\n# submission_df['selected_text'] = submission_df['selected_text'].map(f)\n\n# submission_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}