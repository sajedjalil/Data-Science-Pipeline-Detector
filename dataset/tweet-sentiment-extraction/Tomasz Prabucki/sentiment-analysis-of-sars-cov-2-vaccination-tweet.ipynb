{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome\nThis is a supplementary Jupyter Notebook to a paper \"Sentiment Analysis of SARS-CoV-2 vaccination tweet\" made for 7088CEM - Artificial Neural Networks - 2021JANMAY module at Coventry University, as part MSc Computer Science programme. ","metadata":{}},{"cell_type":"markdown","source":"# Setup\n\n## Kaggle default setup","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment analysis setup\n\nLet's first import all the necessary modules that our notebook will use as part of this project.","metadata":{}},{"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\nimport string\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nimport nltk\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport gensim\nfrom sklearn.model_selection import train_test_split\nimport spacy\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport keras\nfrom keras.utils.vis_utils import plot_model\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly as pl\nimport datetime\n\n#for confusion matrix\nimport seaborn\nfrom sklearn.metrics import confusion_matrix\n\n\nnltk.download('wordnet')\nprint('Done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training set import\n\nBelow we are importing training set for our models, taken from:\nhttps://www.kaggle.com/c/tweet-sentiment-extraction","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's test whether it was loaded correctly by printing first 10 rows:","metadata":{}},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make sure that the sentiment data is correct, let's check that it does not contain any other sentiment values than positive/negative/neutral:","metadata":{}},{"cell_type":"code","source":"train['sentiment'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's take a look at the distribution of the dataset:","metadata":{}},{"cell_type":"code","source":"train.groupby('sentiment').nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pre-processing\n\nThis section covers the techniques described in Section 5 of attached paper.\n\nFirst of all, let's remove all the unncessary columns apart fomr text and sentiment. As this is only a training set, we do not actually use this dataset for sentiment analysis.","metadata":{}},{"cell_type":"code","source":"train = train[['selected_text','sentiment']]\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's also ensure that there is no null value.\n\nIf there is, let's fill it.","metadata":{}},{"cell_type":"code","source":"if (train[\"selected_text\"].isnull().sum() > 0):\n    train[\"selected_text\"].fillna(\"No content\", inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The next steps about data cleaning will be:\n\n* Remove URLs from the tweets\n* Tokenize text\n* Remove emails\n* Remove new lines characters\n* Remove distracting single quotes\n* Remove all punctuation signs\n* Lowercase all text\n* Detokenize text\n* Convert list of texts to Numpy array","metadata":{}},{"cell_type":"code","source":"def depure_data(data):\n    \n    #Removing URLs with a regular expression\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    data = url_pattern.sub(r'', data)\n\n    # Remove Emails\n    data = re.sub('\\S*@\\S*\\s?', '', data)\n\n    # Remove new line characters\n    data = re.sub('\\s+', ' ', data)\n\n    # Remove distracting single quotes\n    data = re.sub(\"\\'\", \"\", data)\n        \n    return data\n\ntemp = []\n#Splitting pd.Series to list\ndata_to_list = train['selected_text'].values.tolist()\nfor i in range(len(data_to_list)):\n    temp.append(depure_data(data_to_list[i]))\nlist(temp[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n        \n\ndata_words = list(sent_to_words(temp))\n\nprint(data_words[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detokenization, Stemming and Lemmatization","metadata":{}},{"cell_type":"code","source":"def detokenize(text):\n    return TreebankWordDetokenizer().detokenize(text)\n    \ndata = []\nfor i in range(len(data_words)):\n    \n    #for j in range(len(data_words[i])):\n        #stemming\n        #data_words[i][j] = lancaster.stem(data_words[i][j])\n        #lemmatization\n        #data_words[i][j] = data_words[i][j].format(data_words[i][j],wordnet_lemmatizer.lemmatize(data_words[i][j]))\n    \n    data.append(detokenize(data_words[i]))\nprint(data[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.array(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label encoding\n\nAs the dataset is categorical, we need to convert the sentiment labels from Neutral, Negative and Positive to a float type that our model can understand. To achieve this task, we'll implement the to_categorical method from Keras.","metadata":{}},{"cell_type":"code","source":"labels = np.array(train['sentiment'])\ny = []\nfor i in range(len(labels)):\n    if labels[i] == 'neutral':\n        y.append(0)\n    if labels[i] == 'negative':\n        y.append(1)\n    if labels[i] == 'positive':\n        y.append(2)\ny = np.array(y)\nlabels = tf.keras.utils.to_categorical(y, 3, dtype=\"float32\")\ndel y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data sequencing and splitting\n\nWe'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint\nmax_words = 5000\nmax_len = 200\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\ntweets = pad_sequences(sequences, maxlen=max_len)\nprint(tweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(tweets,labels, random_state=0)\nprint (len(X_train),len(X_test),len(y_train),len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"markdown","source":"## Model 1: SingleLSTM","metadata":{}},{"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(layers.Embedding(max_words, 20))\nmodel1.add(layers.LSTM(15,dropout=0.5))\nmodel1.add(layers.Dense(3,activation='softmax'))\n\n\nmodel1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\ncheckpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model1.fit(X_train, y_train, epochs=1,validation_data=(X_test, y_test),callbacks=[checkpoint1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 2: Bidirectional LSTM","metadata":{}},{"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(layers.Embedding(max_words, 40, input_length=max_len))\nmodel2.add(layers.Bidirectional(layers.LSTM(20,dropout=0.6)))\nmodel2.add(layers.Dense(3,activation='softmax'))\nmodel2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n#Implementing model checkpoins to save the best metric and do not lose it on training.\n#model2 = keras.models.load_model('./best_model2.hdf5')\ncheckpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model2.fit(X_train, y_train, epochs=1,validation_data=(X_test, y_test),callbacks=[checkpoint2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 3: One-dimensional Convolutional Neural Network\n\n","metadata":{}},{"cell_type":"code","source":"from keras import regularizers\nmodel3 = Sequential()\nmodel3.add(layers.Embedding(max_words, 40, input_length=max_len))\nmodel3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\nmodel3.add(layers.MaxPooling1D(5))\nmodel3.add(layers.Conv1D(20, 6, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=2e-3, l2=2e-3),bias_regularizer=regularizers.l2(2e-3)))\nmodel3.add(layers.GlobalMaxPooling1D())\nmodel3.add(layers.Dense(3,activation='softmax'))\nmodel3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\ncheckpoint3 = ModelCheckpoint(\"best_model3.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\nhistory = model3.fit(X_train, y_train, epochs=1,validation_data=(X_test, y_test),callbacks=[checkpoint3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you check the val_accuracy metric in the training logs you won't find better score than the one achieved by the BidRNN. Again, the previous model is not the best for this task becaue is majorly used for short translation tasks, but the good thing to notice is its speed to train.\n\nLet's move on.","metadata":{}},{"cell_type":"markdown","source":"# Best model validation\n\nRunning the training has shown that Bidirectional RNN provides best accuracy.\nBecause of that, that's the network we're going to choose for our sentiment analysis.","metadata":{}},{"cell_type":"code","source":"#Let's load the best model obtained during training\nbest_model = keras.models.load_model(\"./best_model2.hdf5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\nprint('Model accuracy: ',test_acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix\n\nAs accuracy is not a good metric to measure the quality of the model,\nwe are going to plot a confusion matrix:","metadata":{}},{"cell_type":"code","source":"matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))\nconf_matrix = pd.DataFrame(matrix, index = ['Neutral','Negative','Positive'],columns = ['Neutral','Negative','Positive'])\n#Normalizing\nconf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\nplt.figure(figsize = (15,15))\nseaborn.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, the model's score is very poor, but keep in mind it hasn't gone through hyperparameter tuning. Let's see how it perfoms on some sample texts:","metadata":{}},{"cell_type":"code","source":"sentiment = ['Neutral','Negative','Positive']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['the trump administration failed to deliver on vaccine promises shocker covidiots coronavirus covidvaccine'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['i hate youtube ads, they are annoying'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence = tokenizer.texts_to_sequences(['i really loved how the technician helped me with the issue that i had'])\ntest = pad_sequences(sequence, maxlen=max_len)\nsentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applying the model to COVID-19 Vaccination set","metadata":{}},{"cell_type":"markdown","source":"# COVID-19 Vaccination set\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Load set","metadata":{}},{"cell_type":"code","source":"covidtweets = pd.read_csv('../input/all-covid19-vaccines-tweets/vaccination_all_tweets.csv')\ncovidtweets.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's examine the columns:","metadata":{}},{"cell_type":"code","source":"print(covidtweets['id'].nunique())\nprint(covidtweets['user_name'].nunique())\nprint(covidtweets['user_location'].nunique())\nprint(covidtweets['user_description'].nunique())\nprint(covidtweets['user_created'].nunique())\nprint(covidtweets['user_friends'].nunique())\nprint(covidtweets['user_favourites'].nunique())\nprint(covidtweets['user_verified'].nunique())\nprint(covidtweets['date'].nunique())\nprint(covidtweets['text'].nunique())\nprint(covidtweets['hashtags'].nunique())\nprint(covidtweets['source'].nunique())\nprint(covidtweets['retweets'].nunique())\nprint(covidtweets['favorites'].nunique())\nprint(covidtweets['is_retweet'].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Is there any null value?","metadata":{}},{"cell_type":"code","source":"covidtweets[\"text\"].isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing","metadata":{}},{"cell_type":"markdown","source":"Just like with training set, we need to pre-process this set with the same rules:","metadata":{}},{"cell_type":"code","source":"temp = []\n#Splitting pd.Series to list\ndata_to_list = covidtweets[\"text\"].values.tolist()\nfor i in range(len(data_to_list)):\n    temp.append(depure_data(data_to_list[i]))\nlist(temp[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_words = list(sent_to_words(temp))\nprint(data_words[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\nfor i in range(len(data_words)):\n    \n    #for j in range(len(data_words[i])):\n        #stemming\n        #data_words[i][j] = lancaster.stem(data_words[i][j])\n        #lemmatization\n        #data_words[i][j] = data_words[i][j].format(data_words[i][j],wordnet_lemmatizer.lemmatize(data_words[i][j]))\n        \n    data.append(detokenize(data_words[i]))\nprint(data[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.array(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding empty sentiment column:","metadata":{}},{"cell_type":"code","source":"covidtweets[\"sentiment\"] = np.nan","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying model to COVID dataset","metadata":{}},{"cell_type":"code","source":"#covidtweets[\"sentiment\"] = best_model.predict_classes(data)]\nfor i in range(len(temp)):\n    sequence = tokenizer.texts_to_sequences([temp[i]])\n    test = pad_sequences(sequence, maxlen=max_len)\n    covidtweets[\"sentiment\"][i] = sentiment[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]]\n    if (i % 461 == 0):\n        percentage = i/len(temp) * 100\n        formatted_percentage = \"{:.0f}\".format(percentage)\n        print(formatted_percentage,\"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's view first 10 results:","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ncovidtweets[[\"text\", \"sentiment\"]].head(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data analysis\n\nFirst let's plot the distribution chart of different sentiments:","metadata":{}},{"cell_type":"code","source":"# Plot sentiment value counts\ncovidtweets['sentiment'].value_counts(normalize=True).plot.bar();\n\ntimeline = covidtweets.groupby(['sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\nprint(timeline)\nfig = px.bar(timeline,\n            x='sentiment', y=\"tweets\", color='sentiment', color_discrete_sequence=[\"#EF553B\", \"#636EFA\", \"#00CC96\"]\n        )\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We neeed to exclude last day's data from the set as is it incomplete:","metadata":{}},{"cell_type":"code","source":"# Convert dates\ncovidtweets['date'] = pd.to_datetime(covidtweets['date'], errors='coerce').dt.date\n\n# Get counts of number of tweets by sentiment for each date\ntimeline = covidtweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\n\nfig = px.line(timeline, x='date', y='tweets', color='sentiment', color_discrete_sequence=[\"#EF553B\", \"#636EFA\", \"#00CC96\"], category_orders={'sentiment': ['neutral', 'negative', 'positive']},\n             title='Timeline showing sentiment of tweets about COVID-19 vaccines')\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Z scores of negative sentiment for each vaccine:","metadata":{}},{"cell_type":"code","source":"all_vax = ['covaxin', 'sinopharm', 'sinovac', 'moderna', 'pfizer', 'biontech', 'oxford', 'astrazeneca', 'sputnik']\n\n# Function to filter the data to a single vaccine and plot the timeline\n# Note: a lot of the tweets seem to contain hashtags for multiple vaccines even though they are specifically referring to one vaccine - not very helpful!\ndef filtered_timeline(df, vax, title):\n    df = df.dropna()\n    title_str = 'Timeline showing sentiment of tweets about the '+title+' vaccine'\n    df_filt = pd.DataFrame()\n    for o in vax:\n        df_filt = df_filt.append(df[df['text'].str.lower().str.contains(o)])\n    other_vax = list(set(all_vax)-set(vax))\n    for o in other_vax:\n        df_filt = df_filt[~df_filt['text'].str.lower().str.contains(o)]\n    df_filt = df_filt.drop_duplicates()\n    timeline = df_filt.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n    fig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},title=title_str)\n    fig.show()\n    return df_filt\n\ncovaxin = filtered_timeline(covidtweets, ['covaxin'], title='Covaxin')\nsinovac = filtered_timeline(covidtweets, ['sinovac'], title='Sinovac')\nsinopharm = filtered_timeline(covidtweets, ['sinopharm'], title='Sinopharm')\nmoderna = filtered_timeline(covidtweets, ['moderna'], title='Moderna')\nsputnikv = filtered_timeline(covidtweets, ['sputnik'], title='Sputnik V')\noxford = filtered_timeline(covidtweets, ['oxford', 'astrazeneca'], title='Oxford/AstraZeneca')\npfizer = filtered_timeline(covidtweets, ['pfizer', 'biontech'], title='Pfizer/BioNTech')\n\n# Get z scores of sentiment for each vaccine\nvax_names = {'Covaxin': covaxin, 'Sputnik V': sputnikv, 'Sinovac': sinovac, 'Sinopharm': sinopharm,\n            'Moderna': moderna, 'Oxford/AstraZeneca': oxford, 'PfizerBioNTech': pfizer}\nsentiment_zscores = pd.DataFrame()\nfor k, v in vax_names.items():\n    senti = v['sentiment'].value_counts(normalize=True)\n    senti['vaccine'] = k\n    sentiment_zscores = sentiment_zscores.append(senti)\nfor col in ['Negative', 'Neutral', 'Positive']:\n    sentiment_zscores[col+'_zscore'] = (sentiment_zscores[col] - sentiment_zscores[col].mean())/sentiment_zscores[col].std(ddof=0)\nsentiment_zscores.set_index('vaccine', inplace=True)\n\n# Plot the results\nax = sentiment_zscores.sort_values('Negative_zscore')['Negative_zscore'].plot.barh(title='Z scores of negative sentiment')\nax.set_ylabel('Vaccine')\nax.set_xlabel('Z score');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to filter the data to a single date and print tweets from users with the most followers\ndef date_filter(df, date):\n    return df[df['date'].astype(str)==date].sort_values('user_followers', ascending=False)[['date' ,'text']]\n\ndef date_printer(df, dates, num=10): \n    for date in dates:\n        display(date_filter(df, date).head(num))\n\ndate_printer(moderna, ['2021-01-01', '2021-03-03'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncovidtweets[covidtweets['text'].str.lower().str.contains(\"pfizer\")].head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Word clouds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wordninja pyspellchecker\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport wordninja\nfrom spellchecker import SpellChecker\nfrom collections import Counter\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))  \nstop_words.add(\"amp\")\nimport math\nimport random","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FUNCTIONS REQUIRED\n\ndef flatten_list(l):\n    return [x for y in l for x in y]\n\ndef is_acceptable(word: str):\n    return word not in stop_words and len(word) > 2\n\n# Color coding our wordclouds \ndef red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \n\ndef green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \n\ndef yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \n\n# Reusable function to generate word clouds \ndef generate_word_clouds(neg_doc, neu_doc, pos_doc):\n    # Display the generated image:\n    fig, axes = plt.subplots(1,3, figsize=(20,10))\n    \n    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n    axes[0].set_title(\"Negative Words\")\n    axes[0].axis(\"off\")\n\n    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n    axes[1].set_title(\"Neutral Words\")\n    axes[1].axis(\"off\")\n\n    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n    axes[2].set_title(\"Positive Words\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show();\n\ndef get_top_percent_words(doc, percent):\n    # Returns a list of \"top-n\" most frequent words in a list \n    top_n = int(percent * len(set(doc)))\n    counter = Counter(doc).most_common(top_n)\n    top_n_words = [x[0] for x in counter]\n    \n    return top_n_words\n    \ndef clean_document(doc):\n    spell = SpellChecker()\n    lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize words (needed for calculating frequencies correctly )\n    doc = [lemmatizer.lemmatize(x) for x in doc]\n    \n    # Get the top 10% of all words. This may include \"misspelled\" words \n    top_n_words = get_top_percent_words(doc, 0.1)\n\n    # Get a list of misspelled words \n    misspelled = spell.unknown(doc)\n    \n    # Accept the correctly spelled words and top_n words \n    clean_words = [x for x in doc if x not in misspelled or x in top_n_words]\n    \n    # Try to split the misspelled words to generate good words (ex. \"lifeisstrange\" -> [\"life\", \"is\", \"strange\"])\n    words_to_split = [x for x in doc if x in misspelled and x not in top_n_words]\n    split_words = flatten_list([wordninja.split(x) for x in words_to_split])\n    \n    # Some splits may be nonsensical, so reject them (\"llouis\" -> ['ll', 'ou', \"is\"])\n    clean_words.extend(spell.known(split_words))\n    \n    return clean_words\n\ndef get_log_likelihood(doc1, doc2):    \n    doc1_counts = Counter(doc1)\n    doc1_freq = {\n        x: doc1_counts[x]/len(doc1)\n        for x in doc1_counts\n    }\n    \n    doc2_counts = Counter(doc2)\n    doc2_freq = {\n        x: doc2_counts[x]/len(doc2)\n        for x in doc2_counts\n    }\n    \n    doc_ratios = {\n        # 1 is added to prevent division by 0\n        x: math.log((doc1_freq[x] +1 )/(doc2_freq[x]+1))\n        for x in doc1_freq if x in doc2_freq\n    }\n    \n    top_ratios = Counter(doc_ratios).most_common()\n    top_percent = int(0.1 * len(top_ratios))\n    return top_ratios[:top_percent]\n\n# Function to generate a document based on likelihood values for words \ndef get_scaled_list(log_list):\n    counts = [int(x[1]*100000) for x in log_list]\n    words = [x[0] for x in log_list]\n    cloud = []\n    for i, word in enumerate(words):\n        cloud.extend([word]*counts[i])\n    # Shuffle to make it more \"real\"\n    random.shuffle(cloud)\n    return cloud\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert string to a list of words\ncovidtweets['words'] = covidtweets.text.apply(lambda x:re.findall(r'\\w+', x ))\n\ndef get_smart_clouds(df):\n\n    neg_doc = flatten_list(df[df['sentiment']=='Negative']['words'])\n    neg_doc = [x for x in neg_doc if is_acceptable(x)]\n\n    pos_doc = flatten_list(df[df['sentiment']=='Positive']['words'])\n    pos_doc = [x for x in pos_doc if is_acceptable(x)]\n\n    neu_doc = flatten_list(df[df['sentiment']=='Neutral']['words'])\n    neu_doc = [x for x in neu_doc if is_acceptable(x)]\n\n    # Clean all the documents\n    neg_doc_clean = clean_document(neg_doc)\n    neu_doc_clean = clean_document(neu_doc)\n    pos_doc_clean = clean_document(pos_doc)\n\n    # Combine classes B and C to compare against A (ex. \"positive\" vs \"non-positive\")\n    top_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean]))\n    top_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean]))\n    top_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean]))\n\n    # Generate syntetic a corpus using our loglikelihood values \n    neg_doc_final = get_scaled_list(top_neg_words)\n    neu_doc_final = get_scaled_list(top_neu_words)\n    pos_doc_final = get_scaled_list(top_pos_words)\n\n    # Visualise our synthetic corpus\n    generate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final)\n    \nget_smart_clouds(covidtweets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}