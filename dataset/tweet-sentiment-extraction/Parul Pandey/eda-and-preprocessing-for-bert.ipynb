{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Objective\n\nSentiment analysis is a common use case of NLP where the idea is to classify the tweet as positive, negative or neutral depending upon the text in the tweet. This problem goes a way ahead and expects us to also determine the words in the tweet which decide the polarity of the tweet.\n\n## Understanding the Evaluation Metric\n\nThe metric in this competition is the word-level **Jaccard score**. Jaccard Score is a measure of how similar/dissimilar two sets are.  The higher the score, the more similar the two strings. The idea is to find the number of common tokens and divide it by the total number of unique tokens. Its expressed in the mathematical terms by,\n\n![](https://imgur.com/lMHa8CL.png)\n\n![](https://images.deepai.org/glossary-terms/jaccard-index-391304.jpg)\n\n[Source](https://en.wikipedia.org/wiki/Jaccard_index)\n\nHere is a great example to understand the Jaccard Similarity Metric in an inutitve way.Refer to the main blog for more details:[FIVE MOST POPULAR SIMILARITY MEASURES IMPLEMENTATION IN PYTHON](https://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)\n\n![](https://i0.wp.com/dataaspirant.com/wp-content/uploads/2015/04/jaccaard2.png?resize=768%2C307&ssl=1)\n\n**Here is how one can implement the jaccard score in Python:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nSentence_1 = 'Life well spent is life good'\nSentence_2 = 'Life is an art and it is good so far'\nSentence_3 = 'Life is good'\n\n    \nprint(jaccard(Sentence_1,Sentence_2))\nprint(jaccard(Sentence_1,Sentence_3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now begin with the Exploratory data analysis. Let's quickly go over the Table of contents to get an idea what we shall be covering in this notebook:\n\n# Table of Contents\n\n- 1. Dataset and Dependencies\n   - 1.1 Importing the necessary libraries\n   - 1.2 Reading in the Dataset\n- 2. General EDA\n   - 2.1 Missing Vales\n- 3. Analysis of the Sentiment Column\n   - 3.1 An example of each sentiment\n   - 3.2 Distribution of the Sentiment column\n- 4. Text Data Preprocessing\n- 5. Text Statistics\n   - 5.1 Sentence Length\n   - 5.2 Word Count\n   - 5.3 Word Frequency\n- 6. Ngram Analysis\n- 7. Exploring the selected_text column\n- 8  Wordclouds\n- 9. Extracting the sentiment terms : Resources\n  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Datasets & Dependencies\n## 1.1 Importing the necessary libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n!pip install chart_studio\n!pip install textstat\n\nimport numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n\n# Visualisation libraries\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\n# File system manangement\nimport os\n\n# Pytorch\nimport torch\n\n#Transformers\nfrom transformers import BertTokenizer\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('../input/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Reading the datasets","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Training data\ntrain = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)\n\n# First few rows of the training dataset\ntrain.head()\n\n# First few rows of the testing dataset\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The columns denote the following:\n\n* The `textID` of a tweet\n* The `text` of a tweet\n* The `selected text` which determines the polarity of the tweet\n* `sentiment` of the tweet\n\nThe test dataset doesn't have the selected text column which needs to be identified.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. General EDA\n## 2.1 Missing Values treatment in the dataset\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in training set\ntrain.isnull().sum()\n#Missing values in test set\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `text` and `selected_text` column have one row missing each. Let's get rid of the missing rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping missing values\ntrain.dropna(axis = 0, how ='any',inplace=True) ;\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Analysis of the Sentiment Column\n\nNow, let's analyse and see how the `Sentiment` column looks like. I have only used the training dataset but the process will remain the same if we wish to to do it for the test dataset as well.\n\n## 3.1 Examples of each sentiment\nLet's look at an example of each sentiment: Positive, negative and neutral","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\",train[train['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train[train['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train[train['sentiment']=='neutral']['text'].values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Distribution of the Sentiment Column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It'll be better if we could get a relative percentage instead of the count. It is very simple with `value_counts` and can be achieved with a minor modification in the above code. Incase you want to know some similar useful value_counts tricks, be sure to check out my other notebook: [Five ways to use value_counts()](https://www.kaggle.com/parulpandey/five-ways-to-use-value-counts)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['sentiment'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a better representation. About 40 percent of the tweets are neutral followed by positive and negative tweets. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train['sentiment'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                     \n                                                      title='Distribution of Sentiment column in the training set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It would be interesting to see if the distribution is also same in the test set.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test['sentiment'].value_counts(normalize=True).iplot(kind='bar',\n                                                      yTitle='Percentage', \n                                                      linecolor='black', \n                                                      opacity=0.7,\n                                                      color='red',\n                                                      theme='pearl',\n                                                      bargap=0.6,\n                                                      gridcolor='white',\n                                                      title='Distribution  of Sentiment column in the test set')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Text Data Preprocessing\nBefore we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\n- Make text lowercase, \n- removes hyperlinks,\n- remove punctuation\n- removes numbers\n- tokenizes\n- removes stopwords","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`clean_text()` function applies a first round of text cleaning techniques.the function `text_preprocessing` then takes in the processed text from the `clean_text()` function and applies techniques like tokenization and stop word removal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ntrain['text_clean'] = train['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest['text_clean'] = test['text'].apply(str).apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Analyzing Text Statistics\n\nWe can now do some statistical analysis to explore the fundamental characteristics of the text data. Some of the analysis which can be useful are:\n\n- Text length analysis\n- word frequency analysis\n\n\nTo perform these analysis, let us create two new features i.e \n- one which calculates the length of the text, and \n- second which calculates the word count.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_len'] = train['text_clean'].astype(str).apply(len)\ntrain['text_word_count'] = train['text_clean'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create three separate dataframes for positive, neutral and negative sentiments. This will help in analyzing the text statistics separately for separate polarities.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = train[train['sentiment']=='positive']\nneg = train[train['sentiment']=='negative']\nneutral = train[train['sentiment']=='neutral']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Sentence length analysis ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pos['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Positive Text Length Distribution')\n\nneg['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Negative Text Length Distribution')\n\nneutral['text_len'].iplot(\n    kind='hist',\n    bins=100,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Neutral Text Length Distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The histogram shows that the length of the cleaned text ranges from around 2 to 140 characters and generally,it is almost same for all the polarities.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nLet's see a more consolidated comparison of the relationship of text lengths with sentiment of the text.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"trace0 = go.Box(\n    y=pos['text_len'],\n    name = 'Positive Text',\n    marker = dict(\n        color = 'red',\n    )\n)\n\ntrace1 = go.Box(\n    y=neg['text_len'],\n    name = 'Negative Text',\n    marker = dict(\n        color = 'green',\n    )\n)\n\ntrace2 = go.Box(\n    y=neutral['text_len'],\n    name = 'Neutral Text',\n    marker = dict(\n        color = 'orange',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"Length of the text\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"Length of the text of different polarities\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the text appear to have more or less same length. Hence, length of the text isn't a powerful indicator of the polarity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.2  Text word count analysis","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pos['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    color='red',\n    yTitle='count',\n    title='Positive Text word count')\n\nneg['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    color='green',\n    yTitle='count',\n    title='Negative Text word count')\nneutral['text_word_count'].iplot(\n    kind='hist',\n    bins=50,\n    xTitle='text length',\n    linecolor='black',\n    yTitle='count',\n    title='Neutral Text word count')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, more or less, word count is also similar across positive, negative and neutral texts.This will be more clear with the Box Plots below.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"trace0 = go.Box(\n    y=pos['text_word_count'],\n    name = 'Positive Text',\n    marker = dict(\n        color = 'red',\n    )\n)\n\ntrace1 = go.Box(\n    y=neg['text_word_count'],\n    name = 'Negative Text',\n    marker = dict(\n        color = 'green',\n    )\n)\n\ntrace2 = go.Box(\n    y=neutral['text_word_count'],\n    name = 'Neutral Text',\n    marker = dict(\n        color = 'orange',\n    )\n)\ndata = [trace0, trace1, trace2]\nlayout = go.Layout(\n    title = \"word count of the text\"\n)\n\nfig = go.Figure(data=data,layout=layout)\niplot(fig, filename = \"word count of the text of different polarities\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"# 6. Ngram exploration\n\nTo analyse the `text` column we will be extracting the N-Gram features.\n\nN-grams are used to describe the number of words used as observation points, e.g., unigram means singly-worded, bigram means 2-worded phrase, and trigram means 3-worded phrase. Here is a nice way to understand this:\n![](https://i.stack.imgur.com/8ARA1.png)\n\nSource: https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram\n\n\nIn order to do this, we will use scikit-learnâ€™s CountVectorizer function. The Scikit-Learn's CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.You can read more about the it [here](https://www.kaggle.com/parulpandey/getting-started-with-nlp-feature-vectors)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of top unigrams ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pos_unigrams = get_top_n_words(pos['text_clean'],20)\nneg_unigrams = get_top_n_words(neg['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\n\n\n#for word, freq in top_unigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Unigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Unigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Unigrams in neutral text',orientation='h')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Distribution of top Bigrams","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),20)\nneg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\n\n\n#for word, freq in top_bigrams:\n    #print(word, freq)\ndf1 = pd.DataFrame(pos_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Bigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Bigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_bigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Bigrams in neutral text',orientation='h')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Distribution of top Trigrams","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pos_trigrams = get_top_n_gram(pos['text_clean'],(3,3),20)\nneg_trigrams = get_top_n_gram(neg['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(pos_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black',color='red', title='Top 20 Trigrams in positve text',orientation='h')\n\ndf2 = pd.DataFrame(neg_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', color='green',title='Top 20 Trigrams in negative text',orientation='h')\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 Trigrams in neutral text',orientation='h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Exploring the `selected_text` column\n\nNow let us explore the `selected_text` column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_text = train[train['sentiment'] == 'positive']['selected_text']\nnegative_text = train[train['sentiment'] == 'negative']['selected_text']\nneutral_text = train[train['sentiment'] == 'neutral']['selected_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive text\nprint(\"Positive Text example :\",positive_text.values[0])\n#negative_text\nprint(\"Negative Tweet example :\",negative_text.values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",neutral_text.values[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processed selected text columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_text_clean = positive_text.apply(lambda x: text_preprocessing(x))\nnegative_text_clean = negative_text.apply(lambda x: text_preprocessing(x))\nneutral_text_clean = neutral_text.apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"top_words_in_positive_text = get_top_n_words(positive_text_clean)\ntop_words_in_negative_text = get_top_n_words(negative_text_clean)\ntop_words_in_neutral_text = get_top_n_words(neutral_text_clean)\n\np1 = [x[0] for x in top_words_in_positive_text[:20]]\np2 = [x[1] for x in top_words_in_positive_text[:20]]\n\n\nn1 = [x[0] for x in top_words_in_negative_text[:20]]\nn2 = [x[1] for x in top_words_in_negative_text[:20]]\n\n\nnu1 = [x[0] for x in top_words_in_neutral_text[:20]]\nnu2 = [x[1] for x in top_words_in_neutral_text[:20]]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure([go.Bar(x=p1, y=p2, text=p2 )])\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',title_text='Most common positive_text words')\n#fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n\n\n\nfig1 = go.Figure([go.Bar(x=n1, y=n2, text=n2,marker_color='indianred')])\n#fig1.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig1.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',title_text='Most common negative_text words')\n\n\n\nfig2 = go.Figure([go.Bar(x=nu1, y=nu2, text=nu2, marker_color='lightsalmon' )])\n#fig2.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig2.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',title_text='Most common neutral_text words')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Wordclouds\n\nLet's create wordclouds to see which words contribute to which type of polarity.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The wordclouds give an idea of the words which might influence the polarity of the tweet.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 9. Extracting the sentiment terms : Resources\n\nThere are a number of ways in which the problem can be approached and a lot of good solutions have been provided as notebooks which I would like to point out:\n\n## 1. Question-Answering Starter pack\n\nHere is a great solution by [Jonathan Besomi](https://www.kaggle.com/jonathanbesomi) provided in his notebook titled: [**Question-Answering Starter pack**](https://www.kaggle.com/jonathanbesomi/question-answering-starter-pack/output) wherein he formulates the task as a question answering problem. He writes\n\n*We formulate the task as question answering problem: given a question and a context, we train a transformer model to find the answer in the text column (the context).*\n\n*We have:*\n\n*Question: sentiment column (positive or negative)*\n*Context: text column*\n*Answer: selected_text column*\n\n\n## 2. Sentiment Extraction using Bert \n\n[This](https://www.kaggle.com/abhishek/roberta-inference-5-folds) is kernel by [Abhishek Thakur](https://www.kaggle.com/abhishek/text-extraction-using-bert-w-sentiment-inference) which his based on his awesome videos on youtube where he live codes the entire problem solution. Be sure to check them out. \n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%%HTML\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XaQ0CBlQ4cY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 10 : Sentiment Extraction using Bert\n\nAfter going through the above resources, I decided to use Bert for the task. The normal Question Answering tasks typically have the following format:\n\n- A Question \n- A Reference text which contains the answers\n\nHere is an example from the famous [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/Amazon_rainforest.html) dataset: ![](https://imgur.com/HqIzGO5.png)\n\n\nHence, the current problem can also be converted to a QA task wherein:\n\n- Question == Sentiment column\n- Reference Text == Text column \n- Answer == Selected Text column\n  \n**Selected text** column is a portion of the **text** column.Hence, it can be simply represented as START and END tokens.\n![](https://imgur.com/u3WkjBP.png)\n\nTherefore, our training data will consist of Text columns(concatenated Sentiment column and Text column) and labels will consist of the the start and end tokens of the selected text.\n\n\n## 10.1 : Tokenization\n\nTokenizing the sentences using Bert Tokenizer from Hugging Face ðŸ¤—","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Tokenizing the first text\nprint(train['text'][10]) # original sentence\nprint(tokenizer.tokenize(train['text'][10], add_special_tokens=True))\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train['text'][10])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10.2 Preprocessing data for Bert\n\nBERT expects input data in a specific format i.e\n- Tokenized dataset\n- All the vectors should be of the same size.Hence,we need to pad the text to achieve a common length.This length will be decided by the length of longest sentence, which we will have to calculate.Also, since we shall concatanate the `text` and the `sentiment` column, the length should be decided by the concatanated text column.\n- We need to then differentiate between the Actual tokens and the Padded ones with the help of \"Attention Masks\".\n\n*[From the Hugging face documentation](https://huggingface.co/transformers/glossary.html#attention-mask)\nThe attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them. For the BertTokenizer, 1 indicate a value that should be attended to while 0 indicate a padded value.\n*\n- Adding special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]). \n\n   ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.sep_token, tokenizer.sep_token_id\ntokenizer.cls_token, tokenizer.cls_token_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating length of the longest text\n\nmax_len = 0\n\nfor text in train['text']:\n\n    # Tokenize the text and add special tokens i.e `[CLS]` and `[SEP]`\n    input_ids = tokenizer.encode(text, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    max_len = max(max_len, len(input_ids))\n\n\nprint('Max length: ', max_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We shall take the maximum length to be 150 since we shall be concatenating text and sentiment columns. There is a very helpful function called [encode_plus](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus) provided in the Tokenizer class which can prove to be real handly. It can seamlessly perform the following operations:\n\n- Tokenize the text\n- Add special tokens - [CLS] and [SEP]\n- create token IDs\n- Pad the sentences to a common length\n- Create attention masks for the above PAD tokens","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"text = train['text'].values\nsentiment = train['sentiment'].values\n\ninput_ids = []\nattention_masks = []\ntoken_type_ids = []\n\nfor i in range(len(text)):\n    encoded = tokenizer.encode_plus(\n      sentiment[i],\n      text[i],\n      add_special_tokens=True,\n      max_length=150,\n      pad_to_max_length=True,\n      return_token_type_ids=True,\n      return_attention_mask=True,\n      return_tensors='pt'\n    )\n    \n    input_ids.append(encoded['input_ids'])\n    attention_masks.append(encoded['attention_mask'])\n    token_type_ids.append(encoded['token_type_ids'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\ntoken_type_ids = torch.cat(token_type_ids, dim=0)\n\nprint('Original text: ',text[10])\nprint(len(input_ids[10]))\nprint(input_ids[10])\nprint(attention_masks[10])\nprint(token_type_ids[10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we get are 3 tensors. Let me briefly go over them:\n\n- 1) **input_ids** : list of token ids to be fed to a model. We can see that all token ids have been padded to a length of 150.\n- 2) **attention_masks**:  list of indices specifying which tokens should be attended to by the model.The input sequences are denoted by 1 and the padded ones by 0. These masks help to differentiate between the two.\n- 3) **token_type_ids** :  list of token type ids to be fed to a model. These are also in the form of 0s and 1s and help to differentiate between the two sequences. The first sequence is the sentiment and the second is the text. Remember we had concatanated them. But why we need it? This is because the model has to differentiate between the two sequences and it will use these IDs to place a SEP token between them.\n\nThe last thing that we have to do before we begin training the model is to convert the selected text column as a combination of the START and END tokens. \n\nAfter going through a lot of resources, I think there is no point in reinventing the wheel. The following kernel from Abhishek clearly explains the whole process very well.\n\n[BERT Base Uncased using PyTorch.](https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Other Notebooks in Getting started with NLP series\n\nI have written a couple of other notebooks in getting started with NLP which some of you may find useful:\n\n* [Getting started with NLP - A general Intro](https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro)\n\n* [Getting started with NLP-Feature Vectors](https://www.kaggle.com/parulpandey/getting-started-with-nlp-feature-vectors)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}