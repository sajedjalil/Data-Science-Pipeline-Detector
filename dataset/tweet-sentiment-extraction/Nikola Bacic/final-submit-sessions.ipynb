{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# imports\n\nimport os\nimport torch\nimport random\nimport statistics\nimport tokenizers\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler, RandomSampler\nfrom transformers import BertPreTrainedModel, RobertaConfig, RobertaModel, AdamW\nfrom transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"roberta_path = \"/kaggle/input/roberta-base/\"\nmax_len = 108\nhidden_size = 768\nbatch_size = 32\nepochs = 5\nlr = 2.5e-5\ndropout_rate = 0.0\nhidden_dropout_prob = 0.1\nattention_probs_dropout_prob = 0.2\nnum_classes = 2\nn_splits = 5\nrandom_seed = 0\nwarmup_steps = 199\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file = f\"{roberta_path}vocab.json\", \n                                             merges_file = f\"{roberta_path}merges.txt\",\n                                             lowercase=False,\n                                             add_prefix_space=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# my utils\n\nchars = [\".\", \"!\", \"?\"]\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef find_sub_list(l,sl):\n    \"\"\"\n    returns all occurences of sublist sl in list l\n    i.e. their start and end position\n    e.g.[(0, 4), (5, 9)]\n    \"\"\"\n    \n    if len(sl) == 0:\n        return []\n    \n    else:\n        results=[]\n        sll=len(sl)\n        for ind in (i for i,e in enumerate(l) if e==sl[0]):\n            if l[ind:ind+sll]==sl:\n                results.append((ind,ind+sll))\n\n        return results\n    \ndef findOccurrences(s, ch):\n    occ = [i for i, letter in enumerate(s) if letter == ch]\n    return occ, len(occ)\n\ndef preprocess_train(train_df):\n    # I STEP\n    # make a copy of original text and selected_text\n    train_df[\"preprocessed_text\"] = train_df[\"text\"]\n    train_df[\"preprocessed_selected_text\"] = train_df[\"selected_text\"]\n    # add space at the begginings\n    train_df[\"preprocessed_text\"] = train_df[\"preprocessed_text\"].apply(lambda x: \" \" + x if x[0] != \" \" else x)\n    \n    # II STEP\n    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n                                              train_df.preprocessed_selected_text.values)):\n        spaces = []\n    \n        for char in chars:\n            idxs, num_idxs = findOccurrences(text, char)\n            if num_idxs > 1:\n                for pos in range(num_idxs-2, -1, -1):\n                    if idxs[pos] == idxs[pos+1]-1:\n                        text = text[:idxs[pos]+1] + \" \" + text[idxs[pos]+1:]\n                        spaces.append(char)\n        if any(spaces):\n            train_df.loc[i, \"preprocessed_text\"] = text\n    \n        for char in chars:\n            idxs, num_idxs = findOccurrences(selected_text, char)\n            if num_idxs > 1:\n                for pos in range(num_idxs-2, -1, -1):\n                    if idxs[pos] == idxs[pos+1]-1:\n                        selected_text = selected_text[:idxs[pos]+1] + \" \" + selected_text[idxs[pos]+1:]\n    \n        train_df.loc[i, \"preprocessed_selected_text\"] = selected_text\n    \n    # III STEP\n    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n                                              train_df.preprocessed_selected_text.values)):\n       \n        text_tokenized = tokenizer.encode(text).tokens\n        selected_tokenized = tokenizer.encode(selected_text).tokens\n        subi_spl = find_sub_list(text_tokenized, selected_tokenized)\n    \n        selected_tokenized1 = tokenizer.encode(\" \" + selected_text).tokens\n        subi_spl1 = find_sub_list(text_tokenized, selected_tokenized1)\n        \n        if len(subi_spl) == 0 and len(subi_spl1) > 0:\n            train_df.loc[i, \"preprocessed_selected_text\"] = \" \" + selected_text\n        \n    # IV STEP\n    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n                                              train_df.preprocessed_selected_text.values)):\n       \n        text_tokenized = tokenizer.encode(text).tokens\n        selected_tokenized = tokenizer.encode(selected_text).tokens\n        subi_spl = find_sub_list(text_tokenized, selected_tokenized)\n    \n        if len(subi_spl) == 0:\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            for s in range(start-1, -1, -1):\n                if text[s] != \" \":\n                    selected_text = text[s] + selected_text\n                else:\n                    selected_text = \" \" + selected_text\n                    break\n            for s in range(end, len(text)):\n                if text[s] != \" \":\n                    selected_text = selected_text + text[s] \n                else:\n                    break\n        train_df.loc[i, \"preprocessed_selected_text\"] = selected_text\n        \n    \n    # V STEP\n    for i, (text, selected_text) in enumerate(zip(train_df.preprocessed_text.values, \n                                              train_df.preprocessed_selected_text.values)):\n       \n        text_tokenized = tokenizer.encode(text).tokens\n        selected_tokenized = tokenizer.encode(selected_text).tokens\n        subi_spl = find_sub_list(text_tokenized, selected_tokenized)\n        \n        if len(subi_spl) == 0:\n            train_df.drop(i, inplace=True)\n\n    train_df.reset_index(drop=True, inplace=True)\n\ndef preprocess_test(test_df):\n    # I STEP\n    # make a copy of original text and selected_text\n    test_df[\"preprocessed_text\"] = test_df[\"text\"]\n    # add space at the begginings\n    test_df[\"preprocessed_text\"] = test_df[\"preprocessed_text\"].apply(lambda x: \" \" + x if x[0] != \" \" else x)   \n    \n    # II STEP\n    for i, text in enumerate(test_df.preprocessed_text.values):\n        spaces = []\n        for char in chars:\n            idxs, num_idxs = findOccurrences(text, char)\n            if num_idxs > 1:\n                for pos in range(num_idxs-2, -1, -1):\n                    if idxs[pos] == idxs[pos+1]-1:\n                        text = text[:idxs[pos]+1] + \" \" + text[idxs[pos]+1:]\n                        spaces.append(char)\n        if any(spaces):\n            test_df.loc[i, \"preprocessed_text\"] = text    \n    \ndef get_labels(tokens, pr_selected_text):\n    \n    start_labels = [0] * len(tokens)\n    end_labels = [0] * len(tokens)\n\n    selected_tokens = tokenizer.encode(pr_selected_text).tokens\n    subi = find_sub_list(tokens, selected_tokens)\n    assert len(subi) > 0, \"Something is wrong!!!\"\n    \n    start, end = subi[0]\n    start_labels[start] = 1\n    end_labels[end-1] = 1\n    \n    start_labels = [0] * 4 + start_labels + [0]\n    end_labels = [0] * 4 + end_labels + [0]\n\n    return start_labels, end_labels\n\n\ndef process_sample(text, pr_text, sentiment, max_len, tokenizer, selected_text=None, pr_selected_text=None):\n    \"\"\"\n    gets all info for one sample\n    \"\"\"\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    info = tokenizer.encode(pr_text)\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + info.ids + [2]    \n    attention_mask = [1] * len(input_ids)\n    tokens = info.tokens\n    offsets = info.offsets\n    \n    loss_mask = [0] * 4 + [1] * len(tokens) + [0]\n                \n    if selected_text is not None:\n        start_labels, end_labels = get_labels(tokens, pr_selected_text)\n    else: start_labels, end_labels = None, None\n        \n    pad_len = max_len - len(input_ids)\n    if pad_len > 0:\n        input_ids = input_ids + ([1] * pad_len)\n        attention_mask = attention_mask + ([0] * pad_len)\n        loss_mask = loss_mask + ([0] * pad_len)\n        \n        if selected_text is not None:\n            start_labels = start_labels + ([0] * pad_len)\n            end_labels = end_labels + ([0] * pad_len)\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"loss_mask\": loss_mask,\n        \"tokens\": tokens,\n        \"start_labels\": start_labels,\n        \"end_labels\": end_labels,\n        \"offsets\": offsets,\n        \"text\": text,\n        \"preprocessed_text\": pr_text,\n        \"selected_text\": selected_text,\n        \"preprocessed_selected_text\": pr_selected_text,\n        \"sentiment\": sentiment\n    }\n\ndef tweet_loss(start_logits, start_labels, end_logits, end_labels, loss_mask):\n\n    loss_fn = nn.BCEWithLogitsLoss()\n\n    # mask actives\n    actives = loss_mask.reshape(-1) == 1\n    # apply the masks\n    start_logits = start_logits.reshape(-1)[actives]\n    start_labels = start_labels.reshape(-1)[actives]\n    end_logits = end_logits.reshape(-1)[actives]\n    end_labels = end_labels.reshape(-1)[actives]\n        \n    start_loss =  loss_fn(start_logits, start_labels.type_as(start_logits))\n    end_loss =  loss_fn(end_logits, end_labels.type_as(end_logits))    \n    \n    return (start_loss + end_loss) / 2\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nclass TweetDataset:\n    \n    max_len = max_len\n    tokenizer = tokenizer\n    \n    def __init__(self, df, sentiment_values=None):\n        if sentiment_values is not None:\n            df = df.loc[df.sentiment.isin(sentiment_values)].reset_index(drop=True)\n        self.df = df\n        self.indexs = df.index.values\n        self.text = df.text.values\n        self.preprocessed_text = df.preprocessed_text.values\n        self.sentiments = df.sentiment.values\n        if \"selected_text\" in df:\n            self.selected_text = df.selected_text.values\n            self.preprocessed_selected_text = df.preprocessed_selected_text.values\n        else:\n            self.selected_text = None\n            self.preprocessed_selected_text = None\n        \n    def __len__(self):\n        return len(self.indexs)\n        \n    def __getitem__(self, idx):\n        if self.selected_text is not None:\n            return process_sample(self.text[idx], self.preprocessed_text[idx], self.sentiments[idx], self.max_len,\n                                self.tokenizer, self.selected_text[idx], self.preprocessed_selected_text[idx])\n        else:\n            return process_sample(self.text[idx], self.preprocessed_text[idx], self.sentiments[idx], self.max_len,\n                                self.tokenizer)\n    \n    def get_metadata(self):\n        \n        return [self[i] for i in range(len(self))]\n    \n    def get_train_dataloader(self, idxs):\n        \n        input_ids_all = []\n        attention_mask_all = []\n        loss_mask_all = []\n        start_labels_all = []\n        end_labels_all = []\n        \n        for i in idxs:\n            \n            sample = self[i]\n        \n            input_ids_all.append(sample[\"input_ids\"])\n            attention_mask_all.append(sample[\"attention_mask\"])\n            loss_mask_all.append(sample[\"loss_mask\"])\n            start_labels_all.append(sample[\"start_labels\"])\n            end_labels_all.append(sample[\"end_labels\"])\n\n        data = TensorDataset(torch.LongTensor(input_ids_all),\n                             torch.FloatTensor(attention_mask_all),\n                             torch.LongTensor(loss_mask_all),\n                             torch.LongTensor(start_labels_all),\n                             torch.LongTensor(end_labels_all)\n                                    )\n        \n        sampler = SequentialSampler(data)\n        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size) \n\n        return dataloader\n    \n    def get_test_dataloader(self):\n        \n        input_ids_all = []\n        attention_mask_all = []\n        loss_mask_all = []\n        \n        for i in self.indexs:\n            \n            sample = self[i]\n        \n            input_ids_all.append(sample[\"input_ids\"])\n            attention_mask_all.append(sample[\"attention_mask\"])\n            loss_mask_all.append(sample[\"loss_mask\"])\n\n        data = TensorDataset(torch.LongTensor(input_ids_all),\n                             torch.FloatTensor(attention_mask_all),\n                             torch.LongTensor(loss_mask_all)\n                                  )\n        \n        sampler = SequentialSampler(data)\n        dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size) \n\n        return dataloader\n    \nclass TweetModel(BertPreTrainedModel):\n    \n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = RobertaModel.from_pretrained(roberta_path, config=conf)\n        self.drop_out = nn.Dropout(dropout_rate)\n        self.clf = nn.Linear(hidden_size, num_classes)\n        nn.init.normal_(self.clf.weight, std=0.02)\n        \n    def forward(self, input_ids, attention_mask):\n        \n        sequence_outputs, _ = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n                \n        sequence_outputs = self.drop_out(sequence_outputs)\n        logits = self.clf(sequence_outputs)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        \n        return start_logits, end_logits\n\ndef prepare_predictions(pp):\n    for char in chars:\n        idxs, num_idxs = findOccurrences(pp, char)\n        if num_idxs > 1:\n            for pos in range(num_idxs-2, -1, -1):\n                if idxs[pos] == idxs[pos+1] - 2 :\n                    pp = pp[:idxs[pos]+1] + pp[idxs[pos+1]:]\n             \n    return pp\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(random_seed)\n\ntrain_path = \"/kaggle/input/tweet-sentiment-extraction/train.csv\"\ntest_path = \"/kaggle/input/tweet-sentiment-extraction/test.csv\"\n\ntrain_raw = pd.read_csv(train_path).dropna().reset_index(drop=True)\ntest_raw = pd.read_csv(test_path)\n\nprint(f\"train data: {train_raw.shape}\")\nprint(f\"test data: {test_raw.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get only negative and positive sentiment samples\ntrain_df = train_raw.loc[train_raw.sentiment != \"neutral\"].reset_index(drop=True, inplace=False)\ntest_df = test_raw.loc[test_raw.sentiment != \"neutral\"].reset_index(drop=True, inplace=False)\nprint(f\"non neutral train data: {train_df.shape}\")\nprint(f\"non neutral test data: {test_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npreprocess_train(train_df)\npreprocess_test(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = TweetDataset(train_df)\ntest_dataset = TweetDataset(test_df)\n\ntrain_metadata = train_dataset.get_metadata()\ntest_metadata = test_dataset.get_metadata()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_loop():\n    \n    # create splits\n    kf = StratifiedKFold(n_splits=n_splits, random_state=random_seed, shuffle=True)\n    \n    # lists to store oof predictions\n    val_start_logits = [0 for tm in train_metadata]\n    val_end_logits = [0 for tm in train_metadata]\n    \n    # lists to accumulate folds test set predictions\n    test_start_logits = [torch.zeros(len(tm[\"input_ids\"]), dtype=torch.float) for tm in test_metadata]\n    test_end_logits = [torch.zeros(len(tm[\"input_ids\"]), dtype=torch.float) for tm in test_metadata]\n    \n    test_dataloader = test_dataset.get_test_dataloader()\n    for fold_idx, (train_idxs, val_idxs) in enumerate(kf.split(X=train_dataset.df, \n                                                               y=train_dataset.df.sentiment.values)):\n                        \n        # create dataloaders\n        train_dataloader = train_dataset.get_train_dataloader(train_idxs)\n        val_dataloader = train_dataset.get_train_dataloader(val_idxs)\n        \n        # model init\n        model_config = RobertaConfig.from_pretrained(roberta_path, lowercase=True)\n        model_config.hidden_dropout_prob = hidden_dropout_prob\n        model_config.attention_probs_dropout_prob = attention_probs_dropout_prob\n        model = TweetModel(conf=model_config)\n        model.cuda()\n        \n        # optimizer init\n        optimizer = AdamW(model.parameters(), lr=lr)\n        num_train_steps = epochs * len(train_dataloader)\n        # create the learning rate scheduler\n        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, \n                                            num_train_steps)\n        \n        for epoch in range(epochs):\n            \n            model.train()\n            \n            for batch in train_dataloader:\n                \n                # add batch to GPU\n                batch = tuple(t.to(device) for t in batch) \n                \n                input_ids, attention_mask, loss_mask, start_labels, end_labels = batch\n                                \n                start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n                \n                # calculate loss\n                loss = tweet_loss(start_logits, start_labels, end_logits, end_labels, loss_mask)\n                \n                # calculate gradients\n                loss.backward()               \n                # update model param\n                optimizer.step()\n                # update lr\n                scheduler.step()\n                # clean the gradients\n                model.zero_grad()\n            \n        # save validation and test logits\n        model.eval()\n        \n        val_fold_start_logits = []\n        val_fold_end_logits = []\n        \n        for batch in val_dataloader:\n            # add batch to GPU\n            batch = tuple(t.to(device) for t in batch) \n                \n            input_ids, attention_mask, loss_mask, start_labels, end_labels = batch\n    \n            with torch.no_grad():\n                start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n            batch_start_logits = [start_logits[s].cpu() for s in range(input_ids.shape[0])]\n            batch_end_logits = [end_logits[s].cpu() for s in range(input_ids.shape[0])]\n    \n            val_fold_start_logits += batch_start_logits\n            val_fold_end_logits += batch_end_logits\n        \n        for e, idx in enumerate(val_idxs):\n            val_start_logits[idx] = val_fold_start_logits[e]\n            val_end_logits[idx] = val_fold_end_logits[e]\n        \n        test_fold_start_logits = []\n        test_fold_end_logits = []\n        \n        for batch in test_dataloader:\n            # add batch to GPU\n            batch = tuple(t.to(device) for t in batch) \n                \n            input_ids, attention_mask, loss_mask = batch\n    \n            with torch.no_grad():\n                start_logits, end_logits = model(input_ids=input_ids, attention_mask=attention_mask)\n        \n            batch_start_logits = [start_logits[s].cpu() for s in range(input_ids.shape[0])]\n            batch_end_logits = [end_logits[s].cpu() for s in range(input_ids.shape[0])]\n    \n            test_fold_start_logits += batch_start_logits\n            test_fold_end_logits += batch_end_logits\n        \n        for e in range(len(test_dataset)):\n            test_start_logits[e] += test_fold_start_logits[e].squeeze(1)\n            test_end_logits[e] += test_fold_end_logits[e].squeeze(1)\n            \n        print(\"one fold over!\")\n           \n    return val_start_logits, val_end_logits, test_start_logits, test_end_logits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# set the device to GPU\ndevice = torch.device(\"cuda\")\n\nval_start_logits, val_end_logits, test_start_logits, test_end_logits = train_loop()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OOF EVALUATION\n\njaccs = []\npredictions = []\nbr = 0    \nfor e, meta in enumerate(train_metadata):\n    \n    text = meta[\"text\"]\n    preprocessed_text = meta[\"preprocessed_text\"]\n    selected_text = meta[\"selected_text\"]\n    loss_mask = meta[\"loss_mask\"]\n    tokens = meta[\"tokens\"]\n    offsets = meta[\"offsets\"]\n    \n    actives = np.asarray(meta[\"loss_mask\"]).reshape(-1) == 1\n    start_probs = val_start_logits[e][actives].sigmoid()\n    end_probs = val_end_logits[e][actives].sigmoid()\n        \n    start_idx = start_probs.argmax().item()\n    end_idx = end_probs.argmax().item()\n    \n    if start_idx > end_idx:\n        br += 1\n        start_idx , end_idx = 0, start_probs.shape[0]-1\n    \n    start_max = start_probs.max().item()\n    end_max = end_probs.max().item()        \n\n    pp = preprocessed_text[offsets[start_idx][0]:offsets[end_idx][1]]\n    \n    if pp not in text or pp not in \" \" + text:\n        pp = prepare_predictions(pp)\n    \n    predictions.append(pp)\n    jaccs.append(jaccard(selected_text, pp))\n\nprint(len(jaccs))\nprint(round(statistics.mean(jaccs),4), round(statistics.mean(jaccs)*0.5955+0.9767*0.4045,4))\nprint(br) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### format test set predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_raw[\"selected_text\"] = \"\" # make a column for predictions\n\n# make predictions for neutral sentiment\ntest_raw.loc[test_raw.sentiment == \"neutral\", \"selected_text\"] = test_raw.loc[test_raw.sentiment == \"neutral\", \"text\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get positive and negative sentiment predictions\n\npredictions = []\nbr = 0    \nfor e, meta in enumerate(test_metadata):\n    \n    text = meta[\"text\"]\n    preprocessed_text = meta[\"preprocessed_text\"]\n    loss_mask = meta[\"loss_mask\"]\n    tokens = meta[\"tokens\"]\n    offsets = meta[\"offsets\"]\n    \n    actives = np.asarray(meta[\"loss_mask\"]).reshape(-1) == 1\n    start_probs = test_start_logits[e][actives].sigmoid()\n    end_probs = test_end_logits[e][actives].sigmoid()\n        \n    start_idx = start_probs.argmax().item()\n    end_idx = end_probs.argmax().item()\n    \n    if start_idx > end_idx:\n        br += 1\n        start_idx , end_idx = 0, start_probs.shape[0]-1       \n\n    pp = preprocessed_text[offsets[start_idx][0]:offsets[end_idx][1]]\n    \n    if pp not in text or pp not in \" \" + text:\n        pp = prepare_predictions(pp)\n    \n    predictions.append(pp)\n\nprint(br)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions for neutral sentiment\ntest_raw.loc[test_raw.sentiment != \"neutral\", \"selected_text\"] = predictions\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test_raw.drop(columns = [\"text\", \"sentiment\"])\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}