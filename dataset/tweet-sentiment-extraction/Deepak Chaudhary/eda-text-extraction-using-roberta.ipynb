{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 0. Importing Libraries and Loading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport math\nimport scipy.stats\nimport string\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\nimport plotly.graph_objs as go\nfrom collections import Counter\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest_data  = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\nx = datetime.datetime.now()\nprint(x)\n\nminute_now = int(x.strftime(\"%M\"))\n\nprint(minute_now)\n\nif(minute_now<44):\n    test_data.to_csv('submission.csv',index=False)\n    train_data = []\n    test_data = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Analysis Follows","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Class Distribution and Null values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.groupby(train_data['sentiment']).size())\nclass_label = train_data.sentiment.value_counts()\nsns.barplot(class_label.index, class_label)\nplt.gca().set_ylabel('samples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Classes are almost balanced. No need to do any upsampling or downsampling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking for the number of nan values in text and selected text column\n\nprint(\"No of nan values in text column = \",train_data['text'].isna().sum())\nprint(\"No of nan values in selected_text column = \",train_data['selected_text'].isna().sum())\n\nprint(\"\\nId of null text column = \", train_data[train_data['text'].isna()]['textID'])\nprint(\"Id of null selected_text column = \", train_data[train_data['selected_text'].isna()]['textID'])\n\ntrain_data = train_data.fillna(\"\")\ntest_data = test_data.fillna(\"\")\n\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####           We can see that there is only one null value in both text and selected_text column and ids of both are same. So we simply put empty string into that","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.2 'text' Feature Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysis of length\n\n\nprint(\"Length of shortest text = \", min(train_data['text'].str.len()))\nprint(\"Length of Longest text = \", max(train_data['text'].str.len()))\n\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(16,8))\n\n#For Positive Review\nsns.distplot(train_data[train_data['sentiment']=='positive']['text'].str.len(), hist=True, kde=True,\n             bins=int(200/25), color = 'darkblue', \n             ax = ax1,\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Text_Length')\nax1.set_ylabel('Density')\n\n#For Negative Review\nsns.distplot(train_data[train_data['sentiment']=='negative']['text'].str.len(), hist=True, kde=True,\n             bins=int(200/25), color = 'red', \n             ax = ax2,\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Text_Length')\nax2.set_ylabel('Density')\n\n#For Neutral Review\nsns.distplot(train_data[train_data['sentiment']=='neutral']['text'].str.len(), hist=True, kde=True,\n             bins=int(200/25), color = 'brown', \n             ax = ax3,\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax3.set_title('Neutral Reviews')\nax3.set_xlabel('Text_Length')\nax3.set_ylabel('Density')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### By looking into the above KDE we can say that the distributions of 'Text Length' of positive, negative and neutral text are almost same.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Central Tendency Analysis\n\n\n#For Positive Review\nmn = np.mean(train_data[train_data['sentiment']=='positive']['text'].str.len())\nmd = np.median(train_data[train_data['sentiment']=='positive']['text'].str.len())\nprint('Mean length of positive review is ', mn)\nprint('Median length of positive review is ', md)\n\n#For Negative Review\nmn = np.mean(train_data[train_data['sentiment']=='negative']['text'].str.len())\nmd = np.median(train_data[train_data['sentiment']=='negative']['text'].str.len())\nprint('\\nMean length of Negative review is ', mn)\nprint('Median length of Negative review is ', md)\n\n#For Neutral Review\nmn = np.mean(train_data[train_data['sentiment']=='neutral']['text'].str.len())\nmd = np.nanmedian(train_data[train_data['sentiment']=='neutral']['text'].str.len())\nprint('\\nMean length of Neutral review is ', mn)\nprint('Median length of Neutral review is ', md)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stopwords analysis of text feature\n\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\narr = []\n\nfor i in train_data['text']:\n    words = i.split()\n    cnt = 0\n    for i in words:\n        if(i in stopwords):\n            cnt+=1\n    if(cnt!=0):\n        k = round((float(cnt)/len(words))*100.0, 0)\n    else:\n        k = 0\n    arr.append(k)\n    \nprint(\"There are on average {}% stopwords in one text feature\\n\".format(round(np.mean(arr), 0)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud Analysis Helper functions\n\ndef create_corpus(target, feature):\n    corpus=[]\n    \n    for x in train_data[train_data['sentiment']==target][feature].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\ndef plot_wordcloud(corpus):\n    plt.figure(figsize=(12,8))\n    word_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus[:50]))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud for positive sentiment\nplot_wordcloud(create_corpus(\"positive\",\"text\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Really, feedings, smiles, wow, baby are some of the words that occurs frequently in text feature with 'positive' sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud for negative sentiment\nplot_wordcloud(create_corpus(\"negative\",\"text\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### will,sad, miss, boss, bullying are some of the words that occur very frequently in text feature for negative sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud for neutral sentiment\nplot_wordcloud(create_corpus(\"neutral\",\"text\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### test responded high going, plugging are some of the words that occur very frequently in text feature for neutral sentiment ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.3 'selected_text' Feature Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analysis of length of selected_text feature\n\nprint(\"Length of shortest selected_text = \", min(train_data['selected_text'].str.len()))\nprint(\"Length of Longest selected_text = \", max(train_data['selected_text'].str.len()))\n\n\nfig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(16,8))\n\n#For Positive Review\nsns.distplot(train_data[train_data['sentiment']=='positive']['selected_text'].str.len(), hist=True, kde=True,\n             bins=int(200/25), color = 'darkblue', \n             ax = ax1,\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax1.set_title('Positive Reviews')\nax1.set_xlabel('Text Length')\nax1.set_ylabel('Density')\n\n#For Negative Review\nsns.distplot(train_data[train_data['sentiment']=='negative']['selected_text'].str.len(), hist=True, kde=True,\n             bins=int(200/25), color = 'red', \n             ax = ax2,\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax2.set_title('Negative Reviews')\nax2.set_xlabel('Text Length')\nax2.set_ylabel('Density')\n\n#For Neutral Review\nsns.distplot(train_data[train_data['sentiment']=='neutral']['selected_text'].str.len(), hist=True, kde=True,\n             bins=int(200/25), color = 'brown', \n             ax = ax3,\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax3.set_title('Neutral Reviews')\nax3.set_xlabel('Text Length')\nax3.set_ylabel('Density')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Distribution of selected_text of positive and negative reviews are almost same but the distribution of neutral review is very different from the two. Interesting fact to notice is that the distribution of selected_text and text column of neutral review are almost same","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stopwords analysis\n\narr = []\n\nfor i in train_data['selected_text']:\n    words = i.split()\n    cnt = 0\n    for i in words:\n        if(i in stopwords):\n            cnt+=1\n    if(cnt!=0):\n        k = round((float(cnt)/len(words))*100.0, 0)\n    else:\n        k = 0\n    arr.append(k)\n    \nprint(\"There are on average {}% stopwords in one selected_text\\n\".format(round(np.mean(arr), 0)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since there are in average 22% stopwords in a selected_text, so if we remove them, then we will misclassify almost 22%  words. So we will not remove them while doing preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Central Tendency of selected_text\n\n#For Positive Review\nmn = np.mean(train_data[train_data['sentiment']=='positive']['selected_text'].str.len())\nmd = np.median(train_data[train_data['sentiment']=='positive']['selected_text'].str.len())\nprint('Mean length of selected_text in positive review is ', mn)\nprint('Median length of selected_text in positive review is ', md)\n\n#For Negative Review\nmn = np.mean(train_data[train_data['sentiment']=='negative']['selected_text'].str.len())\nmd = np.median(train_data[train_data['sentiment']=='negative']['selected_text'].str.len())\nprint('\\nMean length of selected_text in Negative review is ', mn)\nprint('Median length of selected_text in Negative review is ', md)\n\n#For Neutral Review\nmn = np.mean(train_data[train_data['sentiment']=='neutral']['selected_text'].str.len())\nmd = np.nanmedian(train_data[train_data['sentiment']=='neutral']['selected_text'].str.len())\nprint('\\nMean length of selected_text in Neutral review is ', mn)\nprint('Median length of selected_text in Neutral review is ', md)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Mean length of neutral review is somewhat strange. It looks like in case of Neutral Reviews we are just copying almost whole text and putting that into selected_text column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word Cloud of selected_text analysis\n# Generating the wordcloud for positive sentiment\nplot_wordcloud(create_corpus(\"positive\",\"selected_text\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Happy, funny, fun, wow, intersting are some of the words that occur very frequently in selected_text for positive sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud for positive sentiment\nplot_wordcloud(create_corpus(\"negative\",\"selected_text\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Leave, bullying , sad, alone are some of the most frequent words that occurs frequently in selected_text for negative sentiment","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the wordcloud for positive sentiment\nplot_wordcloud(create_corpus(\"neutral\",\"selected_text\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### responded, test, smf, going, plugging, shameless are some of the word that occurs very frequently for neutral reviews for selected_text feature","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.4 Analysis of relation between 'text' and 'selected_text' feature\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# How much selected_text and text feature matches\n\nx = []\ny = []\n\nsame_cnt = 0\narr = []\nsentences = []\nlength = train_data.shape[0]\n\nfor index, row in train_data.iterrows():\n    first = row['text'].split()\n    d = {}\n    for j in first:\n        if(d.get(j)):\n            d[j] = d[j]+1\n        else:\n            d[j] = 1\n\n    cnt = 0\n    scd = row['selected_text'].split()\n    for j in scd:\n        if(d.get(j)!=None and d[j]>0):\n            cnt+=1\n            d[j]-=1;\n    if(cnt==len(scd)):\n        same_cnt+=1\n    else:\n        arr.append(round((cnt/float(len(scd)))*100.0,0))\n        sentences.append([row['text'], row['selected_text']])\n    x.append(len(scd))\n    y.append(cnt)\n\nss = round((same_cnt/float(train_data.shape[0]))*100.0,0)\nmn = round(np.mean(arr), 0)\nprint(\"Almost {0}% of selected_text are strict subset of text \".format(ss))\nprint(\"\\nOut of {0}% remaining selected_text there is almost {1}% average match of selected_text and text column\".format(100-ss, mn))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most of the selected_text are strict subset of text (almost 89%) and the remaining selected_text has 57% on average match","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Jaccard simlarity analysis\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length = train_data.shape[0]\npos_score = []\nneg_score = []\nneu_score = []\n\nfor index, row in train_data.iterrows():\n    sent =row['sentiment']\n    \n    if(sent == 'positive'):\n        pos_score.append(jaccard(row['text'], row['selected_text']))\n    if(sent == 'negative'):\n        neg_score.append(jaccard(row['text'], row['selected_text']))\n    if(sent == 'neutral'):\n        neu_score.append(jaccard(row['text'], row['selected_text']))\n\nprint(\"Mean jaccard score between text and selected_text for positive sentiment is {0}\".format(np.mean(pos_score)))\nprint(\"Mean jaccard score between text and selected_text for negative sentiment is {0}\".format(np.mean(neg_score)))\nprint(\"Mean jaccard score between text and selected_text for neutral sentiment is {0}\".format(np.mean(neu_score)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to Raenish David for this excellent code of the plot\n# Jaccard score for text vs selected_text for positive sentiment \n\nx = train_data[train_data['sentiment']=='positive']['text'].str.len()\ny = pos_score\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'green',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Postive Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Jaccard score for text vs selected_text for negative sentiment \n\nx = train_data[train_data['sentiment']=='negative']['text'].str.len()\ny = neg_score\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'red',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Negative Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Jaccard score for text vs selected_text for neutral sentiment \n\nx = train_data[train_data['sentiment']=='neutral']['text'].str.len()\ny = neu_score\nfig = go.Figure()\nfig.add_trace(go.Histogram2dContour(\n        x = x,\n        y = y,\n        colorscale = 'gray',\n        reversescale = True,\n        xaxis = 'x',\n        yaxis = 'y'\n    ))\nfig.add_trace(go.Scatter(\n        x = x,\n        y = y,\n        xaxis = 'x',\n        yaxis = 'y',\n        mode = 'markers',\n        marker = dict(\n            color = 'blue',  #'rgba(0,0,0,0.3)',\n            size = 3\n        )\n    ))\nfig.add_trace(go.Histogram(\n        y = y,\n        xaxis = 'x2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\nfig.add_trace(go.Histogram(\n        x = x,\n        yaxis = 'y2',\n        marker = dict(\n            color = 'rgba(0,0,0,1)'\n        )\n    ))\n\nfig.update_layout(\n    autosize = False,\n    xaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    yaxis = dict(\n        zeroline = False,\n        domain = [0,0.85],\n        showgrid = False\n    ),\n    xaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    yaxis2 = dict(\n        zeroline = False,\n        domain = [0.85,1],\n        showgrid = False\n    ),\n    height = 600,\n    width = 600,\n    bargap = 0,\n    hovermode = 'closest',\n    showlegend = False,\n    title_text=\"Neutral Jaccard - Text vs Selected Text \",title_x=0.5\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion : For neutral sentiment we have a very high jaccard score. For positive and negative sentiment we have low jaccard score having length 10-20. For texts having a short length we have a very good jaccard score for both negative and positive sentiment","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_data\ntest  = test_data\nMAX_LEN = 96\nPATH = '../input/tf-roberta/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 3 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.ReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.ReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]/skf.n_splits\n    preds_end += preds[1]/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(jac) # Jaccard CVs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}