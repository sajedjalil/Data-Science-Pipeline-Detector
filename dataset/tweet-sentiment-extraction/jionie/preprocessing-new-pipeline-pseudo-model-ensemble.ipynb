{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nfrom transformers import *\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\nfrom functools import partial\nfrom multiprocessing import Pool, cpu_count\nimport tokenizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define config","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Config:\n    # config settings\n    def __init__(self):\n        # dataset setting\n        self.max_seq_length = 192\n        # dataloader settings\n        self.val_batch_size = 256\n        self.num_workers = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Config = Config()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess test data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(text):\n\n    text = text.replace(\"....\", \". . . .\")\n    text = text.replace(\"...\", \". . .\")\n    text = text.replace(\"..\", \". .\")\n    text = text.replace(\"!!!!\", \"! ! ! !\")\n    text = text.replace(\"!!!\", \"! ! !\")\n    text = text.replace(\"!!\", \"! !\")\n    text = text.replace(\"????\", \"? ? ? ?\")\n    text = text.replace(\"???\", \"? ? ?\")\n    text = text.replace(\"??\", \"? ?\")\n\n    return text\n\n\ndef process_data(tweet, selected_text, old_selected_text, sentiment, tokenizer, model_type, max_len, augment=False):\n\n    tweet_with_extra_space = preprocessing(copy.deepcopy(str(tweet).lower()))\n    tweet = preprocessing(\" \" + \" \".join(str(tweet).lower().split()))\n    selected_text = preprocessing(\" \" + \" \".join(str(selected_text).lower().split()))\n    old_selected_text = \" \" + \" \".join(str(old_selected_text).lower().split())\n\n    # remove first \" \"\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    # get char idx\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind + len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    if idx0 is None and idx1 is None:\n        print(\"--------------------------------------------- error cleaned selected----------------------------------\")\n        print(\"tweet:\", tweet)\n        print(\"selected_text:\", selected_text)\n        print(\"old_selected_text:\", old_selected_text)\n        print(\"--------------------------------------------- error cleaned selected----------------------------------\")\n\n        for ind in (i for i, e in enumerate(tweet) if e == old_selected_text[1]):\n            if \" \" + tweet[ind: ind + len_st] == old_selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n                \n    # get char mask\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n\n    # get word offsets\n    tweet_offsets_word_level = []\n    tweet_offsets_token_level = []\n    cursor = 0\n    input_ids_orig = []\n\n    for i, word in enumerate(tweet.split()):\n\n        sub_words = tokenizer.tokenize(\" \" + word)\n        encoded_word = tokenizer.convert_tokens_to_ids(sub_words)\n        number_of_tokens = len(encoded_word)\n        input_ids_orig += encoded_word\n\n        start_offsets = cursor\n\n        token_level_cursor = start_offsets\n\n        for i in range(number_of_tokens):\n\n            if (model_type == \"bert-base-uncased\") or (model_type == \"bert-large-uncased\") \\\n                    or (model_type == \"bert-base-cased\") or (model_type == \"bert-large-cased\"):\n\n                # for bert tokenizer, replace \"##\" and add \" \" for first sub_word\n                sub_word_len = len(sub_words[i].replace(\"##\", \"\"))\n                if i == 0:\n                    sub_word_len += 1\n            else:\n                sub_word_len = len(sub_words[i])\n\n            tweet_offsets_token_level.append((token_level_cursor, token_level_cursor + sub_word_len))\n            cursor = token_level_cursor + sub_word_len\n            token_level_cursor += sub_word_len\n\n        end_offsets = cursor\n\n        for i in range(number_of_tokens):\n            tweet_offsets_word_level.append((start_offsets, end_offsets))\n\n    # get word idx\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets_token_level):\n\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n\n    if len(target_idx) == 0:\n        print(tweet, selected_text)\n\n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    # print(tweet[tweet_offsets_token_level[targets_start][0]: tweet_offsets_token_level[targets_end][1]],\n    #       \"------------\", selected_text)\n\n\n    if model_type == \"roberta-base\" or model_type == \"roberta-large\" or model_type == \"roberta-base-squad\":\n\n        sentiment_id = {\n            'positive': 1313,\n            'negative': 2430,\n            'neutral': 7974\n        }\n\n        input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n        token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets_token_level = [(0, 0)] * 4 + tweet_offsets_token_level + [(0, 0)]\n        tweet_offsets_word_level = [(0, 0)] * 4 + tweet_offsets_word_level + [(0, 0)]\n        targets_start += 4\n        targets_end += 4\n\n    elif (model_type == \"albert-base-v2\") or (model_type == \"albert-large-v2\") or (model_type == \"albert-xlarge-v2\"):\n\n        sentiment_id = {\n            'positive': 2221,\n            'negative': 3682,\n            'neutral': 8387\n        }\n\n        input_ids = [2] + [sentiment_id[sentiment]] + [3] + input_ids_orig + [3]\n        token_type_ids = [0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets_token_level = [(0, 0)] * 3 + tweet_offsets_token_level + [(0, 0)]\n        tweet_offsets_word_level = [(0, 0)] * 3 + tweet_offsets_word_level + [(0, 0)]\n        targets_start += 3\n        targets_end += 3\n\n    elif (model_type == \"xlnet-base-cased\") or (model_type == \"xlnet-large-cased\"):\n\n        sentiment_id = {\n            'positive': 1654,\n            'negative': 2981,\n            'neutral': 9201\n        }\n\n        input_ids = [sentiment_id[sentiment]] + [4] + input_ids_orig + [3]\n        token_type_ids = [0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets_token_level = [(0, 0)] * 2 + tweet_offsets_token_level + [(0, 0)]\n        tweet_offsets_word_level = [(0, 0)] * 2 + tweet_offsets_word_level + [(0, 0)]\n        targets_start += 2\n        targets_end += 2\n\n    elif (model_type == \"bert-base-uncased\") or (model_type == \"bert-large-uncased\"):\n\n        sentiment_id = {\n            'positive': 3893,\n            'negative': 4997,\n            'neutral': 8699\n        }\n\n        input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n        token_type_ids = [0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets_token_level = [(0, 0)] * 3 + tweet_offsets_token_level + [(0, 0)]\n        tweet_offsets_word_level = [(0, 0)] * 3 + tweet_offsets_word_level + [(0, 0)]\n        targets_start += 3\n        targets_end += 3\n\n    elif (model_type == \"bert-base-cased\") or (model_type == \"bert-large-cased\"):\n\n        sentiment_id = {\n            'positive': 3112,\n            'negative': 4366,\n            'neutral': 8795\n        }\n\n        input_ids = [101] + [sentiment_id[sentiment]] + [102] + input_ids_orig + [102]\n        token_type_ids = [0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n        mask = [1] * len(token_type_ids)\n        tweet_offsets_token_level = [(0, 0)] * 3 + tweet_offsets_token_level + [(0, 0)]\n        tweet_offsets_word_level = [(0, 0)] * 3 + tweet_offsets_word_level + [(0, 0)]\n        targets_start += 3\n        targets_end += 3\n\n    else:\n        raise NotImplementedError\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets_token_level = tweet_offsets_token_level + ([(0, 0)] * padding_length)\n        tweet_offsets_word_level = tweet_offsets_word_level + ([(0, 0)] * padding_length)\n    else:\n        input_ids = input_ids[:max_len]\n        mask = mask[:max_len]\n        token_type_ids = token_type_ids[:max_len]\n        tweet_offsets_token_level = tweet_offsets_token_level[:max_len]\n        tweet_offsets_word_level = tweet_offsets_word_level[:max_len]\n\n\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_tweet_with_extra_space': tweet_with_extra_space,\n        'orig_selected': old_selected_text,\n        'sentiment': sentiment,\n        'offsets_token_level': tweet_offsets_token_level,\n        'offsets_word_level': tweet_offsets_word_level\n    }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text, old_selected_text, tokenizer, model_type, max_len):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.old_selected_text = old_selected_text\n        self.tokenizer = tokenizer\n        self.model_type = model_type\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item],\n            self.selected_text[item],\n            self.old_selected_text[item],\n            self.sentiment[item],\n            self.tokenizer,\n            self.model_type,\n            self.max_len,\n        )\n\n        return torch.tensor(data[\"ids\"], dtype=torch.long), \\\n               torch.tensor(data[\"mask\"], dtype=torch.long), \\\n               torch.tensor(data[\"token_type_ids\"], dtype=torch.long), \\\n               torch.tensor(data[\"targets_start\"], dtype=torch.long), \\\n               torch.tensor(data[\"targets_end\"], dtype=torch.long), \\\n               data[\"orig_tweet\"], \\\n               data[\"orig_tweet_with_extra_space\"], \\\n               data[\"orig_selected\"], \\\n               data[\"sentiment\"], \\\n               torch.tensor(data[\"offsets_token_level\"], dtype=torch.long), \\\n               torch.tensor(data[\"offsets_word_level\"], dtype=torch.long)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define dataloader","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_loader(data_path=\"../input/tweet-sentiment-extraction/\",\n                    csv_name=\"test.csv\",\n                    max_seq_length=384,\n                    model_type=\"bert-base-uncased\",\n                    batch_size=4,\n                    num_workers=4):\n\n    CURR_PATH = \"../input/\"\n    csv_path = os.path.join(data_path, csv_name)\n    df_test = pd.read_csv(csv_path)\n    df_test.loc[:, \"selected_text\"] = df_test.text.values\n    df_test.loc[:, \"cleaned_selected_text\"] = df_test.text.values\n#     df_test = df_test[:6]\n\n    if (model_type == \"bert-base-uncased\"):\n        tokenizer = BertTokenizer.from_pretrained(\n            pretrained_model_name_or_path=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.txt\".format(model_type)),\n            lowercase=True,\n        )\n    elif (model_type == \"bert-large-uncased\"):\n        tokenizer = BertTokenizer.from_pretrained(\n            pretrained_model_name_or_path=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.txt\".format(model_type)),\n            lowercase=True,\n        )\n    elif (model_type == \"bert-base-cased\"):\n        tokenizer = BertTokenizer.from_pretrained(\n            pretrained_model_name_or_path=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.txt\".format(model_type)),\n            lowercase=True,\n        )\n    elif (model_type == \"bert-large-cased\"):\n        tokenizer = BertTokenizer.from_pretrained(\n            pretrained_model_name_or_path=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.txt\".format(model_type)),\n            lowercase=True,\n        )\n    elif (model_type == \"xlnet-base-cased\") or (model_type == \"xlnet-large-cased\"):\n        tokenizer = XLNetTokenizer.from_pretrained(\n            pretrained_model_name_or_path=os.path.join(CURR_PATH, \"transformers-vocab/{}-spiece.model\".format(model_type)),\n            lowercase=True,\n        )\n    elif (model_type == \"albert-base-v2\") or (model_type == \"albert-large-v2\") or (model_type == \"albert-xlarge-v2\"):\n        tokenizer = AlbertTokenizer.from_pretrained(\n            pretrained_model_name_or_path=os.path.join(CURR_PATH, \"transformers-vocab/{}-spiece.model\".format(model_type)),\n            lowercase=True,\n        )\n    elif model_type == \"roberta-base\":\n        tokenizer = RobertaTokenizer(\n            vocab_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.json\".format(model_type)),\n            merges_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-merges.txt\".format(model_type)),\n            lowercase=True,\n        )\n    elif model_type == \"roberta-base-squad\":\n        tokenizer = RobertaTokenizer(\n            vocab_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.json\".format(model_type)),\n            merges_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-merges.txt\".format(model_type)),\n            lowercase=True,\n        )\n    elif model_type == \"roberta-large\":\n        tokenizer = RobertaTokenizer(\n            vocab_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.json\".format(model_type)),\n            merges_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-merges.txt\".format(model_type)),\n            lowercase=True,\n        )\n    else:\n\n        raise NotImplementedError\n\n    ds_test = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.cleaned_selected_text.values,\n        old_selected_text=df_test.selected_text.values,\n        tokenizer=tokenizer,\n        model_type=model_type,\n        max_len=max_seq_length\n    )\n    # print(len(ds_test.tensors))\n    loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n    return loader, tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"############################################ Define Net Class\nclass TweetBert(nn.Module):\n    def __init__(self, model_type=\"bert-large-uncased\", hidden_layers=None):\n        super(TweetBert, self).__init__()\n\n        self.model_name = 'TweetBert'\n        self.model_type = model_type\n\n        if hidden_layers is None:\n            hidden_layers = [-1]\n        self.hidden_layers = hidden_layers\n\n        if model_type == \"bert-large-uncased\":\n            bert_model_config = \\\n            '../input/bertlargewholewordmaskingfinetunedsquad/bert-large-uncased-whole-word-masking-finetuned-squad-config.json'\n            self.config = BertConfig.from_json_file(bert_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            self.bert = BertModel(self.config)   \n        elif model_type == \"bert-large-cased\":\n            bert_model_config = \\\n            '../input/bertlargewholewordmaskingfinetunedsquad/bert-large-cased-whole-word-masking-finetuned-squad-config.json'\n            self.config = BertConfig.from_json_file(bert_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            self.bert = BertModel(self.config)   \n        elif model_type == \"bert-base-uncased\":\n            bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n            self.config = BertConfig.from_json_file(bert_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            self.bert = BertModel(self.config)   \n        elif model_type == \"bert-base-cased\":\n            bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-cased/bert_config.json'\n            self.config = BertConfig.from_json_file(bert_model_config)\n            self.config.output_hidden_states = True\n            self.bert = BertModel(self.config)   \n        elif model_type == \"roberta-base\":\n            roberta_model_config = '../input/roberta-transformers-pytorch/roberta-base/config.json'\n            self.config = RobertaConfig.from_json_file(roberta_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            model_path = os.path.join('../input/roberta-transformers-pytorch/roberta-base/pytorch_model.bin')\n            self.bert = RobertaModel.from_pretrained(model_path, config=self.config)  \n        elif model_type == \"roberta-large\":\n            roberta_model_config = '../input/roberta-transformers-pytorch/roberta-large/config.json'\n            self.config = RobertaConfig.from_json_file(roberta_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            model_path = os.path.join('../input/roberta-transformers-pytorch/roberta-large/pytorch_model.bin')\n            self.bert = RobertaModel.from_pretrained(model_path, config=self.config)  \n        elif model_type == \"albert-large-v2\":\n            albert_model_config = '../input/pretrained-albert-pytorch/albert-large-v2/config.json'\n            self.config = AlbertConfig.from_json_file(albert_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            model_path = os.path.join('../input/pretrained-albert-pytorch/albert-large-v2/pytorch_model.bin')\n            self.bert = AlbertModel.from_pretrained(model_path, config=self.config)  \n        elif model_type == \"albert-base-v2\":\n            albert_model_config = '../input/pretrained-albert-pytorch/albert-base-v2/config.json'\n            self.config = AlbertConfig.from_json_file(albert_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            model_path = os.path.join('../input/pretrained-albert-pytorch/albert-large-v2/pytorch_model.bin')\n            self.bert = AlbertModel.from_pretrained(model_path, config=self.config) \n        elif model_type == \"xlnet-base-cased\":\n            xlnet_model_config = '../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-config.json'\n            self.config = XLNetConfig.from_json_file(xlnet_model_config)\n            self.config.output_hidden_states = True\n            self.config.hidden_dropout_prob = 0.1\n            model_path = os.path.join('../input/xlnet-pretrained-models-pytorch/' + model_type + '-pytorch_model.bin')\n            self.bert = XLNetModel.from_pretrained(model_path, config=self.config)  \n        else:\n            raise NotImplementedError\n\n        # hidden states fusion\n        weights_init = torch.zeros(len(hidden_layers)).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n\n        self.qa_start_end = nn.Linear(self.config.hidden_size, 2)\n\n        def init_weights_linear(m):\n            if type(m) == torch.nn.Linear:\n                torch.nn.init.normal_(m.weight, std=0.02)\n                torch.nn.init.normal_(m.bias, 0)\n\n        self.qa_start_end.apply(init_weights_linear)\n\n        self.dropouts = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n\n    def get_hidden_states(self, hidden_states):\n\n        fuse_hidden = None\n        # concat hidden\n        for i in range(len(self.hidden_layers)):\n            if i == 0:\n                hidden_layer = self.hidden_layers[i]\n                fuse_hidden = hidden_states[hidden_layer].unsqueeze(-1)\n            else:\n                hidden_layer = self.hidden_layers[i]\n                hidden_state = hidden_states[hidden_layer].unsqueeze(-1)\n                fuse_hidden = torch.cat([fuse_hidden, hidden_state], dim=-1)\n\n        fuse_hidden = (torch.softmax(self.layer_weights, dim=0) * fuse_hidden).sum(-1)\n\n        return fuse_hidden\n\n    def get_logits_by_random_dropout(self, fuse_hidden, fc):\n\n        logit = None\n        h = fuse_hidden\n\n        for j, dropout in enumerate(self.dropouts):\n\n            if j == 0:\n                logit = fc(dropout(h))\n            else:\n                logit += fc(dropout(h))\n\n        return logit / len(self.dropouts)\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            start_positions=None,\n            end_positions=None,\n    ):\n\n        if self.model_type == \"roberta-base\" or self.model_type == \"roberta-base-squad\" \\\n                or self.model_type == \"roberta-large\":\n\n            outputs = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                head_mask=head_mask,\n                inputs_embeds=inputs_embeds,\n            )\n            hidden_states = outputs[2]\n        elif self.model_type == \"xlnet-base-cased\":\n\n            outputs = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n            )\n            hidden_states = outputs[1]\n        else:\n\n            outputs = self.bert(\n                input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                position_ids=position_ids,\n                head_mask=head_mask,\n                inputs_embeds=inputs_embeds,\n            )\n            hidden_states = outputs[2]\n\n        # bs, seq len, hidden size\n        fuse_hidden = self.get_hidden_states(hidden_states)\n\n        fuse_hidden_context = fuse_hidden\n        if self.model_type == \"xlnet-base-cased\":\n            hidden_classification = fuse_hidden[:, -1, :]\n        else:\n            hidden_classification = fuse_hidden[:, 0, :]\n\n        # #################################################################### direct approach\n        logits = self.get_logits_by_random_dropout(fuse_hidden_context, self.qa_start_end)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits, end_logits = start_logits.squeeze(-1), end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n\n        return outputs  # start_logits, end_logits, (hidden_states), (attentions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_check_point(model, checkpoint_path, skip_layers=[]):\n    \n    checkpoint_to_load = torch.load(checkpoint_path)\n    model_state_dict = checkpoint_to_load['model']\n    \n    state_dict = model.state_dict()\n\n    keys = list(state_dict.keys())\n\n    for key in keys:\n        if any(s in key for s in skip_layers):\n            continue\n        try:\n            state_dict[key] = model_state_dict[key]\n        except:\n            print(\"Missing key:\", key)\n\n    model.load_state_dict(state_dict)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# define logits function","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_logits(model, checkpoint_path, folds, all_input_ids, all_attention_masks, all_token_type_ids=None):\n    \"\"\"\n    Get start and end logits of a batch by ensemble of all folds\n    Args:\n        model: pytorch model instance\n        checkpoint_path: string, path for checkpoints\n        folds: list, all folds for ensemble\n        all_input_ids: tensor, cuda, a batch of input_ids tensor\n        all_attention_masks: tensor, cuda, a batch of attention_masks tensor\n        all_token_type_ids: tensor, cuda, a batch of token_type_ids tensor\n    Returns:\n        start_logits: tensor, cpu\n        end_logits: tensor, cpu\n    \"\"\"\n    \n    for model_idx, fold in enumerate(folds):\n        \n        checkpoint = os.path.join(checkpoint_path, \"fold_{}.pth\".format(fold))\n        \n        model = load_check_point(model, checkpoint, skip_layers=[])\n        model.eval()\n        \n        outputs = model(input_ids=all_input_ids, attention_mask=all_attention_masks,\n                                     token_type_ids=all_token_type_ids)\n        if model_idx == 0:\n            start_logits, end_logits = torch.softmax(outputs[0], dim=-1) / len(folds), torch.softmax(outputs[1], dim=-1) / len(folds)\n#             start_logits, end_logits = outputs[0] / len(folds), outputs[1] / len(folds)\n        else:\n            start_logits += torch.softmax(outputs[0], dim=-1) / len(folds)\n            end_logits += torch.softmax(outputs[1], dim=-1) / len(folds)\n#             start_logits += outputs[0] / len(folds)\n#             end_logits += outputs[1] / len(folds)\n            \n    return start_logits.detach().cpu(), end_logits.detach().cpu()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_level_logits(start_logits,\n                          end_logits,\n                          model_type,\n                          tweet_offsets_word_level):\n\n    tweet_offsets_word_level = np.array(tweet_offsets_word_level)\n\n    if model_type == \"roberta-base\" or model_type == \"roberta-large\" or model_type == \"roberta-base-squad\":\n        logit_offset = 4\n\n    elif (model_type == \"albert-base-v2\") or (model_type == \"albert-large-v2\") or (\n            model_type == \"albert-xlarge-v2\"):\n        logit_offset = 3\n\n    elif (model_type == \"xlnet-base-cased\") or (model_type == \"xlnet-large-cased\"):\n        logit_offset = 2\n\n    elif (model_type == \"bert-base-uncased\") or (model_type == \"bert-large-uncased\"):\n        logit_offset = 3\n\n    elif (model_type == \"bert-base-cased\") or (model_type == \"bert-large-cased\"):\n        logit_offset = 3\n\n    prev = tweet_offsets_word_level[logit_offset]\n    word_level_bbx = []\n    curr_bbx = []\n\n    for i in range(len(tweet_offsets_word_level) - logit_offset - 1):\n\n        curr = tweet_offsets_word_level[i + logit_offset]\n\n        if curr[0] < prev[0] and curr[1] > prev[1]:\n            break\n\n        if curr[0] == prev[0] and curr[1] == prev[1]:\n            curr_bbx.append(i)\n        else:\n            word_level_bbx.append(curr_bbx)\n            curr_bbx = [i]\n\n        prev = curr\n\n    if len(word_level_bbx) == 0:\n        word_level_bbx.append(curr_bbx)\n\n    for i in range(len(word_level_bbx)):\n        word_level_bbx[i].append(word_level_bbx[i][-1] + 1)\n\n    start_logits_word_level = [np.max(start_logits[bbx[0]: bbx[-1]]) for bbx in word_level_bbx]\n    end_logits_word_level = [np.max(end_logits[bbx[0]: bbx[-1]]) for bbx in word_level_bbx]\n\n    return start_logits_word_level, end_logits_word_level, word_level_bbx\n\n\ndef get_token_level_idx(start_logits,\n                        end_logits,\n                        start_logits_word_level,\n                        end_logits_word_level,\n                        word_level_bbx):\n\n    # get most possible word\n    start_idx_word = np.argmax(start_logits_word_level)\n    end_idx_word = np.argmax(end_logits_word_level)\n\n    # get all token idx in selected word\n    start_word_bbx = word_level_bbx[start_idx_word]\n    end_word_bbx = word_level_bbx[end_idx_word]\n\n    # find most possible token idx in selected word\n    start_idx_in_word = np.argmax(start_logits[start_word_bbx[0]: start_word_bbx[-1]])\n    end_idx_in_word = np.argmax(end_logits[end_word_bbx[0]: end_word_bbx[-1]])\n\n    # find most possible token idx in whole sentence\n    start_idx_token = start_word_bbx[start_idx_in_word]\n    end_idx_token = end_word_bbx[end_idx_in_word]\n\n#     return start_idx_token, end_idx_token\n    return np.argmax(start_logits), np.argmax(end_logits)\n\n\ndef calculate_jaccard_score(\n        original_tweet,\n        selected_text,\n        idx_start,\n        idx_end,\n        model_type,\n        tweet_offsets,\n        tokenizer,\n        sentiment):\n    \n    if idx_end < idx_start:\n        filtered_output = original_tweet\n\n    else:\n        input_ids_orig = tokenizer.encode(original_tweet).ids\n        input_ids = input_ids_orig + [2]\n        filtered_output = tokenizer.decode(input_ids[idx_start:idx_end+1])\n\n    jac = 0\n    return jac, filtered_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_different_model_logits(model_type=\"roberta-base\", \\\n                               hidden_layers=[-1, -2, -3, -4], \\\n                               checkpoint_path=\"../input/tweetrobertabase5fold42v8/\", \\\n                               Config=None):\n    \"\"\"\n    Get start and end logits of a batch by ensemble of all folds\n    Args:\n        model_type: string\n        hidden_layers: list\n        checkpoint_path: string\n        Config: Config class instance\n    Returns:\n        start_logits_token_level: list of array\n        end_logits_token_level: list of array\n        start_logits_word_level: list of array\n        end_logits_word_level: list of array\n        word_level_bbx: list of list of list\n        token_level_offsets: list of list of tuple\n        tweet: list of string\n    \"\"\"\n    \n    if model_type == \"roberta-base\" or model_type == \"roberta-large\" or model_type == \"roberta-base-squad\":\n\n        offsets = 4\n\n    elif model_type == \"albert-base-v2\" or model_type == \"albert-large-v2\" or model_type == \"albert-xlarge-v2\":\n\n        offsets = 3\n\n    elif model_type == \"xlnet-base-cased\" or model_type == \"xlnet-large-cased\":\n\n        offsets = 2\n\n    elif model_type == \"bert-base-uncased\" or model_type == \"bert-large-uncased\" or model_type == \"bert-base-cased\" or model_type == \"bert-large-cased\":\n\n        offsets = 3\n\n    else:\n        raise NotImplementedError\n\n    loader, tokenizer = get_test_loader(data_path=\"../input/tweet-sentiment-extraction/\",\n                                        max_seq_length=Config.max_seq_length,\n                                        model_type=model_type,\n                                        batch_size=Config.val_batch_size,\n                                        num_workers=Config.num_workers)\n\n    model = TweetBert(model_type=model_type, hidden_layers=hidden_layers).cuda().eval()\n\n    start_logits_token_level = []\n    end_logits_token_level = []\n    start_logits_word_level = []\n    end_logits_word_level = []\n    word_level_bbx = []\n    token_level_offsets = []\n    tweets = []\n    tweets_with_extra_spaces = []\n    sentiments = []\n    \n    # init cache\n    torch.cuda.empty_cache()\n\n    with torch.no_grad():\n\n        for test_batch_i, (all_input_ids, all_attention_masks, all_token_type_ids, \n                               all_start_positions, all_end_positions, \n                               all_orig_tweet, all_orig_tweet_with_extra_space, \n                               all_orig_selected, all_sentiment, \n                               all_offsets_token_level, all_offsets_word_level) in enumerate(loader):\n\n            if (test_batch_i % 5 == 0):\n                print(\"Inferencing: \", test_batch_i, \"of\", len(loader))\n\n            # set input to cuda mode\n            all_input_ids = all_input_ids.cuda()\n            all_attention_masks = all_attention_masks.cuda()\n            all_token_type_ids = all_token_type_ids.cuda()\n\n            start_logits, end_logits = get_logits(model, \\\n                                                  checkpoint_path, \\\n                                                  [0, 1, 2, 3, 4], \\\n                                                  all_input_ids, \\\n                                                  all_attention_masks, \\\n                                                  all_token_type_ids)\n\n            start_logits = start_logits[:, offsets:]\n            end_logits = end_logits[:, offsets:]\n            \n            start_logits = torch.softmax(start_logits, dim=-1)\n            end_logits = torch.softmax(end_logits, dim=-1)\n            \n            def to_numpy(tensor):\n                return tensor.numpy()\n\n                # batch size, seq len\n\n            start_logits = to_numpy(start_logits)\n            end_logits = to_numpy((end_logits))\n            \n            for px, tweet in enumerate(all_orig_tweet):\n                \n                start_logits_word_level_sample, end_logits_word_level_sample, word_level_bbx_sample = get_word_level_logits(\n                    start_logits[px],\n                    end_logits[px],\n                    model_type,\n                    all_offsets_word_level[px])\n\n                start_logits_token_level.append(start_logits[px])\n                end_logits_token_level.append(end_logits[px])\n                start_logits_word_level.append(start_logits_word_level_sample)\n                end_logits_word_level.append(end_logits_word_level_sample)\n                word_level_bbx.append(word_level_bbx_sample)\n                token_level_offsets.append(all_offsets_token_level[px])\n                tweets.append(tweet)\n                tweets_with_extra_spaces.append(all_orig_tweet_with_extra_space[px])\n                sentiments.append(all_sentiment[px])\n\n\n    return start_logits_token_level, end_logits_token_level, \\\n           start_logits_word_level, end_logits_word_level, \\\n           word_level_bbx, token_level_offsets, \\\n           tweets, tweets_with_extra_spaces, sentiments, tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# roberta-base","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta_base_start_logits_token_level_42, roberta_base_end_logits_token_level_42, \\\nroberta_base_start_logits_word_level_42, roberta_base_end_logits_word_level_42, \\\nroberta_base_word_level_bbx, roberta_base_token_level_offsets, \\\ntweets, tweets_with_extra_spaces, sentiments, roberta_base_tokenizer = get_different_model_logits(model_type=\"roberta-base\", \\\n                                                                       hidden_layers=[-1, -2, -3, -4], \\\n                                                                       checkpoint_path=\"../input/tweetrobertabasenewpipelinepreprocessing42/\",\\\n                                                                       Config=Config)\n\n\nroberta_base_start_logits_token_level_666, roberta_base_end_logits_token_level_666, \\\nroberta_base_start_logits_word_level_666, roberta_base_end_logits_word_level_666, \\\n_, _, _, _, _, _ = get_different_model_logits(model_type=\"roberta-base\", \\\n                                                                       hidden_layers=[-1, -2, -3, -4], \\\n                                                                       checkpoint_path=\"../input/tweetrobertabasenewpipelinepreprocessing666/\",\\\n                                                                       Config=Config)\n\n\nroberta_base_start_logits_token_level_1234, roberta_base_end_logits_token_level_1234, \\\nroberta_base_start_logits_word_level_1234, roberta_base_end_logits_word_level_1234, \\\n_, _, _, _, _, _ = get_different_model_logits(model_type=\"roberta-base\", \\\n                                                                       hidden_layers=[-1, -2, -3, -4], \\\n                                                                       checkpoint_path=\"../input/tweetrobertabasenewpipelinepreprocessing1234/\",\\\n                                                                       Config=Config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = len(roberta_base_start_logits_token_level_42)\n\nroberta_base_start_logits_token_level = [(roberta_base_start_logits_token_level_42[i] + \n                                          roberta_base_start_logits_token_level_666[i] + \n                                          roberta_base_start_logits_token_level_1234[i]\n                                         ) / 3 for i in range(sample_size)]\n\nroberta_base_end_logits_token_level = [(roberta_base_end_logits_token_level_42[i] + \n                                        roberta_base_end_logits_token_level_666[i] +\n                                        roberta_base_end_logits_token_level_1234[i]\n                                       ) / 3 for i in range(sample_size)]\n\nroberta_base_start_logits_word_level = [(np.array(roberta_base_start_logits_word_level_42[i]) + \n                                         np.array(roberta_base_start_logits_word_level_666[i]) + \n                                         np.array(roberta_base_start_logits_word_level_1234[i])\n                                        ) / 3 for i in range(sample_size)]\n\nroberta_base_end_logits_word_level = [(np.array(roberta_base_end_logits_word_level_42[i]) + \n                                       np.array(roberta_base_end_logits_word_level_666[i]) +\n                                       np.array(roberta_base_end_logits_word_level_1234[i])\n                                      ) / 3 for i in range(sample_size)]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# albert-large","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"_, _, albert_large_start_logits_word_level, albert_large_end_logits_word_level, _, _, _, _, _, _ = get_different_model_logits(model_type=\"albert-large-v2\", \\\n                                                                                                                          hidden_layers=[-1, -2, -3, -4], \\\n                                                                                                                          checkpoint_path=\"../input/tweetalbertlargenewpipelinepreprocessingv1/\",\\\n                                                                                                                          Config=Config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# xlnet-base","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"_, _, xlnet_base_start_logits_word_level, xlnet_base_end_logits_word_level, _, _, _, _, _, _ = get_different_model_logits(model_type=\"xlnet-base-cased\", \\\n                                                                                                                          hidden_layers=[-1, -2, -3, -4], \\\n                                                                                                                          checkpoint_path=\"../input/tweetxlnetbasenewpipelinepreprocessingv1/\",\\\n                                                                                                                          Config=Config)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"start_logits_word_level = []\nend_logits_word_level = []\n\nfor i in range(len(roberta_base_start_logits_word_level)):\n    \n#     start_logits_word_level.append(np.array(roberta_base_start_logits_word_level[i]))\n#     end_logits_word_level.append((np.array(roberta_base_end_logits_word_level[i])))\n    \n    start_logits_word_level.append((np.array(roberta_base_start_logits_word_level[i]) + np.array(albert_large_start_logits_word_level[i]) + np.array(xlnet_base_start_logits_word_level[i])) / 3)\n    end_logits_word_level.append((np.array(roberta_base_end_logits_word_level[i]) + np.array(albert_large_end_logits_word_level[i]) + np.array(xlnet_base_end_logits_word_level[i])) / 3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tokenizers\n\n# we based on roberta base token level offsets for final prediction\nall_results = []\nall_start_end = []\nbase_model_type = \"roberta-base\"\n\nCURR_PATH = \"../input/\"\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-vocab.json\".format(base_model_type)),\n            merges_file=os.path.join(CURR_PATH, \"transformers-vocab/{}-merges.txt\".format(base_model_type)),\n            lowercase=True,\n            add_prefix_space=True\n        )\n\nfor i in range(len(start_logits_word_level)):\n    \n    start_idx_token, end_idx_token = get_token_level_idx(roberta_base_start_logits_token_level[i], \\\n                                                         roberta_base_end_logits_token_level[i], \\\n                                                         start_logits_word_level[i], \\\n                                                         end_logits_word_level[i], \\\n                                                         roberta_base_word_level_bbx[i]\n                                                         )\n#     start_idx_token, end_idx_token = np.argmax(roberta_base_start_logits_token_level[i], axis=-1), np.argmax(roberta_base_end_logits_token_level[i], axis=-1)\n    all_start_end.append((start_idx_token, end_idx_token))\n    _, final_text = calculate_jaccard_score(\n                original_tweet=tweets[i],\n                selected_text=tweets[i],\n                idx_start=start_idx_token,\n                idx_end=end_idx_token,\n                model_type=base_model_type,\n                tweet_offsets=roberta_base_token_level_offsets[i],\n                tokenizer=tokenizer,\n                sentiment=sentiments[i]\n            )\n        \n    all_results.append(final_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(all_start_end[:30])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(os.path.join(\"/kaggle/input/tweet-sentiment-extraction\", \"sample_submission.csv\"))\ntest = pd.read_csv(os.path.join(\"/kaggle/input/tweet-sentiment-extraction\", \"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(submission)):\n    if test['sentiment'][i] == 'neutral' or len(test['text'][i].split()) < 4:  # neutral postprocessing\n        submission.loc[i, 'selected_text'] = test['text'][i]\n    else:\n        submission.loc[i, 'selected_text'] = all_results[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"text\"] = test[\"text\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reverse_preprocessing(text):\n\n    text = text.replace(\". . . .\", \"....\")\n    text = text.replace(\". . .\", \"...\")\n    text = text.replace(\". .\", \"..\")\n    text = text.replace(\"! ! ! !\", \"!!!!\")\n    text = text.replace(\"! ! !\", \"!!!\")\n    text = text.replace(\"! !\", \"!!\")\n    text = text.replace(\"? ? ? ?\", \"????\")\n    text = text.replace(\"? ? ?\", \"???\")\n    text = text.replace(\"? ?\", \"??\")\n\n    return text\n\n\ndef find_text_idx(text, selected_text):\n\n    text_len = len(text)\n\n    for start_idx in range(text_len):\n        if text[start_idx] == selected_text[0]:\n            for end_idx in range(start_idx+1, text_len+1):\n                contained_text = \"\".join(text[start_idx: end_idx].split())\n                # print(\"contained_text:\", contained_text, \"selected_text:\", selected_text)\n                if contained_text == \"\".join(selected_text.split()):\n                    return start_idx, end_idx\n\n    return None, None\n\n\ndef calculate_spaces(text, selected_text):\n\n    selected_text = \" \".join(selected_text.split())\n    start_idx, end_idx = find_text_idx(text, selected_text)\n    # print(\"text:\", text[start_idx: end_idx], \"prediction:\", selected_text)\n\n    if start_idx is None:\n        start_idx = 0\n        print(\"----------------- error no start idx find ------------------\")\n        print(\"text:\", text, \"prediction:\", selected_text)\n        print(\"----------------- error no start idx find ------------------\")\n\n    if end_idx is None:\n        end_idx = len(text)\n        print(\"----------------- error no end idx find ------------------\")\n        print(\"text:\", text, \"prediction:\", selected_text)\n        print(\"----------------- error no end idx find ------------------\")\n\n    x = text[:start_idx]\n    try:\n        if x[-1] == \" \":\n            x = x[:-1]\n    except:\n        pass\n\n    l1 = len(x)\n    l2 = len(\" \".join(x.split()))\n    return l1 - l2, start_idx, end_idx\n\n\ndef pp_v2(text, predicted):\n\n    text = str(text).lower()\n    predicted = predicted.lower()\n    predicted = predicted.strip()\n\n    if len(predicted) == 0:\n        return predicted\n\n    predicted = reverse_preprocessing(str(predicted))\n\n    spaces, index_start, index_end = calculate_spaces(text, predicted)\n\n    if spaces == 1:\n        if len(text[max(0, index_start-1): index_end+1]) <= 0 or text[max(0, index_start-1): index_end+1][-1] != \".\":\n            return text[max(0, index_start - 1): index_end]\n        else:\n            return text[max(0, index_start-1): index_end+1]\n    elif spaces == 2:\n        return text[max(0, index_start-2): index_end]\n    elif spaces == 3:\n        return text[max(0, index_start-3): index_end-1]\n    elif spaces == 4:\n        return text[max(0, index_start-4): index_end-2]\n    else:\n        return predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"new_selected\"] = submission.apply(lambda x: pp_v2(x.text, x.selected_text), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.selected_text = submission[\"new_selected\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[[\"textID\",\"selected_text\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}