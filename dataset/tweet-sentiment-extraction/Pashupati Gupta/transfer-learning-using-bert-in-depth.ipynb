{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction \n\nBERT is a language representation model that makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The pre-trained bert model can be use for many NLP tasks. Here we'll se we can use it for binary text classification. On a very high level the architecture will look like this. (Architecture example of spam classifier. Image Credits - [Jay Amammar](http://jalammar.github.io/) )"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/input/notadataset/bert-cls.PNG')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's beign with installing and importing the libraries."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install pytorch_pretrained_bert","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install tools","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# basic libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\n\n# Transformer libraries\nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom tqdm import tqdm_notebook, trange\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\nfrom pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\nfrom sklearn.model_selection import train_test_split\nfrom multiprocessing import Pool, cpu_count\nfrom tools import *\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.metrics import confusion_matrix\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Paths and Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The input data dir. Should contain the .tsv files (or other data files) for the task.\nDATA_DIR = \"/kaggle/working/bert_inputs/\"\n\n# Bert pre-trained model selected in the list: bert-base-uncased, \n# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n# bert-base-multilingual-cased, bert-base-chinese.\nBERT_MODEL = 'bert-base-uncased'\n\n# The name of the task to train.I'm going to name this 'ngs'. acrony for nlp-getting-started\nTASK_NAME = 'ngs'\n\n# The output directory where the fine-tuned model and checkpoints will be written.\nOUTPUT_DIR = '/kaggle/working/bert_outputs/'\n\n# The directory where the evaluation reports will be written to.\nREPORTS_DIR = '/kaggle/working/'\n\n# This is where BERT will look for pre-trained models to load parameters from.\nCACHE_DIR = '/kaggle/working/'\n\n# The maximum total input sequence length after WordPiece tokenization.\n# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\nMAX_SEQ_LENGTH = 128\n\nTRAIN_BATCH_SIZE = 24\nEVAL_BATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 2\nRANDOM_SEED = 42\nGRADIENT_ACCUMULATION_STEPS = 1\nWARMUP_PROPORTION = 0.1\nOUTPUT_MODE = 'classification'\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"pytorch_model.bin\"\n\noutput_mode = OUTPUT_MODE\ncache_dir = CACHE_DIR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n        os.makedirs(REPORTS_DIR)\nif not os.path.exists(REPORTS_DIR):\n    os.makedirs(REPORTS_DIR)\n    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n    os.makedirs(REPORTS_DIR)\n    \nprint (REPORTS_DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n    \nprint (OUTPUT_DIR)\n\nif os.path.exists(DATA_DIR) and os.listdir(DATA_DIR):\n        raise ValueError(\"Data directory ({}) already exists and is not empty.\".format(DATA_DIR))\nif not os.path.exists(DATA_DIR):\n    os.makedirs(DATA_DIR)\n    \nprint (DATA_DIR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation\n\nBERT is built on top of some processors (they are basically python classes). In our case BinaryClassificationProcessor will be used that inherits DataProcessor class. This DataProcessor class requires the data in a particular schema. W'll prepare out dataset according to that schema ('tweet_id', 'label', 'alpha', 'text')\n\nYou'll understand it better once you see the implimentation of these classes in the below cells. Let's prepare the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bert = pd.DataFrame({'tweet_id':df['id'],\n                       'label':df['target'],\n                       \"alpha\":['a']*df.shape[0],\n                       'text':df['text']})\n\ndf_bert.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The schema looks good now. Let's do train and dev split and save the data in tsv format. Because the processer that I have read the tsv data. One can define his own processor to read any format/schema of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.05)\ndf_bert_train.to_csv('/kaggle/working/bert_inputs/train.tsv', sep='\\t', index=False, header=False)\ndf_bert_dev.to_csv('/kaggle/working/bert_inputs/dev.tsv', sep='\\t', index=False, header=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Tuning\n\nNow that we have out paths, parameters and data ready. Let's jump into Fine-tuning BERT. In the below cell we have BinaryClassificationProcessor that reads the data and return it in desired format i.e. InputExample "},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport csv\nimport os\nimport sys\n\ncsv.field_size_limit(2147483647) # Increase CSV reader's field limit incase we have long text.\n\n\nclass InputExample(object):\n    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass DataProcessor(object):\n    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n        raise NotImplementedError()\n\n    def get_labels(self):\n        \"\"\"Gets the list of labels for this data set.\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        \"\"\"Reads a tab separated value file.\"\"\"\n        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, 'utf-8') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass BinaryClassificationProcessor(DataProcessor):\n    \"\"\"Processor for binary classification dataset.\"\"\"\n\n    def get_train_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n    def get_dev_examples(self, data_dir):\n        \"\"\"See base class.\"\"\"\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n\n    def get_labels(self):\n        \"\"\"See base class.\"\"\"\n        return [\"0\", \"1\"]\n\n    def _create_examples(self, lines, set_type):\n        \"\"\"Creates examples for the training and dev sets.\"\"\"\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = \"%s-%s\" % (set_type, i)\n            try:\n                text_a = line[3]\n                label = line[1]\n                examples.append(\n                    InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n            except:\n                pass\n        return examples\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above processor gives the data in desired format. Now we want to convert the data to feature (The tokenization, padding, masking steps) The method convert_example_to_feature does these steps and returns the features in desired format i.e. InputFeatures"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InputFeatures(object):\n    \"\"\"A single set of features of data.\"\"\"\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that's truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef convert_example_to_feature(example_row):\n    # return example_row\n    example, label_map, max_seq_length, tokenizer, output_mode = example_row\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n\n    tokens_b = None\n    if example.text_b:\n        tokens_b = tokenizer.tokenize(example.text_b)\n        # Modifies `tokens_a` and `tokens_b` in place so that the total\n        # length is less than the specified length.\n        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n    else:\n        # Account for [CLS] and [SEP] with \"- 2\"\n        if len(tokens_a) > max_seq_length - 2:\n            tokens_a = tokens_a[:(max_seq_length - 2)]\n\n    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n    segment_ids = [0] * len(tokens)\n\n    if tokens_b:\n        tokens += tokens_b + [\"[SEP]\"]\n        segment_ids += [1] * (len(tokens_b) + 1)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += padding\n    input_mask += padding\n    segment_ids += padding\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    if output_mode == \"classification\":\n        label_id = label_map[example.label]\n    elif output_mode == \"regression\":\n        label_id = float(example.label)\n    else:\n        raise KeyError(output_mode)\n\n    return InputFeatures(input_ids=input_ids,\n                         input_mask=input_mask,\n                         segment_ids=segment_ids,\n                         label_id=label_id)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have out processors and functions ready. Let's run them.\n\n1) Reading the trainings examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"processor = BinaryClassificationProcessor()\ntrain_examples = processor.get_train_examples(DATA_DIR)\ntrain_examples_len = len(train_examples)\nprint (train_examples_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2) Training labels and number of steps"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_list = processor.get_labels()\nnum_labels = len(label_list)\nprint (num_labels)\n\nnum_train_optimization_steps = int(\n    train_examples_len / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS) * NUM_TRAIN_EPOCHS\n\nprint (num_train_optimization_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3) Loading pre-trained tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4) Convering training examples to features"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_map = {label: i for i, label in enumerate(label_list)}\ntrain_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in train_examples]\n\nprocess_count = cpu_count() - 1\nif __name__ ==  '__main__':\n    print(f'Preparing to convert {train_examples_len} examples..')\n    print(f'Spawning {process_count} processes..')\n    with Pool(process_count) as p:\n        train_features = list(tqdm_notebook(p.imap(convert_example_to_feature, train_examples_for_processing), total=train_examples_len))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5) Saving the training features"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(DATA_DIR + \"train_features.pkl\", \"wb\") as f:\n    pickle.dump(train_features, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"6) Loading pre-trained BERT model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre-trained model (weights)\nmodel = BertForSequenceClassification.from_pretrained(BERT_MODEL, cache_dir=CACHE_DIR, num_labels=num_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"7) Setting optimizer and it's parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=LEARNING_RATE,\n                     warmup=WARMUP_PROPORTION,\n                     t_total=num_train_optimization_steps)\n\nglobal_step = 0\nnb_tr_steps = 0\ntr_loss = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"8) Converting features to tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"logger = logging.getLogger(\"logger\")\n\nlogger.info(\"***** Running training *****\")\nlogger.info(\"  Num examples = %d\", train_examples_len)\nlogger.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\nlogger.info(\"  Num steps = %d\", num_train_optimization_steps)\nall_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\nall_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\nall_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n\nif OUTPUT_MODE == \"classification\":\n    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\nelif OUTPUT_MODE == \"regression\":\n    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"9) Preparing the data-loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"10) Finally!!! The Training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\nfor _ in trange(int(NUM_TRAIN_EPOCHS), desc=\"Epoch\"):\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(tqdm_notebook(train_dataloader, desc=\"Iteration\")):\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, input_mask, segment_ids, label_ids = batch\n\n        logits = model(input_ids, segment_ids, input_mask, labels=None)\n\n        if OUTPUT_MODE == \"classification\":\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n        elif OUTPUT_MODE == \"regression\":\n            loss_fct = MSELoss()\n            loss = loss_fct(logits.view(-1), label_ids.view(-1))\n\n        if GRADIENT_ACCUMULATION_STEPS > 1:\n            loss = loss / GRADIENT_ACCUMULATION_STEPS\n\n        loss.backward()\n        print(\"\\r%f\" % loss, end='')\n        \n        tr_loss += loss.item()\n        nb_tr_examples += input_ids.size(0)\n        nb_tr_steps += 1\n        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model evaluation\n\nSuperb! The BERT model if fine tuned now on our dataset. Let's evaluate it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_eval_report(task_name, labels, preds):\n    mcc = matthews_corrcoef(labels, preds)\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    return {\n        \"task\": task_name,\n        \"mcc\": mcc,\n        \"tp\": tp,\n        \"tn\": tn,\n        \"fp\": fp,\n        \"fn\": fn,\n        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n    }\n\ndef compute_metrics(task_name, labels, preds):\n    assert len(preds) == len(labels)\n    return get_eval_report(task_name, labels, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Preparing evaluation set input for BERT model (Like we did earlier)\neval_examples = processor.get_dev_examples(DATA_DIR)\neval_examples_len = len(eval_examples)\neval_examples_for_processing = [(example, label_map, MAX_SEQ_LENGTH, tokenizer, OUTPUT_MODE) for example in eval_examples]\n\nprocess_count = cpu_count() - 1\nif __name__ ==  '__main__':\n    print(f'Preparing to convert {eval_examples_len} examples..')\n    print(f'Spawning {process_count} processes..')\n    with Pool(process_count) as p:\n        eval_features = list(tqdm_notebook(p.imap(convert_example_to_feature, eval_examples_for_processing), total=eval_examples_len))\n        \neval_all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\neval_all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\neval_all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n\nif OUTPUT_MODE == \"classification\":\n    eval_all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\nelif OUTPUT_MODE == \"regression\":\n    eval_all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)\n    \neval_data = TensorDataset(eval_all_input_ids, eval_all_input_mask, eval_all_segment_ids, eval_all_label_ids)\n\n# Run prediction for full data\neval_sampler = SequentialSampler(eval_data)\neval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=EVAL_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The evaluation loop!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\neval_loss = 0\nnb_eval_steps = 0\npreds = []\n\nfor input_ids, input_mask, segment_ids, label_ids in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n    input_ids = input_ids.to(device)\n    input_mask = input_mask.to(device)\n    segment_ids = segment_ids.to(device)\n    label_ids = label_ids.to(device)\n\n    with torch.no_grad():\n        logits = model(input_ids, segment_ids, input_mask, labels=None)\n\n    # create eval loss and other metric required by the task\n    if OUTPUT_MODE == \"classification\":\n        loss_fct = CrossEntropyLoss()\n        tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n    elif OUTPUT_MODE == \"regression\":\n        loss_fct = MSELoss()\n        tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n\n    eval_loss += tmp_eval_loss.mean().item()\n    nb_eval_steps += 1\n    if len(preds) == 0:\n        preds.append(logits.detach().cpu().numpy())\n    else:\n        preds[0] = np.append(\n            preds[0], logits.detach().cpu().numpy(), axis=0)\n\neval_loss = eval_loss / nb_eval_steps\npreds = preds[0]\nif OUTPUT_MODE == \"classification\":\n    preds = np.argmax(preds, axis=1)\nelif OUTPUT_MODE == \"regression\":\n    preds = np.squeeze(preds)\nresult = compute_metrics(TASK_NAME, eval_all_label_ids.numpy(), preds)\n\nresult['eval_loss'] = eval_loss\n\noutput_eval_file = os.path.join(REPORTS_DIR, \"eval_results.txt\")\nwith open(output_eval_file, \"w\") as writer:\n    logger.info(\"***** Eval results *****\")\n    for key in (result.keys()):\n        logger.info(\"  %s = %s\", key, str(result[key]))\n        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n        print(\"{} ->\".format(key), result[key])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Thanks for viewing this notebook. If you fount it useful consider UPVOTING it."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}