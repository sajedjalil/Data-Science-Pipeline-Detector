{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport transformers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport inspect\nimport torch.nn as nn\nimport torch\nfrom tqdm import tqdm\nimport gc\n\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\nMAX_LEN = 60\nEPOCHS = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR = '../input/tweet-sentiment-extraction/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DIR+'train.csv')\n#test = pd.read_csv(DIR+'train.csv')\n#ss = pd.read_csv(DIR+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sentiment[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {'positive':2,'negative':0,'neutral':1}\ntrain.replace({'sentiment':mapping},inplace=True)\n##test.replace({'sentiment':mapping},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetReviewDataset(Dataset):\n    def __init__(self,review,tokenizer,targets,max_len):\n        self.review = review\n        self.tokenizer = tokenizer\n        self.targets = targets\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self,index):\n        review = str(self.review[index])\n        target = self.targets[index]\n        \n        encoding = self.tokenizer.encode_plus(\n        review,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n        )\n        \n        return {'review_text':review,'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train , df_val = train_test_split(train,test_size = 0.2,random_state = 23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    \n    ds = TweetReviewDataset(\n    review=df.text.to_numpy(),\n    targets=df.sentiment.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=0\n  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_len = len(df_train)\nval_len = len(df_val)\n\ntrain_loader = create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\nval_loader = create_data_loader(df_val,tokenizer,MAX_LEN,BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train\ndel df_val\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    def __init__(self,n_classes):\n        super(SentimentClassifier,self).__init__()\n        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n        \n    def forward(self,input_ids,attention_mask):\n        _,pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n        \n        output = self.drop(pooled_output)\n        return self.out(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SentimentClassifier(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = transformers.AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * EPOCHS\n\nscheduler = transformers.get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model,data_loader,loss_fn,optimizer,scheduler,n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for i,d in tqdm(enumerate(data_loader)):\n        \n        input_ids = d['input_ids']\n        attention_mask = d['attention_mask']\n        targets = d[\"targets\"]\n        \n        outputs = model(input_ids = input_ids,attention_mask = attention_mask)\n        \n        _,preds = torch.max(outputs,dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() / n_examples, np.mean(losses)\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, data_loader,loss_fn, n_examples):\n    \n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        \n        for i,d in tqdm(enumerate(data_loader)):\n            \n            input_ids = d[\"input_ids\"]\n            attention_mask = d[\"attention_mask\"]\n            targets = d[\"targets\"]\n            \n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n          )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor epoch in range(EPOCHS):\n    \n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n    model,\n    train_loader,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_len\n  )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n    model,\n    val_loader,\n    loss_fn,\n    val_len\n  )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = 'model.pth'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(model,PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this is only till training of model i will do the submission code as soon as possible","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}