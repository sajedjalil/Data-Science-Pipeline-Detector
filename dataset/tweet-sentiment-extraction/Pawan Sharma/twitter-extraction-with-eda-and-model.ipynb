{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Necesary Library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\n#spacy.prefer_gpu()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/ import string\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nplt.rcParams['axes.grid']=True\nplt.gray()\n\nuse_cuda = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the data\nBASE_PATH = '/kaggle/input/tweet-sentiment-extraction/'\n\ntrain_df = pd.read_csv(BASE_PATH + 'train.csv')\ntest_df = pd.read_csv( BASE_PATH + 'test.csv')\nsubmission_df = pd.read_csv( BASE_PATH + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of train and test data\nprint(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Missing value in the training set\nprint(train_df.isnull().sum())\n# Checking Missing Value in the testing set\nprint(test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping the row with missing values\ntrain_df.dropna(axis = 0, how ='any',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive tweet\nprint(\"Positive Tweet example :\",train_df[train_df['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train_df[train_df['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train_df[train_df['sentiment']=='neutral']['text'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of the Sentiment Column\ntrain_df['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=train_df['sentiment'],data=train_df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['sentiment'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['sentiment'].value_counts(normalize=True).plot(kind='bar')\nplt.xlabel('Sentiments')\nplt.ylabel('Percentage')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying the cleaning function to both test and training datasets\ntrain_df['text_clean'] = train_df['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest_df['text_clean'] = test_df['text'].apply(str).apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyzing Text statistics\n\ntrain_df['text_len'] = train_df['text_clean'].astype(str).apply(len)\ntrain_df['text_word_count'] = train_df['text_clean'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create three separate dataframes for positive, neutral and negative sentiments. \n#This will help in analyzing the text statistics separately for separate polarities.\n\npos = train_df[train_df['sentiment']=='positive']\nneg = train_df[train_df['sentiment']=='negative']\nneutral = train_df[train_df['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.hist(pos['text_len'],bins=50,color='g')\nplt.title('Positive Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 2)\nplt.hist(neg['text_len'],bins=50,color='r')\nplt.title('Negative Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 3)\nplt.hist(neutral['text_len'],bins=50,color='y')\nplt.title('Neutral Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The histogram shows that the length of the cleaned text ranges from around 2 to 140 characters and generally,it is almost same for all the polarities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of top unigrams\npos_unigrams = get_top_n_words(pos['text_clean'],20)\nneg_unigrams = get_top_n_words(neg['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 unigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 unigram in Neutral text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of top Bigrams\npos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),20)\nneg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 Bigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 Bigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 Bigram in Neutral text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding top trigram\npos_trigrams = get_top_n_gram(pos['text_clean'],(3,3),20)\nneg_trigrams = get_top_n_gram(neg['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(pos_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 trigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(neg_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 trigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 trigram in Neutral text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Exploring the selected_text column\n\npositive_text = train_df[train_df['sentiment'] == 'positive']['selected_text']\nnegative_text = train_df[train_df['sentiment'] == 'negative']['selected_text']\nneutral_text = train_df[train_df['sentiment'] == 'neutral']['selected_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Positive text\nprint(\"Positive Text example :\",positive_text.values[0])\n#negative_text\nprint(\"Negative Tweet example :\",negative_text.values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",neutral_text.values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocess Selected_text\n\npositive_text_clean = positive_text.apply(lambda x: text_preprocessing(x))\nnegative_text_clean = negative_text.apply(lambda x: text_preprocessing(x))\nneutral_text_clean = neutral_text.apply(lambda x: text_preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_text_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_words_in_positive_text = get_top_n_words(positive_text_clean)\ntop_words_in_negative_text = get_top_n_words(negative_text_clean)\ntop_words_in_neutral_text = get_top_n_words(neutral_text_clean)\n\np1 = [x[0] for x in top_words_in_positive_text[:20]]\np2 = [x[1] for x in top_words_in_positive_text[:20]]\n\n\nn1 = [x[0] for x in top_words_in_negative_text[:20]]\nn2 = [x[1] for x in top_words_in_negative_text[:20]]\n\n\nnu1 = [x[0] for x in top_words_in_neutral_text[:20]]\nnu2 = [x[1] for x in top_words_in_neutral_text[:20]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top positive word\nsns.barplot(x=p1,y=p2,color = 'green')\nplt.xticks(rotation=45)\nplt.title('Top 20 Positive Word')\nplt.show()\n\nsns.barplot(x=n1,y=n2,color='red')\nplt.xticks(rotation=45)\nplt.title('Top 20 Negative Word')\nplt.show()\n\nsns.barplot(x=nu1,y=nu2,color='yellow')\nplt.xticks(rotation=45)\nplt.title('Top 20 Neutral Word')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/ekhtiar/unintended-eda-with-tutorial-notes\ndef generate_word_cloud(df_data, text_col):\n    # convert stop words to sets as required by the wordcloud library\n    stop_words = set(stopwords.words(\"english\"))\n    \n    data_neutral = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"neutral\", text_col].map(lambda x: str(x).lower()))\n    data_positive = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"positive\", text_col].map(lambda x: str(x).lower()))\n    data_negative = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"negative\", text_col].map(lambda x: str(x).lower()))\n\n    wc_neutral = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_neutral)\n    wc_positive = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_positive)\n    wc_negative = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_negative)\n\n    # draw the two wordclouds side by side using subplot\n    fig, ax = plt.subplots(1, 3, figsize=(20, 20))\n    ax[0].set_title(\"Neutral Wordcloud\" , fontsize=10)\n    ax[0].imshow(wc_neutral, interpolation=\"bilinear\")\n    ax[0].axis(\"off\")\n    \n    ax[1].set_title(\"Positive Wordcloud\", fontsize=10)\n    ax[1].imshow(wc_positive, interpolation=\"bilinear\")\n    ax[1].axis(\"off\")\n    \n    ax[2].set_title(\"Negative Wordcloud\", fontsize=10)\n    ax[2].imshow(wc_negative, interpolation=\"bilinear\")\n    ax[2].axis(\"off\")\n    plt.show()\n    \n    return [wc_neutral, wc_positive, wc_negative]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_wc = generate_word_cloud(train_df, \"text\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sel_text_wc = generate_word_cloud(train_df, \"selected_text\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Num_words_text'] = train_df['text'].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.loc[train_df['Num_words_text']>=3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(output_dir, nlp, new_model_name):\n    output_dir = f'../working/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(train_data, output_dir, n_iter=30, model=None):\n    \"\"\"Load the model,set up the pipeline and train the entity recognizer\"\"\"\n    if model is not None:\n        nlp=spacy.load(model) #load existing spaCy model\n        print(\"Loaded model '%s'\" %model)\n    else:\n        nlp = spacy.blank(\"en\") #create blank Language class\n        print(\"Created blank 'en' model \")\n        \n        # The pipeline execution\n        # Create the built-in pipeline components and them to the pipeline\n        # nlp.create_pipe works for built-ins that are registered in the spacy\n        \n        if \"ner\" not in nlp.pipe_names:\n            ner = nlp.create_pipe(\"ner\")\n            nlp.add_pipe(ner,last=True)\n            \n        # otherwise, get it so we can add labels\n        \n        else:\n            ner = nlp.get_pipe(\"ner\")\n            \n        # add labels \n        for _, annotations in train_data:\n                for ent in annotations.get(\"entities\"):\n                    ner.add_label(ent[2])\n        \n        # get names of other pipes to disable them during training\n        \n        pipe_exceptions = [\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        \n        #other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n        \n        with nlp.disable_pipes(*other_pipes): # training of only NER\n            \n            # reset and intialize the weights randoml - but only if we're\n            # training a model\n            \n            if model is None:\n                nlp.begin_training()\n            else:\n                nlp.resume_training()\n            \n            for itn in tqdm(range(n_iter)):\n                random.shuffle(train_data)\n                losses={}\n                \n                # batch up the example using spaCy's mnibatch\n                batches = minibatch(train_data,size=compounding(4.0,1000.0,1.001))\n                \n                for batch in batches:\n                    texts , annotations = zip(*batch)\n                    nlp.update(\n                        texts, #batch of texts\n                        annotations, # batch of annotations\n                        drop = 0.5,  # dropout - make it harder to memorise data\n                        losses = losses,\n                )\n            print(\"Losses\", losses)\n        save_model(output_dir, nlp, 'st_ner')\n                    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_out_path(sentiment):\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models/model_neg'\n    else:\n        model_out_path = 'models/model_neu'\n    return model_out_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_training_data(sentiment):\n    train_data=[]\n    '''\n    Returns Training data in the format needed to train spacy NER\n    '''\n    for index,row in train_df.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start,end,'selected_text']]}))\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Positive sentiments\nsentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# Training model iteration \ntrain(train_data, model_path, n_iter=10, model=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Negative Sentiment\n\nsentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# Training model iteration \ntrain(train_data, model_path, n_iter=10, model=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Neutral Sentiment\n\nsentiment = 'neutral'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# Training model iteration \ntrain(train_data, model_path, n_iter=10, model=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAINED_MODELS_BASE_PATH = '../input/tse-spacy-model/models/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nif TRAINED_MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", TRAINED_MODELS_BASE_PATH)\n    model_pos = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neg')\n    model_neu = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neu')\n        \n    jaccard_score = 0\n    for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n        text = row.text\n        if row.sentiment == 'neutral':\n            jaccard_score += jaccard(predict_entities(text, model_neu), row.selected_text)\n        elif row.sentiment == 'positive':\n            jaccard_score += jaccard(predict_entities(text, model_pos), row.selected_text)\n        else:\n            jaccard_score += jaccard(predict_entities(text, model_neg), row.selected_text) \n        \n    print(f'Average Jaccard Score is {jaccard_score / train_df.shape[0]}') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODELS_BASE_PATH = '../input/tse-spacy-model/models/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts = []\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n    model_neu = spacy.load(MODELS_BASE_PATH + 'model_neu')\n        \n    for index, row in test_df.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) < 2:\n#             output_str = text\n#             selected_texts.append(predict_entities(text, model_neu))\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ntest_df['selected_text'] = selected_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df['selected_text'] = test_df['selected_text']\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head(10))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}