{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is a modification of codes from two great existing kernels (see acknowledgement below), with early stopping added in the training part. While the public score is closer to the baseline model, I hope this will be helpful to anyone wishing to apply early stopping in the training of spaCy NER in this or other projects.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Acknowledgement: \n\n* https://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-ensemble\n\n* https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model\n\nReference: \n\n* https://spacy.io/usage/training","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport time\nfrom datetime import datetime\nimport nltk\nimport spacy\nfrom spacy.util import minibatch, compounding\nimport random\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_colwidth', -1)\nspacy.prefer_gpu()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_PATH = '../input/tweet-sentiment-extraction/'\nMODELS_BASE_PATH = '../working/models/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(BASE_PATH + 'train.csv')\ntest_df = pd.read_csv( BASE_PATH + 'test.csv')\nsubmission_df = pd.read_csv( BASE_PATH + 'sample_submission.csv')\n\ntrain_df = train_df.dropna()\ntrain_df = train_df.applymap(lambda x : x.strip())\ntest_df = test_df.applymap(lambda x : x.strip())\n\nprint(f\"train_df size: {len(train_df) :,}\")\nprint(f\"test_df size: {len(test_df) :,}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\ndef get_training_data(sentiment):\n    tmp_df = train_df[train_df.sentiment == sentiment]    \n    train_data = []\n    for textID, text, selected_text, _ in tmp_df.values:\n        start = text.find(selected_text)\n        end = start + len(selected_text)\n        train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data\n\n\ndef save_model(output_dir, nlp, new_model_name):\n    output_dir = f'{MODELS_BASE_PATH}{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        \ndef predict_entities(text, model):\n    doc = model(text)\n    start, end = -1, -1\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)  \n        break #We're only interested in the first one (and there should only be one!)\n        \n    if start > -1:\n        return text[start: end]\n    else:        \n        return text\n    \n    \ndef get_jaccard(data, model, to_print=False, N=-1):\n    jaccard_sum = 0\n    for text, annotations in data[:N]:\n        start = annotations['entities'][0][0]\n        end = annotations['entities'][0][1]\n        selected_text = text[start:end]\n        \n        pred = predict_entities(text, model)\n        jaccard_sum += jaccard(selected_text, pred)\n        \n        if to_print:\n            print(f\"{text}; 'predict': {pred}; 'actual': {selected_text}\")        \n        \n    jaccard_mean = jaccard_sum / len(data)\n    \n    return jaccard_mean\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code modified from https://spacy.io/usage/training\n\ndef train(train_data, sentiment, val_size=0.2, patience=2, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    pipe_exceptions = [\"ner\"]    \n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n    \n    # only train NER\n    with nlp.disable_pipes(*other_pipes) and warnings.catch_warnings():\n        # show warnings for misaligned entity spans once\n        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n\n        # reset and initialize the weights randomly â€“ but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n            \n        last_val_jaccard = 0\n        patience_ = 0\n        itn = 0\n            \n        while True:\n            random.shuffle(train_data)\n            losses = {}\n            \n            # Split into train and validation set\n            N = int(np.floor(len(train_data) * (1-val_size)))\n            train = train_data[:N]\n            val   = train_data[N:]\n            \n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(train, size=compounding(4.0, 30.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )      \n                    \n            #Calculate Jaccard for train and val dataset\n            train_jaccard = get_jaccard(train, nlp)\n            val_jaccard = get_jaccard(val, nlp)\n                                    \n            print(f\"iter {itn}. Losses: {losses['ner'] :.2f}; Train Jaccard: {train_jaccard :.4f}; Val Jaccard: {val_jaccard :.4f};\")\n                                                                \n            if val_jaccard > last_val_jaccard:\n                save_model(f\"{sentiment}\", nlp, f\"nlp_{sentiment}\")\n                last_val_jaccard = val_jaccard\n                patience_ = 0\n            else:\n                patience_ += 1   \n                print(f\"patience count increased to {patience_}\")                \n                                        \n            if patience_ >= patience:\n                break\n                \n            itn += 1\n                \n    return nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" Training the NER models, one for each sentiment\n\"\"\"\n\nto_train_model  = True\n# to_train_model  = False\n\n# model_tags = ['positive', 'negative', 'neutral']\nmodel_tags = ['positive', 'negative']\n\nval_size = 0.2 \npatience = 2\n\nif to_train_model:\n    t0 = time.time()\n   \n    for sentiment in model_tags:\n        print(f\"Training '{sentiment}' model:\")\n        train_data = get_training_data(sentiment)\n        nlp = train(train_data, sentiment, val_size=val_size, patience=patience)\n\n        print(f\"Training NER for '{sentiment}' takes {(time.time()-t0)/60 :.2f} minutes\")\n        t0 = time.time()\n        print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" Loading the models\n\"\"\"\n\nmodels = dict()\nfor sentiment in model_tags:\n    models[sentiment] = spacy.load(f\"{MODELS_BASE_PATH}{sentiment}\")    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\"\"\" Apply models to test data\n\"\"\"\n\nselected_texts = []\n\nfor textID, text, sentiment in test_df.values:\n    if sentiment == 'neutral' or len(text.split()) < 3:\n        selected_texts.append(text)    \n    else:\n        pred = predict_entities(text, models[sentiment])\n        selected_texts.append(pred)\n             \n# for textID, text, sentiment in test_df.values:    \n#     selected_texts.append(predict_entities(text, models[sentiment]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" Submission \n\"\"\"\nsubmission_df['selected_text'] = selected_texts\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head(10))\n\ndt_string = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\nprint(f\"Successfully submitted at {dt_string}\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}