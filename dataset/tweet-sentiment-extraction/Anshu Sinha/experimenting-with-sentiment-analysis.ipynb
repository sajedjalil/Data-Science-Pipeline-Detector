{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n**The given kernel explains the basic hands on with nlp. Those who are starting with nlp must first try  to pour in their knowledge in Sentiment Analysis. Those who are  familiar with sentiment analysis, you are good to try your learning on this dataset and for those who are naive,this article will give you a clear bird's eye view \n([https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17](http://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)).**\n\n**For the given dataset (will explain later what's it is about) spaCy's training model with ner(entity recognizer) is used. Further other nlp models such as BERT as a text classification can also be used. You can find the explanation of the above model in the given link ([https://spacy.io/usage/training](http://spacy.io/usage/training)) and for better understanding about entity recognizer ([https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da](http://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)).**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Articles worth to read before starting\n**Kaggle is a great platform for ML and data science enthusiasts that not only provide to try out your skills in different feature competitions but also a great community that lead you to the solution to your problem and enhance more of your skills.**\n**Such great articles are:**\n* **For understanding the EDA of the dataset:[https://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model](http://www.kaggle.com/tanulsingh077/twitter-sentiment-extaction-analysis-eda-and-model) **\n* **WORDSCLOUD FUNCTION -[https://www.kaggle.com/aashita/word-clouds-of-various-shapes](http://https://www.kaggle.com/aashita/word-clouds-of-various-shapes)**\n* **Training the model from spaCy NER on inputs [https://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-0-628-lb](http://www.kaggle.com/rohitsingh9990/ner-training-using-spacy-0-628-lb)**\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# About the Dataset \n**The dataset contains the training set , test set and sample set(on which submission should be made). For training set four columns are given**\n1. **text id**\n2. **text**\n3. **selected_text**\n4. **sentiment**\n\n**For a given text a selected text is given and the sentiment it describes. So training of data should be done in such a way that we model should return the selected text for a given sentiment and a text.**\n**The simple and naive way to approach this is to find the similarities between the text and selected text and learn the model using this and then test on the given test set.**\n\n**[https://medium.com/@adriensieg/text-similarities-da019229c894](http://medium.com/@adriensieg/text-similarities-da019229c894)**\n\n**The above mentioned link provide the basic understanding for such scenarios.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Helper functions**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport string\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport random\nimport spacy\nfrom spacy.util import compounding,minibatch\nfrom tqdm import trange\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data loaded to the kernel\ntrain = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nss = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training data\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping one null value in the training data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing the null data\ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test data\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of tweets for each sentiment**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of tweets in the training set\ntemp=train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generating the features**\n* The Jaccard index of the text and the selected text (it will depict the similarity between the text and the selected text). Follow up this discussion to know more.[https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/138520](http://www.kaggle.com/c/tweet-sentiment-extraction/discussion/138520)\n* Difference in number of words selected text and text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Defining Jaccard Index\ndef jaccard(str1,str2):\n    a=set(str1.lower().split())\n    b=set(str2.lower().split())\n    c=a.intersection(b)\n    return float(len(c)) / (len(a)+len(b)-len(c))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying the Jaccard index function \nresult=[]\n\nfor ind,row in train.iterrows():\n    sent1=row.text\n    sent2=row.selected_text\n    \n    jaccard_score=jaccard(sent1,sent2)\n    result.append([sent1,sent2,jaccard_score])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jaccard=pd.DataFrame(result,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain=train.merge(jaccard,how='outer')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Difference between the lengths of text and selected_text\ntrain['Num_of_words_T']=train['text'].apply(lambda x :len(str(x).split()))\ntrain['Num_of_words_ST']=train['selected_text'].apply(lambda x:len(str(x).split()))\ntrain['Diff_Num_of_words']=train['Num_of_words_T']-train['Num_of_words_ST']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note : By calculatig the jaccard index we infer that for neutral sentiment the text and selecte text are the same.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Cleaning of data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(text):\n    text=str(text).lower()\n    text=re.sub('\\[.*?\\]','',text)\n    text=re.sub(r'\\d+','',text)\n    text=re.sub('https?://\\S+|www\\.\\S+','',text)\n    text=re.sub('<.*?>+','',text)\n    text=re.sub('\\n','',text)\n    text=re.sub('\\w*\\d\\w*','',text)\n    text=re.sub('[%s]'% re.escape(string.punctuation),'',text)\n    return text\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train['text'].apply(lambda x:clean_data(x))\ntrain['selected_text']=train['selected_text'].apply(lambda x:clean_data(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization(Implementing the word clouds)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent =  train[train['sentiment']=='neutral']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wordcloud function (masked on a twitter logo image)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_wordcloud(text,mask=None,max_words=400,max_font_size=100,figure_size=(24.0,16.0),title=None,title_size=40,image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords={'u',\"im\"}\n    stopwords=stopwords.union(more_stopwords)\n    \n    wordcloud = WordCloud(background_color='white',\n                         stopwords = stopwords,max_words=max_words,\n                         max_font_size=max_font_size,random_state=42,mask=mask)\n    \n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation=\"bilinear\");\n        plt.title(title,fontdict={'size':title_size,\n                                  'verticalalignment':'bottom'})\n    else:\n            plt.imshow(wordcloud);\n            plt.title(title,fontdict={'size':title_size,'color':'red',\n                                     'verticalalignment':'bottom'})\n            plt.axis('off');\n    plt.tight_layout()  \n    \nd = '../input/imagetc/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wordclouds for positive,neutral,negative tweets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"twitter_mask=np.array(Image.open(d+'twitter.png'))\nplot_wordcloud(Neutral_sent.text,mask=twitter_mask,max_font_size=80,title_size=30,title=\"WordCloud for Neutral tweets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(Positive_sent.text,mask=twitter_mask,max_font_size=80,title_size=30,title=\"WordCloud for Positive tweets\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_wordcloud(Negative_sent.text,mask=twitter_mask,max_font_size=80,title_size=30,title=\"WordCloud for Negative tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndata_test = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\nsubmission = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train['Num_words_text'] = data_train['text'].apply(lambda x: len(str(x).split()))\ndata_train  = data_train[data_train['Num_words_text']>=3]\n                                              ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Saving the model to output directory**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(output_dir,nlp,new_model_name):\n    if output_dir is not None:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\",output_dir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Training the model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def training(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model,set up the pipeline and train the entity recognizer\"\"\"\n    if model is not None:\n        nlp=sapcy.load(model) #load existing spaCy model\n        print(\"Loaded model '%s'\" %model)\n    else:\n        nlp = spacy.blank(\"en\") #create blank Language class\n        print(\"Created blank 'en' model \")\n        \n        # The pipeline execution\n        # Create the built-in pipeline components and them to the pipeline\n        # nlp.create_pipe works for built-ins that are registered in the spacy\n        \n        if \"ner\" not in nlp.pipe_names:\n            ner = nlp.create_pipe(\"ner\")\n            nlp.add_pipe(ner,last=True)\n            \n        # otherwise, get it so we can add labels\n        \n        else:\n            ner = nlp.get_pipe(\"ner\")\n            \n        # add labels \n        for _, annotations in train_data:\n                for ent in annotations.get(\"entities\"):\n                    ner.add_label(ent[2])\n        \n        # get names of other pipes to disable them during training\n        \n        pipe_exceptions = [\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        \n        with nlp.disable_pipes(*other_pipes): # training of only NER\n            \n            # reset and intialize the weights randoml - but only if we're\n            # training a model\n            \n            if model is None:\n                nlp.begin_training()\n            else:\n                nlp.resume_training()\n            \n            for itn in trange(n_iter):\n                random.shuffle(train_data)\n                losses={}\n                \n                # batch up the example using spaCy's mnibatch\n                batches = minibatch(train_data,size=compounding(4.0,1000.0,1.001))\n                \n                for batch in batches:\n                    texts , annotations = zip(*batch)\n                    nlp.update(\n                        texts, #batch of texts\n                        annotations, # batch of annotations\n                        drop = 0.5,  # dropout - make it harder to memorise data\n                        losses = losses,\n                )\n            print(\"Losses\", losses)\n        save_model(output_dir, nlp, 'st_ner')\n                    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Return the model path**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_path(sentiment):\n    model_out_path = None \n    if sentiment == 'positive':\n        model_out_path = 'models/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models/model_neg'\n    return model_out_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_training_data(sentiment):\n    train_data=[]\n    \n    '''\n    Returns Training data in the format needed to train spacy NER\n    '''\n    for index,row in data_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start,end,'selected_text']]}))\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Training for Positive and Negative tweets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment ='positive'\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_path(sentiment)\ntraining(train_data,model_path,n_iter=3,model=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment ='negative'\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_path(sentiment)\ntraining(train_data,model_path,n_iter=3,model=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Predicting from the trained models**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(text,model):\n    doc=model(text)\n    ent_array=[]\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start,end,ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start,end,ent.label_])\n    selected_text = text[ent_array[0][0]:ent_array[0][1]] if len(ent_array)>0 else text\n    return selected_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_texts = []\nMODELS_BASE_PATH = '../input/tse-spacy-model/models/'\nif MODELS_BASE_PATH is not None:\n    print(\"Model is loading from\",MODELS_BASE_PATH)\n    models_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    models_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n    for index,row in data_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split())<=2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict(text,models_pos))\n        else:\n             selected_texts.append(predict(text,models_neg))\n    \ndata_test['selected_text']=selected_texts        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final result on sample data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['selected_text'] = data_test['selected_text']\nsubmission.to_csv(\"submission.csv\",index=False)\nprint(\"Hola Done\")\ndisplay(submission.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}