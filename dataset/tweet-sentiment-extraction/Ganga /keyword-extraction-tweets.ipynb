{"cells":[{"metadata":{},"cell_type":"markdown","source":"Extracting keywords is one of the most important tasks when working with text. Readers benefit from keywords because they can judge more quickly whether the text is worth reading.\n\nA typical keyword extraction algorithm has three main components:\n\nCandidate selection: Here, we extract all possible words, phrases, terms or concepts (depending on the task) that can potentially be keywords.\nProperties calculation: For each candidate, we need to calculate properties that indicate that it may be a keyword. For example, a candidate appearing in the title of a book is a likely keyword.\nScoring and selecting keywords: All candidates can be scored by either combining the properties into a formula, or using a machine learning technique to determine probability of a candidate being a keyword. A score or probability threshold, or a limit on the number of keywords is then used to select the final set of keywords..\n\nThe focus of this post is a keyword extraction algorithm called Rapid Automatic Keyword Extraction (RAKE). \n\nThe hallmarks of the RAKE algorithm are\n\nits ability to operate independently on documents without referring to a corpus (domain independence); and\nits very reasonable precision despite its simplicity and computational efficiency.\n\nThe entire algorithm is as follows.\n\nGiven an input document on which we want to extract keywords,\n\n1.Split the document into an array of words, breaking it at word delimiters (like spaces and punctuation).\n2.Split the words into sequences of contiguous words, breaking each sequence at a stopword. Each sequence is now a “candidate keyword”.\n3.Calculate the “score” of each indivudual word in the list of candidate keywords. This is calculated using the metric : degree(word)/frequency(word)\n4.For each candidate keyword, add the word scores of its constituent words to find the candidate keyword score.\n5.Take the first one-third highest scoring candidates from the list of candidates as the final list of extracted keywords.\n\nLet me jump into out code below :)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install mglearn","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom textblob import TextBlob\nimport re\nimport itertools\nimport datetime\nimport csv\n\n# Download Wordnet through NLTK in python console:\nimport nltk\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport unidecode\nimport string\n\nfrom nltk.probability import FreqDist\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n%matplotlib inline\n#from plotly import graph_objs as go\n#import plotly.express as px\n#import plotly.figure_factory as ff\n\n#sentiment analyser packages\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC  \nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\n#import mglearn\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\n#import fasttext\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ntrain_df = train_df[train_df['text'].notna()]\n#train_df = train_df.head(1000)\ntrain_df = train_df.reset_index()\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Function to classify sentiment**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_check(word, list):\n    if word in list:\n        return 1\n    else:\n        return 0\n    \ndef word_cooccurance(word1,word2,candi_kw_lst):\n    value = 0\n    for k in range(len(candi_kw_lst)) :\n        value  = value + check_both(word1,word2,candi_kw_lst[k])\n    \n    return value\n\ndef check_both(word1, word2 , list): \n    if word1 in list:\n        if word2 in list:\n            return 1\n        else:\n            return 0\n    else:\n        return 0\n    \ndef word_freq(word):\n    return flat_list_keyword.count(word)\n    \n\ndef strip_links(text):\n    text = str(text)\n    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], '')    \n    return text\n\n\n\ndef get_tweet_sentiment(tweet): \n    ''' \n    Utility function to classify sentiment of passed tweet \n    using textblob's sentiment method \n    '''\n    # create TextBlob object of passed tweet text \n    analysis = TextBlob(clean_tweet(tweet)) \n    # set sentiment \n    if analysis.sentiment.polarity > 0: \n        return 'positive'\n    elif analysis.sentiment.polarity == 0: \n        return 'neutral'\n    else: \n        return 'negative'\n    \ndef clean_tweet(tweet): \n    ''' \n    Utility function to clean tweet text by removing links, special characters \n    using simple regex statements. \n    '''\n    tweet = str(tweet)\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n\ndef textblob_sentiment(tweet):\n    pol_score = TextBlob(tweet).sentiment.polarity\n    if pol_score > 0: \n        return 'positive'\n    elif pol_score == 0: \n        return 'neutral'\n    else: \n        return 'negative'\n\ndef vader_sentiment(tweet):\n    senti = SentimentIntensityAnalyzer()\n    compound_score = senti.polarity_scores(tweet)['compound']\n    \n    # set sentiment \n    if compound_score >= 0.05: \n        return 'positive'\n    elif (compound_score > -0.05) and (compound_score < 0.05): \n        return 'neutral'\n    else: \n        return 'negative'\n    \n\n\n\ndef document_features(document):\n    document_words = set(document)\n    features = {}\n    for word in word_features:\n        features['contains({})'.format(word)] = (word in document_words)\n    return features\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## text preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\n\nappos = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"i would\",\n\"i'd\" : \"i had\",\n\"i'll\" : \"i will\",\n\"i'm\" : \"i am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"i have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"\n}\n\n\n\ndef text_preprocess(text):\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    \n    text = str(text)\n    \n    #removing mentions and hashtags\n\n    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", text).split())\n    \n    #remove http links from tweets\n    \n    \n    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], '')  \n    \n    text_pattern = re.sub(\"`\", \"'\", text)\n    \n    #fix misspelled words\n\n    '''Here we are not actually building any complex function to correct the misspelled words but just checking that each character \n    should occur not more than 2 times in every word. It’s a very basic misspelling check.'''\n\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    \n    \n   # print(text_pattern)\n    \n    #Convert to lower and negation handling\n    \n    text_lr = text_pattern.lower()\n    \n   # print(text_lr)\n    \n    words = text_lr.split()\n    text_neg = [appos[word] if word in appos else word for word in words]\n    text_neg = \" \".join(text_neg) \n   # print(text_neg)\n    \n    #remove stopwords\n    \n    tokens = word_tokenize(text_neg)\n    text_nsw = [i for i in tokens if i not in stop_words]\n    text_nsw = \" \".join(text_nsw) \n   # print(text_nsw)\n    \n    \n    #remove tags\n    \n    text_tags=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text_nsw)\n\n    # remove special characters and digits\n    text_alpha=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_tags)\n    \n    #Remove accented characters\n    text = unidecode.unidecode(text_alpha)\n    \n    '''#Remove punctuation\n    table = str.maketrans('', '', string.punctuation)\n    text = [w.translate(table) for w in text.split()]'''\n    \n    sent = TextBlob(text)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n   \n    return \" \".join(lemmatized_list)\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['processed_text'] = None\n#train_df['clean_text2'] = None\n\nfor i in range(len(train_df)):\n    train_df.processed_text[i] = text_preprocess(train_df.text[i])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Is the dataset Balanced?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nax = train_df['sentiment'].value_counts(sort=False).plot(kind='barh')\nax.set_xlabel('Number of Samples in training Set')\nax.set_ylabel('Label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Polarity ==  negative\ntrain_s0 = train_df[train_df.sentiment == 'negative']\nall_text = ' '.join(word for word in train_s0.processed_text)\nwordcloud_neg = WordCloud(colormap='Reds', width=1000, height=1000, background_color='white').generate(all_text) #mode='RGBA'\nplt.figure(figsize=(20,10))\nplt.title('Negative sentiment - Wordcloud')\nplt.imshow(wordcloud_neg, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_neg.to_file('negative_senti_wordcloud.jpg')\n\n# Polarity ==  neutral\ntrain_s1 = train_df[train_df.sentiment == 'neutral']\nall_text = ' '.join(word for word in train_s1.processed_text)\nwordcloud_neu = WordCloud(width=1000, height=1000, colormap='Blues', background_color='white').generate(all_text)\nplt.figure( figsize=(20,10))\nplt.title('Neutral sentiment - Wordcloud')\nplt.imshow(wordcloud_neu, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_neu.to_file('neutral_senti_wordcloud.jpg')\n\n# Polarity ==  positive\ntrain_s2 = train_df[train_df.sentiment  == 'positive']\nall_text = ' '.join(word for word in train_s2.processed_text)\nwordcloud_pos = WordCloud(width=1000, height=1000, colormap='Wistia',background_color='white').generate(all_text)\nplt.figure(figsize=(20,10))\nplt.title('Positive sentiment - Wordcloud')\nplt.imshow(wordcloud_pos, interpolation='bilinear')\nplt.axis(\"off\")\nplt.margins(x=0, y=0)\nplt.show()\n\nwordcloud_pos.to_file('positive_senti_wordcloud.jpg')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most commonly discussed words in all wordcloud images are 'Work' and 'go'.\n\nNegative tweets are having words : sorry , miss , lose\n\nNeutral tweets words : time, good,think and watch\n\nPositive tweets words :  Love , thanks , mother","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below we are printing wordclouds for most frequently occuring bigrams in Tweets. It has a combiniation of all tweets irrespective of sentiment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud for frequently occuring bigrams\n\nimport nltk\nfrom string import digits\n\n# Load default stop words and add a few more.\n\nstopwordsList = []\n \n# Load default stop words and add a few more specific to my text.\nstopwordsList = stopwords.words('english')\nstopwordsList.append('dont')\nstopwordsList.append('didnt')\nstopwordsList.append('doesnt')\nstopwordsList.append('cant')\nstopwordsList.append('couldnt')\nstopwordsList.append('couldve')\nstopwordsList.append('im')\nstopwordsList.append('ive')\nstopwordsList.append('isnt')\nstopwordsList.append('theres')\nstopwordsList.append('wasnt')\nstopwordsList.append('wouldnt')\nstopwordsList.append('a')\nstopwordsList.append('also')\nstopwordsList.append('rt')\n\n\nWNL = nltk.WordNetLemmatizer()\n\ntext_content = train_df['processed_text']\n\n# After the punctuation above is removed it still leaves empty entries in the list.\ntext_content = [s for s in text_content if len(s) != 0]\n\n# Best to get the lemmas of each word to reduce the number of similar words\ntext_content = [WNL.lemmatize(t) for t in text_content]\n\n\n#nltk_tokens = nltk.word_tokenize(text)  \n\nbigrams_list = list(nltk.bigrams(text_content))\n#print(bigrams_list)\n\ndictionary2 = [' '.join(tup) for tup in bigrams_list]\n#print (dictionary2)\n\nvectorizer = CountVectorizer(ngram_range=(2, 2))\nbag_of_words = vectorizer.fit_transform(dictionary2)\nvectorizer.vocabulary_\nsum_words = bag_of_words.sum(axis=0) \nwords_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\nwords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n#print (words_freq[:100])\nwords_dict = dict(words_freq)\n\nWC_height = 1000\nWC_width = 1500\nWC_max_words = 200\n\nwordCloud = WordCloud(max_words=WC_max_words, height=WC_height, width=WC_width,stopwords=stopwordsList,background_color='white')\n\nwordCloud.generate_from_frequencies(words_dict)\n\nplt.figure(figsize=(20,10))\nplt.title('Most frequently occurring bigrams connected by colour')\nplt.imshow(wordCloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\nwordCloud.to_file('wordcloud_freq_bigrams.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hashtags ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting hashtags from positive tweets\n\nHT_positive = hashtag_extract(train_df['text'][train_df['sentiment'] == 'positive'])\n\n# extracting hashtags from negative tweets\nHT_negative = hashtag_extract(train_df['text'][train_df['sentiment'] == 'negative'])\n\n# extracting hashtags from neutral tweets\nHT_neutral = hashtag_extract(train_df['text'][train_df['sentiment'] == 'neutral'])\n\n\n# unnesting list\nHT_positive = sum(HT_positive,[])\nHT_negative = sum(HT_negative,[])\nHT_neutral = sum(HT_neutral,[])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hashtags contributing to positive tweets\n\na = nltk.FreqDist(HT_positive)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nplt.title('Hashtags contributing to positive tweets')\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hashtags contributing to negative tweets\n\nb = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n# selecting top 10 most frequent hashtags\ne = e.nlargest(columns=\"Count\", n = 10)   \nplt.figure(figsize=(16,5))\nplt.title('Hashtags contributing to negative tweets')\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hashtags contributing to neutral tweets\n\nb = nltk.FreqDist(HT_neutral)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n\n# selecting top 10 most frequent hashtags\ne = e.nlargest(columns=\"Count\", n = 10)   \nplt.figure(figsize=(16,5))\nplt.title('Hashtags contributing to neutral tweets')\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bag of word & TFIDF features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# bag-of-words feature matrix\nbow = bow_vectorizer.fit_transform(train_df['text'])\n\ntop_sum=bow.toarray().sum(axis=0)\ntop_sum_cv=[top_sum]#to let pandas know that these are rows\ncolumns_cv = bow_vectorizer.get_feature_names()\nx_traincvdf = pd.DataFrame(top_sum_cv,columns=columns_cv)\n\n\nimport operator\ndic = {}\nfor i in range(len(top_sum_cv[0])):\n    dic[columns_cv[i]]=top_sum_cv[0][i]\nsorted_dic=sorted(dic.items(),reverse=True,key=operator.itemgetter(1))\nprint(sorted_dic[1:])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nsorted_dic = sorted_dic[:15]\n\nxs, ys = [*zip(*sorted_dic)]\n\n\nplt.figure(figsize=(10,8))\nplt.bar(xs, ys)\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Top words - Count Vectorizer')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(train_df['text'])\n\ntop_sum=tfidf.toarray().sum(axis=0)\ntop_sum_tfidf=[top_sum]#to let pandas know that these are rows\ncolumns_tfidf = tfidf_vectorizer.get_feature_names()\nx_traintfidf_df = pd.DataFrame(top_sum_tfidf,columns=columns_tfidf)\n\n\nimport operator\ndic = {}\nfor i in range(len(top_sum_tfidf[0])):\n    dic[columns_cv[i]]=top_sum_tfidf[0][i]\nsorted_dic=sorted(dic.items(),reverse=True,key=operator.itemgetter(1))\nprint(sorted_dic[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nsorted_dic = sorted_dic[:15]\n\nxs, ys = [*zip(*sorted_dic)]\n\n\nplt.figure(figsize=(10,8))\nplt.bar(xs, ys)\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.title('Top words - Count Vectorizer')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.countplot(x='sentiment',data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test dataset results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading test data\ntest_df = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\ntest_df = test_df[test_df['text'].notna()]\ntest_df = test_df.reset_index()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keyword extraction using RAKE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom collections import OrderedDict \n\nimport nltk\nfrom nltk.corpus import stopwords\nstopwords_en = set(stopwords.words('english'))\n\n\ntext = 'my boss is bullying me...'\nprint (text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''step 1 : lower each word and tokenize each word in sentences (text)'''\n\ntext = text.lower()\ntext = clean_tweet(text)\n\nprint (text)\n\ntokenized_sents = word_tokenize(text)\nprint (tokenized_sents)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''step 2 : a)Split by delimiters       b)Split by stop word        c)Candidate Keyword'''\n\ncandi_kw = []\ncandi_kw_lst = []\n\nfor i in tokenized_sents:\n    \n    if i not in stopwords_en:\n        candi_kw.append(i)\n    else:\n        if len(candi_kw) == 0:\n            print ('stopword')\n        else:\n            candi_kw_lst.append(candi_kw) \n            candi_kw = []\ncandi_kw_lst.append(candi_kw)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flat_list_keyword = [item for sublist in candi_kw_lst for item in sublist]\nflat_unique_keyword = set(flat_list_keyword)\n\nflat_unique_list = [] \n      \n# traverse for all elements \nfor x in flat_list_keyword: \n    # check if exists in unique_list or not \n    if x not in flat_unique_list: \n        flat_unique_list.append(x) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing cooccurance matrix\n\nmatrix_df = pd.DataFrame(0, columns=flat_unique_list, index=flat_unique_list)\nmatrix_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"candi_kw_lst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating co-occurance matrix\n\nj = 0\n\nfor i in range(len(matrix_df)):\n    for j in range(len(matrix_df.columns)):\n        if (matrix_df.index[i] == matrix_df.columns[j]):\n            matrix_df.iloc[i,j] = flat_list_keyword.count(matrix_df.index[j])\n        else:\n            value = 0\n            for k in range(len(candi_kw_lst)) :\n                value  = value + check_both(matrix_df.index[i],matrix_df.columns[j],candi_kw_lst[k])\n            matrix_df.iloc[i,j] = value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sum row wise and create column for word degree\n\n'''# find degree of word \n\nWord Degree (deg(w)) = word_freq+ # howmany times a word has a interaction with other words\n\n'''\n\nmatrix_df['degree'] = matrix_df.sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word_frequency\n\n'''# find frequency of word\n\nWord Frequency (freq(w)) # how many times a particular word appeared among all candidate keywords.\n\nKeyword score = (deg(w)/freq(w))'''\n\nmatrix_df['word_frequency'] = None\n\nfor i in range(len(matrix_df)):\n    \n    matrix_df.word_frequency[i] = word_freq(matrix_df.index[i])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate keyword score\n\nmatrix_df['keyword_score'] = None\n\nfor i in range(len(matrix_df)):\n    \n    matrix_df.keyword_score[i] = matrix_df.degree[i] / matrix_df.word_frequency[i]\n\nmatrix_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting keyword_score for each word\n\nmatrix_dict = matrix_df.to_dict()['keyword_score']\nmatrix_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate keyword score for candidate keywords\ncandi_kw_score = {}\n\nfor i in range(len(candi_kw_lst)):\n    score = 0\n    for j in range(len(candi_kw_lst[i])):\n        key_name = str(candi_kw_lst[i])\n        score = score + matrix_dict[candi_kw_lst[i][j]]\n    \n    candi_kw_score [key_name] = score\n    \n\ncandi_kw_score = {k: v for k, v in sorted(candi_kw_score.items(), key=lambda item: item[1],reverse=True)}\n\ncandi_kw_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract top 3 scored candidate keywords\n\nn_items = dict(itertools.islice(candi_kw_score.items(), 4)) \n\nfinal_phrase = []\n\nfor x in list(n_items)[0:3]:\n    final_phrase.append(x)\n\n# removing special characters\n\nremovetable = str.maketrans('', '', \"@,#%[]'\")\nout_list = [s.translate(removetable) for s in final_phrase]\n\nextracted_keywords = ' '.join(out_list)\nextracted_keywords","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are combining above steps and going to implement the above process into test data set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_keywords(text):\n    \n    global key_name\n    \n    text = text.lower()\n    \n    text = clean_tweet(text)\n\n    tokenized_sents = word_tokenize(text)\n    \n    candi_kw = []\n    candi_kw_lst = []\n\n    for i in tokenized_sents:\n\n        if i not in stopwords_en:\n            candi_kw.append(i)\n        else:\n            if len(candi_kw) == 0:\n                print ('Processing')\n            else:\n                candi_kw_lst.append(candi_kw) \n                candi_kw = []\n    candi_kw_lst.append(candi_kw)\n    \n    flat_list_keyword = [item for sublist in candi_kw_lst for item in sublist]\n    flat_unique_keyword = set(flat_list_keyword)\n\n    flat_unique_list = [] \n\n    # traverse for all elements \n    for x in flat_list_keyword: \n        # check if exists in unique_list or not \n        if x not in flat_unique_list: \n            flat_unique_list.append(x) \n            \n    # initializing cooccurance matrix\n\n    matrix_df = pd.DataFrame(0, columns=flat_unique_list, index=flat_unique_list)\n\n    \n    # creating co-occurance matrix\n\n    j = 0\n\n    for i in range(len(matrix_df)):\n        for j in range(len(matrix_df.columns)):\n            if (matrix_df.index[i] == matrix_df.columns[j]):\n                matrix_df.iloc[i,j] = flat_list_keyword.count(matrix_df.index[j])\n            else:\n                value = 0\n                for k in range(len(candi_kw_lst)) :\n                    value  = value + check_both(matrix_df.index[i],matrix_df.columns[j],candi_kw_lst[k])\n                matrix_df.iloc[i,j] = value\n\n        # sum row wise and create column for word degree\n\n    '''# find degree of word \n\n    Word Degree (deg(w)) = word_freq+ # howmany times a word has a interaction with other words\n\n    '''\n\n    matrix_df['degree'] = matrix_df.sum(axis=1)\n    \n    # word_frequency\n\n    '''# find frequency of word\n\n    Word Frequency (freq(w)) # how many times a particular word appeared among all candidate keywords.\n\n    Keyword score = (deg(w)/freq(w))'''\n\n    matrix_df['word_frequency'] = None\n\n    for i in range(len(matrix_df)):\n\n        matrix_df.word_frequency[i] = word_freq(matrix_df.index[i])\n\n    \n        # calculate keyword score\n\n    matrix_df['keyword_score'] = None\n\n    for i in range(len(matrix_df)):\n\n        matrix_df.keyword_score[i] = matrix_df.degree[i] / matrix_df.word_frequency[i]\n\n   \n    #getting keyword_score for each word\n\n    matrix_dict = matrix_df.to_dict()['keyword_score']\n    \n    #calculate keyword score for candidate keywords\n    \n    candi_kw_score = {}\n\n    for i in range(len(candi_kw_lst)):\n        score = 0\n        for j in range(len(candi_kw_lst[i])):\n            key_name = str(candi_kw_lst[i])\n            score = score + matrix_dict[candi_kw_lst[i][j]]\n        candi_kw_score [key_name] = score\n\n\n    candi_kw_score = {k: v for k, v in sorted(candi_kw_score.items(), key=lambda item: item[1],reverse=True)}\n    \n    # extract top 3 scored candidate keywords\n\n    n_items = dict(itertools.islice(candi_kw_score.items(), 4)) \n\n    final_phrase = []\n\n    for x in list(n_items)[0:3]:\n        final_phrase.append(x)\n\n    # removing special characters\n\n    removetable = str.maketrans('', '', \"@,#%[]'\")\n    out_list = [s.translate(removetable) for s in final_phrase]\n\n    extracted_keywords = ' '.join(out_list)\n    \n    return extracted_keywords\n\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train set prediction based on RAKE algo:\n\ntrain_df['predicted_text'] = None\n\nfor i in range(len(train_df)):\n    train_df.predicted_text[i] = extract_keywords(train_df.text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluation on train set\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\ntrain_df['jaccard'] = None\n\nfor i in range(len(train_df)):\n    train_df.jaccard[i] = jaccard(train_df.selected_text[i],train_df.predicted_text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"jaccard\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test set prediction \n\ntest_df ['selected_text'] = None\n\nfor i in range(len(test_df)):\n    test_df.selected_text[i] = extract_keywords(test_df.text[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove column name 'A' \ntest_df1 = test_df.drop(['index','text','sentiment'], axis = 1) \ntest_df1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df1.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Future work : Highlight the selected words in tweet text and extend the algorithm to summarize the article (Sentences needs to selected instead of words based on score)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}