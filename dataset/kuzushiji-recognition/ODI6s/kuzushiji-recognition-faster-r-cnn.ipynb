{"cells":[{"metadata":{"_uuid":"3e5f9039-6c3d-4338-8459-240344fa38e7","_cell_guid":"c64750cd-92ce-4a96-af4b-ca1041756fc8","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #print(dirname)\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0437706-1e8b-4635-9a4c-223d7b514d47","_cell_guid":"eb7aad87-2fbf-4eb8-af67-3f0c0401104d","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n#%matplotlib inline\nfrom matplotlib import patches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path = \"/kaggle/input/kuzushiji-recognition/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea4b3977-ecae-4f57-b713-bdffd0f48c6a","_cell_guid":"4efc4afe-0a6a-4423-960d-9e82da599d16","trusted":true},"cell_type":"code","source":"# import data\nraw_train_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\nraw_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cff635b9-dfe3-4a3c-b531-3e7a781e374c","_cell_guid":"cb3220ae-57ea-4f05-98bd-ca1593ed4e1c","trusted":true},"cell_type":"code","source":"# reformat with just the first label \ntrain_df = raw_train_df.drop('labels', 1).join(raw_train_df[\"labels\"].str.split(\" \", n=5, expand = True).drop(5,1))\ntrain_df.columns = ['FileName','ClassName','XMin','YMin','width','height']\ntrain_df['FileName'] = train_df['FileName'].astype(str) + '.jpg'\ntrain_df['ClassName'] = train_df['ClassName'].fillna('none')\ntrain_df['XMin'] = train_df['XMin'].fillna(0).astype(int)\ntrain_df['YMin'] = train_df['YMin'].fillna(0).astype(int)\ntrain_df['width'] = train_df['width'].fillna(0).astype(int)\ntrain_df['height'] = train_df['height'].fillna(0).astype(int)\ntrain_df['XMax'] = train_df['XMin']+train_df['width']\ntrain_df['YMax'] = train_df['YMin']+train_df['height']\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e2aa2b7-f25c-4742-89ca-dcee026726b5","_cell_guid":"f64472f5-a90b-4cf8-b172-b8b71a139c78","trusted":true},"cell_type":"code","source":"# Number of unique training images\ntrain_df['FileName'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9bcaa66-e36a-4d96-b149-a067beb42663","_cell_guid":"5ab3aaf5-b6a1-492b-a640-0c8b8b9627dd","trusted":true},"cell_type":"code","source":"# show training image and boxes\n\nfig = plt.figure()\n\n#add axes to the image\nax = fig.add_axes([0,0,1,1])\n\n# read and plot the image\ntop_image = plt.imread(base_path + 'train_images/100241706_00004_2.jpg')\nplt.imshow(top_image)\n\n# iterating over the image for different objects\nfor _,row in train_df[train_df.FileName == \"100241706_00004_2.jpg\"].iterrows():\n    XMin = row.XMin\n    YMin = row.YMin\n    width = row.width\n    height = row.height\n    \n    # add bounding boxes to the image\n    rect = patches.Rectangle((XMin,YMin), width, height, edgecolor = 'r', facecolor = 'none')\n    ax.add_patch(rect)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Code Set-up**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nimport random\nimport pprint\nimport sys\nimport time\nimport numpy as np\nfrom optparse import OptionParser\nimport pickle\nimport math\nimport cv2\nimport copy\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport os\n\nfrom sklearn.metrics import average_precision_score\n\nfrom keras import backend as K\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\nfrom keras.engine.topology import get_source_inputs\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.objectives import categorical_crossentropy\n\nfrom keras.models import Model\nfrom keras.utils import generic_utils\nfrom keras.engine import Layer, InputSpec\nfrom keras import initializers, regularizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from faster_r_cnn_function_script import Config, RoiPoolingConv\nfrom faster_r_cnn_function_script import get_anchor_gt, get_img_output_length, nn_base, rpn_layer, classifier_layer\nfrom faster_r_cnn_function_script import rpn_loss_regr, rpn_loss_cls, class_loss_regr, class_loss_cls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Start Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_rois = 4 # Number of RoIs to process at once.\n\n# Augmentation flag\nhorizontal_flips = True # Augment with horizontal flips in training. \nvertical_flips = True   # Augment with vertical flips in training. \nrot_90 = True           # Augment with 90 degree rotations in training. \n\noutput_weight_path = os.path.join(base_path, 'model/model_frcnn_vgg.hdf5')\n\nrecord_path = os.path.join(base_path, 'model/record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n\nbase_weight_path = os.path.join(base_path, 'model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n\nconfig_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the config\nC = Config()\n\nC.use_horizontal_flips = horizontal_flips\nC.use_vertical_flips = vertical_flips\nC.rot_90 = rot_90\n\nC.record_path = record_path\nC.model_path = output_weight_path\nC.num_rois = num_rois\n\nC.base_net_weights = base_weight_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# userp def get_data(input_path): ...... return all_data, classes_count, class_mapping\n\nclasses_count_df = train_df['ClassName'].value_counts()\nclasses_count_df = classes_count_df.rename_axis('ClassName').reset_index(name='counts')\nclasses_count = {}\nfor label_no in range(len(classes_count_df)-1):\n    class_name = classes_count_df['ClassName'][label_no]\n    count = classes_count_df['counts'][label_no]\n    classes_count[class_name] = count\n\nclass_mapping = {}\nfor label_no in range(len(classes_count_df)-1):\n    class_name = classes_count_df['ClassName'][label_no]\n    class_mapping[class_name] = label_no\n\nall_imgs = {}\nfor file_no in range(len(train_df)):\n    filename = train_df['FileName'][file_no]\n    class_name = train_df['ClassName'][file_no]\n    x1 = train_df['XMin'][file_no]\n    x2 = train_df['XMax'][file_no]\n    y1 = train_df['YMin'][file_no]\n    y2 = train_df['YMax'][file_no]\n    if filename not in all_imgs:\n        all_imgs[filename] = {}\n        img = cv2.imread((base_path + 'train_images/' + row['FileName']))\n        (rows,cols) = img.shape[:2]\n        all_imgs[filename]['filepath'] = filename\n        all_imgs[filename]['width'] = cols\n        all_imgs[filename]['height'] = rows\n        all_imgs[filename]['bboxes'] = []\n    all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\ntrain_imgs = []\nfor key in all_imgs:\n    train_imgs.append(all_imgs[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if 'bg' not in classes_count:\n\tclasses_count['bg'] = 0\n\tclass_mapping['bg'] = len(class_mapping)\n# e.g.\n#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}\n#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\nC.class_mapping = class_mapping\n\nprint('Training images per class:')\npprint.pprint(classes_count)\nprint('Num classes (including bg) = {}'.format(len(classes_count)))\nprint(class_mapping)\n\n# Save the configuration\n#with open(config_output_filename, 'wb') as config_f:\n#\tpickle.dump(C,config_f)\n#\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the images with seed\nrandom.seed(1)\nrandom.shuffle(train_imgs)\n\nprint('Num train samples (images) {}'.format(len(train_imgs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get train data generator which generate X, Y, image_data\ndata_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explore 'data_gen_train'**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y, image_data, debug_img, debug_num_pos = next(data_gen_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape_img = (None, None, 3)\n\nimg_input = Input(shape=input_shape_img)\nroi_input = Input(shape=(None, 4))\n\n# define the base network (VGG here, can be Resnet50, Inception, etc)\nshared_layers = nn_base(img_input, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nvgg16_model = VGG16(weights='imagenet', include_top=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the RPN, built on the base layers\nnum_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios) # 9\nrpn = rpn_layer(shared_layers, num_anchors)\n\nclassifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n\nmodel_rpn = Model(img_input, rpn[:2])\nmodel_classifier = Model([img_input, roi_input], classifier)\n\n# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\nmodel_all = Model([img_input, roi_input], rpn[:2] + classifier)\n\n# Because the google colab can only run the session several hours one time (then you need to connect again), \n# we need to save the model and load the model to continue training\nif not os.path.isfile(C.model_path):\n    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n    try:\n        print('This is the first time of your training')\n        print('loading weights from VGG16 model')\n        model_rpn.set_weights(vgg16_model.get_weights()) #load_weights(C.base_net_weights, by_name=True)\n        model_classifier.set_weights(vgg16_model.get_weights()) #load_weights(C.base_net_weights, by_name=True)\n    except:\n        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n            https://github.com/fchollet/keras/tree/master/keras/applications')\n    \n    # Create the record.csv file to record losses, acc and mAP\n    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\nelse:\n    # If this is a continued training, load the trained model from before\n    print('Continue training based on previous trained model')\n    print('Loading weights from {}'.format(C.model_path))\n    model_rpn.load_weights(C.model_path, by_name=True)\n    model_classifier.load_weights(C.model_path, by_name=True)\n    \n    # Load the records\n    record_df = pd.read_csv(record_path)\n\n    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n    r_class_acc = record_df['class_acc']\n    r_loss_rpn_cls = record_df['loss_rpn_cls']\n    r_loss_rpn_regr = record_df['loss_rpn_regr']\n    r_loss_class_cls = record_df['loss_class_cls']\n    r_loss_class_regr = record_df['loss_class_regr']\n    r_curr_loss = record_df['curr_loss']\n    r_elapsed_time = record_df['elapsed_time']\n    r_mAP = record_df['mAP']\n\n    print('Already train %dK batches'% (len(record_df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=1e-5)\noptimizer_classifier = Adam(lr=1e-5)\nmodel_rpn.compile(optimizer=optimizer, loss=[rpn_loss_cls(num_anchors), rpn_loss_regr(num_anchors)])\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=[class_loss_cls, class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\nmodel_all.compile(optimizer='sgd', loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training setting\ntotal_epochs = len(record_df)\nr_epochs = len(record_df)\n\nepoch_length = 1000\nnum_epochs = 40\niter_num = 0\n\ntotal_epochs += num_epochs\n\nlosses = np.zeros((epoch_length, 5))\nrpn_accuracy_rpn_monitor = []\nrpn_accuracy_for_epoch = []\n\nif len(record_df)==0:\n    best_loss = np.Inf\nelse:\n    best_loss = np.min(r_curr_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(record_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\nfor epoch_num in range(num_epochs):\n\n    progbar = generic_utils.Progbar(epoch_length)\n    print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n    \n    r_epochs += 1\n\n    while True:\n#        try:\n\n            if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n                rpn_accuracy_rpn_monitor = []\n#                 print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n                if mean_overlapping_bboxes == 0:\n                    print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n\n            # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n            X, Y, img_data, debug_img, debug_num_pos = next(data_gen_train)\n\n            # Train rpn model and get loss value [_, loss_rpn_cls, loss_rpn_regr]\n            loss_rpn = model_rpn.train_on_batch(X, Y)\n\n            # Get predicted rpn from rpn model [rpn_cls, rpn_regr]\n            P_rpn = model_rpn.predict_on_batch(X)\n\n            # R: bboxes (shape=(300,4))\n            # Convert rpn layer to roi bboxes\n            R = rpn_to_roi(P_rpn[0], P_rpn[1], C, K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n            \n            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n            # X2: bboxes that iou > C.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n            # Y1: one hot code for bboxes from above => x_roi (X)\n            # Y2: corresponding labels and corresponding gt bboxes\n            X2, Y1, Y2, IouS = calc_iou(R, img_data, C, class_mapping)\n\n            # If X2 is None means there are no matching bboxes\n            if X2 is None:\n                rpn_accuracy_rpn_monitor.append(0)\n                rpn_accuracy_for_epoch.append(0)\n                continue\n            \n            # Find out the positive anchors and negative anchors\n            neg_samples = np.where(Y1[0, :, -1] == 1)\n            pos_samples = np.where(Y1[0, :, -1] == 0)\n\n            if len(neg_samples) > 0:\n                neg_samples = neg_samples[0]\n            else:\n                neg_samples = []\n\n            if len(pos_samples) > 0:\n                pos_samples = pos_samples[0]\n            else:\n                pos_samples = []\n\n            rpn_accuracy_rpn_monitor.append(len(pos_samples))\n            rpn_accuracy_for_epoch.append((len(pos_samples)))\n\n            if C.num_rois > 1:\n                # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n                if len(pos_samples) < C.num_rois//2:\n                    selected_pos_samples = pos_samples.tolist()\n                else:\n                    selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n                \n                # Randomly choose (num_rois - num_pos) neg samples\n                try:\n                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n                except:\n                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n                \n                # Save all the pos and neg samples in sel_samples\n                sel_samples = selected_pos_samples + selected_neg_samples\n            else:\n                # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n                selected_pos_samples = pos_samples.tolist()\n                selected_neg_samples = neg_samples.tolist()\n                if np.random.randint(0, 2):\n                    sel_samples = random.choice(neg_samples)\n                else:\n                    sel_samples = random.choice(pos_samples)\n\n            # training_data: [X, X2[:, sel_samples, :]]\n            # labels: [Y1[:, sel_samples, :], Y2[:, sel_samples, :]]\n            #  X                     => img_data resized image\n            #  X2[:, sel_samples, :] => num_rois (4 in here) bboxes which contains selected neg and pos\n            #  Y1[:, sel_samples, :] => one hot encode for num_rois bboxes which contains selected neg and pos\n            #  Y2[:, sel_samples, :] => labels and gt bboxes for num_rois bboxes which contains selected neg and pos\n            loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n\n            losses[iter_num, 0] = loss_rpn[1]\n            losses[iter_num, 1] = loss_rpn[2]\n\n            losses[iter_num, 2] = loss_class[1]\n            losses[iter_num, 3] = loss_class[2]\n            losses[iter_num, 4] = loss_class[3]\n\n            iter_num += 1\n\n            progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n                                      ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3]))])\n\n            if iter_num == epoch_length:\n                loss_rpn_cls = np.mean(losses[:, 0])\n                loss_rpn_regr = np.mean(losses[:, 1])\n                loss_class_cls = np.mean(losses[:, 2])\n                loss_class_regr = np.mean(losses[:, 3])\n                class_acc = np.mean(losses[:, 4])\n\n                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n                rpn_accuracy_for_epoch = []\n\n                if C.verbose:\n                    print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n                    print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n                    print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n                    print('Loss RPN regression: {}'.format(loss_rpn_regr))\n                    print('Loss Detector classifier: {}'.format(loss_class_cls))\n                    print('Loss Detector regression: {}'.format(loss_class_regr))\n                    print('Total loss: {}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n                    print('Elapsed time: {}'.format(time.time() - start_time))\n                    elapsed_time = (time.time()-start_time)/60\n\n                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n                iter_num = 0\n                start_time = time.time()\n\n                if curr_loss < best_loss:\n                    if C.verbose:\n                        print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n                    best_loss = curr_loss\n                    model_all.save_weights(C.model_path)\n\n                new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n                           'class_acc':round(class_acc, 3), \n                           'loss_rpn_cls':round(loss_rpn_cls, 3), \n                           'loss_rpn_regr':round(loss_rpn_regr, 3), \n                           'loss_class_cls':round(loss_class_cls, 3), \n                           'loss_class_regr':round(loss_class_regr, 3), \n                           'curr_loss':round(curr_loss, 3), \n                           'elapsed_time':round(elapsed_time, 3), \n                           'mAP': 0}\n\n                record_df = record_df.append(new_row, ignore_index=True)\n                record_df.to_csv(record_path, index=0)\n\n                break\n\n#        except Exception as e:\n#            print('Exception: {}'.format(e))\n#            continue\n\nprint('Training complete, exiting.')","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}