{"cells":[{"metadata":{},"cell_type":"markdown","source":"Pytorch Yoloish net for object detection:\nThis model is similar to YOlO but has more prediction boxes (32*32 instead of 7*7) becouse there are much more boxes to predict. \nI done anything from scratch, so I would like to know any comments/commentds.\nWhen having accurate bounding boxes, the next stage should be a classification net (yet to come).\nI will keep updating this notebook.\n\nAlgorithm steps:\n0.Load packeges\n1.Explore images shape \n2.train-validetion split\n3.Define data-set for GPU training\n4.Design Yolo-shape net\n5.Plot images\n6.Yolo label and loss calculation\n7.Main NN loop\n8.Predicted bounding box visualizetion."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\n# step 0: Laod packeges\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.utils import data\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\ninput_path = Path(\"../input/kuzushiji-recognition/\")\nimport cv2\n\n%matplotlib inline\nimport os\n\n\n\n\n\n# Any results you write to the current directory are saved as output.\nos.listdir(input_path)\n#os.listdir(\"../input/kuzushiji-recognition/train_images/\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Step 1:Explore images shape  (I skip this stage by defoult)\n\nMinMaxCalc = 0 \nif MinMaxCalc :\n\n    MinMax = np.zeros((2,2))\n    MinMax[1,:] = 10000\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n            if os.path.join(dirname, filename)[-1] == 'g':\n                im = Image.open(os.path.join(dirname, filename))\n                #print(np.array(im).shape)\n\n                if np.array(im).shape[0] > MinMax[0,0] : \n                    MinMax[0,0] =   np.array(im).shape[0] \n                if np.array(im).shape[1] > MinMax[0,1] : \n                    MinMax[0,1] = np.array(im).shape[1] \n                if np.array(im).shape[0] < MinMax[1,0] : \n                    MinMax[1,0] = np.array(im).shape[0] \n                if np.array(im).shape[1] < MinMax[1,1] : \n                    MinMax[1,1] = np.array(im).shape[1] \n                    \nelse:\n    MinMax = np.array([[5286., 3442.],\n       [2353., 1750.]])\nprint(MinMax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Step 2: train-validetion split\n\ndf_train = pd.read_csv( input_path / 'train.csv')\nunicode_trans = pd.read_csv( input_path / 'unicode_translation.csv')\ntrain_image_path = input_path / \"train_images\"\ntest_image_path = input_path / \"test_images\"\nunicode_map = {codepoint: char for codepoint, char in unicode_trans.values}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import scipy.ndimage\n\n# if 0 :\n#     df_train[\"Height\"] = \"\"\n#     df_train[\"Width\"] = \"\"\n#     for i,im in enumerate(df_train[\"image_id\"]):\n#         height, width, channels = scipy.ndimage.imread(\"../input/kuzushiji-recognition/train_images/\"+im+\".jpg\").shape\n#         df_train[\"Height\"][i] = height\n#         df_train[\"Width\"][i] = width\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read some labels exmple:\nlength = 5\nsplit_labels = df_train[\"labels\"][0].split()\nfor idx in range(len(split_labels) // length):\n    start_idx = idx * length\n    print(split_labels[start_idx : start_idx + length])\n    if idx == 14:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split_labels = df_train[\"labels\"][0].split()\n# del split_labels[::5]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data frame\nL_im = len(df_train)\nRandOrd = np.array(range(L_im))\n\nnp.random.shuffle(RandOrd)\n\nSplitFrac = 0.2\nSplitInd = int(L_im*SplitFrac)\nTrain_Im = df_train.iloc[RandOrd[:SplitInd]]\nVal_Im = df_train.iloc[RandOrd[SplitInd:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 3:Define data-set for GPU training . \n# image is resize to 1024 pixels width and hight\nclass Dataset(data.Dataset):\n    def __init__(self,DataPath, list_IDs,ListLabels,BatchSize,transforms,TestFlag=0):\n        'Initialization'\n        self.BatchSize = BatchSize\n        self.list_IDs = list_IDs\n        self.DataPath = DataPath\n        self.ListLabels = ListLabels\n        self.transforms = transforms\n        if TestFlag ==0 :\n            self.Fpath =  \"train_images\"\n        else:\n            self.Fpath =  \"test_images\"\n\n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample\n        ID = self.list_IDs[index]\n        if len(self.ListLabels)>1 :\n            Label = self.ListLabels[index]\n        else:\n            Label = 0 \n        \n        \n        # Load data and get label\n        img = Image.open(self.DataPath +\"/\"+ID+\".jpg\")\n        width, height = img.size\n        #img = img.resize((256,256),resample=Image.BILINEAR)\n        img = img.resize((1024,1024),resample=Image.BICUBIC)\n        Torch_img = self.transforms(img)\n        \n        LabelOut = torch.zeros(2000)\n        try:          \n            split_labels = Label.split()\n            del split_labels[::5]\n            Label = torch.tensor(np.array(split_labels,dtype =np.float32 ),dtype=torch.float32)\n            LabelOut[:len(Label)] = Label \n        except:\n            Label = torch.tensor(0,dtype=torch.float32)\n        return Torch_img, LabelOut ,ID,width, height","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imagge transformations:\ntrain_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\nVal_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset load\nbatch_size = 6\nDataPath = \"../input/kuzushiji-recognition/train_images/\"\ntraining_set = Dataset(DataPath,list(Train_Im[\"image_id\"]),list(Train_Im[\"labels\"]),batch_size,train_transforms)\nVal_set = Dataset(DataPath,list(Val_Im[\"image_id\"]),list(Train_Im[\"labels\"]),batch_size,train_transforms)\nTest_Set = Dataset(\"../input/kuzushiji-recognition/test_images/\",os.listdir(\"../input/kuzushiji-recognition/test_images/\"),[],batch_size,Val_transforms,TestFlag=1) \n\ntrain_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, num_workers=1,shuffle=True) # num_workers=12\nvalid_loader = torch.utils.data.DataLoader(Val_set, batch_size=batch_size, num_workers=1,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(Test_Set, batch_size=batch_size, num_workers=1,shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 4:Design Yolo-shape net\nclass YoloishNet(nn.Module):\n    def __init__(self):\n        super(YoloishNet,self).__init__()\n        self.Fc_features = 128\n        self.C1 = nn.Conv2d(3,8,3,padding=1)\n        self.C2 = nn.Conv2d(8,16,3,padding=1)\n        self.C3 = nn.Conv2d(16,32,3,padding=1)\n        self.C4 = nn.Conv2d(32,64,3,padding=1)\n        self.C5 = nn.Conv2d(64,128,3,padding=1)\n        self.C6 = nn.Conv2d(128,128,3,padding=1)\n        self.C7 = nn.Conv2d(128,10,3,padding=1)\n    \n        self.L_Relu = nn.LeakyReLU(0.1, inplace=True)\n        self.UpConv1 = nn.ConvTranspose2d(256,256,4,stride=2)\n        \n        self.BN1 = nn.BatchNorm2d(8)\n        self.BN2 = nn.BatchNorm2d(16)\n        self.BN3 = nn.BatchNorm2d(32)\n        self.BN4 = nn.BatchNorm2d(64)\n        self.BN5 = nn.BatchNorm2d(128)\n        self.BN6 = nn.BatchNorm2d(128)\n        self.maxpoll = nn.MaxPool2d(2,2)\n        self.maxpool2 = nn.MaxPool2d((2,1),(2,1))\n        self.FC = nn.Linear(128*32*32,10*32*32)\n        \n        self.fc1 = nn.Linear(self.Fc_features,128)\n        #self.fc2 = nn.Linear(128,self.NumClasses )\n        self.dropout = nn.Dropout(0.45)\n        self.Bat1 = nn.BatchNorm1d(self.Fc_features)\n        \n        \n        \n    def forward(self,x):\n        # add sequence of convolutional and max pooling layers\n        x = self.maxpoll(self.L_Relu(self.BN1(self.C1(x))))\n        x = self.maxpoll(self.L_Relu(self.BN2(self.C2(x))))\n        x = self.maxpoll(self.L_Relu(self.BN3(self.C3(x))))\n        x = self.maxpoll(self.L_Relu(self.BN4(self.C4(x))))\n        x = self.dropout(self.maxpoll(self.L_Relu(self.BN5(self.C5(x)))))\n        x = self.C7(x)\n        # flatten image input\n        #print(x.shape)\n       # x = x.view(-1, self.Fc_features)\n        # add dropout layer\n        #x = self.Bat1(x)\n        # add 1st hidden layer, with relu activation function\n        #x = self.dropout(F.relu(self.fc1(x)))\n        # add dropout layer\n        # add 2nd hidden layer, with relu activation function\n        #x = torch.sigmoid(self.fc2(x))\n        #x = self.fc2(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model and optimizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = YoloishNet()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 5:Plot images \ndef PlotImageWithFrame(Im,labels,NumPic,h,w,TrueLabel = 1):\n    \n    \n    \n    Im = inputs[NumPic,:,:,:].cpu().detach().numpy()\n    Im = np.moveaxis(Im,[0,1,2],[2,0,1])\n\n    Im = Im - np.min(Im)\n    Im = Im / np.max(Im) * 255\n    Im = Im.astype(np.uint8)\n\n\n    img = np.zeros((1024,1024,3), np.uint8)\n    img[:,:,:] = Im[:,:,:]\n    #plt.figure(figsize=(20,20))\n\n\n    Lab = labels[NumPic]\n    height1  =h[NumPic].cpu().detach().numpy()\n    heightRetio = 1024/height1\n    Width1 = w[NumPic].cpu().detach().numpy()\n    WidthRetio = 1024/Width1 \n\n\n    Lab = Lab[:np.min(np.where(Lab==0))].cpu().detach().numpy()\n\n    for i in range(0,len(Lab),4):\n        cv2.rectangle(img, (int(Lab[i]*heightRetio), int(Lab[i+1]*WidthRetio)), ( int((Lab[i]+Lab[i+2])*heightRetio), int((Lab[i+1]+Lab[i+3])*WidthRetio)), (0,100,0), 2) \n        #cv2.putText(img, 'fdsvfrewsf', (int(Lab[i]*heightRetio), int(Lab[i+1]*WidthRetio) - 5), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,100,0), 2)\n        print(int(Lab[i]*heightRetio),int(Lab[i+1]*WidthRetio),int((Lab[i+2])*heightRetio),int((Lab[i+3])*WidthRetio))\n    plt.figure(figsize=(20,20))\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 6: Yolo label and loss calculation\n# takes a 4*n (n- number of BB) label and converte it to 10*32*32 labels\ndef CreatLabel(Lab_all,h_all,w_all):\n    \n    LabelesFullOut = np.zeros((len(h_all),10,32,32))\n\n    for j in range(len(h_all)):\n        \n        h = h_all[j]\n        w = w_all[j]\n        Lab = Lab_all[j]\n        \n        \n        Lab = Lab.detach().numpy()\n        height1 = h.detach().numpy()\n        heightRetio = 1024/height1\n        Width1 = w.detach().numpy()\n        WidthRetio = 1024/Width1 \n\n        NumSubSquere  = 32 \n        H_Center_Norm =  (Lab[::4]+Lab[2::4]/2)*heightRetio/NumSubSquere\n        W_Center_Norm = (Lab[1::4]+Lab[3::4]/2)*WidthRetio/NumSubSquere\n        H_box = Lab[2::4]\n        W_box = Lab[3::4]\n        LabelesFull = np.zeros((10,32,32))\n\n\n        for i in range(len(H_Center_Norm)):\n            H1 = int(np.floor(H_Center_Norm[i]))\n            W1 = int(np.floor(W_Center_Norm[i]))\n\n\n            Ind = 0 \n            if LabelesFull[0,H1,W1] == 1:\n                Ind += 5\n            LabelesFull[Ind,H1,W1] = 1\n            LabelesFull[Ind+1,H1,W1] = np.remainder(H_Center_Norm[i],1)*NumSubSquere\n            LabelesFull[Ind+2,H1,W1] = np.remainder(W_Center_Norm[i],1)*NumSubSquere\n            LabelesFull[Ind+3,H1,W1] = np.sqrt(H_box[i])\n            LabelesFull[Ind+4,H1,W1] = np.sqrt(W_box[i])\n\n\n        LabelesFullOut[j,:,:,:] = LabelesFull\n    return torch.from_numpy(LabelesFullOut)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform  back  labels\ndef TransformLabel(LabelesFull,NumIm):\n\n\n    Label = []\n    for i in range(32):\n        for j in range(32):\n            if LabelesFull[NumIm,0,i,j] == 1:\n                Label.append(i+LabelesFull[NumIm,1,i,j]/32)\n                print(i+LabelesFull[NumIm,1,i,j]/32)\n                Label.append(j+LabelesFull[NumIm,2,i,j]/32)\n                Label.append(np.square(LabelesFull[NumIm,3,i,j] ) ) \n                Label.append(np.square(LabelesFull[NumIm,4,i,j] ) )\n\n                \n\n    return np.array(Label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate loss based on YOLO paper (more or less)\nimport torch.nn.functional as F\n\ndef CalcMyLoss(LabelesFull,logps):\n    LabelesFull = LabelesFull.float()\n    #logps = logps.cpu()\n\n    TotalFullLoss = 0\n    for i in range(logps.shape[0]):\n\n        #PredSum = torch.sum(torch.pow(LabelesFull[i,0,:,:]-torch.sigmoid(logps[i,0,:,:]),2))/(32**2)\n#         PredSum = F.binary_cross_entropy(F.sigmoid(logps[i,0,:,:]), LabelesFull[i,0,:,:])\n        PredSum = torch.sum(torch.pow( (logps[i,0,:,:])- LabelesFull[i,0,:,:],2))/(32**2)\n\n        PredSum2 = torch.sum(torch.pow( (logps[i,5,:,:])- LabelesFull[i,5,:,:],2))/(32**2)\n        \n        NumBB = 0\n        PositionLoss = 0\n        HeightWidthLoss = 0 \n        for m in range(32):\n            for n in range(32):\n                for k in [0,5] :\n                    if LabelesFull[i,k,n,m] == 1:\n                        PositionLoss += torch.pow( LabelesFull[i,k+1,n,m] - logps[i,k+1,n,m],2) + torch.pow( LabelesFull[i,k+2,n,m] - logps[i,k+2,n,m],2)    \n                        HeightWidthLoss += torch.pow( LabelesFull[i,k+3,n,m] - logps[i,k+3,n,m],2) + torch.pow( LabelesFull[i,k+4,n,m] - logps[i,k+4,n,m],2)\n                        NumBB += 1\n\n        #print(\"Pred 1 loss: \"+str(PredSum.detach().cpu() )+\", Pred 2 loss: \"+str(PredSum2.detach().cpu())+\"Position Loss: \"+str((PositionLoss/NumBB/32/6).detach().cpu())+\", Height Width Loss: \"+str((HeightWidthLoss/NumBB/32).detach().cpu()))\n        TotalLoss = 10*PredSum + PredSum2*0.5 + PositionLoss/NumBB/32/6 + HeightWidthLoss/NumBB/32\n        PredLossCoeff = 10\n        PositionLoss = 0.2\n        #TotalLoss = PredLossCoeff*PredSum  + PositionLoss/NumBB/32*PositionLoss + HeightWidthLoss/NumBB/32\n        TotalLoss = PredLossCoeff*PredSum \n        TotalFullLoss += TotalLoss\n        #print(TotalFullLoss)\n        \n        \n    return TotalFullLoss\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def NonMaxSupression(logps):\n# logps1 = logps.cpu().detach().numpy()\n# print(type(logps1))\n# for ImNum in range(logps1.shape[0]):\n#     ImData = logps1[ImNum,:,:,:]\n#     PredValues = ImData[0,:,:]\n#     sortedI = np.argsort(PredValues,axis=None)\n#     print(sortedI.shape)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 7:Main NN loop\nepochs = 6\nvalid_loss_min = np.Inf\n\nimport time\n\nfor epoch in range(epochs):\n    start = time.time()\n    \n    #scheduler.step()\n    model.train()\n    \n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    for inputs, labels,_,h,w in train_loader:\n        \n        LabelesFull = CreatLabel(labels,h,w)\n\n        # Move input and label tensors to the default device\n        inputs, LabelesFull = inputs.to(device), LabelesFull.to(device)\n        optimizer.zero_grad()\n        \n        logps = model(inputs)\n        \n        loss = CalcMyLoss(LabelesFull,logps)\n        print(loss)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 8: Predicted bounding box visualizetion.\ndef PlotImageWithFrame_2(Im,labelsFull,NumPic,TrueLabel = 1,Label2=[],BB_thr = 0.5):\n    \n    \n    Im = inputs[NumPic,:,:,:].cpu().detach().numpy()\n    Im = np.moveaxis(Im,[0,1,2],[2,0,1])\n\n    Im = Im - np.min(Im)\n    Im = Im / np.max(Im) * 255\n    Im = Im.astype(np.uint8)\n\n\n    img = np.zeros((1024,1024,3), np.uint8)\n    img[:,:,:] = Im[:,:,:]\n        #plt.figure(figsize=(20,20))\n\n\n    Lab = labelsFull[NumPic,:,:,:]\n    \n\n    #     height1  =h[NumPic].cpu().detach().numpy()\n    #     heightRetio = 1024/height1\n    #     Width1 = w[NumPic].cpu().detach().numpy()\n    #     WidthRetio = 1024/Width1 \n\n\n\n    # for i in range(0,len(Lab),4):\n    #     cv2.rectangle(img, (int(Lab[i]), int(Lab[i+1])), ( int((Lab[i]+Lab[i+2])), int((Lab[i+1]+Lab[i+3]))), (0,100,0), 2) \n    #     #cv2.putText(img, 'fdsvfrewsf', (int(Lab[i]*heightRetio), int(Lab[i+1]*WidthRetio) - 5), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,100,0), 2)\n    #     print(int(Lab[i]),int(Lab[i+1]),int((Lab[i+2])),int((Lab[i+3])))\n\n    for i in range(32):\n        for j in range(32):\n            if Lab[0,i,j] == 1 :\n                cv2.rectangle(img, (int(i*32+Lab[1,i,j]- (Lab[3,i,j]**2)/4), int(j*32+Lab[2,i,j]-(Lab[4,i,j]**2)/4)), (int(i*32+ (Lab[3,i,j]**2)/4+Lab[1,i,j] ),  int(j*32+(Lab[4,i,j]**2)/4) +Lab[2,i,j]), (0,100,0), 2) \n            if Lab[5,i,j] == 1 :\n                cv2.rectangle(img, (int(i*32+Lab[5+1,i,j]- (Lab[5+3,i,j]**2)/4), int(j*32+Lab[5+2,i,j]-(Lab[5+4,i,j]**2)/4)), (int(i*32+ (Lab[5+3,i,j]**2)/4+Lab[5+1,i,j] ),  int(j*32+(Lab[5+4,i,j]**2)/4) +Lab[5+2,i,j]), (0,100,0), 2) \n            if TrueLabel == 0 and Lab[0,i,j] >BB_thr:\n            #if TrueLabel == 0 and Label2[NumPic,0,i,j] ==1:\n                cv2.rectangle(img, (int(i*32+Lab[1,i,j]- (Lab[3,i,j]**2)/4), int(j*32+Lab[2,i,j]-(Lab[4,i,j]**2)/4)), (int(i*32+ (Lab[3,i,j]**2)/4+Lab[1,i,j] ),  int(j*32+(Lab[4,i,j]**2)/4) +Lab[2,i,j]), (0,100,0), 2) \n            #if TrueLabel == 0 and  Label2[NumPic,0,i,j]==1 :#Lab[5,i,j] >0:\n             #   cv2.rectangle(img, (int(i*32+Lab[5+1,i,j]- (Lab[5+3,i,j]**2)/4), int(j*32+Lab[5+2,i,j]-(Lab[5+4,i,j]**2)/4)), (int(i*32+ (Lab[5+3,i,j]**2)/4+Lab[5+1,i,j] ),  int(j*32+(Lab[5+4,i,j]**2)/4) +Lab[5+2,i,j]), (0,100,0), 2) \n\n    plt.figure(figsize=(20,20))\n    plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = 0\nfor inputs, labels,_,h,w in train_loader:\n        \n    LabelesFull = CreatLabel(labels,h,w)\n\n        # Move input and label tensors to the default device\n    inputs, LabelesFull = inputs.to(device), LabelesFull.to(device)\n    optimizer.zero_grad()\n    logps = model(inputs)\n      \n    if a == 1 :    \n        break\n    a +=1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NumIm = 1\nPlotImageWithFrame_2(inputs,logps,NumIm,TrueLabel = 0,Label2 =LabelesFull,BB_thr=0.1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.hist(logps[0,0,:,:].cpu().detach().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(logps[0,0,:,:].cpu().detach().numpy()[:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}