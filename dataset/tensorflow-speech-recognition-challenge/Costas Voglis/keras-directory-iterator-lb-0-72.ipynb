{"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this kernel I present an end-to-end solution using the custom Keras Directory Iterator and a small training/validation subset that I compiled. You can find my training data as well as the whole test dataset here https://www.kaggle.com/voglinio/speechtest. Once loaded, this dataset will give you access to the following folders\n\n* train_path='../input/speechtest/part1/part1/train/'\n* val_path ='../input/speechtest/part1/part1/val/'\n* test_path ='../input/speechtest/test/'\n\nThis kernel ends with a submission file, that scores 0.72 on the LB. I have used limited number of epochs because of the 1 hour running time limit imposed by the kernel.","metadata":{"_cell_guid":"56376727-f3ff-4618-bdaa-b9602ed267ac","_uuid":"a5f3d1677a66f695d7979a2f61c80e3d44ae14f3"}},{"cell_type":"markdown","source":"## Some statistics about the training/validation files","metadata":{"_cell_guid":"3f520269-8af7-4ff1-90d6-deebc2bed8fd","_uuid":"5d9676904a734b9d7702fcfa392d78a7d4e0f766"}},{"cell_type":"code","outputs":[],"source":"import numpy as np\nfrom keras import backend as K\nfrom keras.preprocessing.image import Iterator\nfrom keras.preprocessing.image import img_to_array\n\nimport librosa\nimport os\nimport multiprocessing.pool\nfrom functools import partial\nfrom random import getrandbits\n\ntrain_path='../input/speechtest/part1/part1/train/'\nval_path ='../input/speechtest/part1/part1/val/'\ntest_path ='../input/speechtest/test/test/'\n\nclassnames=os.listdir(train_path)\ntrain_count_dict = {}\nfor d in classnames:\n    train_count_dict[d] = len(os.listdir(os.path.join(train_path, d)))\nprint('train freq')\nfor k, v in train_count_dict.items():\n    print ( '%7s  %i' % (k, v))\nval_count_dict = {}\nfor d in classnames:\n    val_count_dict[d] = len(os.listdir(os.path.join(val_path, d)))\nprint('\\nval freq')\nfor k, v in val_count_dict.items():\n    print ( '%7s  %i' % (k, v))\nprint ('')\nprint ('test files', len(os.listdir(test_path+'/audio')))","metadata":{"_cell_guid":"d84ac3bc-6fe4-4430-bdc5-1f4cbb70c88b","_uuid":"05e4765e5b14b61511b2ca91b54721c0848defab"},"execution_count":15},{"cell_type":"markdown","source":"## Directory Iterator definition","metadata":{}},{"cell_type":"code","outputs":[],"source":"def spect_loader(path, window_size, window_stride, window, normalize, max_len=101, \n                 augment=False, allow_speedandpitch=False, allow_pitch=False,\n                 allow_speed=False, allow_dyn=False, allow_noise=False,\n                allow_timeshift=False ):\n    y, sr = librosa.load(path, sr=None)\n    # n_fft = 4096\n    n_fft = int(sr * window_size)\n    win_length = n_fft\n    hop_length = int(sr * window_stride)\n\n    # STFT\n    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n                     win_length=win_length, window=window)\n    spect, phase = librosa.magphase(D)\n\n    # S = log(S+1)\n    spect = np.log1p(spect)\n\n    # make all spects with the same dims\n    # TODO: change that in the future\n    if spect.shape[1] < max_len:\n        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n        spect = np.hstack((spect, pad))\n    elif spect.shape[1] > max_len:\n        spect = spect[:max_len, ]\n    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n    #spect = torch.FloatTensor(spect)\n\n    # z-score normalization\n    if normalize:\n        mean = np.mean(np.ravel(spect))\n        std = np.std(np.ravel(spect))\n        if std != 0:\n            spect = spect -mean\n            spect = spect / std\n\n    return spect\n\ndef _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n    \"\"\"Count files with extension in `white_list_formats` contained in a directory.\n    # Arguments\n        directory: absolute path to the directory containing files to be counted\n        white_list_formats: set of strings containing allowed extensions for\n            the files to be counted.\n    # Returns\n        the count of files with extension in `white_list_formats` contained in\n        the directory.\n    \"\"\"\n    def _recursive_list(subpath):\n        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n\n    samples = 0\n    for root, _, files in _recursive_list(directory):\n        for fname in files:\n            is_valid = False\n            for extension in white_list_formats:\n                if fname.lower().endswith('.' + extension):\n                    is_valid = True\n                    break\n            if is_valid:\n                samples += 1\n    return samples\n\ndef _list_valid_filenames_in_directory(directory, white_list_formats,\n                                       class_indices, follow_links):\n    \"\"\"List paths of files in `subdir` relative from `directory` whose extensions are in `white_list_formats`.\n    # Arguments\n        directory: absolute path to a directory containing the files to list.\n            The directory name is used as class label and must be a key of `class_indices`.\n        white_list_formats: set of strings containing allowed extensions for\n            the files to be counted.\n        class_indices: dictionary mapping a class name to its index.\n    # Returns\n        classes: a list of class indices\n        filenames: the path of valid files in `directory`, relative from\n            `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n            the filenames will be [\"class1/file1.jpg\", \"class1/file2.jpg\", ...]).\n    \"\"\"\n    def _recursive_list(subpath):\n        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n\n    classes = []\n    filenames = []\n    subdir = os.path.basename(directory)\n    basedir = os.path.dirname(directory)\n    for root, _, files in _recursive_list(directory):\n        for fname in sorted(files):\n            is_valid = False\n            for extension in white_list_formats:\n                if fname.lower().endswith('.' + extension):\n                    is_valid = True\n                    break\n            if is_valid:\n                classes.append(class_indices[subdir])\n                # add filename relative to directory\n                absolute_path = os.path.join(root, fname)\n                filenames.append(os.path.relpath(absolute_path, basedir))\n    return classes, filenames\n\nclass SpeechDirectoryIterator(Iterator):\n    \"\"\"Iterator capable of reading images from a directory on disk.\n    # Arguments\n       \n    \"\"\"\n\n    def __init__(self, directory, window_size, window_stride, \n                 window_type, normalize, max_len=101,\n                 target_size=(256, 256), color_mode='grayscale',\n                 classes=None, class_mode='categorical',\n                 batch_size=32, shuffle=True, seed=None,\n                 data_format=None, save_to_dir=None,\n                 save_prefix='', save_format='png',\n                 follow_links=False, interpolation='nearest', augment=False,\n                allow_speedandpitch = False, allow_pitch = False,\n                allow_speed = False, allow_dyn = False, allow_noise = False, allow_timeshift=False ):\n        if data_format is None:\n            data_format = K.image_data_format()\n        self.window_size = window_size\n        self.window_stride = window_stride\n        self.window_type = window_type\n        self.normalize = normalize\n        self.max_len = max_len\n        self.directory = directory\n        self.allow_speedandpitch = allow_speedandpitch\n        self.allow_pitch = allow_pitch\n        self.allow_speed = allow_speed \n        self.allow_dyn = allow_dyn\n        self.allow_noise = allow_noise\n        self.allow_timeshift = allow_timeshift \n        self.augment = augment\n#        self.image_data_generator = image_data_generator\n        self.target_size = tuple(target_size)\n        if color_mode not in {'rgb', 'grayscale'}:\n            raise ValueError('Invalid color mode:', color_mode,\n                             '; expected \"rgb\" or \"grayscale\".')\n        self.color_mode = color_mode\n        self.data_format = data_format\n        if self.color_mode == 'rgb':\n            if self.data_format == 'channels_last':\n                self.image_shape = self.target_size + (3,)\n            else:\n                self.image_shape = (3,) + self.target_size\n        else:\n            if self.data_format == 'channels_last':\n                self.image_shape = self.target_size + (1,)\n            else:\n                self.image_shape = (1,) + self.target_size\n        self.classes = classes\n        if class_mode not in {'categorical', 'binary', 'sparse',\n                              'input', None}:\n            raise ValueError('Invalid class_mode:', class_mode,\n                             '; expected one of \"categorical\", '\n                             '\"binary\", \"sparse\", \"input\"'\n                             ' or None.')\n        self.class_mode = class_mode\n        self.save_to_dir = save_to_dir\n        self.save_prefix = save_prefix\n        self.save_format = save_format\n        self.interpolation = interpolation\n\n        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'wav'}\n\n        # first, count the number of samples and classes\n        self.samples = 0\n\n        if not classes:\n            classes = []\n            for subdir in sorted(os.listdir(directory)):\n                if os.path.isdir(os.path.join(directory, subdir)):\n                    classes.append(subdir)\n        self.num_classes = len(classes)\n        self.class_indices = dict(zip(classes, range(len(classes))))\n\n        pool = multiprocessing.pool.ThreadPool()\n        function_partial = partial(_count_valid_files_in_directory,\n                                   white_list_formats=white_list_formats,\n                                   follow_links=follow_links)\n        self.samples = sum(pool.map(function_partial,\n                                    (os.path.join(directory, subdir)\n                                     for subdir in classes)))\n\n        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))\n\n        # second, build an index of the images in the different class subfolders\n        results = []\n\n        self.filenames = []\n        self.classes = np.zeros((self.samples,), dtype='int32')\n        i = 0\n        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n                                            (dirpath, white_list_formats,\n                                             self.class_indices, follow_links)))\n            \n        \n        for res in results:\n            classes, filenames = res.get()\n            self.classes[i:i + len(classes)] = classes\n            self.filenames += filenames\n            if i==0:\n                img = spect_loader(os.path.join(self.directory, filenames[0]), \n                               self.window_size, \n                               self.window_stride, \n                               self.window_type, \n                               self.normalize, \n                               self.max_len, \n                               self.augment,\n                               self.allow_speedandpitch,\n                               self.allow_pitch,\n                               self.allow_speed, \n                               self.allow_dyn,\n                               self.allow_noise,\n                               self.allow_timeshift ) \n                img=np.swapaxes(img, 0, 2)\n                self.target_size = tuple((img.shape[0], img.shape[1]))\n                print(self.target_size)\n                if self.color_mode == 'rgb':\n                    if self.data_format == 'channels_last':\n                        self.image_shape = self.target_size + (3,)\n                    else:\n                        self.image_shape = (3,) + self.target_size\n                else:\n                    if self.data_format == 'channels_last':\n                        self.image_shape = self.target_size + (1,)\n                    else:\n                        self.image_shape = (1,) + self.target_size\n                        \n            i += len(classes)\n        pool.close()\n        pool.join()\n        super(SpeechDirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n    \n\n    \n    \n    def _get_batches_of_transformed_samples(self, index_array):\n        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n        batch_f = []\n        grayscale = self.color_mode == 'grayscale'\n        # build batch of image data\n        #print(index_array)\n        for i, j in enumerate(index_array):\n            #print(i, j, self.filenames[j])\n            fname = self.filenames[j]\n            #img = load_img(os.path.join(self.directory, fname),\n            #               grayscale=grayscale,\n            #               target_size=self.target_size,\n            #               interpolation=self.interpolation)\n            img = spect_loader(os.path.join(self.directory, fname), \n                               self.window_size, \n                               self.window_stride, \n                               self.window_type, \n                               self.normalize, \n                               self.max_len, \n                                )\n            img=np.swapaxes(img, 0, 2)\n            \n            x = img_to_array(img, data_format=self.data_format)\n            #x = self.image_data_generator.random_transform(x)\n            #x = self.image_data_generator.standardize(x)\n            batch_x[i] = x\n            batch_f.append(fname)\n        # optionally save augmented images to disk for debugging purposes\n        if self.save_to_dir:\n            for i, j in enumerate(index_array):\n                img = array_to_img(batch_x[i], self.data_format, scale=True)\n                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n                                                                  index=j,\n                                                                  hash=np.random.randint(1e7),\n                                                                  format=self.save_format)\n                img.save(os.path.join(self.save_to_dir, fname))\n        # build batch of labels\n        if self.class_mode == 'input':\n            batch_y = batch_x.copy()\n        elif self.class_mode == 'sparse':\n            batch_y = self.classes[index_array]\n        elif self.class_mode == 'binary':\n            batch_y = self.classes[index_array].astype(K.floatx())\n        elif self.class_mode == 'categorical':\n            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n            for i, label in enumerate(self.classes[index_array]):\n                batch_y[i, label] = 1.\n        else:\n            return batch_x\n        return batch_x, batch_y\n\n    def next(self):\n        \"\"\"For python 2.x.\n        # Returns\n            The next batch.\n        \"\"\"\n        with self.lock:\n            index_array = next(self.index_generator)[0]\n        # The transformation of images is not under thread lock\n        # so it can be done in parallel\n        return self._get_batches_of_transformed_samples(index_array)","metadata":{"collapsed":true},"execution_count":16},{"cell_type":"code","outputs":[],"source":"window_size=.02\nwindow_stride=.01\nwindow_type='hamming'\nnormalize=True\nmax_len=101\nbatch_size = 64\ntrain_iterator = SpeechDirectoryIterator(directory=train_path, \n                                   batch_size=batch_size, \n                                   window_size=window_size, \n                                   window_stride=window_stride, \n                                   window_type=window_type,\n                                   normalize=normalize, \n                                   max_len=max_len)","metadata":{},"execution_count":17},{"cell_type":"code","outputs":[],"source":"val_iterator = SpeechDirectoryIterator(directory=val_path, \n                                   batch_size=batch_size, \n                                   window_size=window_size, \n                                   window_stride=window_stride, \n                                   window_type=window_type,\n                                   normalize=normalize, \n                                   max_len=max_len)","metadata":{},"execution_count":18},{"cell_type":"markdown","source":"## Model definition","metadata":{}},{"cell_type":"code","outputs":[],"source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nmodel = Sequential()\nmodel.add(Conv2D(12, (5, 5), activation = 'relu', input_shape=train_iterator.image_shape))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(25, (5, 5), activation = 'relu'))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(180, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(classnames), activation = 'softmax')) #Last layer with one output per class\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\nmodel.summary()","metadata":{},"execution_count":19},{"cell_type":"markdown","source":"## Training and callbacs","metadata":{}},{"cell_type":"code","outputs":[],"source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nearly = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\nreduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, mode='auto', min_lr=0.00001)\nmodel.fit_generator(train_iterator,\n        steps_per_epoch=int(np.ceil(train_iterator.n / batch_size)),\n        epochs=6,\n        validation_data=val_iterator,\n        validation_steps=int(np.ceil(val_iterator.n / batch_size)),\n        verbose=1, callbacks=[early, reduce])","metadata":{},"execution_count":20},{"cell_type":"markdown","source":"## Prediction on test set\n\nI use the keras.util.Sequence class to iterate through test images. It is a thread safe way to predict on a series of test files.","metadata":{}},{"cell_type":"code","outputs":[],"source":"test_path_audio=os.path.join(test_path, 'audio')\ntest_filenames = os.listdir(test_path_audio) \ntest_filenames=np.sort(test_filenames)\nlist(test_filenames)[:10]","metadata":{},"execution_count":37},{"cell_type":"code","outputs":[],"source":"import math\nfrom keras.utils import Sequence\nfrom keras.preprocessing.image import img_to_array\n\ndef loadAndSpect(fname,  window_size, window_stride, window_type, normalize, max_len):\n    img = spect_loader(os.path.join(test_path_audio, fname), \n                       window_size, \n                       window_stride, \n                       window_type, \n                       normalize, \n                       max_len)\n    img=np.swapaxes(img, 0, 2)\n\n    x = img_to_array(img, data_format='channels_last')\n    return x\n            \nclass WavSequence(Sequence):\n\n    def __init__(self, x_set, batch_size=64, window_size=0.02, window_stride=0.01, window_type='hamming', normalize=True, max_len=101):\n        self.x = x_set\n        self.batch_size = batch_size\n        self.window_size = window_size\n        self.window_stride = window_stride\n        self.window_type = window_type\n        self.normalize = normalize\n        self.max_len = max_len\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return np.array([\n            loadAndSpect(file_name, window_size, window_stride, window_type, normalize, max_len)\n               for file_name in batch_x])","metadata":{"collapsed":true},"execution_count":22},{"cell_type":"code","outputs":[],"source":"seq = WavSequence(test_filenames, batch_size=batch_size)\npreds = model.predict_generator(generator=seq, \n                        steps=len(seq), \n                        workers=1, \n                        use_multiprocessing=False, \n                        verbose=1)","metadata":{},"execution_count":23},{"cell_type":"markdown","source":"We can now create a submission file.","metadata":{}},{"cell_type":"code","outputs":[],"source":"inv_map = {v: k for k, v in train_iterator.class_indices.items()}\nprint(inv_map)\nclasses = np.argmax(preds, axis=1)\nprobes = np.max(preds, axis=1)\nprint (classes[:10])\nprint (probes[:10])\n\nunique_elements, counts_elements = np.unique(classes, return_counts=True)\nprint(np.asarray((unique_elements, counts_elements)))\n\nres = []\nfor cl in classes:\n    res.append(inv_map[cl])\n\n\nimport pandas as pd\ndf = pd.DataFrame(np.transpose(np.vstack((np.array(test_filenames), res))), columns=['fname', 'label'])\ndf.to_csv('submission.csv', header=True, quoting=0, index=False)","metadata":{},"execution_count":24},{"cell_type":"markdown","source":"Let us view the first 20 predictions:","metadata":{}},{"cell_type":"code","outputs":[],"source":"df.head(n=20)","metadata":{},"execution_count":25},{"cell_type":"markdown","source":"and listen to a couple of them.","metadata":{}},{"cell_type":"code","outputs":[],"source":"import IPython.display as ipd\nipd.Audio(test_path_audio+'/'+'clip_d54618666.wav')","metadata":{},"execution_count":32},{"cell_type":"code","outputs":[],"source":"ipd.Audio(test_path_audio+'/'+'clip_ca68ee4c5.wav')","metadata":{},"execution_count":34},{"cell_type":"code","outputs":[],"source":"","metadata":{"collapsed":true},"execution_count":null}],"nbformat_minor":1,"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","version":"3.6.3","name":"python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}}}