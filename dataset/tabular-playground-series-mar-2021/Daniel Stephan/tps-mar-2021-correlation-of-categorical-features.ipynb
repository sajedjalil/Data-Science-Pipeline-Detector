{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nI made this notebook because I wanted to see how to measure the correlation between different types of variables. I will consider two types of variables: categorical and continuous. As a really simple definition, we'll say that a categorical variable can have only have one of several values (e.g. 'A', 'B', 'C' or 1, 2, 3) but a categorical variable can have arbitrary (floating point) values. For example, in the March Tabular Playground competition we have 19 categorical features (which contain string values), 11 continuous features and the target is categorical (0 or 1). It is important to know how strongly the target depends on the features, and also how strong each features depends on the others. This information is helpful for example when deciding which features to drop, if it's a good idea to try feature engineering with PCA.\n\nThere are three possibilities when investigating two features: both are categorial, both are continuous or one is catgorical and the other is continuous. After some searching, I found [this article by Outside Two Standard Deviations](https://medium.com/@outside2SDs/an-overview-of-correlation-measures-between-categorical-and-continuous-variables-4c7f85610365) that outlines some statistical methods that can be used for these cases, the take-away is this:\n\n- 2 categorical features: use Lambda or Corrected Cramer's V\n- 1 categorical & 1 continuous feature: use Point Biserial or Logistic Regression\n- 2 continuous features: use Spearman, Kendall or Pearson correlation (where Spearman seems to be the most useful in general)\n\nI will go into some of the details and apply these methods to a toy dataset. Then I will analyse the competition dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport random\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nsns.set(palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate toy dataset\n\nI will make a dataframe with continuous and categorical variables which are related to a target column in various ways:\n\n- The target itself is 0 (75 % of rows) or 1\n- `cat0` and `cat1` are strongly correlated with the target\n- `cat2` is partially correlated with the target\n- `cat3` is uncorrelated with the target\n- `cont0` is negatively correlated with the target\n- `cont1` is partially correlated with the target\n- `cont2` is uncorrelated with the target\n- `cont3` is partially correlated with cont2"},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(0)\nROWS = 100\ndf_toy = pd.DataFrame(dict(target=np.zeros(ROWS)))\ndf_toy.target.iloc[:75] = 0\ndf_toy.target.iloc[75:] = 1\ndf_toy['cat0'] = df_toy.target.apply(lambda x: 'A' if x else 'B')\ndf_toy['cat1'] = df_toy.target.apply(lambda x: random.choice(['A', 'B']) if int(x) else random.choice(['C', 'D']))\ndf_toy['cat2'] = df_toy.target.apply(lambda x: random.choice(['A', 'B']) if int(x) else random.choice(['B', 'C']))\ndf_toy['cat3'] = df_toy.target.apply(lambda x: random.choice(['A', 'B']))\ndf_toy['cont0'] = df_toy.target.apply(lambda x: -3*int(x))\ndf_toy['cont1'] = df_toy.target.apply(lambda x: int(x) + random.gauss(0, 1))\ndf_toy['cont2'] = np.random.rand(ROWS)\ndf_toy['cont3'] = 2 * df_toy.cont2 + np.random.rand(ROWS)\ndf_toy = df_toy.sample(frac=1)   # shuffle dataset\ndisplay(df_toy.sample(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize toy dataset\n\nWe'll make some quick plots to check that the relationships are as intended.\n\n### Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = ['cat0', 'cat1', 'cat2', 'cat3']\ndf_toy_cat = df_toy.melt(value_vars=cat_features, id_vars=['target']).sort_values(['variable', 'value'])\nsns.catplot(data=df_toy_cat, x='value', y='target', col='variable', col_wrap=2, sharex=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Continuous features vs target"},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = ['cont0', 'cont1', 'cont2', 'cont3']\ndf_toy_cont = df_toy.melt(value_vars=cont_features, id_vars=['target']).sort_values(['variable', 'value'])\nsns.relplot(data=df_toy_cont, x='value', y='target', col='variable', col_wrap=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### cont2 vs cont3"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.relplot(data=df_toy, x='cont2', y='cont3', hue='target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze toy dataset\n\nNow that we have a toy dataset where we know the relationships between the features and the target, we can use it to test and understand the statistical metrics for correlations mentionend erlier.\n\n## Correlation of two continuous features\n\nWe'll start with the easiest case. For continuous features, pandas provides the `corr` method, which I've seen in plenty of Kaggle notebooks. Pandas provides the Pearson, Kendall Tau or Spearman correlation, and Pearson is the default. It's easy  to make a correlation heatmap using `corr`. When comparing the different correlation methods, we can observe that the partial correlations are assigned a lower value when using Kendal Tau, but overall, the gist is the same for all three methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(10, 10))\nplt.subplots_adjust(wspace=0.3)\n\nfor method, ax in zip(['pearson', 'kendall', 'spearman'], axs.flatten()):\n    corr = df_toy.corr(method=method)\n    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n    # plot heatmap\n    sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n                cbar_kws={\"shrink\": .8}, ax=ax)\n    # yticks\n    ax.tick_params(labelrotation=0);\n    ax.title.set_text(f'Correlation method: {method.capitalize()}')\n    \nfig.delaxes(axs.flatten()[-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe the following:\n\n- Although the target is categorical, it has been included by pandas because its data type is numerical.\n- As expected, `target` is perfectly correlated with `cont0` and somewhat correlated with `cont1`.\n- We also see the expected correlation between `cont2` and `cont3`."},{"metadata":{},"cell_type":"markdown","source":"## Correlation of two categorical features\n\nWe'll use [Cramer's V](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V) to make a correlation matrix similar to the one for categorical features. This metric is essentially $\\chi^2$ which has been scaled to be between zero and one. It is calculated like this:\n$$\n    V = \\sqrt{\\frac{\\chi^2 / n}{\\min(k-1, r-1)}}\n$$\nwhere n is the number of observations, k is the number of columns and r is the number of rows. Just like for the categorical variables, a value of 0 means no correlation, and a value of one means perfect correlation. The matrix we get fits the expectations stated above quite nicely. There is a bias correction available, and when it is turned on, the values for the uncorrelated features (e.g. cat2 and target) are somewhat closer to zero, while the more strongly correlated features are not affected much. Note, that we're cheating a little bit here: we're not really calculating the main diagonal, it is just set to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"def cramer_v(df, feature1, feature2, bias_correction=True):\n    tab = pd.crosstab(df[feature1], df[feature2])\n    chi2 = stats.chi2_contingency(tab)[0]\n    n = tab.to_numpy().sum()\n    phi2 = chi2 / n\n    k, r = tab.shape\n    if bias_correction:\n        phi2 = max(0, phi2 - (k - 1) * (r - 1) / (n - 1))\n        k = k - (k - 1)**2 / (n - 1)\n        r = r - (r - 1)**2 / (n - 1)\n    v = np.sqrt(phi2 / min(k-1, r-1))\n    return(v)\n\ndef cramer_matrix(df, features, bias_correction=True):\n    n = len(features)\n    data = np.ones([n, n])\n    for i, j in itertools.combinations(range(n), 2):\n        data[i, j] = cramer_v(df, features[i], features[j], bias_correction)\n        data[j, i] = data[i, j]\n    df_cramer = pd.DataFrame(data, index=features, columns=features)\n    return df_cramer\n\nfig, axs = plt.subplots(1, 2, figsize=(14, 7))\n\ndf_cramer = cramer_matrix(df_toy, cat_features + ['target'], False)\nsns.heatmap(df_cramer, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .8}, ax=axs[0])\naxs[0].set_title('Cramer`s V without bias correction')\n\ndf_cramer = cramer_matrix(df_toy, cat_features + ['target'], True)\nsns.heatmap(df_cramer, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .8}, ax=axs[1])\naxs[1].set_title('Cramer`s V with bias correction');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Best categorical values to predict target\n\nCramer's V looks at all values of a categorical feature, but what happens if there is a value that is better at predicting the target than the others? For example, the correlation between cat2 and the target is only 66%, but we know that all \"A\" values correspond to a target value of 1, while all \"C\" values correspond to a target value of zero - only value \"B\" does not tell us anything about the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.histplot(data=df_toy, x='cat2', hue='target', stat='probability', multiple='dodge')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As a starting point, we'll arrange our data by the values of every feature and see how strongly it will predict the target. In this case, an average value of the target of 0 or 1 indicates the best predicion power, while an average of 0.25 means this value tells us nothing about the target.\n\nFor `cat2`, the table below reveals the information that is also shown in the bar plot above, namely that if values 'A' or 'C' are encountered, a perfect prediction for the target value can be made, while value 'B' does not reveal anything about the target. This information cannot be extracted from Cramer's V."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_toy_melt = df_toy.melt(value_vars=['cat0', 'cat1', 'cat2', 'cat3'], id_vars='target')\ndf_toy_pred_power = df_toy_melt.groupby(['variable', 'value']).mean('target')\ndf_toy_pred_power.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to generalize this idea. The Gini Impurity seems like a good way to achieve this. It is calculated like this:\n$$\n    I_G = 1 - \\sum_{i} \\left(\\frac{n_i}{N}\\right)^2\n$$\nwhere $n_i$ are the counts four category $i$ and $N$ is the total number of observations. For example, if we have 5 observations where the target is 0, and 15 where it is 1, the impurity would be\n$$\n    I_G = 1 - \\left(\\frac{5}{20}\\right)^2 - \\left(\\frac{15}{20}\\right)^2 = 0.375\n$$\n\nWe can take all observations for a given categorical variable and value, and calculate the impurity of the target values. The lowest possible value of 0 means that all target values are identical (i.e. the distribution 100% pure), higher values imply more of a mixture."},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction_power_gini(df, value_vars=None, id_var='target'):\n    df_res = df.melt(\n        value_vars=value_vars, id_vars=id_var\n    ).groupby(['variable', 'value']).size().reset_index().drop(0, axis=1)\n    def gini(variable, value):\n        df_gini = df[df[variable] == value][id_var].value_counts()\n        impurity = 1\n        for val in df_gini:\n            impurity -= (val / df_gini.sum())**2\n        return impurity\n    df_res['gini'] = df_res.apply(lambda x: gini(x['variable'], x['value']), axis=1)\n    return df_res\n\ndf_toy_pred_gini = prediction_power_gini(df_toy, value_vars=['cat0', 'cat1', 'cat2', 'cat3'], id_var='target')\ndisplay(df_toy_pred_gini.style.background_gradient(cmap='coolwarm_r'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between a continuous & a categorical feature\n\nThis is the last combination that we haven't discussed yet. Although the target has been included with the correlation of the continuous features above (and it works in this case), it is actually a categorical variable, so I would like to try a technique a more appropriate technique for this case. The most straight forward way seems to be point biserial correlation, so let's give it a try, there is a corresponding function available in `scipy.stats`. Note, that method only works for a binary categorical variable. This is fine for the target, but we cannot use it to find the correlations e.g. between cont0 and cat1.\n\nThe values we get are essentiale the same ones as for Spearman or Pearson correlation - so perhaps in general it would not be worth to spend time on biserial correlation, at least not in the context of machine learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"biserial = [stats.pointbiserialr(df_toy['target'], df_toy[feature])[0] for feature in cont_features]\ndf_biserial = pd.DataFrame(dict(target_correlation=biserial), index=cont_features)\ndf_biserial.style.background_gradient(cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze the competition dataset\n\n## Read in the data files\n\nWe'll only use the train data here, since the test data does not contain the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = Path('/kaggle/input/tabular-playground-series-mar-2021/')\ndf_train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(df_train.head())\n\ncat_features = list(df_train.drop('target', axis=1).select_dtypes(include=['object', 'category']).columns)\ncont_features = list(df_train.drop('target', axis=1).select_dtypes(exclude=['object', 'category']).columns)\nprint(f'{len(df_train)} rows in train set, {len(cat_features)} categorical features, {len(cont_features)} continuous features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation between categorical features\n\nAs above, we'll use Cramer's V to check out the correlations among the categorical features and the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 16))\ndf_cramer = cramer_matrix(df_train, cat_features + ['target'])\nsns.heatmap(df_cramer, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={\"shrink\": .8})\nplt.title('Cramer`s V')\nplt.yticks(rotation=0);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that cat15, cat16 and cat18 have the strongest correlation with the target.\n\n## Digging deeper into categorical features\n\nLet's check if we can find the categorical values which are the best predictors for the target. We'll use the Gini impurity that was introduced for the toy dataset. We can make a simple plot of the impurity vs the variable. We see, that there is more or less the entire range from 0 to 1 for every variable. Many of the values with low impurity are extremely rare, so they are very useful for modeling. But we there are a few values that look interesting."},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_values(row):\n    return len(df_train[df_train[row['variable']] == row['value']])\n\ndf_train_pred_power = prediction_power_gini(df_train, value_vars=cat_features, id_var='target').reset_index()\ndf_train_pred_power['counts'] = df_train_pred_power.apply(count_values, axis=1)\ndisplay(df_train_pred_power.head())\ndisplay(sns.catplot(data=df_train_pred_power, y='variable', x='gini'))\ndisplay(sns.relplot(data=df_train_pred_power, hue='variable', y='gini', x='counts'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can filter out the points with few counts and high impurities to get a better look at the interesting values in the dataset. I think this is a strong case to not just ignore cat10, since it has some promising categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred_power_filtered = df_train_pred_power[(df_train_pred_power.gini < 0.25) & (df_train_pred_power.counts > 1000)]\ndisplay(df_pred_power_filtered.sort_values('gini'))\nsns.relplot(data=df_pred_power_filtered, hue='variable', y='gini', x='counts')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlations between continuous features\n\nAs we saw with the toy dataset, we can get away with using the pandas `corr` method for measuring correlation of continuous features with a bivariate target variable. We'll compare how the three different methods perform on the competition dataset.\n\nBut first, we'll visualize the distribution of the continuous features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(data=df_train[cont_features].melt(), x='value', col='variable', col_wrap=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are obviously not normal distributions, however, this may have an effect when calculating correlations (see below). We can perform a quantile transformation to normalize the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\ndf_train_norm = df_train.copy()\nfor feature in cont_features:\n    df_train_norm[feature] = QuantileTransformer(\n        output_distribution='normal').fit_transform(df_train_norm[feature].values.reshape(-1, 1))\n\nsns.displot(data=df_train_norm[cont_features].melt(), x='value', col='variable', col_wrap=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's see how big the difference is between the 3 correlation methods, and between normalized and not-normalized data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(3, 2, figsize=(20, 30))\nplt.subplots_adjust(wspace=0.3)\n\nmethods = ['pearson', 'kendall', 'spearman']\ndatasets = [('Not normalized', df_train), ('Normalized', df_train_norm)]\nfor i in range(3):\n    for j in range(2):\n        method = methods[i]\n        ax = axs[i, j]\n        df_label, df = datasets[j]\n        corr = df.corr(method=method)\n        mask = np.triu(np.ones_like(corr, dtype=np.bool))\n        # plot heatmap\n        sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n                    cbar_kws={\"shrink\": .8}, ax=ax)\n        # yticks\n        ax.tick_params(labelrotation=0);\n        ax.title.set_text(f'{df_label}, correlation method: {method.capitalize()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion:\n\nThe only significant difference between the correlations when using normalized vs non-normalized data happens for Pearson. Kendall & Spearman are not affected by normalizing, but the values for Kendall are somewhat smaller than the other two methods. Given that Spearman correlation is very similar to Pearson with normalized data, my personal take-away is this: **Forget about normalization and just use Spearman, avoid Pearson & Kendall.** In any case, even though the values are different, all of the above heat-maps tell a similar story. In the context of machine learning, the normalization and correlation method probably doesn't matter that much."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}