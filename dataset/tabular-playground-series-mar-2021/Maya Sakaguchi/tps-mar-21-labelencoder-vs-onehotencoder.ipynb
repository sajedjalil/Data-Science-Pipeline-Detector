{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Purpose\n\nIn this notebook, I'd like to compare LabeEncoder and OneHotEncoder in the LightGBM model.\n\nThank you [@maostack](http://) and I referred to [https://www.kaggle.com/maostack/tps-mar-baseline](https://www.kaggle.com/maostack/tps-mar-baseline) to build the LightGBM model.\n\n\n---Conclusion---\n\n**OneHotEncoder for cat1,cat5 and cat10** can slightly boost the score."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns',None)\nfrom itertools import product\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading Data (though I will now use the test data in this notebook)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\ntest=pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=train['target']\ntrain.drop(['id','target'],axis=1,inplace=True)\ntest_id=test['id']\ntest.drop(['id'],axis=1,inplace=True)\ncol_cat=train.select_dtypes('object').columns.to_list()\ncol_num=train.select_dtypes('number').columns.to_list()\n\ntrain.shape,y.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Function to encode categorical features separately (also applicable to adding PCA columns of numerical features)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SeparateEncoder():\n    def cat_transform(self,df,cat_cols,encoding_list):\n        df_mod=pd.DataFrame()\n        for idx,cat in zip(encoding_list,cat_cols):\n            if idx==0:#One Hot\n                df_oh=pd.get_dummies(df[cat],prefix=cat,prefix_sep='_')\n                df_mod=pd.concat([df_mod,df_oh],axis=1)\n            elif idx==1:#LabelEncoder\n                le=LabelEncoder()\n                df_le=pd.DataFrame(le.fit_transform(df[cat]),columns=[cat])\n                df_mod=pd.concat([df_mod,df_le],axis=1)\n        (_,cat_feature_count)=df_mod.shape\n        return df_mod,cat_feature_count\n    \n    def num_transform(self,df,num_cols,add_pca=True,n_components=5):\n        df_num=df[num_cols]\n        SS=StandardScaler()\n        df_num=pd.DataFrame(SS.fit_transform(df_num),columns=num_cols)\n        if add_pca:\n            pca=PCA(n_components=n_components,random_state=0)\n            df_pca=pd.DataFrame(pca.fit_transform(df_num))\n            df_num=pd.concat([df_num,df_pca],axis=1)\n        else:\n            pass\n        return df_num  \n    \n    def merge_transform(self,df,cat_cols,num_cols,encoding_list,add_pca=True,n_components=5):\n        df_cat,cat_feature_count=self.cat_transform(df,cat_cols,encoding_list)\n        df_num=self.num_transform(df,num_cols,add_pca=add_pca,n_components=n_components)\n        df_merge=pd.concat([df_cat,df_num],axis=1)\n        return df_merge,cat_feature_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_params = {'max_depth': 16,\n               'subsample': 0.8, \n               'colsample_bytree': 0.2,\n               'learning_rate': 0.01,\n               'reg_lambda': 10,\n               'reg_alpha': 17,\n               'min_child_samples': 31, \n               'num_leaves': 66,\n               'max_bin': 522,\n               'cat_smooth': 81,\n               'cat_l2': 0.03,\n               'metric': 'auc',\n               'objective':'binary',\n               'n_jobs': -1, \n               'n_estimators': 100,\n               'force_col_wise':True,\n               'verbosity':-2\n              }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits=5\nskf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=0)\nse=SeparateEncoder()\nresults=pd.DataFrame(columns=['encoding_list','score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If I try every combination (i.e. cat0: LabelEncoding/OneHotEncoding, cat1: LabelEncoding/OneHotEncoding, etc.), it will take tremendous amount of time.\n\nTherefore, in the first trial, I've chosen one categorical feature to apply OneHotEncoding, to see whether these choice improve the score or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfor i in range(len(col_cat)+1):\n    print(f'-----Cat{i}')\n    encoding_list=np.ones(len(col_cat))\n    if i==len(col_cat):pass\n    else:encoding_list[i]=0\n    df_merge,cat_feature_count=se.merge_transform(train,col_cat,col_num,encoding_list,add_pca=False)\n    score=0\n    for k,(train_idx,val_idx) in enumerate(skf.split(df_merge,y)):\n        print(f'------Fold{k+1}')\n        X_train,y_train=df_merge.iloc[train_idx,:],y[train_idx]\n        X_val,y_val=df_merge.iloc[val_idx,:],y[val_idx]\n\n        l_train=lgb.Dataset(X_train,y_train)\n        l_val=lgb.Dataset(X_val,y_val)\n        model = lgb.train(params=lgbm_params,\n                      num_boost_round=1000,\n                      early_stopping_rounds=400,\n                      train_set=l_train,\n                      valid_sets=[l_val,l_train],\n                      verbose_eval=500)         \n        val_pred=model.predict(X_val)\n        score+=roc_auc_score(y_val,val_pred)/n_splits\n\n    results.loc[i,'encoding_list']=encoding_list\n    results.loc[i,'score']=score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.sort_values('score',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since index=19 is controll (all LabelEncoder), applying OneHotEncoder to most of the categories (except cat3,9) might improve the score.\n\nFrom now, I'd like to focus on the top 5 categories (cat1,5,6,10,16) to figure out the exact combination of LabelEncoder or OneHotEncoder in those categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"results_2=pd.DataFrame(columns=['encoding_list','score'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom itertools import product\nfor i,e_list in enumerate(product([0,1],repeat=5)):\n    if sum(e_list)==5 or sum(e_list)==4: continue\n\n    print(f'-----Cat{i}')\n    encoding_list=np.ones(len(col_cat))\n    for l,m in zip([1,5,6,10,16],list(range(6))):\n        encoding_list[l]=e_list[m]\n\n    df_merge,cat_feature_count=se.merge_transform(train,col_cat,col_num,encoding_list,add_pca=False)\n    score=0\n    for k,(train_idx,val_idx) in enumerate(skf.split(df_merge,y)):\n        print(f'------Fold{k+1}')\n        X_train,y_train=df_merge.iloc[train_idx,:],y[train_idx]\n        X_val,y_val=df_merge.iloc[val_idx,:],y[val_idx]\n\n        l_train=lgb.Dataset(X_train,y_train)\n        l_val=lgb.Dataset(X_val,y_val)\n        model = lgb.train(params=lgbm_params,\n                      num_boost_round=1000,\n                      early_stopping_rounds=400,\n                      train_set=l_train,\n                      valid_sets=[l_val,l_train],\n                      verbose_eval=500)         \n        val_pred=model.predict(X_val)\n        score+=roc_auc_score(y_val,val_pred)/n_splits\n\n    results_2.loc[i,'encoding_list']=encoding_list\n    results_2.loc[i,'score']=score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results_2.sort_values('score',ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_idx=results_2.sort_values('score',ascending=False).head(1).index[0]\nbest_encoding_list=results_2.loc[best_idx,'encoding_list']\nbest_encoding_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"best_encoding_list\" shows that \n\n\n# OneHotEncoder for cat1, cat5, cat10\n\ncan slightly boost the score."},{"metadata":{},"cell_type":"markdown","source":"**Thanks for reading!**\n\n**I'm sorry for being messy at some parts.**\n\n**Please let me know if you have any questions and advice.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}