{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble        import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier \nfrom sklearn.neighbors       import KNeighborsClassifier\nfrom sklearn.svm             import LinearSVC\nfrom sklearn.naive_bayes     import GaussianNB\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.neural_network  import MLPClassifier\nfrom sklearn.tree            import DecisionTreeClassifier\nfrom sklearn.svm             import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model    import LogisticRegression\nfrom lightgbm                import LGBMClassifier\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration     import calibration_curve\nfrom sklearn.pipeline        import make_pipeline\nfrom sklearn.metrics         import plot_confusion_matrix\nfrom sklearn.metrics         import classification_report, confusion_matrix, accuracy_score\n%matplotlib inline","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting parameter","metadata":{}},{"cell_type":"code","source":"N_SPLITS   = 2         # Number of Stratified K Folds\nDATA_PERC  = 1         # Percentage of the training set (execution speed parameter during dev & test)\nSEED       = 1         # Random seed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data file","metadata":{}},{"cell_type":"code","source":"all_train = pd.read_csv(\"../input/tabular-playground-series-mar-2021/train.csv\")\nall_test = pd.read_csv(\"../input/tabular-playground-series-mar-2021/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Review data","metadata":{}},{"cell_type":"markdown","source":"## Review NaN impact","metadata":{}},{"cell_type":"code","source":"print(\"+-------------------- train ----------------------------+ +-------------------- test ----------------------------+\")\nrf = lambda df :  [ \"{:>10s} - Count: {:4d} - Nan: {:4d} - type:{:10s}\".format(col,df[col].count(),\n                                                                 df[col].isna().sum(),\n                                                                 df.dtypes[col].name) for col in df.columns]\nprint(\"\\n\")\n_ = [print(tr + tr) for (tr,te) in zip(rf(all_train),rf(all_test))]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training data","metadata":{}},{"cell_type":"code","source":"all_train.describe().T.style.background_gradient(cmap='YlOrRd',vmin=0,vmax=1,subset=pd.IndexSlice[:,'mean':'max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test data","metadata":{}},{"cell_type":"code","source":"all_test.describe().T.style.background_gradient(cmap='YlOrRd',vmin=0,vmax=1,subset=pd.IndexSlice[:,'mean':'max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Translate categorical feature\nAcross the all data set (train and test) to ensure encoded feature consistency","metadata":{}},{"cell_type":"code","source":"def build_feature(no_hot_array, hot_array,dataset) :\n        tmp = dataset[no_hot_array]\n        for a_hot in hot_array :\n             tmp = pd.concat([tmp, pd.get_dummies(dataset[a_hot],prefix=a_hot)], axis=1);   \n        return tmp\n    \ncont_columns = [col for col in all_train.columns if 'cont' in col]\ncat_columns  = [col for col in all_train.columns if 'cat' in col]\n\nall_data = pd.concat([all_train,all_test]).reset_index(drop=True)\nall_data_encoded  = build_feature(cont_columns, cat_columns,all_data)\nall_train_encoded = all_data_encoded[:all_train.shape[0]]\nall_test_encoded  = all_data_encoded[all_train.shape[0]:]\nprint(\"all_data.shape          : \",all_data.shape,\" - all_data_encoded.shape : \",all_data_encoded.shape, \" \")\nprint(\"all_train_encoded.shape : \",all_train_encoded.shape,\" - all_test_encoded.shape : \",all_test_encoded.shape, \" \")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select a subset of train data set for shorter runtime, during notebook developement","metadata":{}},{"cell_type":"code","source":"np.random.seed(SEED)\nmask = np.random.rand(all_train_encoded.shape[0]) <= DATA_PERC\ntrain_encoded = all_train_encoded[mask]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Review each classifiers","metadata":{}},{"cell_type":"code","source":"classifiers = [\n    LGBMClassifier(),\n#         KNeighborsClassifier(4),\n#         SVC(probability=True),\n#         DecisionTreeClassifier(),\n    RandomForestClassifier (n_estimators=20, random_state=0),\n#         AdaBoostClassifier(),\n#         GradientBoostingClassifier(),\n    GaussianNB(),\n    MLPClassifier( solver='adam', alpha=0.314, random_state=1, max_iter=4000,\n                       early_stopping=True, hidden_layer_sizes=[40, 40, 40], ),\n    LinearSVC(C=1.0),\n    LinearDiscriminantAnalysis(),\n#         QuadraticDiscriminantAnalysis(),\n#         LogisticRegression(max_iter=4000)\n]\n\nX = train_encoded.values\ny = all_train[mask].loc[:,\"target\"].values\nlog_res  = []\nfor clf in classifiers :\n        print(\"\\n----------------------------------------------------------\" )\n        print(\"Classifier: {:20s} - # fold: {:2d}\".format(clf.__class__.__name__,N_SPLITS) )\n        skf = StratifiedKFold(n_splits=N_SPLITS)\n        # skf.get_n_splits(X, y)\n        fold_no = 1\n        accuracy = []\n        for train_index, test_index in skf.split(X, y):\n            tic = time.perf_counter()\n            print(\"   fold: {:2d} -\".format(fold_no), end=\" \")\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n#             print(confusion_matrix(y_test,y_pred))\n#             print(classification_report(y_test,y_pred))\n            acc = accuracy_score(y_test, y_pred)\n            toc = time.perf_counter()\n            print(\"accuracy: {:2.2f} time: {:0.1f} sec\".format(100*acc,toc - tic))\n            accuracy.append(acc)\n            fold_no = fold_no + 1\n        log_entry = [clf, np.mean(accuracy) ,toc - tic,sum(y_train)/len(y_train)]\n        log_res.append(log_entry)\n\nlog_cols = [\"Classifier\", \"Accuracy\",\"Time\",\"target distrib. %\"]\nlog \t = pd.DataFrame(log_res, columns=log_cols)\nprint(log.sort_values(['Accuracy'],ascending=False))\n\n# Get best classifier \n\nbest_clf = log.loc[0,\"Classifier\"] \nprint(\"\\nBest classifier :\",best_clf)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare submission\nUse the best performing classifier","metadata":{}},{"cell_type":"code","source":"if DATA_PERC == 1 : \n    # Train best_clf on the entire train set\n    best_clf.fit(X,y)\n    y_train_pred = best_clf.predict(all_train_encoded)\n    acc = accuracy_score(y_train_pred, y)\n    print(\" Accuracy against the all train set : {:.4f}\".format(acc))\n\n    # Test prediction\n\n    y_test_pred = best_clf.predict(all_test_encoded)\n    print(log)\n\n    # Saving the file\n    sub = pd.DataFrame({'id': all_test['id'].values, 'target': y_test_pred})\n    sub.to_csv('sub.csv', index=False)\nelse :\n    print(\"Set DATA_PERC to 1 to save submission\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}