{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Playground Series\n\n\n**Work in progress**\n\n- Still to do: fine-tune models and perform feature engineering \n\nIn this notebook I will focus on tree methods (Random Forest, CatBoost, XGBoost), and I will also take a deep-dive in to Random Forest - elimination features that aren't above a self-defined importance threshold, and visualising how each individual feature combines to build the final prediction for an individual row.\n\n\n**Overview**\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\n**The data**\n\nTrain set has 300,000 rows while test set has 200,000 rows.\nThere are 19 categorical features from cat0 - cat18 and 11 continuous features from cont0 - cont10 with total of 30 features.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')\nsub_df = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/sample_submission.csv')\n\ntest_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the data between Continuous & Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_cols = train_df.drop(['id', 'target'], axis=1).columns\n\n## Split numeric & categoric data. Several ways to do this... \nnumerical_columns = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\ncategorical_columns = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\n## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_indexs = train_df.index\ntest_indexs = test_df.index\n\ndf =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\ndf = df.drop(['id', 'target'], axis=1)\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n\nWe do see that some features have strong correlations in both the Train (left) & Test (right) datasets:\n\n- Cont2 & Cont1 have a correlation of 0.9; this is very high\n- Several other features are correlated strongly too\n\nThis is good for our model"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(18, 8), facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=-0.36, hspace=0.27)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ncolors = [\"#fbfbfb\", \"lightgray\",\"#0e4f66\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(0, 0, 'Train', fontsize=20, fontweight='bold', fontfamily='serif',color='lightgray')\n\nax1.set_facecolor(background_color)\nax1.text(9.5, 11, 'Test', fontsize=20, fontweight='bold', fontfamily='serif',color='lightgray')\n\n\nfig.text(0.5,0.5,'Correlation of Features\\nFor Train & Test\\nDatasets', fontsize=20, fontweight='bold', fontfamily='serif',va='center',ha='center')\n\ncorr = train_df[numerical_columns].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr, ax=ax0, vmin=-1, vmax=1, annot=True, square=True, mask=mask,\n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g',linewidth=3,linecolor=background_color)\n\n\ncorr = test_df[numerical_columns].corr().abs()\nmask = np.tril(corr)\nsns.heatmap(corr, ax=ax1, vmin=-1, vmax=1, annot=True, square=True, mask=mask,\n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g',linewidth=3,linecolor=background_color)\nax1.xaxis.tick_top()\nax1.yaxis.tick_right()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(12, 6), facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.set_facecolor(background_color)\nax0.text(-1.1, 0.26, 'Correlation of Continuous Features with Target', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.1, 0.24, 'We see correlation in both directions, with cont5 having the highest positive correlation.' ,fontsize=13, fontweight='light', fontfamily='serif')\n\nchart_df = pd.DataFrame(train_df[numerical_columns].corrwith(train_df['target']))\nchart_df.columns = ['corr']\nchart_df['positive'] = chart_df['corr'] > 0\n\nsns.barplot(x=chart_df.index, y=chart_df['corr'], ax=ax0, palette=chart_df.positive.map({True: '#0e4f66', False: 'gray'}), zorder=3,dodge=False)\nax0.grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_ylabel('')\n\n\nfor s in [\"top\",\"right\", 'left']:\n    ax0.spines[s].set_visible(False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.offsetbox import AnchoredText\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfig, ax = plt.subplots(3, 4, figsize=(17, 12), sharex=True)\nfig.set_facecolor('#f6f5f5') \n\nfor i in range(11): \n    sns.kdeplot(data=train_df[train_df['target']==1], x=f'cont{i}', \n                fill=True,\n                linewidth=0,\n                color='#0e4f66', alpha=1,\n                ax=ax[i%3][i//3])\n    \n    sns.kdeplot(data=train_df[train_df['target']==0], x=f'cont{i}', \n                fill=True,\n                linewidth=0,\n                color='#d0d0d0', alpha=0.8,\n                ax=ax[i%3][i//3])\n    \n    ax[i%3][i//3].set_yticks([])\n    ax[i%3][i//3].set_ylabel('',visible=False)\n    ax[i%3][i//3].set_xlabel('',visible=False)\n    ax[i%3][i//3].margins(0.05, 0.2)\n    ax[i%3][i//3].set_facecolor(background_color) \n    for s in [\"top\",\"right\", 'left']:\n            ax[i%3][i//3].spines[s].set_visible(False)           \n            \n    # bar\n    divider = make_axes_locatable(ax[i%3][i//3])\n    cax = divider.append_axes(\"top\", size=\"8%\", pad=0)\n    cax.get_xaxis().set_visible(False)\n    cax.get_yaxis().set_visible(False)\n    cax.set_facecolor('black')\n\n    at = AnchoredText(f'cont{i}', loc=10, \n                      prop=dict(backgroundcolor='black',\n                                size=10, color='white', weight='bold'))\n    cax.add_artist(at)\n    \nax[-1][-1].set_visible(False)\nfig.text(0.018, 1.03, 'Continuous Feature Distribution by Target [Train]', fontsize=20, fontweight='bold', fontfamily='serif')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\n\nI will focus on tree methods. \n\nAs a baseline, I will use a simple Random Forest Classifier, then progress to gradient boosting with CatBoost and XGBoost. \n\nFinally, I will use a soft Voting Classifier to see if the scores of each model individually can be improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Various libraries I may or may not use...\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom category_encoders import CatBoostEncoder, LeaveOneOutEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score\nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding catergorical variables\n\nfor c in train_df.columns:\n    if train_df[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train_df[c].values) + list(test_df[c].values))\n        train_df[c] = lbl.transform(train_df[c].values)\n        test_df[c] = lbl.transform(test_df[c].values)\n        \ndisplay(train_df.head())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_df.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(train_df, target, train_size=0.80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Score: 0.88022\n\nThis is pretty good for our first model.\n\nThe benefits of a model such as this is that it is incredibly fast to train - an important attribute in industry."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Random Forest\n\nrfc = RandomForestClassifier(n_estimators=300, max_depth=10, n_jobs=-1)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\nscore = roc_auc_score(y_test, rfc_pred)\nprint(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_df = pd.DataFrame(data=[roc_auc_score(y_test, rfc_pred)], \n             columns=['Random Forest Score'],\n             index=[\"ROC AUC Score\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Diving deeper in to the Random Forest model interpretation\n\nLet's look first at feature importance, and then at how a single record is effected by each variable by using a Waterfall Chart"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'Feature':df.columns, 'Importance':m.feature_importances_}\n                       ).sort_values('Importance', ascending=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(rfc, X_train)\nfi[:10]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In industry, a simpler model is often better\n\nThis makes running the model faster, and the results easier to explain,\n\nLet's now re-run the Random Forest model with only those feautures with an importance greater than 0.005\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only keeping feautures with importance greater than 0.005\nto_keep = fi[fi.Importance>0.005].Feature\nlen(to_keep)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_imp  =  X_train[to_keep]\nX_test_imp  = X_test[to_keep]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Re-fitting with fewer variables\nrfc = RandomForestClassifier(n_estimators=300, max_depth=10, n_jobs=-1)\nrfc.fit(X_train_imp, y_train)\nrfc_pred = rfc.predict_proba(X_test_imp)[:, 1] # This grabs the positive class prediction\nscore = roc_auc_score(y_test, rfc_pred)\nprint(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A decent score, but with far few variables to study"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train.columns), len(X_train_imp.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install treeinterpreter\n!pip install waterfallcharts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hide\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction is simply the prediction that the random forest makes. bias is the prediction based on taking the mean of the dependent variable (i.e., the model that is the root of every tree). contributions is the most interesting bitâ€”it tells us the total change in predicition due to each of the independent variables. Therefore, the sum of contributions plus bias must equal the prediction, for each row. Let's look just at the first row:"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrow =  X_test_imp.values[None,100] #This grabs the row we want to examine, row 100 chosen here\nprediction, bias, contributions = treeinterpreter.predict(rfc, row)\nprediction[0], bias[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contributions = [contributions[0][i][0] for i in range(len(contributions[0]))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames = X_test_imp.columns[0:].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Waterfall Chart\n\nWhy is this useful?\n\nImagine a scenario where a business wants to know why a given prediciton has been made for a certain row or input. Instead of giving them the general feature importance, we can go one step further and actually show how the features indiviudual interact and produce the end-result"},{"metadata":{},"cell_type":"markdown","source":"Source for the Waterfall visual:\n\nhttps://github.com/chrispaulca/waterfall\n\nVery informative, even more so for regression problems.\n\nI haven't figured out true customization of this visual yet, so this is pretty much 'out-of-the-box'"},{"metadata":{"trusted":true},"cell_type":"code","source":"import waterfall_chart\nmy_plot=waterfall_chart.plot(colnames,contributions, rotation_value=90, threshold=0.3,formatting='{:,.3f}',net_label=\"End Result\", other_label=\"Remaining Vars\",\n            Title=\"Waterfall Chart: How does each variable effect the outcome?\", x_lab = \"Variables\", y_lab = \"Prediction\",green_color='#247747',blue_color='#0e4f66',red_color='#ff0000')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cross Val - took a long time with no benefit\n\n#print(cross_val_score(rfc,X_train, y_train, scoring=\"roc_auc\", cv = 5))\n#mean_score = cross_val_score(rfc,X_train, y_train, scoring=\"roc_auc\", cv = 5).mean()\n#std_score = cross_val_score(rfc,X_train, y_train, scoring=\"roc_auc\", cv = 5).std()\n\n#print(mean_score)\n#print(std_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost scored even better: 0.89167"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'iterations': 5000,\n         'learning_rate':0.013933182980403087,\n          'reg_lambda': 47.79748127808107,\n         'depth':5,\n         'eval_metric':'AUC',\n         'verbose':200,\n         'od_type':'Iter',\n         'od_wait':50}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_model = CatBoostClassifier(**params)\ncat_model.fit(X_train,y_train)\ncb_pred = cat_model.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\nscore = roc_auc_score(y_test, cb_pred)\nprint(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_df = pd.DataFrame(data=[roc_auc_score(y_test, cb_pred)], \n             columns=['CatBoost Score'],\n             index=[\"ROC AUC Score\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost scored: 0.89167\n\nVery similar to CatBoost. A good score, but as with CatBoost takes a while to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(eval_metric=\"auc\",\n                          random_state=42,\n                          tree_method=\"gpu_hist\",\n                          gpu_id=\"0\",\n                          use_label_encoder=False,verbose=200)\n\n\nxgb_model = CatBoostClassifier(**params)\nxgb_model.fit(X_train,y_train)\nxgb_pred = xgb_model.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\nscore = roc_auc_score(y_test, xgb_pred)\nprint(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_df = pd.DataFrame(data=[roc_auc_score(y_test, xgb_pred)], \n             columns=['XGBoost Score'],\n             index=[\"ROC AUC Score\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(xgb_model, X_train)\nxgb10_fi = fi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"background_color = \"#f6f5f5\"\n\nfig, ax = plt.subplots(1,1, figsize=(10, 8),facecolor=background_color)\n\ncolor_map = ['gray' for _ in range(10)]\ncolor_map[0] = color_map[1] = color_map[2] =  '#0e4f66' # color highlight\n\nsns.barplot(data=xgb10_fi,x='Importance',y='Feature',ax=ax,palette=color_map)\nax.set_facecolor(background_color) \nfor s in ['top', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n    \nfig.text(0.12,0.92,\"Feature Importance for XGBoost\", fontsize=20, fontweight='bold', fontfamily='serif')\n\n    \nplt.xlabel(\" \", fontsize=12, fontweight='light', fontfamily='serif',loc='left',y=-1.5)\nplt.ylabel(\" \", fontsize=12, fontweight='light', fontfamily='serif')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Voting Classifier: 88.957\n\n\nVoting classifier takes all of the inputs and averages the results. \n\nFor a \"hard\" voting classifier each classifier gets one vote, \"yes\"/1 or \"no\"/0, and the result is just a popular vote; you'll generally want odd numbers here so there is never a tie.\n\nA \"soft\" classifier - the one I have opted for here - averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such.\n\nI'll combined the Random Forest, CatBoost, & XGBoost. "},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf = VotingClassifier(estimators = [('rfc',rfc),('cat_model',cat_model),('xgb_model',xgb_model)], voting = 'soft') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"voting_clf.fit(X_train,y_train)\nvc_pred = voting_clf.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\nscore = roc_auc_score(y_test, vc_pred)\nprint(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vc_df = pd.DataFrame(data=[roc_auc_score(y_test, vc_pred)], \n             columns=['Voting Classifier Score'],\n             index=[\"ROC AUC Score\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nI haven't tuned any of these models to find optimal parameters simply becasue it takes far too long on my computer. \n\nA simple GridSearch would likley improve these scores by a few points.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall\ncolors = [\"lightgray\",\"#0e4f66\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\nbackground_color = \"#f6f5f5\"\n\n\nfig = plt.figure(figsize=(12,3), facecolor=background_color) # create figure\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\n\ndf_models = round(pd.concat([vc_df,rf_df,cb_df,xgb_df], axis=1),3)\nsns.heatmap(df_models, cmap=colormap,annot=True,fmt=\".2%\",vmax=0.891,vmin=0.89, linewidths=2.5,cbar=False,annot_kws={\"fontsize\":15})\n\n\nax0.text(-0.63,-0.4,'Model Performance Overview & Selection',fontfamily='serif',fontsize=20,fontweight='bold')\nax0.text(-0.63,-0.23,'We tried several tree models, but Gradient Boosting Models out-performed the rest.',fontfamily='serif',fontsize=15)\n\n\n\nax0.set_yticklabels(ax0.get_yticklabels(), fontfamily='serif', rotation = 0, fontsize=12)\nax0.set_xticklabels(ax0.get_xticklabels(), fontfamily='serif', rotation=0, fontsize=12)\n\nfor lab, annot in zip(ax0.get_xticklabels(), ax0.texts):\n    text =  lab.get_text()\n    if text == 'CatBoost Score': \n        # set the properties of the ticklabel\n        lab.set_weight('bold')\n        lab.set_size(15)\n        lab.set_color('black')\n        \nfor lab, annot in zip(ax0.get_xticklabels(), ax0.texts):\n    text =  lab.get_text()\n    if text == 'XGBoost Score': \n        # set the properties of the ticklabel\n        lab.set_weight('bold')\n        lab.set_size(15)\n        lab.set_color('black')\n        \nfrom matplotlib.patches import Rectangle\n\n\n#ax0.add_patch(Rectangle((0, 0), 2, 4, fill=True,color='#0e4f66', lw=0,alpha=0.5))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rather than Label Encoding, I'll try One-Hot Encoding\n\nTo explore whether or not how I encoded the categorical variables makes a difference to the ROC AUC score of our models."},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\n#test_df = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')\n\n\n#feature_cols = train_df.drop(['id', 'target'], axis=1).columns\n\n## Split numeric & categoric data. Several ways to do this... \n#numerical_columns = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\n#categorical_columns = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\n## Join train and test datasets in order to obtain the same number of features during categorical conversion\n#train_indexs = train_df.index\n#test_indexs = test_df.index\n\n#df =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\n#df = df.drop(['id', 'target'], axis=1)\n\n#train_df = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\n#test_df = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')\n#combine_df = pd.concat([train_df, test_df], axis=0)\n\n#for col in categorical_columns:\n#    combine_df[col] = pd.get_dummies(combine_df[col],drop_first=True)\n#train_df = combine_df.iloc[:len(train_df), :]\n#test_df = combine_df.iloc[len(train_df):, :]\n#test_df = test_df.drop('target', axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target = train_df.pop('target')\n#X_train, X_test, y_train, y_test = train_test_split(train_df, target, train_size=0.80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple Random Forest\n\n#rfc = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\n#rfc.fit(X_train, y_train)\n#rfc_pred = rfc.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n#score = roc_auc_score(y_test, rfc_pred)\n#print(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#xgb = XGBClassifier(eval_metric=\"auc\",\n #                         random_state=42,\n  #                        tree_method=\"gpu_hist\",\n   #                       gpu_id=\"0\",\n    #                      use_label_encoder=False,verbose=200)\n\n\n#xgb_model = CatBoostClassifier(**params)\n#xgb_model.fit(X_train,y_train)\n#xgb_pred = xgb_model.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n#score = roc_auc_score(y_test, xgb_pred)\n#print(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cat_model = CatBoostClassifier(**params)\n#cat_model.fit(X_train,y_train)\n#cb_pred = cat_model.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\n#score = roc_auc_score(y_test, cb_pred)\n#print(f'{score:0.5f}') # 0.873 shows we're doing better than a dummy model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# All models performed worse with One-Hot Encoding\n\n# Conclusion\n\nWithout recourse to hyperparameter tuning, I am happy with the results I acheived. \n\nI will next try feature engineering as I do beleive we can acheive a better score with only a little extra effort.\n\n\nWe also took a deep-dive into Random Forest model interpretation - a useful skill for sure.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}