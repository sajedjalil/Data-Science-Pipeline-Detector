{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nfrom datetime import datetime\nimport pickle\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport gc\nimport random\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.model_selection import StratifiedKFold\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you find this notebook useful, please visit original discussion post (#1 Solution in Feb21 Comp, which is where I have taken this code from) and upvote.\n\nhttps://www.kaggle.com/c/tabular-playground-series-feb-2021/discussion/222745\n\nAnd winning solution from Jan\n\nhttps://www.kaggle.com/c/tabular-playground-series-jan-2021/discussion/216037"},{"metadata":{},"cell_type":"markdown","source":"Some credit with regards to pytorch code is also due to this notebook below from MOA competition. I've modified the code quite a bit since then but the outline is probably still recognisable.\n\nhttps://www.kaggle.com/namanj27/new-baseline-pytorch-moa/notebook"},{"metadata":{},"cell_type":"markdown","source":"**VERSION 5 - this version is modified to run direct from train data through DAE / MLP**\n\nThis doesn't really change end result (you can play around with settings in either case), but it's easier on RAM and gives the option to add a little noise while training MLP."},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=42):\n    print('Setting Random Seed')\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{},"cell_type":"markdown","source":"The weights used in this notebook (VERSION 5) are from a run with the best settings I managed to develop during the competition."},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/tabular-playground-series-mar-2021/'\nSAVE_PATH = ''\n\nrun_key = 'MLPMARTAB_FINAL_DAE'\n\nCFG = {'debug':False,\n       'debug_epochs':2,\n    'lr' : 1e-04,\n    'weight_decay':9.72918866945795E-06,\n    'epochs':10,\n    'device':'cuda',\n    'nfolds':10,       \n    'mlp_hidden_size':391,\n       'mlp_size_decline' : 1.0,\n    'mlp_batch_size':512,\n    'mlp_dropout':0.3,\n       'bce_smooth' : 0.0001,\n    'target_dae' : '/kaggle/input/tabmar21-final-dae-030421/',\n'target_epoch' : 'DAE_TABMAR21_ST3_model_checkpoint_final.pth',\n       'random_seed':0,\n       'mlp_start_noise' : 0.15,\n       'mlp_noise_decay' : 0.65\n    }\n\nif CFG['debug']:\n    CFG['epochs'] = CFG['debug_epochs']\n\nwith open(SAVE_PATH+f\"{run_key}_CFG\", 'wb') as f:\n    pickle.dump(CFG, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot noise during MLP training\nplt.plot(range(CFG['epochs']),\n        np.array([CFG['mlp_start_noise']*(CFG['mlp_noise_decay']**e) for e in range(CFG['epochs'])]))\nplt.xlabel('Epochs')\nplt.ylabel('Noise During Training')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fts_categorical = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', \n                   'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\n\nfts_continuous = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\n\n#unique counts should be the count of train PLUS test\nunique_counts=[  2,  15,  19,  13,  20,  84,  16,  51,  61,  19, 307,   2,   2,\n         2,   2,   4,   4,   4,   4]\n\nprint('Categorical Features', fts_categorical)\nprint('Continuous Features', fts_continuous)\n\nprint('Categorical Feature Count', len(fts_categorical))\nprint('Continuous Feature Count', len(fts_continuous))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is just to control settings for the DAE hidden size etc, there is no further training of the DAE in this notebook\n\nDAE_CFG = {'batch_size' : 384,\n    'init_lr' : 3e-4,\n    'lr_decay' : .998,\n    'noise_decay' : 0.999,\n    'max_epochs' : 600,\n    'save_freq':50,\n    'hidden_size':1024,\n    'num_subspaces':8,\n    'embed_dim':128,\n    'num_heads':8,\n    'dropout':0,\n    'feedforward_dim':512,\n    'emphasis':.75,\n    'task_weights':[len(fts_categorical), len(fts_continuous)],\n    'mask_loss_weight':2,\n    'prob_categorical' : 0.5,\n    'prob_continuous' : 0.5,}\n\n\nmodel_params = dict(\n    hidden_size=DAE_CFG['hidden_size'],\n    num_subspaces=DAE_CFG['num_subspaces'],\n    embed_dim=DAE_CFG['embed_dim'],\n    num_heads=DAE_CFG['num_heads'],\n    dropout=DAE_CFG['dropout'],\n    feedforward_dim=DAE_CFG['feedforward_dim'],\n    emphasis=DAE_CFG['emphasis'],\n    mask_loss_weight=DAE_CFG['mask_loss_weight']\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DAE Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data import Dataset\n\ndef get_data():\n    train_data = pd.read_csv(PATH+'train.csv')\n    test_data = pd.read_csv(PATH+'test.csv')\n    \n    #combine train and test data vertically\n    X_nums = np.vstack([\n        train_data.iloc[:, 20:-1].to_numpy(),\n        test_data.iloc[:, 20:].to_numpy()\n    ])\n    X_nums = (X_nums - X_nums.mean(0)) / X_nums.std(0) #normalize\n    \n    #stack the categorical data\n    X_cat = np.vstack([\n        train_data.iloc[:, 1:20].to_numpy(),\n        test_data.iloc[:, 1:20].to_numpy()\n    ])\n    #encode the categoricals\n    encoder = OneHotEncoder(sparse=False)\n    X_cat = encoder.fit_transform(X_cat)\n    \n    #join the categorical and continuous data horizontally\n    X = np.hstack([X_cat, X_nums])\n    y = train_data['target'].to_numpy().reshape(-1, 1)\n    return X, y, X_cat.shape[1], X_nums.shape[1] #this lets us know how many categorical and continuous features there are\n\n\nclass FeatureDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx], dtype=torch.float)            \n        }\n        return dct\n    \n    \nclass TestFeatureDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),   \n        }\n        return dct ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bce_logits = torch.nn.functional.binary_cross_entropy_with_logits\nmse = torch.nn.functional.mse_loss\n\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n        super().__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n    \n    def forward(self, x_in):\n        attn_out, _ = self.attn(x_in, x_in, x_in)\n        x = self.layernorm_1(x_in + attn_out)\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n        x = self.layernorm_2(x + ff_out)\n        return x\n\n\nclass TransformerAutoEncoder(torch.nn.Module):\n    def __init__(\n            self, \n            num_inputs, \n            n_cats, \n            n_nums, \n            hidden_size=1024, \n            num_subspaces=8,\n            embed_dim=128, \n            num_heads=8, \n            dropout=0, \n            feedforward_dim=512, \n            emphasis=.75, \n            task_weights=[len(fts_categorical), len(fts_continuous)],\n            mask_loss_weight=2,\n        ):\n        super().__init__()\n        assert hidden_size == embed_dim * num_subspaces\n        self.n_cats = n_cats\n        self.n_nums = n_nums\n        self.num_subspaces = num_subspaces\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.emphasis = emphasis\n        self.task_weights = np.array(task_weights) / sum(task_weights)\n        self.mask_loss_weight = mask_loss_weight\n\n        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        \n        \n        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n\n    def divide(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n        return x\n\n    def combine(self, x):\n        batch_size = x.shape[1]\n        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n        return x\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        \n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        x = self.combine(x3)\n        \n        predicted_mask = self.mask_predictor(x)\n        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n        return (x1, x2, x3), (reconstruction, predicted_mask)\n\n    def split(self, t):\n        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n\n    #def feature(self, x):\n        #attn_outs, _ = self.forward(x)\n        #return torch.cat([self.combine(x) for x in attn_outs], dim=1)\n    \n    #i have modified the feature output to include the reconstruction / mask outputs\n    #this needs checking in more detail - think range of values may be different\n    def feature(self, x):\n        #this returns the autoencoder layer outputs as a concatenated feature set\n        attn_outs, _ = self.forward(x)\n        attn_outs = torch.cat([self.combine(x) for x in attn_outs], dim=1)\n        masks = torch.cat([x for x in _], dim=1)\n        return torch.cat([attn_outs, masks], dim=1)\n\n    def loss(self, x, y, mask, reduction='mean'):        \n        _, (reconstruction, predicted_mask) = self.forward(x)\n        \n        x_cats, x_nums = self.split(reconstruction)\n        y_cats, y_nums = self.split(y)\n        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none'))\n        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n        \n        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n        \n        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n\n        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SwapNoiseMasker(object):\n    def __init__(self, probas):\n        self.probas = torch.from_numpy(np.array(probas))\n\n    def apply(self, X):\n        #provides a distribution of points where we want to corrupt the data        \n        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n        \n        #provides a corruped X output\n        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n        \n        #calculates the mask which we aim to predict\n        mask = (corrupted_X != X).float()\n        return corrupted_X, mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MLP Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features=3000, num_targets=1, hidden_size=1000):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(CFG['mlp_dropout'])\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(CFG['mlp_dropout'])\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(CFG['mlp_dropout'])\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Smoothed BCE Loss"},{"metadata":{},"cell_type":"markdown","source":"Note: I don't observe a huge benefit from smoothing - maybe a small gain from a very small smoothing amount. Just used this to have the option. There are some good discussions / notebooks on label smoothing elsewhere on Kaggle if not familiar."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.modules.loss import _WeightedLoss\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)        \n        \n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, epoch, device=CFG['device']):\n    \n    dae.eval()\n    model.train()\n    final_loss = 0  \n    \n    noise_maker = SwapNoiseMasker(CFG['mlp_start_noise']*(CFG['mlp_noise_decay']**epoch))\n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        \n        inputs, mask = noise_maker.apply(inputs)\n        \n        outputs = model(dae.feature(inputs)) #pass source data through DAE and MLP in one line\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        \n        optimizer.step()\n        scheduler.step()        \n        final_loss += loss.item()        \n    final_loss /= len(dataloader)    \n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device=CFG['device']):\n    \n    dae.eval()\n    model.eval()\n    final_loss = 0\n    valid_preds = []    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(dae.feature(inputs))\n        loss = loss_fn(outputs, targets)        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device=CFG['device']):\n    \n    dae.eval()\n    model.eval()\n    preds = []\n    for data in dataloader:\n        inputs = data['x'].to(device)\n        with torch.no_grad():\n            outputs = model(dae.feature(inputs))\n        #this predicts by BATCH requiring listing and concatenation\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n    \n    #then we need to concatenate the list of batches\n    preds = np.concatenate(preds).reshape(-1,)  \n    return preds\n\n\ndef run_training(X, y, len_train, len_test,folds, seed=42,batch_size=256, model_name='model',\n               num_features=3000,\n            num_targets=1,\n            hidden_size=1000,\n                device=CFG['device']):   \n    \n    print(len_train)\n    print(len_train+len_test)\n    \n    seed_everything(seed)    \n    \n    #placeholder - out of fold predictions\n    oof = np.zeros((len_train, ))\n    \n    #placeholder - test predictions\n    predictions = np.zeros((len_test, ))\n    \n    #placeholder - training/validation graph\n    fig,axes=plt.subplots(figsize=(18,6))\n    \n    #fold losses list\n    fold_losses = []\n    \n    for fold in sorted(np.unique(folds,return_counts=False)): \n        train_idx=folds[:len_train]!=fold\n        valid_idx=folds[:len_train]==fold\n            \n        print('     ')\n        print(f'training for fold {fold}')\n        #create the data set\n        train_dataset = FeatureDataset(X[:len_train][train_idx], y[:len_train][train_idx])\n        valid_dataset = FeatureDataset(X[:len_train][valid_idx], y[:len_train][valid_idx])\n\n        #apply to the data loader\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)        \n        \n        #create the model itself\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n\n        #send to device and set up the loss and optimizer\n        model.to(device)\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'],eps=0.00001 )\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                                  max_lr=1e-2, epochs=CFG['epochs'], steps_per_epoch=len(trainloader))\n\n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing =CFG['bce_smooth'])\n\n        train_loss_list = []\n        valid_loss_list = []\n        best_loss = 9999999\n        for epoch in range(CFG['epochs']):\n            #the below updates the model and loss\n            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader,epoch, device)            \n            train_loss_list+=[train_loss]\n            \n            #the below returns the validation predictions for the fold for each epoch\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, device)\n            valid_loss_list+=[valid_loss]            \n            \n            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n            \n            if valid_loss < best_loss:\n                #continue training if improving\n                best_loss = valid_loss\n                oof[valid_idx] = valid_preds.reshape(-1,)\n                torch.save(model.state_dict(), f\"MODEL_{model_name}_FOLD_{fold}SEED{seed}_.pth\")                    \n                    \n        fold_losses += [valid_loss_list[-1]]\n                    \n        del trainloader, validloader, train_dataset, valid_dataset\n        gc.collect()\n\n        sns.lineplot(x=range(CFG['epochs']), y=pd.Series(train_loss_list), color='Blue', ax=axes)\n        sns.lineplot(x=range(CFG['epochs']), y=pd.Series(valid_loss_list), color='Red', ax=axes)\n        \n        #--------------------- PREDICTION---------------------\n        #predict test data for fold\n        testdataset = TestFeatureDataset(X[len_train:len_train+len_test])\n        testloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size, shuffle=False)\n\n        #we create the model and then we input the latest weights\n        model = Model(\n            num_features=num_features,\n            num_targets=num_targets,\n            hidden_size=hidden_size,\n        )\n\n        model.load_state_dict(torch.load(f\"MODEL_{model_name}_FOLD_{fold}SEED{seed}_.pth\"))\n        model.to(device)\n\n        #predictions need to be added for the fold\n        predictions += inference_fn(model, testloader, device)            \n    \n    print('finished with fold losses', fold_losses)\n    \n    predictions/=CFG['nfolds']\n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data and Model and Create Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"#  get data\nX, Y, n_cats, n_nums = get_data()\n\n# setup model\ndae = TransformerAutoEncoder(\n    num_inputs=X.shape[1],\n    n_cats=n_cats,\n    n_nums=n_nums,\n    **model_params\n).cuda()\n\ndae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Using Simple Target Stratified KFold')\n\nskf = StratifiedKFold(n_splits=CFG['nfolds'], random_state=42,\n                          shuffle=True)\n\nfolds=np.zeros((len(X),)).astype(np.int32)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X[:300000], Y)):\n    folds[val_idx] = fold\n\nfold_values = sorted(np.unique(folds,return_counts=False))\nfold_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Weights and Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load weights and train\nmodel_state = torch.load(CFG['target_dae']+CFG['target_epoch'])\ndae.load_state_dict(model_state['model'])\n\n#note - I am not creating and saving features because the DAE will create the feature for each batch and immediately feed to MLP\n\n#run training\noof, predictions = run_training(X, Y, 300000, 200000,folds, \n                    seed=CFG['random_seed'],batch_size=CFG['mlp_batch_size'], model_name='model',\n               num_features=3072+2*642,\n            num_targets=1,\n            hidden_size=CFG['mlp_hidden_size'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df = pd.read_csv(PATH+'train.csv')[['id', 'target']]\noof_df['oof_prediction'] = oof\n\nprint('CV Score', roc_auc_score(oof_df['target'], oof_df['oof_prediction']))\noof_df.to_csv(SAVE_PATH+f'mlp_{run_key}_oof.csv', index=False)\noof_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(PATH+'sample_submission.csv')\nsubmission['target'] = predictions\nsubmission.to_csv(SAVE_PATH+f'mlp_{run_key}_submission.csv', index=False)\nsubmission.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}