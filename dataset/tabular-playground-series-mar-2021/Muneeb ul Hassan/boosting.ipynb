{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom catboost import CatBoostClassifier, Pool\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom sklearn.metrics import plot_confusion_matrix, roc_auc_score, plot_roc_curve, classification_report\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv')\ntest  = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Purpose of this Notebook is to use the data directly and train on it and do prediction. Usually if we have a mix of Categorical and Continuous Features we need to convert it to ONE HOT Encoding. But Boosting frameworks now provide a way to directly use Categorical Features. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'train: {len(train)}, test: { len(test)}')\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop(columns=['id', 'target'])\ny_train = train['target']\nx_train = x_train.values\ny_train = y_train.values\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n\n# Need to specify which columns are Categorical. You need to provide the indices of columns and also the name of the columns.\n#categorical_features = [col for c, col in enumerate(train.columns) if 'cat' in col]\ncategorical_features =[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model_1 = CatBoostClassifier(verbose=True)\n#model_1.fit(x_train, y_train, cat_features=categorical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Model with CatBoost Got the score 0.77076 on Public LB. As it is very naive model, so it is expected.**\n\n*Now we can test the new Framework lightGBM *"},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_feat_train = list(train.loc[:, train.dtypes == 'object'].columns.values)\nobj_feat_test = list(test.loc[:, test.dtypes == 'object'].columns.values)\nfor feature in obj_feat_train:\n    train[feature] = train[feature].astype('category')\nfor feature in obj_feat_test:\n    test[feature] = test[feature].astype('category')\n    \n\nx_train = train.drop(columns=['id', 'target'])\ny_train = train['target']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\ntrain_data = lgb.Dataset(x_train, label=y_train,categorical_feature=[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18])\nvalid_data = lgb.Dataset(x_valid, label=y_valid,categorical_feature=[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18])\n\n#Specifying the parameter\nparams={}\nparams['objective']='binary' #Binary target feature\nparams['metric']='binary_logloss' #metric for binary classification\n#train the model \n#model_2 = lgb.train(params,train_data, valid_sets=valid_data) #0.347189","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LightGBM increase the score to 0.88537, which is a lot without any parameter tuning. CatBoost can also get this score but it need to be tuned.**\n> VERSION: 03 **Now we will do some tuning this lightGBM to make it work more better.** \n\nA person in the comments suggested me to try the OPtuna OPtimization. SO i thought why not we give it a try so here. we are trying that."},{"metadata":{},"cell_type":"markdown","source":"[Thanks to Raj Gandhi for K_FOLD_CV Function](https://www.kaggle.com/rajgandhi/tps-march-lgbm-7-fold-cv)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def K_fold_CV(X, y, model, params, folds=5):\n    roc_score = []\n    # preds = np.zeros(len(test))\n    #        yp += model.predict_proba(X_test)[:, 1] / k\n    # Using Stratified K-fold CV for preserving the percentage of samples for each classes\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n    for fold, (tr_idx, ts_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        x_ts, y_ts = X.iloc[ts_idx], y.iloc[ts_idx]\n\n        clf = model(**params)\n        clf.fit(x_tr, y_tr,\n                eval_set=[(x_ts, y_ts)],\n                early_stopping_rounds=100,\n                verbose=False)\n\n        score = roc_auc_score(y_ts, clf.predict_proba(x_ts)[:, 1])\n        roc_score.append(score)\n        print(f\"ROC AUC Score: {score}\")\n        print(\"-\"*25)\n    \n    return clf, np.mean(roc_score)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import optuna\nimport sklearn\n# 1. Define an objective function to be maximized.\n# def objective(trial):\n    \n#     train_data = lgb.Dataset(x_train, label=y_train, categorical_feature=[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18], free_raw_data=False)\n#     valid_data = lgb.Dataset(x_valid, label=y_valid, categorical_feature=[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18], free_raw_data=False)\n\n#     # 2. Suggest values of the hyperparameters using a trial object.\n#     param = {\n#         'objective': 'binary',\n#         'metric': 'binary_logloss',\n#         #'boosting_type': 'dart',\n#         \"metric\":\"auc\",\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n#         'n_estimators': trial.suggest_int('n_estimators', 20, 1000),\n#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#         'feature_pre_filter': False\n#     }\n\n#     gbm = lgb.train(param, train_data)\n#     preds = gbm.predict(x_valid)\n#     pred_labels = np.rint(preds)\n#     accuracy = sklearn.metrics.accuracy_score(y_valid, pred_labels)\n#     return accuracy\n\n# # 3. Create a study object and optimize the objective function.\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=100)\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trial with gbdt: 0.84461\n### Trial With dart: 0.84451"},{"metadata":{"trusted":true},"cell_type":"code","source":"obj_feat_train = list(train.loc[:, train.dtypes == 'object'].columns.values)\nobj_feat_test = list(test.loc[:, test.dtypes == 'object'].columns.values)\nfor feature in obj_feat_train:\n    train[feature] = train[feature].astype('category')\nfor feature in obj_feat_test:\n    test[feature] = test[feature].astype('category')\n    \nx = train.drop(columns=['id', 'target'])\ny = train['target']\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom  lightgbm import LGBMClassifier\ntrain_data = lgb.Dataset(x_train, label=y_train,categorical_feature=[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18])\nvalid_data = lgb.Dataset(x_valid, label=y_valid,categorical_feature=[0,1,2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18])\n\n#Specifying the parameter\n# Parameters get from optune Optimization I comment the code bcz it take time to run.\nparams = { 'objective': 'binary',\n        'metric': 'binary_logloss',\n        'lambda_l1': 4.778750845902905,\n    'lambda_l2': 2.6003948800594086e-07,\n    'num_leaves': 41,\n    'n_estimators': 519,\n    \"metric\":\"auc\",\n    'feature_fraction': 0.45127412515749443,\n    'bagging_fraction': 0.9793967272010233,\n    'bagging_freq': 4,\n    'min_child_samples': 94}\n#train the model \n#model_optune = lgb.train(params,train_data, valid_sets=valid_data)\nclf, score = K_fold_CV(x,y, LGBMClassifier, params, 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test\nids = submission['id'].values\nsubmission.drop('id', inplace=True, axis=1)\n\nx = submission\ny = clf.predict_proba(submission[x.columns])[:, 1]#clf.predict(x)\n\noutput = pd.DataFrame({'id': ids, 'target': y})\noutput.to_csv(\"submission.csv\", index=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}