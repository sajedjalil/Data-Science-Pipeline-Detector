{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing modules","metadata":{}},{"cell_type":"code","source":"# to load and manipulate data\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\n\n# to visualize the data\nimport matplotlib.pyplot as plt\n\n# to preprocess the data\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# to fit the neural network\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"# getting the train data\ntrain_data = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')\ntrain_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting the test data\ntest_data = pd.read_csv('../input/tabular-playground-series-mar-2021/test.csv')\ntest_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"code","source":"# merging train and test data\ndf = pd.concat([train_data, test_data])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extracting the first letter of all two letter objects\ndf['cat5_first'] = df.cat5.str.extract(pat=r'(^[A-Z])', expand=False)\ndf['cat7_first'] = df.cat7.str.extract(pat=r'(^[A-Z])', expand=False)\ndf['cat8_first'] = df.cat8.str.extract(pat=r'(^[A-Z])', expand=False)\ndf['cat10_first'] = df.cat10.str.extract(pat=r'(^[A-Z])', expand=False)\n\n# extracting the last letter of all two letter objects - adding a flag to encode \ndf['cat5_second'] = df.cat5.str.extract(pat=r'(?<=[A-Z])([A-Z]$)', expand=False).fillna('NS')\ndf['cat7_second'] = df.cat7.str.extract(pat=r'(?<=[A-Z])([A-Z]$)', expand=False).fillna('NS')\ndf['cat8_second'] = df.cat8.str.extract(pat=r'(?<=[A-Z])([A-Z]$)', expand=False).fillna('NS')\ndf['cat10_second'] = df.cat10.str.extract(pat=r'(?<=[A-Z])([A-Z]$)', expand=False).fillna('NS')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding categorical data","metadata":{}},{"cell_type":"code","source":"# getting a list of categorical columns\ncat_columns = [column for column in df.columns if 'cat' in column]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating the frequency of occurrence of each level\nfrequencies_cat10 = df['cat10'].value_counts(normalize=False)\n\n# mapping each frequency to do the level of cat10 and creating a mask to filter next\nmasking = df['cat10'].map(frequencies_cat10)\n\n# replacing rare levels by 'Other'\ndf['cat10'] = df['cat10'].mask(masking < 500, 'Other')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating dictionary to store the label encoder\ndict_le = defaultdict(LabelEncoder)\n\n# label encoding categorical columns\ndf[cat_columns] = df[cat_columns].apply(lambda x: dict_le[x.name].fit_transform(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing columns to drop or retain\n## columns that were splitted\ncat_drop_split = ['cat5', 'cat7', 'cat8', 'cat10', 'id']\n\n## categorical columns that have a poor support from Inforation Value\ncat_drop_support = ['cat13', 'cat9', 'cat6', 'cat10_second', 'cat8_second', 'cat10_first', 'cat5', 'cat5_second', 'cat12', 'cat3', 'cat5_first']\n\n## numerical columns that have poor suport from Inforation Value\ncont_drop_support = ['cont8', 'cont3', 'cont9', 'cont4', 'cont2', 'cont10', 'cont7', 'cont0']\n\n## engineered columns\ncat_drop_eng = ['cat5_first', 'cat7_first', 'cat8_first', 'cat10_first', 'cat5_second', 'cat7_second', 'cat8_second', 'cat10_second', 'id']\n\n## columns with more support from Information Value\ncolumns_support = ['cat16', 'cat15', 'cat18', 'cat10', 'cat1', 'cat8', 'cat0', 'cat14', 'cat2', 'cat7', 'cat11', \n                   'cat17', 'cat4', 'cat8_first', 'cont5', 'cat6', 'cat7_second', 'cont1', 'cat7_first', 'target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# selecting columns to try\ndf = df.drop(columns=cat_drop_eng)\n\n# uptdating the list of numerical and categorical columns\ncat_columns = [column for column in df.columns if 'cat' in column]\nnum_columns = [column for column in df.columns if 'cont' in column]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting training and test data once again\ntrain_df, test_df = df[:train_data.shape[0]], df[train_data.shape[0]:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Separating inputs from targets","metadata":{}},{"cell_type":"code","source":"# applying the split for the training data\nX, y = train_df[cat_columns + num_columns], train_df.target\n\n# encoding the target values\ny = LabelEncoder().fit_transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying the split for the test data\nX_test = test_df[cat_columns + num_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiating the StratifiedKFold","metadata":{}},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Function to separate categorical from numerical features","metadata":{}},{"cell_type":"code","source":"def separate_inputs(X_input, categoricals, numerics):\n    # separating numerical from categorical columns\n    X_cat, X_num = X_input[categoricals], X_input[numerics]\n    \n    # parsing the numerical inputs to an array\n    X_num = np.array(X_num)\n\n    # parsing the categorical inputs to an array\n    ## creating an empty list to store the data\n    X_cat_enc = list()\n    \n    ## looping through columns to extract the data\n    for column in range(X_cat.shape[1]):\n        X_cat_enc.append(X_cat.iloc[:, column].values)\n        \n    return X_num, X_cat_enc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## applying the function to the test set\nX_test_num, X_test_cat_enc = separate_inputs(X_input = X_test, categoricals = cat_columns, numerics = num_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the model architecture","metadata":{}},{"cell_type":"code","source":"# wrapping the architecture and compilation into a function\ndef get_model(X_numeric, X_categorical):\n    # creating the input layers for the numerical values\n    input_layer_numerical = Input(shape = X_numeric.shape[1])\n\n    # creating a dense layer to encode the numerical features\n    dense_numerical = Dense(units = 64, activation = 'relu')(input_layer_numerical)\n\n    # creating the input layers for the categories that will go through the embedding\n    ## creating empty lists to store each of the input and embedding layers\n    input_layer_categorical = list()\n    embedding_layers = list()\n\n    ## looping through each of categorical columns and creating their input and embedding layers\n    for column in range(len(X_categorical)):\n        # defining the size of the input that will be used - each label in a column will have its own embedding\n        n_labels = len(np.unique(X_categorical[column]))\n        # defining the input layer of the column\n        input_layer = Input(shape = (1, ))\n        # defining the embedding layer of the columns\n        if n_labels == 2:\n            embedding_layer = Embedding(input_dim = n_labels + 1, output_dim = n_labels)(input_layer)\n        else:\n            embedding_layer = Embedding(input_dim = n_labels + 1, output_dim = 20)(input_layer)\n        # storing the input-embedding layer pairs\n        input_layer_categorical.append(input_layer)\n        embedding_layers.append(embedding_layer)\n    \n    ## concatenating the embedding layer\n    embedding = concatenate(embedding_layers)\n    \n    ## flattening the embedding layer\n    embedding_flat = Flatten()(embedding)\n    \n    ## creating a dense representation of the embedding\n    dense_embedding = Dense(units = 128, activation = 'relu')(embedding_flat)\n    \n    # combining the embedding and the numerical inputs\n    combined_inputs = Concatenate()([dense_numerical, dense_embedding])\n    \n    # batch normalizing\n    bn_0 = BatchNormalization()(combined_inputs)\n    \n    # dropout layer\n    dropout_1 = Dropout(rate = 0.5)(bn_0)\n    \n    # creating the first dense layer\n    dense_1 = Dense(units = 256)(dropout_1)\n    \n    # batch normalizing\n    bn_1 = BatchNormalization()(dense_1)\n    \n    # relu on bn\n    relu_1 = ReLU()(bn_1)\n    \n    # dropout layer\n    dropout_2 = Dropout(rate = 0.5)(relu_1)\n    \n    # creating the second dense layer\n    dense_2 = Dense(units = 512, activation = 'relu')(dropout_2)\n    # dropout layer\n    dropout_3 = Dropout(rate = 0.5)(dense_2)\n    \n    # creating the output layer\n    output_layer = Dense(units = 1, activation = 'sigmoid')(dropout_3)\n    \n    # instantiating the model\n    embedding_model = Model(inputs = [input_layer_numerical, input_layer_categorical], outputs = output_layer)\n    \n    # compiling the model\n    embedding_model.compile(optimizer = Adam(learning_rate = 0.0006), loss = 'binary_crossentropy', metrics = [AUC()])\n    \n    # returning the model\n    return embedding_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting the model","metadata":{}},{"cell_type":"code","source":"# creating a numpy array to store the predictions of each fold\npredictions = np.zeros(shape=(test_df.shape[0],1))\n\n# starting a fold counter\nfold = 1\n\n# looping over each fold and fitting the model to a different subset of the data\nfor train_index, test_index in skf.split(X, y):\n    \n    print(f'\\nStarting fold {fold}.\\n')\n    \n    # filter the indexes for training and test data\n    X_train, X_val, y_train, y_val = X.loc[train_index], X.loc[test_index], y[train_index], y[test_index]\n    \n    # separate the categorical and numerical inputs for each dataset\n    X_train_num, X_train_cat = separate_inputs(X_input = X_train, categoricals = cat_columns, numerics = num_columns)\n    X_val_num, X_val_cat = separate_inputs(X_input = X_val, categoricals = cat_columns, numerics = num_columns)\n    \n    # instantiating the model\n    embedding_model = get_model(X_train_num, X_train_cat)\n    \n    # defining the callbacks\n    early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10, min_delta = 0.0001, mode = 'min', restore_best_weights = True)\n    \n    # fitting the model\n    embedding_model.fit(x = [X_train_num, X_train_cat], y = y_train, batch_size = 512, epochs = 100, callbacks = [early_stopping], \n                        validation_data = ([X_val_num, X_val_cat], y_val))\n    \n    # getting the predictions for the model trained on that fold\n    predicted_probas = embedding_model.predict(x = [X_test_num, X_test_cat_enc], verbose = 1, batch_size = 128)\n    \n    # summing up the predictions made by the model\n    predictions = predictions + predicted_probas\n    \n    # incrementing the fold counter\n    fold += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarizing predictions","metadata":{}},{"cell_type":"code","source":"# putting the predictions in the target column\ntest_data['target'] = predictions / 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the submission data frame\nsubmission = test_data.loc[:, ['id', 'target']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving predictions","metadata":{}},{"cell_type":"code","source":"# Saving the submission\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}