{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This Notebook use the approach defined in https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers\n\nIt builds a set of specialized layer to transform categorical feature and to normalize numerical features\nand concatenates all to create the input to a fully connected network\n\nThe Notebook shows the following **techniques:**\n* how to put all the preprocessing inside the network\n* how to use TF dataset\n* how to use K-fold Cross validation to improve accuracy in validation\n* saving best model (lower val_loss) for each fold\n\nthe Notebook has achieved **AUC = 0.871**\nThe Notebook is intended to explore and show the technique.\nIt is not a record (and not my best result in this competition. Better results with NN requires extensive hyper-parameter optimizations and I think it easier to get a better score\nwith GBM."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport random as rn\nimport tensorflow as tf\n\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n\nimport os\n\nimport logging\n# added to remove TF warnings !\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'GPU'\n\nif DEVICE == \"GPU\":\n    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n    print(\"Num GPUs Available: \", n_gpu)\n    \n    if n_gpu > 1:\n        print(\"Using strategy for multiple GPU\")\n        strategy = tf.distribute.MirroredStrategy()\n    else:\n        print('Standard strategy for GPU...')\n        strategy = tf.distribute.get_strategy()\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def enable_reproducibility(seed):\n    SEED = seed\n    os.environ['PYTHONHASHSEED'] = '0'\n    # The below is needed for starting Numpy generated random numbers\n    # in a well-defined initial state.\n    np.random.seed(SEED)\n    # The below is necessary for starting core Python generated random numbers\n    # in a well-defined state.\n    rn.seed(SEED)\n    tf.random.set_seed(SEED)\n    \nenable_reproducibility(1234)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DIR = '/kaggle/input/tabular-playground-series-mar-2021'\n\nFILE_TRAIN = BASE_DIR + '/train.csv'\nFILE_TEST = BASE_DIR + '/test.csv'\nFILE_SAMPLE = BASE_DIR + '/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# globals\nFOLDS = 5\n\nBATCH_SIZE = 128\nEPOCHS = 15\nMAX_TOKENS = 100\n\nPREDICTOR = 'target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig_data = pd.read_csv(FILE_TRAIN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"orig_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare TF dataset\ndef df_to_dataset(df, predictor,  shuffle=True, batch_size=32):\n    df = df.copy()\n    labels = df.pop(predictor)\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    \n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(df))\n    \n    ds = ds.batch(batch_size)\n    # ds = ds.prefetch(batch_size)\n    return ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will be used for all numerical (cont) columns\ndef get_normalization_layer(name, dataset):\n  # Create a Normalization layer for our feature.\n  normalizer = preprocessing.Normalization()\n\n  # Prepare a Dataset that only yields our feature.\n  feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the statistics of the data.\n  normalizer.adapt(feature_ds)\n\n  return normalizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's prepare a first version of train_ds for creating the model (neededto define mean, std for normalization)\n# half the total data is OK\nFRAC = 0.5\n\nN_TRAIN = int(orig_data.shape[0] * FRAC)\ndf_train = orig_data[:N_TRAIN]\nds_train = df_to_dataset(df_train, PREDICTOR,  shuffle=True, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's prepare continuous features\nnum_col_list = [ 'cont0', 'cont1', 'cont2', 'cont3', 'cont4',\n       'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\n\n# for each of the features I want in input I have to update those two lists:\nall_inputs = []\nencoded_features = []\n\n# Numeric features.\nfor header in num_col_list:\n    print('preparing', header)\n    numeric_col = tf.keras.Input(shape=(1,), name=header)\n    normalization_layer = get_normalization_layer(header, ds_train)\n    encoded_numeric_col = normalization_layer(numeric_col)\n    all_inputs.append(numeric_col)\n    encoded_features.append(encoded_numeric_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will be used for all categorical (cont) columns, that will be one-hot encoded\ndef get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n  # Create a StringLookup layer which will turn strings into integer indices\n  if dtype == 'string':\n    index = preprocessing.StringLookup(max_tokens=max_tokens)\n  else:\n    index = preprocessing.IntegerLookup(max_values=max_tokens)\n\n  # Prepare a Dataset that only yields our feature\n  feature_ds = dataset.map(lambda x, y: x[name])\n\n  # Learn the set of possible values and assign them a fixed integer index.\n  index.adapt(feature_ds)\n\n  # Create a Discretization for our integer indices.\n  encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n\n  # Prepare a Dataset that only yields our feature.\n  feature_ds = feature_ds.map(index)\n\n  # Learn the space of possible indices.\n  encoder.adapt(feature_ds)\n\n  # Apply one-hot encoding to our indices. The lambda function captures the\n  # layer so we can use them, or include them in the functional model later.\n  return lambda feature: encoder(index(feature))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's add some categorical features (I'll start with low dimensional, with < 5 distinct values)\ncat_col_list = ['cat0', 'cat11', 'cat12','cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\n\nfor header in cat_col_list:\n    print('preparing', header)\n    categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n    encoding_layer = get_category_encoding_layer(header, ds_train, dtype='string',\n                                               max_tokens=5)\n    encoded_categorical_col = encoding_layer(categorical_col)\n    all_inputs.append(categorical_col)\n    encoded_features.append(encoded_categorical_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function builds the model\n# rather simple multi-level NN, with 3 layers\ndef build_model(n_units):\n    # use functional API\n    # concatenate all input columns\n    all_features = tf.keras.layers.concatenate(encoded_features)\n    # the 'traditional' NN\n    # x = tf.keras.layers.Dropout(0.1)(all_features)\n    x = tf.keras.layers.Dense(n_units, activation=\"relu\")(all_features)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(n_units, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(n_units, activation=\"relu\")(x)\n    # x = tf.keras.layers.Dropout(0.1)(x)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(all_inputs, output)\n    \n    model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[\"AUC\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with this we get a nice picture of the NN, the layers on the left are the 'preprocessing layers'\n# rankdir='LR' is used to make the graph horizontal.\ntf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we do the training\n# K-fold CV, we save for each fold the best epoch\n# adding K-fold CV\nskf = KFold(n_splits = FOLDS, shuffle = True, random_state=42)\n\n# for others investigations\n# we store all the history\nhistories = []\n\navg_auc = 0.\n\n# these will be split in folds\nfor fold,(idxT,idxV) in enumerate(skf.split(orig_data)):\n    n_fold = fold + 1\n    print()\n    print('***** Fold n.', n_fold)\n    \n    df_train = orig_data.iloc[idxT]\n    df_valid = orig_data.iloc[idxV]\n    \n    # create tf dataset\n    ds_train = df_to_dataset(df_train, PREDICTOR,  shuffle=True, batch_size=BATCH_SIZE)\n    ds_valid = df_to_dataset(df_valid, PREDICTOR,  shuffle=False, batch_size=BATCH_SIZE)\n    \n    # clear\n    clear_session()\n    \n    with strategy.scope():\n        model = build_model(32)\n    \n    # don't use h5 format (to avoid a problem)\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i'%n_fold, monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='min', save_freq='epoch')\n    \n    history = model.fit(ds_train, validation_data=ds_valid,\n                        epochs=EPOCHS, verbose=1, callbacks = [sv])\n    \n    # save all histories\n    histories.append(history)\n    \n    # reload the best model\n    model.load_weights('fold-%i'%n_fold)\n    \n    results = model.evaluate(ds_valid)\n    \n    avg_auc += results[1]\n    \n# compute avg AUC across folds\navg_auc = avg_auc/float(FOLDS)\n\nprint()\nprint('Average AUC across folds is', round(avg_auc, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss(hist, skip):\n    plt.figure(figsize=(14,6))\n    \n    plt.plot(hist.history['loss'][skip:], label='Training loss')\n    plt.plot(hist.history['val_loss'][skip:], label='Validation loss')\n    plt.title('Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.grid(True)\n    plt.show();\n\ndef plot_auc(hist, skip):\n    plt.figure(figsize=(14,6))\n    \n    plt.plot(hist.history['auc'][skip:], label='Training AUC')\n    plt.plot(hist.history['val_auc'][skip:], label='Validation AUC')\n    plt.title('AUC')\n    plt.legend(loc='upper right')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.grid(True)\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in range(FOLDS):\n    plot_loss(histories[fold], skip=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in range(FOLDS):\n    plot_auc(histories[fold], skip=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare the evaluation on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"orig_test = pd.read_csv(FILE_TEST)\n\norig_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# higher batch size\n# this one doesn't reference targe (obviously)\ndef df_test_to_dataset(df, batch_size=32):\n    df = df.copy()\n    ds = tf.data.Dataset.from_tensor_slices(dict(df))\n    ds = ds.batch(batch_size)\n    return ds\n\nds_test = df_test_to_dataset(orig_test, batch_size=512)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare for average\navg_preds = np.zeros(orig_test.shape[0])\n\nfor fold in range(FOLDS):\n    n_fold = fold + 1\n    \n    print('Predictions with fold n.', n_fold)\n    \n    # load best model for fold\n    model = build_model(32)\n    model.load_weights('fold-%i'%n_fold)\n    \n    # get the probability for predictions\n    preds = model.predict(ds_test)\n    \n    # make it one dimensional\n    preds =  preds.reshape(preds.shape[0])\n    \n    avg_preds += preds\n    \navg_preds = avg_preds/float(FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission file\ndict_sub = {\"id\": orig_test['id'],\n           \"target\": avg_preds}\n\nSUB_NAME = 'submission00.csv'\n\ndf_submission = pd.DataFrame(dict_sub)\n\n# df_submission.head()\n\ndf_submission.to_csv(SUB_NAME, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}