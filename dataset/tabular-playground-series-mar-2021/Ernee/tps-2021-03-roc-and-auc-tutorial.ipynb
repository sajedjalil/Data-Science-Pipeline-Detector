{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is the \"Area Under the Curve\" (AUC)?\n\nIt took me a while to grasp what the **ROC curve** means when I first read the [Wikipedia article](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) referenced in the description of this competition. There are many underlying concepts that must be understood before we can have a sense of why the **area under the curve** (AUC) can be used as a metric for binary classification. This tutorial is my two cents on trying to clearify some of these concepts.\n\nLet's start by loading libraries and data, encoding categorical variables and creating train and validation sets.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom lightgbm import LGBMClassifier\n\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/train.csv', index_col='id')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-mar-2021/test.csv', index_col='id')\ntarget = train.pop('target')\n\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, test_size=0.1, random_state=0)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base model and ROC curve\n\nWe will use the LGBM base model as a first example. Let's start by fitting the model and making predictions.","metadata":{}},{"cell_type":"code","source":"model = LGBMClassifier(random_state=0, metric='auc')\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_valid)[:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To compute the Area Under the Curve (AUC), we can use the built-in method `roc_auc_score`.","metadata":{}},{"cell_type":"code","source":"auc = metrics.roc_auc_score(y_valid, y_pred)\nprint('AUC =', f'{auc:0.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To plot the curve and the area under it, we need to compute two things: the **False Positive Rate** (`fpr`) and the **True Positive Rate** (`tpr`). I will explain what those mean shortly, but for now let's just run the follwing code and observe the results.","metadata":{}},{"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve(y_valid, y_pred)\n\nplt.figure(figsize=(4, 4), dpi=100)\nplt.axis('scaled')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.title('ROC curve')\nplt.plot(fpr, tpr, 'b')\nplt.fill_between(fpr, tpr, facecolor='lightblue', alpha=0.5)\nplt.text(0.95, 0.05, 'AUC = %0.4f' % auc, ha='right', fontsize=12, weight='bold', color='red')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how the graph is contained inside a unit square, and so the AUC can be at most 1. This ideal situation (i.e., AUC = 1) only happens if we predict every single output correctly and with total certainty.","metadata":{}},{"cell_type":"markdown","source":"# True Positives, False Positives, True Negatives and False Negatives\n\nOK, but how do we compute `tpr` and `fpr` to build the graph? In order to explain that, I will use a much smaller example, with only 10 values (`actual_values`). In what follows we will assume that **1 means \"positive\"** and **0 means \"negative\"**. As the code below shows, the list has 4 positives and 6 negatives.","metadata":{}},{"cell_type":"code","source":"actual_values = [0, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n\nn_positives = sum(actual_values)\nn_negatives = len(actual_values) - n_positives\n\nprint('Total positives =', n_positives)\nprint('Total negatives =', n_negatives)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Suppose that we have the follwing predicted probabilities (`predicted_probs`) for these values being equal to 1 (\"positive\").","metadata":{}},{"cell_type":"code","source":"predicted_probs = [0.40, 0.95, 0.18, 0.59, 0.81, 0.61, 0.19, 0.20, 0.24, 0.24]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These probabilities are not our final predictions. They only serve as information to help us decide whether to predict 0 or 1 for each outcome. And since they are all numbers **between 0 and 1**, we need to set a `threshold` and establish that:\n\n* if the predicted probability is above the threshold, predict 1 (positive)\n* otherwise, predict 0 (negative)\n\nThe threshold can be any number in [0, 1]. In fact, the ROC curve is built considering **all numbers** in this interval. But lt's go one step at a time and choose a single number as our threshold. A natural choice is 0.5.","metadata":{}},{"cell_type":"code","source":"threshold = 0.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the value of the threshold and on the predicted probabilities, the code below will make binary predictions for the 10 outcomes and store them in `predicted_values`.","metadata":{}},{"cell_type":"code","source":"def compute_predicted_values(predicted_probs, threshold):\n    predicted_values = []\n    for i in range(len(predicted_probs)):\n        if predicted_probs[i] > threshold:\n            predicted_values.append(1)\n        else:\n            predicted_values.append(0)\n    return predicted_values\n\npredicted_values = compute_predicted_values(predicted_probs, threshold)\n\nprint('actual_values   :', actual_values)\nprint('predicted_values:', predicted_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you compare the two lists shown above, you will realize that there are four types of outcomes:\n\n1. The actual value is 1 and we predicted 1: this is called a **True Positive** (TP) ‚úîÔ∏è\n2. The actual value is 0 and we predicted 0: this is called a **True Negative** (TN) ‚úîÔ∏è\n3. The actual value is 0 and we predicted 1: this is called a **False Positive** (FP) ‚ùå\n4. The actual value is 1 and we predicted 0: this is called a **False Negative** (FN) ‚ùå\n\nThe code below classifies each prediction in one of these four cases.","metadata":{}},{"cell_type":"code","source":"def compute_outcomes(actual_values, predicted_values):\n    outcomes = []\n    for i in range(len(actual_values)):\n        if actual_values[i] == 1:\n            if predicted_values[i] == 1:\n                outcomes.append('TP')\n            else:\n                outcomes.append('FN')\n        else:\n            if predicted_values[i] == 1:\n                outcomes.append('FP')\n            else:\n                outcomes.append('TN')\n    return outcomes\n    \noutcomes = compute_outcomes(actual_values, predicted_values)\n                \nprint('outcomes:', outcomes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# True Positive Rate and False Positive Rate\n\nOf course, we want to have as many TP and TN as possible (correct predictions) and avoid FP and FN (wrong predictions). In this example and **for a threshold of 0.5**, there were 2 TP, 4 TN, 2 FP and 2 FN.\n\nNow, to draw the ROC curve, we need the **True Positive Rate** (`tpr`) and the **False Positive Rate** (`fpr`). These are given by:\n\n* **True Positive Rate = True Positives / Total Positives**\n* **False Positive Rate = False Positives / Total Negatives**\n\nSo let's compute those values.","metadata":{}},{"cell_type":"code","source":"tpr = outcomes.count('TP') / n_positives\nfpr = outcomes.count('FP') / n_negatives\n\nprint('True Positive Rate = %0.2f' % tpr)\nprint('False Positive Rrate = %0.2f' % fpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the ROC curve and computing AUC\n\nNow let's plot the single point we just obtained, with `fpr` as the x-coordinate and `tpr` as the y-coordinate.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4, 4), dpi=100)\nplt.axis('scaled')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.title('ROC curve')\nplt.plot(fpr, tpr, 'bo')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, we've calculated one point of the ROC curve. But how can se get the entire graph? As mentioned before, the curve is obtained by considering all values in the interval [0, 1] as possible thresholds. Let's first consider the two extreme points:\n\n* By setting `threshold = 0`, all our predictions would be equal to 1 (unless we have predictions probabilities equal to 0, which is rare). If everything is predicted as positives, then there can only be TP and FP (and no TN or FN). Therefore we must alse have `fpr = 1` and `tpr = 1`, which corresponds to the upper right corner of the graph, i.e. the point with coordinates (1, 1).\n* By setting `threshold = 1`, nothing will be predicted as positive, so TP = FP = 0 and consequently `fpr = 0` and `tpr = 0`, which corresponds to the lower left corner of the graph, i.e. the point with coordinates (0, 0).\n\nFor every value of the threshold between 0 and 1, we will obtain a new point of the curve. If we start from 1 and continuously decrease the value of the threshold down to 0, the number of TP and FP will either increase or remain unchanged. Therefore, the **ROC curve goes from (0, 0) to (1, 1) continuously and is monotonically increasing**.\n\nNotice, however, that if we vary the threshold from 1 to 0, the values of `fpr` and `tpr` will only change when the threshold \"crosses\" one of the values in the list of predicted probabilities (`predicted_probs`). Hence we only need to use these values (plus 0 and 1) as thresholds to build the curve. Let's do just that.","metadata":{}},{"cell_type":"code","source":"thresholds = [1] + sorted(predicted_probs, reverse=True) + [0]\n\ndef compute_tpr_fpr(actual_values, predicted_probs, thresholds, n_positives, n_negatives):\n    tpr = []\n    fpr = []\n    for threshold in thresholds:\n        predicted_values = compute_predicted_values(predicted_probs, threshold)\n        outcomes = compute_outcomes(actual_values, predicted_values)\n        tpr.append(outcomes.count('TP') / n_positives)\n        fpr.append(outcomes.count('FP') / n_negatives)\n    return tpr, fpr\n\ntpr, fpr = compute_tpr_fpr(actual_values, predicted_probs, thresholds, n_positives, n_negatives)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code above is not very efficient, but I will stick to it for presentation purposes. Now let's plot the curve.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(4, 4), dpi=100)\nplt.axis('scaled')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.title('ROC curve')\nplt.plot(fpr, tpr, 'bo-')\nplt.fill_between(fpr, tpr, facecolor='lightblue', alpha=0.5)\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's compute the AUC. Because the curve is piecewise linear, we can calculate this area by dividing it into trapezoids. Thankfully, there is a numpy function that does ir for us.","metadata":{}},{"cell_type":"code","source":"auc = np.trapz(tpr, fpr)\nprint('AUC =', f'{auc:0.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This notebook presented some of the concepts necessary to understand the ROC curve and the AUC. Of course, there is more to be explored regarding the subject, but I prefer to keep it short and simple.\n\nThanks for reaching the end of it! üòä","metadata":{}}]}