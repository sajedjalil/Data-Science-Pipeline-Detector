{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)          \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-09T10:13:10.31916Z","iopub.execute_input":"2021-07-09T10:13:10.319537Z","iopub.status.idle":"2021-07-09T10:13:10.327352Z","shell.execute_reply.started":"2021-07-09T10:13:10.319503Z","shell.execute_reply":"2021-07-09T10:13:10.326366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Standard Neural Networks with Static Semantic Embeddings Baseline\n\n\n<img src=\"https://miro.medium.com/max/688/1*zR61FG9RUd6ul4ecXA_euQ.jpeg\">\n\n\nIn this context, we will be building a preliminary deep model using sophisticated neural networks and variants of RNNs. We will be building a simple LSTM model for validating the influence of deep models with respect to the statistical ones. In the first case, we will be using the Keras Embedding layer and visualize the results before using the embedding models.\n\n[Keras LSTM](https://keras.io/api/layers/recurrent_layers/lstm/)\n[Keras](https://keras.io/)\n[Keras Starter Guides](https://keras.io/examples/nlp/)\n[Tensorflow Starter](https://www.tensorflow.org/tutorials/keras/text_classification)\n[Tensorflow Hub](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub)\n[Jason's Blog-Best practises](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)\n[Jason's Blog-Convolution Networks](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)\n\n\nMore resources will be provided, and for now we will be focussing on creating specific  RNN (Recurrent Neural Variants) with/without Static Semantic Embeddings to create a Neural Model Baseline. \n\n\n<img src=\"https://miro.medium.com/max/875/1*n-IgHZM5baBUjq0T7RYDBw.gif\">\n\n\n### Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n- Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n- Ease of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\n\nA classic RNN appears as follows:\n\n<img src=\"https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\nThis [video](https://youtu.be/8HyCNIVRbSU) provides a good description of how RNNs work.\n\n\nParticulary a RNN works on the logic:\n\n\n<img src=\"https://miro.medium.com/max/875/1*3mDe6V5DRXqpHYKDfxN4Rg.png\">\n\n\nThere are various kinds of such networks:\n\n\n- Encoding Recurrent Neural Networks are just folds. They‚Äôre often used to allow a neural network to take a variable length list as input, for example taking a sentence as input.\n\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-encoding.png\">\n\n\n- Generating Recurrent Neural Networks are just unfolds. They‚Äôre often used to allow a neural network to produce a list of outputs, such as words in a sentence.\n\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-generating.png\">\n\n\n- General Recurrent Neural Networks are accumulating maps. They‚Äôre often used when we‚Äôre trying to make predictions in a sequence. For example, in voice recognition, we might wish to predict a phenome for every time step in an audio segment, based on past context.\n\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-general.png\">\n\n\n- Bidirectional Recursive Neural Networks are a more obscure variant, which I mention primarily for flavor. In functional programming terms, they are a left and a right accumulating map zipped together. They‚Äôre used to make predictions over a sequence with both past and future context.\n\n<img src=\"https://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-bidirectional.png\">\n\n \n \nSome resources for understanding the derivatives and optimization inside the RNNs:\n\n- [Maths](https://www.cs.toronto.edu/~tingwuwang/rnn_tutorial.pdf)\n- [Blog](https://colah.github.io/posts/2015-09-NN-Types-FP/)\n- [Blog](https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)\n- [Kernel](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india#Neural-Networks)\n\n\nThese are some starter resources for creating preliminary networks for sentiment analysis, text/intent classifications. There will be some advanced architectures which will be focussed later.\n\n\n### Long Short Term Memory (LSTM)\n\n[Drawbacks of RNNS](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they‚Äôd be extremely useful. But can they? It depends.Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in ‚Äúthe clouds are in the sky,‚Äù we don‚Äôt need any further context ‚Äì it‚Äôs pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it‚Äôs needed is small, RNNs can learn to use the past information.But there are also cases where we need more context. Consider trying to predict the last word in the text ‚ÄúI grew up in France‚Ä¶ I speak fluent French.‚Äù Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It‚Äôs entirely possible for the gap between the relevant information and the point where it is needed to become very large.\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information.In theory, RNNs are absolutely capable of handling such ‚Äúlong-term dependencies.‚Äù A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don‚Äôt seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult.\nThankfully, LSTMs don‚Äôt have this problem!\n\n- LSTMs:\n \n <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n \n The first step in our LSTM is to decide what information we‚Äôre going to throw away from the cell state. This decision is made by a sigmoid layer called the ‚Äúforget gate layer.‚Äù It looks at ```ht‚àí1``` and ```xt```, and outputs a number between 0 and 1 for each number in the cell state ```Ct‚àí1```. A 1 represents ‚Äúcompletely keep this‚Äù while a 0 represents ‚Äúcompletely get rid of this.‚Äù\n\nLet‚Äôs go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\">\n\n\nThe next step is to decide what new information we‚Äôre going to store in the cell state. This has two parts. First, a sigmoid layer called the ‚Äúinput gate layer‚Äù decides which values we‚Äôll update. Next, a tanh layer creates a vector of new candidate values, ```C~t```, that could be added to the state. In the next step, we‚Äôll combine these two to create an update to the state.\n\nIn the example of our language model, we‚Äôd want to add the gender of the new subject to the cell state, to replace the old one we‚Äôre forgetting.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\">\n\nIt‚Äôs now time to update the old cell state, ```Ct‚àí1```, into the new cell state ```Ct```. The previous steps already decided what to do, we just need to actually do it.\n\nWe multiply the old state by ```ft```, forgetting the things we decided to forget earlier. Then we add ```it‚àóC~t```. This is the new candidate values, scaled by how much we decided to update each state value.\n\nIn the case of the language model, this is where we‚Äôd actually drop the information about the old subject‚Äôs gender and add the new information, as we decided in the previous steps.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n\nFinally, we need to decide what we‚Äôre going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we‚Äôre going to output. Then, we put the cell state through tanh (to push the values to be between ‚àí1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that‚Äôs what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that‚Äôs what follows next.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\">\n\n\nAn illustrated working of the LSTM is provided:\n\n\n<img src=\"https://miro.medium.com/max/1900/1*GjehOa513_BgpDDP6Vkw2Q.gif\">\n\n\nSome blogs:\n\n- [Blog](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21&psig=AOvVaw3GJ2-g9jyCgtlUxlTAmyJ8&ust=1608535825759000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCLjax4eF3O0CFQAAAAAdAAAAABAD)\n- [Blog](https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/)\n- [Blog](https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/)\n- [Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf)\n\nThere are several Variants of LSTMs some of the most famous being Depth GRU /Gated Recurrent Units:\n\nA slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single ‚Äúupdate gate.‚Äù It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\">\n\n\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:16.869193Z","iopub.execute_input":"2021-07-09T10:13:16.86951Z","iopub.status.idle":"2021-07-09T10:13:21.833665Z","shell.execute_reply.started":"2021-07-09T10:13:16.869482Z","shell.execute_reply":"2021-07-09T10:13:21.832825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/google-quest-challenge/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:21.83499Z","iopub.execute_input":"2021-07-09T10:13:21.835309Z","iopub.status.idle":"2021-07-09T10:13:22.138032Z","shell.execute_reply.started":"2021-07-09T10:13:21.835273Z","shell.execute_reply":"2021-07-09T10:13:22.136991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1186%2Fs12859-019-3079-8/MediaObjects/12859_2019_3079_Fig2_HTML.png\">\n\n\nA proper model overview comprising of LSTMs and Embeddings is provided here:\n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/6ac8328113639044d2beb83246b9d07f513ac6c8/3-Figure1-1.png\">\n\nSome resources:\n\n- [Kernels](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n- [Kernels](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)\n","metadata":{}},{"cell_type":"code","source":"##First Step is to test model performance without pretrained Embeddings\n## Will be using only Keras Embeddings in this case with a minimal neural network model\n\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\n#clean some null words or use the previously cleaned & lemmatized corpus\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['category'][:2000])\nlabels\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'][:2000],train_y,test_size=0.2,random_state=42)\n\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:22.139856Z","iopub.execute_input":"2021-07-09T10:13:22.140229Z","iopub.status.idle":"2021-07-09T10:13:22.591198Z","shell.execute_reply.started":"2021-07-09T10:13:22.140189Z","shell.execute_reply":"2021-07-09T10:13:22.590345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=Sequential()\nmodel.add(Embedding(max_features,embed_size,input_length=maxlen))\nmodel.add(LSTM(60))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"simple_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\nmodel.fit(train_x,train_y,batch_size=512,epochs=3,verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:22.594179Z","iopub.execute_input":"2021-07-09T10:13:22.594436Z","iopub.status.idle":"2021-07-09T10:13:33.140702Z","shell.execute_reply.started":"2021-07-09T10:13:22.594411Z","shell.execute_reply":"2021-07-09T10:13:33.139809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image \nimport matplotlib.pyplot as plt\nim = Image.open(\"simple_model.png\") \nim.show()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:33.14259Z","iopub.execute_input":"2021-07-09T10:13:33.142983Z","iopub.status.idle":"2021-07-09T10:13:33.209476Z","shell.execute_reply.started":"2021-07-09T10:13:33.142943Z","shell.execute_reply":"2021-07-09T10:13:33.208254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a Static Semantic Embedding Neural Network(LSTM) Baseline\n\nIn this case, we will be using pretrained embeddings for ouruse case. \nParticularly this lines of code:\n\n```python\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['review']\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '../input/wikinews300d1msubwordvec/wiki-news-300d-1M-subword.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[20])\nplt.show()\n```","metadata":{}},{"cell_type":"code","source":"##Build Static Embedding on top of a Neural Model\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['question_body'][3000:4000]\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"glove_simple_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\nmodel.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:33.21271Z","iopub.execute_input":"2021-07-09T10:13:33.213006Z","iopub.status.idle":"2021-07-09T10:13:52.611175Z","shell.execute_reply.started":"2021-07-09T10:13:33.212978Z","shell.execute_reply":"2021-07-09T10:13:52.610329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References for now:\n- [Kernel](https://www.kaggle.com/colearninglounge/nlp-model-building-transformers-attention-more)","metadata":{}},{"cell_type":"markdown","source":"## Model Performance\n\nWe see that the model performance based on Question and Category baseline with pretrained glove embeddings is quite low, and this is due to the fact that the dataset is not appropriately cleared. The semantics taken into consideration by taking a feature size of 5000 tokens is not sufficient to identify and categorize the questions based on the categories. This is an example where previously stated Graph Embeddings space outperform the traditional LSTMs for infering accurate information.","metadata":{}},{"cell_type":"markdown","source":"# Reference Notebook\n\nAll the materials of this notebook is taken from this [notebook](https://www.kaggle.com/abhilash1910/nlp-workshop-playing-with-transformers) as pre-requisites , and only the major ones are taken.\nIt is recommended to study the notebook (also added here) , for further enhanced concepts.\nEverything under this section is taken from the aforementioned notebook.","metadata":{}},{"cell_type":"markdown","source":"# Understanding Attention Mechanism\n\n[Attention mechanism](https://arxiv.org/abs/1706.03762) is the most important aspect in language modelling. There are many variants of attention such as Bahdanau Attention, Luong Attention, Dot Product Attention,Self Attention. A detailed description of the attention mechanism is provided in this [kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n\n\n## Bahdanau Attention\n\n\n<img src=\"https://miro.medium.com/max/639/1*qhOlQHLdtfZORIXYuoCtaA.png\">\n\n\nBahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n\nlet‚Äôs understand the Attention mechanism suggested by Bahdanau\n\nAll hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\nThe attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\nThe model predicts a target word based on the context vectors associated with the source position and the previously generated target words.\nAlignment Score\nThe alignment score maps how well the inputs around position ‚Äúj‚Äù and the output at position ‚Äúi‚Äù match. The score is based on the previous decoder‚Äôs hidden state, s‚Çç·µ¢‚Çã‚ÇÅ‚Çé just before predicting the target word and the hidden state, h‚±º of the input sentence.\n\n\n<img src=\"https://miro.medium.com/max/535/1*u2YdTRPjN34Fpr-zxvoJsg.png\">\n\n\nThe decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector. The alignment vector that has the same length with the source sequence and is computed at every time step of the decode.\n\nAttention Weights\nWe apply a softmax activation function to the alignment scores to obtain the attention weights.\n\n\n<img src=\"https://miro.medium.com/max/685/1*3aCyU9aSVHvxzOwvQdExdQ.png\">\n\n\n\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\">\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\">\n\n\n\n\n## Luong Attention\n\n\n\n\n### Global Attention\n\n\n<img src=\"https://miro.medium.com/max/626/1*LhEapXF1mtaB3rDgIjcceg.png\">\n\nLuong, et al., 2015 proposed the ‚Äúglobal‚Äù and ‚Äúlocal‚Äù attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n\nThe commonality between Global and Local attention\n\n- At each time step t, in the decoding phase, both approaches, global and local attention, first take the hidden state h‚Çú at the top layer of a stacking LSTM as an input.\n- The goal of both approaches is to derive a context vector ùí∏‚Çú to capture relevant source-side information to help predict the current target word y‚Çú\n- Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions.\n- Global and local attention models differ in how the context vector ùí∏‚Çú is derived\n- Before we discuss the global and local attention, let‚Äôs understand the conventions used by Luong‚Äôs attention mechanism for any given time t\n\n   - ùí∏‚Çú : context vector\n   - a‚Çú : alignment vector\n   - h‚Çú : current target hidden state\n   - h‚Çõ : current source hidden state\n   - y‚Çú: predicted current target word\n   - hÀú‚Çú : Attentional vectors\n   \n   \nThe global attentional model considers all the hidden states of the encoder when calculating the context vector ùí∏‚Çú.\n\nA variable-length alignment vector a‚Çú equal to the size of the number of time steps in the source sequence is derived by comparing the current target hidden state h‚Çú with each of the source hidden state h‚Çõ\nThe alignment score is referred to as a content-based function for which we consider three different alternatives\n\n\n### Local Attention\n\n\n<img src=\"https://miro.medium.com/max/538/1*YXjdGl3CnSfHfzYpQiObgg.png\">\n\n\n\n- Local attention only focuses on a small subset of source positions per target words unlike the entire source sequence as in global attention\n- Computationally less expensive than global attention\n- The local attention model first generates an aligned position P‚Çú for each target word at time t.\n- The context vector ùí∏‚Çú is derived as a weighted average over the set of source hidden states within selected the window\n- The aligned position can be monotonically or predictively selected\n\n\n### Formulation\n\n\n<img src=\"https://miro.medium.com/max/875/1*_Ta67S8_lXTbVzJMztkxKg.png\">\n\n\n\n## List of Different Attention Mechanisms\n\n\n<img src=\"https://theaisummer.com/assets/img/posts/attention/attention-calculation.png\">\n\n\n## Resources:\n\n- [Luong Paper](https://arxiv.org/abs/1508.04025)\n- [Bahdanau Paper](https://arxiv.org/abs/1409.0473)\n- [Luong 2015](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n- [Kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T11:09:48.760764Z","iopub.execute_input":"2021-07-03T11:09:48.761191Z","iopub.status.idle":"2021-07-03T11:09:48.785715Z","shell.execute_reply.started":"2021-07-03T11:09:48.761159Z","shell.execute_reply":"2021-07-03T11:09:48.784505Z"}}},{"cell_type":"code","source":"#Implementing Different Attention Layers for our keras model\nimport math\nclass Bahdanau_Attention_1D(tf.keras.layers.Layer):\n    #A class for Bahdanau Attention\n    def __init__(self,units):\n        super(Bahdanau_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q):\n        self.q=q\n        self.v=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\n\nclass Luong_Attention_1D(tf.keras.layers.Layer):\n    #A class for Luong Attention\n    def __init__(self,units):\n        super(Luong_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q):\n        self.q=q\n        self.v=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.q)*(self.v)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\nclass Scaled_Dot_Product_Attention_1D(tf.keras.layers.Layer):\n    #Scaled dot product Attention\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,n):\n        self.q=q\n        self.v=q\n        self.n=n\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=((self.q)*(self.v))/math.sqrt(self.n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:52.612836Z","iopub.execute_input":"2021-07-09T10:13:52.613096Z","iopub.status.idle":"2021-07-09T10:13:52.627427Z","shell.execute_reply.started":"2021-07-09T10:13:52.613071Z","shell.execute_reply":"2021-07-09T10:13:52.626465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nlstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nluong_attention=Luong_Attention_1D(60)\n_,attention_weights_h=luong_attention(lstm_cell)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Luong.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['category'])\nlabels\ntrain_y=labels\n# train_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n# Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n# Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=20,verbose=2,validation_data=(val_x,val_y))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:13:52.629206Z","iopub.execute_input":"2021-07-09T10:13:52.629837Z","iopub.status.idle":"2021-07-09T10:17:28.044969Z","shell.execute_reply.started":"2021-07-09T10:13:52.629799Z","shell.execute_reply":"2021-07-09T10:17:28.043986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Self Attention Mechanism\n\n\nSelf Attention is a mechanism for specifying the the attention on parts of a sentence to retain more semantic information. Self-attention is the method the Transformer uses to bake the ‚Äúunderstanding‚Äù of other relevant words into the one we‚Äôre currently processing. [Jay's Blog](http://jalammar.github.io/illustrated-transformer/) provide a very good idea of this logic.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\">\n\nThree vectors q,k and v (query,key and value) are taken into consideration for computation of the self attention mechanism.The q,k and v are normally of 64 dimensions.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_vectors.png\">\n\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we‚Äôre scoring. So if we‚Äôre processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_score.png\">\n\n\nThe third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper ‚Äì 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they‚Äôre all positive and add up to 1.\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention_softmax.png\">\n\n\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it‚Äôs useful to attend to another word that is relevant to the current word. The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example). The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-output.png\">\n\n\nThe computation process for Self Attention can be regarded as follows:\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\">","metadata":{}},{"cell_type":"code","source":"\n\nclass Scaled_Dot_Product_Self_Attention_1D(tf.keras.layers.Layer):\n    #A class for Self Attention- 1 Dimension\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,n):\n        self.q=q\n        self.v=q\n        self.n=n\n        self.k=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nBilstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(Bilstm_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Self.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:17:28.046976Z","iopub.execute_input":"2021-07-09T10:17:28.047348Z","iopub.status.idle":"2021-07-09T10:19:17.287224Z","shell.execute_reply.started":"2021-07-09T10:17:28.047307Z","shell.execute_reply":"2021-07-09T10:19:17.286371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enter Transformers - BERT\n\nWe will be working with the [HuggingFace](https://huggingface.co/) repository as it contains all SOTA Transformer models. In this context, it is useful to mention some important resources:\n\n- Transformer Keras(https://keras.io/examples/nlp/text_classification_with_transformer/)\n- Kaggle Kernel(https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb)\n\nHowever in this case, since we would be using the models just for extracting embeddings or features, it is important to know the intermediate layers which should be chosen. Since Transformer architectures are really huge, (BERT/GPT variants), it is very complicated to fully understand which layer should be extracted for the features. While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.\n\n\n\n<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">\n\n\n## BERT Embeddings\n\n[BERT](https://arxiv.org/abs/1810.04805) is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n\nMsked Language Model (MLM)\nNext Sentence Prediction(NSP)\nThe bidirectional pre-training is essentially helpful to be used for any tasks. The Huggingface implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n\n\n<img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\">\n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/df2b0e26d0599ce3e70df8a9da02e51594e0e992/15-Figure4-1.png\">\n\n\n## Extracting Embeddings from BERT variant Transformers\n\nFor finetuning, it is to be kept in mind, there are many ways to do this. We are using BERT from Huggingface repository while it can also be used from [TF-HUB](https://tfhub.dev/s?module-type=text-embedding) or from [Google-Research](https://github.com/google-research/bert) repository. The reason for using HuggingFace is that the same codebase is applicable for all language models. The 3 most important input features that any language model asks for is:\n\n- input_ids\n- attention_masks\n- token_ids\n\nLet us first try to analyse and understand how BERT tokenizers, and model can be used in this context. The BERT documentation provides an outline of how to use BERT tokenizers and also modify it for downstream tasks.\n\nGenerally by virtue of transfer learning through weight transfer, we use pretrained BERT models from the list. This allows us to finetune it to extract only the embeddings. Since we are using Keras, we have to build up a small model containing an Input Layer and apply the tokenized(encoded) input ids, attention masks as input to the pretrained and loaded BERT model.This is very similar to creating a very own classification model for BERT using Keras/Tensorflow, but since we will be needing only the Embeddings it is safe to extract only the sentence vectors in the last layer of the model output. In most of the cases , we will see that the dimensions of the output vector is (x,768) where x depends on the number of tokenized input features. For this we extract the [CLS] tokenized feature from the ouput to just extract the sentence embeddings.\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\">\n\n\nSo the following pattern is to be done for our use case:\n\n- First Attain BERT embeddings by capturing the last hidden state output\n- Create the standard Neural network model (which we created till now)\n- In the Embedding Layer fit the BERT embeddings\n- Apply Self-Attention/any Attention mechanism on BERT embeddings\n- Apply FFNN Dense Networks with required activation functions\n\nby BERT , I mean all transformer models  under huggingface [pretrained library](https://huggingface.co/transformers/pretrained_models.html)\n\n\n\nSome resources and source codes:\n\n- [My NLP Kernels](https://kaggle.com/abhilash1910)\n- [Extensive Word Embeddings with Distilbert/Roberta/XL NET](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop#BERT-Embeddings-with-Alternate-Strategy)\n- [NLP workshop-2](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n\n\n<img src=\"https://i0.wp.com/esciencegroup.com/wp-content/uploads/2020/02/01.png?resize=506%2C641&ssl=1\">","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nfrom scipy.spatial.distance import cosine\nimport tensorflow as tf\ndef get_embeddings(model_name,tokenizer,name,inp):\n    #Specify which transformer model will be used\n    tokenizer = tokenizer.from_pretrained(name)\n    model = model_name.from_pretrained(name)\n    input_ids = tf.constant(tokenizer.encode(inp))[None, :]  # Batch size 1\n    outputs = model(input_ids)\n    #Take the last output\n    last_hidden_states = outputs[0]\n    cls_token=last_hidden_states[0]\n    return cls_token\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the BERT word embeddings\ncls_token1=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',input_word1)\ncls_token2=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\n# plt.plot(cls_token1[0])\n# plt.plot(cls_token2[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:19:17.289075Z","iopub.execute_input":"2021-07-09T10:19:17.289445Z","iopub.status.idle":"2021-07-09T10:20:06.403987Z","shell.execute_reply.started":"2021-07-09T10:19:17.289407Z","shell.execute_reply":"2021-07-09T10:20:06.402865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DistilBERT Embeddings\n\n[This](https://huggingface.co/transformers/model_doc/distilbert.html) is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. DistilBERT Paper provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n\n\n<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\nThe overall workflow is similar to BERT extracting BERT word embeddings\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png\">","metadata":{}},{"cell_type":"code","source":"#DistilBERT word Embeddings\nfrom transformers import *\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the DistilBERT word embeddings\ncls_token1=get_embeddings(TFDistilBertModel,BertTokenizer,'distilbert-base-uncased',input_word1)\ncls_token2=get_embeddings(TFDistilBertModel,BertTokenizer,'distilbert-base-uncased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:20:06.405245Z","iopub.execute_input":"2021-07-09T10:20:06.40561Z","iopub.status.idle":"2021-07-09T10:20:42.646784Z","shell.execute_reply.started":"2021-07-09T10:20:06.405574Z","shell.execute_reply":"2021-07-09T10:20:42.645922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XL NET Embeddings\n\n[This paper](https://arxiv.org/abs/1906.08237) provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n\n- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n- It is a permutation language model and a pictorial representation can be :\n\n<img src=\"https://zdnet2.cbsistatic.com/hub/i/r/2019/06/21/2a4e6548-9dee-491d-b638-8cfae9bbb2fe/resize/1200x900/ab279544c2631111754a357ada50ef29/google-xlnet-architecture-2019.png\">\n\n\n\nHere we will be using an alternate strategy for building the word embeddings\nWe will be using the Feature Extraction Pipeline from Huggingface- just to show that there are more than one ways of retrieving the embeddings.","metadata":{}},{"cell_type":"code","source":"#XLNET word Embeddings\n#Using the Feature Extraction pipeline from Huggingface\n\nfrom transformers import AutoTokenizer, pipeline\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(model_name,name,inp):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the XLNET word embeddings\ncls_token1=transformer_embedding(TFXLNetModel,'xlnet-base-cased',input_word1)\ncls_token2=transformer_embedding(TFXLNetModel,'xlnet-base-cased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:20:42.649081Z","iopub.execute_input":"2021-07-09T10:20:42.649442Z","iopub.status.idle":"2021-07-09T10:21:28.584838Z","shell.execute_reply.started":"2021-07-09T10:20:42.649402Z","shell.execute_reply":"2021-07-09T10:21:28.584032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring Generative Transformers - GPT2\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png\">\n\n\nIt is a [robust model](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n\nGPT-2 is a model with absolute position embeddings so it‚Äôs usually advised to pad the inputs on the right rather than the left.\n\nGPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nGPT-2 is a decoder Transformer model. Generally most transformers are encoder-decoders, but in the case of GPT-2 this is a decoder-only model. GPT-2 has stacks of decoder cells on top of one another, and inside each decoder block resides -Masked Self Attention and FFNN (Dense) Networks.\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-1-2.png\">\n\n\n\n## Masked Self Attention\n\n\nThis is the core part which separated GPT from BERT variants:\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\">\n\n\nMore details can be found here:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [NLP](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n- [Jay's blog](http://jalammar.github.io/illustrated-gpt2/)","metadata":{}},{"cell_type":"code","source":"#For GPT variants, it is important to know that these use 'PAD' tokens additionally and are used from left to right.\n#Unlike BERT variants which are bidirectional in nature, GPT is more of a left to right tokenizer due to the Masking of Attention.\n\ndef transformer_gpt_embedding(model_name,name,inp):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    tokenizer.pad_token = \"[PAD]\"\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the GPT-2 word embeddings\ncls_token1=transformer_gpt_embedding(TFGPT2Model,'openai-gpt',input_word1)\ncls_token2=transformer_gpt_embedding(TFGPT2Model,'openai-gpt',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1,cls_token2)\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1)\nplt.plot(cls_token2)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:21:28.587787Z","iopub.execute_input":"2021-07-09T10:21:28.588048Z","iopub.status.idle":"2021-07-09T10:22:13.194843Z","shell.execute_reply.started":"2021-07-09T10:21:28.588018Z","shell.execute_reply":"2021-07-09T10:22:13.19384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Any Transformer Model can be used for our Classification Use case\n\n\nIn this case, we will be creating a function to create an embedding matrix (instead of word vectors) , very much like Glove. Important point is to remember that, this code is compatible with any BERT variant transformers and also the embedding dimensions should be compatible with the Transformer size mentioned in the [pretrained section](https://huggingface.co/transformers/pretrained_models.html)\n\nThe steps are straightforward, and it is as follows:\n\n- Batch tokenize the input features\n- Once these have been tokenized (with Transformer tokenizers) , we will be applying certain masks\n- Padding the tokenized text \n- Then we have to apply an attention mask\n- The attention mask signifies that we have to segregate the input features in 0s and 1s.\n- Extract the last hidden outputs \n\nUsing these embeddings, we can plug them into our standard neural network architecture (along with Attention).","metadata":{}},{"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\ndistilbert_embeddings = fetch_vectors(train_df.question_body.values,'distilbert-base-uncased')\nprint(distilbert_embeddings.shape)\nplt.plot(distilbert_embeddings[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:28:41.676168Z","iopub.execute_input":"2021-07-09T10:28:41.67658Z","iopub.status.idle":"2021-07-09T10:29:52.247525Z","shell.execute_reply.started":"2021-07-09T10:28:41.676545Z","shell.execute_reply":"2021-07-09T10:29:52.246566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply the Older GRU with Self Attention Model on DistilBert Embeddings\n\nNow, we can apply the GRU model which we created on the DistilBert Embeddings and evaluate the classification performance.Important point to note, that here the dimension is 768, and the 6079 includes the number of features (max_features). We have to replace those variables accordingly.\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-model-input-output-1.png\">","metadata":{}},{"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Distilbert and Self Attention on GRU\nmaxlen=1000\n#Obsserve the max_features which is -> distilbert_embeddings.shape[0]\nmax_features=6079\n#Observe the embed_size->distilbert_embeddings.shape[1]\nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Distilbert)\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[distilbert_embeddings])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(gru_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Self_attention_Distilbert.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:31:06.80503Z","iopub.execute_input":"2021-07-09T10:31:06.805363Z","iopub.status.idle":"2021-07-09T10:32:56.682433Z","shell.execute_reply.started":"2021-07-09T10:31:06.805335Z","shell.execute_reply":"2021-07-09T10:32:56.681519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End of the Previous Notebook Content\n\nThis completes the basic part of using Huggingface transformers which we used to create the transformer embeddings and followed the previous notebook. We included attention and BERT based/GPT based embedding techniques, and now we will be looking into some advanced inference techniques. Also for training on Cloud TPU ,please follow this [notebook](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india). Most of the notebooks are on the same problem statement and can be used as and when required.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Creating and evaluating Bert based Finetuning\n\nThis is a method for using pretrained BERT embeddings and passing it through a series of Dense networks for creating a finetuned BERT/BERT based model for classification.\n\nThe steps followed in this part include:\n- Instantiating a bert model from config files or as a pretrained model\n- Converting the inputs into and encoded form by using bert tokenizers\n- The 3 important aspects are input ids, input masks/attention masks and input segments\n- The question and answer pair are simultaneously encoded \n- Building a keras model using the BERT embeddings and extracting the last layers\n- Fitting the model and evaluating the loss\n","metadata":{}},{"cell_type":"code","source":"from transformers import *\nfrom tensorflow import keras\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nmax_len = 512\n\ndf_train = pd.read_csv('../input/google-quest-challenge/train.csv')\ndf_test = pd.read_csv('../input/google-quest-challenge/test.csv')\ndf_sub = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\n# print('train shape =', df_train.shape)\n# print('test shape =', df_test.shape)\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\n\n\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def encode_qa(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = encode_qa(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = encode_qa(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a =_convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n\ndef create_model(model_name):\n    q_id = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    \n#     config = BertConfig() # print(config) to see settings\n#     config.output_hidden_states = False # Set to True to obtain hidden states\n    \n    bert_model = TFBertModel.from_pretrained(model_name)\n    \n    #  obtain hidden states via bert_model(...)[-1]\n    #Second way of extracting embeddings from bert\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n#     q_hidden_state=bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[-1]\n#     a_hidden_state=bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[-1]\n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    x = tf.keras.layers.Concatenate()([q, a])\n#     x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(30, activation='softmax')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    return model\n\ndf_train=df_train[:1000]\noutputs = compute_output_arrays(df_train, output_categories)\ninputs = compute_input_arrays(df_train, input_categories, tokenizer, max_len)\ntest_inputs = compute_input_arrays(df_test, input_categories, tokenizer, max_len)\nmodel_name='bert-base-uncased'\nmodel = create_model(model_name)\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nmodel.fit(inputs, outputs, epochs=3, batch_size=6)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:37:07.059818Z","iopub.execute_input":"2021-07-09T10:37:07.060176Z","iopub.status.idle":"2021-07-09T10:45:06.342149Z","shell.execute_reply.started":"2021-07-09T10:37:07.060142Z","shell.execute_reply":"2021-07-09T10:45:06.341341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference on BERT/GPT/T5/Electra based Transformers for Downstream task\n\nIn this scope we will be seeing how to create inference logics for some of the popular downstream tasks and optimize the model performance . These include question answering ,token classification, summarization to name a few. The methodology followed in this case should be similar which includes tokenization , followed by running the forward pass of the model and also detokenization of the model outputs.\n\n## Question Answering Inference\n\nThe first topic includes question answer based inference, where the tokens are seoarated into questions and answers for evaluation. The tokenization is also done separately first  on the [CLS] based tokens for BERT based models and the answers are delimited by [SEP ] tokens. The following figure illustrates this:\n\n<img src=\"http://www.mccormickml.com/assets/BERT/SQuAD/input_formatting.png\">\n\nFor infering the answers, there are different ways . The most popular way is to use the transformer pipeline for the \"question answering\" task as shown below.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering,AutoTokenizer,pipeline\nmodel=AutoModelForQuestionAnswering.from_pretrained('abhilash1910/distilbert-squadv1')\ntokenizer=AutoTokenizer.from_pretrained('abhilash1910/distilbert-squadv1')\nnlp_QA=pipeline('question-answering',model=model,tokenizer=tokenizer)\nQA_inp={\n    'question': 'How many parameters does Bert large have?',\n    'context': 'Bert large is really big... it has 24 layers, for a total of 340M parameters.Altogether it is 1.34 GB so expect it to take a couple minutes to download to your Colab instance.'\n}\nresult=nlp_QA(QA_inp)\nresult","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:45:06.343663Z","iopub.execute_input":"2021-07-09T10:45:06.344024Z","iopub.status.idle":"2021-07-09T10:45:32.472963Z","shell.execute_reply.started":"2021-07-09T10:45:06.343988Z","shell.execute_reply":"2021-07-09T10:45:32.471918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## QA Inference using Bert /Bert based models\n\nThe steps are as follows:\n1. Tokenize the question and context tags by CLS and SEP enabled tokenizers\n2. Pass it through the model \n3. Detokenize the model outputs and select the tokens having the maximum scores\n\nWe will be using torch for inference. In the first case, it is important to note that tokenizer.encode_plus uses some arguements which include the input_ids, attention masks and segment ids /token_type_ids . We can also see the input ids by converting them to a list. Since we will only be using the forward pass of the Bert base transformer for QA inference, it is important to write ```torch.no_grad()``` to speed up performance for larger inference datasets. The outputs of the model contain scores in the form of tensors. The QA pipeline output can be used to segment the range containing the maximum scores between the start and the end tokens. The rest of the part involves detokenization of the tokens in the specified maximum value range. The answer is then received by using ```tokenizer.convert_tokens_to_string``` method. This is a simpler way to write a QA pipeline using torch.\n\nResources:\n\n- [HF](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.QuestionAnsweringPipeline)\n- [Blog](https://colab.research.google.com/github/NVIDIA/DeepLearningExamples/blob/master/TensorFlow/LanguageModeling/BERT/notebooks/bert_squad_tf_inference_colab.ipynb)\n- [Blog](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/#:~:text=Super%20Bowl%2050.-,BERT%20Input%20Format,question%20from%20the%20reference%20text.)\n- [QApipeline](https://huggingface.co/transformers/_modules/transformers/pipelines/question_answering.html#QuestionAnsweringPipeline)\n\n<img src=\"http://www.mccormickml.com/assets/BERT/SQuAD/start_token_classification.png\">","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nmodel=AutoModelForQuestionAnswering.from_pretrained('huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad')\ntokenizer=AutoTokenizer.from_pretrained('huggingface/prunebert-base-uncased-6-finepruned-w-distil-squad')\ndef inference(question,context):\n    question_first=bool(tokenizer.padding_side=='right')\n    max_answer_len=5\n    encoded_text=tokenizer.encode_plus(question,context,padding='longest',\n                    truncation=\"longest_first\" ,\n                    max_length=512,\n                    stride=30,\n                    return_tensors=\"pt\",\n                    return_token_type_ids=False,\n                    return_overflowing_tokens=False,\n                    return_offsets_mapping=False,\n                    return_special_tokens_mask=False)\n    input_ids=encoded_text['input_ids'].tolist()[0]\n    #print(input_ids)\n    tokens=tokenizer.convert_ids_to_tokens(input_ids)\n    with torch.no_grad():\n        outputs=model(**encoded_text)\n#     answer_st=outputs.start_logits\n#     answer_et=outputs.end_logits\n    start_,end_=outputs[:2]\n    answer_start=torch.argmax(start_)\n    answer_end=torch.argmax(end_)+1\n    answer=tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n    print(answer)\n    \nquestion= 'How many parameters does Bert large have?'\ncontext= 'Bert is really large...it has 24 layers, for a total of 340M parameters.Altogether it is 1.34 GB so expect it to take a couple minutes to download to your Colab instance.'    \ninference(question,context)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:49:10.623398Z","iopub.execute_input":"2021-07-09T10:49:10.623745Z","iopub.status.idle":"2021-07-09T10:49:45.21387Z","shell.execute_reply.started":"2021-07-09T10:49:10.623696Z","shell.execute_reply":"2021-07-09T10:49:45.213001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summarization using T5 Encoder-Decoder Transformer\n\nIn this case, we are using a Enc-Dec T5 transformer for summarization task. The steps involved here include:\n\n- Tokenization of the input corpus /context which needs to be summarized\n- Since the summarization/text2textgeneration/translation uses the same base class in huggingface it is important to mention the tasks ```summarize:``` inside the tokenizer arguements\n- Perform checks to see if the model configuration values of the min and lax length are less than the tokenized input\n- Perform inference using the torch.no_grad() call\n- Since this is a generation task and all generative decoder models like GPT and enc-dec models like BART/T5/Mariam/electra can be used for this task\n- There are different ways to generate and this is performed using ```model.generate()``` method which specifies the search type used for generating the summary\n- In this case we are using the beam search mechanism with early stopping set to True to prevent overtraining\n- The next part invoves using ```tokenizer.decode``` to abstract the first summarized output (torch tensor) from the model.Some additional arguements like ```skip_special_tokens``` are set as True.\n- The final summarized text is received at the output.\n\n<img src=\"https://iq.opengenus.org/content/images/2020/01/pic2.png\">\n\nResources:\n\n- [Blog](https://towardsdatascience.com/simple-abstractive-text-summarization-with-pretrained-t5-text-to-text-transfer-transformer-10f6d602c426)\n- [Blog](https://betterprogramming.pub/how-to-summarize-text-with-googles-t5-4dd1ae6238b6)\n- [Blog](https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d)\n- [HF](https://huggingface.co/transformers/_modules/transformers/pipelines/text2text_generation.html#Text2TextGenerationPipeline)","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nimport torch\nmodel=T5ForConditionalGeneration.from_pretrained('t5-small') #also autotokenizer/automodel\ntokenizer=T5Tokenizer.from_pretrained('t5-small')\ndef summarize_infer(context):\n    encoded=tokenizer.encode(\"summarize: \"+context,max_length=512,return_tensors='pt',truncation='longest_first')\n    input_length=encoded.shape[-1]\n    print(input_length)\n    min_length=model.config.min_length\n    max_length=model.config.max_length\n    print(min_length,max_length)\n    if(min_length//2 > input_length):\n        model.config.summarization.min_length=input_length\n    if(max_length>input_length):\n        model.config.max_length=input_length\n    print('Model Configuration',model.config)   \n    with torch.no_grad():\n        summarize=model.generate(encoded,model.config.max_length+50,model.config.min_length,num_beams=4,early_stopping=True)\n    #print('summarized tokens',summarize)\n    summarized_text=tokenizer.decode(\n                    summarize[0],\n                    skip_special_tokens=True,\n                    clean_up_tokenization_spaces=False,\n                )\n    print(summarized_text)\ncontext=\"\"\"\nThe US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.\nThe US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\nAt the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\n\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"\nThe Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n\"\"\"\n\nsummarization=summarize_infer(context)\nprint(summarization)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:45:32.561634Z","iopub.execute_input":"2021-07-09T10:45:32.562291Z","iopub.status.idle":"2021-07-09T10:45:58.608991Z","shell.execute_reply.started":"2021-07-09T10:45:32.562232Z","shell.execute_reply":"2021-07-09T10:45:58.60751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Generation using GPT2 Decoder Transformer\n\nIn this case we will be looking how to generate sequence of tokens using decoder GPT2 transformer. The steps are as follows:\n\n- Since GPT2 uses special tokens , we are specifying them : bos_token,eos_token specify the beginning and end of sentence respectively. There are additional tokens which include unk (for unknows), pad(padding) and sep (separator) token as in BERT\n- We first load the GPT2 transformer with the added special tokens parameter set in the tokenzier config.\n- We load the GPT2 model from with the special tokens and also resize the models token embeddings using the ```model.resize_token_embedding`` method\n- We first tokenize the data tor torch tensors and use the forward pass of  the GPT 2 model.\n- In this case we are using the beam generation mechanism, and selecting the topk items from the resulting tensors. There is also a ```repitition_penalty``` applied which implies that the model will try to avoid predicting same tokens.\n- The next part is decoding the tokens by using ```tokenizer.decode``` method as specified earlier and we get the final predicted generations\n\nSome blogs related to beam search and topk sampling/nucleus sampling\n\n- [Blog HF](https://huggingface.co/blog/how-to-generate)\n- [Blog](https://snappishproductions.com/blog/2020/03/01/chapter-9.5-text-generation-with-gpt-2-and-only-pytorch.html.html)\n- [HF](https://huggingface.co/transformers/_modules/transformers/pipelines/text_generation.html#TextGenerationPipeline)\n- [HF](https://huggingface.co/transformers/main_classes/pipelines.html#textgenerationpipeline)\n\n<img src=\"https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png\">","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel,AutoConfig,AutoTokenizer,AutoModelForPreTraining\nSPECIAL_TOKENS = { \"bos_token\": \"<|BOS|>\",\n                    \"eos_token\": \"<|EOS|>\",\n                    \"unk_token\": \"<|UNK|>\",                    \n                    \"pad_token\": \"<|PAD|>\",\n                    \"sep_token\": \"<|SEP|>\"}\ndef generation_infer(context,special_tokens=None):\n    tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n    if special_tokens:\n        tokenizer.add_special_tokens(special_tokens)\n    config=AutoConfig.from_pretrained('gpt2',bos_token_id=tokenizer.bos_token_id,eos_token_id=tokenizer.eos_token_id,\n                                            sep_token_id=tokenizer.sep_token_id,\n                                            pad_token_id=tokenizer.pad_token_id,\n                                            output_hidden_states=False)\n    model=GPT2LMHeadModel.from_pretrained('gpt2',config=config)\n    if special_tokens:\n        model.resize_token_embeddings(len(tokenizer))\n    #context=SPECIAL_TOKENS['bos_token']+context+SPECIAL_TOKENS['eos_token']\n    context =  context \n    generated=tokenizer.encode(context,return_tensors='pt')\n    print(model.config)\n    print(generated)\n#     with torch.no_grad():\n    sample_generations= model.generate(generated,do_sample=True,   \n                                min_length=50, \n                                max_length=100,\n                                num_beams=5,\n                                top_k=30,                                 \n                                top_p=0.7,        \n                                temperature=0.9,\n                                repetition_penalty=2.0,\n                                num_return_sequences=10)\n    generated_list=[]\n    for i,generation in enumerate(sample_generations):\n        text=tokenizer.decode(generation,skip_special_tokens=True)\n        \n        generate=context+text[len(context):]\n        generated_list.append(generate)\n    #generated_list.extend(generate)\n    return generated_list\n\ncontext=\"I enjoy walking with my cute dog at evening\"\n# generation_infer(context,SPECIAL_TOKENS)\ngeneration_infer(context)\n        \n        \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:45:58.610325Z","iopub.execute_input":"2021-07-09T10:45:58.610687Z","iopub.status.idle":"2021-07-09T10:47:22.753629Z","shell.execute_reply.started":"2021-07-09T10:45:58.610648Z","shell.execute_reply":"2021-07-09T10:47:22.75276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NER using Electra Encoder-Decoder Transformer\n\nIn this case, we will be seeing how to solve NER using electra transformer.Since this a little complicated we will be looking at some of the important points:\n\n- The initial steps are similar as it involves tokenization of the input corpus and then running it through the model.The ```token_classification_infer``` method is the driver part of the program\n- After the inference, there is an issue that subtokens may conflict with the NER tag such as 'Micro|Soft' and 'MicroSoft 'should ideally represent the same NER token. \n- To resolve this issue, we apply softmax on the model output tensors, and then detokenize the outputs to detect subwords and  determine the start and end of each entity in the corpus. This is done in the ```gather_pre_entities``` method\n- The next step is aggregating subwords/subtokens into complete couterparts so that they can be represented as a single NER tag.The aggregation strategy is specified as None we return the entities. Else the aggregation happens by grouping subwords on their relative scores(average etc) for creating the tags. These are in the ```aggreagate_word`` method\n- The final step is returning all the tokens combined together and removing [0] token since it can be ignored for NER.\n- The resultant output provides the NER tags of all the items and their corresponding scores\n\n<img src=\"https://www.researchgate.net/publication/347087784/figure/fig2/AS:1003243366780929@1616203266581/Illustration-of-BERT-models-for-A-NER-B-family-member-attributes-including-side-and.ppm\">\n\nResource:\n\n- [HF](https://huggingface.co/transformers/_modules/transformers/pipelines/question_answering.html#QuestionAnsweringPipeline)\n\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForTokenClassification\nimport torch\nimport numpy as np\n\ntokenizer=AutoTokenizer.from_pretrained('dbmdz/electra-large-discriminator-finetuned-conll03-english')\nmodel=AutoModelForTokenClassification.from_pretrained('dbmdz/electra-large-discriminator-finetuned-conll03-english')\n\nclass AggregationStrategy:\n    \"\"\"All the valid aggregation strategies for TokenClassificationPipeline\"\"\"\n\n    NONE = \"none\"\n    SIMPLE = \"simple\"\n    FIRST = \"first\"\n    AVERAGE = \"average\"\n    MAX = \"max\"\ndef gather_pre_entities(sentence: str,input_ids: np.ndarray,scores: np.ndarray,offset_mapping,special_tokens_mask: np.ndarray) :\n    \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\"\n    pre_entities = []\n    for idx, token_scores in enumerate(scores):\n        # Filter special_tokens, they should only occur\n        # at the sentence boundaries since we're not encoding pairs of\n        # sentences so we don't have to keep track of those.\n        if special_tokens_mask[idx]:\n            continue\n\n        word = tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n        if offset_mapping is not None:\n            start_ind, end_ind = offset_mapping[idx]\n            word_ref = sentence[start_ind:end_ind]\n            is_subword = len(word_ref) != len(word)\n\n            if int(input_ids[idx]) == tokenizer.unk_token_id:\n                word = word_ref\n                is_subword = False\n        else:\n            start_ind = None\n            end_ind = None\n            is_subword = False\n\n        pre_entity = {\n                \"word\": word,\n                \"scores\": token_scores,\n                \"start\": start_ind,\n                \"end\": end_ind,\n                \"index\": idx,\n                \"is_subword\": is_subword,\n            }\n        pre_entities.append(pre_entity)\n    return pre_entities\n\n\ndef group_sub_entities( entities) -> dict:\n    \"\"\"\n    Group together the adjacent tokens with the same entity predicted.\n\n    Args:\n    entities (:obj:`dict`): The entities predicted by the pipeline.\n    \"\"\"\n    # Get the first entity in the entity group\n    entity = entities[0][\"entity\"].split(\"-\")[-1]\n    scores = np.nanmean([entity[\"score\"] for entity in entities])\n    tokens = [entity[\"word\"] for entity in entities]\n\n    entity_group = {\n            \"entity_group\": entity,\n            \"score\": np.mean(scores),\n            \"word\": tokenizer.convert_tokens_to_string(tokens),\n            \"start\": entities[0][\"start\"],\n            \"end\": entities[-1][\"end\"],\n    }\n    return entity_group\n\n\ndef get_tag( entity_name: str) :\n    if entity_name.startswith(\"B-\"):\n        bi = \"B\"\n        tag = entity_name[2:]\n    elif entity_name.startswith(\"I-\"):\n        bi = \"I\"\n        tag = entity_name[2:]\n    else:\n        # It's not in B-, I- format\n        bi = \"B\"\n        tag = entity_name\n    return bi, tag\n\ndef group_entities( entities) :\n    \"\"\"\n    Find and group together the adjacent tokens with the same entity predicted.\n\n    Args:\n    entities (:obj:`dict`): The entities predicted by the pipeline.\n    \"\"\"\n\n    entity_groups = []\n    entity_group_disagg = []\n\n    for entity in entities:\n        if not entity_group_disagg:\n            entity_group_disagg.append(entity)\n            continue\n\n    # If the current entity is similar and adjacent to the previous entity,\n    # append it to the disaggregated entity group\n    # The split is meant to account for the \"B\" and \"I\" prefixes\n    # Shouldn't merge if both entities are B-type\n    bi, tag = get_tag(entity[\"entity\"])\n    last_bi, last_tag = get_tag(entity_group_disagg[-1][\"entity\"])\n\n    if tag == last_tag and bi != \"B\":\n        # Modify subword type to be previous_type\n        entity_group_disagg.append(entity)\n    else:\n        # If the current entity is different from the previous entity\n        # aggregate the disaggregated entity group\n        entity_groups.append(group_sub_entities(entity_group_disagg))\n        entity_group_disagg = [entity]\n    if entity_group_disagg:\n        # it's the last entity, add it to the entity groups\n        entity_groups.append(group_sub_entities(entity_group_disagg))\n\n    return entity_groups\n\ndef aggregate(pre_entities, aggregation_strategy: AggregationStrategy):\n    if aggregation_strategy in {AggregationStrategy.NONE, AggregationStrategy.SIMPLE}:\n        entities = []\n        for pre_entity in pre_entities:\n            entity_idx = pre_entity[\"scores\"].argmax()\n            score = pre_entity[\"scores\"][entity_idx]\n            entity = {\n                    \"entity\": model.config.id2label[entity_idx],\n                    \"score\": score,\n                    \"index\": pre_entity[\"index\"],\n                    \"word\": pre_entity[\"word\"],\n                    \"start\": pre_entity[\"start\"],\n                    \"end\": pre_entity[\"end\"],\n                }\n            entities.append(entity)\n    else:\n        entities = aggregate_words(pre_entities, aggregation_strategy)\n\n    if aggregation_strategy == AggregationStrategy.NONE:\n        return entities\n    return group_entities(entities)\n\ndef aggregate_word( entities, aggregation_strategy: AggregationStrategy) -> dict:\n    word = tokenizer.convert_tokens_to_string([entity[\"word\"] for entity in entities])\n    if aggregation_strategy == AggregationStrategy.FIRST:\n        scores = entities[0][\"scores\"]\n        idx = scores.argmax()\n        score = scores[idx]\n        entity = model.config.id2label[idx]\n    elif aggregation_strategy == AggregationStrategy.MAX:\n        max_entity = max(entities, key=lambda entity: entity[\"scores\"].max())\n        scores = max_entity[\"scores\"]\n        idx = scores.argmax()\n        score = scores[idx]\n        entity = model.config.id2label[idx]\n    elif aggregation_strategy == AggregationStrategy.AVERAGE:\n        scores = np.stack([entity[\"scores\"] for entity in entities])\n        average_scores = np.nanmean(scores, axis=0)\n        entity_idx = average_scores.argmax()\n        entity = model.config.id2label[entity_idx]\n        score = average_scores[entity_idx]\n    else:\n        raise ValueError(\"Invalid aggregation_strategy\")\n    new_entity = {\n            \"entity\": entity,\n            \"score\": score,\n            \"word\": word,\n            \"start\": entities[0][\"start\"],\n            \"end\": entities[-1][\"end\"],\n    }\n    return new_entity\n\ndef token_classification_infer(_inputs):\n    answers = []\n    for i, sentence in enumerate(_inputs):\n        \n        tokens = tokenizer(\n                sentence,\n                return_attention_mask=False,\n                return_tensors='pt',\n                truncation=\"longest_first\",\n                return_special_tokens_mask=True,\n                return_offsets_mapping=tokenizer.is_fast,\n            )\n        if tokenizer.is_fast:\n            offset_mapping = tokens.pop(\"offset_mapping\").cpu().numpy()[0]\n        elif offset_mappings:\n            offset_mapping = offset_mappings[i]\n        else:\n            offset_mapping = None\n\n        special_tokens_mask = tokens.pop(\"special_tokens_mask\").cpu().numpy()[0]\n\n        with torch.no_grad():\n            entities = model(**tokens)[0][0].cpu().numpy()\n            input_ids = tokens[\"input_ids\"].cpu().numpy()[0]\n\n        scores = np.exp(entities) / np.exp(entities).sum(-1, keepdims=True)\n        pre_entities = gather_pre_entities(sentence, input_ids, scores, offset_mapping, special_tokens_mask)\n        grouped_entities = aggregate(pre_entities, aggregation_strategy=AggregationStrategy.NONE)\n        # Filter anything that is in ignore_labels\n        ignore_labels=[\"O\"]\n        entities = [entity for entity in grouped_entities if entity.get(\"entity\", None) not in ignore_labels and entity.get(\"entity_group\", None) not in ignore_labels ]\n        answers.append(entities)\n\n    if len(answers) == 1:\n        return answers[0]\n    return answers\nsentences=[\"The UEFA Champions League (abbreviated as UCL) is an annual club football competition organised by the Union of European Football Associations (UEFA) and contested by top-division European clubs, deciding the competition winners through a round robin group stage to qualify for a double-legged knockout format, and a single leg final. It is one of the most prestigious football tournaments in the world and the most prestigious club competition in European football, played by the national league champions (and, for some nations, one or more runners-up) of their national associations.\"]\ntoken_classification_infer(sentences)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:47:22.755119Z","iopub.execute_input":"2021-07-09T10:47:22.755473Z","iopub.status.idle":"2021-07-09T10:48:41.507834Z","shell.execute_reply.started":"2021-07-09T10:47:22.755436Z","shell.execute_reply":"2021-07-09T10:48:41.507028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Classification with Microsoft DialogRPT\n\nIn this case, we will be inferencing for simple text classification on a corpus, using the following steps:\n\n- This is astraightforward method where we first tokenize the text using the arguements specified.\n- We then pass the tensor through the model.\n- The next step involves perfoming softmax of 1st and higher order depending on the number of labels we would like to have. If the model config has 1 label then a sigmoid (softmax order 1) is applied else traditional higher order softmax is applied.\n- The next part involes associating the labels with the corresponding softmax scores. Since this is the backend of the Text Classification pipeline, the labels are defaulted from the model configurations.\n\n<img src=\"https://i.morioh.com/200613/a1cacd81.jpg\">\n\nResource:\n\n- [Repository](https://i.morioh.com/200613/a1cacd81.jpg)\n- [HF](https://huggingface.co/transformers/main_classes/pipelines.html#textclassificationpipeline)\n\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForSequenceClassification\nimport torch\ntokenizer=AutoTokenizer.from_pretrained('microsoft/DialogRPT-human-vs-rand')\nmodel=AutoModelForSequenceClassification.from_pretrained('microsoft/DialogRPT-human-vs-rand')\ndef sequence_infer(context):\n    tokens=tokenizer(context,return_attention_mask=False,\n                return_tensors='pt',\n                truncation=\"longest_first\",\n                return_special_tokens_mask=False)\n    with torch.no_grad():\n        outputs = model(**tokens)[0][0].cpu().numpy()\n    \n\n    if model.config.num_labels == 1:\n        scores = 1.0 / (1.0 + np.exp(-outputs))\n    else:\n        scores = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True)\n        \n    print(scores)\n    return_all_scores=True\n    \n    return [\n                {\"label\": model.config.id2label[item.argmax()], \"score\": item.max().item()} for item in scores\n        ]\nsequence_infer([\"I like icecream\"]*2)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:50:09.302318Z","iopub.execute_input":"2021-07-09T10:50:09.30263Z","iopub.status.idle":"2021-07-09T10:51:38.800157Z","shell.execute_reply.started":"2021-07-09T10:50:09.302602Z","shell.execute_reply":"2021-07-09T10:51:38.798908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text To Text Paraphrasing/Generation with Encoder-Decoder T5\n\nThis example is similar to T5 for summarization since both of them use the same base class of ```text2textgeneration```:\n- We tokenize the data using t5 small model and use the ```tokenizer.encode_plus``` module to abstract the input ids ,attention masks separately and pass them to the model\n- In the model generation /inference stage , we have used separate options which include either beam searching the generations (slipgram,classical or returnseq) or by sampling(nucleus, tokp/topp).\n- We then decode the abstractions of the model inference and use ```tokenizer.decode``` to decode the tokens.\n- Ther resulting final array is then returned which contains the generated/paraphrased text\n\nResource:\n\n- [HF](https://huggingface.co/transformers/_modules/transformers/pipelines/text_generation.html#TextGenerationPipeline)\n- [HF](https://huggingface.co/transformers/main_classes/pipelines.html#text2textgenerationpipeline)\n- [Blog](https://huggingface.co/blog/how-to-generate)\n\n<img src=\"https://huggingface.co/blog/assets/02_how-to-generate/top_p_sampling.png\">\n","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\nimport torch\nmodel=T5ForConditionalGeneration.from_pretrained('t5-small') #also autotokenizer/automodel\ntokenizer=T5Tokenizer.from_pretrained('t5-small')\ndef beam_generations(model,input_ids,attention_masks,types):\n    if (types=='skipgram'):\n        beam_output_skipgram = model.generate(\n                            input_ids=input_ids,attention_mask=attention_masks, \n                            max_length=256, \n                            num_beams=5, \n                            no_repeat_ngram_size=2, \n                            early_stopping=True\n                                )\n        return beam_output_skipgram\n    elif(types==\"classical\"):\n        beam_output_classic = model.generate(\n                               input_ids=input_ids,attention_mask=attention_masks, \n                                max_length=256, \n                                num_beams=5,\n                                early_stopping=True\n                                )\n        return beam_output_classic\n    elif (types==\"returnseq\"):\n        beam_output_returnseq = model.generate(\n                                input_ids=input_ids,attention_mask=attention_masks, \n                                num_beams=50, \n                                no_repeat_ngram_size=2,\n                                num_return_sequences=10,     \n                                early_stopping=True\n                                )\n        return beam_output_returnseq\n    \ndef sampling_generations(model,input_ids,attention_masks,types):\n    if types==\"nucleus\":\n        sample_nucleus = model.generate(\n                        input_ids=input_ids,attention_mask=attention_masks,\n                        do_sample=True, \n                        max_length=256, \n                        top_p=0.98, \n                        top_k=120\n                    )\n        return sample_nucleus\n    elif types==\"kpsampling\":\n        sample_outputskp = model.generate(\n                        input_ids=input_ids,attention_mask=attention_masks,\n                        do_sample=True, \n                        max_length=256, \n                        top_k=120, \n                        top_p=0.98, \n                        early_stopping=True,\n                        num_return_sequences=10\n        )\n        return sample_outputskp\n    \n\ndef paraphrase_infer(context):\n    encoded=tokenizer.encode_plus(context,max_length=512,return_tensors='pt',truncation='longest_first',pad_to_max_length=True)\n    input_length=encoded['input_ids'].shape[-1]\n    print(input_length)\n    min_length=model.config.min_length\n    max_length=model.config.max_length\n    print(min_length,max_length)\n    if(min_length//2 > input_length):\n        model.config.min_length=input_length\n    if(max_length>input_length):\n        model.config.max_length=input_length\n    print('Model Configuration',model.config)   \n    input_ids, attention_masks = encoded[\"input_ids\"].to(torch.device('cpu')), encoded[\"attention_mask\"].to(torch.device('cpu'))\n    with torch.no_grad():\n        paraphrase=beam_generations(model,input_ids,attention_masks,\"skipgram\")\n    paraphrased=[]\n    for beam_output in paraphrase:\n        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n        paraphrased.append(sent)\n    return paraphrased\nparaphrase_infer(\"Tha match was postponed due to bad weather\")    ","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:51:38.801846Z","iopub.execute_input":"2021-07-09T10:51:38.802239Z","iopub.status.idle":"2021-07-09T10:51:48.570108Z","shell.execute_reply.started":"2021-07-09T10:51:38.802199Z","shell.execute_reply":"2021-07-09T10:51:48.569262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion of Autoregressive Transformer Training and Inference Module\n\nHere we have looked at some of the ways to build custom models and transform them to use Transformers. We have seen the Transformer embeddings which can be used for any downstream task such as classfication, and have also created finetuned model based on BERT base/ (or any bert variant) for the classification task. We have also looked into different attention strategies and transformer architecture in depth. Finally we have learnt how to use pretrained models and write custom inference modules for some of the most popular downstream tasks which include token classification (NER),QA,text classification,text2textparaphrasing, text generation, and summarization. For additional inference logics related to zero shot classification , it is present in the [Github gist](https://gist.github.com/abhilash1910/5e2bee7c3eccea843a8e9406b221d8f2) and also for other inference tasks based on transformers are present in my [Github](https://gist.github.com/abhilash1910). This concludes the session on Transformers.","metadata":{}}]}