{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Workshop- ML India\n\n\n\n## Understanding Transformers\n\n\nIn this session, we will be having an indepth overview of what Transformers are and how simply we can use them for any sentiment analysis or any language modelling task in general. Tre traditional methods of deep learning ,using LSTM variants for processing data in the form of sequential networks has underwent a severe modification and forms the current building blocks of all the SOTA transformer models that we see today. In this case, we will start will traditional neural networks to understand how they work and then try to build a simple transformer using those building blocks. Once that is completed, we will be leveraging state of the art open source libraries for our use case. Keeping the transformer architecture for reference,which we will be visiting again as we progress. \n\n\n<img src=\"https://miro.medium.com/max/500/1*do7YDFF2sads0p9BnjzrWA.png\">\n\n\n\n\n\nIn the first section we will be using simple neural networks for our use case. We will be understanding about the traditional LSTM and Recurrent Neural Networks.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Basics of Neural Networks\n\nIn this scope, we will be looking into standard neural networks with very little modifications and minimalistic codebase. For simplicity we will be using Keras Framework:\n\n<img src=\"https://keras.io/img/logo.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Building a Bare Minimal Neural Network \n\nHere, we will be building a stand-alonw neural network model just for classifying the labels to the respective questions. For this we will be using RNNs/LSTMs/GRU for our usecase. A classic LSTM based network is one of the most fundamental building blocks of all the robust architectures that we see today.For the first part we will be focussing on standard RNNs. Some resources for RNNs:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [RNN](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india)\n\n\n<img src=\"https://miro.medium.com/max/875/1*n-IgHZM5baBUjq0T7RYDBw.gif\">\n\n\n## Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far. Forward pass of Classical RNNs have the following formula :\n\n\n## Forward Pass Formula \n\n\n For the hidden gates: <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/12/06005300/eq2.png\">\n \n For the output gate: <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/12/06005750/outeq.png\">\n \n \nGenerally for the output of the forward pass, we generally use a softmax activation on the output.\n\n\n## Backward Pass Equations and BPTT\n\n[Backpropagation Through Time](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) is done in RNNs which allows flow of gradients through each hidden time step. The effective loss function for RNNs is : \n\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++E_t%28y_t%2C+%5Chat%7By%7D_t%29+%26%3D+-+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D+%5C%5C++E%28y%2C+%5Chat%7By%7D%29+%26%3D%5Csum%5Climits_%7Bt%7D+E_t%28y_t%2C%5Chat%7By%7D_t%29+%5C%5C++%26+%3D+-%5Csum%5Climits_%7Bt%7D+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nour goal is to calculate the gradients of the error with respect to our parameters U, V and W and then learn good parameters using Stochastic Gradient Descent. Just like we sum up the errors, we also sum up the gradients at each time step for one training example: <img src=\"http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+E%7D%7B%5Cpartial+W%7D+%3D+%5Csum%5Climits_%7Bt%7D+%5Cfrac%7B%5Cpartial+E_t%7D%7B%5Cpartial+W%7D&bg=ffffff&fg=000&s=1\">\n\nTo calculate these gradients we use the chain rule of differentiation. That‚Äôs the backpropagation algorithm when applied backwards starting from the error. For the rest of this post we‚Äôll use E_3 as an example, just to have concrete numbers to work with.\n\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+V%7D+%26%3D%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+V%7D%5C%5C++%26%3D%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+z_3%7D%5Cfrac%7B%5Cpartial+z_3%7D%7B%5Cpartial+V%7D%5C%5C++%26%3D%28%5Chat%7By%7D_3+-+y_3%29+%5Cotimes+s_3+%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nEffectively the logic behind the chain rule is denoted by the following formula:\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nBPTT can be understood clearly with this image\n\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTBrnLrCdE0ae14fv4Wc7ThY61Ikr6lzsyJSQ&usqp=CAU\">\n\n\n\n## Classical RNN image\n\n\nA classic RNN consists of the following image:\n\n\n<img src=\"https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\n\nSome resources:\n    \n- [Blog](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n- [Video](https://youtu.be/-eBjweSRgFc)\n- [Blog](https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)\n- [Blog](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)\n- [Documentation](https://keras.io/api/layers/recurrent_layers/simple_rnn/)\n\n## Drawbacks of RNNs\n\n\nVanishing Gradients: The chain rule of differentiation of the weight vectors often lead to shrinkage in the change in the weights of the gradients for each iteration. This leads to  a slower convergence and many times it leads to an oscillation around local minimas. The chain rule formula: <img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\nNote that <img src=\"http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D+&bg=ffffff&fg=000&s=1\">  is a chain rule in itself! For example, <img src=\"http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_1%7D+%3D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_2%7D%5Cfrac%7B%5Cpartial+s_2%7D%7B%5Cpartial+s_1%7D&bg=ffffff&fg=000&s=1\">. Also note that because we are taking the derivative of a vector function with respect to a vector, the result is a matrix (called the Jacobian matrix) whose elements are all the pointwise derivatives. We can rewrite the above gradient:\n\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D++%5Cleft%28%5Cprod%5Climits_%7Bj%3Dk%2B1%7D%5E%7B3%7D++%5Cfrac%7B%5Cpartial+s_j%7D%7B%5Cpartial+s_%7Bj-1%7D%7D%5Cright%29++%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\nExploding Gradients: The chain rule (mainly due to tanh activation) often leads to overshooting of the gradient weights.This may lead to gradients which are really large at each iteration of the training process.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/google-quest-challenge/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Articulating the Problem statement\n\nIn the Google Quest QA challenge we will be using 'question_body' and 'category' and build a classifier (sentiment) and test the performance on this discriminative dataset. Along with this, we will be focussing later,on how to create appropriate answers from the questions using Transformers. For the initial aspects, we will be focussing on how to create simple models and encoder decoders."},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import AutoTokenizer,AutoModelForQuestionAnswering\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D,SimpleRNN\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is used to label encode the labels for categorization\nfrom sklearn.preprocessing import LabelEncoder\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['category'])\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(SimpleRNN(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_RNN.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture\n\nThe model architecture for the Bidirectional Simple RNN can be seen as below:\n\n<img src=\"https://i.imgur.com/QFsESSn.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Glove Embeddings, In this case, we will be using pretrained Glove 200dimension embeddings.\n#The importance of using pretrained embeddings is to allow more semantic references of the word/sentence vectors.\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\nEMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using with pretrained Glove 200d embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding -with pretrained glove weights\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(SimpleRNN(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_RNN_Glove200d.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple RNN with Glove200D pretrained embeddings\n\nThe model architecture can be shown as below:\n\n<img src=\"https://i.imgur.com/3ZBQApl.png\">"},{"metadata":{},"cell_type":"markdown","source":"# LSTM- Long Short Term Memory\n\n[LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) are gated recurrent networks having 4 gates with (tanh/sigmoid) activation units. These architectures are the the building blocks of all the transformer architectures that we see, and the 4 gates combine input from different time stamps to produce the output. In a LSTM, there are typically 3 input and output signals: The h (hidden cell output from the previous timestep), c (the signal from previous cell), and the x(input vectors). Outputs involve the updated ht+1(hidden cell output of current block) value, ct+1, (updated c signal from the present cell) and the output(o).\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n\n\n## Operation Steps - LSTM\n\nThe first step in our LSTM is to decide what information we‚Äôre going to throw away from the cell state. This decision is made by a sigmoid layer called the ‚Äúforget gate layer.‚Äù It looks at ht‚àí1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct‚àí1. A 1 represents ‚Äúcompletely keep this‚Äù while a 0 represents ‚Äúcompletely get rid of this.‚Äù\n\nLet‚Äôs go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\"> \n\n\nThe next step is to decide what new information we‚Äôre going to store in the cell state. This has two parts. First, a sigmoid layer called the ‚Äúinput gate layer‚Äù decides which values we‚Äôll update. Next, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the next step, we‚Äôll combine these two to create an update to the state.\n\nIn the example of our language model, we‚Äôd want to add the gender of the new subject to the cell state, to replace the old one we‚Äôre forgetting.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\">\n\nIt‚Äôs now time to update the old cell state, Ct‚àí1, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it.\n\nWe multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it‚àóC~t. This is the new candidate values, scaled by how much we decided to update each state value.\n\nIn the case of the language model, this is where we‚Äôd actually drop the information about the old subject‚Äôs gender and add the new information, as we decided in the previous steps.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n\nFinally, we need to decide what we‚Äôre going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we‚Äôre going to output. Then, we put the cell state through tanh (to push the values to be between ‚àí1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that‚Äôs what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that‚Äôs what follows next.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\">\n\n\n\nResources:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\n\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM model architecture\n\n\nThe model architecture can be shown as below:\n\n\n<img src=\"https://i.imgur.com/81b4WSd.png\">"},{"metadata":{},"cell_type":"markdown","source":"## LSTM model with embeddings\n\nNow we will be applying the glove embeddings (200d) for boosting performance (if any)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using with pretrained Glove 200d embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding -with pretrained glove weights\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Glove200d.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM with Glove Embedding Architecture\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/oOmKx56.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Gated Recurrent Units\n\n[GRUs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single ‚Äúupdate gate.‚Äù It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(GRU(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture for vanilla GRU\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/jaZegBX.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of Basics of Neural Network Models\n\nWe have seen a glimpse of LSTM/GRU along with Simple RNNs and also saw how to create a very simple classification neural network model with minimal lines of code , that too along with pretrained glove embeddings. Now we will be going into understanding about Transformer architectures."},{"metadata":{},"cell_type":"markdown","source":"# Understanding Attention Mechanism\n\n[Attention mechanism](https://arxiv.org/abs/1706.03762) is the most important aspect in language modelling. There are many variants of attention such as Bahdanau Attention, Luong Attention, Dot Product Attention,Self Attention. A detailed description of the attention mechanism is provided in this [kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n\n\n## Bahdanau Attention\n\n\n<img src=\"https://miro.medium.com/max/639/1*qhOlQHLdtfZORIXYuoCtaA.png\">\n\n\nBahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n\nlet‚Äôs understand the Attention mechanism suggested by Bahdanau\n\nAll hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\nThe attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\nThe model predicts a target word based on the context vectors associated with the source position and the previously generated target words.\nAlignment Score\nThe alignment score maps how well the inputs around position ‚Äúj‚Äù and the output at position ‚Äúi‚Äù match. The score is based on the previous decoder‚Äôs hidden state, s‚Çç·µ¢‚Çã‚ÇÅ‚Çé just before predicting the target word and the hidden state, h‚±º of the input sentence.\n\n\n<img src=\"https://miro.medium.com/max/535/1*u2YdTRPjN34Fpr-zxvoJsg.png\">\n\n\nThe decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector. The alignment vector that has the same length with the source sequence and is computed at every time step of the decode.\n\nAttention Weights\nWe apply a softmax activation function to the alignment scores to obtain the attention weights.\n\n\n<img src=\"https://miro.medium.com/max/685/1*3aCyU9aSVHvxzOwvQdExdQ.png\">\n\n\n\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\">\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\">\n\n\n\n\n## Luong Attention\n\n\n\n\n### Global Attention\n\n\n<img src=\"https://miro.medium.com/max/626/1*LhEapXF1mtaB3rDgIjcceg.png\">\n\nLuong, et al., 2015 proposed the ‚Äúglobal‚Äù and ‚Äúlocal‚Äù attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n\nThe commonality between Global and Local attention\n\n- At each time step t, in the decoding phase, both approaches, global and local attention, first take the hidden state h‚Çú at the top layer of a stacking LSTM as an input.\n- The goal of both approaches is to derive a context vector ùí∏‚Çú to capture relevant source-side information to help predict the current target word y‚Çú\n- Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions.\n- Global and local attention models differ in how the context vector ùí∏‚Çú is derived\n- Before we discuss the global and local attention, let‚Äôs understand the conventions used by Luong‚Äôs attention mechanism for any given time t\n\n   - ùí∏‚Çú : context vector\n   - a‚Çú : alignment vector\n   - h‚Çú : current target hidden state\n   - h‚Çõ : current source hidden state\n   - y‚Çú: predicted current target word\n   - hÀú‚Çú : Attentional vectors\n   \n   \nThe global attentional model considers all the hidden states of the encoder when calculating the context vector ùí∏‚Çú.\n\nA variable-length alignment vector a‚Çú equal to the size of the number of time steps in the source sequence is derived by comparing the current target hidden state h‚Çú with each of the source hidden state h‚Çõ\nThe alignment score is referred to as a content-based function for which we consider three different alternatives\n\n\n### Local Attention\n\n\n<img src=\"https://miro.medium.com/max/538/1*YXjdGl3CnSfHfzYpQiObgg.png\">\n\n\n\n- Local attention only focuses on a small subset of source positions per target words unlike the entire source sequence as in global attention\n- Computationally less expensive than global attention\n- The local attention model first generates an aligned position P‚Çú for each target word at time t.\n- The context vector ùí∏‚Çú is derived as a weighted average over the set of source hidden states within selected the window\n- The aligned position can be monotonically or predictively selected\n\n\n### Formulation\n\n\n<img src=\"https://miro.medium.com/max/875/1*_Ta67S8_lXTbVzJMztkxKg.png\">\n\n\n\n## List of Different Attention Mechanisms\n\n\n<img src=\"https://theaisummer.com/assets/img/posts/attention/attention-calculation.png\">\n\n\n## Resources:\n\n- [Luong Paper](https://arxiv.org/abs/1508.04025)\n- [Bahdanau Paper](https://arxiv.org/abs/1409.0473)\n- [Luong 2015](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n- [Kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)"},{"metadata":{},"cell_type":"markdown","source":"# Using a minimalistic implementation of Attention\n\nIn this scope, we will be building our own implementations of Attention mechanisms for our use case.Most of the implementations will have similar parameters. In the 'call' method, we generally have 2 vector inputs, but here we will be using a single input vector. The reason is because these algorithms are built for encoder-decoder models but in this case, since we only have a single neural network model , we need only one dimensional input vectors.\n\nHence, \n\n```python\ndef call(self,q):\n        self.q=q\n        self.v=q\n```\n\nWe are using the same q for both the inputs. In our sequential model, the q is the output from the, LSTM cell (only the outputs and no hidden cell states). Since the outputs/inputs of LSTM are 3 and for a GRU are 2, we have to modify the basic meural network model to adapt with the Attention mechanism.In most cases, the implementation of the model remains the same:\n\n- Creating Input layer\n- Creating the Embedding Layer (if pretrained embeddings are used or not)\n- Applying a LSTM/GRU variant or a bidirectional variant\n- Applying an Attention mechanism on top of the outputs\n- Feed Forward Dense Networks with the required activation functions\n\n\nWhen we will be going through a simple use case of Encoder Decoder based attention, then we will be using a 2d input based Attention mechanisms. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementing Different Attention Layers for our keras model\nimport math\nclass Bahdanau_Attention_1D(tf.keras.layers.Layer):\n    #A class for Bahdanau Attention\n    def __init__(self,units):\n        super(Bahdanau_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q):\n        self.q=q\n        self.v=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\n\nclass Luong_Attention_1D(tf.keras.layers.Layer):\n    #A class for Luong Attention\n    def __init__(self,units):\n        super(Luong_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q):\n        self.q=q\n        self.v=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.q)*(self.v)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\nclass Scaled_Dot_Product_Attention_1D(tf.keras.layers.Layer):\n    #Scaled dot product Attention\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,n):\n        self.q=q\n        self.v=q\n        self.n=n\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=((self.q)*(self.v))/math.sqrt(self.n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nBilstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nbahdanau_attention=Bahdanau_Attention_1D(60)\n_,attention_weights_h=bahdanau_attention(Bilstm_cell)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Bahdanau.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model architecture of simple LSTM with Bahdanau Attention\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/a0q8CnD.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nlstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nluong_attention=Luong_Attention_1D(60)\n_,attention_weights_h=luong_attention(lstm_cell)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Luong.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=20,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\n\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nlstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nsdp_attention=Scaled_Dot_Product_Attention_1D(60)\n_,attention_weights_h=sdp_attention(lstm_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_SDPAttention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=20,verbose=2,validation_data=(val_x,val_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating a 1 Layer Encoder Decoder Transformer with Self Attention\n\n\n\n## Self Attention Mechanism\n\n\nSelf Attention is a mechanism for specifying the the attention on parts of a sentence to retain more semantic information. Self-attention is the method the Transformer uses to bake the ‚Äúunderstanding‚Äù of other relevant words into the one we‚Äôre currently processing. [Jay's Blog](http://jalammar.github.io/illustrated-transformer/) provide a very good idea of this logic.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\">\n\nThree vectors q,k and v (query,key and value) are taken into consideration for computation of the self attention mechanism.The q,k and v are normally of 64 dimensions.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_vectors.png\">\n\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we‚Äôre scoring. So if we‚Äôre processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_score.png\">\n\n\nThe third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper ‚Äì 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they‚Äôre all positive and add up to 1.\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention_softmax.png\">\n\n\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it‚Äôs useful to attend to another word that is relevant to the current word. The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example). The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-output.png\">\n\n\nThe computation process for Self Attention can be regarded as follows:\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Creating 1D self Attention Mechanism for LSTM networks\n\nBefore moving to Encoder Decoder Models, we can try applying a Self Attention mechanism on the standard network that we have created."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass Scaled_Dot_Product_Self_Attention_1D(tf.keras.layers.Layer):\n    #A class for Self Attention- 1 Dimension\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,n):\n        self.q=q\n        self.v=q\n        self.n=n\n        self.k=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nBilstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(Bilstm_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Self.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Glove 200 D and Self Attention on GRU\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Glove 200D)\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(gru_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Self_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using embedding matrices for semantic reference\n\nThe entire attention concepts can be applied with any embedding and in this case, we will be looking how to create transformer embeddings using minimal lines of code and use that along with the Attention mechanisms.\nWe have  already seen how to create Glove pretrained embeddings and using it; and as such any static pretrained embedding can be used for that purpose- be it - Fasttext/Paragram etc. Now we will be using Transformer embeddings for our use case. But before let us try to plot some word embeddings with the help of the state of the art Transformers.\n"},{"metadata":{},"cell_type":"markdown","source":"# Enter Transformers\n\nWe will be working with the [HuggingFace](https://huggingface.co/) repository as it contains all SOTA Transformer models. In this context, it is useful to mention some important resources:\n\n- Transformer Keras(https://keras.io/examples/nlp/text_classification_with_transformer/)\n- Kaggle Kernel(https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb)\n\nHowever in this case, since we would be using the models just for extracting embeddings or features, it is important to know the intermediate layers which should be chosen. Since Transformer architectures are really huge, (BERT/GPT variants), it is very complicated to fully understand which layer should be extracted for the features. While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.\n\n\n\n<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">\n\n\n## BERT Embeddings\n\n[BERT](https://arxiv.org/abs/1810.04805) is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n\nMsked Language Model (MLM)\nNext Sentence Prediction(NSP)\nThe bidirectional pre-training is essentially helpful to be used for any tasks. The Huggingface implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n\n\n<img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\">\n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/df2b0e26d0599ce3e70df8a9da02e51594e0e992/15-Figure4-1.png\">\n\n\n## Extracting Embeddings from BERT variant Transformers\n\nFor finetuning, it is to be kept in mind, there are many ways to do this. We are using BERT from Huggingface repository while it can also be used from [TF-HUB](https://tfhub.dev/s?module-type=text-embedding) or from [Google-Research](https://github.com/google-research/bert) repository. The reason for using HuggingFace is that the same codebase is applicable for all language models. The 3 most important input features that any language model asks for is:\n\n- input_ids\n- attention_masks\n- token_ids\n\nLet us first try to analyse and understand how BERT tokenizers, and model can be used in this context. The BERT documentation provides an outline of how to use BERT tokenizers and also modify it for downstream tasks.\n\nGenerally by virtue of transfer learning through weight transfer, we use pretrained BERT models from the list. This allows us to finetune it to extract only the embeddings. Since we are using Keras, we have to build up a small model containing an Input Layer and apply the tokenized(encoded) input ids, attention masks as input to the pretrained and loaded BERT model.This is very similar to creating a very own classification model for BERT using Keras/Tensorflow, but since we will be needing only the Embeddings it is safe to extract only the sentence vectors in the last layer of the model output. In most of the cases , we will see that the dimensions of the output vector is (x,768) where x depends on the number of tokenized input features. For this we extract the [CLS] tokenized feature from the ouput to just extract the sentence embeddings.\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\">\n\n\nSo the following pattern is to be done for our use case:\n\n- First Attain BERT embeddings by capturing the last hidden state output\n- Create the standard Neural network model (which we created till now)\n- In the Embedding Layer fit the BERT embeddings\n- Apply Self-Attention/any Attention mechanism on BERT embeddings\n- Apply FFNN Dense Networks with required activation functions\n\nby BERT , I mean all transformer models  under huggingface [pretrained library](https://huggingface.co/transformers/pretrained_models.html)\n\n\n\nSome resources and source codes:\n\n- [My NLP Kernels](https://kaggle.com/abhilash1910)\n- [Extensive Word Embeddings with Distilbert/Roberta/XL NET](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop#BERT-Embeddings-with-Alternate-Strategy)\n- [NLP workshop-2](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)"},{"metadata":{},"cell_type":"markdown","source":"## Generate any word pair similarity with Transformer (BERT variants)\n\nHere we will see how to create word pair similarity with very minimal lines of code using pretrained huge models like BERT,Distilbert,Roberta,XLNET,Camembert,BART,GPT etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate word by word embeddings with any BERT variant transformer models\n\nfrom transformers import BertTokenizer, TFBertModel\nfrom scipy.spatial.distance import cosine\n\ndef get_embeddings(model_name,tokenizer,name,inp):\n    #Specify which transformer model will be used\n    tokenizer = tokenizer.from_pretrained(name)\n    model = model_name.from_pretrained(name)\n    input_ids = tf.constant(tokenizer.encode(inp))[None, :]  # Batch size 1\n    outputs = model(input_ids)\n    #Take the last output\n    last_hidden_states = outputs[0]\n    cls_token=last_hidden_states[0]\n    return cls_token\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the BERT word embeddings\ncls_token1=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',input_word1)\ncls_token2=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DistilBERT Embeddings\n\n[This](https://huggingface.co/transformers/model_doc/distilbert.html) is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. DistilBERT Paper provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n\n\n<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\nThe overall workflow is similar to BERT extracting BERT word embeddings\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#DistilBERT word Embeddings\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the DistilBERT word embeddings\ncls_token1=get_embeddings(transformers.TFDistilBertModel,BertTokenizer,'distilbert-base-uncased',input_word1)\ncls_token2=get_embeddings(transformers.TFDistilBertModel,BertTokenizer,'distilbert-base-uncased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XL NET Embeddings\n\n[This paper](https://arxiv.org/abs/1906.08237) provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n\n- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n- It is a permutation language model and a pictorial representation can be :\n\n<img src=\"https://zdnet2.cbsistatic.com/hub/i/r/2019/06/21/2a4e6548-9dee-491d-b638-8cfae9bbb2fe/resize/1200x900/ab279544c2631111754a357ada50ef29/google-xlnet-architecture-2019.png\">\n\n\n\nHere we will be using an alternate strategy for building the word embeddings\nWe will be using the Feature Extraction Pipeline from Huggingface- just to show that there are more than one ways of retrieving the embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"#XLNET word Embeddings\n#Using the Feature Extraction pipeline from Huggingface\n\nfrom transformers import AutoTokenizer, pipeline\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(model_name,name,inp):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the XLNET word embeddings\ncls_token1=transformer_embedding(transformers.TFXLNetModel,'xlnet-base-cased',input_word1)\ncls_token2=transformer_embedding(transformers.TFXLNetModel,'xlnet-base-cased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Generative Transformers - GPT2\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png\">\n\n\nIt is a [robust model](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n\nGPT-2 is a model with absolute position embeddings so it‚Äôs usually advised to pad the inputs on the right rather than the left.\n\nGPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nGPT-2 is a decoder Transformer model. Generally most transformers are encoder-decoders, but in the case of GPT-2 this is a decoder-only model. GPT-2 has stacks of decoder cells on top of one another, and inside each decoder block resides -Masked Self Attention and FFNN (Dense) Networks.\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-1-2.png\">\n\n\n\n## Masked Self Attention\n\n\nThis is the core part which separated GPT from BERT variants:\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\">\n\n\nMore details can be found here:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [NLP](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n- [Jay's blog](http://jalammar.github.io/illustrated-gpt2/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For GPT variants, it is important to know that these use 'PAD' tokens additionally and are used from left to right.\n#Unlike BERT variants which are bidirectional in nature, GPT is more of a left to right tokenizer due to the Masking of Attention.\n\ndef transformer_gpt_embedding(model_name,name,inp):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    tokenizer.pad_token = \"[PAD]\"\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the GPT-2 word embeddings\ncls_token1=transformer_gpt_embedding(transformers.TFGPT2Model,'openai-gpt',input_word1)\ncls_token2=transformer_gpt_embedding(transformers.TFGPT2Model,'openai-gpt',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1,cls_token2)\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1)\nplt.plot(cls_token2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take any word pair from the provided question body for measuring distance similarity.\ntrain_df['question_body'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Any Transformer Model can be used for our Classification Use case\n\n\nIn this case, we will be creating a function to create an embedding matrix (instead of word vectors) , very much like Glove. Important point is to remember that, this code is compatible with any BERT variant transformers and also the embedding dimensions should be compatible with the Transformer size mentioned in the [pretrained section](https://huggingface.co/transformers/pretrained_models.html)\n\nThe steps are straightforward, and it is as follows:\n\n- Batch tokenize the input features\n- Once these have been tokenized (with Transformer tokenizers) , we will be applying certain masks\n- Padding the tokenized text \n- Then we have to apply an attention mask\n- The attention mask signifies that we have to segregate the input features in 0s and 1s.\n- Extract the last hidden outputs \n\nUsing these embeddings, we can plug them into our standard neural network architecture (along with Attention)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\ndistilbert_embeddings = fetch_vectors(train_df.question_body.values,'distilbert-base-uncased')\nprint(distilbert_embeddings.shape)\nplt.plot(distilbert_embeddings[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Apply the Older GRU with Self Attention Model on DistilBert Embeddings\n\nNow, we can apply the GRU model which we created on the DistilBert Embeddings and evaluate the classification performance.Important point to note, that here the dimension is 768, and the 6079 includes the number of features (max_features). We have to replace those variables accordingly.\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-model-input-output-1.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Distilbert and Self Attention on GRU\nmaxlen=1000\n#Obsserve the max_features which is -> distilbert_embeddings.shape[0]\nmax_features=6079\n#Observe the embed_size->distilbert_embeddings.shape[1]\nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Distilbert)\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[distilbert_embeddings])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(gru_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Self_attention_Distilbert.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voila, we have added 2 major parts in our code- Transformer embeddings & Self Attention\n\n\nNow let us see how the performance is when we use just the Distilbert Embeddings for our use case, and no self attention /attention mechanism. We will see a steeper rise in the accuracy, this is because distilbert already has 12 transformer blocks with multihead self attention being plugged into it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Distilbert and Self Attention on GRU\nmaxlen=1000\n#Obsserve the max_features which is -> distilbert_embeddings.shape[0]\nmax_features=6079\n#Observe the embed_size->distilbert_embeddings.shape[1]\nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Distilbert)\n#4.Attention- Applying 1D attention-Self Attention(Not applicable here)\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[distilbert_embeddings])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nz=Dense(16,activation='relu')(gru_cell)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Distilbert.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting a Simple Traditional Classifier on DistilBERT\n\nNow we will try to fit a simple Logistic Regression function on the Distilbert Embeddings and evaluate the performance\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification-example.png\">\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=train_y\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n#Building a simple logistic regression classifier\nmodel=LogisticRegression()\nmodel.fit(train_x,train_y)\npred=model.predict(val_x)\nprint(\"Evaluate confusion matrix for LR\")\nprint(confusion_matrix(val_y,pred))\nprint(f\"Accuracy Score for LR with C=1.0  ={accuracy_score(val_y,pred)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Why the poor performance?\n\nThis is because the problem requires a multi-class classification rather than a sigmoid binary classification. We need to try another model ,let's say XGBoost?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoosting\n\nfrom xgboost import XGBClassifier as xg\nmodel_xgb= xg(n_estimators=100,random_state=42)\nmodel_xgb.fit(train_x,train_y)\ny_pred_xgb=model_xgb.predict(val_x)\nprint('Accuracy Score',accuracy_score(val_y,y_pred_xgb.round()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a 1 block Transformer\n\nNow, we will be using the same concept of distilbert embeddings and create a simple encoder-decoder with Attention block which is also known as a 1 block Transformer. [Encoder-Decoder](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) is a classic architecture mostly popular for sequence2sequence learning. Encoder-Decoders are most popularly used for neural machine translation (seq2seq learning with attention). The general workflow revolves around stacks of RNNs (LSTMs/GRUs/TimeDistributed Cells) which behaves as an encoder takes as input 3 parameters (max_features,embed_size,maxlen in our example) and returns an output. We then save the 2 output LSTM cell states ,the h and c states. We design the decoder model in a similar manner (if the internal layers are modified it becomes a hybrid decoder). And while passing the inputs of the decoder, we also pass the 2 output LSTM cell states from the encoder output (namely the h and c states). The output of the decoder is then passed through a activation/distribution function to optimize our target loss function.\n\n\n<img src=\"https://miro.medium.com/max/1250/1*LYGO4IxqUYftFdAccg5fVQ.png\">\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final 1 Block Transformer which consists of 1 encoder decoder with distilbert embedding\nimport math\nmaxlen=500\nembed_size=768\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n\n\ndef distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings):\n    #Creating LSTM  encoder neural model with distilbert pretrained embeddings\n    #Encoder Block\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    #Decoder Block\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(5,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\n\n#Use the method for creating distilbert embeddings\n\ndistilbert_embeddings = fetch_vectors(train_df.question_body.values,'distilbert-base-uncased')\nmax_features=distilbert_embeddings.shape[0]\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\nmodel=distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings)  \nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"distilbert_encoder_decoder_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=10,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture of 1 block Transformer with Distilbert Embeddings\n\nThe model architecture can be shown as below:\n\n<img src=\"https://i.imgur.com/m9tgoG6.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Testing With Albert Embeddings\n\n[Albert](https://arxiv.org/abs/1909.11942) is a lightweight bert which introduces parameter sharing, caching, and intermediate repeated splitting of the embedding matrix for efficient modelling tasks.\n\nAccording to the paper:\n\n'The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.'\n\n\n<img src=\"data:image/jpeg;base64,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\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final 1 Block Transformer which consists of 1 encoder decoder with albert embedding\nimport math\nmaxlen=500\nembed_size=768\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFAlbertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n\n\ndef albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings):\n    #Creating LSTM  encoder neural model with albert pretrained embeddings\n    #Encoder Block\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    #Decoder Block\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(5,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\n\n#Use the method for creating albert embeddings\n\nalbert_embeddings = fetch_vectors(train_df.question_body.values,'albert-base-v1')\nmax_features=albert_embeddings.shape[0]\nmodel=albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings)  \nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"albert_encoder_decoder_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=train_y\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=10,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture on Albert\n\n\nThe model architecture can be shown as follows:\n\n\n<img src=\"https://i.imgur.com/ELj9zmF.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Modifying the Codebase to Create a 2 Block Transformer\n\nWe will now be modifying the codebase and create a 2 block transformer architecture. Traditionally  a classical transformer is made of 8 blocks, and BERT variants have 12/24 such blocks. The most important aspect of these blocks is that only intermediate hidden cell states (h) get communicated . So between each encoder block we can keep an attention block to modify the internal h states of the LSTM stacks . Similarly between each decoder we can create a transmission of the hidden h states of the decoder cells.\n\nThis looks like the image shown below:\n\n<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFAlbertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n\n\n\ndef albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings):\n    #Creating LSTM  encoder neural model with distilbert pretrained embeddings\n    #Encoder Block-I\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Encoder Block -II\n#     encoder_inp_2=Input(shape=(maxlen,))\n#     encoder_embed_2=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(encoder_inp_2)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    encoder_embed_attention_h,encoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    encoder_embed_attention_c,encoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n    encoder_lstm_cell_2=LSTM(60,return_sequences='True',return_state=True)\n    encoder_outputs_2,encoder_state_lstm_h,encoder_state_lstm_c=encoder_lstm_cell_2(encoder_embed,initial_state=[encoder_embed_wghts_h,encoder_embed_wghts_c])\n    print(f'Second Encoder Ouputs Shape{encoder_outputs.shape}')\n    \n    \n    #Decoder Block - I\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs_2,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs_2,64)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n    \n    #Decoder Block - II\n#     decoder_inp=Input(shape=(maxlen,))\n#     decoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(decoder_state_lstm_h,decoder_state_lstm_h,decoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(decoder_state_lstm_c,decoder_state_lstm_c,decoder_outputs,64)\n    decoder_lstm_cell_2=LSTM(60,return_state=True)\n    decoder_outputs_2,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell_2(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n    \n    #Final FFNN - Dense Layer\n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs_2)\n    decoder_dense_cell2=Dense(5,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\nmaxlen=500\nembed_size=768\n#Use the method for creating  embeddings\nalbert_embeddings = fetch_vectors(train_df.question_body.values,'albert-base-v1')\n\nmodel=albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings)  \nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"albert_encoder_decoder_attention_II.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=10,verbose=2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture of 2 Block Transformer with Albert Embeddings\n\n\nThe model architecture can be defined as follows:\n\n\n<img src=\"https://i.imgur.com/6Jn2xz7.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of Transformers \n\nIn this case, we saw and built a 2Block transformer with intermediate Albert Embeddings (quite complex) along with Self Attention multiple times (Multi head self attention). And there are many other ways to build a classifier, and [this Kernel](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india) provides a good overview of creating another high performant Classifier using TPU Cluster (Google Cloud Storage) in Kaggle.With the help of Huggingface we can create any language model for any NLP tasks. Now we will be looking into a classic example of creating answers from a given context with a QA Transformer model."},{"metadata":{},"cell_type":"markdown","source":"# Question Answering with Transformers Pipeline\n\n\n\n<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\">\n\n\n[HuggingFace Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are extensively used in any language modelling tasks from NER, to QA, MNLI  or even Summarisation. The pipeline is built to be robust and very efficient to handle the different downstream tasks from a transformer model. Once the embeddings are received from a Transformer model, the downstream tasks can be solved very simply by taking those tokenized embeddings and solving it for our downstream tasks. Particularly in the context of question answering this is done by using some segment tokens (tagging questions with tag 1 and answers with tag 0). Here we will be looking how to build a simple pipeline to extract answers from a given context when questions are provided.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"abhilash1910/distilbert-squadv1\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"abhilash1910/distilbert-squadv1\")\ncontext=train_df['answer'][2]\nprint('The context:',context)\nquestion='What is the thickness of the boards?'\nqa_pipeline=pipeline('question-answering',model=model,tokenizer=tokenizer)\nqa_inputs={\n    'question':question,\n    'context':context\n}\nfinal_container=qa_pipeline(qa_inputs)\nprint('The final answer: ',final_container['answer'])\nprint('The corresponding score: ',final_container['score'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A Detailed Example From Huggingface\n\nThis provides a detailed overview of using [Huggingface Library](https://huggingface.co/transformers/usage.html) for creating a QA model. Here the following steps are to be followed:\n\n\n- Tokenize the context and questions.Special Tokens can be added as well\n- After the tokenization is completed, we have to convert the encoded tokens to ids\n- Pass these ids (segment ids) into the Transformer model to extract the last embedding output\n- Collect the start and end scores from the model output\n- Understand the corresponding ids which are present in those ranges\n- Convert those ids back into tokens (words)\n- These converted tokens gives the answer\n\nSome important points:\n\n- We only consider ourselves with the segment tokens. In the case of BERT, there are 3 such tokens- inputs, position and segments \n- Masking of tokens is optional and can be used\n- For QA models BERT variants perform really well as opposed to GPT variants."},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\ntext = r\"\"\"\nü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\narchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural\nLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\nTensorFlow 2.0 and PyTorch.\n\"\"\"\n\nquestions = [\n    \"How many pretrained models are available in Transformers?\",\n    \"What does Transformers provide?\",\n    \"Transformers provides interoperability between which frameworks?\",\n]\n\nfor question in questions:\n    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"tf\")\n    input_ids = inputs[\"input_ids\"].numpy()[0]\n\n    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer_start_scores, answer_end_scores = model(inputs)\n\n    answer_start = tf.argmax(\n        answer_start_scores, axis=1\n    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = (\n        tf.argmax(answer_end_scores, axis=1) + 1\n    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating an NER pipeline with Transformers\n\nUsing the huggingface pipeline we can create finetune our models for downstream tasks as well. We use the same code example as in the question answer model (using pipeline).There are many ways to do this:\n\n- We can use the pipeline ('ner') as it is\n- We can also use any models , and for that we can specify model name and tokenizer name from huggingface (same as in the QA example)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Without using any specific model as such\nner_pipeline=pipeline('ner')\ncontext=train_df['answer'][100]\nprint('Context',context)\nprint('NER tagging of the sample')\nprint(ner_pipeline(context))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of Session\n\nWe have come to the end of the session, abd we have explored many things from the basics of neural networks to creating an advanced architecture . Also we have explored the different world of embeddings,created our SOTA Transformer models with embeddings. We also saw how to create a 2 block Transformer, and saw the different downstream tasks like QA modelling and NER where Transformers can be used so easily. There are lots of concepts in Transformers which have not yet been covered but this provides a gentle introduction how to scale a simple neural network to a Transformer.\n\n\n<img src=\"https://media.tenor.com/images/cfbe42db018b64c9806a6b4ae89f3f2c/tenor.gif\">"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}