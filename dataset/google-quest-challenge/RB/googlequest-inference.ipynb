{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#This is cleaned up copy of my LB score. \n\n#Thanks to following public Kernels:\n#https://www.kaggle.com/adityaecdrid/pytorch-bert-end-to-end-with-cv\n#https://www.kaggle.com/phoenix9032/pytorch-bert-plain\n#https://www.kaggle.com/artgor/pytorch-approach . ##crawl USE \n#https://www.kaggle.com/manyregression/fastai-no-spacy-transformers-here  ##Post processing\n\n#And this post\n#https://www.kaggle.com/c/google-quest-challenge/discussion/123770","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom os.path import join as path_join\n\ndata_dir = '../input/'\n\nNETLOC_PATH = '../input/bert-use-netloc/unique_netloc.pickle'\n\nUSE_URL = '../input/universalsentenceencoder4/'\nTRAIN_CSV_PATH = path_join(data_dir,'google-quest-challenge', 'train.csv')\nTEST_CSV_PATH = path_join(data_dir, 'google-quest-challenge','test.csv')\nPOST_PROCESS = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport os\nimport transformers, sys, os, gc\nimport numpy as np, pandas as pd, math\nimport torch, random, os, multiprocessing, glob\nimport torch.nn.functional as F\nimport torch, time\n\nfrom scipy.stats import spearmanr\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n    get_cosine_schedule_with_warmup, BertPreTrainedModel,\n)\nfrom os.path import join as path_join\n\nfrom tqdm import tqdm\nprint(transformers.__version__)\n\n\n#wiki/crawl\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport pickle\n\n\ndef set_seeds(SEED=42):\n  os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n  np.random.seed(SEED)\n  torch.manual_seed(SEED)\n  torch.cuda.manual_seed(SEED)\n  torch.backends.cudnn.deterministic = True\n\nSEED=2019\nset_seeds(SEED)\nHEAD_TAIL = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Crawl Embedding matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# added preprocessing from https://www.kaggle.com/wowfattie/3rd-place/data\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\n', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\", \"aren  '  t\" : \"are not\",\n\"can't\" : \"cannot\", \"can  '  t\" : \"cannot\",\n\"couldn't\" : \"could not\",\"couldn  '  t\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\"didn  '  t\" : \"did not\",\n\"doesn't\" : \"does not\",\"doesn  '  t\" : \"does not\",\n\"doesnt\" : \"does not\", \n\"don't\" : \"do not\",\"don  '  t\" : \"do not\",\n\"hadn't\" : \"had not\",\"hadn  '  t\" : \"had not\",\n\"hasn't\" : \"has not\", \"hasn  '  t\" : \"has not\",\n\"haven't\" : \"have not\", \"haven  '  t\" : \"have not\",\n\"havent\" : \"have not\",  \n\"he'd\" : \"he would\", \"he  '  d\" : \"he would\",\n\"he'll\" : \"he will\",\"he  '  ll\" : \"he will\",\n\"he's\" : \"he is\",\"he  '  s\" : \"he is\",\n\"i'd\" : \"I would\",\"i  '  d\" : \"I would\",\n\"i'd\" : \"I had\",\"i  '  d\" : \"I had\",\n\"i'll\" : \"I will\",  \"i  '  ll\" : \"I will\",\n\"i'm\" : \"I am\",  \"i  '  m\" : \"I am\",\n\"isn't\" : \"is not\", \"isn  '  t\" : \"is not\",\n\"it's\" : \"it is\",  \"it  '  s\" : \"it is\",\n\"it'll\":\"it will\",   \"it  '  ll\":\"it will\",\n\"i've\" : \"I have\",   \"i  '  ve\" : \"I have\",\n\"let's\" : \"let us\",  \"let  '  s\" : \"let us\",\n\"mightn't\" : \"might not\",  \"mightn  '  t\" : \"might not\",\n\"mustn't\" : \"must not\",\"mustn  '  t\" : \"must not\",\n\"shan't\" : \"shall not\",\"shan  '  t\" : \"shall not\",\n\"she'd\" : \"she would\",\"she  '  d\" : \"she would\",\n\"she'll\" : \"she will\",\"she  '  ll\" : \"she will\",\n\"she's\" : \"she is\",\"she  '  s\" : \"she is\",\n\"shouldn't\" : \"should not\",\"shouldn  '  t\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\"that  '  s\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\"there  '  s\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\"they  '  d\" : \"they would\",\n\"they'll\" : \"they will\",\"they  '  ll\" : \"they will\",\n\"they're\" : \"they are\",\"they  '  re\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\"they  '  ve\" : \"they have\",\n\"we'd\" : \"we would\",\"we  '  d\" : \"we would\",\n\"we're\" : \"we are\",\"we  '  re\" : \"we are\",\n\"weren't\" : \"were not\",\"weren  '  t\" : \"were not\",\n\"we've\" : \"we have\",\"we  '  ve\" : \"we have\",\n\"what'll\" : \"what will\",\"what  '  ll\" : \"what will\",\n\"what're\" : \"what are\",\"what  '  re\" : \"what are\",\n\"what's\" : \"what is\",\"what  '  s\" : \"what is\",\n\"what've\" : \"what have\",\"what  '  ve\" : \"what have\",\n\"where's\" : \"where is\",\"where  '  s\" : \"where is\",\n\"who'd\" : \"who would\",\"who  '  d\" : \"who would\",\n\"who'll\" : \"who will\",\"who  '  ll\" : \"who will\",\n\"who're\" : \"who are\",\"who  '  re\" : \"who are\",\n\"who's\" : \"who is\",\"who  '  s\" : \"who is\",\n\"who've\" : \"who have\",\"who  '  ve\" : \"who have\",\n\"won't\" : \"will not\",\"won  '  t\" : \"will not\",\n\"wouldn't\" : \"would not\",\"wouldn  '  t\" : \"would not\",\n\"you'd\" : \"you would\",\"you  '  d\" : \"you would\",\n\n\"you'll\" : \"you will\",\"you  '  ll\" : \"you will\",\n\"you're\" : \"you are\",\"you  '  re\" : \"you are\",\n\"you've\" : \"you have\",\"you  '  ve\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\"wasn  '  t\": \"was not\",\n\"we'll\":\" will\",\"we  '  ll\":\" will\",\n\"didn't\": \"did not\",\"didn  '  t\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df\n\n\n\ndef get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\n\n\ndef build_matrix(embedding_path: str = '',\n                 embedding_path_spellcheck: str = path_join(data_dir, 'wiki-news-300d-1M.vec'),\n                 word_dict: dict = None, max_features: int = 100000,\n                 embed_size: int= 300, ):\n\n    # embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding='utf-8'))\n    embedding_index = load_embeddings(embedding_path)\n\n    nb_words = min(max_features, len(word_dict))\n    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n    unknown_words = []\n    for word, i in word_dict.items():\n        key = word\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.lower())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.upper())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        embedding_vector = embedding_index.get(word.capitalize())\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            continue\n        unknown_words.append(key)\n\n    print(f'{len(unknown_words) * 100 / len(word_dict):.4f}% words are not in embeddings')\n    return embedding_matrix, nb_words, unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(TEST_CSV_PATH).fillna(' ')\ntrain = pd.read_csv(TRAIN_CSV_PATH).fillna(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])\n\ntokenizer = Tokenizer()\nfull_text = list(train['question_body']) + \\\n                       list(train['answer']) + \\\n                       list(train['question_title']) + \\\n                       list(test['question_body']) + \\\n                       list(test['answer']) + \\\n                       list(test['question_title'])\ntokenizer.fit_on_texts(full_text)\n\nembed_size=300\nembedding_path = \"/kaggle/input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl\" #path_join(data_dir,'crawl-300d-2M.pkl')\n\nembedding_matrix, nb_words, unknown_words = build_matrix(embedding_path,  path_join(data_dir,'wiki-news-300d-1M.vec') , tokenizer.word_index,\n                                              100000, embed_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## USE Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nimport re\n\nseed(SEED)\nrandom.seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_CSV_PATH)\ntest = pd.read_csv(TEST_CSV_PATH)\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#If test has urls other than these 59 = we have a problem \n\nfind = re.compile(r\"^[^.]*\")\n\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\nwith open(NETLOC_PATH, \"rb\") as input_file:\n     unique_netloc = pickle.load(input_file)\n        \ndiff1  = list(set(test.netloc.unique()).difference(set(unique_netloc))) #netlocs that are in test set but not in pickled netlocs.\n#replace these with unkowns\n\n\nunknownlist = ['unknown1','unknown2','unknown3', 'unknown4', 'unknown5', 'unknown6', 'unknown7', 'unknown8', 'unknown9',\n              'unknown10', 'unknown11', 'unknown12', 'unknown13', 'unknown14', 'unknown15',  'unknown16', 'unknown17', 'unknown18', \n             'unknown19', 'unknown20', 'unknown21',  'unknown22', 'unknown23', 'unknown24',  'unknown25', 'unknown26', 'unknown27',\n              'unknown28', 'unknown29', 'unknown30', 'unknown31', 'unknown32', 'unknown33',  'unknown34', 'unknown35', 'unknown36', \n              'unknown37', 'unknown38','unknown39',  'unknown40']\n\nif len(diff1) > 50: \n    exit(1)\n\nfor i,k in enumerate(diff1): \n    idx = test[test.netloc==k].index\n    test.iloc[idx,11] = unknownlist[i]\n    \n    \ntmp = pd.DataFrame([['unknown1', 'LIFE_ARTS'],['unknown2', 'CULTURE'],['unknown3', 'SCIENCE'],\n             ['unknown4', 'STACKOVERFLOW'],['unknown5', 'TECHNOLOGY'],['unknown6', 'LIFE_ARTS'],\n              ['unknown7', 'CULTURE'],['unknown8', 'SCIENCE'],['unknown9', 'STACKOVERFLOW'],\n              ['unknown10', 'TECHNOLOGY'],['unknown11', 'CULTURE'],['unknown12', 'SCIENCE'],\n              ['unknown13', 'STACKOVERFLOW'],['unknown14', 'TECHNOLOGY'],['unknown15', 'LIFE_ARTS'],\n              ['unknown16', 'LIFE_ARTS'],['unknown17', 'CULTURE'],['unknown18', 'SCIENCE'],\n             ['unknown19', 'STACKOVERFLOW'],['unknown20', 'TECHNOLOGY'],['unknown21', 'LIFE_ARTS'],\n              ['unknown22', 'CULTURE'],['unknown23', 'SCIENCE'],['unknown24', 'STACKOVERFLOW'],\n              ['unknown25', 'TECHNOLOGY'],['unknown26', 'CULTURE'],['unknown27', 'SCIENCE'],\n              ['unknown28', 'STACKOVERFLOW'],['unknown29', 'TECHNOLOGY'],['unknown30', 'LIFE_ARTS'] ,\n                    ['unknown31', 'LIFE_ARTS'],['unknown32', 'CULTURE'],['unknown33', 'SCIENCE'],\n             ['unknown34', 'STACKOVERFLOW'],['unknown35', 'TECHNOLOGY'],['unknown36', 'LIFE_ARTS'],\n              ['unknown37', 'CULTURE'],['unknown38', 'SCIENCE'],['unknown39', 'STACKOVERFLOW'],\n              ['unknown40', 'TECHNOLOGY']\n             ],columns =  ['netloc', 'category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['netloc', 'category']\nmerged = pd.concat([train[features], test[features]])\nmerged = merged.append(tmp[-(40 - len(diff1)):])\n\nohe = OneHotEncoder()\nohe.fit(merged)\n\nfeatures_train = ohe.transform(train[features]).toarray()\nfeatures_test = ohe.transform(test[features]).toarray()\nprint(features_train.shape, features_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = USE_URL #path_join(data_dir,'use/')\nembed = hub.load(module_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninput_columns = ['question_title', 'question_body', 'answer']\n\nembeddings_train = {}\nembeddings_test = {}\nfor text in input_columns:\n    print(text)\n    #train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    #curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    #while ind*batch_size < len(train_text):\n    #    curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n    #    ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    #embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \ndel embed\ngc.collect()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#512 columns for each row, each column i.e. Row 1 -> question_title -> 512 features\n#len(embeddings_train),\nlen(embeddings_test), embeddings_test['question_title_embedding'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numba import cuda\ncuda.select_device(0)\ncuda.close() \n\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distance features"},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\n# dist_features_train = np.array([\n#     l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n#     l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n#     l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n#     cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n#     cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n#     cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n# ]).T\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T\n\n#X_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])\n#y_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del embeddings_test, features_test, dist_features_test\ngc.collect()\ntorch.cuda.empty_cache()\n\n#X_train.shape, \nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 500\nmax_len_title = 30\ntrain_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_body']), maxlen = max_len)\ntrain_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['answer']), maxlen = max_len)\ntrain_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(train['question_title']), maxlen = max_len_title)\ntest_question_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_body']), maxlen = max_len)\ntest_answer_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['answer']), maxlen = max_len)\ntest_title_tokenized = pad_sequences(tokenizer.texts_to_sequences(test['question_title']), maxlen = max_len_title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bert Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change the model to include only question+title embedding and answer+title embedding\nLEN_USE_FEATURES = 1646 #1606 \nclass BertForSequenceClassification_v2Answer(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    \"\"\"\n\n    def __init__(self, config,embedding_matrix=None):\n        \n        super(BertForSequenceClassification_v2Answer,self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.embedding = nn.Embedding(*embedding_matrix.shape) \n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n\n        self.linear_q_add = nn.Linear(300, 128)\n        self.linear_q_add1 = nn.Linear(128, self.config.num_labels)\n       \n        # MINE - works\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.linear1 = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  ,config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels)\n        self.classifier = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  , self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        usefeatures = None, \n        question_data = None,\n        answer_data = None,\n        title_data = None\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n \n        last_hidden_state = outputs[0][:,0,:].reshape((-1, 1, 768)) #CLS Token \n        pooled_output = outputs[1] \n        hidden_states = outputs[2]\n\n        # MINE - works\n        h12 = hidden_states[-1][:, 0].reshape((-1, 1, 768))\n        h11 = hidden_states[-2][:, 0].reshape((-1, 1, 768))\n        h10 = hidden_states[-3][:, 0].reshape((-1, 1, 768))\n        h9  = hidden_states[-4][:, 0].reshape((-1, 1, 768))\n        all_h = torch.cat([ h9, h10, h11, h12], 1)\n        all_h_mean = torch.mean(all_h, 1).reshape((-1,1,768))\n        mean_pool = torch.mean(torch.cat([last_hidden_state,all_h_mean],1),1)\n        pooled_output = self.dropout(mean_pool)\n\n\n        #Embedding \n        h_embedding_q = self.embedding(question_data)\n        h_embedding_q = self.embedding_dropout(h_embedding_q)\n\n        h_embedding_a = self.embedding(answer_data)\n        h_embedding_a = self.embedding_dropout(h_embedding_a)\n\n        h_embedding_t = self.embedding(title_data)\n        h_embedding_t = self.embedding_dropout(h_embedding_t)\n\n        add = torch.cat((h_embedding_t, h_embedding_q, h_embedding_a), 1) #h_embedding_a,\n        add = self.linear_q_add(torch.mean(add, 1))\n        add = self.linear_q_add1(add)\n\n        x = torch.nn.ELU()(self.linear1(torch.cat([pooled_output,usefeatures,add],1)))\n        logits = self.classifier(x)\n\n        #logits = self.classifier(pooled_output)\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Change the model to include only question+title embedding and answer+title embedding\nLEN_USE_FEATURES = 1646 #1606 \nclass BertForSequenceClassification_v2Question(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    \"\"\"\n\n    def __init__(self, config,embedding_matrix=None):\n        \n        #super(BertForSequenceClassification, self).__init__(config)\n        super(BertForSequenceClassification_v2Question,self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.embedding = nn.Embedding(*embedding_matrix.shape) \n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n\n        self.linear_q_add = nn.Linear(300, 128)\n          \n        self.linear_q_add1 = nn.Linear(128, self.config.num_labels)\n\n        \n        # MINE - works\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.linear1 = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  ,config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels)\n        self.classifier = nn.Linear(config.hidden_size+LEN_USE_FEATURES +  self.config.num_labels  , self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        usefeatures = None, \n        question_data = None,\n        answer_data = None,\n        title_data = None\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n     \n        last_hidden_state = outputs[0][:,0,:].reshape((-1, 1, 768)) #CLS Token \n        pooled_output = outputs[1] \n        hidden_states = outputs[2]\n\n        # MINE - works\n        h12 = hidden_states[-1][:, 0].reshape((-1, 1, 768))\n        h11 = hidden_states[-2][:, 0].reshape((-1, 1, 768))\n        h10 = hidden_states[-3][:, 0].reshape((-1, 1, 768))\n        h9  = hidden_states[-4][:, 0].reshape((-1, 1, 768))\n        all_h = torch.cat([ h9, h10, h11, h12], 1)\n        all_h_mean = torch.mean(all_h, 1).reshape((-1,1,768))\n        mean_pool = torch.mean(torch.cat([last_hidden_state,all_h_mean],1),1)\n        pooled_output = self.dropout(mean_pool)\n\n        #Embedding \n        h_embedding_q = self.embedding(question_data)\n        h_embedding_q = self.embedding_dropout(h_embedding_q)\n\n        h_embedding_t = self.embedding(title_data)\n        h_embedding_t = self.embedding_dropout(h_embedding_t)\n\n        add = torch.cat((h_embedding_t,  h_embedding_q), 1) #h_embedding_a,\n        add = self.linear_q_add(torch.mean(add,1))\n        add = self.linear_q_add1(add)\n\n        x = torch.nn.ELU()(self.linear1(torch.cat([pooled_output,usefeatures,add],1)))\n        logits = self.classifier(x)\n        \n        #logits = self.classifier(pooled_output)\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\ntrain = pd.read_csv(TRAIN_CSV_PATH) # path_join(data_dir,\"train.csv\"))\ntest = pd.read_csv(TEST_CSV_PATH) #path_join(data_dir, \"test.csv\"))\n\ntarget_cols_questions = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice',\n               'question_type_compare', 'question_type_consequence',\n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written'] \n\ntarget_cols_answers =  ['answer_helpful',\n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helpers\n# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize Answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _trim_input_answers(title, question, answer, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n    \n    #293+239+30 = 508 + 4 = 512\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n\n\n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        \n        q_len_head = round(q_new_len/2)\n        q_len_tail = -1* (q_new_len -q_len_head)\n        a_len_head = round(a_new_len/2)\n        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n        t = t[:t_new_len]\n        if HEAD_TAIL :\n            q = q[:q_len_head]+q[q_len_tail:]\n            a = a[:a_len_head]+a[a_len_tail:]\n        else:\n            q = q[:q_new_len]\n            a = a[:a_new_len] ## No Head+Tail ,usual processing\n    \n    return t, q, a\n\n\n\ndef _convert_to_bert_inputs_answers(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"] \n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\n\n\ndef compute_input_arays_answers(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows(),position=0, leave=True ):\n        t, q, a  = instance.question_title,instance.question_body, instance.answer\n        t, q, a  = _trim_input_answers(t, q, a, tokenizer, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs_answers(t,q, a,tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize Questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _trim_input(title, question, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=479):\n    \n    #3 + 479 + 30\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    \n    t_len = len(t)\n    q_len = len(q)\n    \n    if (t_len+q_len+3) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            q_max_len = q_max_len + (t_max_len - t_len)\n            q_new_len = q_max_len\n        else:\n            t_new_len = t_max_len\n            q_new_len = q_max_len\n      \n        if q_max_len > q_len:\n            q_new_len = q_len\n            \n            \n        if t_new_len+q_new_len+3 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + q_new_len + 3)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        \n    return t, q\n\ndef _convert_to_bert_inputs(title, question, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] \n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows(),position=0, leave=True ):\n        t, q  = instance.question_title, instance.question_body\n        t, q  = _trim_input(t, q, tokenizer, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t, q,tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, usefeatures,question_data, answer_data, title_data, lengths, labels = None):\n        \n        self.inputs = inputs\n        self.usefeatures = usefeatures\n        self.question_data = question_data\n        self.answer_data = answer_data\n        self.title_data = title_data\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        self.lengths = lengths\n       \n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        lengths         = self.lengths[idx]\n        usef            = self.usefeatures[idx,:]\n        question_data   = self.question_data[idx,:]\n        answer_data     = self.answer_data[idx,:]\n        title_data      = self.title_data[idx,:]\n\n        if self.labels is not None: # targets\n            labels = self.labels[idx]\n            return input_ids, input_masks, input_segments,  usef,  question_data, answer_data, title_data, labels, lengths\n        return input_ids, input_masks, input_segments, usef,  question_data,answer_data, title_data,  lengths\n\n    def __len__(self):\n        return len(self.inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_result_question(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), 21))\n    \n    model.eval();\n    tk0 = tqdm(enumerate(test_loader),position=0, leave=True )\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                            usefeatures = x_batch[3].to(device),\n                            question_data = x_batch[4].to(device),\n                            answer_data = x_batch[5].to(device),\n                            title_data = x_batch[6].to(device)\n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output\n\n\ndef predict_result_answer(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), 9))\n    \n    model.eval();\n    tk0 = tqdm(enumerate(test_loader),position=0, leave=True )\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                            usefeatures = x_batch[3].to(device),\n                            question_data = x_batch[4].to(device),\n                            answer_data = x_batch[5].to(device),\n                            title_data = x_batch[6].to(device)\n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seeds(SEED)\n\nquestion_tokenizer = BertTokenizer.from_pretrained(\"../input/bert-vocab1/vocab.txt\")\ninput_categories_q = list(train.columns[[1,2]]); \n\nbert_model_config_question = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\nbert_config_q = BertConfig.from_json_file(bert_model_config_question)\nbert_config_q.output_hidden_states = True\nbert_config_q.num_labels = 21\n\nanswer_tokenizer = BertTokenizer.from_pretrained(\"../input/bert-vocab1/vocab.txt\")\ninput_categories_a = list(train.columns[[1,2,5]]); \n\nbert_model_config_answer = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\nbert_config_a = BertConfig.from_json_file(bert_model_config_answer)\nbert_config_a.output_hidden_states = True\nbert_config_a.num_labels = 9\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ntest_inputs_q = compute_input_arays(df =  test, columns = input_categories_q, tokenizer = question_tokenizer, max_sequence_length=512)\nlengths_test_q = np.argmax(test_inputs_q[0] == 0, axis=1)\nlengths_test_q[lengths_test_q == 0] = test_inputs_q[0].shape[1]\n\n\ntest_inputs_a = compute_input_arays_answers(df =  test, columns = input_categories_a, tokenizer = answer_tokenizer, max_sequence_length=512)\nlengths_test_a = np.argmax(test_inputs_a[0] == 0, axis=1)\nlengths_test_a[lengths_test_a == 0] = test_inputs_a[0].shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 5   \nBATCH_SIZE = 8\nepochs = 4    \ntest_predictions_q = [] \ntest_predictions_a = [] \ntest_predictions = []\n\ntest_set_q = QuestDataset(inputs=test_inputs_q, \n                          usefeatures=torch.FloatTensor(X_test) , \n                          question_data =  torch.LongTensor(test_question_tokenized),\n                          answer_data =  torch.LongTensor(test_answer_tokenized),\n                          title_data =  torch.LongTensor(test_title_tokenized),\n                          lengths=lengths_test_q, labels=None)\ntest_loader_q  = DataLoader(test_set_q, batch_size=32, shuffle=False)\nresult_q = np.zeros((len(test), 21))\n\n\ntest_set_a = QuestDataset(inputs=test_inputs_a, \n                          usefeatures=torch.FloatTensor(X_test) , \n                           question_data =  torch.LongTensor(test_question_tokenized),\n                          answer_data =  torch.LongTensor(test_answer_tokenized),\n                          title_data =  torch.LongTensor(test_title_tokenized),                          \n                          lengths=lengths_test_a, labels=None)\ntest_loader_a  = DataLoader(test_set_a, batch_size=32, shuffle=False)\nresult_a = np.zeros((len(test), 9))\n\n\n#y_train = train[target_cols].values # dummy\nset_seeds(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_labels_dict = {'question_conversational': 2, 'question_expect_short_answer': 3, 'question_fact_seeking':4, \n 'question_has_commonly_accepted_answer':5,\n 'question_multi_intent' : 8,\n 'question_not_really_a_question':9,\n 'question_opinion_seeking':10,\n 'question_type_choice' :11, \n 'question_type_compare':12,\n 'question_type_consequence': 13,\n 'question_type_definition':14,\n 'question_type_entity':15,\n 'question_type_instructions':16,\n 'question_type_procedure':17,\n 'question_type_reason_explanation':18,\n 'question_type_spelling':19,\n 'answer_type_instructions':26,\n 'answer_type_procedure':27,\n 'answer_type_reason_explanation':28}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_SAVED_MODEL_ANSWERS = ['../input/bert-use-crawl2-dataset-answers/', \n                            '../input/bert-use-crawl-seed42-answers/',\n                            '../input/bert-use-crawl-seed101-answers/',\n                           '../input/bert-use-crawl2-dataset-seed1029-answers',\n                           '../input/bert-use-crawl2-dataset-seed1977-answers']\n                            \n                            \nBERT_SAVED_MODEL_QUESTIONS = ['../input/bert-use-crawl2-dataset-questions/'  ,\n                              '../input/bert-use-crawl-seed42-questions/',\n                            '../input/bert-use-crawl-seed101-questions/',\n                             '../input/bert-use-crawl2-dataset-seed1029-questions',\n                               '../input/bert-use-crawl2-dataset-seed1977-questions'\n                             ]\n\n\nfor qa in range(len(BERT_SAVED_MODEL_ANSWERS)):    \n    for f in range(NUM_FOLDS):\n\n        model_q = BertForSequenceClassification_v2Question.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config_q , embedding_matrix=embedding_matrix)\n        model_q.zero_grad();\n        model_q.to(device);\n        torch.cuda.empty_cache()\n        dict_load = torch.load(path_join(BERT_SAVED_MODEL_QUESTIONS[qa], 'Q_model_save_fold_{}.pt'.format(f)))\n        dict_load['embedding.weight'] = torch.from_numpy(embedding_matrix)\n        model_q.load_state_dict(dict_load)\n        out_q = predict_result_question(model_q, test_loader_q)\n\n        model_a = BertForSequenceClassification_v2Answer.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/',config=bert_config_a , embedding_matrix=embedding_matrix)\n        model_a.zero_grad();\n        model_a.to(device);\n        torch.cuda.empty_cache()\n        dict_load = torch.load(path_join(BERT_SAVED_MODEL_ANSWERS[qa], 'A_model_save_fold_{}.pt'.format(f)))\n        dict_load['embedding.weight'] = torch.from_numpy(embedding_matrix)\n        model_a.load_state_dict(dict_load)\n        out_a = predict_result_answer(model_a, test_loader_a)\n        preds = np.hstack((out_q,out_a))\n        \n        if POST_PROCESS:             \n            # Round categories\n            for l in class_labels_dict.keys():\n                preds[:,class_labels_dict[l]] = pd.cut(preds[:,class_labels_dict[l]], train[l].nunique(), labels=train[l].value_counts().index.sort_values())\n\n\n        test_predictions.append(preds)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_bert = np.mean(test_predictions,axis=0)\nidx = np.argwhere(test.category!='CULTURE').flatten()\narr = 0.0001 * np.clip(np.random.normal(size=len(idx)),0.1,0.99)\ntest_preds_bert[idx,19] = arr  \n\n# class_labels = ['question_conversational',\n#  'question_expect_short_answer',\n#  'question_fact_seeking',\n#  'question_has_commonly_accepted_answer',\n#  'question_multi_intent',\n#  'question_not_really_a_question',\n#  'question_opinion_seeking',\n#  'question_type_choice',\n#  'question_type_compare',\n#  'question_type_consequence',\n#  'question_type_definition',\n#  'question_type_entity',\n#  'question_type_instructions',\n#  'question_type_procedure',\n#  'question_type_reason_explanation',\n#  'question_type_spelling',\n#  'answer_type_instructions',\n#  'answer_type_procedure',\n#  'answer_type_reason_explanation']\n\n\n# submission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\n# submission.iloc[:,1:] = test_preds  \n\n# # Round categories\n# for l in class_labels:\n#     submission[l] = pd.cut(submission[l], train[l].nunique(), labels=train[l].value_counts().index.sort_values())\n\n# submission.to_csv('submission.csv', index=False)\n# submission.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Roberta Inference"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nROBERTA_PRETRAINED = '../input/roberta-transformers-pytorch/roberta-base'\n\n\nimport os\n\nimport transformers, sys, os, gc\nimport numpy as np, pandas as pd, math\nimport torch, random, os, multiprocessing, glob\nimport torch.nn.functional as F\nimport torch, time\n\n#sys.path.insert(0, \"gdrive/My Drive/kaggle/google-quest-challenge/\")\nsys.path.insert(0, data_dir)\n\n#from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\nfrom scipy.stats import spearmanr\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (RobertaTokenizer, RobertaModel, BertPreTrainedModel, \n                          RobertaForSequenceClassification, RobertaConfig, AdamW, \n                          get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP )\n#WEIGHTS_NAME, CONFIG_NAME, \n\nfrom os.path import join as path_join\n\nfrom tqdm import tqdm\n# import logging\n# logging.basicConfig(filename=path_join('gdrive','My Drive','kaggle','submissions','roberta','robertamodel.log') ,level=logging.DEBUG)\n    \nprint(transformers.__version__)\n\ndef set_seeds(SEED=42):\n  os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n  np.random.seed(SEED)\n  torch.manual_seed(SEED)\n  torch.cuda.manual_seed(SEED)\n  torch.backends.cudnn.deterministic = True\n\nSEED=2019\nset_seeds(SEED)\nHEAD_TAIL = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Use the same USE + distance features created for Bert\n\n## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"LEN_USE_FEATURES = 1646 #1606 \n\n\nclass RobertaClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n      \n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        \n        return x\n\nclass RobertaForSequenceClassification_v2(BertPreTrainedModel):\n    config_class = RobertaConfig\n    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = \"roberta\"\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config)\n        self.classifier = RobertaClassificationHead(config)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        usefeatures = None\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.RobertaConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n            Classification (or regression if config.num_labels==1) loss.\n        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n\n    Examples::\n\n        from transformers import RobertaTokenizer, RobertaForSequenceClassification\n        import torch\n\n        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n        model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n        \"\"\"\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n        sequence_output = outputs[0]\n\n        pooled_output = outputs[1] \n        hidden_states = outputs[2]\n        logits = self.classifier(sequence_output)   #I realize now that I am not using usefeatures here :( too late\n        \n        outputs = (logits,) + outputs[2:]\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(TRAIN_CSV_PATH)\ntest = pd.read_csv( TEST_CSV_PATH)\n\ntarget_cols_questions = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice',\n               'question_type_compare', 'question_type_consequence',\n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written'] \n\ntarget_cols_answers =  ['answer_helpful',\n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"</s>\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize Answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef _trim_input_answers(title, question, answer, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n    \n    #293+239+30 = 508 + 4 = 512\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        \n        q_len_head = round(q_new_len/2)\n        q_len_tail = -1* (q_new_len -q_len_head)\n        a_len_head = round(a_new_len/2)\n        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n        t = t[:t_new_len]\n        if HEAD_TAIL :\n            q = q[:q_len_head]+q[q_len_tail:]\n            a = a[:a_len_head]+a[a_len_tail:]\n        else:\n            q = q[:q_new_len]\n            a = a[:a_new_len] ## No Head+Tail ,usual processing\n    \n    return t, q, a\n\n\n\ndef _convert_to_bert_inputs_answers(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken =  ['<s> '] + title + ['</s> </s> '] + question + ['</s> </s> '] + answer + ['</s>'] \n\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\n\n\ndef compute_input_arays_answers(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows(),position=0, leave=True ):\n        t, q, a  = instance.question_title,instance.question_body, instance.answer\n        t, q, a  = _trim_input_answers(t, q, a, tokenizer, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs_answers(t,q, a,tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _trim_input(title, question, tokenizer, max_sequence_length=512, t_max_len=30, q_max_len=479):\n    \n    #3 + 479 + 30\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    \n    t_len = len(t)\n    q_len = len(q)\n    \n    if (t_len+q_len+3) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            q_max_len = q_max_len + (t_max_len - t_len)\n            q_new_len = q_max_len\n        else:\n            t_new_len = t_max_len\n            q_new_len = q_max_len\n      \n        if q_max_len > q_len:\n            q_new_len = q_len\n            \n\n        if t_new_len+q_new_len+3 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + q_new_len + 3)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        \n    return t, q\n\ndef _convert_to_bert_inputs(title, question, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = ['<s>'] + title + ['</s> </s> '] + question + ['</s>'] \n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows(),position=0, leave=True ):\n        t, q  = instance.question_title, instance.question_body\n        t, q  = _trim_input(t, q, tokenizer, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t, q,tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, usefeatures, lengths, labels = None):\n        \n        self.inputs = inputs\n        self.usefeatures = usefeatures\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        self.lengths = lengths\n       \n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        lengths         = self.lengths[idx]\n        usef            = self.usefeatures[idx,:]\n        if self.labels is not None: # targets\n            labels = self.labels[idx]\n            return input_ids, input_masks, input_segments,  usef, labels, lengths\n        return input_ids, input_masks, input_segments, usef, lengths\n\n    def __len__(self):\n        return len(self.inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_result_question(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), 21))\n    \n    model.eval();\n    tk0 = tqdm(enumerate(test_loader),position=0, leave=True )\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                            usefeatures = x_batch[3].to(device)\n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output\n\n\ndef predict_result_answer(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), 9))\n    \n    model.eval();\n    tk0 = tqdm(enumerate(test_loader),position=0, leave=True )\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                            usefeatures = x_batch[3].to(device)                            \n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seeds(SEED)\n\nquestion_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_PRETRAINED)\ninput_categories_q = list(train.columns[[1,2]]); \n\nrobert_config_question =  RobertaConfig.from_pretrained(ROBERTA_PRETRAINED)\nrobert_config_question.num_labels = 21\nrobert_config_question.output_hidden_states = True \n\nanswer_tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_PRETRAINED)\ninput_categories_a = list(train.columns[[1,2,5]]); \n\nrobert_config_answer =  RobertaConfig.from_pretrained(ROBERTA_PRETRAINED)\nrobert_config_answer.num_labels = 9\nrobert_config_answer.output_hidden_states = True \n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_inputs_q = compute_input_arays(df =  test, columns = input_categories_q, tokenizer = question_tokenizer, max_sequence_length=512)\nlengths_test_q = np.argmax(test_inputs_q[0] == 0, axis=1)\nlengths_test_q[lengths_test_q == 0] = test_inputs_q[0].shape[1]\n\n\ntest_inputs_a = compute_input_arays_answers(df =  test, columns = input_categories_a, tokenizer = answer_tokenizer, max_sequence_length=512)\nlengths_test_a = np.argmax(test_inputs_a[0] == 0, axis=1)\nlengths_test_a[lengths_test_a == 0] = test_inputs_a[0].shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS = 5   \nBATCH_SIZE = 8\nepochs = 5\ntest_predictions_q = [] \ntest_predictions_a = [] \ntest_predictions = []\n\ntest_set_q = QuestDataset(inputs=test_inputs_q, \n                          usefeatures=torch.FloatTensor(X_test), \n                          lengths=lengths_test_q, labels=None)\ntest_loader_q  = DataLoader(test_set_q, batch_size=32, shuffle=False)\nresult_q = np.zeros((len(test), 21))\n\ntest_set_a = QuestDataset(inputs=test_inputs_a, \n                          usefeatures=torch.FloatTensor(X_test),                  \n                          lengths=lengths_test_a, labels=None)\ntest_loader_a  = DataLoader(test_set_a, batch_size=32, shuffle=False)\nresult_a = np.zeros((len(test), 9))\n\nset_seeds(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROBERTA_SAVED_MODEL_ANSWERS = ['../input/roberta-dataset-answers/', \n                               '../input/roberta-seed-42-answers/' , \n                               '../input/roberta-dataset-seed101-answers/',\n                              '../input/roberta-dataset-seed1029-answers'\n                              ]\n\nROBERTA_SAVED_MODEL_QUESTIONS = ['../input/roberta-dataset-questions/',\n                                '../input/roberta-seed-42-questions/',\n                                 '../input/roberta-dataset-seed101-questions/',\n                                '../input/roberta-dataset-seed1029-questions']  \n\n    \nfor qa in range(len(ROBERTA_SAVED_MODEL_ANSWERS)):\n    for f in range(NUM_FOLDS):\n\n        model_q = RobertaForSequenceClassification_v2.from_pretrained(ROBERTA_PRETRAINED, config=robert_config_question)\n        model_q.zero_grad();\n        model_q.to(device);\n        torch.cuda.empty_cache()\n        dict_load = torch.load(path_join(ROBERTA_SAVED_MODEL_QUESTIONS[qa], 'Q_model_save_fold_{}.pt'.format(f)))\n        model_q.load_state_dict(dict_load)\n        out_q = predict_result_question(model_q, test_loader_q)\n\n        model_a = RobertaForSequenceClassification_v2.from_pretrained(ROBERTA_PRETRAINED,config=robert_config_answer)\n        model_a.zero_grad();\n        model_a.to(device);\n        torch.cuda.empty_cache()\n        dict_load = torch.load(path_join(ROBERTA_SAVED_MODEL_ANSWERS[qa], 'A_model_save_fold_{}.pt'.format(f)))\n        model_a.load_state_dict(dict_load)\n        out_a = predict_result_answer(model_a, test_loader_a)\n        \n        preds = np.hstack((out_q,out_a))\n        \n        if POST_PROCESS:             \n            # Round categories\n            for l in class_labels_dict.keys():\n                preds[:,class_labels_dict[l]] = pd.cut(preds[:,class_labels_dict[l]], train[l].nunique(), labels=train[l].value_counts().index.sort_values())\n\n        test_predictions.append(preds)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds_roberta = np.mean(test_predictions,axis=0)\nidx = np.argwhere(test.category!='CULTURE').flatten()\narr = 0.0001 * np.clip(np.random.normal(size=len(idx)),0.1,0.99)\ntest_preds_roberta[idx,19] = arr    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_labels = ['question_conversational',\n 'question_expect_short_answer',\n 'question_fact_seeking',\n 'question_has_commonly_accepted_answer',\n 'question_multi_intent',\n 'question_not_really_a_question',\n 'question_opinion_seeking',\n 'question_type_choice',\n 'question_type_compare',\n 'question_type_consequence',\n 'question_type_definition',\n 'question_type_entity',\n 'question_type_instructions',\n 'question_type_procedure',\n 'question_type_reason_explanation',\n 'question_type_spelling',\n 'answer_type_instructions',\n 'answer_type_procedure',\n 'answer_type_reason_explanation']\n\n\nsubmission = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\nsubmission.iloc[:,1:] = test_preds_bert * 0.70 + test_preds_roberta*0.30\n\n\n# Round categories\nfor l in class_labels:\n    submission[l] = pd.cut(submission[l], train[l].nunique(), labels=train[l].value_counts().index.sort_values())\n\n    \nsubmission.to_csv('submission.csv', index=False)\nsubmission.head() \n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}