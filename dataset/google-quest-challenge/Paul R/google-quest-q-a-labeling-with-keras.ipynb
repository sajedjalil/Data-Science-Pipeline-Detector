{"cells":[{"metadata":{},"cell_type":"markdown","source":"# My Approach"},{"metadata":{},"cell_type":"markdown","source":"The training file has 11 input variables and 30 target variables. The 'Question Title' and 'Question Body' has the enough data to predict the target variables related to question. The 'Answer' variable has the data to predict the target variables related to answer. Hence I am training the model with inputs related to question and answer seperately. I am intentioanlly just repeating the same set of code while training the model for question and answer just to look simple for any beginners. A disclaimer- I, myself, a beginner for Keras/Deep Learning.\n#### Consider upvoting if you like my kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import all required Lobraries\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten, LSTM\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\n\nimport pandas as pd\nimport numpy as np\nimport re","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets train our model on the question part"},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_train=pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\ncsv_train.shape\ncsv_test=pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\ncsv_test.shape\n#csv.head()\n#check if there is any null column\n#csv_test.shape\ncsv_train['question_input']=csv_train['question_title']+'. '+csv_train['question_body']\n#Defining a function to preprocess the inout data using regular expression.\ndef preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n\n#define a empty list and run a for loop to process each and every record in the train data set. and move the processed\n#data to the list\nQI_train = []\nsentences = list(csv_train['question_input'])\nfor sen in sentences:\n    QI_train.append(preprocess_text(sen))\n\n#Defining the target variables related to question\nQO_train=csv_train[['question_asker_intent_understanding','question_body_critical', 'question_conversational','question_expect_short_answer','question_fact_seeking',\n       'question_has_commonly_accepted_answer','question_interestingness_others','question_interestingness_self','question_multi_intent','question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']].values\n\n#if you are locally running and like to see how does the preprocessed the data comment out the below line\n#QO_train[0]\n\n#Lets preprocess the data from the test dataset as well for prediction to run once the model is built\ncsv_test['question_input']=csv_test['question_title']+'. '+csv_test['question_body']\ndef preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n#same as above. define a empty list and run a for loop to process each and every record in the test data set...\n#..and move the processed data to the list\nQI_test = []\nsentences = list(csv_test['question_input'])\nfor sen in sentences:\n    QI_test.append(preprocess_text(sen))\nQI_test[0]\n\n#Lets consider the 40000 unique words. Use the text_to_sequence method to convert the words to vector\ntokenizer = Tokenizer(num_words=40000)\ntokenizer.fit_on_texts(QI_train)\n\nQI_train = tokenizer.texts_to_sequences(QI_train)\nQI_test = tokenizer.texts_to_sequences(QI_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 6079\n\n#Padding - Adding the zeros post to the vector to equalize the length of all input rows to 2000\nQI_train = pad_sequences(QI_train, padding='post', maxlen=maxlen)\nQI_test = pad_sequences(QI_test, padding='post', maxlen=maxlen)\n\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\n\nglove_file = open('/kaggle/input/glove6b100d/glove.6B.100d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n# Feature extraction\nembedding_matrix = zeros((vocab_size, 100))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\n#Using one embedding layer, one LSTM layer and 21 neurons for output\ndeep_inputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape=(maxlen,)))\nmodel.add(layers.Dense(21, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n\nprint(model.summary())\n#Lets go for prediction using the test data set\nhistory = model.fit(QI_train, QO_train, batch_size=128, epochs=2, verbose=1,validation_split=0.1)\nQO_test=model.predict(QI_test, batch_size=128, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets now work on the answer part"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I am just repeating the same set of code here to train the model on the answer part\ndef preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n\nAI_train = []\nsentences = list(csv_train['answer'])\nfor sen in sentences:\n    AI_train.append(preprocess_text(sen))\nAI_train[0]\n\nAO_train=csv_train[['answer_helpful','answer_level_of_information', 'answer_plausible', 'answer_relevance','answer_satisfaction', 'answer_type_instructions','answer_type_procedure', 'answer_type_reason_explanation','answer_well_written']].values\nAO_train[0]\n\ndef preprocess_text(sen):\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence\n\nAI_test = []\nsentences = list(csv_test['answer'])\nfor sen in sentences:\n    AI_test.append(preprocess_text(sen))\nAI_test[0]\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(AI_train)\n\nAI_train = tokenizer.texts_to_sequences(AI_train)\nAI_test = tokenizer.texts_to_sequences(AI_test)\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 6079\n\nAI_train = pad_sequences(AI_train, padding='post', maxlen=maxlen)\nAI_test = pad_sequences(AI_test, padding='post', maxlen=maxlen)\n\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\nglove_file = open('/kaggle/input/glove6b100d/glove.6B.100d.txt', encoding=\"utf8\")\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()\n\nembedding_matrix = zeros((vocab_size, 100))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\ndeep_inputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation='relu', input_shape=(maxlen,)))\nmodel.add(layers.Dense(9, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n\nprint(model.summary())\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\nprint(model.summary())\n\nhistory = model.fit(AI_train, AO_train, batch_size=128, epochs=2, verbose=1,validation_split=0.1)\nAO_test=model.predict(AI_test, batch_size=128, verbose=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"\noutanscol=['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\noutquescol=['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\ndfoutans=pd.DataFrame(AO_test,columns=outanscol)\ndfoutque=pd.DataFrame(QO_test,columns=outquescol)\nhorizontal_stack = pd.concat([dfoutque,dfoutans], axis=1)\nhorizontal_stack['qa_id']=csv_test['qa_id']\ncols = horizontal_stack.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsubmission=horizontal_stack[cols]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}