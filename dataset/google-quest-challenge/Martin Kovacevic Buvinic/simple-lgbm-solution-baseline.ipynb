{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nimport lightgbm as lgb\nfrom sklearn import metrics\n\nseed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Thanks to https://www.kaggle.com/abazdyrev/use-features-oof with the preprocessing part"},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    data_dir = '../input/google-quest-challenge/'\n    print('Reading train set')\n    train = pd.read_csv(path_join(data_dir, 'train.csv'))\n    print('Reading test set')\n    test = pd.read_csv(path_join(data_dir, 'test.csv'))\n    print('Our training data have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n    print('Our testing data have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 30 target variables where the range is [0, 1]\ntargets = ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', \n           'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', \n           'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', \n           'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', \n           'question_type_compare', 'question_type_consequence', 'question_type_definition', \n           'question_type_entity', 'question_type_instructions', 'question_type_procedure', \n           'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', \n           'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', \n           'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', \n           'answer_type_reason_explanation', 'answer_well_written']\n\n# text features, using embedding trained model to preprocess them\ninput_columns = ['question_title', 'question_body', 'answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get categorical features and ohe\ndef get_cat_features(train, test):\n    find = re.compile(r\"^[^.]*\")\n\n    train['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n    test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\n    features = ['netloc', 'category']\n    merged = pd.concat([train[features], test[features]])\n    ohe = OneHotEncoder()\n    ohe.fit(merged)\n    \n    # one hot encode netloc and category\n    features_train = ohe.transform(train[features]).toarray()\n    features_test = ohe.transform(test[features]).toarray()\n    print('Our categorical features have {} rows and {} columns'.format(features_train.shape[0], features_train.shape[1]))\n    return features_train, features_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_embedding_features(train, test, input_columns):\n    \n    # load universal sentence encoder model to get sentence ambeddings\n    module_url = \"../input/universalsentenceencoderlarge4/\"\n    embed = hub.load(module_url)\n    \n    # create empty dictionaries to store final results\n    embedding_train = {}\n    embedding_test = {}\n\n    # iterate over text columns to get senteces embeddings with the previous loaded model\n    for text in input_columns:\n    \n        print(text)\n        train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n        # create empy list to save each batch\n        curr_train_emb = []\n        curr_test_emb = []\n    \n        # define a batch to transform senteces to their correspinding embedding (1 X 512 for each sentece)\n        batch_size = 4\n        ind = 0\n        while ind * batch_size < len(train_text):\n            curr_train_emb.append(embed(train_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n            ind += 1\n        \n        ind = 0\n        while ind * batch_size < len(test_text):\n            curr_test_emb.append(embed(test_text[ind * batch_size: (ind + 1) * batch_size])['outputs'].numpy())\n            ind += 1\n\n        # stack arrays to get a 2D array (dataframe) corresponding with all the sentences and dim 512 for columns (sentence encoder output)\n        embedding_train[text + '_embedding'] = np.vstack(curr_train_emb)\n        embedding_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \n    del embed\n    K.clear_session()\n    gc.collect()\n    \n    return embedding_train, embedding_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have our categorical array of dimension 6079 x 64\n* We have our embedding dict were each key have a dimension of 6079 x 512, we have 3 keys that corresponds to the embedding of the sentece of question title, question body and answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dist_features(embedding_train, embedding_test, features_train, features_test):\n    \n    # define a square dist lambda function were (x1 - y1) ^ 2 + (x2 - y2) ^ 2 + (x3 - y3) ^ 2 + ... + (xn - yn) ^ 2\n    # with this we get one vector of dimension 6079\n    l2_dist = lambda x, y: np.power(x - y, 2).sum(axis = 1)\n    \n    # define a cosine dist lambda function were (x1 * y1) ^ 2 + (x2 * y2) + (x3 * y3) + ... + (xn * yn)\n    cos_dist = lambda x, y: (x * y).sum(axis = 1)\n    \n    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n    dist_features_train = np.array([\n        l2_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n        l2_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n        l2_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding']),\n        cos_dist(embedding_train['question_title_embedding'], embedding_train['answer_embedding']),\n        cos_dist(embedding_train['question_body_embedding'], embedding_train['answer_embedding']),\n        cos_dist(embedding_train['question_body_embedding'], embedding_train['question_title_embedding'])]).T\n    \n    # transpose it because we have 6 vector of dimension 6079, need 6079 x 6\n    dist_features_test = np.array([\n        l2_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n        l2_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n        l2_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding']),\n        cos_dist(embedding_test['question_title_embedding'], embedding_test['answer_embedding']),\n        cos_dist(embedding_test['question_body_embedding'], embedding_test['answer_embedding']),\n        cos_dist(embedding_test['question_body_embedding'], embedding_test['question_title_embedding'])]).T\n    \n    # get the values of each key, therefore we have 3 arrays of dim 6079 x 512 (hstack to get 6079 x 1536)\n    x_train = np.hstack([values for key, values in embedding_train.items()] + [features_train, dist_features_train])\n    x_test = np.hstack([values for key, values in embedding_test.items()] + [features_test, dist_features_test])\n    print('Our preprocess training set have {} rows and {} columns'.format(x_train.shape[0], x_train.shape[1]))\n    print('Our preprocess testing set have {} rows and {} columns'.format(x_test.shape[0], x_test.shape[1]))\n    \n    return pd.DataFrame(x_train), pd.DataFrame(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(x_train, x_test, train, target):\n    \n    # 5 random KFold\n    kf = KFold(n_splits = 10, random_state = 42)\n    \n    # get empty vectors (0) to store out of fold predictions and test predictions\n    oof_pred = np.zeros(len(x_train))\n    y_pred = np.zeros(len(x_test))\n    \n    # for each fold train a model and then get the mean to predict the test\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(x_train)):\n        print('Fold {}'.format(fold + 1))\n        x_trn, x_val = x_train.iloc[tr_ind], x_train.iloc[val_ind]\n        y_trn, y_val = train[target].iloc[tr_ind], train[target].iloc[val_ind]\n        train_set = lgb.Dataset(x_trn, y_trn)\n        val_set = lgb.Dataset(x_val, y_val)\n\n        params = {\n            'learning_rate': 0.1,\n            'metric': 'rmse',\n            'objective': 'regression',\n            'feature_fraction': 0.85,\n            'subsample': 0.85,\n            'n_jobs': -1,\n            'seed': 42,\n            'max_depth': -1\n        }\n        \n        # train the model with early stoping\n        model = lgb.train(params, train_set, num_boost_round = 1000000, early_stopping_rounds = 10, \n                          valid_sets=[train_set, val_set], verbose_eval = 10)\n        oof_pred[val_ind] = model.predict(x_val)\n        y_pred += model.predict(x_test) / kf.n_splits\n    loss_score = np.sqrt(metrics.mean_squared_error(train[target], oof_pred))\n    print('Our oof rmse score is: ', loss_score)\n    return y_pred\n\ndef predict(x_train, x_test, train, targets):\n    predictions = []\n    for num, target in enumerate(targets):\n        print('Train model {}'.format(num + 1))\n        print('Predicting target {}'.format(target))\n        predictions.append(np.clip(run_lgb(x_train, x_test, train, target), a_min = 0, a_max = 1))\n    data_dir = '../input/google-quest-challenge/'\n    submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\n    submission[targets] = np.array(predictions).T\n    submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run all\n# read training and test data\ntrain, test = read_data()\n# get one hot encoded categorical data\nfeatures_train, features_test = get_cat_features(train, test)\n# get embedding features\nembedding_train, embedding_test = get_embedding_features(train, test, input_columns)\n# get dist features from the embedding features and concatenate with embedding and categorical features\nx_train, x_test = get_dist_features(embedding_train, embedding_test, features_train, features_test)\n# trian and predict\npredict(x_train, x_test, train, targets)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}