{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook will show the usage of the post-processing discussed here: https://www.kaggle.com/c/google-quest-challenge/discussion/130083"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom scipy.stats.mstats import hmean\nfrom scipy.stats import spearmanr\nfrom functools import partial\n# suppress scientific notation in numpy and pandas\nnp.set_printoptions(suppress=True)\npd.options.display.float_format = '{:.6f}'.format\npd.set_option('display.max_columns', None)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Postprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns=pd.read_csv(f'../input/ensemble-data/fold_0_labels.csv').iloc[:,1:].columns.tolist()\n\ntarget_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# labels.npy stores the frequencies of every labels for every column\nclasses = np.load('../input/labels/labels.npy', allow_pickle=True)\n\nprior_freqs_list = [np.array([classes[i][key] for key in sorted(classes[i])]) for i in range(len(classes))]\nprior_probs_list = [freqs / sum(freqs) for freqs in prior_freqs_list]\n\nprior_probs_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef deal_column(s: np.ndarray, freq):\n    \"\"\"\n    the idea is illustrated here: https://www.kaggle.com/c/google-quest-challenge/discussion/130083\n    s is the original predictions, and freq is the number of every labels from small to large.\n    Example: \n    If a column only has 3 lables: 0, 1/3, 2/3 and the distribution is [0.5, 0.2, 0.3]\n    assume the original prediction s for this column is [0.01,0.03,0.05,0.02,0.07,0.04,0.09,0.0,0.08,0.06]\n    This method will map the lowest 5 predictions to 0 because theoretically this test set has 10*0.5=5 examples that labeled 0.\n    The processing for labels 1/3 and 2/3 is similar, and the output will be:\n    [0.0,0.0,0.05,0.0,0.07,0.0,0.07,0.0,0.07,0.05]\n    \"\"\"\n    res = s.copy()  # use a copy to return\n    d = {i: v for i, v in enumerate(s)}  # <index, original_value>\n    d = sorted(d.items(), key=lambda item: item[1])\n    j = 0\n    for i in range(len(freq)):\n        if freq[i] > 0 and j < len(d):\n            fixed_value = d[j][1]\n            while freq[i] > 0:\n                res[d[j][0]] = fixed_value\n                freq[i] -= 1\n                j += 1\n    return res\n\n\n# prob is the distribution of the column in trainning set, n is the number of examples of test set\ndef estimate_frequency(prob: np.ndarray, n):\n    tmp = prob * n\n    freq = [int(round(t)) for t in tmp]\n    # the prob times #example and and use round operation cannot make sure the sum of freq equals to #example\n    # here we consider the error of round operation, e.g. round(1.9)=2 so the error is 0.1, and round(1.5)=2 so error is 0.5\n    confidence = {i: np.abs(0.5 - (x - int(x))) for i, x in enumerate(tmp)}  # the smaller the error, the higher the confidence we have in round\n    confidence = sorted(confidence.items(), key=lambda item: item[1])\n    # fix frequency according to confidence of 'round' operation\n    fix_order = [idx for idx, _ in confidence]\n    idx = 0\n    s = np.sum(freq)\n    # fix the frequency of every label, until the sum is #example\n    while s != n:\n        if s > n:\n            freq[fix_order[idx]] -= 1\n        else:\n            freq[fix_order[idx]] += 1\n        s = np.sum(freq)\n        # theoretically we can fix the freq in one round, but here we use a loop\n        idx = (idx + 1) % len(fix_order)\n    # if the resulting freq only has 1 label/class, we change it to 2 labels: one has n-1 examples and the other has 1 example\n    if np.sum(np.array(freq) > 0) < 2:  # in case there is only one class\n        freq[0], freq[len(freq) - 1] = n - 1, 1\n    return freq\n\n\ndef align(predictions: np.ndarray, ban_list=None) -> np.ndarray:\n    num_samples = predictions.shape[0]  # number of examples of test set\n    predictions_new = predictions.copy()\n    for i in range(30):\n        # deal with every column but skip the columns that post-processing won't improve the score\n        if ban_list is not None and i in ban_list:\n            continue\n        frequency = estimate_frequency(prior_probs_list[i], num_samples)\n        predictions_new[:, i] = deal_column(predictions[:, i], frequency)\n    return predictions_new\n\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(spearmanr(col_trues, col_pred).correlation)\n    return np.mean(rhos).item()\n    \n\ndef cal(arr1, arr2): # calculate column-wise scores\n    return np.array([compute_spearmanr(arr1[:, i].reshape(-1, 1), arr2[:, i].reshape(-1, 1)) for i in range(30)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndiffs=pd.DataFrame(columns=target_columns)\n\nfor FOLD in range(5):\n    #Read csv files\n    labels=pd.read_csv(f'../input/ensemble-data/fold_{FOLD}_labels.csv').iloc[:,1:]\n    base=pd.read_csv(f'../input/ensemble-data/bert_base_uncased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    wwm_uncased=pd.read_csv(f'../input/ensemble-data/wwm_uncased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    wwm_cased=pd.read_csv(f'../input/ensemble-data/wwm_cased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    large_uncased=pd.read_csv(f'../input/ensemble-data/large_uncased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    roberta=pd.read_csv(f'../input/ensemble-data/roberta_large_fold_{FOLD}_preds.csv').iloc[:,1:]\n\n    ps=[base.values, wwm_uncased.values, wwm_cased.values, large_uncased.values, roberta.values]\n\n    \n    mv=np.average(ps,axis=0)\n    original_scores=cal(labels.values,mv)\n    \n    # post-processing\n    mv_1=mv.copy()\n    mv_1=align(mv_1)\n\n    relative_scores=cal(labels.values,mv_1)-original_scores\n    row = pd.DataFrame(relative_scores).T\n    row.columns = target_columns\n    diffs=diffs.append(row)\ndiffs.index=[f'fold-{n}' for n in range(5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply post-processing to the following columns will lower the scores. The numbers are the indices of the column in target_columns\nban_list = [0, 1, 3, 4, 6, 10, 16, 17, 18] + list(range(20, 30))\n\nscores,post_scores,post_ban_scores=[],[],[]\n# test the performance of PP\nfor FOLD in range(5):\n    #Read csv files\n    labels=pd.read_csv(f'../input/ensemble-data/fold_{FOLD}_labels.csv').iloc[:,1:]\n    base=pd.read_csv(f'../input/ensemble-data/bert_base_uncased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    wwm_uncased=pd.read_csv(f'../input/ensemble-data/wwm_uncased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    wwm_cased=pd.read_csv(f'../input/ensemble-data/wwm_cased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    large_uncased=pd.read_csv(f'../input/ensemble-data/large_uncased_fold_{FOLD}_preds.csv').iloc[:,1:]\n    roberta=pd.read_csv(f'../input/ensemble-data/roberta_large_fold_{FOLD}_preds.csv').iloc[:,1:]\n\n    ps=[base.values, wwm_uncased.values, wwm_cased.values, large_uncased.values, roberta.values]\n\n    \n    mv=np.average(ps,axis=0)\n    scores.append(compute_spearmanr(labels.values,mv))\n    \n    # post-processing\n    mv_1=mv.copy()\n    mv_2=mv.copy()\n    mv_1=align(mv_1)\n    mv_2=align(mv_2,ban_list)\n\n    post_scores.append(compute_spearmanr(labels.values,mv_1))\n    post_ban_scores.append(compute_spearmanr(labels.values,mv_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"original score: {np.mean(scores)}\\npost without ban: {np.mean(post_scores)}\\npost with ban: {np.mean(post_ban_scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}