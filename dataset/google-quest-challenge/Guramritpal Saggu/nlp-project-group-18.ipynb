{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\n\nWe have observed that many questions on web based question-answering/discussion platforms go unanswered for a long time.  The main reason behind that is either the question is asked in the wrong category or the similar kind of question has been asked before So people tend not to answer it. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\nWe use that dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common sense to complete the task. Demonstrating these subjective labels can be predicted reliably and can shine a new light on this research area.\n\nThe fundamental tasks of our project are:\n\n- Classify the questions based on the labels into various categories\n\n- Relevant question-answer retrieval using semantic similarity\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/google-quest-challenge/train.csv')\ntrain_df.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[['question_title', 'question_body', 'answer', 'category']]\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D,SimpleRNN\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is used to label encode the labels for categorization\nfrom sklearn.preprocessing import LabelEncoder\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['category'])\nlabels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing the URL's from the text\nURLs (or Uniform Resource Locators) in a text are references to a location on the web, but do not provide any additional information. We thus, remove these too using the library named re, which provides regular expression matching operations.","metadata":{}},{"cell_type":"code","source":"import re\ndef remove_url(s):\n  return re.sub(r'http\\S+', '', s)\n\ntrain_df['question_body'] = train_df['question_body'].apply(remove_url)\ntrain_df['answer'] = train_df['answer'].apply(remove_url)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Removing the Tags from the text\nThe web generates tons of text data and this text might have HTML tags in it. These HTML tags do not add any value to text data and only enable proper browser rendering. Hence we will remove the HTML tags from the text using re library","metadata":{}},{"cell_type":"code","source":"def remove_tag(s):\n  return re.sub(r'<.*?>', ' ', s)\n\n\ntrain_df['question_body'] = train_df['question_body'].apply(remove_tag)\ntrain_df['answer'] = train_df['answer'].apply(remove_tag)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lowercasing the text\nThe generated text contains both uppecase characters as well as lower case characters. Systems are usually case sensitive so it would consider \"the\" and \"The\" as different word, which would not only increase the number of words we have process but also cause same word to have multiple meaning. Hence we will lower case the entire text","metadata":{}},{"cell_type":"code","source":"def lower_words(s):\n   return s.lower()\n\ntrain_df['question_body'] = train_df['question_body'].apply(lower_words)\ntrain_df['answer'] = train_df['answer'].apply(lower_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Expand contracted words in the text\nIn our everyday verbal and written communication, a lot of us tend to contract common words like “you are” becomes “you’re”. Converting contractions into their natural form will bring more insights.","metadata":{}},{"cell_type":"code","source":"def decontracted(phrase):\n  \"\"\"decontracted takes text and convert contractions into natural form.\n     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n\n  # specific\n  phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n  phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n  phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n  phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n\n  # general\n  phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n  phrase = re.sub(r\"\\'re\", \" are\", phrase)\n  phrase = re.sub(r\"\\'s\", \" is\", phrase)\n  phrase = re.sub(r\"\\'d\", \" would\", phrase)\n  phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n  phrase = re.sub(r\"\\'t\", \" not\", phrase)\n  phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n  phrase = re.sub(r\"\\'m\", \" am\", phrase)\n\n  phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n  phrase = re.sub(r\"\\’re\", \" are\", phrase)\n  phrase = re.sub(r\"\\’s\", \" is\", phrase)\n  phrase = re.sub(r\"\\’d\", \" would\", phrase)\n  phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n  phrase = re.sub(r\"\\’t\", \" not\", phrase)\n  phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n  phrase = re.sub(r\"\\’m\", \" am\", phrase)\n\n  return phrase\n\ntrain_df['question_body'] = train_df['question_body'].apply(decontracted)\ntrain_df['answer'] = train_df['answer'].apply(decontracted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove words with numbers\nThe words which contain number tend to be spam, and add more noise to the data. Hence we'll remove them","metadata":{}},{"cell_type":"code","source":"def remove_words_with_nums(s):\n  return re.sub(r\"\\S*\\d\\S*\", \"\", s)\n\n\ntrain_df['question_body'] = train_df['question_body'].apply(remove_words_with_nums)\ntrain_df['answer'] = train_df['answer'].apply(remove_words_with_nums)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove special characters\nSpecial characters like  – (hyphen) or / (slash) don’t add any value, so we generally remove those. Characters are removed depending on the use case. If we are performing a task where the currency doesn’t play a role (for example in sentiment analysis), we remove the any currency sign.","metadata":{}},{"cell_type":"code","source":"def remove_special_character(s):\n  return re.sub('[^A-Za-z0-9]+', ' ', s)\n\ntrain_df['question_body'] = train_df['question_body'].apply(remove_special_character)\ntrain_df['answer'] = train_df['answer'].apply(remove_special_character)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stop Word Removal\nApart from URLs, HTML tags and special characters, there are words that are not required for tasks such as sentiment analysis or text classification. Words like I, me, you, he and others increase the size of text data but don’t improve results dramatically and thus it is a good idea to remove those.\n\nInstead of going with standard NLTK stopword set we decided to make our own set, as other set also includes negative words like 'not' which could be useful for the task","metadata":{}},{"cell_type":"code","source":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stopword(s):\n    res = ' '.join([word for word in s.split(' ') if word not in stopwords])\n    return res\n\ntrain_df['question_body'] = train_df['question_body'].apply(remove_stopword)\ntrain_df['answer'] = train_df['answer'].apply(remove_stopword)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lemmatization\nNow that we have removed all the “noise” from the text, it is time to normalize the data set. A word in a text may exist in multiple forms like stop and stopped (past participle or price and prices (plural). Text normalization converts variations of the word into root form of the same word.","metadata":{}},{"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef lemmatization(s):\n    res = ' '.join([lemmatizer.lemmatize(word) for word in s.split(' ')])\n    return res\n\ntrain_df['question_body'] = train_df['question_body'].apply(lemmatization)\ntrain_df['answer'] = train_df['answer'].apply(lemmatization)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = remove_url(text)\n    text = remove_tag(text)\n    text = lower_words(text)\n    text = decontracted(text)\n    text = remove_words_with_nums(text)\n    text = remove_special_character(text)\n    text = remove_stopword(text)\n    text = lemmatization(text)\n    return text\n\ntrain_df['question_body'] = train_df['question_body'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a Bare Minimal Neural Network \n\nHere, we will be building a stand-alonw neural network model just for classifying the labels to the respective questions. For this we will be using RNNs/LSTMs/GRU for our usecase. A classic LSTM based network is one of the most fundamental building blocks of all the robust architectures that we see today.For the first part we will be focussing on standard RNNs. Some resources for RNNs:\n\n\n## Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far. Forward pass of Classical RNNs have the following formula :\n\n\n## Classical RNN image\n\n\nA classic RNN consists of the following image:\n\n\n<img src=\"https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\n","metadata":{}},{"cell_type":"markdown","source":"## Simple RNN","metadata":{}},{"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(SimpleRNN(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_RNN.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture\n\nThe model architecture for the Bidirectional Simple RNN can be seen as below:\n\n<img src=\"https://i.imgur.com/QFsESSn.png\">","metadata":{}},{"cell_type":"code","source":"temp = 'I am new to Wordpress. i have issue with Feature image. just i need to add URL to feature image(when we click on that feature image , it should redirect to that particular URL). also is it possible to give URL to Title of the Portfolio categories page which i used in normal page. This is Portfolio , i have used in the \"mypage\" . so in that\" mypage\" when we click on that image and title it should be redirect to the link (should able to give individual link) Any help would be appreciated. Thanks.'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = [preprocess_text(temp)]\ntemp = tokenizer.texts_to_sequences(temp)\ntemp = pad_sequences(temp,maxlen=maxlen)\ntemp_prediction = model.predict(temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_label = np.argmax(temp_prediction)\nprint('Predicted Category', label_y.inverse_transform([temp_label])[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Embedding Matrix using GloVe Word Embeddings","metadata":{}},{"cell_type":"code","source":"#Using Glove Embeddings, In this case, we will be using pretrained Glove 200dimension embeddings.\n#The importance of using pretrained embeddings is to allow more semantic references of the word/sentence vectors.\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\nEMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simple RNN with Glove200D pretrained embeddings\n","metadata":{}},{"cell_type":"code","source":"#Important parameters when using with pretrained Glove 200d embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding -with pretrained glove weights\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(SimpleRNN(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_RNN_Glove200d.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe model architecture can be shown as below:\n\n<img src=\"https://i.imgur.com/3ZBQApl.png\">","metadata":{}},{"cell_type":"markdown","source":"# LSTM- Long Short Term Memory\n\n[LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) are gated recurrent networks having 4 gates with (tanh/sigmoid) activation units. These architectures are the the building blocks of all the transformer architectures that we see, and the 4 gates combine input from different time stamps to produce the output. In a LSTM, there are typically 3 input and output signals: The h (hidden cell output from the previous timestep), c (the signal from previous cell), and the x(input vectors). Outputs involve the updated ht+1(hidden cell output of current block) value, ct+1, (updated c signal from the present cell) and the output(o).\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">","metadata":{}},{"cell_type":"markdown","source":"## LSTM model with Glove200D pretrained embeddings\n\nNow we will be applying the glove embeddings (200d) for boosting performance (if any).","metadata":{}},{"cell_type":"code","source":"#Important parameters when using with pretrained Glove 200d embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding -with pretrained glove weights\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Glove200d.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/oOmKx56.png\">","metadata":{}},{"cell_type":"markdown","source":"## Gated Recurrent Units\n\n[GRUs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\">","metadata":{}},{"cell_type":"markdown","source":"## BiDirectional GRU","metadata":{}},{"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(GRU(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Architecture for vanilla GRU\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/jaZegBX.png\">","metadata":{}},{"cell_type":"markdown","source":"## Relevant question-answer retrieval using semantic similarity","metadata":{}},{"cell_type":"markdown","source":"## Universal Sentence Encoder\n\n<img src=\"https://jinglescode.github.io/assets/img/posts/build-textual-similarity-analysis-web-app-09.jpg\">\n\nWe'll download the Universal Sentence Encoder model from tensorflow hub and use the same to obtain the embeddings for titles of all the question answer pairs","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\nmodel = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-large/5?tf-hub-format=compressed\")\ntrain_df['question_title'] = train_df['question_title'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = [model([train_df.iloc[i].question_title])[0] for i in range(train_df.shape[0])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Semantic Similarity Based Retrival\n\nWe'll find the cosine similarity of query with every every question title and return the question title with maximum similarity","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\ndef query_match(query):\n    query_embedding = model([query])\n    sim = cosine_similarity(question_embeddings, query_embedding)\n    sim_scores = [sim[i][0] for i in range(sim.shape[0])]\n    return np.argmax(sim_scores)\n    print(np.shape(sim))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Input the query you want to search')\n# query = input()\nquery = \"delete facebook appeal\"\ncleaned_query = preprocess_text(query)\nquery_idx = query_match(query)\nprint('Here is the result')\nprint(train_df.iloc[query_idx].question_title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}