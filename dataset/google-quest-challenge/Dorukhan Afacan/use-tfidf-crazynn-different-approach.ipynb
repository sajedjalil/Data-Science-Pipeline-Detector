{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Congratulations to Everyone!\n\nThis kernel does not give a comparable score to the BERT models; but I tried to make a different use of USE model by providing question title as \"context\" to the **\"question-response\"** architecture of **USE** while exctracting embeddings.\n\nAlso I played a little with NN architecture instead of using a plain FC network. \n\nUSE Embedding extraction and NN were my own castles in this sandbox and had fun creating this. Even though it is not a LB jumper, it jumped above other USE and TfIdf models. Love getting creative in this platform and wanted to share.\n\nThe kernel scored better on public LB (0.363) when compared to direct use(not using the qa model but just treating t,q,a of our data as separate sentences.) of USE for extracting embeddings and plain FC-NN.\n\n****\nAfter obtaining embeddings for title-question-answer trio: \n- I Obtained TfIdf vectors and reduced them with TSVD\n- Calculated different similarity measures between the trio.\n\nAfter merging these features, they were fed to an NN where: \n- Two NNs take care of USE and TfIdf vectors.\n- Title, question, answers are fed to 3 seperate dense layers.\n- Their outputs are concatenated as **\"title-question\"** and **\"question-answer-similaritymeasures\"** and send to two seperate dense hidden layers.\n- Question labels are the outputs of the first hidden layer.\n- Answer labels are the outputs of the second hidden layer.\n- Question-Answer labels are concatenated\n- Outputs of USE-NN and TfIdf NN are averaged in a layer.\n\n\n\nFor the rest I thank to the kernels below.\nhttps://www.kaggle.com/abhishek/distilbert-use-features-oof\nhttps://www.kaggle.com/abazdyrev/use-features-oof"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"from collections import OrderedDict,defaultdict\nimport scipy.spatial.distance as sci_dist\nfrom scipy.stats import spearmanr,rankdata\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.backend as K\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Lambda, Concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')\ntrain = pd.read_csv('../input/google-quest-challenge/train.csv')\ntest = pd.read_csv('../input/google-quest-challenge/test.csv')\nfull = pd.concat([train,test],axis=0)\ntrain_size = len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"module_url = \"../input/universalsentenceencodermodels/universal-sentence-encoder-models/use-qa\"\nlarge_module_url = \"../input/universalsentenceencodermodels/universal-sentence-encoder-models/use-large\"\nmodule = hub.load(module_url)\nlarge_module = hub.load(large_module_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"labels = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntext_features = ['question_title','question_body','answer']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"question_labels = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n\nanswer_labels = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"texts = full[text_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Use tensorflow dataset elements for feature extraction part of the pipeline"},{"metadata":{"trusted":false},"cell_type":"code","source":"contexts = tf.data.Dataset.from_tensor_slices(texts.question_title.values.tolist())\nquestions = tf.data.Dataset.from_tensor_slices(texts.question_body.values.tolist())\nanswers = tf.data.Dataset.from_tensor_slices(texts.answer.values.tolist())\ntf_text_data = tf.data.Dataset.zip((contexts,questions,answers))\ntext_batches = tf_text_data.batch(32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Extract embeddings batch by batch for resource mgmt."},{"metadata":{"trusted":false},"cell_type":"code","source":"embeddings = defaultdict(list)\nfor i,batch in enumerate(text_batches):\n    print(f'Extracting embeddings from batch {i+1}...')\n    \n    context = tf.constant(batch[0])\n    questions = tf.constant(batch[1])\n    answers = tf.constant(batch[2])\n    \n    embeddings['title_embs'].append(large_module(context).numpy())\n    embeddings['question_embs'].append(module.signatures['question_encoder'](questions)['outputs'].numpy())\n    embeddings['answer_embs'].append(module.signatures['response_encoder'](input=answers,context=context)['outputs'].numpy())\n    \n                        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"question_embeddings = np.vstack(embeddings['question_embs'])\nanswer_embeddings = np.vstack(embeddings['answer_embs'])\ntitle_embeddings = np.vstack(embeddings['title_embs'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF"},{"metadata":{"trusted":false},"cell_type":"code","source":"tfidf_title = TfidfVectorizer(ngram_range=(1,3))\ntfidf_question = TfidfVectorizer(ngram_range=(1,3))\ntfidf_answer = TfidfVectorizer(ngram_range=(1,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tsvd_title = TruncatedSVD(n_components=100)\ntsvd_question = TruncatedSVD(n_components=100)\ntsvd_answer = TruncatedSVD(n_components=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_texts, test_texts = texts[:train_size], texts[train_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"title_vec_train = tfidf_title.fit_transform(train_texts.question_title)\ntitle_vec_test = tfidf_title.transform(test_texts.question_title)\n\nquestion_vec_train = tfidf_question.fit_transform(train_texts.question_body)\nquestion_vec_test = tfidf_question.transform(test_texts.question_body)\n\nanswer_vec_train = tfidf_answer.fit_transform(train_texts.answer)\nanswer_vec_test = tfidf_answer.transform(test_texts.answer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"title_train = tsvd_title.fit_transform(title_vec_train)\ntitle_test = tsvd_title.transform(title_vec_test)\n\nquestion_train = tsvd_question.fit_transform(question_vec_train)\nquestion_test = tsvd_question.transform(question_vec_test)                         \n\nanswer_train = tsvd_answer.fit_transform(answer_vec_train)\nanswer_test = tsvd_answer.transform(answer_vec_test)            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_x = np.hstack([title_train,question_train,answer_train])\ntest_x = np.hstack([title_test,question_test,answer_test])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distance and Similarity Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"qa_train_similarity_matrix = np.inner(question_embeddings[:train_size],answer_embeddings[:train_size])\nqa_test_similarity_matrix = np.inner(question_embeddings[train_size:],answer_embeddings[train_size:])\n\nqt_train_similarity_matrix = np.inner(question_embeddings[:train_size],title_embeddings[:train_size])\nqt_test_similarity_matrix = np.inner(question_embeddings[train_size:],title_embeddings[train_size:])\n\nta_train_similarity_matrix = np.inner(title_embeddings[:train_size],answer_embeddings[:train_size])\nta_test_similarity_matrix = np.inner(title_embeddings[train_size:],answer_embeddings[train_size:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"qa_train_sim_score = np.diag(qa_train_similarity_matrix)\nqa_test_sim_score = np.diag(qa_test_similarity_matrix)\n\nqt_train_sim_score = np.diag(qt_train_similarity_matrix)\nqt_test_sim_score = np.diag(qt_test_similarity_matrix)\n\nta_train_sim_score = np.diag(ta_train_similarity_matrix)\nta_test_sim_score = np.diag(ta_test_similarity_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"l2_dist = lambda x, y: np.linalg.norm(x-y,axis=1)\ncos_dist = lambda x, y: 1 - np.sum(x*y,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Question Answer Pairs\nqa_l2_dist_train = l2_dist(question_embeddings[:train_size],answer_embeddings[:train_size])\nqa_l2_dist_test = l2_dist(question_embeddings[train_size:],answer_embeddings[train_size:])\n\nqa_cos_dist_train = cos_dist(question_embeddings[:train_size],answer_embeddings[:train_size])\nqa_cos_dist_test = cos_dist(question_embeddings[train_size:],answer_embeddings[train_size:])\n\n#Question Title Pairs\nqt_l2_dist_train = l2_dist(question_embeddings[:train_size],title_embeddings[:train_size])\nqt_l2_dist_test = l2_dist(question_embeddings[train_size:],title_embeddings[train_size:])\n\nqt_cos_dist_train = cos_dist(question_embeddings[:train_size],title_embeddings[:train_size])\nqt_cos_dist_test = cos_dist(question_embeddings[train_size:],title_embeddings[train_size:])\n\n#Title Answer Pairs\nta_l2_dist_train = l2_dist(title_embeddings[:train_size],answer_embeddings[:train_size])\nta_l2_dist_test = l2_dist(title_embeddings[train_size:],answer_embeddings[train_size:])\n\nta_cos_dist_train = cos_dist(title_embeddings[:train_size],answer_embeddings[:train_size])\nta_cos_dist_test = cos_dist(title_embeddings[train_size:],answer_embeddings[train_size:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge Features"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = np.hstack([title_embeddings[:train_size],question_embeddings[:train_size],answer_embeddings[:train_size],\n                         qa_train_sim_score.reshape(-1,1),qt_train_sim_score.reshape(-1,1),ta_train_sim_score.reshape(-1,1),\n                         qa_l2_dist_train.reshape(-1,1),qa_cos_dist_train.reshape(-1,1),\n                         qt_l2_dist_train.reshape(-1,1),qt_cos_dist_train.reshape(-1,1),\n                         ta_l2_dist_train.reshape(-1,1),ta_cos_dist_train.reshape(-1,1),\n                         train_x\n                    ])\n\nX_test = np.hstack([title_embeddings[train_size:],question_embeddings[train_size:],answer_embeddings[train_size:],\n                         qa_test_sim_score.reshape(-1,1),qt_test_sim_score.reshape(-1,1),ta_test_sim_score.reshape(-1,1),\n                         qa_l2_dist_test.reshape(-1,1),qa_cos_dist_test.reshape(-1,1),\n                         qt_l2_dist_test.reshape(-1,1),qt_cos_dist_test.reshape(-1,1),\n                         ta_l2_dist_test.reshape(-1,1),ta_cos_dist_test.reshape(-1,1),\n                         test_x\n                   ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train = train[labels].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.shape,y_train.shape,X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"class SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def crazy_model():    \n    inp = Input(shape=(X_train.shape[1],))\n\n    title_input = inp[:,:512]\n    question_input = inp[:,512:1024]\n    answer_input = inp[:,1024:1536]\n    \n    similarity_input = inp[:,1536:1545]\n    \n    tfidf_title_input = inp[:,1545:1645]\n    tfidf_question_input = inp[:,1645:1745]\n    tfidf_answer_input = inp[:,1745:]\n    \n    #USE Model\n    title_hidden = Dense(256, activation='relu')(title_input)\n    title_hidden = Dropout(0.2)(title_hidden)\n    question_hidden = Dense(256, activation='relu')(question_input)\n    question_hidden = Dropout(0.2)(question_hidden)\n    answer_hidden = Dense(256, activation='relu')(answer_input)\n    answer_hidden = Dropout(0.2)(answer_hidden)\n    \n    title_out = Dense(128, activation='relu')(title_hidden)\n    question_out = Dense(128, activation='relu')(question_hidden)\n    answer_out = Dense(128, activation='relu')(answer_hidden)\n    \n    qa_pair_vector = Concatenate(axis=1)([question_out,answer_out,similarity_input])\n    qa_hidden = Dense(128,activation='relu')(qa_pair_vector)\n    qa_out = Dense(len(answer_labels),activation='sigmoid')(Concatenate(axis=1)([qa_hidden,answer_hidden]))\n\n    qt_pair_vector = Concatenate(axis=1)([title_out,question_out])\n    qt_hidden = Dense(128,activation='relu')(qt_pair_vector)\n    qt_out = Dense(len(question_labels),activation='sigmoid')(Concatenate(axis=1)([qt_hidden,question_hidden]))\n    \n    use_output = Concatenate(axis=1)([qt_out,qa_out])\n    \n    #TF-IDF Model\n    tfidf_title_hidden = Dense(256, activation='relu')(tfidf_title_input)\n    tfidf_title_hidden = Dropout(0.2)(tfidf_title_hidden)\n    tfidf_question_hidden = Dense(256, activation='relu')(tfidf_question_input)\n    tfidf_question_hidden = Dropout(0.2)(tfidf_question_hidden)\n    tfidf_answer_hidden = Dense(256, activation='relu')(tfidf_answer_input)\n    tfidf_title_hidden = Dropout(0.2)(tfidf_title_hidden)\n    \n    tfidf_title_out = Dense(128, activation='relu')(tfidf_title_hidden)\n    tfidf_question_out = Dense(128, activation='relu')(tfidf_question_hidden)\n    tfidf_answer_out = Dense(128, activation='relu')(tfidf_answer_hidden)\n    \n    tfidf_qa_pair_vector = Concatenate(axis=1)([tfidf_question_out,tfidf_answer_out,similarity_input])\n    tfidf_qa_hidden = Dense(128,activation='relu')(tfidf_qa_pair_vector)\n    tfidf_qa_out = Dense(len(answer_labels),activation='sigmoid')(Concatenate(axis=1)([tfidf_qa_hidden,tfidf_answer_hidden]))\n\n    tfidf_qt_pair_vector = Concatenate(axis=1)([tfidf_title_out,tfidf_question_out])\n    tfidf_qt_hidden = Dense(128,activation='relu')(tfidf_qt_pair_vector)\n    tfidf_qt_out = Dense(len(question_labels),activation='sigmoid')(Concatenate(axis=1)([tfidf_qt_hidden,tfidf_question_hidden]))\n    \n    tfidf_output = Concatenate(axis=1)([tfidf_qt_out,tfidf_qa_out])\n    \n    \n    output = keras.layers.Average()([use_output,tfidf_output])\n    \n    model = Model(inputs=inp, outputs=output)\n    \n    model.compile(\n        optimizer=Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n    print(similarity_input.shape)\n    model.summary()\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_predictions = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = crazy_model()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = crazy_model()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n    \nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n    model.fit(X_tr, y_tr)\n    all_predictions.append(model.predict(X_test))\n    \nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\nmodel.fit(X_train, y_train)\nall_predictions.append(model.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds/max_val + 1e-12","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub[labels] = test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}