{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This kernel from [https://www.kaggle.com/kyakovlev/preprocessing-bert-public/data#data](https://www.kaggle.com/kyakovlev/preprocessing-bert-public/data#data)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"GOOGLE_PATH = \"/kaggle/input/google-quest-challenge/\"\n\ntrain = pd.read_csv(GOOGLE_PATH+\"train.csv\")\ntest = pd.read_csv(GOOGLE_PATH+\"test.csv\")\nsample_submission = pd.read_csv(GOOGLE_PATH+\"sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def url_id_ex(url):\n    try:\n        ids = int(url.split(\"/\")[-2])\n    except:\n        ids = int(url.split(\"/\")[-1])\n    return ids\n\n# category_type\ntrain[\"category_type\"] = train[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"/\")[-1])\ntest[\"category_type\"] = test[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"/\")[-1])\n\ntrain[\"quser_id\"] = train[\"question_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntrain[\"auser_id\"] = train[\"answer_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntrain[\"url_id\"] = train[\"url\"].apply(url_id_ex)\n\ntest[\"quser_id\"] = test[\"question_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntest[\"auser_id\"] = test[\"answer_user_page\"].apply(lambda x: int(x.split(\"/\")[-1]))\ntest[\"url_id\"] = test[\"url\"].apply(url_id_ex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## bert preprocess","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata\n\n# custom imports\nfrom gensim.utils import deaccent\nfrom collections import Counter\nfrom bs4 import BeautifulSoup\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 10\npd.options.display.max_colwidth = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################################################\n## Multiprocessing Run.\n# :df - DataFrame to split                      # type: pandas DataFrame\n# :func - Function to apply on each split       # type: python function\n# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\ndef df_parallelize_run(df, func):\n    num_partitions, num_cores = 16, psutil.cpu_count()  # number of partitions and cores\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\n## Build of vocabulary from file - reading data line by line\n## Line splited by 'space' and we store just first argument - Word\n# :path - txt/vec/csv absolute file path        # type: str\ndef get_vocabulary(path):\n    with open(path) as f:\n        return [line.strip().split()[0] for line in f][0:]\n\n## Check how many words are in Vocabulary\n# :c_list - 1d array with 'comment_text'        # type: pandas Series\n# :vocabulary - words in vocabulary to check    # type: list of str\n# :response - type of response                  # type: str\ndef check_vocab(c_list, vocabulary, response='default'):\n    try:\n        words = set([w for line in c_list for w in line.split()])\n        u_list = words.difference(set(vocabulary))\n        k_list = words.difference(u_list)\n    \n        if response=='default':\n            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n        elif response=='unknown_list':\n            return list(u_list)\n        elif response=='known_list':\n            return list(k_list)\n    except:\n        return []\n        \n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n    if 'torch' in sys.modules:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n \n## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n    \n## Export pickle\ndef make_export(tr, tt, file_name):\n    train_export = train[['id']]\n    test_export = test[['id']]\n\n    try:\n        cur_shape = tr.shape[1]>1\n        train_export = pd.concat([train_export, tr], axis=1)\n        test_export = pd.concat([test_export, tt], axis=1)        \n    except:\n        train_export['p_comment'] = tr\n        test_export['p_comment'] = tt\n    \n    train_export.to_pickle(file_name + '_x_train.pkl')\n    test_export.to_pickle(file_name + '_x_test.pkl')\n\n## Domain Search\nre_3986_enhanced = re.compile(r\"\"\"\n        # Parse and capture RFC-3986 Generic URI components.\n        ^                                    # anchor to beginning of string\n        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n             (?P<path>        [^?#\\s]*)      # capture required path\n        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n        $                                    # anchor to end of string\n        \"\"\", re.MULTILINE | re.VERBOSE)\n\nre_domain =  re.compile(r\"\"\"\n        # Pick out top two levels of DNS domain from authority.\n        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n        (?::[0-9]*)?                      # Optional port number.\n        $                                 # Anchor to end of string.\n        \"\"\", \n        re.MULTILINE | re.VERBOSE)\n\ndef domain_search(text):\n    try:\n        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n    except:\n        return 'url'\n\n## Load helper helper))\ndef load_helper_file(filename):\n    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n        temp_obj = pickle.load(f)\n    return temp_obj\n        \n## Preprocess helpers\ndef place_hold(w):\n    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n\ndef check_replace(w):\n    return not bool(re.search(WPLACEHOLDER, w))\n\ndef make_cleaning(s, c_dict):\n    if check_replace(s):\n        s = s.translate(c_dict)\n    return s\n  \ndef make_dict_cleaning(s, w_dict):\n    if check_replace(s):\n        s = w_dict.get(s, s)\n    return s\n\ndef export_dict(temp_dict, serial_num):\n    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n\ndef print_dict(temp_dict, n_items=10):\n    run = 0\n    for k,v in temp_dict.items():\n        print(k,'---',v)\n        run +=1\n        if run==n_items:\n            break    \n## ----------------------------------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#################################################################################\nHELPER_PATH             = '../input/jigsaw-general-helper-public/'\n\nLOCAL_TEST = True       ## Local test - for test performance on part of the train set only\nSEED = 42               ## Seed for enviroment\nseed_everything(SEED)   ## Seed everything\n\nWPLACEHOLDER = 'word_placeholder'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_text = train[\"question_title\"] + \" \" + train[\"question_body\"]+ \" \" + train[\"answer\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n########################### Get basic helpers\n#################################################################################\nprint('1.2. Basic helpers')\nbert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\nbert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\nbert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n\nurl_extensions          = load_helper_file('helper_url_extensions')\nhtml_tags               = load_helper_file('helper_html_tags')\ngood_chars_dieter       = load_helper_file('helper_good_chars_dieter')\nbad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\nhelper_contractions     = load_helper_file('helper_contractions')\nglobal_vocabulary       = load_helper_file('helper_global_vocabulary')\nglobal_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\nnormalized_chars        = load_helper_file('helper_normalized_chars')\nwhite_list_chars        = load_helper_file('helper_white_list_chars')\nwhite_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\npictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\ntoxic_misspell_dict     = load_helper_file('helper_toxic_misspell_dict')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = total_text\nlocal_vocab = bert_uncased_vocabulary\nverbose = True\nglobal_lower=True\ndata = data.astype(str)\nif verbose: print('#' *20 ,'Initial State:'); check_vocab(data, local_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if global_lower:\n    data = data.apply(lambda x: x.lower())\n    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(data, local_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize chars and dots - SEE HELPER FOR DETAILS\n# Global\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\ndata = data.apply(lambda x: re.sub('\\(dot\\)', '.', x))\ndata = data.apply(lambda x: deaccent(x))\nif verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(data, local_vocab)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove 'control' chars\n# Global    \nglobal_chars_list = list(set([c for line in data for c in line]))\nchars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(data, local_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove hrefs\n# Global    \ndata = data.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\nif verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(data, local_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert or remove Bad Symbols\n# Global\nglobal_chars_list = list(set([c for line in data for c in line]))\nchars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\nchars_dict = {}\nfor char in chars:\n    try:\n        new_char = unicodedata.name(char).split()[-1:][0].lower()\n        if len(new_char)==1:\n            chars_dict[ord(char)] = new_char\n        else:\n            chars_dict[ord(char)] = ''\n    except:\n        chars_dict[ord(char)] = ''\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(data, local_vocab)\nif verbose: print(chars)\nif verbose: print_dict(chars_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove Bad Symbols PART 2\n# Global\nglobal_chars_list = list(set([c for line in data for c in line]))\nchars = 'Â·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\nchars_dict = {}\nfor char in chars:\n    try:\n        new_char = unicodedata.name(char).split()[-1:][0].lower()\n        if len(new_char)==1:\n            chars_dict[ord(char)] = new_char\n        else:\n            chars_dict[ord(char)] = ''\n    except:\n        chars_dict[ord(char)] = ''\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(data, local_vocab)\nif verbose: print(chars)\nif verbose: print_dict(chars_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if ('<' in word) and ('>' in word):\n        for tag in html_tags:\n            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n                temp_dict[word] = BeautifulSoup(word, 'html5lib').text  \ndata = data.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove links (There is valuable information in links (probably you will find a way to use it)) \n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\nurl_rule = r'(?P<url>https?://[^\\s]+)'\ntemp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n    \nfor word in temp_dict:\n    new_value = temp_dict[word]\n    if word.find('http')>2:\n        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n    else:\n        temp_dict[word] = place_hold(new_value)\n            \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert urls part 2\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\n\nfor word in temp_vocab:\n    url_check = False\n    if 'file:' in word:\n        url_check = True\n    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n        if 'Aww' not in word:\n            for d_zone in url_extensions:\n                if '.' + d_zone in word:\n                    url_check = True\n                    break            \n    elif ('/' in word) and ('.' in word):\n        for d_zone in url_extensions:\n            if '.' + d_zone + '/' in word:\n                url_check = True\n                break\n\n    if url_check:\n        temp_dict[word] =  place_hold(domain_search(word))\n        \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize pictograms\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n        for pict in pictograms_to_emoji:\n            if (pict in word) and (len(pict)>2):\n                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n            elif pict==word:  \n                temp_dict[word] = pictograms_to_emoji[pict]\n\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate emoji\n# Global\nglobal_chars_list = list(set([c for line in data for c in line]))\nchars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\nchars_dict = {ord(c):f' {c} ' for c in chars}\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(data, local_vocab)\nif verbose: print(chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Duplicated dots, question marks and exclamations\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n        if (Counter(word)['.']>1):\n            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n        if (Counter(word)['!']>1):\n            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n        if (Counter(word)['?']>1):\n            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n        if (Counter(word)[',']>1):\n            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n        temp_dict[word] = new_word\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(data, local_vocab);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove underscore for spam words\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n        temp_dict[word] = re.sub('_', '', word)       \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate spam chars repetition\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize pictograms part 2\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n        for pict in pictograms_to_emoji:\n            if pict==word:  \n                temp_dict[word] = pictograms_to_emoji[pict]\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate brakets and quotes\n# Global\nchars = '()[]{}<>\"'\nchars_dict = {ord(c):f' {c} ' for c in chars}\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(data, local_vocab)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Break short words\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_vocab = [k for k in temp_vocab if len(k)<=20]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    if '/' in word:\n        temp_dict[word] = re.sub('/', ' / ', word)\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Break long words\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_vocab = [k for k in temp_vocab if len(k)>20]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    if '_' in word:\n        temp_dict[word] = re.sub('_', ' ', word)\n    elif '/' in word:\n        temp_dict[word] = re.sub('/', ' / ', word)\n    elif len(' '.join(word.split('-')).split())>2:\n        temp_dict[word] = re.sub('-', ' ', word)\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove/Convert usernames and hashtags (add username/hashtag word?????)\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n        if word[len(word)-1].isalnum():\n            if (word.startswith('@')) or (word.startswith('#')):\n                new_word = place_hold(new_word[0] + ' ' + new_word[1:]) \n        else:\n            if (word.startswith('@')) or (word.startswith('#')):\n                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n\n    temp_dict[word] = new_word\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove ending underscore (or add quotation marks???)\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if word[len(word)-1]=='_':\n        for i in range(len(word),0,-1):\n            if word[i-1]!='_':\n                new_word = word[:i]\n                temp_dict[word] = new_word   \n                break\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove starting underscore \n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if word[0]=='_':\n        for i in range(len(word)):\n            if word[i]!='_':\n                new_word = word[i:]\n                temp_dict[word] = new_word   \n                break\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# End word punctuations\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    for i in range(len(word),0,-1):\n        if word[i-1].isalnum():\n            new_word = word[:i] + ' ' + word[i:]\n            break\n    temp_dict[word] = new_word     \ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)                           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start word punctuations\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    for i in range(len(word)):\n        if word[i].isalnum():\n            new_word = word[:i] + ' ' + word[i:]\n            break\n    temp_dict[word] = new_word     \ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find and replace acronims\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if (Counter(word)['.']>1) and (check_replace(word)):\n        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n            temp_dict[word] = place_hold('url ' + domain_search(word))\n        else: \n            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply spellchecker for contractions\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if word in helper_contractions:\n        temp_dict[word] = place_hold(helper_contractions[word])\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(data, local_vocab)\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate obscene (or just keep the word????)\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word)\n    if len(Counter(new_word))>2:\n        temp_dict[word] = place_hold('fuck')\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Possible obscene:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove 's (DO WE NEED TO REMOVE IT???)\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)                                        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Convert backslash\n# Global\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]    \ntemp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(data, local_vocab)\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try remove duplicated chars (not sure about this!!!!!)\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\ntemp_vocab_dup = []\n    \nfor word in temp_vocab:\n    temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\ntemp_vocab_dup = set(temp_vocab_dup)\ntemp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n            \nfor word in temp_vocab:\n    new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n    if new_word in temp_vocab_dup:\n        temp_dict[word] = new_word\ntemp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Isolate numbers\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if re.compile('[a-zA-Z]').sub('', word) == word:\n        if re.compile('[0-9]').sub('', word) != word:\n            temp_dict[word] = word\n\nglobal_chars_list = list(set([c for line in temp_dict for c in line]))\nchars = ''.join([c for c in global_chars_list if not c.isdigit()])\nchars_dict = {ord(c):f' {c} ' for c in chars}                \ntemp_dict = {k:place_hold(make_cleaning(k,chars_dict)) for k in temp_dict}\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join dashes\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    temp_dict[word] = re.sub('\\-\\-+', '-', word)\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try join word (Sloooow)\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = ''.join(['' if c in '-' else c for c in word])\n    if (new_word in local_vocab) and (len(new_word)>3):\n        temp_dict[word] = new_word    \n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try Split word\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# L33T vocabulary (SLOW)\n# https://simple.wikipedia.org/wiki/Leet\n# Local (only unknown words)\ndef convert_leet(word):\n    # basic conversion \n    word = re.sub('0', 'o', word)\n    word = re.sub('1', 'i', word)\n    word = re.sub('3', 'e', word)\n    word = re.sub('\\$', 's', word)\n    word = re.sub('\\@', 'a', word)\n    return word\n            \ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = convert_leet(word)\n    if (new_word!=word): \n        if (len(word)>2) and (new_word in local_vocab):\n            temp_dict[word] = new_word\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Open Holded words\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if (not check_replace(k))]\ntemp_dict = {}\nfor word in temp_vocab:\n    temp_dict[word] = re.sub('___', ' ', word[17:-1])\ndata = data.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\ndata = data.apply(lambda x: ' '.join([i for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(data, local_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Search multiple form\n# Local | example -> flashlights / flashlight -> False / True\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\ntemp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)                                                                      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert emoji to text\n# Local \ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (k in emoji.UNICODE_EMOJI)]\ntemp_dict = {}\nfor word in temp_vocab:\n    temp_dict[word] = re.compile('[:_]').sub(' ', emoji.UNICODE_EMOJI.get(word)) \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert emoji to text:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}