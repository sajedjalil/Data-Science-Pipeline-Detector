{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n<font size=4> This Notebook gives u guys a gentle introduction towards the QA labeling Competition, which includes a detailed EDA about the dataset and a benchmark Keras DNN model with Word2Vec. Wish you Happy Kaggling!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gensim\nimport matplotlib.pyplot as plt\nimport random\nimport gc\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nrandom.seed(1234)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/google-quest-challenge/train.csv')\ntest_df = pd.read_csv('../input/google-quest-challenge/test.csv')\n\n# the dataset size\nprint('train set size is %d' % len(train_df))\nprint('test set size is %d' % len(test_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_cols = [\n    'question_title', \n    'question_body', \n    'question_user_name', \n    'question_user_page', \n    'answer', \n    'answer_user_name', \n    'answer_user_page', \n    'url', \n    'category', \n    'host']\ntarget_cols = [\n    'question_asker_intent_understanding', \n    'question_body_critical', \n    'question_conversational', \n    'question_expect_short_answer', \n    'question_fact_seeking', \n    'question_has_commonly_accepted_answer',\n    'question_interestingness_others', \n    'question_interestingness_self', \n    'question_multi_intent', \n    'question_not_really_a_question', \n    'question_opinion_seeking', \n    'question_type_choice',\n    'question_type_compare', \n    'question_type_consequence', \n    'question_type_definition', \n    'question_type_entity', \n    'question_type_instructions', \n    'question_type_procedure', \n    'question_type_reason_explanation', \n    'question_type_spelling',\n    'question_well_written', \n    'answer_helpful', \n    'answer_level_of_information', \n    'answer_plausible', \n    'answer_relevance', \n    'answer_satisfaction', \n    'answer_type_instructions', \n    'answer_type_procedure', \n    'answer_type_reason_explanation',\n    'answer_well_written'\n]\n\nprint('we have %d feature columns and %d target columns' % (len(feat_cols) , len(target_cols)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4>Now a quick look for one sample</font>"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"peek = train_df.sample()\ntext_cols = [\n    'question_title',\n    'question_body',\n    'answer'\n]\n\nfor col_name in feat_cols + target_cols:\n    print(col_name)\n    print('='* 10)\n    print(str(peek[col_name].values[0]) + '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inspect Feature Variables"},{"metadata":{},"cell_type":"markdown","source":"## 1. question_title"},{"metadata":{},"cell_type":"markdown","source":"<font size=4> Here are many duplicated question titles. The higest one named 'What is the best introductory Bayesian statistics textbook?' attracts 12 answers in the trainset.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('question_title').count()['qa_id'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a closer look to the most popular question\ntrain_df[train_df['question_title'] == 'What is the best introductory Bayesian statistics textbook?'][feat_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NOTE:**\n<font size=4>\n    <p>*question_title*, *question_body*, *question_user_page*, *url* and etc. are bounded together.</p>\n    <p>ALSO The asker might answer his own question [see row 1647 in the above example].</p>\n</font>"},{"metadata":{},"cell_type":"markdown","source":"## 2. Category & Host\n<font size=4> The Distribution over category and host</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('category').count()['qa_id'].sort_values(ascending=False).plot(kind='bar', alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('host').count()['qa_id'].sort_values(ascending=False).plot(kind='bar', figsize=(16, 6), fontsize=15, alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Answer User Name & Question User Name\n<font size=4> Who is the most active user </font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=4> Here lists the top5 active answer user"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('answer_user_name').count()['qa_id'].sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4> **Scott** seems not just one person."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train_df[train_df['answer_user_name'] == 'Scott'][['answer_user_name', 'question_title', 'category', 'host']].sort_values(by='category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------\n<font size=4 >Here lists the top5 active question user"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby(['question_user_name']).count()['qa_id'].sort_values(ascending=False).iloc[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4>A closer look to Mike</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['question_user_name'] == 'Mike'][['question_user_name', 'question_title', 'category', 'host']].sort_values(by=['category', 'question_title'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Text"},{"metadata":{},"cell_type":"markdown","source":"<font size=4> Let's see the distribution of text length"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\nTOKENIZER = RegexpTokenizer(r'\\w+')\nSTOPWORDS = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['question_title_len'] = train_df['question_title'].map(lambda x: len(TOKENIZER.tokenize(x)))\ntrain_df['question_body_len'] = train_df['question_body'].map(lambda x: len(TOKENIZER.tokenize(x)))\ntrain_df['answer_len'] = train_df['answer'].map(lambda x: len(TOKENIZER.tokenize(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[['answer_len', 'question_body_len', 'question_title_len']].plot(kind='box', showfliers=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4>Now see the word distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_word_cloud(col):\n    rows = train_df[col].map(lambda x: TOKENIZER.tokenize(x)).values.tolist()\n    words = []\n    for row in rows:\n        for w in row:\n            if w not in STOPWORDS:\n                words.append(w.lower())\n    \n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(words))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n\ngen_word_cloud('question_title')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_word_cloud('question_body')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_word_cloud('answer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------------"},{"metadata":{},"cell_type":"markdown","source":"# Inspect Target Variable"},{"metadata":{},"cell_type":"markdown","source":"## 1. Variable Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nquestion_related_target_cols = [ col for col in target_cols if re.search('^question_', col)]\nanswer_related_target_cols = [ col for col in target_cols if re.search('^answer_', col)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[answer_related_target_cols[:5]].plot(kind='hist', figsize=(12, 6), alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[question_related_target_cols[:5]].plot(kind='hist', figsize=(12, 6), alpha=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Variable Correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nsns.heatmap(data=train_df[answer_related_target_cols].corr(), \n            square=True, \n            annot=True,\n            linewidths=1, \n            cmap=sns.color_palette(\"Blues\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nsns.heatmap(data=train_df[question_related_target_cols].corr(), \n            square=True, \n            linewidths=1, \n            cmap=sns.color_palette(\"Blues\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target vs Features"},{"metadata":{},"cell_type":"markdown","source":"## 1. target vs Category"},{"metadata":{},"cell_type":"markdown","source":"<font size=4> We compute the averages of several target values for each category, and display them with std as error bars. It seems some question related target variables are slighted affected by the category, while the question related ones are not. "},{"metadata":{"trusted":true},"cell_type":"code","source":"std = train_df.groupby('category')[question_related_target_cols[:8]].std()\ntrain_df.groupby('category')[question_related_target_cols[:8]].mean().plot(kind='bar', figsize=(16, 8), \n                                                                           yerr=std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std = train_df.groupby('category')[answer_related_target_cols].std()\ntrain_df.groupby('category')[answer_related_target_cols].mean().plot(kind='bar', figsize=(16, 8), \n                                                                           yerr=std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. target vs Host\n<font size=4> we filter out some unfrequent host at first"},{"metadata":{"trusted":true},"cell_type":"code","source":"frequent_hosts = set(train_df.groupby('host').count()['qa_id'].sort_values(ascending=False)[:10].index)\nidx = train_df['host'].map(lambda x: x in frequent_hosts)\ntrain_subset = train_df[idx]\n\nstd = train_subset.groupby('host')[answer_related_target_cols].std()\ntrain_subset.groupby('host')[answer_related_target_cols].mean().plot(kind='bar', figsize=(16, 8), yerr=std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std = train_subset.groupby('host')[question_related_target_cols[:8]].std()\ntrain_subset.groupby('host')[question_related_target_cols[:8]].mean().plot(kind='bar', figsize=(16, 8), yerr=std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=4> Still, some question realted target (*question_converstional*, *qustion_interestingness_self*, etc.) are sensitive to the *host*"},{"metadata":{},"cell_type":"markdown","source":"## 3. target vs Text Len"},{"metadata":{},"cell_type":"markdown","source":"<font size=4> the correlation between answer_level_of_information and answer_lne is 0.31.. make sense hah."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\ntext_len_cols = ['question_title_len', 'question_body_len', 'answer_len']\ncorr_with_text_len = train_df[answer_related_target_cols + text_len_cols].corr().loc[text_len_cols, answer_related_target_cols]\nsns.heatmap(data=corr_with_text_len.T, \n            square=True, \n            linewidths=1, \n            annot=True,\n            cmap=sns.color_palette(\"Blues\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 12))\ncorr_with_text_len = train_df[question_related_target_cols + text_len_cols].corr().loc[text_len_cols, question_related_target_cols]\nsns.heatmap(data=corr_with_text_len.T, \n            square=True, \n            linewidths=1, \n            annot=True,\n            cmap=sns.color_palette(\"Blues\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------------------"},{"metadata":{},"cell_type":"markdown","source":"# Benchmark with Word2Vec\n"},{"metadata":{},"cell_type":"markdown","source":"## 1. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"<font size=4> <p> The techniques for text vectorization we use in this part are:\n* TFIDF \n* SVD -- dimension reduction for the TFIDF weights --> the dimension for the dense representation is 300d\n* Word2Vec (300d google news) -- we use the average over the word-vectors of each non-stop word in each passage of text </p>\n\n<p> We also include the length of text as a feature. </p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load w2v model, this might take a few moments, grab a coffee and relax\nw2v_model = gensim.models.KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin', binary=True, unicode_errors='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TFIDF_SVD_WORDVEC_DIM = 300\n\ndef get_text_feats(df, col):\n\n    def tokenize_downcase_filtering(x):\n        words = TOKENIZER.tokenize(x)\n        lower_case = map(lambda w: w.lower(), words)\n        content_words = filter(lambda w: w not in STOPWORDS, lower_case)\n        return ' '.join(content_words)\n\n    rows = df[col].map(tokenize_downcase_filtering).values.tolist()\n    tfidf = TfidfVectorizer(tokenizer=lambda x: x.split(' '))  # dont use sklearn default tokenization tool \n    tfidf_weights = tfidf.fit_transform(rows)\n    svd = TruncatedSVD(n_components=TFIDF_SVD_WORDVEC_DIM, n_iter=10)  # reduce dimensionality\n    dense_tfidf_repr_mat = svd.fit_transform(tfidf_weights)\n    \n    word2vec_repr_mat = np.zeros((len(df), w2v_model.vector_size))\n    for i, row in enumerate(rows):\n        word2vec_accum = np.zeros((w2v_model.vector_size, ))\n        word_cnt = 0\n        for w in row.split(' '):\n            if w in w2v_model.wv:\n                word2vec_accum += w2v_model.wv[w]\n                word_cnt += 1\n\n        # compute the average for the wordvec of each non-sptop word\n        if word_cnt != 0:\n            word2vec_repr_mat[i] = word2vec_accum / word_cnt\n\n    return  np.concatenate([word2vec_repr_mat, dense_tfidf_repr_mat], axis=1)  # word2vec + tfidf\n\n\ndef one_hot_feats(df, col):\n    return pd.get_dummies(df['host'], prefix='host', drop_first=True).values\n\n\n# let's build features\ndf_all = pd.concat((train_df, test_df))\ndf_all['question_title_len'] = df_all['question_title'].map(lambda x: len(TOKENIZER.tokenize(x)))\ndf_all['question_body_len'] = df_all['question_body'].map(lambda x: len(TOKENIZER.tokenize(x)))\ndf_all['answer_len'] = df_all['answer'].map(lambda x: len(TOKENIZER.tokenize(x)))\n\ndata = []\nfor col in text_cols:\n    data.append(get_text_feats(df_all, col))\n\nfor col in ['category', 'host']:\n    data.append(one_hot_feats(df_all, col))\n\ndata.append(df_all[text_len_cols].values)\ndata = np.concatenate(data, axis=1)\n\ntrain_feats = data[:len(train_df)]\ntest_feats = data[len(train_df):]\n\n# del w2v_model\n# gc.collect()\nprint(train_feats.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Build a DNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# code from https://www.kaggle.com/ryches/tfidf-benchmark\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.model_selection import KFold\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\n\nnum_folds = 5\nfold_scores = []\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=9102)\n\ntest_preds = np.zeros((len(test_feats), len(target_cols)))\nfor train_index, val_index in kf.split(train_feats):\n    train_X = train_feats[train_index, :]\n    train_y = train_df[target_cols].iloc[train_index]\n    \n    val_X = train_feats[val_index, :]\n    val_y = train_df[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(512, input_shape=(train_feats.shape[1],)),\n        Activation('tanh'),\n        Dense(256),\n        Activation('tanh'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs=50, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    print('-'* 10)\n    for i, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, i], val_y[col].values).correlation / len(target_cols)\n        print('%s\\t%.5f' % (col, spearmanr(preds[:, i], val_y[col].values).correlation))\n\n    fold_scores.append(overall_score)\n    test_preds += model.predict(test_feats) / num_folds\n    \nprint(fold_scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\nfor i, col in enumerate(target_cols):\n    sub[col] = test_preds[:, i]\nsub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}