{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nI will add new information to TFIDF+NN model(https://www.kaggle.com/ryches/tfidf-benchmark ).<br>\nTFIDF can create features based on actual vocabulary, but it can't handle well when there is another word of close meaning.<br>\nTherefore, I thought that adding SWEM(https://arxiv.org/abs/1805.09843) using learned word2vec as a feature value would increase the score."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim\nfrom nltk.corpus import brown\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\nfrom nltk.corpus import wordnet as wn\nimport tqdm\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# fearure engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_prepro(s):\n    return [w for w in s.replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def simple_prepro_tfidf(s):\n    return \" \".join([w for w in s.lower().replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is basic preprocessing. This time, symbols and words are attached, so they are separated here."},{"metadata":{"trusted":true},"cell_type":"code","source":"qt_max = max([len(simple_prepro(l)) for l in list(train[\"question_title\"].values)])\nqb_max = max([len(simple_prepro(l))  for l in list(train[\"question_body\"].values)])\nan_max = max([len(simple_prepro(l))  for l in list(train[\"answer\"].values)])\nprint(\"max lenght of question_title is\",qt_max)\nprint(\"max lenght of question_body is\",qb_max)\nprint(\"max lenght of question_answer is\",an_max)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The text is so long that it is difficult to apply RNN to all series."},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = gensim.models.Word2Vec(brown.sents())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we use a trained word2vec model that is easily available with nltk.<br>\nWe used SWEM with max pooling.<br>\nHere, add information about the length of the sentence and the number of line ,'&nbsp;&nbsp;' ,'?' and '!'.<br>\nConsecutive spaces can be useful information."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_word_embeddings(text):\n    np.random.seed(abs(hash(text)) % (10 ** 8))\n    words = simple_prepro(text)\n    vectors = np.zeros((len(words),100))\n    if len(words)==0:\n        vectors = np.zeros((1,100))\n    for i,word in enumerate(simple_prepro(text)):\n        try:\n            vectors[i]=w2v_model[word]\n        except:\n            vectors[i]=np.random.uniform(-0.01, 0.01,100)\n    return np.concatenate([np.max(np.array(vectors), axis=0),\n                          np.array([min(len(text),5000)/5000,\n                                    min(text.count(\" \"),5000)/5000,\n                                    min(len(words),1000)/1000,\n                                    min(text.count(\"\\n\"),100)/100,\n                                   min(text.count(\"!\"),20)/20,\n                                   min(text.count(\"?\"),20)/20])])\n                           ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"question_title = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_title\"].values)]\nquestion_title_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_title\"].values)]\n\nquestion_body = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_body\"].values)]\nquestion_body_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_body\"].values)]\n\nanswer = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"answer\"].values)]\nanswer_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"answer\"].values)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From here on, I'm quite referring to https://www.kaggle.com/ryches/tfidf-benchmark."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"gc.collect()\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 60)\ntfidf_question_title = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_title\"].values)])\ntfidf_question_title_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_title\"].values)])\ntfidf_question_title = tsvd.fit_transform(tfidf_question_title)\ntfidf_question_title_test = tsvd.transform(tfidf_question_title_test)\n\ntfidf_question_body = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_body\"].values)])\ntfidf_question_body_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_body\"].values)])\ntfidf_question_body = tsvd.fit_transform(tfidf_question_body)\ntfidf_question_body_test = tsvd.transform(tfidf_question_body_test)\n\ntfidf_answer = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"answer\"].values)])\ntfidf_answer_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"answer\"].values)])\ntfidf_answer = tsvd.fit_transform(tfidf_answer)\ntfidf_answer_test = tsvd.transform(tfidf_answer_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type2int = {type:i for i,type in enumerate(list(set(train[\"category\"])))}\ncate = np.identity(5)[np.array(train[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)\ncate_test = np.identity(5)[np.array(test[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = np.concatenate([question_title, question_body, answer,\n                                 tfidf_question_title, tfidf_question_body, tfidf_answer, \n                                 cate\n                                ], axis=1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test, \n                               tfidf_question_title_test, tfidf_question_body_test, tfidf_answer_test,\n                                cate_test\n                                ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"num_folds = 10\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nvalid_preds = np.zeros((train_features.shape[0],30))\nfor train_index, val_index in kf.split(train_features):\n    gc.collect()\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(1024, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(512),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 300, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    valid_preds[val_index] = preds\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n    test_preds += model.predict(test_features)/num_folds\nprint(fold_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid = 0\nfor col_index, col in enumerate(target_cols):\n    valid += spearmanr(valid_preds[:, col_index], train[col].values).correlation/30\nprint(\"valid score is \",valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\nfor col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]\nsub.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The contribution of the score was not great, but if you use bert etc. instead of brown, I think the score will go up more."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}