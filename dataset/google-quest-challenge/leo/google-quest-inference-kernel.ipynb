{"cells":[{"metadata":{},"cell_type":"markdown","source":"## with out postprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python ../input/gq-inf-scripts/tf_bert_large_inf.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python ../input/gq-inf-scripts/pytorch_bert_inf.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python ../input/gq-inf-scripts/pytorch_albert_inf.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python ../input/gq-inf-scripts/tf_bert_base_inf.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !python ../input/gq-inf-scripts/pytorch_base_inf.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python ../input/gq-inf-scripts/pytorch_feat_inf.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom scipy.optimize import minimize\nfrom collections import defaultdict\n\nkf = np.load(\"../input/split-dataset/dataset_indexs.npy\", allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/google-quest-challenge/train.csv')\nsub_df = pd.read_csv('../input/google-quest-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model_names = [\"albert\", \"bert_base\", \"feat_multi\", \"feat_nn\", \"pytorch_base\",\n#                \"tf_bert_base_epoch1\", \"tf_bert_base_epoch2\", \"tf_bert_base_epoch3\"]\nmodel_names = [\"albert\", \"bert_base\", \"tf_bert_large\", \"feat_multi\", \"feat_nn\",\n               \"tf_bert_base_epoch1\", \"tf_bert_base_epoch2\", \"tf_bert_base_epoch3\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_true, col_pred in zip(trues.T, preds.T):\n        rhos.append(spearmanr(col_true, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n# def compute_spearmanr(trues, preds, columns=None):\n#     rhos = []\n#     for i, (col_true, col_pred) in enumerate(zip(trues.T, preds.T)):\n#         if i == 0:\n#             # qa_id\n#             continue\n#         r = spearmanr(col_true, col_pred).correlation\n#         if columns:\n#             print(\"{:.3f}\\t{}\".format(r, columns[i]))\n#         rhos.append(r)\n#     return np.mean(rhos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = list(sub_df.columns)\n\nresults = []\nfor fold_i in range(1, 6):\n    result_df = pd.DataFrame(index=model_names, columns=target_columns[-30:] + [\"ave\"])\n    for model_name in model_names:\n        for target in target_columns[-30:]:\n            true = train_df[target_columns].iloc[kf[fold_i-1][1]].reset_index(drop=True)\n            oof = pd.read_csv(model_name + f\"/oof{fold_i}.csv\")\n            r = spearmanr(true[target].values, oof[target].values + np.random.normal(0, 1e-7, oof.shape[0])).correlation\n            result_df.loc[model_name, target] = r\n        result_df.loc[model_name, \"ave\"] = np.mean(result_df.loc[model_name, target_columns[-30:]].values)\n    results.append(result_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize model weight\nkfold = 5\nmodel_number = len(model_names)\n\ntrue_dfs = []\nfor i in range(5):\n    oof = train_df.iloc[kf[i][1]]\n    true_dfs.append(oof[target_columns])\n\nmodel_df = defaultdict(lambda: [])\nfor model_name in model_names:\n    print(model_name)\n\n    scores = []\n    pred_dfs = []\n\n    for i in range(5):\n        pred = pd.read_csv(f\"{model_name}/oof{i+1}.csv\")\n        model_df[model_name].append(pred)\n        true = true_dfs[i]\n        rho = compute_spearmanr(true.values, pred.values)\n        scores.append(rho)\n        pred_dfs.append(pred.copy())\n    print([f\"fold{i+1}: {scores[i]:.4}\" for i in range(5)])\n    \nscores = [{} for _ in range(kfold)]\nall_coeffs = []\n\nfor col_num, col_name in enumerate(target_columns[-30:]):\n    coefficients = np.zeros((kfold, model_number))\n    print(col_name, \"\\n\")\n    for fold_num, (_, vld_idx) in enumerate(kf): \n        print(\"\\n\"+\"-\"*50+'\\n[Fold {}/{}]'.format(fold_num + 1, kfold))\n        # optimize function\n        def function_spearmanr(x):\n            true = true_dfs[fold_num].loc[:, col_name].values\n            pred = np.zeros(len(true))\n            for k, (model_name, df) in enumerate(model_df.items()):\n                pred += x[k] * df[fold_num].loc[:, col_name].values\n            return -spearmanr(true, pred).correlation\n\n        # initial weights\n        x0 = [1 for _ in range(model_number)]\n        # optimize\n        res = minimize(function_spearmanr, x0, method='nelder-mead', options={'xtol': 1e-4, 'disp': True})\n        # normalize sum(weights) = 1.0\n        coeffs = [max(val, 0) for val in res.x]\n        coefficients[fold_num,:] = coeffs / np.sum(coeffs)\n        print('\\nfinal vector: {}'.format(coefficients[fold_num,:]))\n\n        y_true = true_dfs[fold_num].loc[:, col_name].values\n        y_pred = np.zeros(len(y_true))\n        for k, (model_name, df) in enumerate(model_df.items()):\n            y_pred += coefficients[fold_num, k] * df[fold_num].loc[:, col_name].values\n\n        score = spearmanr(y_true, y_pred).correlation\n        scores[fold_num][col_name] = score\n        print(f\"valid's spearmanr: {score:<7.4f} \\n\")\n    all_coeffs.append(coefficients)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(np.array([list(s.values()) for s in scores]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensamble\nsub_df = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\nsubmission = np.zeros((len(sub_df), 30)).T\n\n# load each submission.csv\nFOLD_NUM = 5\ninf_sub = []\nfor fold_num in range(FOLD_NUM):\n    l = []\n    for model_num, model_name in enumerate(model_names):\n        l.append(pd.read_csv(f\"{model_name}/submission{fold_num+1}.csv\"))\n    inf_sub.append(l.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col_num, col_name in enumerate(target_columns[-30:]):\n    for fold_num in range(FOLD_NUM): \n        for model_num in range(len(model_names)):\n            coef = all_coeffs[col_num][fold_num][model_num]\n            submission[col_num] += coef / FOLD_NUM * inf_sub[fold_num][model_num][col_name].values\n\nsub_df.iloc[:, -30:] = submission.T\nsub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.max(sub_df.iloc[:, -30:].values), np.min(sub_df.iloc[:, -30:].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm_sub(df):\n    for col_name in df.columns[-30:]:\n        tmp_df = df[col_name].values\n        v_max = np.max(tmp_df) + 0.01\n        v_min = np.min(tmp_df) - 0.01\n        df[col_name] = df[col_name].apply(lambda x: (x - v_min) / (v_max - v_min))\n        df[col_name] = df[col_name].values + np.random.normal(0, 1e-7, len(df))\n    return df\nsub_df = norm_sub(sub_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.max(sub_df.iloc[:, -30:].values), np.min(sub_df.iloc[:, -30:].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # check bug\n# debug_sub_files = [\"submission_230.csv\", \"submission_253.csv\", \"submission_288.csv\", \"submission_301.csv\", \n#                    \"submission_305.csv\", \"submission_335.csv\", \"submission_342.csv\", \"submission_350.csv\"]\n# for model_num, model_name in enumerate(model_names):\n#     try:\n#         for i in range(1, 6):\n#             tmp = pd.read_csv(f\"{model_name}/oof{i}.csv\")\n#             tmp = pd.read_csv(f\"{model_name}/submission{i}.csv\")\n#     except:\n#         debug_sub = pd.read_csv(f\"../input/gq-debug/{debug_sub_files[model_num]}\")\n#         sub_df = pd.merge(sub_df[\"qa_id\"], debug_sub, on=\"qa_id\", how='outer')\n#         sub_df = sub_df.fillna(0)\n#         sub_df = norm_sub(sub_df)\n#         break\n# sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class color:\n   PURPLE = '\\033[95m'\n   CYAN = '\\033[96m'\n   DARKCYAN = '\\033[36m'\n   BLUE = '\\033[94m'\n   GREEN = '\\033[92m'\n   YELLOW = '\\033[93m'\n   RED = '\\033[91m'\n   BOLD = '\\033[1m'\n   UNDERLINE = '\\033[4m'\n   END = '\\033[0m'\n\ndef compute_spearmanr(trues, preds, columns=None):\n    rhos = []\n    for i, (col_true, col_pred) in enumerate(zip(trues.T, preds.T)):\n        if i == 0:\n            # qa_id\n            continue\n        r = spearmanr(col_true, col_pred).correlation\n        if columns:\n            print(\"{:.3f}\\t{}\".format(r, columns[i-1]))\n        rhos.append(r)\n    return np.mean(rhos)\n\ndef compare_spearmanr(trues, preds, pp_preds, columns=None):\n    rhos = []\n    pp_rhos = []\n    for i, (col_true, col_pred, col_pp_pred) in enumerate(zip(trues.T, preds.T, pp_preds.T)):\n        if i == 0:\n            # qa_id\n            continue\n        r = spearmanr(col_true, col_pred).correlation\n        pp_r = spearmanr(col_true, col_pp_pred).correlation\n        if columns:\n            if r < pp_r:\n                print(\"{:.3f} ->\".format(r) + color.BOLD +  \" {:.3f}\".format(pp_r) + color.END + \" \\t{}\".format(columns[i-1]))\n            elif (r - pp_r) < 10**(-6):\n                print(\"{:.3f} ->\".format(r) + \" {:.3f}\".format(pp_r) +  \" \\t{}\".format(columns[i-1]))\n            else:\n                print(\"{:.3f} ->\".format(r) + color.RED +  \" {:.3f}\".format(pp_r) + color.END + \" \\t{}\".format(columns[i-1]))\n\n        rhos.append(r)\n        pp_rhos.append(pp_r)\n    whole_base = np.mean(rhos)\n    whole_pp = np.mean(pp_rhos)\n    print(\"Whole Score\")\n    if whole_base < whole_pp:\n        print(\"{:.3f} ->\".format(whole_base) + color.BOLD +  \" {:.3f}\".format(whole_pp) + color.END + \" +{}\".format(abs(whole_pp - whole_base)))\n    elif (whole_base - whole_pp ) < 10**(-6):\n        print(\"{:.3f} ->\".format(whole_base) + \" {:.3f}\".format(whole_pp))\n    else:\n        print(\"{:.3f} ->\".format(whole_base) + color.RED +  \" {:.3f}\".format(whole_pp) + color.END + \" -{}\".format(abs(whole_pp - whole_base)))\n    return whole_base, whole_pp\n\ndef inf_numbert_of_annotators(v):\n    s = set(v)\n    d = 10\n\n    for v in s:\n        for u in s:\n            if v != u:\n                d = min(abs(v - u), d)\n    for i in range(100):\n        if d * i >= 1 - 10**(-3):\n            return i\n\ndef zero_one_scale(v):\n    max_x = np.max(v)\n    min_x = np.min(v)\n    return (v - min_x) / (max_x - min_x)\n\n\ndef post_process(train_df, train, pred_df):\n    print(len(train), len(pred_df))\n    df = pred_df.copy()\n    # question_type_spelling not english or ell\n    df.loc[(train[\"host\"] != \"english.stackexchange.com\") & (train[\"host\"] != \"ell.stackexchange.com\"), \"question_type_spelling\"] = 0\n\n    # 0-1 scalingã€€\n    for col in [\"question_type_spelling\", \"question_type_entity\", \"question_type_definition\", \n                \"question_type_compare\", \"question_conversational\", \"question_interestingness_others\",\n                \"question_type_instructions\", \"question_type_consequence\", \"question_interestingness_self\",\n                \"question_has_commonly_accepted_answer\", \"answer_relevance\", \"question_not_really_a_question\"]:\n#     for col in target_columns[-30:]:\n        n = inf_numbert_of_annotators(train_df[col].values)\n        df[col] = zero_one_scale(df[col].values)\n        for i in range(n+1):\n            base = i / n\n            low = base - 1 / (2 * n)\n            up  = base + 1 / (2 * n)\n            df.loc[(df[col] >= low) & (df[col] <= up), col] = base\n\n        if len(set(df[col].values)) == 1:\n            df[col] = pred_df[col].values\n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/google-quest-challenge/train.csv')\ntest_df = pd.read_csv('../input/google-quest-challenge/test.csv')\n\n# sub_df = post_process(train_df, test_df, sub_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}