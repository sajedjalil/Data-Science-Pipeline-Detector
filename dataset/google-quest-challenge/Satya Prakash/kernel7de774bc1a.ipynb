{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# !pip install /kaggle/input/kerasselfattention/keras-self-attention-0.42.0\n# import tensorflow as tf\nimport os\n# from tensorflow.keras.layers import Layer\n# import tensorflow.keras.backend as K\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import tensorflow_hub as hub\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import config\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# import seaborn as sns\nimport re\nimport numpy as np\nimport nltk\nimport keras.backend as K\n# from nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\n# import string\nfrom keras.preprocessing.sequence import pad_sequences\n# # nltk.download('stopwords')\n# # nltk.download('punkt')\neng_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\nimport gc, os, pickle\nfrom nltk import word_tokenize, sent_tokenize\n\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.decomposition import TruncatedSVD\n\ndef plot_len(df, col_name, i):\n    plt.figure(i)\n    sns.distplot(df[col_name].str.len())\n    plt.ylabel(\"length of string\")\n    plt.show()\n\ndef plot_cnt_words(df, col_name, i):\n    plt.figure(i)\n    vals = df[col_name].apply(lambda x: len(x.strip().split()))\n    sns.distplot(vals)\n    plt.ylabel(\"count of words\")\n    plt.show()\n\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df\n\ndef plot_freq_dist(train_data):\n    freq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\/+-=]',' ') for word in text.split()])\n    plt.figure(figsize=(20, 7))\n    plt.title('Word frequency on question title (Training Data)').set_fontsize(25)\n    plt.xlabel('').set_fontsize(25)\n    plt.ylabel('').set_fontsize(25)\n    freq_dist.plot(60,cumulative=False)\n    plt.show()\n\ndef get_tfidf_features(data, dims=256):\n    tfidf = TfidfVectorizer(ngram_range=(1, 3))\n    tsvd = TruncatedSVD(n_components = dims, n_iter=5)\n    tfquestion_title = tfidf.fit_transform(data[\"question_title\"].values)\n    tfquestion_title = tsvd.fit_transform(tfquestion_title)\n\n    tfquestion_body = tfidf.fit_transform(data[\"question_body\"].values)\n    tfquestion_body = tsvd.fit_transform(tfquestion_body)\n\n    tfanswer = tfidf.fit_transform(data[\"answer\"].values)\n    tfanswer = tsvd.fit_transform(tfanswer)\n\n    return tfquestion_title, tfquestion_body, tfanswer\n\ndef correlation(x, y):    \n    mx = tf.math.reduce_mean(x)\n    my = tf.math.reduce_mean(y)\n    xm, ym = x-mx, y-my\n    r_num = tf.math.reduce_mean(tf.multiply(xm,ym))        \n    r_den = tf.math.reduce_std(xm) * tf.math.reduce_std(ym)\n    return  r_num / r_den\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Input, Activation, Average, Maximum\nfrom keras.layers import Concatenate, GRU, Maximum, GlobalAveragePooling1D, GlobalMaxPooling1D, Lambda, Dot\nfrom keras.models import Model\n# from keras_self_attention import SeqSelfAttention\n\ntokens = []\ndef get_words(col):\n  global tokens\n  toks = []\n  for x in sent_tokenize(col):\n    tokens += word_tokenize(x)\n    toks += word_tokenize(x)\n  return toks\n\ndef convert_to_indx(col, word2idx, vocab_size):\n  return [word2idx[word] if word in word2idx else vocab_size for word in col]\n\ndef LSTM_model_initial(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n    if(rnn_type==\"LSTM\"):\n        BLqt = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqt)\n        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb)\n        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n    elif(rnn_type==\"GRU\"):\n        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n        BLan = Bidirectional(GRU(rnn_units))(Ean)\n        \n    BLqt1 = GlobalAveragePooling1D()(BLqt)\n    BLqb1 = GlobalAveragePooling1D()(BLqb)\n    BLan1 = GlobalAveragePooling1D()(BLan)\n    BLqt2 = GlobalMaxPooling1D()(BLqt)\n    BLqb2 = GlobalMaxPooling1D()(BLqb)\n    BLan2 = GlobalMaxPooling1D()(BLan)\n    \n    BLqt = Concatenate()([BLqt1,BLqt2])\n    BLqb = Concatenate()([BLqb1,BLqb2])\n    BLan = Concatenate()([BLan1,BLan2])\n    \n    Dqt = Dropout(dropout_rate)(BLqt)\n    Dqb = Dropout(dropout_rate)(BLqb)\n    Dan = Dropout(dropout_rate)(BLan)\n    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n    target_columns = df_submission.columns\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(1,len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i-1]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n    \ndef LSTM_model_stacked(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n                      dropout_rate=0.2, dense_hidden_units=60, num_stacks=2, epochs=2):\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n    if(rnn_type==\"LSTM\"):\n        BLqt = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqt)\n        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb)\n        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n        for i in range(num_stacks-1):\n            BLqt = Bidirectional(LSTM(rnn_units, return_sequences=True))(BLqt)\n            BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(BLqb)\n            BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(BLan)\n    elif(rnn_type==\"GRU\"):\n        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n        BLan = Bidirectional(GRU(rnn_units))(Ean)\n    Dqt = Dropout(dropout_rate)(Lambda(lambda x: x[:,-1,:], output_shape=(128,))(BLqt))\n    Dqb = Dropout(dropout_rate)(Lambda(lambda x: x[:,-1,:], output_shape=(128,))(BLqb))\n    Dan = Dropout(dropout_rate)(Lambda(lambda x: x[:,-1,:], output_shape=(128,))(BLan))\n    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    print(model.summary())\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n    target_columns = df_submission.columns\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(1,len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i-1]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n    \n# from keras.layers import Lambda, Dot, Activation, Average\n\ndef attention_3d_block_self(hidden_states, rnn_units=64):\n    hidden_size = int(hidden_states.shape[2])\n    score_first_part = Dense(hidden_size, use_bias=False)(hidden_states)\n    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,))(hidden_states)\n    score = Dot([2, 1])([score_first_part, h_t])\n    attention_weights = Activation('softmax')(score)\n    context_vector = Dot([1, 1])([hidden_states, attention_weights])\n    pre_activation = Concatenate()([context_vector, h_t])\n    attention_vector = Dense(rnn_units*2, use_bias=False, activation='tanh')(pre_activation)\n    return attention_vector\n    \n    \ndef attention_3d_block_another(hidden_states1, hidden_state2,rnn_units=64):\n    hidden_size = int(hidden_states1.shape[2])\n    score_first_part = Dense(hidden_size, use_bias=False)(hidden_states1)\n    score = Dot([2, 1])([score_first_part, hidden_state2])\n    attention_weights = Activation('softmax')(score)\n    context_vector = Dot([1, 1])([hidden_states1, attention_weights])\n    pre_activation = Concatenate()([context_vector, hidden_state2])\n    attention_vector = Dense(rnn_units*2, use_bias=False, activation='tanh')(pre_activation)\n    return attention_vector\n\ndef LSTM_model_modified_with_attention_self(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    \n    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n    \n    if(rnn_type==\"LSTM\"):\n        BLqt = Bidirectional(LSTM(rnn_units, return_state=True))(Eqt)\n        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb, initial_state=BLqt[1:])\n        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n    elif(rnn_type==\"GRU\"):\n        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n        BLan = Bidirectional(GRU(rnn_units))(Ean)\n    \n    AtQ = attention_3d_block_self(BLqb, rnn_units)\n    AtAn = attention_3d_block_self(BLan, rnn_units)\n#     Dqt = Dropout(dropout_rate)(BLqt[0])\n    qbin = GlobalAveragePooling1D()(BLqb)\n    anin = GlobalAveragePooling1D()(BLan)\n#     qbin = Lambda(lambda x: x[:,-1,:], output_shape=(rnn_units*2,), name=\"lambda_layer1\")(BLqb_out)\n#     anin = Lambda(lambda x: x[:,-1,:], output_shape=(rnn_units*2,), name=\"lambda_layer2\")(BLan_out)\n#     attn_out, attn_states = AttentionLayer()([BLqb_out, BLan_out], verbose=True)\n    Dqb = Dropout(dropout_rate)(qbin)\n    Dan = Dropout(dropout_rate)(anin)\n#     print(Dqb.shape, Dan.shape, attn_out.shape)\n    Concatenated = Concatenate()([Dqb, Dan, AtQ, AtAn])\n    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n    target_columns = df_submission.columns\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(1,len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i-1]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n    \ndef LSTM_model_modified_with_attention_a2q(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    \n    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n    \n    if(rnn_type==\"LSTM\"):\n        BLqt = Bidirectional(LSTM(rnn_units, return_state=True))(Eqt)\n        BLqb = Bidirectional(LSTM(rnn_units, return_sequences=True))(Eqb, initial_state=BLqt[1:])\n        BLan = Bidirectional(LSTM(rnn_units, return_sequences=True))(Ean)\n    elif(rnn_type==\"GRU\"):\n        BLqt = Bidirectional(GRU(rnn_units))(Eqt)\n        BLqb = Bidirectional(GRU(rnn_units))(Eqb)\n        BLan = Bidirectional(GRU(rnn_units))(Ean)\n    \n    list1 = [attention_3d_block_another(BLqb, Lambda(lambda x: x[:,i,:], output_shape=(rnn_units*2,))(BLan), rnn_units) for i in range(maxlen_an)]\n    AtA2Qm = Maximum()(list1)\n    AtA2Qa = Average()(list1)\n    \n    Dqbin1 = GlobalAveragePooling1D()(BLqb)\n    Dqbin2 = GlobalMaxPooling1D()(BLqb)\n    Dqbin = Concatenate()([Dqbin1, Dqbin2])\n    Dqb = Dropout(dropout_rate)(Dqbin)\n    \n    Danin1 = GlobalAveragePooling1D()(BLan)\n    Danin2 = GlobalMaxPooling1D()(BLan)\n    Danin = Concatenate()([Danin1, Danin2])\n    Dan = Dropout(dropout_rate)(Danin)\n    \n    Concatenated = Concatenate()([Dqb, Dan, AtA2Qm, AtA2Qa])\n    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n    target_columns = df_submission.columns\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(1,len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i-1]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n    \n    \ndef LSTM_model_modified_concatenated_qa(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n                       rnn_units=64, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n                      dropout_rate=0.2, dense_hidden_units=60, epochs=2):\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    \n    Eqt = Embedding(vocab_size, embedding_size, input_length=maxlen_qt)(inpqt)\n    Eqb = Embedding(vocab_size, embedding_size, input_length=maxlen_qb)(inpqb)\n    Ean = Embedding(vocab_size, embedding_size, input_length=maxlen_an)(inpan)\n    \n    if(rnn_type==\"LSTM\"):\n        BLqt = Bidirectional(LSTM(rnn_units, return_state=True))(Eqt)\n        BLqb = Bidirectional(LSTM(rnn_units, return_state=True))(Eqb, initial_state=BLqt[1:])\n        BLan = Bidirectional(LSTM(rnn_units, return_state=True))(Ean, initial_state=BLqb[1:])\n    elif(rnn_type==\"GRU\"):\n        BLqt = Bidirectional(GRU(rnn_units, return_state=True))(Eqt)\n        BLqb = Bidirectional(GRU(rnn_units, return_state=True))(Eqb, initial_state=BLqt[1:])\n        BLan = Bidirectional(GRU(rnn_units, return_state=True))(Ean, initial_state=BLqb[1:])\n        \n    Dqt = Dropout(dropout_rate)(BLqt[0])\n    Dqb = Dropout(dropout_rate)(BLqb[0])\n    Dan = Dropout(dropout_rate)(BLan[0])\n    \n    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n    \n    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n    df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n    target_columns = df_submission.columns\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(1,len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i-1]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n\ndf_train = pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\ndf_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n# LSTM_model_modified_with_attention_self(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n#                        rnn_units=64, maxlen_qt = 40, maxlen_qb = 260, maxlen_an = 210,\n#                       dropout_rate=0.2, dense_hidden_units=50, epochs=5)\n# LSTM_model_initial(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n#                        rnn_units=128, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n#                       dropout_rate=0.2, dense_hidden_units=60, epochs=6)\n# LSTM_model_stacked(df_train, df_test, df_submission, rnn_type=\"LSTM\")\n# LSTM_model_modified_with_attention_self_with_lib(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n#                        rnn_units=64, maxlen_qt = 40, maxlen_qb = 260, maxlen_an = 210,\n#                       dropout_rate=0.2, dense_hidden_units=40, epochs=3)\n# y_test = model.predict(X_test)\nLSTM_model_modified_with_attention_a2q(df_train, df_test, df_submission, rnn_type=\"LSTM\", embedding_size=200, \n                       rnn_units=128, maxlen_qt = 26, maxlen_qb = 260, maxlen_an = 210,\n                      dropout_rate=0.2, dense_hidden_units=60, epochs=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dan_model(df_train, df_test, df_submission, batch_size=8, epochs=4, hidden_layers=[120]):\n  if(len(hidden_layers)<1):\n    print(\"Non-Empty Hidden Layers List Required!\")\n    return\n  module_url = \"/kaggle/input/sent-embed-model\"\n  model = hub.load(module_url)\n\n  def embed(input):\n    return model(input)\n\n  qt_train = np.array(embed(df_train[\"question_title\"]))\n  qb_train = np.array(embed(df_train[\"question_body\"]))\n  an_train = np.array(embed(df_train[\"answer\"]))\n\n  X_train = np.concatenate([qt_train, qb_train, an_train], axis=1)\n\n  qt_test = np.array(embed(df_test[\"question_title\"]))\n  qb_test = np.array(embed(df_test[\"question_body\"]))\n  an_test = np.array(embed(df_test[\"answer\"]))\n\n  X_test = np.concatenate([qt_test, qb_test, an_test], axis=1)\n\n  target_columns = df_submission.columns[1:]\n  y_train = df_train[target_columns].values\n\n  model = tf.keras.models.Sequential()\n  model.add(tf.keras.layers.Dense(hidden_layers[0], activation=\"relu\"))\n  model.add(tf.keras.layers.Dropout(0.2))  \n  for h in hidden_layers[1:]:\n    model.add(tf.keras.layers.Dense(h, activation=\"relu\"))\n    model.add(tf.keras.layers.Dropout(0.2))  \n  model.add(tf.keras.layers.Dense(30, activation=\"sigmoid\"))\n\n  model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n  model.fit(X_train, y_train, batch_size=batch_size,\n            epochs=epochs, validation_split=0.1)\n  print(model.summary())\n  y_test = model.predict(X_test)\n\n  outp = {}\n  outp[\"qa_id\"] = df_test[\"qa_id\"]\n  for i in range(len(target_columns)):\n      outp[target_columns[i]] = y_test[:, i]\n  my_submission = pd.DataFrame(outp)\n  my_submission.to_csv('submission.csv', index=False)\n\ndef pad_seq_custom(original_series, maxlen=50):\n    n = original_series.size\n    for i in range(n):\n        ll = len(original_series[i])\n        for kk in range(maxlen-ll):\n            np.append(original_series[i],[0 for rr in range(512)])\n    return original_series\n    \n    \ndef dan_model_with_words(df_train, df_test, df_submission, batch_size=8,\n                         epochs=4, hidden_layers=[120],\n                         rnn_units=64, maxlen_qt = 26,\n                         maxlen_qb = 260, maxlen_an = 210,\n                         dropout_rate=0.2, dense_hidden_units=50,\n                         embedding_size=512):\n    if(len(hidden_layers)<1):\n        print(\"Non-Empty Hidden Layers List Required!\")\n        return\n    module_url = \"/kaggle/input/sent-embed-model\"\n    model = hub.load(module_url)\n\n    def embed(input):\n        return model([input])\n\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    embedding_matrix = np.zeros((vocab_size+1, embedding_size))\n    for word, i in word2idx.items():\n        embedding = np.array(embed(word))\n        if embedding is not None:\n            embedding_matrix[i] = embedding\n\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    \n    Eqt = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qt, trainable=False)(inpqt)\n    Eqb = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qb, trainable=False)(inpqb)\n    Ean = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_an, trainable=False)(inpan)\n\n    BLqt = Bidirectional(LSTM(rnn_units))(Eqt)\n    BLqb = Bidirectional(LSTM(rnn_units))(Eqb)\n    BLan = Bidirectional(LSTM(rnn_units))(Ean)\n    \n    Dqt = Dropout(dropout_rate)(BLqt)\n    Dqb = Dropout(dropout_rate)(BLqb)\n    Dan = Dropout(dropout_rate)(BLan)\n\n    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n    \n    Ds = Dense(dense_hidden_units, activation='elu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=8, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n    \n# dan_model_with_words(df_train, df_test, df_submission, epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from gensim.models import KeyedVectors\n\n# df_train = pd.read_csv(\"/kaggle/input/google-quest-challenge/train.csv\")\n# df_test = pd.read_csv(\"/kaggle/input/google-quest-challenge/test.csv\")\n# df_submission = pd.read_csv(\"/kaggle/input/google-quest-challenge/sample_submission.csv\")\n\ndef glove_model_with_words(df_train, df_test, df_submission, batch_size=8,\n                         epochs=4, rnn_units=64, maxlen_qt = 26,\n                         maxlen_qb = 260, maxlen_an = 210,\n                         dropout_rate=0.2, dense_hidden_units=50,\n                         embedding_size=300):\n    filename = '/kaggle/input/googlenewsvectors/GoogleNews-vectors-negative300.bin'\n    model = KeyedVectors.load_word2vec_format(filename, binary=True)\n\n    def embed(model, input):\n        if input not in model.wv.vocab:\n            return np.array([0 for _ in range(300)])\n        return model.wv[input]\n\n    columns = ['question_title','question_body','answer']\n    df_train = clean_data(df_train, columns)\n    df_test = clean_data(df_test, columns)\n    # columns = [\"question_title\", \"question_body\", \"answer\"]\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: get_words(x))\n      df_test[col] = df_test[col].apply(lambda x: get_words(x))\n    vocab = sorted(list(set(tokens)))\n    vocab_size = len(vocab)\n\n    word2idx = {}\n    idx2word = {}\n    for idx, word in enumerate(vocab):\n      word2idx[word] = idx\n      idx2word[idx] = word\n\n    for col in columns:\n      df_train[col] = df_train[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n      df_test[col] = df_test[col].apply(lambda x: convert_to_indx(x,word2idx,vocab_size))\n\n    X_train_question_title = pad_sequences(df_train[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_train_question_body = pad_sequences(df_train[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_train_answer = pad_sequences(df_train[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    X_test_question_title = pad_sequences(df_test[\"question_title\"], maxlen=maxlen_qt, padding='post', value=0)\n    X_test_question_body = pad_sequences(df_test[\"question_body\"], maxlen=maxlen_qb, padding='post', value=0)\n    X_test_answer = pad_sequences(df_test[\"answer\"], maxlen=maxlen_an, padding='post', value=0)\n\n    target_columns = df_submission.columns[1:]\n    y_train = df_train[target_columns]\n\n    embedding_matrix = np.zeros((vocab_size+1, embedding_size))\n    for word, i in word2idx.items():\n        embedding = np.array(embed(model, word))\n        if embedding is not None:\n            embedding_matrix[i] = embedding\n\n\n    inpqt = Input(shape=(maxlen_qt,),name='inpqt')\n    inpqb = Input(shape=(maxlen_qb,),name='inpqb')\n    inpan = Input(shape=(maxlen_an,),name='inpan')\n    \n    Eqt = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qt)(inpqt)\n    Eqb = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_qb)(inpqb)\n    Ean = Embedding(vocab_size+1, embedding_size, weights=[embedding_matrix], input_length=maxlen_an)(inpan)\n\n    BLqt = Bidirectional(LSTM(rnn_units))(Eqt)\n    BLqb = Bidirectional(LSTM(rnn_units))(Eqb)\n    BLan = Bidirectional(LSTM(rnn_units))(Ean)\n\n    Dqt = Dropout(dropout_rate)(BLqt)\n    Dqb = Dropout(dropout_rate)(BLqb)\n    Dan = Dropout(dropout_rate)(BLan)\n    \n    Concatenated = Concatenate()([Dqt, Dqb, Dan])\n    \n    Ds = Dense(dense_hidden_units, activation='relu')(Concatenated)\n    Dsf = Dense(30, activation='sigmoid')(Ds)\n\n    model = Model(inputs=[inpqt, inpqb, inpan], outputs=Dsf)\n    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n    model.fit({'inpqt': X_train_question_title, 'inpqb': X_train_question_body, 'inpan': X_train_answer}, y_train, batch_size=32, epochs=epochs, validation_split=0.1)\n\n    y_test = model.predict({'inpqt': X_test_question_title, 'inpqb': X_test_question_body, 'inpan': X_test_answer})\n\n    outp = {}\n    outp[\"qa_id\"] = df_test[\"qa_id\"]\n    for i in range(len(target_columns)):\n        outp[target_columns[i]] = y_test[:, i]\n    my_submission = pd.DataFrame(outp)\n    my_submission.to_csv('submission.csv', index=False)\n    \n# glove_model_with_words(df_train, df_test, df_submission, epochs=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import matplotlib.pyplot as plt\n# from tqdm.notebook import tqdm\n# import tensorflow as tf\n# import tensorflow.keras.backend as K\n# import os\n# from scipy.stats import spearmanr\n# from math import floor, ceil\n# from transformers import *\n\nBERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\nBERT_LARGE_PATH = '/kaggle/input/bert-large-uncased-huggingface-transformer/'\n\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, length, is_pair = True):\n        if is_pair:\n            inputs = tokenizer.encode_plus(str1, str2,\n                add_special_tokens=True,\n                max_length=length,\n                truncation_strategy=\"only_second\")\n        else:\n            inputs = tokenizer.encode_plus(str1, None,\n                add_special_tokens=True,\n                max_length=length,\n                truncation_strategy=\"only_first\")\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title, question, max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, max_sequence_length, False)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from datetime import datetime\n\ndef create_model_large():\n    q_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_LARGE_PATH+'bert-large-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model\n\ndef bert_uncased():\n    PATH = '/kaggle/input/google-quest-challenge/'\n\n    BERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\n    # !cp /kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt ./vocab.txt\n    tokenizer = BertTokenizer.from_pretrained(BERT_PATH + \"bert-base-uncased-vocab.txt\")\n\n    MAX_LEN = 90\n\n    df_train = pd.read_csv(PATH+'train.csv')\n    df_test = pd.read_csv(PATH+'test.csv')\n    df_submission = pd.read_csv(PATH+'sample_submission.csv')\n\n    output_categories = list(df_submission.columns)\n    input_categories = [\"question_title\", \"question_body\", \"answer\"]\n    \n    outputs = df_train[output_categories[1:]].values\n    inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_LEN)\n    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_LEN)\n    \n    test_preds = []\n    train_inputs = [inputs[i] for i in range(len(inputs))]\n    train_outputs = outputs\n\n    K.clear_session()\n    model = create_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n\n    logdir = \"/kaggle/working/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    model.fit(train_inputs, train_outputs, epochs=1, batch_size=16, validation_split=0.5, callbacks=[tensorboard_callback])\n\n    test_preds.append(model.predict(test_inputs))\n    df_submission.iloc[:, 1:] = np.average(test_preds, axis=0)\n    df_submission.to_csv('submission.csv', index=False)\n    \ndef bert_large_uncased():\n    PATH = '/kaggle/input/google-quest-challenge/'\n\n    BERT_PATH = '/kaggle/input/bert-large-uncased-huggingface-transformer/'\n    # !cp /kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt ./vocab.txt\n    tokenizer = BertTokenizer.from_pretrained(BERT_PATH + \"bert-large-uncased-vocab.txt\")\n\n    MAX_LEN = 90\n\n    df_train = pd.read_csv(PATH+'train.csv')\n    df_test = pd.read_csv(PATH+'test.csv')\n    df_submission = pd.read_csv(PATH+'sample_submission.csv')\n\n    output_categories = list(df_submission.columns)\n    input_categories = [\"question_title\", \"question_body\", \"answer\"]\n    \n    outputs = df_train[output_categories[1:]].values\n    inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_LEN)\n    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_LEN)\n    \n    test_preds = []\n    train_inputs = [inputs[i] for i in range(len(inputs))]\n    train_outputs = outputs\n\n    K.clear_session()\n    model = create_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n\n    logdir = \"/kaggle/working/logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    model.fit(train_inputs, train_outputs, epochs=1, batch_size=16, validation_split=0.5, callbacks=[tensorboard_callback])\n\n    test_preds.append(model.predict(test_inputs))\n    df_submission.iloc[:, 1:] = np.average(test_preds, axis=0)\n    df_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# import matplotlib.pyplot as plt\n# import tensorflow as tf\n# import tensorflow.keras.backend as K\n# import os\n# from scipy.stats import spearmanr\n# from math import floor, ceil\n# from transformers import *\n\n# PATH = '/content/drive/My Drive/NLPDATA/'\n# BERT_PATH = '/kaggle/input/bert-base-uncased-huggingface-transformer/'\n# MAX_LEN = 360\n\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, length, is_pair = True):\n        if is_pair:\n            inputs = tokenizer.encode_plus(str1, str2,\n                add_special_tokens=True,\n                max_length=length,\n                truncation_strategy=\"only_second\")\n        else:\n            inputs = tokenizer.encode_plus(str1, None,\n                add_special_tokens=True,\n                max_length=length,\n                truncation_strategy=\"only_first\")\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title, question, max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, max_sequence_length, False)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in df[columns].iterrows():\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\ndef compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config = XLNetConfig()\n    config.d_inner = 3072\n    config.n_head = 12\n    config.d_model = 768\n    config.n_layer = 12\n    config.output_hidden_states = False\n    \n    bert_model = TFXLNetModel.from_pretrained(\n        '/kaggle/input/xlnet-base-tf/xlnet-base-cased-tf_model.h5', config=config)\n    \n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model\n\n\n# from datetime import datetime\n\ndef xlnet_cased():\n    PATH = '/kaggle/input/google-quest-challenge/'\n\n    BERT_PATH = '/kaggle/input/xlnet-base-tf/'\n    # !cp /kaggle/input/bert-base-uncased-huggingface-transformer/bert-base-uncased-vocab.txt ./vocab.txt\n#     tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n    tokenizer = XLNetTokenizer(\"/kaggle/input/xlnet-base-tf/xlnet-base-cased-spiece.model\")\n\n    df_train = pd.read_csv(PATH+'train.csv')\n    df_test = pd.read_csv(PATH+'test.csv')\n    df_submission = pd.read_csv(PATH+'sample_submission.csv')\n\n    output_categories = list(df_submission.columns)\n    input_categories = [\"question_title\", \"question_body\", \"answer\"]\n    \n    outputs = df_train[output_categories[1:]].values\n    inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_LEN)\n    test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_LEN)\n    \n    test_preds = []\n    train_inputs = [inputs[i] for i in range(len(inputs))]\n    train_outputs = outputs\n\n    K.clear_session()\n    model = create_model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n\n    model.compile(loss='binary_crossentropy', optimizer=optimizer)\n    model.fit(train_inputs, train_outputs, epochs=5, batch_size=6, validation_split=0.1)\n\n    test_preds.append(model.predict(test_inputs))\n    df_submission.iloc[:, 1:] = np.average(test_preds, axis=0)\n    df_submission.to_csv('submission.csv', index=False)\n    \n# xlnet_cased()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}