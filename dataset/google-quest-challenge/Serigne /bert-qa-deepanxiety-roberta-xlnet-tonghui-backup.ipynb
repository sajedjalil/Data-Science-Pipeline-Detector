{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install ../input/sacremoses/sacremoses-master/ \n! pip install ../input/transformers/transformers-master/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/tensorflowv1/tensorboard-1.15.0-py3-none-any.whl\n!pip install ../input/tensorflowv1/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n!pip install ../input/tensorflowv1/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl\nimport os\nimport sys","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom transformers import AdamW, BertConfig, BertModel\nimport time\nimport datetime\nimport  gc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)\n\nMAX_LEN = 512\nSEP_TOKEN_ID = 102\nBERT_PATH = '../input/bert-base-from-tfhub/bert_en_uncased_L-12_H-768_A-12'\nconfigPath = '../input/bert-model-config/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\nif use_cuda:\n    print('GPU available!')\n    print(torch.cuda.get_device_name(0))\nelse:\n    print('GPU not available')\ndevice = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n# torch.backends.cudnn.benchmark = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper time function\ndef format_time(elapsed_time):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round(elapsed_time))\n\n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, df, train_mode=True, labeled=True, target_columns=output_categories):\n        self.df = df\n        self.train_mode = train_mode\n        self.labeled = labeled\n        self.tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'/assets/vocab.txt', do_lower_case=True)\n        self.target_columns = target_columns\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        token_ids, seg_ids = self.get_token_ids(row)\n        if self.labeled:\n            labels = self.get_labels(row, self.target_columns)\n            return token_ids, seg_ids, labels\n        else:\n            return token_ids, seg_ids\n\n    def get_token_ids(self, row):\n        title_tokens, ques_tokens, ans_tokens = self.trim_input(row.question_title, row.question_body, row.answer)\n        tokens = ['[CLS]'] + title_tokens + ['[SEP]'] + ques_tokens + ['[SEP]'] + ans_tokens + ['[SEP]']\n        token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        if (len(token_ids)) < MAX_LEN:\n            # PADDING\n            token_ids += [0] * (MAX_LEN - len(token_ids))\n\n        token_ids_torch = torch.tensor(token_ids)\n        seg_ids = self.get_seg_ids(token_ids_torch)\n\n        return token_ids_torch, seg_ids\n\n    def get_seg_ids(self, ids):\n        seg_ids = torch.zeros_like(ids)\n        seg_idx = 0\n        first_step = True\n        for i, e in enumerate(ids):\n            seg_ids[i] = seg_idx\n            if e == SEP_TOKEN_ID:\n                if first_step:\n                    first_step = False\n                else:\n                    seg_idx = 1\n        # get indexes of paddings\n        pad_idx = torch.nonzero(ids == 0)\n        seg_ids[pad_idx] = 0\n\n        return seg_ids\n\n    \n    def get_labels(self, row, target_columns):\n        return torch.tensor(row[target_columns].values.astype(np.float32))\n\n    def trim_input(self, title, question, answer, max_seq_length=MAX_LEN,\n                   title_max_len=30, ques_max_len=239, ans_max_len=239, \n                   head_len=120):\n        title_tokens = self.tokenizer.tokenize(title)\n        ques_tokens = self.tokenizer.tokenize(question)\n        ans_tokens = self.tokenizer.tokenize(answer)\n\n        title_len = len(title_tokens)\n        ques_len = len(ques_tokens)\n        ans_len = len(ans_tokens)\n\n        if (title_len + ques_len + ans_len + 4) > max_seq_length:\n            if title_max_len > title_len:\n                final_title_len = title_len\n                ans_max_len = int(ans_max_len + np.ceil((title_max_len - title_len) / 2))\n                ques_max_len = int(ques_max_len + np.floor((title_max_len - title_len) / 2))\n            else:\n                final_title_len = title_max_len\n\n            if ans_max_len > ans_len:\n                final_ans_len = ans_len\n                final_ques_len = ques_max_len + (ans_max_len - ans_len)\n            elif ques_max_len > ques_len:\n                final_ques_len = ques_len\n                final_ans_len = ans_max_len + (ques_max_len - ques_len)\n            else:\n                final_ans_len = ans_max_len\n                final_ques_len = ques_max_len\n\n            if final_ans_len + final_ques_len + final_title_len + 4 != max_seq_length:\n                print('Token Sequence Length:', final_ans_len + final_ques_len + final_title_len + 4)\n                raise ValueError('New sequence length does not match.')\n\n            title_tokens = title_tokens[:final_title_len]\n            # head + tail tokenization\n            if ques_len > final_ques_len:\n                max_head_len = max(head_len, int(0.5 * final_ques_len))\n                ques_tokens = ques_tokens[:max_head_len] + ques_tokens[max_head_len - final_ques_len:]\n            else:\n                ques_tokens = ques_tokens[:final_ques_len]\n            if ans_len > final_ans_len:\n                max_head_len = max(head_len, int(0.5 * final_ans_len))\n                ans_tokens = ans_tokens[:max_head_len] + ans_tokens[max_head_len - final_ans_len:]\n            else:\n                ans_tokens = ans_tokens[:final_ans_len]\n\n\n        return title_tokens, ques_tokens, ans_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'batch_size': 8,\n          'shuffle': False,\n          'num_workers': 6}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction_v7(test_dataloader, model, classifier, shape, batch_size=8):\n    print(\"Running Prediction...\")\n\n    t0 = time.time()\n\n    # Put model in evaluation mode to evaluate loss on the prediction set\n    model.eval()\n    classifier.eval()\n\n    # val_shape = len(df_val)\n    test_preds = np.zeros((shape, 30))\n\n    for step, batch in enumerate(test_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n\n        # Unpack the inputs from our dataloader\n        test_input_ids, test_input_mask = batch\n\n        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n        with torch.no_grad():\n            attention_mask = (test_input_ids > 0)\n            last_layer, _ , hidden_layers= model(input_ids=test_input_ids, \n                                                 token_type_ids=test_input_mask, \n                                                 attention_mask=attention_mask)\n            \n            cls = torch.cat(tuple(hidden_layers[i][:, 0, :] for i in [-1, -2, -3, -4]), dim = -1)\n            # cls = torch.cat((cls, external_features), dim = -1)\n            # outputs = torch.cat(tuple(hidden_layers[i] for i in [-1, -2, -3, -4]), dim = -1)\n            # pooling_layer = torch.nn.AdaptiveAvgPool2d((1, 3072))\n            # cls = pooling_layer(outputs).squeeze()\n            # cls = last_layer[:, 0, :]\n            # outputs = torch.nn.AdaptiveAvgPool2d((1, 768))(last_layer).squeeze()\n            # outputs = torch.nn.Dropout(0.2)(outputs)\n            # outputs = outputs * 0.8\n            prediction = classifier(cls)\n\n            test_preds[step * batch_size: (step + 1) * batch_size] = prediction.detach().cpu().numpy()\n    print(\" Prediction took: {:}\".format(format_time(time.time() - t0)))\n\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediction(test_dataloader, model, classifier, shape, batch_size=8):\n    print(\"Running Prediction...\")\n\n    t0 = time.time()\n\n    # Put model in evaluation mode to evaluate loss on the prediction set\n    model.eval()\n    classifier.eval()\n\n    # val_shape = len(df_val)\n    test_preds = np.zeros((shape, 30))\n\n    for step, batch in enumerate(test_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n\n        # Unpack the inputs from our dataloader\n        test_input_ids, test_input_mask= batch\n\n        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n        with torch.no_grad():\n            attention_mask = (test_input_ids > 0)\n            last_layer, _ , hidden_layers= model(input_ids=test_input_ids, \n                                                 token_type_ids=test_input_mask, \n                                                 attention_mask=attention_mask)\n            \n            # cls = torch.cat(tuple(hidden_layers[i][:, 0, :] for i in [-1, -2, -3, -4]), dim = -1)\n            outputs = torch.cat(tuple(hidden_layers[i] for i in [-1, -2, -3, -4]), dim = -1)\n            pooling_layer = torch.nn.AdaptiveAvgPool2d((1, 3072))\n            cls = pooling_layer(outputs).squeeze()\n            # cls = last_layer[:, 0, :]\n            # outputs = torch.nn.AdaptiveAvgPool2d((1, 768))(last_layer).squeeze()\n            # outputs = torch.nn.Dropout(0.2)(outputs)\n            # outputs = outputs * 0.8\n            prediction = classifier(cls)\n\n            test_preds[step * batch_size: (step + 1) * batch_size] = prediction.detach().cpu().numpy()\n    print(\" Prediction took: {:}\".format(format_time(time.time() - t0)))\n\n    return test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_test = QuestDataset(df_test, labeled=False)\ntest_dataloader = torch.utils.data.DataLoader(ds_test, **params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classifier = torch.nn.Sequential(torch.nn.Linear(768, 30),\n#                                  torch.nn.Sigmoid())\nclassifier = torch.nn.Sequential(torch.nn.Linear(3072, 3072),\n                                 torch.nn.Tanh(),\n                                 torch.nn.Linear(3072, 30),\n                                 torch.nn.Sigmoid())\n# classifier = torch.nn.Sequential(torch.nn.Linear(3072 + num_external_features, 3072 + num_external_features),\n#                                  torch.nn.Tanh(),\n#                                  torch.nn.Linear(3072 + num_external_features, 30),\n#                                  torch.nn.Sigmoid())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = config = BertConfig.from_pretrained(configPath)\nconfig.output_hidden_states = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(5):\n    model_path = f'../input/google-quest-pytorch-v7/bert-base-{i}.h5py'\n    classifier_path = f'../input/google-quest-pytorch-v7/bert-base-classifier-{i}.h5py'\n    bert_model_weights = torch.load(model_path)\n    model = BertModel.from_pretrained(configPath, state_dict = bert_model_weights, config=config)\n    classifier.load_state_dict(torch.load(classifier_path))\n    model.cuda()\n    classifier.cuda()\n    \n    test_predictions.append(prediction_v7(test_dataloader, model, classifier, len(df_test)))\n    print('Fold Done')\n    print(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predictions = np.mean(test_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_predictions, ds_test, df_test, test_dataloader, model, classifier; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache() ## Clear_Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np \nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold\nimport datetime\nimport pkg_resources\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm, tqdm_notebook\nfrom scipy.stats import spearmanr\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pickle\nimport random\nimport shutil\nimport transformers\nfrom spacy.lang.en import English\nfrom math import floor, ceil\n#from comment_parser import comment_parser\nfrom xml.sax.saxutils import unescape\nimport torch.nn.init as init\nimport glob\nfrom numba import cuda\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle/input/roberta-large","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED                = 1414\nDATA_DIR            = \"../input/google-quest-challenge/\"\nWORK_DIR            = \"../working/\"\nBERT_VOCAB_PATH     = \"../input/roberta-large/roberta-large-vocab.json\"\nBERT_MERGES_PATH    = \"../input/roberta-large/roberta-large-merges.txt\"\nBERT_MODEL_PATH     = \"../input/roberta-large/roberta-large-pytorch_model.bin\"\nBERT_CONFIG_PATH    = \"../input/roberta-large/roberta-large-config.json\"\ninput_columns       = ['question_title', 'question_body', 'answer']\nseed_everything(SEED)\nbatch_size          = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df  = pd.read_csv(os.path.join(DATA_DIR,\"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\ndef add_external_features(df):\n    \n    eng_stopwords = set(stopwords.words(\"english\"))\n    #If the question is longer, it may be more clear, which may help users give a more \n    df['question_body']      = df['question_body'].apply(lambda x:str(x))\n    df['question_num_words'] = df.question_body.str.count('\\S+')\n    df['question_title_num_words'] = df.question_title.str.count('\\S+')\n    \n    #The assumption here is that longer answer could bring more useful detail\n    df['answer']            = df['answer'].apply(lambda x:str(x))\n    df['answer_num_words']  = df.answer.str.count('\\S+')\n    \n    df[\"question_title_num_unique_words\"] = df[\"question_title\"].apply(lambda x: len(set(str(x).split())))\n    df[\"question_body_num_unique_words\"]  = df[\"question_body\"].apply(lambda x: len(set(str(x).split())))\n    df[\"answer_num_unique_words\"]         = df[\"answer\"].apply(lambda x: len(set(str(x).split())))\n    \n    df[\"question_title_num_chars\"] = df[\"question_title\"].apply(lambda x: len(str(x)))\n    df[\"question_body_num_chars\"]  = df[\"question_body\"].apply(lambda x: len(str(x)))\n    df[\"answer_num_chars\"]         = df[\"answer\"].apply(lambda x: len(str(x)))\n    \n    df['qt_words'] = df['question_title'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n    df['qt_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['qt_words'], s['a_words'])), axis = 1)\n    \n    df['qa_word_overlap_norm'] = df.apply(lambda s: s['qa_word_overlap']/(len(s['a_words']) + len(s['q_words'])  - s['qa_word_overlap']) , axis = 1)\n    df['qta_word_overlap_norm'] = df.apply(lambda s: s['qt_word_overlap']/(len(s['a_words']) + len(s['qt_words']) - s['qt_word_overlap']), axis = 1)\n    df.drop(['q_words', 'a_words', 'qt_words'], axis = 1, inplace = True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = add_external_features(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"handmade_cols = [\"question_body_num_unique_words\", 'question_num_words',\n                 \"question_title_num_unique_words\", \"question_title_num_words\", \n                 \"answer_num_unique_words\", \"answer_num_words\"]\n\nwith open('/kaggle/input/d-large-roberta-b32-steplr3-drop-tb-rmf-h56-2/C_scaler.pickle', mode='rb') as f:\n    num_words_scaler = pickle.load(f)\n\ntest_df[handmade_cols]=  num_words_scaler.transform(test_df[handmade_cols].values)\ntrain_handmade_features= test_df[handmade_cols + ['qa_word_overlap_norm', 'qta_word_overlap_norm']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoder(x, dict_reverse):\n    try:\n        return dict_reverse[x]\n    except:\n        return 0\n\nwith open('/kaggle/input/d-large-roberta-b32-steplr3-drop-tb-rmf-h56-2/C_category.pickle', mode='rb') as f:\n    category_dict_reverse = pickle.load(f)\ntest_df['category'] = test_df['category'].apply(lambda x: label_encoder(x, category_dict_reverse))\n\nwith open('/kaggle/input/d-large-roberta-b32-steplr3-drop-tb-rmf-h56-2/C_host.pickle', mode='rb') as f:\n    host_dict_reverse = pickle.load(f)\ntest_df['host'] = test_df['host'].apply(lambda x: label_encoder(x, host_dict_reverse))\n\nn_cat    = len(category_dict_reverse) + 1\ncat_emb  = 256\nn_host   = len(host_dict_reverse) + 1\nhost_emb = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_host","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bert Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = transformers.RobertaConfig.from_json_file(BERT_CONFIG_PATH)\nbert_config.output_hidden_states = True\ntokenizer = transformers.RobertaTokenizer(BERT_VOCAB_PATH,BERT_MERGES_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments2(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=238, a_max_len=238):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+6) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+6 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+6)))\n        \n        t = t[:t_new_len]\n        q_len_head = round(q_new_len/3)\n        q_len_tail = -1* (q_new_len -q_len_head)\n        a_len_head = round(a_new_len/3)\n        a_len_tail = -1* (a_new_len -a_len_head) \n        \n        q = q[:q_len_head]+q[q_len_tail:]\n        a = a[:a_len_head]+a[a_len_tail:]\n    \n    return t, q, a\n\n\ndef convert_lines(question, answer, max_sequence_length, tokenizer):\n    \n    all_tokens   = []\n    all_masks    = []\n    all_segments = []\n    longer = 0\n    for q, a in tqdm(zip(question, answer)):\n        \n        tokens_q = tokenizer.tokenize(q)\n        tokens_a = tokenizer.tokenize(a)\n        \n        q_len = len(tokens_q)\n        a_len = len(tokens_a)\n        \n        if (q_len+a_len+3) > max_sequence_length:\n            longer += 1\n            new_q_len = q_len/(a_len+q_len) * (max_sequence_length-3)\n            new_a_len = a_len/(q_len+a_len) * (max_sequence_length-3)\n            new_q_len, new_a_len = int(np.ceil(new_q_len)), int(np.floor(new_a_len))\n            \n            if new_a_len+new_q_len+3 != max_sequence_length:\n                raise ValueError(\"too small %s\" % str(new_a_len+new_q_len+3))\n                \n            tokens_q = tokens_q[:new_q_len]\n            tokens_a = tokens_a[:new_a_len]\n            \n        stoken = [\"[CLS]\"] + tokens_q + [\"[SEP]\"] + tokens_a + [\"[SEP]\"]\n        #print(stoken)\n        \n        ##############\n        #token_ids\n        ##############\n        token_ids = tokenizer.encode(stoken, add_special_tokens=Ture)\n        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n        \n        #############\n        #input_masks\n        #############\n        attention_masks = _get_masks(stoken, max_sequence_length)\n        \n        ##############\n        #input_segments\n        ###############\n        input_segments = _get_segments2(stoken, max_sequence_length)\n        \n        all_tokens.append(input_ids)\n        all_masks.append(attention_masks)\n        all_segments.append(input_segments)\n    \n    print(longer)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines2(title, question, answer, max_sequence_length, tokenizer, t_max_len_seq=30, q_max_len_seq=238, a_max_len_seq=238):\n    \n    all_tokens   = []\n    all_masks    = []\n    all_segments = []\n    \n    longer = 0\n    \n    for t, q, a in tqdm(zip(title, question, answer)):\n        \n        tokens_t, tokens_q, tokens_a  = _trim_input(t, q, a, max_sequence_length=max_sequence_length)\n        #print(tokens_t)\n        #print(tokens_q)\n        #print(tokens_a)\n        \n        stoken = [\"<s>\"] + tokens_t + [\"</s>\"] + [\"</s>\"] + tokens_q + [\"</s>\"] + [\"</s>\"] + tokens_a + [\"</s>\"]\n        ##############\n        #token_ids\n        ##############\n        token_ids = tokenizer.convert_tokens_to_ids(stoken)\n        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n        \n        #############\n        #input_masks\n        #############\n        attention_masks = _get_masks(stoken, max_sequence_length)\n        #print(attention_masks)\n        \n        ##############\n        #input_segments\n        ###############\n        input_segments = _get_segments2(stoken, max_sequence_length)\n        #print(len(input_ids))\n        \n        all_tokens.append(input_ids)\n        all_masks.append(attention_masks)\n        all_segments.append(input_segments)\n        #break\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokens, test_masks, test_segments = convert_lines2(test_df[\"question_title\"],\n                                                       test_df[\"question_body\"], \n                                                       test_df[\"answer\"],\n                                                       max_sequence_length=512, \n                                                       tokenizer=tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset_test(Dataset):\n\n    def __init__(self, token_ids, masks, segments, hosts, categories, handmade_features):\n                \n        self.token_ids  = token_ids\n        self.masks      = masks\n        self.segments   = segments\n        self.hosts      = hosts\n        self.categories = categories\n        self.handmades  = handmade_features\n\n    def __len__(self):\n        return self.token_ids.shape[0]\n\n    def __getitem__(self, idx):\n        token_id = self.token_ids[idx]\n        mask     = self.masks[idx]\n        segment  = self.segments[idx]\n        host     = self.hosts[idx]\n        category = self.categories[idx]\n        handmade = self.handmades[idx]\n\n        return [token_id, mask, segment, host, category, handmade]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, torch.nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n\n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim),\n            self.weight\n        ).view(-1, step_dim)\n\n        if self.bias:\n            eij = eij + self.b\n\n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n        weighted_input = x * torch.unsqueeze(a, -1)\n        \n        return torch.sum(weighted_input, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestModel6(nn.Module):\n\n    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n        super().__init__()\n        BERT_DIMS = 1024\n        LSTM_UNITS = 512\n        model_path = os.path.join(BERT_MODEL_PATH)\n        self.bert_model = transformers.RobertaModel.from_pretrained(model_path,config=bert_config)\n        set_trainable(self.bert_model, False)\n        \n        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n        self.host_embedding     = nn.Embedding(n_host, host_emb)\n        self.embedding_dropout  = SpatialDropout(0.5)\n        self.lstm1              = nn.LSTM(BERT_DIMS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2              = nn.GRU(LSTM_UNITS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.atten1             = Attention(LSTM_UNITS*2, 512)\n        self.atten2             = Attention(LSTM_UNITS*2, 512)\n        self.dropout1           = nn.Dropout(0.2)\n        self.fc1                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 21)\n        self.fc2                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 9)\n        self._init_weights(self.category_embedding)\n        self._init_weights(self.host_embedding)\n        self._init_weights(self.fc1)\n        self._init_weights(self.fc2)\n    \n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            print(\"initailize weight\")\n            module.weight.data.normal_(mean=0.0, std=0.02)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            print(\"initailize bias\")\n            module.bias.data.zero_()\n        \n    def forward(self, token_ids, masks, segment, hosts, categories, handmades):\n        \n        category_embed = self.category_embedding(categories)\n        host_embed     = self.host_embedding(hosts)\n        external_features = torch.cat((category_embed, host_embed, handmades), 1)\n        #print(external_features.shape)\n        _, seq_output, hidden_states = self.bert_model(input_ids=token_ids,\n                                                    attention_mask=masks,\n                                                    token_type_ids=segment)\n        \n        last_four_layer = torch.cat((hidden_states[-5], hidden_states[-6]), 2)\n        lstm_input      = self.embedding_dropout(last_four_layer)\n        lstm1_output, _ = self.lstm1(lstm_input)\n        lstm2_output, _ = self.lstm2(lstm1_output)\n        \n        meanpooled_output   = torch.mean(lstm2_output, 1)\n        maxpooled_output, _ = torch.max(lstm2_output, 1)\n        attention_output_q  = self.atten1(lstm2_output)\n        attention_output_a  = self.atten2(lstm2_output)\n        \n        pooled_output_q = torch.cat((meanpooled_output, maxpooled_output, attention_output_q, external_features), 1)\n        pooled_output_q = self.dropout1(pooled_output_q)\n        \n        pooled_output_a = torch.cat((meanpooled_output, maxpooled_output, attention_output_a, external_features), 1)\n        pooled_output_a = self.dropout1(pooled_output_a)\n        \n        q_results     = self.fc1(pooled_output_q)\n        a_results     = self.fc2(pooled_output_a)\n        results       = torch.cat((q_results, a_results), 1)\n        \n        return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_result(model, test_loader, batch_size=batch_size):\n    \n    output = np.zeros((len(test_set), 30))\n    model.eval()\n    with torch.no_grad():\n        for idx, inputs in enumerate(test_loader):\n            start_index = idx * batch_size\n            end_index   = min(start_index + batch_size, len(test_set))\n            token_ids, masks, segments, hosts, categories, handmades = inputs\n            token_ids   = token_ids.long().cuda()\n            masks       = masks.long().cuda()\n            segments    = segments.long().cuda()\n            hosts       = hosts.long().cuda()\n            categories  = categories.long().cuda()\n            handmades   = handmades.float().cuda()\n            \n            predictions = model(token_ids, masks, segments, hosts, categories, handmades)\n            predictions = torch.sigmoid(predictions)\n            output[start_index:end_index, :] = predictions.detach().cpu().numpy()\n            \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\n! ls /kaggle/input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_weighted_1 = glob.glob('../input/d-large-roberta-b32-steplr3-drop-tb-rmf-h56-1/*.pt')\npretrain_weighted_2 = glob.glob('../input/d-large-roberta-b32-steplr3-drop-tb-rmf-h56-2/*.pt')\npretrain_weighted = pretrain_weighted_1 + pretrain_weighted_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_weighted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi # check GPU Memory\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cuda.select_device(0) #clear GPU memory \n#cuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda.select_device(0) #restart cuda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi #Check GPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set    = QuestDataset_test(test_tokens, test_masks, test_segments,\n                                test_df['host'].values,\n                                test_df['category'].values,\n                                train_handmade_features)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = QuestModel6(n_cat, cat_emb, n_host, host_emb, num_labels=30)\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, weight in tqdm(enumerate(pretrain_weighted)):\n    model.load_state_dict(torch.load(weight))\n    results.append(predict_result(model, test_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from scipy.stats import rankdata\n\n#output = np.array([np.array([rankdata(c) for c in p.T]).T for p in results]).mean(axis=0)\n#max_val = output.max() + 1\n#output = output/max_val + 1e-12\n\noutput_roberta = np.zeros((len(test_set),30))\nfor result in results:\n    output_roberta += result\noutput_roberta /= len(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del results, result, test_set, test_loader, model; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache() ## Clear_Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle/input/d-large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED                = 1414\nDATA_DIR            = \"../input/google-quest-challenge/\"\nWORK_DIR            = \"../working/\"\nBERT_VOCAB_PATH     = \"../input/xlnetlargecased/xlnet_cased_L-24_H-1024_A-16/spiece.model\"\nBERT_MODEL_PATH     = \"../input/xlnetlargecased/xlnet_cased_L-24_H-1024_A-16/xlnet_model.ckpt.index\"\nBERT_CONFIG_PATH    = \"../input/xlnetlargecased/xlnet_cased_L-24_H-1024_A-16/xlnet_config.json\"\ninput_columns       = ['question_title', 'question_body', 'answer']\nseed_everything(SEED)\nbatch_size          = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df  = pd.read_csv(os.path.join(DATA_DIR,\"test.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\ndef add_external_features(df):\n    \n    eng_stopwords = set(stopwords.words(\"english\"))\n    #If the question is longer, it may be more clear, which may help users give a more \n    df['question_body']      = df['question_body'].apply(lambda x:str(x))\n    df['question_num_words'] = df.question_body.str.count('\\S+')\n    df['question_title_num_words'] = df.question_title.str.count('\\S+')\n    \n    #The assumption here is that longer answer could bring more useful detail\n    df['answer']            = df['answer'].apply(lambda x:str(x))\n    df['answer_num_words']  = df.answer.str.count('\\S+')\n    \n    df[\"question_title_num_unique_words\"] = df[\"question_title\"].apply(lambda x: len(set(str(x).split())))\n    df[\"question_body_num_unique_words\"]  = df[\"question_body\"].apply(lambda x: len(set(str(x).split())))\n    df[\"answer_num_unique_words\"]         = df[\"answer\"].apply(lambda x: len(set(str(x).split())))\n    \n    df[\"question_title_num_chars\"] = df[\"question_title\"].apply(lambda x: len(str(x)))\n    df[\"question_body_num_chars\"]  = df[\"question_body\"].apply(lambda x: len(str(x)))\n    df[\"answer_num_chars\"]         = df[\"answer\"].apply(lambda x: len(str(x)))\n    \n    df['qt_words'] = df['question_title'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n    df['q_words'] = df['question_body'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n    df['a_words'] = df['answer'].apply(lambda s: [f for f in s.split() if f not in eng_stopwords])\n    df['qa_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['q_words'], s['a_words'])), axis = 1)\n    df['qt_word_overlap'] = df.apply(lambda s: len(np.intersect1d(s['qt_words'], s['a_words'])), axis = 1)\n    \n    df['qa_word_overlap_norm'] = df.apply(lambda s: s['qa_word_overlap']/(len(s['a_words']) + len(s['q_words'])  - s['qa_word_overlap']) , axis = 1)\n    df['qta_word_overlap_norm'] = df.apply(lambda s: s['qt_word_overlap']/(len(s['a_words']) + len(s['qt_words']) - s['qt_word_overlap']), axis = 1)\n    df.drop(['q_words', 'a_words', 'qt_words'], axis = 1, inplace = True)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = add_external_features(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"handmade_cols = [\"question_body_num_unique_words\", 'question_num_words',\n                 \"question_title_num_unique_words\", \"question_title_num_words\", \n                 \"answer_num_unique_words\", \"answer_num_words\"]\n\nwith open('/kaggle/input/d-large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/C_scaler.pickle', mode='rb') as f:\n    num_words_scaler = pickle.load(f)\n\ntest_df[handmade_cols]=  num_words_scaler.transform(test_df[handmade_cols].values)\ntrain_handmade_features= test_df[handmade_cols + ['qa_word_overlap_norm', 'qta_word_overlap_norm']].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoder(x, dict_reverse):\n    try:\n        return dict_reverse[x]\n    except:\n        return 0\n\nwith open('/kaggle/input/d-large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/C_category.pickle', mode='rb') as f:\n    category_dict_reverse = pickle.load(f)\ntest_df['category'] = test_df['category'].apply(lambda x: label_encoder(x, category_dict_reverse))\n\nwith open('/kaggle/input/d-large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/C_host.pickle', mode='rb') as f:\n    host_dict_reverse = pickle.load(f)\ntest_df['host'] = test_df['host'].apply(lambda x: label_encoder(x, host_dict_reverse))\n\nn_cat    = len(category_dict_reverse) + 1\ncat_emb  = 256\nn_host   = len(host_dict_reverse) + 1\nhost_emb = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_host","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = transformers.XLNetConfig.from_json_file(BERT_CONFIG_PATH)\nbert_config.output_hidden_states = True\ntokenizer = transformers.XLNetTokenizer(BERT_VOCAB_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments2(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q_len_head = round(q_new_len/3)\n        q_len_tail = -1* (q_new_len -q_len_head)\n        a_len_head = round(a_new_len/3)\n        a_len_tail = -1* (a_new_len -a_len_head) \n        \n        q = q[:q_len_head]+q[q_len_tail:]\n        a = a[:a_len_head]+a[a_len_tail:]\n    \n    return t, q, a\n\n\ndef convert_lines(question, answer, max_sequence_length, tokenizer):\n    \n    all_tokens   = []\n    all_masks    = []\n    all_segments = []\n    longer = 0\n    for q, a in tqdm(zip(question, answer)):\n        \n        tokens_q = tokenizer.tokenize(q)\n        tokens_a = tokenizer.tokenize(a)\n        \n        q_len = len(tokens_q)\n        a_len = len(tokens_a)\n        \n        if (q_len+a_len+3) > max_sequence_length:\n            longer += 1\n            new_q_len = q_len/(a_len+q_len) * (max_sequence_length-3)\n            new_a_len = a_len/(q_len+a_len) * (max_sequence_length-3)\n            new_q_len, new_a_len = int(np.ceil(new_q_len)), int(np.floor(new_a_len))\n            \n            if new_a_len+new_q_len+3 != max_sequence_length:\n                raise ValueError(\"too small %s\" % str(new_a_len+new_q_len+3))\n                \n            tokens_q = tokens_q[:new_q_len]\n            tokens_a = tokens_a[:new_a_len]\n            \n        stoken = [\"[CLS]\"] + tokens_q + [\"[SEP]\"] + tokens_a + [\"[SEP]\"]\n        #print(stoken)\n        \n        ##############\n        #token_ids\n        ##############\n        token_ids = tokenizer.encode(stoken, add_special_tokens=Ture)\n        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n        \n        #############\n        #input_masks\n        #############\n        attention_masks = _get_masks(stoken, max_sequence_length)\n        \n        ##############\n        #input_segments\n        ###############\n        input_segments = _get_segments2(stoken, max_sequence_length)\n        \n        all_tokens.append(input_ids)\n        all_masks.append(attention_masks)\n        all_segments.append(input_segments)\n    \n    print(longer)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines2(title, question, answer, max_sequence_length, tokenizer, t_max_len_seq=30, q_max_len_seq=239, a_max_len_seq=239):\n    \n    all_tokens   = []\n    all_masks    = []\n    all_segments = []\n    \n    longer = 0\n    \n    for t, q, a in tqdm(zip(title, question, answer)):\n        \n        tokens_t, tokens_q, tokens_a  = _trim_input(t, q, a, max_sequence_length=max_sequence_length)\n        #print(tokens_t)\n        #print(tokens_q)\n        #print(tokens_a)\n        \n        stoken = tokens_t + [\"[SEP]\"] + tokens_q + [\"[SEP]\"] + tokens_a + [\"[SEP]\"] + [\"[CLS]\"]\n        ##############\n        #token_ids\n        ##############\n        token_ids = tokenizer.convert_tokens_to_ids(stoken)\n        input_ids = token_ids + [0] * (max_sequence_length-len(token_ids))\n        \n        #############\n        #input_masks\n        #############\n        attention_masks = _get_masks(stoken, max_sequence_length)\n        #print(attention_masks)\n        \n        ##############\n        #input_segments\n        ###############\n        input_segments = _get_segments2(stoken, max_sequence_length)\n        \n        all_tokens.append(input_ids)\n        all_masks.append(attention_masks)\n        all_segments.append(input_segments)\n        #break\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokens, test_masks, test_segments = convert_lines2(test_df[\"question_title\"],\n                                                       test_df[\"question_body\"], \n                                                       test_df[\"answer\"],\n                                                       max_sequence_length=512, \n                                                       tokenizer=tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset_test(Dataset):\n\n    def __init__(self, token_ids, masks, segments, hosts, categories, handmade_features):\n                \n        self.token_ids  = token_ids\n        self.masks      = masks\n        self.segments   = segments\n        self.hosts      = hosts\n        self.categories = categories\n        self.handmades  = handmade_features\n\n    def __len__(self):\n        return self.token_ids.shape[0]\n\n    def __getitem__(self, idx):\n        token_id = self.token_ids[idx]\n        mask     = self.masks[idx]\n        segment  = self.segments[idx]\n        host     = self.hosts[idx]\n        category = self.categories[idx]\n        handmade = self.handmades[idx]\n\n        return [token_id, mask, segment, host, category, handmade]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def children(m):\n    return m if isinstance(m, (list, tuple)) else list(m.children())\n\ndef set_trainable_attr(m, b):\n    m.trainable = b\n    for p in m.parameters():\n        p.requires_grad = b\n\n\ndef apply_leaf(m, f):\n    c = children(m)\n    if isinstance(m, torch.nn.Module):\n        f(m)\n    if len(c) > 0:\n        for l in c:\n            apply_leaf(l, f)\n\ndef set_trainable(l, b):\n    apply_leaf(l, lambda m: set_trainable_attr(m, b))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n\n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim),\n            self.weight\n        ).view(-1, step_dim)\n\n        if self.bias:\n            eij = eij + self.b\n\n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n        weighted_input = x * torch.unsqueeze(a, -1)\n        \n        return torch.sum(weighted_input, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestModel6(nn.Module):\n\n    def __init__(self, n_cat, cat_emb, n_host, host_emb, num_labels):\n        super().__init__()\n        BERT_DIMS = 1024\n        LSTM_UNITS = 512\n        model_path = os.path.join(BERT_MODEL_PATH)\n        self.bert_model = transformers.XLNetModel.from_pretrained(model_path,from_tf=True, config=bert_config)\n        set_trainable(self.bert_model, False)\n        \n        self.category_embedding = nn.Embedding(n_cat, cat_emb)\n        self.host_embedding     = nn.Embedding(n_host, host_emb)\n        self.embedding_dropout  = SpatialDropout(0.5)\n        self.lstm1              = nn.LSTM(BERT_DIMS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2              = nn.GRU(LSTM_UNITS*2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.atten1             = Attention(LSTM_UNITS*2, 512)\n        self.atten2             = Attention(LSTM_UNITS*2, 512)\n        self.dropout1           = nn.Dropout(0.2)\n        self.fc1                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 21)\n        self.fc2                = nn.Linear(LSTM_UNITS*6 + int(cat_emb) + int(host_emb) + 8, 9)\n        self._init_weights(self.category_embedding)\n        self._init_weights(self.host_embedding)\n        self._init_weights(self.fc1)\n        self._init_weights(self.fc2)\n    \n    def _init_weights(self, module):\n        \"\"\" Initialize the weights \"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            print(\"initailize weight\")\n            module.weight.data.normal_(mean=0.0, std=0.02)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            print(\"initailize bias\")\n            module.bias.data.zero_()\n        \n    def forward(self, token_ids, masks, segment, hosts, categories, handmades):\n        \n        category_embed = self.category_embedding(categories)\n        host_embed     = self.host_embedding(hosts)\n        external_features = torch.cat((category_embed, host_embed, handmades), 1)\n        #print(external_features.shape)\n        seq_output, hidden_states = self.bert_model(input_ids=token_ids,\n                                                    attention_mask=masks,\n                                                    token_type_ids=segment)\n        \n        last_four_layer = torch.cat((hidden_states[-5], hidden_states[-6]), 2)\n        lstm_input      = self.embedding_dropout(last_four_layer)\n        lstm1_output, _ = self.lstm1(lstm_input)\n        lstm2_output, _ = self.lstm2(lstm1_output)\n        \n        meanpooled_output   = torch.mean(lstm2_output, 1)\n        maxpooled_output, _ = torch.max(lstm2_output, 1)\n        attention_output_q  = self.atten1(lstm2_output)\n        attention_output_a  = self.atten2(lstm2_output)\n        \n        pooled_output_q = torch.cat((meanpooled_output, maxpooled_output, attention_output_q, external_features), 1)\n        pooled_output_q = self.dropout1(pooled_output_q)\n        \n        pooled_output_a = torch.cat((meanpooled_output, maxpooled_output, attention_output_a, external_features), 1)\n        pooled_output_a = self.dropout1(pooled_output_a)\n        \n        q_results     = self.fc1(pooled_output_q)\n        a_results     = self.fc2(pooled_output_a)\n        results       = torch.cat((q_results, a_results), 1)\n        \n        return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_result(model, test_loader, batch_size=batch_size):\n    \n    output = np.zeros((len(test_set), 30))\n    model.eval()\n    with torch.no_grad():\n        for idx, inputs in enumerate(test_loader):\n            start_index = idx * batch_size\n            end_index   = min(start_index + batch_size, len(test_set))\n            token_ids, masks, segments, hosts, categories, handmades = inputs\n            token_ids   = token_ids.long().cuda()\n            masks       = masks.long().cuda()\n            segments    = segments.long().cuda()\n            hosts       = hosts.long().cuda()\n            categories  = categories.long().cuda()\n            handmades   = handmades.float().cuda()\n            \n            predictions = model(token_ids, masks, segments, hosts, categories, handmades)\n            predictions = torch.sigmoid(predictions)\n            output[start_index:end_index, :] = predictions.detach().cpu().numpy()\n            \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\n! ls /kaggle/input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_weighted_1 = glob.glob('../input/d-large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-1/*.pt')\npretrain_weighted_2 = glob.glob('../input/d-large-xlnet-b32-steplr3-changedrop-tb-rmf-h56-2/*.pt')\npretrain_weighted = pretrain_weighted_1 + pretrain_weighted_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_weighted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi # check GPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cuda.select_device(0) #clear GPU memory \n#cuda.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cuda.select_device(0) #restart cuda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!nvidia-smi #Check GPU Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set    = QuestDataset_test(test_tokens, test_masks, test_segments,\n                                test_df['host'].values,\n                                test_df['category'].values,\n                                train_handmade_features)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = QuestModel6(n_cat, cat_emb, n_host, host_emb, num_labels=30)\nmodel.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, weight in tqdm(enumerate(pretrain_weighted)):\n    model.load_state_dict(torch.load(weight))\n    results.append(predict_result(model, test_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from scipy.stats import rankdata\n\n#output = np.array([np.array([rankdata(c) for c in p.T]).T for p in results]).mean(axis=0)\n#max_val = output.max() + 1\n#output = output/max_val + 1e-12\n\noutput_xlnet = np.zeros((len(test_set),30))\nfor result in results:\n    output_xlnet += result\noutput_xlnet /= len(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del results, result, test_set, test_loader, model; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache() ## Clear_Memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\n#sys.path.insert(0, \"../input/transformers/transformers-master/\")\nsys.path.append(\"../input/mynlpscripts/\")\n#sys.path.append('/content/gdrive/My Drive/googlequest')\nsys.path.extend(['../input/bert-utils/'])\n#import transformers\nimport numpy as np\nimport pandas as pd\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport matplotlib.pylab as plt\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nprint(\"TF version:\", tf.__version__)\nprint(\"Hub version:\", hub.__version__)\n#print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n\nimport re\nimport os\nimport sys\nimport json\nimport math\n\nimport logging\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n\nfrom sklearn.model_selection import train_test_split\n#from google.colab import auth, drive\n\n#if not 'bert_repo' in sys.path:\n#    sys.path.insert(0, 'bert_repo')\n\nfrom modeling import BertModel, BertConfig\nfrom tokenization import FullTokenizer, convert_to_unicode\nfrom extract_features import InputExample, convert_examples_to_features\n\n\n# get TF logger \nlog = logging.getLogger('tensorflow')\nlog.handlers = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport random\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter, defaultdict\nimport re\nimport gc\nimport pickle  \nimport random\nfrom tensorflow import keras\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport numpy as np\nimport pandas as pd\n#import tensorflow as tf\n#import tensorflow_hub as hub\n#from bert_tokenization import FullTokenizer\n#import bert.tokenization as tokenization\nimport tensorflow.keras.backend as K\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport gc\nimport json\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n#import nltk\n#from nltk.corpus import stopwords\n#eng_stopwords = set(stopwords.words(\"english\"))\nimport string\n\n\nfrom copy import deepcopy\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Lambda\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D, Reshape\nfrom tensorflow.keras.initializers import he_normal, he_uniform, glorot_normal, glorot_uniform, VarianceScaling\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding,GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout, Bidirectional, Conv1D, MaxPool1D, Flatten, GRU, Concatenate, SpatialDropout1D, GlobalMaxPool1D\nfrom tensorflow.keras.models import model_from_json\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet, MultiTaskElasticNetCV\nfrom sklearn.multioutput import MultiOutputRegressor, RegressorChain, ClassifierChain\nfrom sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n                                     StratifiedKFold, GroupShuffleSplit,\n                                     GroupKFold, StratifiedShuffleSplit)\n\nfrom sklearn.model_selection import BaseCrossValidator\n\nseed(42)\ntf.set_random_seed(42)\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"This file includes multilabel cross validators based on an implementation of\nthe Iterative Stratification algorithm described in the following paper:\nSechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-\nLabel Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M. (eds)\nMachine Learning and Knowledge Discovery in Databases. ECML PKDD 2011. Lecture\nNotes in Computer Science, vol 6913. Springer, Berlin, Heidelberg.\nFrom scikit-learn 0.19.0, StratifiedKFold, RepeatedStratifiedKFold, and\nStratifiedShuffleSplit were copied and modified, retaining compatibility\nwith scikit-learn.\nAttribution to authors of scikit-learn/model_selection/_split.py under BSD 3 clause:\n    Alexandre Gramfort <alexandre.gramfort@inria.fr>,\n    Gael Varoquaux <gael.varoquaux@normalesup.org>,\n    Olivier Grisel <olivier.grisel@ensta.org>,\n    Raghav RV <rvraghav93@gmail.com>\n\"\"\"\n\n# Author: Trent J. Bradberry <trentjason@hotmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.utils import check_random_state\nfrom sklearn.utils.validation import _num_samples, check_array\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n    BaseShuffleSplit, _validate_shuffle_split\n\n\ndef IterativeStratification(labels, r, random_state):\n    \"\"\"This function implements the Iterative Stratification algorithm described\n    in the following paper:\n    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n    Heidelberg.\n    \"\"\"\n\n    n_samples = labels.shape[0]\n    test_folds = np.zeros(n_samples, dtype=int)\n\n    # Calculate the desired number of examples at each subset\n    c_folds = r * n_samples\n\n    # Calculate the desired number of examples of each label at each subset\n    c_folds_labels = np.outer(r, labels.sum(axis=0))\n\n    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n\n    while np.any(labels_not_processed_mask):\n        # Find the label with the fewest (but at least one) remaining examples,\n        # breaking ties randomly\n        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n\n        # Handle case where only all-zero labels are left by distributing\n        # across all folds as evenly as possible (not in original algorithm but\n        # mentioned in the text). (By handling this case separately, some\n        # code redundancy is introduced; however, this approach allows for\n        # decreased execution time when there are a relatively large number\n        # of all-zero labels.)\n        if num_labels.sum() == 0:\n            sample_idxs = np.where(labels_not_processed_mask)[0]\n\n            for sample_idx in sample_idxs:\n                fold_idx = np.where(c_folds == c_folds.max())[0]\n\n                if fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n\n                test_folds[sample_idx] = fold_idx\n                c_folds[fold_idx] -= 1\n\n            break\n\n        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n        if label_idx.shape[0] > 1:\n            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n\n        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n\n        for sample_idx in sample_idxs:\n            # Find the subset(s) with the largest number of desired examples\n            # for this label, breaking ties by considering the largest number\n            # of desired examples, breaking further ties randomly\n            label_folds = c_folds_labels[:, label_idx]\n            fold_idx = np.where(label_folds == label_folds.max())[0]\n\n            if fold_idx.shape[0] > 1:\n                temp_fold_idx = np.where(c_folds[fold_idx] ==\n                                         c_folds[fold_idx].max())[0]\n                fold_idx = fold_idx[temp_fold_idx]\n\n                if temp_fold_idx.shape[0] > 1:\n                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n\n            test_folds[sample_idx] = fold_idx\n            labels_not_processed_mask[sample_idx] = False\n\n            # Update desired number of examples\n            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n            c_folds[fold_idx] -= 1\n\n    return test_folds\n\n\nclass MultilabelStratifiedKFold(_BaseKFold):\n    \"\"\"Multilabel stratified K-Folds cross-validator\n    Provides train/test indices to split multilabel data into train/test sets.\n    This cross-validation object is a variation of KFold that returns\n    stratified folds for multilabel data. The folds are made by preserving\n    the percentage of samples for each label.\n    Parameters\n    ----------\n    n_splits : int, default=3\n        Number of folds. Must be at least 2.\n    shuffle : boolean, optional\n        Whether to shuffle each stratification of the data before splitting\n        into batches.\n    random_state : int, RandomState instance or None, optional, default=None\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedKFold that only uses random_state\n        when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n    >>> mskf.get_n_splits(X, y)\n    2\n    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n    >>> for train_index, test_index in mskf.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    Notes\n    -----\n    Train and test sizes may be slightly different in each fold.\n    See also\n    --------\n    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n    n times.\n    \"\"\"\n\n    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n        super(MultilabelStratifiedKFold, self).__init__(n_splits, shuffle, random_state)\n\n    def _make_test_folds(self, X, y):\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n\n        num_samples = y.shape[0]\n\n        rng = check_random_state(self.random_state)\n        indices = np.arange(num_samples)\n\n        if self.shuffle:\n            rng.shuffle(indices)\n            y = y[indices]\n\n        r = np.asarray([1 / self.n_splits] * self.n_splits)\n\n        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n        return test_folds[np.argsort(indices)]\n\n    def _iter_test_masks(self, X=None, y=None, groups=None):\n        test_folds = self._make_test_folds(X, y)\n        for i in range(self.n_splits):\n            yield test_folds == i\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n\n\nclass RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n    Repeats Mulilabel Stratified K-Fold n times with different randomization\n    in each repetition.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    n_repeats : int, default=10\n        Number of times cross-validator needs to be repeated.\n    random_state : None, int or RandomState, default=None\n        Random state to be used to generate random state for each\n        repetition as well as randomly breaking ties within the iterative\n        stratification algorithm.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n    ...     random_state=0)\n    >>> for train_index, test_index in rmskf.split(X, y):\n    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...     X_train, X_test = X[train_index], X[test_index]\n    ...     y_train, y_test = y[train_index], y[test_index]\n    ...\n    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    See also\n    --------\n    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n    n times.\n    \"\"\"\n    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n            MultilabelStratifiedKFold, n_repeats, random_state,\n            n_splits=n_splits)\n\n\nclass MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n    Provides train/test indices to split data into train/test sets.\n    This cross-validation object is a merge of MultilabelStratifiedKFold and\n    ShuffleSplit, which returns stratified randomized folds for multilabel\n    data. The folds are made by preserving the percentage of each label.\n    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n    do not guarantee that all folds will be different, although this is\n    still very likely for sizeable datasets.\n    Parameters\n    ----------\n    n_splits : int, default 10\n        Number of re-shuffling & splitting iterations.\n    test_size : float, int, None, optional\n        If float, should be between 0.0 and 1.0 and represent the proportion\n        of the dataset to include in the test split. If int, represents the\n        absolute number of test samples. If None, the value is set to the\n        complement of the train size. By default, the value is set to 0.1.\n        The default will change in version 0.21. It will remain 0.1 only\n        if ``train_size`` is unspecified, otherwise it will complement\n        the specified ``train_size``.\n    train_size : float, int, or None, default is None\n        If float, should be between 0.0 and 1.0 and represent the\n        proportion of the dataset to include in the train split. If\n        int, represents the absolute number of train samples. If None,\n        the value is automatically set to the complement of the test size.\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance used\n        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n        random_state when ``shuffle`` == True, this multilabel implementation\n        always uses the random_state since the iterative stratification\n        algorithm breaks ties randomly.\n    Examples\n    --------\n    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n    >>> import numpy as np\n    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n    ...    random_state=0)\n    >>> msss.get_n_splits(X, y)\n    3\n    >>> print(mss)       # doctest: +ELLIPSIS\n    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n                                     train_size=None)\n    >>> for train_index, test_index in msss.split(X, y):\n    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    ...    X_train, X_test = X[train_index], X[test_index]\n    ...    y_train, y_test = y[train_index], y[test_index]\n    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n    Notes\n    -----\n    Train and test sizes may be slightly different from desired due to the\n    preference of stratification over perfectly sized folds.\n    \"\"\"\n\n    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n                 random_state=None):\n        super(MultilabelStratifiedShuffleSplit, self).__init__(\n            n_splits, test_size, train_size, random_state)\n\n    def _iter_indices(self, X, y, groups=None):\n        n_samples = _num_samples(X)\n        y = check_array(y, ensure_2d=False, dtype=None)\n        y = np.asarray(y, dtype=bool)\n        type_of_target_y = type_of_target(y)\n\n        if type_of_target_y != 'multilabel-indicator':\n            raise ValueError(\n                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n                    type_of_target_y))\n\n        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n                                                  self.train_size)\n\n        n_samples = y.shape[0]\n        rng = check_random_state(self.random_state)\n        y_orig = y.copy()\n\n        r = np.array([n_train, n_test]) / (n_train + n_test)\n\n        for _ in range(self.n_splits):\n            indices = np.arange(n_samples)\n            rng.shuffle(indices)\n            y = y_orig[indices]\n\n            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n\n            test_idx = test_folds[np.argsort(indices)] == 1\n            test = np.where(test_idx)[0]\n            train = np.where(~test_idx)[0]\n\n            yield train, test\n\n    def split(self, X, y, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n            Note that providing ``y`` is sufficient to generate the splits and\n            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n            ``X`` instead of actual training data.\n        y : array-like, shape (n_samples, n_labels)\n            The target variable for supervised learning problems.\n            Multilabel stratification is done based on the y labels.\n        groups : object\n            Always ignored, exists for compatibility.\n        Returns\n        -------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        Notes\n        -----\n        Randomized CV splitters may return different results for each call of\n        split. You can make the results identical by setting ``random_state``\n        to an integer.\n        \"\"\"\n        y = check_array(y, ensure_2d=False, dtype=None)\n        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_module_fn(config_path, vocab_path, do_lower_case=True):\n\n    def bert_module_fn(is_training):\n        \"\"\"Spec function for a token embedding module.\"\"\"\n\n        input_ids = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_ids\")\n        input_mask = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"input_mask\")\n        token_type = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"segment_ids\")\n\n        config = BertConfig.from_json_file(config_path)\n        model = BertModel(config=config, is_training=is_training,\n                          input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type)\n          \n        seq_output = model.all_encoder_layers[-1]\n        pool_output = model.get_pooled_output()\n\n        config_file = tf.constant(value=config_path, dtype=tf.string, name=\"config_file\")\n        vocab_file = tf.constant(value=vocab_path, dtype=tf.string, name=\"vocab_file\")\n        lower_case = tf.constant(do_lower_case)\n\n        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, config_file)\n        tf.add_to_collection(tf.GraphKeys.ASSET_FILEPATHS, vocab_file)\n        \n        input_map = {\"input_ids\": input_ids,\n                     \"input_mask\": input_mask,\n                     \"segment_ids\": token_type}\n        \n        output_map = {\"pooled_output\": pool_output,\n                      \"sequence_output\": seq_output}\n\n        output_info_map = {\"vocab_file\": vocab_file,\n                           \"do_lower_case\": lower_case}\n                \n        hub.add_signature(name=\"tokens\", inputs=input_map, outputs=output_map)\n        hub.add_signature(name=\"tokenization_info\", inputs={}, outputs=output_info_map)\n\n    return bert_module_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_DIR1 = '../input/bertbase' #\"uncased_L-12_H-768_A-12\" \n\nconfig_path1 = \"{}/config.json\".format(MODEL_DIR1)\nvocab_path1 = \"{}/vocab.txt\".format(MODEL_DIR1)\n\ntags_and_args = []\nfor is_training in (True, False):\n    tags = set()\n    if is_training:\n        tags.add(\"train\")\n    tags_and_args.append((tags, dict(is_training=is_training)))\n\nmodule_fn = build_module_fn(config_path1, vocab_path1)\nspec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\nspec.export(\"bert-module1\", \n            checkpoint_path=\"{}/bert_model.ckpt\".format(MODEL_DIR1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_DIR = '../input/bertlarge' #\"uncased_L-12_H-768_A-12\" \n\nconfig_path = \"{}/bert_config.json\".format(MODEL_DIR)\nvocab_path = \"{}/vocab.txt\".format(MODEL_DIR)\n\ntags_and_args = []\nfor is_training in (True, False):\n    tags = set()\n    if is_training:\n        tags.add(\"train\")\n    tags_and_args.append((tags, dict(is_training=is_training)))\n\nmodule_fn = build_module_fn(config_path, vocab_path)\nspec = hub.create_module_spec(module_fn, tags_and_args=tags_and_args)\nspec.export(\"bert-module\", \n            checkpoint_path=\"{}/bert_model.ckpt\".format(MODEL_DIR))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"PATH = '../input/google-quest-challenge/'\nBERT_PATH = '../input/bert-tfhub' #/bert_en_uncased_L-12_H-768_A-12\nUSE_PATH  = '../input/uselarge4/'\ntokenizer = FullTokenizer(vocab_path, True)\ntokenizer1 = FullTokenizer(vocab_path1, True)\nMAX_SEQUENCE_LENGTH = 512\nMAX_SEQUENCE_LENGTH_t = 120\nMAX_SEQUENCE_LENGTH_q = 416\nMAX_SEQUENCE_LENGTH_a =  416\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_examples(str_list):\n    \"\"\"Read a list of `InputExample`s from a list of strings.\"\"\"\n    unique_id = 0\n    for s in str_list:\n        line = convert_to_unicode(s)\n        if not line:\n            continue\n        line = line.strip()\n        text_a = None\n        text_b = None\n        m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n        if m is None:\n            text_a = line\n        else:\n            text_a = m.group(1)\n            text_b = m.group(2)\n        yield InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b)\n        unique_id += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features_to_arrays(features):\n\n    all_input_ids = []\n    all_input_mask = []\n    all_segment_ids = []\n\n    for feature in features:\n        all_input_ids.append(feature.input_ids)\n        all_input_mask.append(feature.input_mask)\n        all_segment_ids.append(feature.input_type_ids)\n\n    return (np.array(all_input_ids, dtype='int32'), \n            np.array(all_input_mask, dtype='int32'), \n            np.array(all_segment_ids, dtype='int32'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_preprocessor(voc_path, seq_len, lower=True):\n    tokenizer = FullTokenizer(vocab_file=voc_path, do_lower_case=lower)\n    def strings_to_arrays(sents):\n        sents = np.atleast_1d(sents).reshape((-1,))\n        examples = []\n        for example in read_examples(sents):\n            examples.append(example)\n\n        features = convert_examples_to_features(examples, seq_len, tokenizer)\n        arrays = features_to_arrays(features)\n        return arrays\n  \n    return strings_to_arrays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [       \n    'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title', 'question_body', 'answer']\n#y_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def _get_masks1(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments1(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids1(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input1(title, question, answer,tokenizer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n  \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs1(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids1(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks1(stoken, max_sequence_length)\n    input_segments = _get_segments1(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays1(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input1(t, q, a,tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs1(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays1(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n\ndef _trim_input(question,tokenizer,  max_sequence_length):\n    \"\"\"Trims tokenized input to max_sequence_length, \n    while keeping the same ratio of Q and A length\"\"\"\n    q = tokenizer.tokenize(question)\n    #a = tokenizer.tokenize(answer)\n    \n    q_len = len(q)\n    #a_len = len(a)\n    \n    if (q_len+3) > max_sequence_length:\n        \n        new_q_len = max_sequence_length-3\n        #new_a_len = a_len/(q_len+a_len) * (max_sequence_length-3)\n        #new_q_len, new_a_len = int(ceil(new_q_len)), int(floor(new_a_len))\n            \n        if new_q_len+3 != max_sequence_length:\n            raise ValueError(\"too small %s\" % str(new_q_len+3))\n        \n        q = q[:new_q_len]\n        #a = a[:new_a_len]\n    \n    return q\n\ndef _convert_to_bert_inputs(question, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + question + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays_q(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q = instance.question_body\n\n        q= _trim_input(q, tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(\n            q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_input_arays_a(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q = instance.answer\n\n        q= _trim_input(q,  tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(\n            q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\ndef compute_input_arays_t(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q = instance.question_title\n\n        q= _trim_input(q,  tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(\n            q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"coef_nn = 2\ncoef_mte = 1.7\ncoef_lgb =1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TDense(tf.keras.layers.Layer):\n    def __init__(self,\n                 output_size,\n                 kernel_initializer=None,\n                 bias_initializer=\"zeros\",\n                **kwargs):\n        super().__init__(**kwargs)\n        self.output_size = output_size\n        self.kernel_initializer = kernel_initializer\n        self.bias_initializer = bias_initializer\n    def build(self,input_shape):\n        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n        if not (dtype.is_floating or dtype.is_complex):\n            raise TypeError(\"Unable to build `TDense` layer with \"\n                          \"non-floating point (and non-complex) \"\n                          \"dtype %s\" % (dtype,))\n        input_shape = tf.TensorShape(input_shape)\n        if tf.compat.dimension_value(input_shape[-1]) is None:\n            raise ValueError(\"The last dimension of the inputs to \"\n                           \"`TDense` should be defined. \"\n                           \"Found `None`.\")\n        last_dim = tf.compat.dimension_value(input_shape[-1])\n        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n        self.kernel = self.add_weight(\n            \"kernel\",\n            shape=[self.output_size,last_dim],\n            initializer=self.kernel_initializer,\n            dtype=self.dtype,\n            trainable=True)\n        self.bias = self.add_weight(\n            \"bias\",\n            shape=[self.output_size],\n            initializer=self.bias_initializer,\n            dtype=self.dtype,\n            trainable=True)\n        super(TDense, self).build(input_shape)\n    def call(self,x):\n        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compatible with tensorflow backend\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.nanmean(rhos)\n\ndef average_spearmanr(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    return np.average([\n        spearmanr(y_t, y_p).correlation for y_t, y_p in zip(y_true.T, y_pred.T)\n    ])\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.nanmean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        #rho_val = compute_spearmanr(self.y_val, y_pred_val)\n        if rho_val >= self.value:\n            self.value = rho_val\n            self.model.save_weights(self.model_name)\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_learning_rate_scheduler(max_learn_rate=1e-5,\n                                   end_learn_rate=1e-7,\n                                   warmup_epoch_count=2,\n                                   total_epoch_count=5):\n\n    def lr_scheduler(epoch):\n        if epoch < warmup_epoch_count:\n            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n        else:\n            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n        return float(res)\n    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n\n    return learning_rate_scheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertLayer1(tf.keras.layers.Layer):\n    def __init__(self, bert_path, seq_len=64, n_tune_layers=3, \n                 pooling=\"cls\", do_preprocessing=True, verbose=False,\n                 tune_embeddings=False, trainable=True, **kwargs):\n\n        self.trainable = trainable\n        self.n_tune_layers = n_tune_layers\n        self.tune_embeddings = tune_embeddings\n        self.do_preprocessing = do_preprocessing\n\n        self.verbose = verbose\n        self.seq_len = seq_len\n        self.pooling = pooling\n        self.bert_path = bert_path\n\n        self.var_per_encoder = 16\n        if self.pooling not in [\"cls\", \"mean\", None]:\n            raise NameError(\n                f\"Undefined pooling type (must be either 'cls', 'mean', or None, but is {self.pooling}\"\n            )\n\n        super(BertLayer1, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        self.bert = hub.Module(self.build_abspath(self.bert_path), \n                               trainable=self.trainable, name=f\"{self.name}_module\")\n\n        trainable_layers = []\n        #trainable_layers.append(\"pooler\")\n        if self.tune_embeddings:\n            trainable_layers.append(\"embeddings\")\n\n        #if self.pooling == \"cls\":\n        #    trainable_layers.append(\"pooler\")\n\n        if self.n_tune_layers > 0:\n            encoder_var_names = [var.name for var in self.bert.variables if 'encoder' in var.name]\n            n_encoder_layers = int(len(encoder_var_names) / self.var_per_encoder)\n            for i in range(self.n_tune_layers):\n                trainable_layers.append(f\"encoder/layer_{str(n_encoder_layers - 1 - i)}/\")\n        \n        # Add module variables to layer's trainable weights\n        for var in self.bert.variables:\n            if any([l in var.name for l in trainable_layers]):\n                self._trainable_weights.append(var)\n            else:\n                self._non_trainable_weights.append(var)\n\n        if self.verbose:\n            print(\"*** TRAINABLE VARS *** \")\n            for var in self._trainable_weights:\n                print(var)\n\n        self.build_preprocessor()\n        self.initialize_module()\n\n        super(BertLayer1, self).build(input_shape)\n\n    def build_abspath(self, path):\n        if path.startswith(\"https://\") or path.startswith(\"gs://\"):\n            return path\n        else:\n            return os.path.abspath(path)\n\n    def build_preprocessor(self):\n        sess = tf.keras.backend.get_session()\n        tokenization_info = self.bert(signature=\"tokenization_info\", as_dict=True)\n        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                              tokenization_info[\"do_lower_case\"]])\n        self.preprocessor = build_preprocessor(vocab_file, self.seq_len, do_lower_case)\n\n    def initialize_module(self):\n        sess = tf.keras.backend.get_session()\n        \n        vars_initialized = sess.run([tf.is_variable_initialized(var) \n                                     for var in self.bert.variables])\n\n        uninitialized = []\n        for var, is_initialized in zip(self.bert.variables, vars_initialized):\n            if not is_initialized:\n                uninitialized.append(var)\n\n        if len(uninitialized):\n            sess.run(tf.variables_initializer(uninitialized))\n\n    def call(self, input):\n\n        if self.do_preprocessing:\n            input = tf.numpy_function(self.preprocessor, \n                                    [input], [tf.int32, tf.int32, tf.int32], \n                                    name='preprocessor')\n            for feature in input:\n                feature.set_shape((None, self.seq_len))\n        \n        input_ids, input_mask, segment_ids = input\n        \n        bert_inputs = dict(\n            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n        )\n        output = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n        \n        #if self.pooling == \"cls\":\n        pooled_cls = output[\"pooled_output\"]\n        #else:\n        result = output[\"sequence_output\"]\n        \n        input_mask = tf.cast(input_mask, tf.float32)\n        mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n        masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n                tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n        \n        #if self.pooling == \"mean\":\n        pooled_mean = masked_reduce_mean(result, input_mask)\n        #else:\n        sequence_output = mul_mask(result, input_mask)\n        if self.pooling == \"cls\":\n            return pooled_cls\n        else:\n            return pooled_mean, sequence_output\n\n    def get_config(self):\n        config_dict = {\n            \"bert_path\": self.bert_path, \n            \"seq_len\": self.seq_len,\n            \"pooling\": self.pooling,\n            \"n_tune_layers\": self.n_tune_layers,\n            \"tune_embeddings\": self.tune_embeddings,\n            \"do_preprocessing\": self.do_preprocessing,\n            \"verbose\": self.verbose\n        }\n        super(BertLayer1, self).get_config()\n        return config_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays1(df_train, output_categories)\n#inputs = compute_input_arays1(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays1(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n\ntest_inputst = compute_input_arays_t(df_test, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_t)\ntest_inputsq = compute_input_arays_q(df_test, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_q)\ntest_inputsa = compute_input_arays_a(df_test, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d= {0.: 0, 0.3: 1, 0.7: 2}\nfrom numpy import copy\ntheArray = np.round(outputs[:, 19:20], decimals=1)\nnewArray = copy(theArray)\nfor k, v in d.items(): newArray[theArray==k] = v\nnewArray = np.squeeze(newArray)\nnewArray =newArray.astype(int)\nnp.unique(newArray)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nall_predictions = []\ncv_sum = []\ncv_sum2 = []\ncv_sum3 = []\nkf = MultilabelStratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n#kf = KFold(n_splits=10, random_state=42, shuffle=True)\nfor i, (tr, val) in enumerate(stratified_group_k_fold(df_train.question_body, newArray, df_train.question_body, k=10,seed=42)):\n#kf = KFold(n_splits=5, random_state=42, shuffle=True)\n#for i, (tr, val) in enumerate(kf.split(X2.values)):\n    if i==1 or i ==2 or i ==3 or i ==6 or i ==7 or i ==9:\n        continue\n    K.clear_session()\n    print('starting fold: {0}'.format(i))\n    json_file = open('../input/save-groupbert{}/nn_model{}.json'.format(i, i), 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    model = model_from_json(loaded_model_json, custom_objects={\"BertLayer1\": BertLayer1})\n    # load weights into new model\n    bst_model_path = '../input/save-groupbert{}/dl_model{}.h5'.format(i, i)\n    print(bst_model_path)\n    model.load_weights(bst_model_path)\n    model.compile(optimizer=tf.keras.optimizers.Nadam(3e-5),loss=['binary_crossentropy'])\n    #model.summary()\n    print(\"Loaded model from disk\")\n\n    \n    all_predictions.append(model.predict([test_inputs[0],test_inputs[1],test_inputs[2],test_inputst[0],test_inputst[1],test_inputst[2], \n                                          test_inputsq[0],test_inputsq[1],test_inputsq[2], test_inputsa[0],test_inputsa[1],test_inputsa[2]]))\n    #model.save(data_dir + 'nn_model%s.h5'%i)\n    del model\n    #os.remove('best_model_batch.h5')\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del test_inputs, test_inputst, test_inputsq,test_inputsa; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(all_predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def _get_masks2(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments2(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids2(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n\ndef _trim_input2(question, answer,tokenizer, max_sequence_length):\n    \"\"\"Trims tokenized input to max_sequence_length, \n    while keeping the same ratio of Q and A length\"\"\"\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    q_len = len(q)\n    a_len = len(a)\n    \n    if (q_len+a_len+3) > max_sequence_length:\n        \n        new_q_len = q_len/(a_len+q_len) * (max_sequence_length-3)\n        new_a_len = a_len/(q_len+a_len) * (max_sequence_length-3)\n        new_q_len, new_a_len = int(ceil(new_q_len)), int(floor(new_a_len))\n            \n        if new_a_len+new_q_len+3 != max_sequence_length:\n            raise ValueError(\"too small %s\" % str(new_a_len+new_q_len+3))\n        \n        q = q[:new_q_len]\n        a = a[:new_a_len]\n    \n    return q, a\n\ndef _convert_to_bert_inputs2(question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids2(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks2(stoken, max_sequence_length)\n    input_segments = _get_segments2(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays2(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q, a = instance.question_title, instance.question_body\n\n        q, a = _trim_input2(q, a, tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs2(\n            q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def _get_masks3(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments3(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids3(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n\ndef _trim_input3(question, answer,tokenizer, max_sequence_length):\n    \"\"\"Trims tokenized input to max_sequence_length, \n    while keeping the same ratio of Q and A length\"\"\"\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    q_len = len(q)\n    a_len = len(a)\n    \n    if (q_len+a_len+3) > max_sequence_length:\n        \n        new_q_len = q_len/(a_len+q_len) * (max_sequence_length-3)\n        new_a_len = a_len/(q_len+a_len) * (max_sequence_length-3)\n        new_q_len, new_a_len = int(ceil(new_q_len)), int(floor(new_a_len))\n            \n        if new_a_len+new_q_len+3 != max_sequence_length:\n            raise ValueError(\"too small %s\" % str(new_a_len+new_q_len+3))\n        \n        q = q[:new_q_len]\n        a = a[:new_a_len]\n    \n    return q, a\n\ndef _convert_to_bert_inputs3(question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids3(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks3(stoken, max_sequence_length)\n    input_segments = _get_segments3(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays3(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q, a = instance.question_title, instance.answer\n\n        q, a = _trim_input3(q, a,tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs3(\n            q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def _get_masks1(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments1(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids1(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input1(title, question, answer,tokenizer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n  \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs1(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids1(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks1(stoken, max_sequence_length)\n    input_segments = _get_segments1(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays1(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input1(t, q, a,tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs1(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays1(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n\ndef _trim_input(question,tokenizer,  max_sequence_length):\n    \"\"\"Trims tokenized input to max_sequence_length, \n    while keeping the same ratio of Q and A length\"\"\"\n    q = tokenizer.tokenize(question)\n    #a = tokenizer.tokenize(answer)\n    \n    q_len = len(q)\n    #a_len = len(a)\n    \n    if (q_len+3) > max_sequence_length:\n        \n        new_q_len = max_sequence_length-3\n        #new_a_len = a_len/(q_len+a_len) * (max_sequence_length-3)\n        #new_q_len, new_a_len = int(ceil(new_q_len)), int(floor(new_a_len))\n            \n        if new_q_len+3 != max_sequence_length:\n            raise ValueError(\"too small %s\" % str(new_q_len+3))\n        \n        q = q[:new_q_len]\n        #a = a[:new_a_len]\n    \n    return q\n\ndef _convert_to_bert_inputs(question, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + question + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays_q(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q = instance.question_body\n\n        q= _trim_input(q, tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(\n            q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_input_arays_a(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q = instance.answer\n\n        q= _trim_input(q,  tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(\n            q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\ndef compute_input_arays_t(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        q = instance.question_title\n\n        q= _trim_input(q,  tokenizer, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(\n            q, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays1(df_train, output_categories)\ntest_inputs = compute_input_arays1(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputstq = compute_input_arays2(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ninputsq = compute_input_arays_q(df_train, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_q)\ntest_inputst = compute_input_arays_t(df_test, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_t)\ntest_inputsq = compute_input_arays_q(df_test, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_q)\ntest_inputsa = compute_input_arays_a(df_test, input_categories, tokenizer1, MAX_SEQUENCE_LENGTH_a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nkf = MultilabelStratifiedKFold(n_splits=10, random_state=42, shuffle=True)\nfor i, (tr, val) in enumerate(kf.split(inputsq[1], outputs)):\n    if i!=7:\n        continue\n    \n    K.clear_session()\n    print('starting fold: {0}'.format(i))\n    json_file = open('../input/save-groupbertq-fold{}/nn_modelq{}.json'.format(i, i), 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    model = model_from_json(loaded_model_json, custom_objects={\"BertLayer1\": BertLayer1})\n    # load weights into new model\n    bst_model_path = '../input/save-groupbertq-fold{}/dl_modelq{}.h5'.format(i, i)\n    print(bst_model_path)\n    model.load_weights(bst_model_path)\n    model.compile(optimizer=tf.keras.optimizers.Nadam(3e-5),loss=['binary_crossentropy'])\n    #model.summary()\n    print(\"Loaded model from disk\")\n    y_testq =  model.predict([test_inputstq[0],test_inputstq[1],test_inputstq[2],test_inputst[0],test_inputst[1],test_inputst[2], \n                                          test_inputsq[0],test_inputsq[1],test_inputsq[2]])\n    print(\"y_testq shape\", y_testq.shape)\n    del model\n    #os.remove('best_model_batch.h5')\n    gc.collect()\n    K.clear_session()\n    print('starting fold: {0}'.format(i))\n\n    json_file = open('../input/save-groupberta-fold{}/nn_modela{}.json'.format(i, i), 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    model = model_from_json(loaded_model_json, custom_objects={\"BertLayer1\": BertLayer1})\n    # load weights into new model\n    bst_model_path = '../input/save-groupberta-fold{}/dl_modela{}.h5'.format(i, i)\n    print(bst_model_path)\n    model.load_weights(bst_model_path)\n    model.compile(optimizer=tf.keras.optimizers.Nadam(3e-5),loss=['binary_crossentropy'])\n    #model.summary()\n    print(\"Loaded model from disk\")\n    \n    y_testa =  model.predict([test_inputs[0],test_inputs[1],test_inputs[2],test_inputst[0],test_inputst[1],test_inputst[2], \n                                          test_inputsq[0],test_inputsq[1],test_inputsq[2],test_inputsa[0],test_inputsa[1],test_inputsa[2]])\n    print(\"y_testa shape\", y_testa.shape)\n\n    print(\"Saved model to disk\")\n    del model\n    #os.remove('best_model_batch.h5')\n    gc.collect()\n    \n    y_test = np.concatenate((y_testq, y_testa), axis=1)\n    print(\"y_test shape\", y_test.shape)\n    all_predictions.append(y_test)\n    gc.collect()\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(all_predictions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.mean(all_predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output =0.23*output_roberta + 0.31*output_xlnet +  0.24*test_preds + 0.22*final_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"culture_category_list = list(df_test[(df_test['host'] == 'english.stackexchange.com') | (df_test['host'] == 'ell.stackexchange.com')].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_raters_dict = {'question_asker_intent_understanding': 18,\n 'question_body_critical': 18,\n 'question_conversational': 6,\n 'question_expect_short_answer': 6,\n 'question_fact_seeking': 6,\n 'question_has_commonly_accepted_answer': 6,\n 'question_interestingness_others': 18,\n 'question_interestingness_self': 18,\n 'question_multi_intent': 6,\n 'question_not_really_a_question': 6,\n 'question_opinion_seeking': 6,\n 'question_type_choice': 6,\n 'question_type_compare': 6,\n 'question_type_consequence': 6,\n 'question_type_definition': 6,\n 'question_type_entity': 6,\n 'question_type_instructions': 6,\n 'question_type_procedure': 6,\n 'question_type_reason_explanation': 6,\n 'question_type_spelling': 3,\n 'question_well_written': 18,\n 'answer_helpful': 18,\n 'answer_level_of_information': 18,\n 'answer_plausible': 18,\n 'answer_relevance': 18,\n 'answer_satisfaction': 30,\n 'answer_type_instructions': 6,\n 'answer_type_procedure': 6,\n 'answer_type_reason_explanation': 6,\n 'answer_well_written': 18\n }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submssion_trick(output):\n    #min_rater_value = list(num_raters_dict.values())\n    predictions = np.zeros_like(output)\n    for i in range(output.shape[0]):\n        for j in range(output.shape[1]):\n            col = output_categories[j]\n            num_raters = num_raters_dict[col]\n            if j == 19:\n                if i not in culture_category_list:\n                    predictions[i][j] = 0.0\n                else:\n                    predictions[i][j] = np.floor(output[i][j] * num_raters) / num_raters\n            else:\n                predictions[i][j] = np.floor(output[i][j] * num_raters) / num_raters\n            if num_raters == 18:\n                predictions[i][j] = max(predictions[i][j], 1/3)\n            if num_raters == 30:\n                predictions[i][j] = max(predictions[i][j], 1/5)\n            \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfor i in range(30):\n    output[:, i] = scaler.fit_transform(output[:, i].reshape(-1, 1)).reshape(-1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = submssion_trick(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(PATH +'sample_submission.csv')\n## Submission Checker\nfor i in range(30): \n    if len(np.unique(predictions[:, i])) == 1:\n        print(i)\n        print(submission.columns[i+1])\n        #predictions[:, i] = output[:, i]\n        print('before', np.unique(predictions[:, i]))\n        print('after', np.unique(predictions[:, i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.loc[:, 'question_asker_intent_understanding':] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}