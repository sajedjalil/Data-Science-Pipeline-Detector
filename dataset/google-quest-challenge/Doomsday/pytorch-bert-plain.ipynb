{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This is mostly a copy of Aditya Soni's kernel with some small changes . This is nothing complicated . I just wanted to learn BERT and played around with the basics . The training was done in colab . If you want to use the Kernel for training , just change the last parameter to True in Pipeline Config . This was taken from Combat-Wombat team in Toxicity . "},{"metadata":{},"cell_type":"markdown","source":"##### V6 : After first 5 dates with BERT , I am being a bit forward and asking her out . Created CustomBert , because why not?\n##### V5: implement parameter for head-tail, RandomSampler"},{"metadata":{"id":"QfXpLTtBf-Xx","colab_type":"code","outputId":"bde9c211-e4a6-4327-b37f-06c7689320db","colab":{"base_uri":"https://localhost:8080/","height":179},"trusted":true},"cell_type":"code","source":"## Because Kaggle has stopped internet to fight with bad bad guys . I am installing them from local downloads .\nimport os\n#os.system('pip install ../input/sacremoses/sacremoses-master/')\n#os.system('pip install ../input/transformers-2-3-0/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/\n!pip install ../input/transformers-2-3-0/","execution_count":null,"outputs":[]},{"metadata":{"id":"g9pbW0-HfJjs","colab_type":"code","outputId":"8dde87f1-63b4-4206-8e0e-8d32f123c1b5","colab":{"base_uri":"https://localhost:8080/","height":82},"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"## Loading Libraries  -No they are not for books\nimport transformers, sys, os, gc\nimport numpy as np, pandas as pd, math\nimport torch, random, os, multiprocessing, glob\nimport torch.nn.functional as F\nimport torch, time\n\nfrom ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\nfrom scipy.stats import spearmanr\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, Dataset,RandomSampler, SequentialSampler\nfrom transformers import (\n    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n    get_cosine_schedule_with_warmup,\n)\nfrom transformers.modeling_bert import BertPreTrainedModel \nfrom tqdm import tqdm\nprint(transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"id":"xsw-bSSRZ9Tu","colab_type":"text"},"cell_type":"markdown","source":"# Data Understanding"},{"metadata":{"id":"nmpnZIpBZ9Tv","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"## Common Variables for Notebook \nROOT = '../input/google-quest-challenge/' ## This is the root of all evil.","execution_count":null,"outputs":[]},{"metadata":{"id":"pU862P9kB23j","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"## Make results reproducible .Else noone will believe you .\nimport random\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"id":"350euGK3vxmJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"## This is for fast experiements , but we dont have enough GPU anyway, so thats a bummer!\nclass PipeLineConfig:\n    def __init__(self, lr, warmup,accum_steps, epochs, seed, expname,head_tail,freeze,question_weight,answer_weight,fold,train):\n        self.lr = lr\n        self.warmup = warmup\n        self.accum_steps = accum_steps\n        self.epochs = epochs\n        self.seed = seed\n        self.expname = expname\n        self.head_tail = head_tail\n        self.freeze = freeze\n        self.question_weight = question_weight\n        self.answer_weight =answer_weight\n        self.fold = fold\n        self.train = train\n","execution_count":null,"outputs":[]},{"metadata":{"id":"KzL9i2Y6xB-T","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"config_1 = PipeLineConfig(3e-5,0.05,4,0,42,'uncased_1',True,False,0.7,0.3,8,False) ## These are experiments . You can do as much as you want as long as inference is faster \nconfig_2 = PipeLineConfig(4e-5,0.03,4,6,2019,'uncased_2',True,False,0.8,0.2,5,False)## Adding various different seeds , folds and learning rate and mixing them and then doing inference .\nconfig_3 = PipeLineConfig(4e-5,0.03,4,4,2019,'small_test_3',True ,False, 0.8,0.2,3,True) ## For Small tests in Kaggle , less number of fold , less number of epochs.\nconfig_4 = PipeLineConfig(4e-5,0.05,1,4,2019,'small_test_4',True ,False, 0.8,0.2,3,True)\n## I am doing first experiement\nconfig = config_1\n## Note : If you want to train and just not copy this and submit then change the last parameter above to \"True\" it will kick off the training process.","execution_count":null,"outputs":[]},{"metadata":{"id":"J9O6xkhIB46e","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"seed_everything(config.seed)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"XDsALCEWZ9T5","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"## load the data \ntrain = pd.read_csv(ROOT+'train.csv')\ntest = pd.read_csv(ROOT+'test.csv')\nsub = pd.read_csv(ROOT+'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"TDDMcViSZ9UQ","colab_type":"code","outputId":"736910e0-934b-4110-a884-3c961b56b75d","colab":{"base_uri":"https://localhost:8080/","height":35},"trusted":true},"cell_type":"code","source":"## Get the shape of the data\ntrain_len, test_len ,sub_len = len(train.index), len(test.index),len(sub.index)\nprint(f'train size: {train_len}, test size: {test_len} , sample size: {sub_len}')","execution_count":null,"outputs":[]},{"metadata":{"id":"aSk40U1XZ9Uc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"## These are those target data , many of which has created so much controversy . Mr Spearman has also commented NaN for many of them .\ntarget_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice',\n               'question_type_compare', 'question_type_consequence',\n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful',\n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"wcLpotB4gN8_","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length=290, t_max_len=30, q_max_len=128, a_max_len=128):\n    \n    #350+128+30 = 508 + 4 = 512\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        q_len_head = round(q_new_len/2)\n        q_len_tail = -1* (q_new_len -q_len_head)\n        a_len_head = round(a_new_len/2)\n        a_len_tail = -1* (a_new_len -a_len_head)        ## Head+Tail method .\n        t = t[:t_new_len]\n        if config.head_tail :\n            q = q[:q_len_head]+q[q_len_tail:]\n            a = a[:a_len_head]+a[a_len_tail:]\n        else:\n            q = q[:q_new_len]\n            a = a[:a_new_len] ## No Head+Tail ,usual processing\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n   # stoken = [\"[CLS]\"] + title  + question  + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef _get_stoken_output(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n    return stoken\n\ndef compute_input_tokens(df, columns, tokenizer, max_sequence_length):\n    \n    input_tokens, input_masks, input_segments = [], [], []\n    for _, instance in df[columns].iterrows():\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n        tokens= _get_stoken_output(t, q, a, tokenizer, max_sequence_length)\n        input_tokens.append(tokens)\n    return input_tokens\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length,t_max_len=30, q_max_len=128, a_max_len=128):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in df[columns].iterrows():\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length,t_max_len, q_max_len, a_max_len)\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"0SGqTLB1hVxJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, lengths, labels = None):\n        \n        self.inputs = inputs\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        self.lengths = lengths\n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        lengths         = self.lengths[idx]\n        if self.labels is not None: # targets\n            labels = self.labels[idx]\n            return input_ids, input_masks, input_segments, labels, lengths\n        return input_ids, input_masks, input_segments, lengths\n\n    def __len__(self):\n        return len(self.inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Stolen from transformer code base without any noble intention.\nclass CustomBert(BertPreTrainedModel):\n\n    def __init__(self, config):\n        super(CustomBert, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.bn = nn.BatchNorm1d(1024)\n        self.linear  = nn.Linear(config.hidden_size,1024)\n        self.classifier = nn.Linear(1024, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        lin_output = F.relu(self.bn(self.linear(pooled_output))) ## Note : This Linear layer is added without expert supervision . This will worsen the results . \n                                               ## But you are smarter than me , so you will figure out,how to customize better.\n        lin_output = self.dropout(lin_output)    \n        logits = self.classifier(lin_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"umB-V7j_hZey","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def train_model(train_loader, optimizer, criterion, scheduler,config):\n    \n    model.train()\n    avg_loss = 0.\n    avg_loss_1 = 0.\n    avg_loss_2 =0.\n    avg_loss_3 =0.\n    avg_loss_4 =0.\n    avg_loss_5 =0.\n   # tk0 = tqdm(enumerate(train_loader),total =len(train_loader))\n    optimizer.zero_grad()\n    for idx, batch in enumerate(train_loader):\n        \n        input_ids, input_masks, input_segments, labels, _ = batch\n        input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)            \n        \n        output_train = model(input_ids = input_ids.long(),\n                             labels = None,\n                             attention_mask = input_masks,\n                             token_type_ids = input_segments,\n                            )\n        logits = output_train[0] #output preds\n        loss1 = criterion(logits[:,0:9], labels[:,0:9])\n        loss2 = criterion(logits[:,9:10], labels[:,9:10])\n        loss3 = criterion(logits[:,10:21], labels[:,10:21])\n        loss4 = criterion(logits[:,21:26], labels[:,21:26])\n        loss5 = criterion(logits[:,26:30], labels[:,26:30])\n        loss = config.question_weight*loss1+config.answer_weight*loss2+config.question_weight*loss3+config.answer_weight*loss4+config.question_weight*loss5\n        #loss =(config.question_weight*criterion(logits[:,0:21], labels[:,0:21]) + config.answer_weight*criterion(logits[:,21:30], labels[:,21:30]))/config.accum_steps\n        loss.backward()\n        if (i + 1) % config.accum_steps == 0:    \n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        avg_loss += loss.item() / (len(train_loader)*config.accum_steps)\n        avg_loss_1 += loss1.item() / (len(train_loader)*config.accum_steps)\n        avg_loss_2 += loss2.item() / (len(train_loader)*config.accum_steps)\n        avg_loss_3 += loss3.item() / (len(train_loader)*config.accum_steps)\n        avg_loss_4 += loss4.item() / (len(train_loader)*config.accum_steps)\n        avg_loss_5 += loss5.item() / (len(train_loader)*config.accum_steps)\n        del input_ids, input_masks, input_segments, labels\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return avg_loss ,avg_loss_1,avg_loss_2,avg_loss_3,avg_loss_4,avg_loss_5\n\ndef val_model(val_loader, val_shape, batch_size=8):\n\n    avg_val_loss = 0.\n    model.eval() # eval mode\n    \n    valid_preds = np.zeros((val_shape, len(target_cols)))\n    original = np.zeros((val_shape, len(target_cols)))\n    \n    #tk0 = tqdm(enumerate(val_loader))\n    with torch.no_grad():\n        \n        for idx, batch in enumerate(val_loader):\n            input_ids, input_masks, input_segments, labels, _ = batch\n            input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)            \n            \n            output_val = model(input_ids = input_ids.long(),\n                             labels = None,\n                             attention_mask = input_masks,\n                             token_type_ids = input_segments,\n                            )\n            logits = output_val[0] #output preds\n            \n            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n        \n        score = 0\n        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n        \n        # np.save(\"preds.npy\", preds)\n        # np.save(\"actuals.npy\", original)\n        \n        rho_val = np.mean([spearmanr(original[:, i], preds[:,i]).correlation for i in range(preds.shape[1])])\n        print('\\r val_spearman-rho: %s' % (str(round(rho_val, 5))), end = 100*' '+'\\n')\n        \n        for i in range(len(target_cols)):\n            print(i, spearmanr(original[:,i], preds[:,i]))\n            score += np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation)\n    return avg_val_loss, score/len(target_cols)","execution_count":null,"outputs":[]},{"metadata":{"id":"IqOF054Chimg","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def predict_result(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), len(target_cols)))\n    \n    model.eval();\n    tk0 = tqdm(enumerate(test_loader))\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output","execution_count":null,"outputs":[]},{"metadata":{"id":"vVxMiqt3hmzH","colab_type":"code","outputId":"8dd4839a-2ea4-4056-bb93-9f91c97389a7","colab":{"base_uri":"https://localhost:8080/","height":35},"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\", do_lower_case=True)\ninput_categories = list(train.columns[[1,2,5]]); input_categories\n\nbert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\nbert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = len(target_cols)\n\n\nbert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\noutput_model_file = 'bert_pytorch.bin'\n\n\ntest_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512,t_max_len=30, q_max_len=239, a_max_len=239)\nlengths_test = np.argmax(test_inputs[0] == 0, axis=1)\nlengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n\nprint(do_lower_case, bert_model, device, output_model_file)\nprint(test_inputs)\n\ntest_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\ntest_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\nresult = np.zeros((len(test), len(target_cols)))\n\ntest_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=290,t_max_len=30, q_max_len=128, a_max_len=128)\nlengths_test = np.argmax(test_inputs[0] == 0, axis=1)\nlengths_test[lengths_test == 0] = test_inputs[0].shape[1]\n\nprint(do_lower_case, bert_model, device, output_model_file)\nprint(test_inputs)\n\ntest_set1 = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\ntest_loader1  = DataLoader(test_set, batch_size=32, shuffle=False)\nresult1 = np.zeros((len(test), len(target_cols)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_inputs_1 = compute_input_tokens(test, input_categories, tokenizer, max_sequence_length=290)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test['tokenized'] = np.array(test_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(test_inputs_1)","execution_count":null,"outputs":[]},{"metadata":{"id":"7y0fT56MiZ1f","colab_type":"code","outputId":"5ac5450e-4975-49a5-fb84-17dcdade92fc","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"cell_type":"code","source":"class bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n\n\n# In[8]:\n\nNUM_FOLDS = config.fold  # change this\nSEED = config.seed\nBATCH_SIZE = 8\nepochs = config.epochs   # change this\nACCUM_STEPS = 1\n\nkf = MultilabelStratifiedKFold(n_splits = NUM_FOLDS, random_state = SEED)\n\n#test_set = QuestDataset(inputs=test_inputs, lengths=lengths_test, labels=None)\n#test_loader  = DataLoader(test_set, batch_size=32, shuffle=False)\n#result = np.zeros((len(test), len(target_cols)))\n\ny_train = train[target_cols].values # dummy\n\nprint(bcolors.FAIL, f\"For Every Fold, Train {epochs} Epochs\", bcolors.ENDC)\nif config.train :\n    for fold, (train_index, val_index) in enumerate(kf.split(train.values, y_train)):\n        if fold > 0 : ## Saving GPU\n            break \n        print(bcolors.HEADER, \"Current Fold:\", fold, bcolors.ENDC)\n\n        train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n        print(\"Train and Valid Shapes are\", train_df.shape, val_df.shape)\n    \n        print(bcolors.HEADER, \"Preparing train datasets....\", bcolors.ENDC)\n    \n        inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=290)\n        outputs_train = compute_output_arrays(train_df, columns = target_cols)\n        outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n        lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n        lengths_train[lengths_train == 0] = inputs_train[0].shape[1]\n    \n        print(bcolors.HEADER, \"Preparing Valid datasets....\", bcolors.ENDC)\n    \n        inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=290)\n        outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n        outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n        lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n        lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n    \n        print(bcolors.HEADER, \"Preparing Dataloaders Datasets....\", bcolors.ENDC)\n\n        train_set    = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n        train_sampler = RandomSampler(train_set)\n        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,sampler=train_sampler)\n    \n        valid_set    = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n        valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n    \n        model = CustomBert.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config);\n        model.zero_grad();\n        model.to(device);\n        torch.cuda.empty_cache()\n        if config.freeze : ## This is basically using out of the box bert model while training only the classifier head with our data . \n            for param in model.bert.parameters():\n                param.requires_grad = False\n        model.train();\n    \n        i = 0\n        best_avg_loss   = 100.0\n        best_score      = -1.\n        best_param_loss = None\n        best_param_score = None\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.8},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n            ]        \n\n        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=config.lr, eps=4e-5)\n       # optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, eps=4e-5)\n        criterion = nn.BCEWithLogitsLoss()\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup, num_training_steps= epochs*len(train_loader)//ACCUM_STEPS)\n        print(\"Training....\")\n    \n        for epoch in range(config.epochs):\n\n            torch.cuda.empty_cache()\n        \n            start_time   = time.time()\n            avg_loss,avg_loss_1,avg_loss_2 ,avg_loss_3,avg_loss_4,avg_loss_5   = train_model(train_loader, optimizer, criterion, scheduler,config)\n            avg_val_loss, score = val_model(valid_loader, val_shape=val_df.shape[0])\n            elapsed_time = time.time() - start_time\n\n            print(bcolors.OKGREEN, 'Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t train_loss={:.4f} \\t train_loss_1={:.4f} \\t train_loss_2={:.4f} \\t train_loss_3={:.4f} \\t train_loss_4={:.4f}  \\t train_loss_5={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n                epoch + 1, epochs, avg_loss, avg_val_loss,avg_loss,avg_loss_1,avg_loss_2,avg_loss_3,avg_loss_4,avg_loss_5, score, elapsed_time),\n            bcolors.ENDC\n            )\n\n            if best_avg_loss > avg_val_loss:\n                i = 0\n                best_avg_loss = avg_val_loss \n                best_param_loss = model.state_dict()\n\n            if best_score < score:\n                best_score = score\n                best_param_score = model.state_dict()\n                print('best_param_score_{}_{}.pt'.format(config.expname ,fold+1))\n                torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname ,fold+1))\n            else:\n                i += 1\n\n            \n        model.load_state_dict(best_param_score)\n        result += predict_result(model, test_loader)\n        print('best_param_score_{}_{}.pt'.format(config.expname ,fold+1))\n        torch.save(best_param_score, 'best_param_score_{}_{}.pt'.format(config.expname ,fold+1))\n        \n        result /= NUM_FOLDS\n        \n    del train_df, val_df, model, optimizer, criterion, scheduler\n    torch.cuda.empty_cache()\n    del valid_loader, train_loader, valid_set, train_set\n    torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    \nprint(result)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Loading the colab pretrained model , same setting as experiement 1 for uncased . \n## I had to break and load as dataset . Thats why this weird way of loading weights . \nresult = np.zeros((len(test), len(target_cols)))\n#good_folds = [2,3,4,6,8]## Taking only the folds for which the val_spearman was more than 0.381 \nfor i in range(1,9):\n    model = BertForSequenceClassification.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config);\n    model.zero_grad();\n    model.to(device);\n    if i < 5 :\n        model.load_state_dict(torch.load(f'../input/pretrainedbert/drive-download-20200105T035050Z-002/best_param_score_uncased_1_{i}.pt'))\n    else:\n        model.load_state_dict(torch.load(f'../input/pretrainedbert/drive-download-20200105T035050Z-001/best_param_score_uncased_1_{i}.pt'))\n        \n    result += predict_result(model, test_loader)\n    \nfor i in range(1,5):\n    model = BertForSequenceClassification.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config);\n    model.zero_grad();\n    model.to(device);\n    model.load_state_dict(torch.load(f'../input/seed2019/best_param_score_uncased_4_{i}.pt'))\n\n    result += predict_result(model, test_loader)   \n\n    \nfor i in range(2,4):\n    model = BertForSequenceClassification.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config);\n    model.zero_grad();\n    model.to(device);\n    model.load_state_dict(torch.load(f'../input/bert290/best_param_score_uncased_1_{i}_290.pt'))\n\n    result += predict_result(model, test_loader1)       \n    \nresult/=14 ##  12 folds used out of 12 folds   ","execution_count":null,"outputs":[]},{"metadata":{"id":"vigeVO_lXtdH","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(r'../input/google-quest-challenge/sample_submission.csv')\nsubmission.loc[:, 'question_asker_intent_understanding':] = result\n#submission.loc[~submission['qa_id'].isin(qa_id_list),'question_type_spelling']=0.0\n#submission.loc[submission['qa_id'].isin(qa_id_list),'question_type_spelling'] = 1.0\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(30):\n    print(submission[target_cols[i]].value_counts())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"CasedTest Bert of Quest -pytorch BERT finetuning vanilla","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}