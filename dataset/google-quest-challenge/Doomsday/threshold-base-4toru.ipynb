{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport sys\n\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\n\nfrom os.path import join as path_join\nfrom scipy.stats import spearmanr, rankdata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('pip install ../input/sacremoses/sacremoses-master/ > /dev/null')\nos.system('pip install ../input/transformers/transformers-master/ > /dev/null')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"package_path = '../input/radam-pytorch/RAdam' \nsys.path.append(package_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# quest-003-55-bert-07-3-valid"},{"metadata":{"trusted":true},"cell_type":"code","source":"folder = '../input/google-quest-challenge/'\npretrained_bert = \"../input/pretrainedbertpytorch/pretrained-bert-pytorch/bert-base-uncased/\"\nmodel_weight_path1 = '../input/quest-003-55-bert-07-3-training-1/'\nmodel_weight_path2 = '../input/quest-003-55-bert-07-3-training-2/'\nmodel_weight_path3 = '../input/quest-003-55-bert-07-3-training-3/'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\n\nimport pickle  \nimport numpy as np\nimport pandas as pd\nimport random\nimport copy\nimport string\nimport time\n\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelBinarizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom urllib.parse import urlparse\nimport math\nfrom tqdm import tqdm\n\nfrom spacy.lang.en import English\nfrom scipy.stats import spearmanr\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom torch.nn.utils.weight_norm import weight_norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nprint(\"transformers:\", transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 12345","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef read_pickle_from_file(pickle_file):\n    with open(pickle_file,'rb') as f:\n        x = pickle.load(f)\n    return x\n\ndef write_pickle_to_file(pickle_file, x):\n    with open(pickle_file, 'wb') as f:\n        pickle.dump(x, f, pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv( f\"{folder}train.csv\" )\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv( f\"{folder}test.csv\" )\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract target variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_table = [\n    ['question_asker_intent_understanding',   'question'],\n    ['question_body_critical',                'question'],\n    ['question_conversational',               'question'],\n    ['question_expect_short_answer',          'question'],\n    ['question_fact_seeking',                 'question'],\n    ['question_has_commonly_accepted_answer', 'question'],\n    ['question_interestingness_others',       'question'],\n    ['question_interestingness_self',         'question'],\n    ['question_multi_intent',                 'question'],\n    ['question_not_really_a_question',        'question'],\n    ['question_opinion_seeking',              'question'],\n    ['question_type_choice',                  'question'],\n    ['question_type_compare',                 'question'],\n    ['question_type_consequence',             'question'],\n    ['question_type_definition',              'question'],\n    ['question_type_entity',                  'question'],\n    ['question_type_instructions',            'question'],\n    ['question_type_procedure',               'question'],\n    ['question_type_reason_explanation',      'question'],\n    ['question_type_spelling',                'question'],\n    ['question_well_written',                 'question'],\n    ['answer_helpful',                        'answer'],\n    ['answer_level_of_information',           'answer'],\n    ['answer_plausible',                      'answer'],\n    ['answer_relevance',                      'answer'],\n    ['answer_satisfaction',                   'answer'],\n    ['answer_type_instructions',              'answer'],\n    ['answer_type_procedure',                 'answer'],\n    ['answer_type_reason_explanation',        'answer'],\n    ['answer_well_written',                   'answer'],    \n]\n\ninput_columns = [\n    'question_title', \n    'question_body',    \n    'answer'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_question_columns = []\ntarget_answer_columns = []\n\nfor table in target_table:\n    if table[1] == 'question':\n        target_question_columns.append( table[0] )\n    elif table[1] == 'answer':\n        target_answer_columns.append( table[0] )   \n        \ntarget_columns = target_question_columns + target_answer_columns\n\nprint( 'question:', len(target_question_columns) )\nprint( 'answer:', len(target_answer_columns) )\nprint( 'total:', len(target_columns) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train_df, test_df]:\n\n    ## domain components\n    df['domcom'] = df['url'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n\n    # count components\n    df['dom_cnt'] = df['domcom'].apply(lambda s: len(s))\n\n    # extend length\n    df['domcom'] = df['domcom'].apply(lambda s: s + ['none', 'none'])\n\n    # components\n    for ii in range(0,4):\n        df['url_'+str(ii)] = df['domcom'].apply(lambda s: s[ii])\n\n    # clean up\n    df.drop('domcom', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature = pd.DataFrame()\ntest_feature = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Text based features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df, feature in [[train_df, train_feature], [test_df, test_feature]]:\n    for column in input_columns:\n        feature[column+'_total_length'] = df[column].apply(len)\n        feature[column+'_capitals'] = df[column].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n        feature[column+'_caps_vs_length'] = feature.apply(lambda row: float(row[column+'_capitals'])/float(row[column+'_total_length']),axis=1)\n        feature[column+'_num_exclamation_marks'] = df[column].apply(lambda comment: comment.count('!'))\n        feature[column+'_num_question_marks'] = df[column].apply(lambda comment: comment.count('?'))\n        feature[column+'_num_punctuation'] = df[column].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n        feature[column+'_num_symbols'] = df[column].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n        feature[column+\"_num_chars\"] = df[column].apply(lambda x: len(str(x)))\n        feature[column+'_num_words'] = df[column].apply(lambda comment: len(comment.split()))\n        feature[column+'_num_unique_words'] = df[column].apply(lambda comment: len(set(w for w in comment.split())))\n        feature[column+'_words_vs_unique'] = feature[column+'_num_unique_words'] / feature[column+'_num_words']\n        feature[column+'_num_smilies'] = df[column].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n        \n        ## Number of stopwords in the text ##\n        feature[column+\"_num_stopwords\"] = df[column].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n        \n        ## Number of punctuations in the text ##\n        feature[column+\"_num_punctuations\"] =df[column].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n        \n        ## Number of title case words in the text ##\n        feature[column+\"_num_words_upper\"] = df[column].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = English()\nsentencizer = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sentencizer)\n\nans_user_and_category=train_df[train_df[['answer_user_name', 'category']].duplicated()][['answer_user_name', 'category']].values\nans_user_and_category.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_question_metadata_features(text):\n    doc=nlp(text)\n    indirect = 0\n    choice_words=0\n    reason_explanation_words = 0\n    question_count = 0\n    \n    for sent in doc.sents:\n        if '?' in sent.text and '?' == sent.text[-1]:\n            question_count += 1\n            for token in sent:\n                if token.text.lower()=='why':\n                    reason_explanation_words+=1\n                elif token.text.lower()=='or':\n                    choice_words+=1\n    if question_count==0:\n        indirect+=1\n    \n    return np.array([indirect, question_count, reason_explanation_words, choice_words])\n\n\ndef question_answer_author_same(df):\n    q_username = df['question_user_name']\n    a_username = df['answer_user_name']\n    author_same=[]\n    \n    for i in range(len(df)):\n        if q_username[i] == a_username[i]:\n            author_same.append(int(1))\n        else:\n            author_same.append(int(0))\n    return author_same\n\n\ndef add_external_features( df, feature ):\n    feature['question_vs_answer_length'] = feature['question_body_num_words']/feature['answer_num_words']\n    feature['q_a_author_same'] = question_answer_author_same(df)\n    \n    answer_user_cat = []\n    for i in df[['answer_user_name', 'category']].values:\n        if i in ans_user_and_category:\n            answer_user_cat.append(int(1))\n        else:\n            answer_user_cat.append(int(0))\n    feature['answer_user_cat'] = answer_user_cat\n    \n    handmade_features=[]\n    for text in df['question_body'].values:\n        handmade_features.append(add_question_metadata_features(text))\n\n    feature = pd.concat( [ feature, pd.DataFrame( handmade_features, columns=['indirect', 'question_count', 'reason_explanation_words', 'choice_words'])], axis=1 )\n    \n    return feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature = add_external_features( train_df, train_feature )\ntest_feature  = add_external_features( test_df, test_feature )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in input_columns:\n    print( \"{} | Min: {}, Max: {}\".format( column, train_feature[column+'_total_length'].min(), train_feature[column+'_total_length'].max() ) )\n    \nprint( '=====' )\n\nfor column in input_columns:\n    print( \"{} | Min: {}, Max: {}\".format( column, test_feature[column+'_total_length'].min(), test_feature[column+'_total_length'].max() ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = nltk.corpus.stopwords.words('english')\nsymbol = [\"'\", '\"', ':', ';', '.', ',', '-', '!', '?', \"'s\", \")\", \"(\", \"...\", '``', \"''\", \"/\", \"$\", \"%\", \"*\", \"&\", \"{\", \"}\", \"[\", \"]\"]\n\ndef get_prevalent( texts, top_count=15 ):    \n    tokenized_sents = [nltk.word_tokenize(i) for i in texts]\n    tokenized_sents = [flatten for inner in tokenized_sents for flatten in inner]   \n    \n    #fdist = nltk.FreqDist(w for w in tokenized_sents if w not in stop_words + symbol)\n    fdist = nltk.FreqDist(w.lower() for w in tokenized_sents if w.lower() not in stop_words + symbol)\n    comments = fdist.most_common(top_count)\n    \n    return [word[0] for word in comments]\n\nfor column in input_columns:\n    words = get_prevalent( train_df[column])\n    print( column, words )\n    \n    for word in words:\n        for df, feature in [[train_df, train_feature], [test_df, test_feature]]:\n            feature[column+'_num'+word] = df[column].apply(lambda comment: comment.count(word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### count"},{"metadata":{"trusted":true},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\n\ntrain_df['netloc'] = train_df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest_df['netloc'] = test_df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ncount_columns = ['question_title', 'question_user_name', 'answer_user_name', 'category', 'netloc']\n\nfor col in count_columns:\n    value = train_df[col].value_counts()\n    train_feature[col+'_count'] = train_df[col].map( value )\n    test_feature[col+'_count']  = test_df[col].map( value ).fillna( 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_feature.columns: \n    train_mean = np.nanmean( train_feature[col].values )\n    train_feature[col].fillna( train_mean, inplace=True )\n    test_feature[col].fillna( train_mean, inplace=True ) \n    \nprint( \"train: nan=\", np.sum( np.sum( pd.isnull( train_feature ) ) ) )\nprint( \"test : nan=\", np.sum( np.sum( pd.isnull( test_feature  ) ) ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\nscaler.fit(train_feature)\n\ntrain_feature = pd.DataFrame(scaler.transform(train_feature), columns=train_feature.columns)\ntest_feature = pd.DataFrame(scaler.transform(test_feature), columns=test_feature.columns)\n\ndel scaler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Label Preprocessing\n- https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/100961"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_label( df, result_df, column, count=10 ):\n    labels = [ ( df[column].values>=(rate/count) ).astype(np.int) for rate in range( 1, count+1 ) ]\n\n    columns = ['{}_{}'.format(column, i) for i in range(count)]\n    labels = np.array( labels ).T\n    \n    label_df = pd.DataFrame( labels, columns=columns )\n    result_df = pd.concat((result_df, label_df), axis=1)\n    \n    return result_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_convert_count = 12\ntrain_feature2 = pd.DataFrame()\ntest_feature2 = pd.DataFrame()\n\nfor column in train_feature.columns:\n    train_feature2 = convert_label( train_feature, train_feature2, column, label_convert_count )\n    test_feature2 = convert_label( test_feature, test_feature2, column, label_convert_count )   \n    \ntrain_feature = train_feature2.copy()\ntest_feature = test_feature2.copy()\n\ndel train_feature2, test_feature2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Encode"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['question_title', 'question_user_name', 'answer_user_name']\nlimits = [6, 8, 8]\n\nfor col, limit in zip( features, limits ):\n    value = train_df[col].value_counts()\n    train_df['item_count'] = train_df[col].map( value )    \n    train_df['item_value'] = train_df[col].copy()\n    train_df.loc[train_df.item_count < limit, 'item_value'] = \"___###___\"\n    \n    test_df['item_count'] = test_df[col].map( value ).fillna( 1 )    \n    test_df['item_value'] = test_df[col].copy()\n    test_df.loc[test_df.item_count < limit, 'item_value'] = \"___###___\"    \n    \n    lb = LabelBinarizer()\n    lb.fit( train_df['item_value'] )\n    \n    encode_train = lb.transform(train_df['item_value'])\n    encode_test = lb.transform(test_df['item_value'])\n    \n    columns = ['LabelBinarizer_{}'.format(i) for i in range(encode_train.shape[1])]\n    print( \"{}: {}\". format( col, len( train_df['item_value'].value_counts() ) ) )\n    \n    encode_train = pd.DataFrame( encode_train, columns=columns )\n    train_feature = pd.concat((train_feature, encode_train), axis=1)\n\n    encode_test = pd.DataFrame( encode_test, columns=columns )\n    test_feature = pd.concat((test_feature, encode_test), axis=1)      \n    \n    del lb\n    \ntrain_df.drop( ['item_count', 'item_count'], axis=1, inplace=True )\ntest_df.drop( ['item_count', 'item_count'], axis=1, inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['url_0', 'category']\nenc = OneHotEncoder( handle_unknown='ignore' )\nenc.fit( train_df[features] )\n\nencode_train = enc.transform(train_df[features]).toarray()\nencode_test = enc.transform(test_df[features]).toarray()\n\ncolumns = ['encode_{}'.format(i) for i in range(encode_train.shape[1])]\n\nencode_train = pd.DataFrame( encode_train, columns=columns )\ntrain_feature = pd.concat((train_feature, encode_train), axis=1)\n\nencode_test = pd.DataFrame( encode_test, columns=columns )\ntest_feature = pd.concat((test_feature, encode_test), axis=1)   \n\ndel encode_train, encode_test, enc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### remove nan"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_feature.columns: \n    train_mean = np.nanmean( train_feature[col].values )\n    train_feature[col].fillna( train_mean, inplace=True )\n    test_feature[col].fillna( train_mean, inplace=True ) \n    \nprint( \"train: nan=\", np.sum( np.sum( pd.isnull( train_feature ) ) ) )\nprint( \"test : nan=\", np.sum( np.sum( pd.isnull( test_feature  ) ) ) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Text cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/chenshengabc/from-quest-encoding-ensemble-a-little-bit-differen\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n#         df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n        df[col] = df[col].apply(lambda x: handle_contractions(x))  \n        df[col] = df[col].apply(lambda x: fix_quote(x))   \n    \n    return df\n\n\ntrain_df = clean_data( train_df, input_columns )\ntest_df = clean_data( test_df, input_columns )\n\ndel tokenizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bert"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass NeuralNet(nn.Module):\n    def __init__(self, num_features, num_labels, pretrained_bert ):\n        super(NeuralNet, self).__init__()\n        \n        self.bert = transformers.BertModel.from_pretrained( pretrained_bert )\n        self.num_features = num_features\n        self.num_labels = num_labels\n        \n        self.encoded_dropout = SpatialDropout( 0.2 )\n        self.pooled_dropout = nn.Dropout( 0.2 )        \n        \n        self.feature_linear = nn.Sequential(\n            nn.Linear( self.num_features, self.num_features ),\n            nn.ReLU( inplace=True ),\n            nn.Dropout( 0.2 ),            \n        )\n        \n        dense_hidden_units = self.bert.config.hidden_size * 3 + self.num_features\n        \n        self.linear1 = nn.Linear( dense_hidden_units, dense_hidden_units )\n        self.linear2 = nn.Linear( dense_hidden_units, dense_hidden_units )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear( dense_hidden_units, num_labels ),\n        )\n        \n    def forward( self, ids, masks, segments, feature ):  \n        \n        feature_output = self.feature_linear( feature )\n        \n        sequence_output, pooled_output = self.bert( input_ids=ids, attention_mask=masks, token_type_ids=segments )\n        sequence_output = self.encoded_dropout(sequence_output)\n        pooled_output = self.pooled_dropout(pooled_output)        \n        \n        avg_pool = torch.mean( sequence_output, 1 )\n        max_pool, _ = torch.max( sequence_output, 1 )\n        \n        h_conc = torch.cat( ( avg_pool, max_pool, pooled_output, feature_output ), 1 )\n        \n        h_conc_linear  = F.relu(self.linear1(h_conc))\n        hidden = h_conc + h_conc_linear \n            \n        h_conc_linear  = F.relu(self.linear2(hidden))\n        hidden = hidden + h_conc_linear      \n        \n        return self.classifier( hidden )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_title_question( df, tokenizer, max_sequence_length=512 ):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    \n    for _, instance in df.iterrows():\n         \n        title    = tokenizer.tokenize(instance.question_title)\n        question = tokenizer.tokenize(instance.question_body)\n\n        if (len(title)+len(question)+3) > max_sequence_length:\n            if len(title) > 30:\n                title = title[:30]\n                \n            question_len = max_sequence_length - len(title) - 3\n            question = question[:question_len]\n        \n        #token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] \n        #token_ids = tokenizer.convert_tokens_to_ids(token)\n        \n        title_ids    = tokenizer.convert_tokens_to_ids(title)\n        question_ids = tokenizer.convert_tokens_to_ids(question)\n        cls_ids      = tokenizer.convert_tokens_to_ids( [\"[CLS]\"] )   \n        sep_ids      = tokenizer.convert_tokens_to_ids( [\"[SEP]\"] ) \n        \n        token_ids = cls_ids + title_ids + sep_ids + question_ids + sep_ids\n        padding = [0] * (max_sequence_length - len(token_ids))\n        \n        ids      = token_ids + padding\n        masks    = [1]*len(token_ids) + padding\n        segments = [0]*(len(title_ids)+2) + [1]*(len(question_ids)+1) + padding\n        \n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),   \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_arrays( df, tokenizer, max_sequence_length=512 ):\n    \n    input_ids, input_masks, input_segments = [], [], []    \n    t_max_len = 35     \n    \n    for _, instance in df.iterrows():\n        \n        title    = tokenizer.tokenize(instance.question_title)\n        question = tokenizer.tokenize(instance.question_body)\n        answer   = tokenizer.tokenize(instance.answer)\n\n        if (len(title)+len(question)+len(answer)+4) > max_sequence_length:\n            if len(title) > t_max_len:\n                title = title[:t_max_len]\n                \n            question_len = ( max_sequence_length - len(title) - 4 ) // 2\n            question = question[:question_len]\n            \n            answer_len = max_sequence_length - len(title) - len(question) - 4\n            answer = answer[:answer_len]\n        \n        #token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n        #token_ids = tokenizer.convert_tokens_to_ids(token)\n        \n        title_ids    = tokenizer.convert_tokens_to_ids(title)\n        question_ids = tokenizer.convert_tokens_to_ids(question)\n        answer_ids   = tokenizer.convert_tokens_to_ids(answer) \n        cls_ids      = tokenizer.convert_tokens_to_ids( [\"[CLS]\"] )   \n        sep_ids      = tokenizer.convert_tokens_to_ids( [\"[SEP]\"] ) \n        \n        token_ids = cls_ids + title_ids + sep_ids + question_ids + sep_ids + answer_ids + sep_ids\n        padding = [0] * (max_sequence_length - len(token_ids))\n        \n        ids      = token_ids + padding\n        masks    = [1]*len(token_ids) + padding\n        segments = [0]*(len(title_ids)+len(question_ids)+3) + [1]*(len(answer_ids)+1) + padding\n        \n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),   \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 6\n\n'''\nx_train = train_feature.values\ny_train = train_df[target_columns].values\n\ncv = KFold( n_splits=n_splits, random_state=SEED )\nkfold_split = list( cv.split( x_train, y_train ) )\n\nwrite_pickle_to_file( 'kfold_split_index.pkl', kfold_split )\n'''\n\nkfold_split = read_pickle_from_file( model_weight_path1 + 'kfold_split_index.pkl' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_datas = {}\ntest_pred_weights = {}\n\nfor column in target_columns:\n    test_pred_datas[column] = np.zeros( len(test_df) )\n    test_pred_weights[column] = 0.0\n    \ndef add_test_pred_data( prediction, columns, weight ):\n    for column_idx, column in enumerate( columns ):\n        test_pred_datas[column] += weight * prediction[:, column_idx]  \n        test_pred_weights[column] += weight    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_datas = {}\nvalidation_counts = {}\n\nfor column in target_columns:\n    validation_datas[column] = np.zeros( len(train_df) )\n    validation_counts[column] = np.zeros( len(train_df) )\n\ndef add_validation_data( prediction, columns, idx ):\n    for column_idx, column in enumerate( columns ):\n        validation_datas[column][idx] += prediction[:, column_idx]  \n        validation_counts[column][idx] += 1.0    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_spearmanr_correlation_score( y_true, y_pred ):\n    num_labels = y_pred.shape[1]\n    score = np.nanmean( [ spearmanr( y_pred[:, idx], y_true[:, idx] ).correlation for idx in range(num_labels) ] )\n    return score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, features, labels = None):\n        \n        self.inputs   = inputs\n        self.features = features\n        \n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        input_features  = self.features[idx]\n        \n        if self.labels is not None: # targets\n            input_labels = self.labels[idx]\n            return input_ids, input_masks, input_segments, input_features, input_labels\n        \n        return input_ids, input_masks, input_segments, input_features\n\n    def __len__(self):\n        return len(self.inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test_validation( label_columns, train_inputs, test_inputs, x_train, x_test, weight_files ):\n    \n    if len(kfold_split) != len(weight_files):\n        return\n    \n    batch_size   = 6    \n    num_features = x_test.shape[1]\n    num_labels   = len(label_columns)\n\n    test_dataset = QuestDataset( test_inputs, x_test, None )\n    test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, shuffle=False )   \n    \n    for k, (train_idx, valid_idx) in enumerate(kfold_split):\n        \n        fname = weight_files[k]\n        print( k+1, fname )\n        \n        x_train_valid = x_train[valid_idx]\n        train_inputs_valid = [x[valid_idx] for x in train_inputs]\n\n        valid_dataset = QuestDataset( train_inputs_valid, x_train_valid, None )\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False )\n        \n        model = NeuralNet( num_features, num_labels, pretrained_bert )\n        model.to('cuda:0')\n\n        model.load_state_dict( torch.load( fname ) )      \n        model.eval()\n     \n        #====================\n        #validation\n        valid_preds_fold  = []\n\n        with torch.no_grad():\n            for ids, masks, segments, features in valid_loader:\n                ids      = ids.cuda()\n                masks    = masks.cuda()\n                segments = segments.cuda()\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                y_pred = model( ids, masks, segments, features )\n                valid_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n            valid_preds_fold = np.array( valid_preds_fold )\n            add_validation_data( valid_preds_fold, label_columns, valid_idx )\n            \n        #====================\n        #test\n        test_preds_fold  = []\n\n        with torch.no_grad():\n            for ids, masks, segments, features in test_loader:\n                ids      = ids.cuda()\n                masks    = masks.cuda()\n                segments = segments.cuda()\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                y_pred = model( ids, masks, segments, features )\n                test_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n            test_preds_fold = np.array( test_preds_fold )\n            add_test_pred_data( test_preds_fold, label_columns, 1.0 )            \n\n        del model, valid_dataset, valid_loader\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    del test_dataset, test_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained( pretrained_bert )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### question"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs = compute_input_title_question( train_df, tokenizer, max_sequence_length=512 )\ntest_inputs = compute_input_title_question( test_df, tokenizer, max_sequence_length=512 )\n\nx_train = train_feature.values\nx_test  = test_feature.values\n\nprint( x_train.shape )\nprint( x_test.shape )\nprint( len( target_question_columns ) )\n\nweight_files = [\n    model_weight_path1 + 'best_weights_003_55_7_3_question_1.pth',\n    model_weight_path1 + 'best_weights_003_55_7_3_question_2.pth',\n    model_weight_path2 + 'best_weights_003_55_7_3_question_3.pth',\n    model_weight_path2 + 'best_weights_003_55_7_3_question_4.pth',\n    model_weight_path3 + 'best_weights_003_55_7_3_question_5.pth',\n    model_weight_path3 + 'best_weights_003_55_7_3_question_6.pth',\n]\n\nmodel_test_validation( target_question_columns, train_inputs, test_inputs, x_train, x_test, weight_files )\n\ndel x_train, x_test\ndel train_inputs, test_inputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### answer"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs = compute_input_arrays( train_df, tokenizer, max_sequence_length=512 )\ntest_inputs = compute_input_arrays( test_df, tokenizer, max_sequence_length=512 )\n\nx_train = train_feature.values\nx_test  = test_feature.values\n\nprint( x_train.shape )\nprint( x_test.shape )\nprint( len( target_answer_columns ) )\n\nweight_files = [\n    model_weight_path1 + 'best_weights_003_55_7_3_answer_1.pth',\n    model_weight_path1 + 'best_weights_003_55_7_3_answer_2.pth',\n    model_weight_path2 + 'best_weights_003_55_7_3_answer_3.pth',\n    model_weight_path2 + 'best_weights_003_55_7_3_answer_4.pth',\n    model_weight_path3 + 'best_weights_003_55_7_3_answer_5.pth',\n    model_weight_path3 + 'best_weights_003_55_7_3_answer_6.pth',\n]\n\nmodel_test_validation( target_answer_columns, train_inputs, test_inputs, x_train, x_test, weight_files )\n\ndel x_train, x_test\ndel train_inputs, test_inputs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_feature, test_feature\ndel target_question_columns, target_answer_columns ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"validationT = pd.DataFrame()\n\nfor column in target_columns:\n    preds = validation_datas[column]\n    count = validation_counts[column]\n    count = np.where( count < 0.5, 1.0, count )\n    \n    validationT[column] = preds / count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_spearmanr_correlation_score( validationT.values, train_df[target_columns].values )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validationT.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predsT = pd.read_csv( f\"{folder}sample_submission.csv\" )\n\nfor column in target_columns:\n    preds = test_pred_datas[column]\n    weight = test_pred_weights[column]\n    \n    test_predsT[column] = preds / weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predsT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del target_columns, input_columns\ndel test_pred_datas, test_pred_weights\ndel validation_datas, validation_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blabla","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 006-55-gpt2-01-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/photostage1/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folder = '../input/google-quest-challenge/'\npretrained_bert = \"../input/pretrainedbertpytorch/pretrained-bert-pytorch/gpt2/\"\nmodel_weight_path1 = '../input/photostage1/best_weights_006-55-gpt2-01_question_1.pth'\nmodel_weight_path2 = '../input/photostage2/best_weights_006-55-gpt2-01_question_2.pth'\nmodel_weight_path3 = '../input/photostage3/best_weights_006-55-gpt2-01_question_3.pth'\nmodel_weight_path4 = '../input/photostage4/best_weights_006-55-gpt2-01_question_4.pth'\nmodel_weight_path5 = '../input/photostage5/best_weights_006-55-gpt2-01_question_5.pth'\nmodel_weight_path6 = '../input/photostage6/best_weights_006-55-gpt2-01_question_6.pth'\nmodel_weight_path7 = '../input/photostagea1/best_weights_006-55-gpt2-01_answer_1.pth'\nmodel_weight_path8 = '../input/photostagea2/best_weights_006-55-gpt2-01_answer_2.pth'\nmodel_weight_path9 = '../input/photostagea3/best_weights_006-55-gpt2-01_answer_3.pth'\nmodel_weight_path10 = '../input/photostagea4/best_weights_006-55-gpt2-01_answer_4.pth'\nmodel_weight_path11 = '../input/photostagea5/best_weights_006-55-gpt2-01_answer_5.pth'\nmodel_weight_path12 = '../input/photostagea6/best_weights_006-55-gpt2-01_answer_6.pth'\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom torch.nn.utils.weight_norm import weight_norm\nfrom transformers import (\n    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n    get_cosine_schedule_with_warmup,    GPT2Config,GPT2Model,\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n)\nfrom transformers.modeling_gpt2 import GPT2PreTrainedModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nprint(\"transformers:\", transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 12345\n\nstart_time_all = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef read_pickle_from_file(pickle_file):\n    with open(pickle_file,'rb') as f:\n        x = pickle.load(f)\n    return x\n\ndef write_pickle_to_file(pickle_file, x):\n    with open(pickle_file, 'wb') as f:\n        pickle.dump(x, f, pickle.HIGHEST_PROTOCOL)\n        \n        \ndef time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n\n    else:\n        raise NotImplementedError","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv( f\"{folder}train.csv\" )\ntrain_df.shape\ntest_df = pd.read_csv( f\"{folder}test.csv\" )\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_table = [\n    ['question_asker_intent_understanding',   'question'],\n    ['question_body_critical',                'question'],\n    ['question_conversational',               'question'],\n    ['question_expect_short_answer',          'question'],\n    ['question_fact_seeking',                 'question'],\n    ['question_has_commonly_accepted_answer', 'question'],\n    ['question_interestingness_others',       'question'],\n    ['question_interestingness_self',         'question'],\n    ['question_multi_intent',                 'question'],\n    ['question_not_really_a_question',        'question'],\n    ['question_opinion_seeking',              'question'],\n    ['question_type_choice',                  'question'],\n    ['question_type_compare',                 'question'],\n    ['question_type_consequence',             'question'],\n    ['question_type_definition',              'question'],\n    ['question_type_entity',                  'question'],\n    ['question_type_instructions',            'question'],\n    ['question_type_procedure',               'question'],\n    ['question_type_reason_explanation',      'question'],\n    ['question_type_spelling',                'question'],\n    ['question_well_written',                 'question'],\n    ['answer_helpful',                        'answer'],\n    ['answer_level_of_information',           'answer'],\n    ['answer_plausible',                      'answer'],\n    ['answer_relevance',                      'answer'],\n    ['answer_satisfaction',                   'answer'],\n    ['answer_type_instructions',              'answer'],\n    ['answer_type_procedure',                 'answer'],\n    ['answer_type_reason_explanation',        'answer'],\n    ['answer_well_written',                   'answer'],    \n]\n\ninput_columns = [\n    'question_title', \n    'question_body',    \n    'answer'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_question_columns = []\ntarget_answer_columns = []\n\nfor table in target_table:\n    if table[1] == 'question':\n        target_question_columns.append( table[0] )\n    elif table[1] == 'answer':\n        target_answer_columns.append( table[0] )   \n        \ntarget_columns = target_question_columns + target_answer_columns\n\nprint( 'question:', len(target_question_columns) )\nprint( 'answer:', len(target_answer_columns) )\nprint( 'total:', len(target_columns) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import html\nfor df in [train_df, test_df]:\n\n    ## domain components\n    df['domcom'] = df['url'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n\n    # count components\n    df['dom_cnt'] = df['domcom'].apply(lambda s: len(s))\n\n    # extend length\n    df['domcom'] = df['domcom'].apply(lambda s: s + ['none', 'none'])\n\n    # components\n    for ii in range(0,4):\n        df['url_'+str(ii)] = df['domcom'].apply(lambda s: s[ii])\n\n    # clean up\n    df.drop('domcom', axis = 1, inplace = True)\n    \n    df.question_body = df.question_body.apply(html.unescape)\n    df.answer        = df.answer.apply(html.unescape)\n    \n    df['question_body'] = df['question_body'].apply(lambda s: s.replace(\"\\&gt\",\">\"))\n    df['question_body'] = df['question_body'].apply(lambda s: s.replace(\"\\&lt\",\"<\"))\n    df['question_body'] = df['question_body'].apply(lambda s: s.replace(\"\\&amp\",\"&\"))\n    df['question_body'] = df['question_body'].apply(lambda s: s.replace(\"\\&quot;\",\"\\\"\"))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature = pd.DataFrame()\ntest_feature = pd.DataFrame()\nimport nltk\nfrom nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))\nfor df, feature in [[train_df, train_feature], [test_df, test_feature]]:\n    for column in input_columns:\n        feature[column+'_total_length'] = df[column].apply(len)\n        feature[column+'_capitals'] = df[column].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n        feature[column+'_caps_vs_length'] = feature.apply(lambda row: float(row[column+'_capitals'])/float(row[column+'_total_length']),axis=1)\n        feature[column+'_num_exclamation_marks'] = df[column].apply(lambda comment: comment.count('!'))\n        feature[column+'_num_question_marks'] = df[column].apply(lambda comment: comment.count('?'))\n        feature[column+'_num_punctuation'] = df[column].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n        feature[column+'_num_symbols'] = df[column].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n        feature[column+\"_num_chars\"] = df[column].apply(lambda x: len(str(x)))\n        feature[column+'_num_words'] = df[column].apply(lambda comment: len(comment.split()))\n        feature[column+'_num_unique_words'] = df[column].apply(lambda comment: len(set(w for w in comment.split())))\n        feature[column+'_words_vs_unique'] = feature[column+'_num_unique_words'] / feature[column+'_num_words']\n        feature[column+'_num_smilies'] = df[column].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n        feature[column+'_num_doubt'] = df[column].apply(lambda comment: comment.count('not sure'))\n        feature[column+'_num_think'] = df[column].apply(lambda comment: sum(comment.count(w) for w in ['thinking','think','thought']))        \n        ## Number of stopwords in the text ##\n        feature[column+\"_num_stopwords\"] = df[column].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n        \n        ## Number of punctuations in the text ##\n        feature[column+\"_num_punctuations\"] =df[column].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n        \n        ## Number of title case words in the text ##\n        feature[column+\"_num_words_upper\"] = df[column].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n        \nnlp = English()\nsentencizer = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sentencizer)\n\nans_user_and_category=train_df[train_df[['answer_user_name', 'category']].duplicated()][['answer_user_name', 'category']].values\nans_user_and_category.shape        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_question_metadata_features(text):\n    doc=nlp(text)\n    indirect = 0\n    choice_words=0\n    reason_explanation_words = 0\n    question_count = 0\n    \n    for sent in doc.sents:\n        if '?' in sent.text and '?' == sent.text[-1]:\n            question_count += 1\n            for token in sent:\n                if token.text.lower()=='why':\n                    reason_explanation_words+=1\n                elif token.text.lower()=='or':\n                    choice_words+=1\n    if question_count==0:\n        indirect+=1\n    \n    return np.array([indirect, question_count, reason_explanation_words, choice_words])\n\n\ndef question_answer_author_same(df):\n    q_username = df['question_user_name']\n    a_username = df['answer_user_name']\n    author_same=[]\n    \n    for i in range(len(df)):\n        if q_username[i] == a_username[i]:\n            author_same.append(int(1))\n        else:\n            author_same.append(int(0))\n    return author_same\n\n\ndef add_external_features( df, feature ):\n    feature['question_vs_answer_length'] = feature['question_body_num_words']/feature['answer_num_words']\n    feature['q_a_author_same'] = question_answer_author_same(df)\n    \n    answer_user_cat = []\n    for i in df[['answer_user_name', 'category']].values:\n        if i in ans_user_and_category:\n            answer_user_cat.append(int(1))\n        else:\n            answer_user_cat.append(int(0))\n    feature['answer_user_cat'] = answer_user_cat\n    \n    handmade_features=[]\n    for text in df['question_body'].values:\n        handmade_features.append(add_question_metadata_features(text))\n\n    feature = pd.concat( [ feature, pd.DataFrame( handmade_features, columns=['indirect', 'question_count', 'reason_explanation_words', 'choice_words'])], axis=1 )\n    \n    return feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature = add_external_features( train_df, train_feature )\ntest_feature  = add_external_features( test_df, test_feature )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in input_columns:\n    print( \"{} | Min: {}, Max: {}\".format( column, train_feature[column+'_total_length'].min(), train_feature[column+'_total_length'].max() ) )\n    \nprint( '=====' )\n\nfor column in input_columns:\n    print( \"{} | Min: {}, Max: {}\".format( column, test_feature[column+'_total_length'].min(), test_feature[column+'_total_length'].max() ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = nltk.corpus.stopwords.words('english')\nsymbol = [\"'\", '\"', ':', ';', '.', ',', '-', '!', '?', \"'s\", \")\", \"(\", \"...\", '``', \"''\", \"/\", \"$\", \"%\", \"*\", \"&\", \"{\", \"}\", \"[\", \"]\"]\n\ndef get_prevalent( texts, top_count=15 ):    \n    tokenized_sents = [nltk.word_tokenize(i) for i in texts]\n    tokenized_sents = [flatten for inner in tokenized_sents for flatten in inner]   \n    \n    #fdist = nltk.FreqDist(w for w in tokenized_sents if w not in stop_words + symbol)\n    fdist = nltk.FreqDist(w.lower() for w in tokenized_sents if w.lower() not in stop_words + symbol)\n    comments = fdist.most_common(top_count)\n    \n    return [word[0] for word in comments]\n\nfor column in input_columns:\n    words = get_prevalent( train_df[column])\n    print( column, words )\n    \n    for word in words:\n        for df, feature in [[train_df, train_feature], [test_df, test_feature]]:\n            feature[column+'_num'+word] = df[column].apply(lambda comment: comment.count(word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\n\ntrain_df['netloc'] = train_df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest_df['netloc'] = test_df['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ncount_columns = ['question_title', 'question_user_name', 'answer_user_name', 'category', 'netloc']\n\nfor col in count_columns:\n    value = train_df[col].value_counts()\n    train_feature[col+'_count'] = train_df[col].map( value )\n    test_feature[col+'_count']  = test_df[col].map( value ).fillna( 1 )\n    \nfor col in train_feature.columns: \n    train_mean = np.nanmean( train_feature[col].values )\n    train_feature[col].fillna( train_mean, inplace=True )\n    test_feature[col].fillna( train_mean, inplace=True ) \n    \nprint( \"train: nan=\", np.sum( np.sum( pd.isnull( train_feature ) ) ) )\nprint( \"test : nan=\", np.sum( np.sum( pd.isnull( test_feature  ) ) ) )\n\nscaler = MinMaxScaler()\nscaler.fit(train_feature)\n\ntrain_feature = pd.DataFrame(scaler.transform(train_feature), columns=train_feature.columns)\ntest_feature = pd.DataFrame(scaler.transform(test_feature), columns=test_feature.columns)\n\ndel scaler\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( train_feature.shape )\nprint( test_feature.shape )\nprint( 'time: {}'.format( time_to_str((time.time()-start_time_all),'min') ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_label( df, result_df, column, count=10 ):\n    labels = [ ( df[column].values>=(rate/count) ).astype(np.int) for rate in range( 1, count+1 ) ]\n\n    columns = ['{}_{}'.format(column, i) for i in range(count)]\n    labels = np.array( labels ).T\n    \n    label_df = pd.DataFrame( labels, columns=columns )\n    result_df = pd.concat((result_df, label_df), axis=1)\n    \n    return result_df\n\n\ndef convert_label_origin( df, result_df, column, count=10 ):\n    \n    columns = ['{}_{}'.format(column, i) for i in range(count)]\n    labels = df[columns].values\n    values = []\n    \n    for i in range( len(labels ) ):\n        value = 0.0\n                   \n        for j in range(count):\n            if labels[i][j] > 0.5:\n                value = (j+1) / count\n    \n        values.append( value )\n                   \n    label_df = pd.DataFrame( values, columns=[column] )\n    result_df = pd.concat((result_df, label_df), axis=1)\n    \n    return result_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_convert_count = 12\ntrain_feature2 = pd.DataFrame()\ntest_feature2 = pd.DataFrame()\n\nfor column in train_feature.columns:\n    train_feature2 = convert_label( train_feature, train_feature2, column, label_convert_count )\n    test_feature2 = convert_label( test_feature, test_feature2, column, label_convert_count )   \n    \ntrain_feature = train_feature2.copy()\ntest_feature = test_feature2.copy()\n\ndel train_feature2, test_feature2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( train_feature.shape )\nprint( test_feature.shape )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['question_title', 'question_user_name', 'answer_user_name']\nlimits = [6, 8, 8]\n\n#features = ['question_title', 'question_user_name', 'answer_user_name', 'category', 'url_0', 'url_1']\n#limits = [6, 8, 8, 1, 60, 1]\n\nfor col, limit in zip( features, limits ):\n    value = train_df[col].value_counts()\n    train_df['item_count'] = train_df[col].map( value )    \n    train_df['item_value'] = train_df[col].copy()\n    train_df.loc[train_df.item_count < limit, 'item_value'] = \"limit_abcdefg123456789\"\n    \n    test_df['item_count'] = test_df[col].map( value ).fillna( 1 )    \n    test_df['item_value'] = test_df[col].copy()\n    test_df.loc[test_df.item_count < limit, 'item_value'] = \"limit_abcdefg123456789\"    \n    \n    lb = LabelBinarizer()\n    lb.fit( train_df['item_value'] )\n    \n    encode_train = lb.transform(train_df['item_value'])\n    encode_test = lb.transform(test_df['item_value'])\n    \n    columns = ['LabelBinarizer_{}'.format(i) for i in range(encode_train.shape[1])]\n    print( \"{}: {}\". format( col, len( train_df['item_value'].value_counts() ) ) )\n    \n    encode_train = pd.DataFrame( encode_train, columns=columns )\n    train_feature = pd.concat((train_feature, encode_train), axis=1)\n\n    encode_test = pd.DataFrame( encode_test, columns=columns )\n    test_feature = pd.concat((test_feature, encode_test), axis=1)      \n    \n    del lb\n    \ntrain_df.drop( ['item_count', 'item_count'], axis=1, inplace=True )\ntest_df.drop( ['item_count', 'item_count'], axis=1, inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['url_0', 'category']\nenc = OneHotEncoder( handle_unknown='ignore' )\nenc.fit( train_df[features] )\n\nencode_train = enc.transform(train_df[features]).toarray()\nencode_test = enc.transform(test_df[features]).toarray()\n\ncolumns = ['encode_{}'.format(i) for i in range(encode_train.shape[1])]\n\nencode_train = pd.DataFrame( encode_train, columns=columns )\ntrain_feature = pd.concat((train_feature, encode_train), axis=1)\n\nencode_test = pd.DataFrame( encode_test, columns=columns )\ntest_feature = pd.concat((test_feature, encode_test), axis=1)   \n\ndel encode_train, encode_test, enc\nprint( train_feature.shape )\nprint( test_feature.shape )\nprint( 'time: {}'.format( time_to_str((time.time()-start_time_all),'min') ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_feature.columns: \n    train_mean = np.nanmean( train_feature[col].values )\n    train_feature[col].fillna( train_mean, inplace=True )\n    test_feature[col].fillna( train_mean, inplace=True ) \n    \nprint( \"train: nan=\", np.sum( np.sum( pd.isnull( train_feature ) ) ) )\nprint( \"test : nan=\", np.sum( np.sum( pd.isnull( test_feature  ) ) ) )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/chenshengabc/from-quest-encoding-ensemble-a-little-bit-differen\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n#         df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n        df[col] = df[col].apply(lambda x: handle_contractions(x))  \n        df[col] = df[col].apply(lambda x: fix_quote(x))   \n    \n    return df\n\n\ntrain_df = clean_data( train_df, input_columns )\ntest_df = clean_data( test_df, input_columns )\n\ndel tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Cyclical Learning Rates for Training Neural Networks'- Leslie N. Smith, arxiv 2017\n#       https://arxiv.org/abs/1506.01186\n#       https://github.com/bckenstler/CLR\n\nclass CyclicScheduler1():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10 ):\n        super(CyclicScheduler1, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n\n    def __call__(self, time):\n\n        #sawtooth\n        #r = (1-(time%self.period)/self.period)\n\n        #cosine\n        time= time%self.period\n        r = (np.cos(time/self.period *np.pi)+1)/2\n\n        lr = self.min_lr + r*(self.max_lr-self.min_lr)\n        return lr\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.3f, max_lr=%0.3f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\nclass CyclicScheduler2():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10, max_decay=1.0, warm_start=0 ):\n        super(CyclicScheduler2, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n        self.max_decay = max_decay\n        self.warm_start = warm_start\n        self.cycle = -1\n\n    def __call__(self, time):\n        if time<self.warm_start: return self.max_lr\n\n        #cosine\n        self.cycle = (time-self.warm_start)//self.period\n        time = (time-self.warm_start)%self.period\n\n        period = self.period\n        min_lr = self.min_lr\n        max_lr = self.max_lr *(self.max_decay**self.cycle)\n\n\n        r   = (np.cos(time/period *np.pi)+1)/2\n        lr = min_lr + r*(max_lr-min_lr)\n\n        return lr\n\n\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.4f, max_lr=%0.4f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\n#tanh curve\nclass CyclicScheduler3():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10, max_decay=1.0, warm_start=0 ):\n        super(CyclicScheduler3, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n        self.max_decay = max_decay\n        self.warm_start = warm_start\n        self.cycle = -1\n\n    def __call__(self, time):\n        if time<self.warm_start: return self.max_lr\n\n        #cosine\n        self.cycle = (time-self.warm_start)//self.period\n        time = (time-self.warm_start)%self.period\n\n        period = self.period\n        min_lr = self.min_lr\n        max_lr = self.max_lr *(self.max_decay**self.cycle)\n\n        r   = (np.tanh(-time/period *16 +8)+1)*0.5\n        lr = min_lr + r*(max_lr-min_lr)\n\n        return lr\n\n\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.3f, max_lr=%0.3f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\nclass NullScheduler():\n    def __init__(self, lr=0.01 ):\n        super(NullScheduler, self).__init__()\n        self.lr    = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = 'NullScheduler\\n' \\\n                + 'lr=%0.5f '%(self.lr)\n        return string\n\n\n# net ------------------------------------\n# https://github.com/pytorch/examples/blob/master/imagenet/main.py ###############\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n        lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\nclass NeuralNet3(nn.Module):\n    def __init__(self, num_features, num_labels, pretrained_bert):\n        super(NeuralNet3, self).__init__()\n        self.config = transformers.GPT2Config.from_pretrained(pretrained_bert, output_hidden_states=False)\n        self.gptmodel = transformers.GPT2Model(self.config)\n        self.gpt = self.gptmodel.from_pretrained( pretrained_bert,config =self.config)\n        \n        self.num_features = num_features\n        self.num_labels = num_labels\n        \n        self.encoded_dropout = SpatialDropout( 0.2 )\n        self.pooled_dropout = nn.Dropout( 0.2 )        \n        \n        self.feature_linear = nn.Sequential(\n            nn.Linear( self.num_features, self.num_features ),\n            nn.ReLU( inplace=True ),\n            nn.Dropout( 0.2 ),            \n        )\n        \n        '''\n        self.feature_linear = nn.Sequential(\n            nn.Linear( self.num_features, self.num_features ),\n            nn.BatchNorm1d( self.num_features ),\n            nn.ReLU( inplace=True ),\n            nn.Linear( self.num_features, self.num_features2 ),\n        )\n        '''\n        \n        dense_hidden_units = self.gpt.config.hidden_size * 3 + self.num_features\n        \n        self.linear1 = nn.Linear( dense_hidden_units, dense_hidden_units )\n        self.linear2 = nn.Linear( dense_hidden_units, dense_hidden_units )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear( dense_hidden_units, num_labels ),\n        )\n        \n    def forward( self, ids, masks, segments, feature ):  \n        \n        feature_output = self.feature_linear( feature )\n        outputs = self.gpt( input_ids=ids, attention_mask=masks, token_type_ids=segments)\n\n        last_hidden_state,present = self.gpt( input_ids=ids, attention_mask=masks, token_type_ids=segments)\n       # sequence_output = self.encoded_dropout(sequence_output)\n       # pooled_output = self.pooled_dropout(pooled_output)     \n\n      #  h12 = hidden_states[-1][:, 0].reshape((-1, 1, 768))\n      #  h11 = hidden_states[-2][:, 0].reshape((-1, 1, 768))\n      #  h10 = hidden_states[-3][:, 0].reshape((-1, 1, 768))\n      #  h9 = hidden_states[-4][:, 0].reshape((-1, 1, 768))\n\n      #  hidden_states = torch.cat([h9, h10, h11, h12], 1)\n        last_hidden_state = self.encoded_dropout(last_hidden_state)\n\n        avg_pool = torch.mean( last_hidden_state, 1 )\n        max_pool, _ = torch.max( last_hidden_state, 1 )\n\n        h_conc = torch.cat( ( avg_pool, max_pool, last_hidden_state[:, -1, :], feature_output ), 1 )\n        h_conc_linear  = F.relu(self.linear1(h_conc))\n        hidden = h_conc + h_conc_linear \n            \n        h_conc_linear  = F.relu(self.linear2(hidden))\n        hidden = hidden + h_conc_linear      \n        \n        return self.classifier( hidden )\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_title_question( df, tokenizer, max_sequence_length=512 ):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    \n    for _, instance in df.iterrows():\n         \n        title    = tokenizer.tokenize(instance.question_title)\n        question = tokenizer.tokenize(instance.question_body)\n\n        if (len(title)+len(question)+3) > max_sequence_length:\n            if len(title) > 30:\n                title = title[:30]\n                \n            question_len = max_sequence_length - len(title) - 3\n            question = question[:question_len]\n        \n        #token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] \n        #token_ids = tokenizer.convert_tokens_to_ids(token)\n        \n        title_ids    = tokenizer.convert_tokens_to_ids(title)\n        question_ids = tokenizer.convert_tokens_to_ids(question)\n        cls_ids      = tokenizer.convert_tokens_to_ids( [\"[CLS]\"] )   \n        sep_ids      = tokenizer.convert_tokens_to_ids( [\"[SEP]\"] ) \n        \n        token_ids =  title_ids  + question_ids \n        padding = [0] * (max_sequence_length - len(token_ids))\n        \n        ids      = token_ids + padding\n        masks    = [1]*len(token_ids) + padding\n        len_q = len(question_ids)\n        len_qf = int(len(question_ids)/2)\n        len_ql = len_q-len_qf\n        segments = [0]*(len(title_ids)) + [1]*(len(question_ids)) + padding\n       # segments = [0]*(len(title_ids)+2) + [1]*(len_qf+1)+[2]*(len_ql) + padding\n\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),   \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_title_answer( df, tokenizer, max_sequence_length=512 ):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    \n    for _, instance in df.iterrows():\n         \n        title  = tokenizer.tokenize(instance.question_title)\n        answer = tokenizer.tokenize(instance.answer)\n\n        if (len(title)+len(answer)+3) > max_sequence_length:\n            if len(title) > 30:\n                title = title[:30]\n                \n            answer_len = max_sequence_length - len(title) - 3\n            answer = answer[:answer_len]\n        \n        #token = [\"[CLS]\"] + title + [\"[SEP]\"] + answer + [\"[SEP]\"] \n        #token_ids = tokenizer.convert_tokens_to_ids(token)\n        \n        title_ids  = tokenizer.convert_tokens_to_ids(title)\n        answer_ids = tokenizer.convert_tokens_to_ids(answer)\n        cls_ids    = tokenizer.convert_tokens_to_ids( [\"[CLS]\"] )   \n        sep_ids    = tokenizer.convert_tokens_to_ids( [\"[SEP]\"] ) \n        \n        token_ids =  title_ids  + answer_ids \n        padding = [0] * (max_sequence_length - len(token_ids))\n        \n        ids      = token_ids + padding\n        masks    = [1]*len(token_ids) + padding\n        segments = [0]*(len(title_ids)) + [1]*(len(answer_ids)) + padding\n        \n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),   \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_arrays( df, tokenizer, max_sequence_length=512 ):\n    \n    input_ids, input_masks, input_segments = [], [], []    \n    t_max_len = 35     \n    \n    for _, instance in df.iterrows():\n        \n        title    = tokenizer.tokenize(instance.question_title)\n        question = tokenizer.tokenize(instance.question_body)\n        answer   = tokenizer.tokenize(instance.answer)\n\n        if (len(title)+len(question)+len(answer)+4) > max_sequence_length:\n            if len(title) > t_max_len:\n                title = title[:t_max_len]\n                \n            question_len = ( max_sequence_length - len(title) - 4 ) // 2\n            question = question[:question_len]\n            \n            answer_len = max_sequence_length - len(title) - len(question) - 4\n            answer = answer[:answer_len]\n        \n        #token = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n        #token_ids = tokenizer.convert_tokens_to_ids(token)\n        \n        title_ids    = tokenizer.convert_tokens_to_ids(title)\n        question_ids = tokenizer.convert_tokens_to_ids(question)\n        answer_ids   = tokenizer.convert_tokens_to_ids(answer) \n        cls_ids      = tokenizer.convert_tokens_to_ids( [\"[CLS]\"] )   \n        sep_ids      = tokenizer.convert_tokens_to_ids( [\"[SEP]\"] ) \n        \n        token_ids = cls_ids + title_ids + sep_ids + question_ids + sep_ids + answer_ids + sep_ids\n        padding = [0] * (max_sequence_length - len(token_ids))\n        \n        ids      = token_ids + padding\n        masks    = [1]*len(token_ids) + padding\n        segments = [0]*(len(title_ids)+len(question_ids)+3) + [1]*(len(answer_ids)+1) + padding\n        \n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),   \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 6\n\n'''\nx_train = train_feature.values\ny_train = train_df[target_columns].values\n\ncv = KFold( n_splits=n_splits, random_state=SEED )\nkfold_split = list( cv.split( x_train, y_train ) )'''\n\ncv = GroupKFold( n_splits=n_splits )\nkfold_split = list( cv.split( X=train_df.question_body, groups=train_df.question_body ) )   \n\n'''write_pickle_to_file( 'kfold_split_index.pkl', kfold_split )\n'''\n\n#kfold_split = read_pickle_from_file( model_weight_path1 + 'kfold_split_index.pkl' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_datas = {}\ntest_pred_weights = {}\n\nfor column in target_columns:\n    test_pred_datas[column] = np.zeros( len(test_df) )\n    test_pred_weights[column] = 0.0\n    \ndef add_test_pred_data( prediction, columns, weight ):\n    for column_idx, column in enumerate( columns ):\n        test_pred_datas[column] += weight * prediction[:, column_idx]  \n        test_pred_weights[column] += weight    \n        \nvalidation_datas = {}\nvalidation_counts = {}\n\nfor column in target_columns:\n    validation_datas[column] = np.zeros( len(train_df) )\n    validation_counts[column] = np.zeros( len(train_df) )\n\ndef add_validation_data( prediction, columns, idx ):\n    for column_idx, column in enumerate( columns ):\n        validation_datas[column][idx] += prediction[:, column_idx]  \n        validation_counts[column][idx] += 1.0    \n        \n        \ndef mean_spearmanr_correlation_score( y_true, y_pred ):\n    num_labels = y_pred.shape[1]\n    score = np.nanmean( [ spearmanr( y_pred[:, idx], y_true[:, idx] ).correlation for idx in range(num_labels) ] )\n    return score \n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, features, labels = None):\n        \n        self.inputs   = inputs\n        self.features = features\n        \n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        input_features  = self.features[idx]\n        \n        if self.labels is not None: # targets\n            input_labels = self.labels[idx]\n            return input_ids, input_masks, input_segments, input_features, input_labels\n        \n        return input_ids, input_masks, input_segments, input_features\n\n    def __len__(self):\n        return len(self.inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test( test_inputs, x_test, label_columns, weight_files ):\n    \n    batch_size = 6\n\n    test_dataset = QuestDataset( test_inputs, x_test, None )\n    test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, shuffle=False )    \n    \n    num_features = x_test.shape[1]\n    num_labels   = len(label_columns)\n\n    for fname in weight_files:\n\n        model = NeuralNet3( num_features, num_labels, pretrained_bert )\n        model.cuda()\n\n        model.load_state_dict( torch.load( fname ) )      \n        model.eval()\n     \n        test_preds_fold  = []\n\n        with torch.no_grad():\n            for ids, masks, segments, features in test_loader:\n                ids      = ids.cuda()\n                masks    = masks.cuda()\n                segments = segments.cuda()\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                y_pred = model( ids, masks, segments, features )\n                test_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n            test_preds_fold = np.array( test_preds_fold )\n            add_test_pred_data( test_preds_fold, label_columns, 1.0 )\n\n        del model\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    del test_dataset, test_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_validation( train_inputs, x_train, label_columns, weight_files ):\n    \n    if len(kfold_split) != len(weight_files):\n        return\n    \n    batch_size   = 6    \n    num_features = x_test.shape[1]\n    num_labels   = len(label_columns)\n\n    for k, (train_idx, valid_idx) in enumerate(kfold_split):\n        \n        x_train_valid = x_train[valid_idx]\n\n        train_inputs_valid = [x[valid_idx] for x in train_inputs]\n\n        valid_dataset = QuestDataset( train_inputs_valid, x_train_valid, None )\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False )\n        \n        model = NeuralNet3( num_features, num_labels, pretrained_bert )\n        model.cuda()\n\n        fname = weight_files[k]\n        model.load_state_dict( torch.load( fname ) )      \n        model.eval()\n     \n        valid_preds_fold  = []\n\n        with torch.no_grad():\n            for ids, masks, segments, features in valid_loader:\n                ids      = ids.cuda()\n                masks    = masks.cuda()\n                segments = segments.cuda()\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                y_pred = model( ids, masks, segments, features )\n                valid_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n            valid_preds_fold = np.array( valid_preds_fold )\n            add_validation_data( valid_preds_fold, label_columns, valid_idx )\n\n        del model, valid_dataset, valid_loader\n        torch.cuda.empty_cache()\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test_validation( label_columns, train_inputs, test_inputs, x_train, x_test, weight_files ):\n    \n    if len(kfold_split) != len(weight_files):\n        return\n    \n    batch_size   = 6    \n    num_features = x_test.shape[1]\n    num_labels   = len(label_columns)\n\n    test_dataset = QuestDataset( test_inputs, x_test, None )\n    test_loader = torch.utils.data.DataLoader( test_dataset, batch_size=batch_size, shuffle=False )   \n    \n    for k, (train_idx, valid_idx) in enumerate(kfold_split):\n        \n        fname = weight_files[k]\n        print( k+1, fname )\n        \n        x_train_valid = x_train[valid_idx]\n        train_inputs_valid = [x[valid_idx] for x in train_inputs]\n\n        valid_dataset = QuestDataset( train_inputs_valid, x_train_valid, None )\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False )\n        \n        model = NeuralNet3( num_features, num_labels, pretrained_bert )\n        model.cuda()\n\n        model.load_state_dict( torch.load( fname ) )      \n        model.eval()\n     \n        #====================\n        #validation\n        valid_preds_fold  = []\n\n        with torch.no_grad():\n            for ids, masks, segments, features in valid_loader:\n                ids      = ids.cuda()\n                masks    = masks.cuda()\n                segments = segments.cuda()\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                y_pred = model( ids, masks, segments, features )\n                valid_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n            valid_preds_fold = np.array( valid_preds_fold )\n            add_validation_data( valid_preds_fold, label_columns, valid_idx )\n            \n        #====================\n        #test\n        test_preds_fold  = []\n\n        with torch.no_grad():\n            for ids, masks, segments, features in test_loader:\n                ids      = ids.cuda()\n                masks    = masks.cuda()\n                segments = segments.cuda()\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                y_pred = model( ids, masks, segments, features )\n                test_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n            test_preds_fold = np.array( test_preds_fold )\n            add_test_pred_data( test_preds_fold, label_columns, 1.0 )            \n\n        del model, valid_dataset, valid_loader\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n    del test_dataset, test_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.GPT2Tokenizer.from_pretrained( pretrained_bert )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( 'time: {}'.format( time_to_str((time.time()-start_time_all), 'min' ) ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs = compute_input_title_question( train_df, tokenizer, max_sequence_length=512 )\ntest_inputs = compute_input_title_question( test_df, tokenizer, max_sequence_length=512 )\n\nx_train = train_feature.values\nx_test  = test_feature.values\n\nprint( x_train.shape )\nprint( x_test.shape )\nprint( len( target_question_columns ) )\n\nweight_files = [\n    model_weight_path1 ,\n    model_weight_path2 ,\n    model_weight_path3 ,\n    model_weight_path4 ,\n    model_weight_path5,\n    model_weight_path6,\n]\n\nmodel_test_validation( target_question_columns, train_inputs, test_inputs, x_train, x_test, weight_files )\n\ndel x_train, x_test\ndel train_inputs, test_inputs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs = compute_input_arrays( train_df, tokenizer, max_sequence_length=512 )\ntest_inputs = compute_input_arrays( test_df, tokenizer, max_sequence_length=512 )\n\nx_train = train_feature.values\nx_test  = test_feature.values\n\nprint( x_train.shape )\nprint( x_test.shape )\nprint( len( target_answer_columns ) )\n\nweight_files = [\n    model_weight_path7,\n    model_weight_path8,\n    model_weight_path9,\n    model_weight_path10,\n    model_weight_path11,\n    model_weight_path12,\n]\n\nmodel_test_validation( target_answer_columns, train_inputs, test_inputs, x_train, x_test, weight_files )\n\ndel x_train, x_test\ndel train_inputs, test_inputs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_feature, test_feature\ndel target_question_columns, target_answer_columns \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in target_columns:\n    print( np.sum( validation_counts[column] ), np.min( validation_counts[column] ), np.max( validation_counts[column] ), column )\n    \nvalidationTG  = pd.DataFrame()\n\nfor column in target_columns:\n    preds = validation_datas[column]\n    count = validation_counts[column]\n    count = np.where( count < 0.5, 1.0, count )\n    \n    validationTG[column] = preds / count\n    \nmean_spearmanr_correlation_score( validationTG.values, train_df[target_columns].values )   \nvalidationTG.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_spearmanr_correlation_score( validationTG.values, train_df[target_columns].values )   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predsTG = pd.read_csv( f\"{folder}sample_submission.csv\" )\n\nfor column in target_columns:\n    preds = test_pred_datas[column]\n    weight = test_pred_weights[column]\n    test_predsTG[column] = preds / weight    \n    #output = rankdata( output )\n    #max_val = np.max(output) + 1\n    #output = output / max_val + 1e-12\n        \ntest_predsTG.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del target_columns, input_columns\ndel test_pred_datas, test_pred_weights\ndel validation_datas, validation_counts\n\ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# QUEST_005_04_1"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# QUEST_005_07_DISTILBERT+LSTM (DISTILBERT)"},{"metadata":{"trusted":true},"cell_type":"code","source":"folder = '../input/google-quest-challenge/'\npretrained_distilbert_base_uncased = \"../input/pretrainedbertpytorch/pretrained-bert-pytorch/distilbert-base-uncased/\"\nuniversal_sentence_encoder_path = \"../input/universalsentenceencoderlarge4/\"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\n\nimport pickle  \nimport numpy as np\nimport pandas as pd\nimport random\nimport copy\nimport string\nimport time\n\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import OneHotEncoder, LabelBinarizer\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport math\nfrom tqdm import tqdm\n\nfrom spacy.lang.en import English\nfrom urllib.parse import urlparse\nimport math\n\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom torch.nn.utils.weight_norm import weight_norm\n\nfrom scipy.stats import spearmanr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nprint(\"transformers:\", transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from radam import RAdam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 12345","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef read_pickle_from_file(pickle_file):\n    with open(pickle_file,'rb') as f:\n        x = pickle.load(f)\n    return x\n\ndef write_pickle_to_file(pickle_file, x):\n    with open(pickle_file, 'wb') as f:\n        pickle.dump(x, f, pickle.HIGHEST_PROTOCOL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv( f\"{folder}train.csv\" )\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv( f\"{folder}test.csv\" )\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df.head(3).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract target variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_columns = [\n    'question_asker_intent_understanding',\n    'question_body_critical',\n    'question_conversational',\n    'question_expect_short_answer',\n    'question_fact_seeking',\n    'question_has_commonly_accepted_answer',\n    'question_interestingness_others',\n    'question_interestingness_self',\n    'question_multi_intent',\n    'question_not_really_a_question',\n    'question_opinion_seeking',\n    'question_type_choice',\n    'question_type_compare',\n    'question_type_consequence',\n    'question_type_definition',\n    'question_type_entity',\n    'question_type_instructions',\n    'question_type_procedure',\n    'question_type_reason_explanation',\n    'question_type_spelling',\n    'question_well_written',\n    'answer_helpful',\n    'answer_level_of_information',\n    'answer_plausible',\n    'answer_relevance',\n    'answer_satisfaction',\n    'answer_type_instructions',\n    'answer_type_procedure',\n    'answer_type_reason_explanation',\n    'answer_well_written'    \n]\n\ninput_columns = [\n    'question_title', \n    'question_body',    \n    'answer'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( 'target_columns:', len(target_columns) )\nprint( 'input_columns:', len(input_columns) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in [train_df, test_df]:\n\n    ## domain components\n    df['domcom'] = df['url'].apply(lambda s: s.split('://')[1].split('/')[0].split('.'))\n\n    # count components\n    df['dom_cnt'] = df['domcom'].apply(lambda s: len(s))\n\n    # extend length\n    df['domcom'] = df['domcom'].apply(lambda s: s + ['none', 'none'])\n\n    # components\n    for ii in range(0,4):\n        df['url_'+str(ii)] = df['domcom'].apply(lambda s: s[ii])\n\n    # clean up\n    df.drop('domcom', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature = pd.DataFrame()\ntest_feature = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['url_0', 'category']\nenc = OneHotEncoder( handle_unknown='ignore' )\nenc.fit( train_df[features] )\n\nencode_train = enc.transform(train_df[features]).toarray()\nencode_test = enc.transform(test_df[features]).toarray()\n\ncolumns = ['encode_{}'.format(i) for i in range(encode_train.shape[1])]\n\nencode_train = pd.DataFrame( encode_train, columns=columns )\ntrain_feature = pd.concat((train_feature, encode_train), axis=1)\n\nencode_test = pd.DataFrame( encode_test, columns=columns )\ntest_feature = pd.concat((test_feature, encode_test), axis=1)   \n\ndel encode_train, encode_test, enc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( train_feature.shape )\nprint( test_feature.shape )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DISTILBERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestBertDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, labels = None):\n        \n        self.inputs   = inputs\n        \n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        \n        if self.labels is not None: # targets\n            input_labels = self.labels[idx]\n            return input_ids, input_masks, input_labels\n        \n        return input_ids, input_masks\n\n    def __len__(self):\n        return len(self.inputs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_input_text( texts, tokenizer, max_sequence_length=512 ):\n    \n    input_ids   = []\n    input_masks = []\n    \n    for text in texts:\n         \n        text = tokenizer.tokenize(text)\n        \n        text_ids = tokenizer.convert_tokens_to_ids(text)\n        cls_ids  = tokenizer.convert_tokens_to_ids( [\"[CLS]\"] )   \n        sep_ids  = tokenizer.convert_tokens_to_ids( [\"[SEP]\"] ) \n        \n        if (len(text_ids)+2) > max_sequence_length:\n            text_ids = text_ids[:max_sequence_length-2]        \n        \n        token_ids = cls_ids + text_ids + sep_ids\n        padding = [0] * (max_sequence_length - len(token_ids))\n        \n        ids      = token_ids + padding\n        masks    = [1]*len(token_ids) + padding\n        \n        input_ids.append(ids)\n        input_masks.append(masks)\n\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long()\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_bert_feature( tokenizer, model, df, column ):\n    \n    print( column )\n    \n    inputs = compute_input_text( df[column].values, tokenizer )\n    dataset = QuestBertDataset( inputs, None )\n    loader = torch.utils.data.DataLoader( dataset, batch_size=6, shuffle=False, drop_last=False )   \n    \n    preds_fold = []\n\n    model.eval() \n\n    with torch.no_grad():\n        for ids, masks in tqdm(loader):\n            ids   = ids.cuda()\n            masks = masks.cuda()\n\n            outputs = model( input_ids=ids, attention_mask=masks )  \n            x = outputs[0][:, 0, :]\n\n            preds_fold.extend( x.cpu().data.numpy().tolist() )    \n            \n    preds_fold = np.array( preds_fold )\n    \n    return preds_fold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.DistilBertTokenizer.from_pretrained( pretrained_distilbert_base_uncased )\nmodel = transformers.DistilBertModel.from_pretrained( pretrained_distilbert_base_uncased )\nmodel.cuda()\n\nprint( 'train:' )\ntrain_question_title_dense = get_bert_feature( tokenizer, model, train_df, 'question_title' )\ntrain_question_body_dense  = get_bert_feature( tokenizer, model, train_df, 'question_body' )\ntrain_answer_dense         = get_bert_feature( tokenizer, model, train_df, 'answer' )\n\nprint( 'test:' )\ntest_question_title_dense = get_bert_feature( tokenizer, model, test_df, 'question_title' )\ntest_question_body_dense  = get_bert_feature( tokenizer, model, test_df, 'question_body' )\ntest_answer_dense         = get_bert_feature( tokenizer, model, test_df, 'answer' )\n\ndel tokenizer, model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Universal Sentence Encoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed = hub.load(universal_sentence_encoder_path)\n\nembeddings_train = {}\nembeddings_test = {}\n\nfor text in input_columns:\n    print(text)\n    train_text = train_df[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = test_df[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \ndel embed\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_train_df = pd.DataFrame()\nembeddings_test_df = pd.DataFrame()\n\nl2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\nabs_dist = lambda x, y: np.abs(x-y).sum(axis=1)\nsum_dist = lambda x, y: (x+y).sum(axis=1)\n\ndist_columns = [\n    ['question_title_embedding', 'answer_embedding'],\n    ['question_body_embedding', 'answer_embedding'],\n    ['question_body_embedding', 'question_title_embedding'],\n]\n\nfor i, columns in enumerate(dist_columns):\n    embeddings_train_df[f'l2_dist_embedding_{i}']  = l2_dist( embeddings_train[columns[0]], embeddings_train[columns[1]] )\n    embeddings_train_df[f'cos_dist_embedding_{i}'] = cos_dist( embeddings_train[columns[0]], embeddings_train[columns[1]] )\n    embeddings_train_df[f'abs_dist_embedding_{i}'] = abs_dist( embeddings_train[columns[0]], embeddings_train[columns[1]] )\n    embeddings_train_df[f'l2_dist_embedding_{i}']  = sum_dist( embeddings_train[columns[0]], embeddings_train[columns[1]] )\n    \n    embeddings_test_df[f'l2_dist_embedding_{i}']  = l2_dist( embeddings_test[columns[0]], embeddings_test[columns[1]] )\n    embeddings_test_df[f'cos_dist_embedding_{i}'] = cos_dist( embeddings_test[columns[0]], embeddings_test[columns[1]] )\n    embeddings_test_df[f'abs_dist_embedding_{i}'] = abs_dist( embeddings_test[columns[0]], embeddings_test[columns[1]] )\n    embeddings_test_df[f'l2_dist_embedding_{i}']  = sum_dist( embeddings_test[columns[0]], embeddings_test[columns[1]] )    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.hstack( [item for k, item in embeddings_train.items()] + [train_question_title_dense, train_question_body_dense, train_answer_dense, embeddings_train_df.values, train_feature.values]  )\nx_test  = np.hstack( [item for k, item in embeddings_test.items()] + [test_question_title_dense, test_question_body_dense, test_answer_dense, embeddings_test_df.values, test_feature.values]  )\ny_train = train_df[target_columns].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_feature, test_feature\ndel embeddings_train, embeddings_test\ndel embeddings_train_df, embeddings_test_df\ndel train_question_title_dense, train_question_body_dense, train_answer_dense\ndel test_question_title_dense, test_question_body_dense, test_answer_dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( x_train.shape )\nprint( x_test.shape )\nprint( y_train.shape )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'Cyclical Learning Rates for Training Neural Networks'- Leslie N. Smith, arxiv 2017\n#       https://arxiv.org/abs/1506.01186\n#       https://github.com/bckenstler/CLR\n\nclass CyclicScheduler1():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10 ):\n        super(CyclicScheduler1, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n\n    def __call__(self, time):\n\n        #sawtooth\n        #r = (1-(time%self.period)/self.period)\n\n        #cosine\n        time= time%self.period\n        r = (np.cos(time/self.period *np.pi)+1)/2\n\n        lr = self.min_lr + r*(self.max_lr-self.min_lr)\n        return lr\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.3f, max_lr=%0.3f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\nclass CyclicScheduler2():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10, max_decay=1.0, warm_start=0 ):\n        super(CyclicScheduler2, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n        self.max_decay = max_decay\n        self.warm_start = warm_start\n        self.cycle = -1\n\n    def __call__(self, time):\n        if time<self.warm_start: return self.max_lr\n\n        #cosine\n        self.cycle = (time-self.warm_start)//self.period\n        time = (time-self.warm_start)%self.period\n\n        period = self.period\n        min_lr = self.min_lr\n        max_lr = self.max_lr *(self.max_decay**self.cycle)\n\n\n        r   = (np.cos(time/period *np.pi)+1)/2\n        lr = min_lr + r*(max_lr-min_lr)\n\n        return lr\n\n\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.4f, max_lr=%0.4f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\n#tanh curve\nclass CyclicScheduler3():\n\n    def __init__(self, min_lr=0.001, max_lr=0.01, period=10, max_decay=1.0, warm_start=0 ):\n        super(CyclicScheduler3, self).__init__()\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.period = period\n        self.max_decay = max_decay\n        self.warm_start = warm_start\n        self.cycle = -1\n\n    def __call__(self, time):\n        if time<self.warm_start: return self.max_lr\n\n        #cosine\n        self.cycle = (time-self.warm_start)//self.period\n        time = (time-self.warm_start)%self.period\n\n        period = self.period\n        min_lr = self.min_lr\n        max_lr = self.max_lr *(self.max_decay**self.cycle)\n\n        r   = (np.tanh(-time/period *16 +8)+1)*0.5\n        lr = min_lr + r*(max_lr-min_lr)\n\n        return lr\n\n\n\n    def __str__(self):\n        string = 'CyclicScheduler\\n' \\\n                + 'min_lr=%0.3f, max_lr=%0.3f, period=%8.1f'%(self.min_lr, self.max_lr, self.period)\n        return string\n\n\nclass NullScheduler():\n    def __init__(self, lr=0.01 ):\n        super(NullScheduler, self).__init__()\n        self.lr    = lr\n        self.cycle = 0\n\n    def __call__(self, time):\n        return self.lr\n\n    def __str__(self):\n        string = 'NullScheduler\\n' \\\n                + 'lr=%0.5f '%(self.lr)\n        return string\n\n\n# net ------------------------------------\n# https://github.com/pytorch/examples/blob/master/imagenet/main.py ###############\ndef adjust_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\ndef get_learning_rate(optimizer):\n    lr=[]\n    for param_group in optimizer.param_groups:\n        lr +=[ param_group['lr'] ]\n\n    assert(len(lr)==1) #we support only one param_group\n    lr = lr[0]\n\n    return lr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mean_spearmanr_correlation_score( y_true, y_pred ):\n    num_labels = y_pred.shape[1]\n    return np.nanmean( [ spearmanr( y_pred[:, idx], y_true[:, idx] ).correlation for idx in range(num_labels) ] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestDataset( torch.utils.data.Dataset ):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.x[idx], self.y[idx]\n        else:\n            return self.x[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNet(nn.Module):\n    def __init__(self, num_features, num_labels):\n        super(NeuralNet, self).__init__()\n        \n        self.num_features = num_features\n        self.num_labels = num_labels\n        \n        self.classifier = nn.Sequential(\n            nn.Linear( self.num_features, self.num_features ),\n            nn.Dropout( 0.2 ),\n            nn.Linear( self.num_features, self.num_features),\n          #  nn.Dropout( 0.1 ),                 \n            nn.Linear( self.num_features, self.num_labels ),\n        )\n        \n    def forward( self, features ):\n        return self.classifier( features )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_datas = {}\ntest_pred_weights = {}\n\ndef init_test_pred_data():\n    for column in target_columns:\n        test_pred_datas[column] = np.zeros( len(test_df) )\n        test_pred_weights[column] = 0.0\n    \ndef add_test_pred_data( prediction, columns, weight ):\n    for column_idx, column in enumerate( columns ):\n        test_pred_datas[column] += weight * prediction[:, column_idx]  \n        test_pred_weights[column] += weight    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_datas = {}\nvalidation_counts = {}\n\ndef init_validation_data():\n    for column in target_columns:\n        validation_datas[column] = np.zeros( len(train_df) )\n        validation_counts[column] = np.zeros( len(train_df) )\n    \ndef add_validation_data( prediction, columns, idx ):\n    for column_idx, column in enumerate( columns ):\n        validation_datas[column][idx] += prediction[:, column_idx]  \n        validation_counts[column][idx] += 1.0    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_test_pred_data()\ninit_validation_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5\n\n#cv = KFold(n_splits=n_splits, random_state=SEED)\n#kfold_split = list( cv.split( x_train, y_train ) )\n\ncv = GroupKFold( n_splits=n_splits )\nkfold_split = list( cv.split( X=train_df.question_body, groups=train_df.question_body ) )   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 50\npatience = 5\nscores   = []\n\nbatch_size = 32\n\nnum_features = x_train.shape[1]\nnum_labels   = y_train.shape[1]\n    \nfor k, (train_idx, valid_idx) in enumerate( kfold_split ):\n    \n    print( \"k:\", k+1 )\n    \n    #train\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    \n    train_dataset = QuestDataset( x_train_train, y_train_train )\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True )\n    \n    #valid\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    valid_dataset = QuestDataset( x_train_valid, y_train_valid )\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False )\n    \n    model = NeuralNet( num_features, num_labels )\n    model.cuda()\n    \n    best_weights = copy.deepcopy( model.state_dict() ) \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    #optimizer = optim.Adam( model.parameters(), lr=2e-5 )\n    optimizer = RAdam( model.parameters(), lr=2e-5 )\n    \n    schduler = CyclicScheduler2( min_lr=2e-6, max_lr=2e-5, period=20, warm_start=0, max_decay=0.9 )\n    \n    min_loss = np.inf\n    counter = 0\n    \n    for epoch in range(n_epochs):\n    #for epoch in tqdm( range(n_epochs) ):\n        \n        lr = schduler( epoch )\n        adjust_learning_rate( optimizer, lr )\n        #lr = get_learning_rate( optimizer )        \n        \n        model.train()\n        train_loss = []\n            \n        for features, labels in train_loader:\n            features = torch.tensor(features,dtype=torch.float32).cuda()\n            labels   = torch.tensor(labels,dtype=torch.float32).cuda()   \n            \n            y_pred = model( features )\n            loss = loss_fn( y_pred, labels )        \n                           \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss.append( loss.item() )\n        \n        avg_train_loss = np.mean( train_loss )\n            \n        model.eval()\n        val_loss = []\n        \n        with torch.no_grad():\n            for features, labels in valid_loader:\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                labels   = torch.tensor(labels,dtype=torch.float32).cuda()   \n                \n                y_pred = model( features )\n                loss = loss_fn( y_pred, labels )\n                val_loss.append( loss.item() )\n\n        avg_val_loss = np.mean( val_loss )       \n        \n        if avg_val_loss < min_loss:\n            min_loss = avg_val_loss\n            counter = 0\n            best_weights = copy.deepcopy( model.state_dict() ) \n                \n        else:\n            counter += 1\n            # print('Early stopping: %i / %i' % (counter, self.patience))\n            if counter >= patience and epoch > 12:\n                # print('Early stopping at epoch', epoch + 1)\n                break        \n        \n    model.load_state_dict( best_weights )    \n    model.eval()\n    valid_preds_fold = []\n    valid_true_fold = []\n    \n    with torch.no_grad():\n        for features, labels in valid_loader:\n            features = torch.tensor(features,dtype=torch.float32).cuda()\n            labels   = torch.tensor(labels,dtype=torch.float32).cuda()  \n            y_pred = model( features )\n            \n            valid_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n            valid_true_fold.extend( labels.cpu().data.numpy().tolist() )\n       \n    valid_preds_fold = np.array( valid_preds_fold )\n    valid_true_fold = np.array( valid_true_fold )\n    \n    add_validation_data( valid_preds_fold, target_columns, valid_idx )\n    score = mean_spearmanr_correlation_score( valid_preds_fold, valid_true_fold )\n    print('Score:', score)\n    \n    scores.append(score)\n    test_preds_fold  = []\n    \n    test_dataset = QuestDataset( x_test, None )\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False )\n\n    with torch.no_grad():\n        for features in test_loader:\n            features = torch.tensor(features,dtype=torch.float32).cuda()\n            y_pred = model( features )\n            \n            test_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n        test_preds_fold = np.array( test_preds_fold )\n        add_test_pred_data( test_preds_fold, target_columns, 1.0 )\n        \n    del model, optimizer, loss_fn, best_weights\n    del train_dataset, train_loader\n    del valid_dataset, valid_loader  \n    del test_dataset, test_loader\n    torch.cuda.empty_cache()\n    gc.collect() \n\nprint( '================' )\nprint( 'Mean score:', np.mean(scores)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"validation0 = pd.DataFrame()\n\nfor column in target_columns:\n    preds = validation_datas[column]\n    count = validation_counts[column]\n    count = np.where( count < 0.5, 1.0, count )\n    \n    validation0[column] = preds / count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_spearmanr_correlation_score( train_df[target_columns].values, validation0.values )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds0 = pd.read_csv( f\"{folder}sample_submission.csv\" )\n\nfor column in target_columns:\n    output = test_pred_datas[column] / test_pred_weights[column]\n    test_preds0[column] = output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# QUEST_005_07_DISTILBERT+LSTM (LSTM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_UNITS = 512\n#LSTM_UNITS = 1024\nDENSE_HIDDEN_UNITS = 6 * LSTM_UNITS\n\nclass MODEL_v001(nn.Module):\n    def __init__(self, num_features, num_labels ):\n        super().__init__()\n        self.lstm1 = nn.LSTM( num_features , LSTM_UNITS, bidirectional=True, batch_first=True )\n        self.lstm2 = nn.LSTM( LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True,dropout=0.2 )\n        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linearnorm = nn.LayerNorm(DENSE_HIDDEN_UNITS)\n        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.linearnorm2 = nn.LayerNorm(DENSE_HIDDEN_UNITS)\n        self.linear_sub_out = nn.Linear(DENSE_HIDDEN_UNITS, num_labels)\n\n    def forward(self, x, lengths=None):\n        h_lstm1, _ = self.lstm1(x)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n\n        avg_pool1 = torch.mean(h_lstm1, 1)\n        avg_pool2 = torch.mean(h_lstm2, 1)\n        max_pool2, _ = torch.max(h_lstm2, 1)\n\n        h_conc = torch.cat((avg_pool1, max_pool2, avg_pool2), 1)\n        h_conc_linear1 = self.linearnorm(F.relu(self.linear1(h_conc)))\n        h_conc_linear2 = self.linearnorm2(F.relu(self.linear2(h_conc)))\n\n        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n        out = self.linear_sub_out(hidden)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.expand_dims( x_train, 1 )\nx_test = np.expand_dims( x_test, 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"init_test_pred_data()\ninit_validation_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5\n\n#cv = KFold(n_splits=n_splits, random_state=SEED)\n#kfold_split = list( cv.split( x_train, y_train ) )\n\ncv = GroupKFold( n_splits=n_splits )\nkfold_split = list( cv.split( X=train_df.question_body, groups=train_df.question_body ) )   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 8\npatience = 4\nscores   = []\n\nbatch_size = 8\n\nnum_features = x_train.shape[2]\nnum_labels   = y_train.shape[1]\n    \nfor k, (train_idx, valid_idx) in enumerate( kfold_split ):\n    \n    print( \"k:\", k+1 )\n    \n    #train\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    \n    train_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(x_train[train_idx]), \n        torch.from_numpy(y_train[train_idx])\n    )\n    \n    train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=batch_size, shuffle=True, drop_last=True )\n    \n    #valid\n    valid_dataset = torch.utils.data.TensorDataset(\n        torch.from_numpy(x_train[valid_idx]), \n        torch.from_numpy(y_train[valid_idx])\n    )\n    \n    valid_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=batch_size, shuffle=False, drop_last=False )\n    \n    model = MODEL_v001( num_features, num_labels )\n    model.cuda()\n    \n    best_weights = copy.deepcopy( model.state_dict() ) \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    optimizer = torch.optim.Adam( model.parameters() )\n    #optimizer = optim.Adam( model.parameters(), lr=2e-5 )\n    #optimizer = RAdam( model.parameters(), lr=2e-5 )\n    \n    min_loss = np.inf\n    counter = 0\n    \n    for epoch in range(n_epochs):\n    #for epoch in tqdm( range(n_epochs) ):   \n        \n        model.train()\n        train_loss = []\n            \n        for features, labels in train_loader:\n            features = torch.tensor(features,dtype=torch.float32).cuda()\n            labels   = torch.tensor(labels,dtype=torch.float32).cuda()   \n            \n            y_pred = model( features )\n            loss = loss_fn( y_pred, labels )        \n                           \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss.append( loss.item() )\n        \n        avg_train_loss = np.mean( train_loss )\n            \n        model.eval()\n        val_loss = []\n        \n        with torch.no_grad():\n            for features, labels in valid_loader:\n                features = torch.tensor(features,dtype=torch.float32).cuda()\n                labels   = torch.tensor(labels,dtype=torch.float32).cuda()   \n                \n                y_pred = model( features )\n                loss = loss_fn( y_pred, labels )\n                val_loss.append( loss.item() )\n\n        avg_val_loss = np.mean( val_loss )       \n        \n        if avg_val_loss < min_loss:\n            min_loss = avg_val_loss\n            counter = 0\n            best_weights = copy.deepcopy( model.state_dict() ) \n                \n        else:\n            counter += 1\n            # print('Early stopping: %i / %i' % (counter, self.patience))\n            if counter >= patience:\n                # print('Early stopping at epoch', epoch + 1)\n                break        \n        \n    model.load_state_dict( best_weights )    \n    model.eval()\n    valid_preds_fold = []\n    valid_true_fold = []\n    \n    with torch.no_grad():\n        for features, labels in valid_loader:\n            features = torch.tensor(features,dtype=torch.float32).cuda()\n            labels   = torch.tensor(labels,dtype=torch.float32).cuda()  \n            y_pred = model( features )\n            \n            valid_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n            valid_true_fold.extend( labels.cpu().data.numpy().tolist() )\n       \n    valid_preds_fold = np.array( valid_preds_fold )\n    valid_true_fold = np.array( valid_true_fold )\n    \n    add_validation_data( valid_preds_fold, target_columns, valid_idx )\n    score = mean_spearmanr_correlation_score( valid_preds_fold, valid_true_fold )\n    print('Score:', score)\n    \n    scores.append(score)\n    test_preds_fold  = []\n    \n    test_dataset = QuestDataset( x_test, None )\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False )\n\n    with torch.no_grad():\n        for features in test_loader:\n            features = torch.tensor(features,dtype=torch.float32).cuda()\n            y_pred = model( features )\n            \n            test_preds_fold.extend( torch.sigmoid( y_pred ).cpu().data.numpy().tolist() )\n\n        test_preds_fold = np.array( test_preds_fold )\n        add_test_pred_data( test_preds_fold, target_columns, 1.0 )\n        \n    del model, optimizer, loss_fn, best_weights\n    del train_dataset, train_loader\n    del valid_dataset, valid_loader  \n    del test_dataset, test_loader\n    torch.cuda.empty_cache()\n    gc.collect() \n\nprint( '================' )\nprint( 'Mean score:', np.mean(scores)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"validationLSTM = pd.DataFrame()\n\nfor column in target_columns:\n    preds = validation_datas[column]\n    count = validation_counts[column]\n    count = np.where( count < 0.5, 1.0, count )\n    \n    validationLSTM[column] = preds / count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validationLSTM.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_spearmanr_correlation_score( validationLSTM.values, train_df[target_columns].values )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predsLSTM = pd.read_csv( f\"{folder}sample_submission.csv\" )\n\nfor column in target_columns:\n    output = test_pred_datas[column] / test_pred_weights[column]\n    test_predsLSTM[column] = output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predsLSTM.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, test_df\ndel x_train, x_test, y_train\ndel validation_datas, validation_counts\ndel test_pred_datas, test_pred_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blabla","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# post processing"},{"metadata":{},"cell_type":"markdown","source":"| Name                    | valid          | test_preds     |\n|-------------------------|----------------|----------------|\n| QUEST_005_07-distilbert | validation0    | test_preds0    |\n| QUEST-003-55            | validationT    | test_predsT    |\n| QUEST-006-55 -GPT2      | validationTG    | test_predsTG  |\n| QUEST-005-04-1          | validationT2   | test_predsT2   |\n| QUEST-005_07-lstm       | validationLSTM | test_predsLSTM |"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/google-quest-challenge/'\ntrain = pd.read_csv(path_join(data_dir, 'train.csv'))\ntest = pd.read_csv(path_join(data_dir, 'test.csv'))\nsample = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\n\nprint(train.shape, test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import partial\nimport scipy as sp\n\nclass OptimizedRounder(object):\n    def __init__(self,correlation):\n        self.correlation = correlation\n        self.coef_ = 0\n        self.score = 0\n\n    def _kappa_loss(self, coef, X, y):\n        a= X.copy()\n        b=y.copy()\n        X_p = pd.cut(a, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0,1,2])\n        \n        a[X_p == 0] = 0\n        a[X_p == 2] = 1 \n\n        #print(\"validation score = {}\".format(spearmanr(a, b).correlation))\n        if spearmanr(a, b).correlation < self.correlation:\n            self.score = spearmanr(a, b).correlation\n            return - spearmanr(a, b).correlation + (self.correlation - spearmanr(a, b).correlation + 1)**10\n        else:\n            self.score = spearmanr(a, b).correlation\n            return - spearmanr(a, b).correlation\n\n    def fit(self, X, y,coef_ini):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = coef_ini\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def coefficients(self):\n        return self.coef_['x']\n    def score_fc(self):\n        return self.score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\ndef optimize_preds( y_df, x_df, preds_df ):\n    \n    for title in targets:\n        \n        y1 = np.asarray(y_df[title].copy())\n        X1 = np.asarray(x_df[title].copy())\n        \n        x_original     = np.asarray(x_df[title].copy())\n        preds_original = np.asarray(preds_df[title].copy())     \n\n        correlation = spearmanr(y1, X1).correlation\n        print(title)\n        print(correlation)\n\n        coefficients_target = [0,0]\n        correlation = spearmanr(y1, X1).correlation\n        okcor = spearmanr(y1, X1).correlation\n        maxi = correlation\n        liste = [[0.1,0.9],[0.2,0.9],[0.3,0.9],[0.3,0.8],[0.2,0.7],[0.3,0.7],[0.2,0.6],[0.5,0.9],[0.3,0.6],[0.4,0.8],[0.7,0.9],[0.8,0.9]]\n\n        for L in liste:\n            optR = OptimizedRounder(correlation)\n            optR.fit(X1,y1,L)\n            coefficients = optR.coefficients()\n            if optR.score_fc() > maxi:\n                maxi = optR.score_fc()\n                coefficients_target = coefficients\n\n        if maxi != spearmanr(y1, X1).correlation:\n            oof = X1.copy()\n            oof[oof > coefficients_target[1]] = 1\n            oof[oof <= coefficients_target[0]] = 0\n\n            X1 = np.asarray(preds_df[title].copy())\n            oof_test = X1.copy()\n            oof_test[oof_test > coefficients_target[1]] = 1\n            oof_test[oof_test <= coefficients_target[0]] = 0\n\n            score = spearmanr(y1, oof).correlation\n            if score - okcor > 0.001:\n                print(\"difference validation score = {}\".format(score - okcor))\n                x_df[title] = oof\n\n                dist = Counter(x_df[title])\n\n                if 0 in list(dist.keys()):\n                    dist[0] /= len(x_df)\n                if 1 in list(dist.keys()):\n                    dist[1] /= len(x_df)\n\n                acum = 0\n                bound = {}\n\n                if 0 in list(dist.keys()):\n                    acum = dist[0]\n                    bound[0] = np.percentile(preds_df[title], acum * 100)\n\n                if 1 in list(dist.keys()):\n                    acum = 1 - dist[1]\n                    bound[1] = np.percentile(preds_df[title], acum * 100)\n\n                def classify(x):\n                    if 0 in list(dist.keys()):\n                        if 1 in list(dist.keys()):\n                            if x <= bound[0]:\n                                return 0\n                            elif x <= bound[1]:\n                                return x\n                            else:\n                                return 1\n                        else:\n                            if x <= bound[0]:\n                                return 0\n                            else:\n                                return x\n                    else:\n                        if 1 in list(dist.keys()):\n                            if x <= bound[1]:\n                                return x\n                            else:\n                                return 1\n                        else:\n                            return x\n\n                final_pred = np.array(list(map(classify, preds_df[title])))\n                if len(np.unique(oof_test)) != 1:\n                    print(coefficients_target)\n                    preds_df[title] = final_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalidationN=pd.read_csv('../input/nirjharbert003/validation_preds_df_003.csv') \ntest_predsN=pd.read_csv('../input/nirjharbert003/submission_003.csv')\n\nprint(\"Nirjhar model : \")\ncompute_spearmanr_ignore_nan(np.asarray(train[targets].copy()), np.asarray(validationN[targets].copy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nvalidationN1=pd.read_csv('../input/dataset-nirjhar/validation_preds_df_002.csv') \ntest_predsN1=pd.read_csv('../input/dataset-nirjhar/submission_002.csv')\n\nprint(\"Nirjhar model : \")\ncompute_spearmanr_ignore_nan(np.asarray(train[targets].copy()), np.asarray(validationN1[targets].copy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_table = []\ndata_table.append( {'Name': 'QUEST-003-55',            'valid': copy.deepcopy( validationT ),    'test_preds': copy.deepcopy( test_predsT ) } )\n#data_table.append( {'Name': 'QUEST-005-04-1',          'valid': copy.deepcopy( validationT2 ),   'test_preds': copy.deepcopy( test_predsT2 ) } )\ndata_table.append( {'Name': 'QUEST-006-03-GPT2',        'valid': copy.deepcopy( validationTG ),   'test_preds': copy.deepcopy( test_predsTG ) } )\ndata_table.append( {'Name': 'QUEST-005-07-distilbert', 'valid': copy.deepcopy( validation0 ),    'test_preds': copy.deepcopy( test_preds0 ) } )\ndata_table.append( {'Name': 'QUEST-005-07-lstm',       'valid': copy.deepcopy( validationLSTM ), 'test_preds': copy.deepcopy( test_predsLSTM ) } )\ndata_table.append( {'Name': 'QUEST-Bert-Nirjhar',       'valid': copy.deepcopy( validationN ), 'test_preds': copy.deepcopy( test_predsN ) } )\ndata_table.append( {'Name': 'QUEST-Bert-Nirjhar1',       'valid': copy.deepcopy( validationN1 ), 'test_preds': copy.deepcopy( test_predsN1 ) } )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_original = len(data_table)\nscore_list = []\n\nfor i in range( n_original ):\n    print( '=========================' )\n    print( data_table[i]['Name'] )\n    print( '=========================' )\n    \n    data_table[i]['valid'] = pd.DataFrame(data_table[i]['valid']).rank() / len(data_table[i]['valid'])\n    data_table[i]['test_preds'] = pd.DataFrame(data_table[i]['test_preds']).rank() / len(data_table[i]['test_preds'])   \n    \n    data_table.append( copy.deepcopy( data_table[i] ) )\n        \n    score1 = compute_spearmanr_ignore_nan( np.asarray( train[targets].copy() ), np.asarray( data_table[i]['valid'].copy() ) )\n    \n    optimize_preds( train, data_table[i]['valid'], data_table[i]['test_preds'] )\n    \n    score2 = compute_spearmanr_ignore_nan( np.asarray( train[targets].copy() ), np.asarray( data_table[i]['valid'].copy() ) )\n    score_list.append( [score1, score2] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range( len(score_list) ):    \n    score1, score2 = score_list[i][0], score_list[i][1]\n    print( '{:.6f} -> {:.6f} ({:.6f}) : {}'.format( score1, score2, score2 - score1, data_table[i]['Name'] ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_totale = train[targets].copy()\ntest_preds_totale = sample.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for title in targets:\n\n    y1 = np.asarray(train[title].copy())\n\n    X_list = [ np.asarray( x['valid'][title].copy() ) for x in data_table ]\n    test_list = [ np.asarray( x['test_preds'][title].copy() ) for x in data_table ]     \n    corr_list = [ spearmanr(y1, x).correlation for x in X_list ]\n\n    i = np.argmax(corr_list)\n\n    corr_best  = copy.deepcopy( corr_list[i] )\n    x_best     = copy.deepcopy( X_list[i] )\n    test_best  = copy.deepcopy( test_list[i] )\n    \n    for weight_range in np.arange( 0.45, 0.95, 0.005 ):\n        \n        corr_list2 = [c*c for c in corr_list]\n        cmin = np.min( corr_list2 )\n        cmax = np.max( corr_list2 )\n\n        weights = ( corr_list2 - cmin ) / ( cmax - cmin ) \n        weights *= weight_range\n        weights += ( 1.0 - weight_range )\n\n        #print( weight_range, weights, title )\n\n        x = np.average( X_list, axis=0, weights=weights )   \n        x = rankdata( x ) / len( x )\n        corr = spearmanr( y1, x ).correlation\n\n        if corr > corr_best:\n            corr_best  = copy.deepcopy( corr )\n            x_best     = copy.deepcopy( x )\n\n            t = np.average( test_list, axis=0, weights=weights )   \n            t = rankdata( t ) / len( t )       \n            test_best  = copy.deepcopy( t )  \n            \n\n    validation_totale[title] = copy.deepcopy( x_best )\n    test_preds_totale[title] = copy.deepcopy( test_best )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_spearmanr_ignore_nan(np.asarray(train[targets].copy()), np.asarray(validation_totale[targets].copy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def function_spelling(row):\n    if row in 'CULTURE':\n        return 1\n    else:\n        return 0\n\nvalidation_totale['question_type_spelling'] = train['category'].apply(function_spelling)*validation_totale['question_type_spelling']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"def function_spelling2(row):\n    if 'english.stackexchange' in str(row):\n        return 1\n    if 'ell.stackexchange' in str(row):\n        return 1\n    else:\n        return 0\n#validation_totale['question_type_spelling'] = train['question_user_page'].apply(function_spelling2)*validation_totale['question_type_spelling']"},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_spearmanr_ignore_nan(np.asarray(train[targets].copy()), np.asarray(validation_totale[targets].copy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\n\nfor title in targets:\n    for i, row in test_preds_totale.iterrows():\n        submission.loc[submission['qa_id'] == row['qa_id'], title] = row[title]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['question_type_spelling'] = test['category'].apply(function_spelling)*submission['question_type_spelling']\n#submission['question_type_spelling'] = test['question_user_page'].apply(function_spelling2)*validation['question_type_spelling']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"compute_spearmanr_ignore_nan(np.asarray(train[targets].copy()), np.asarray(validation_totale[targets].copy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for title in targets:\n    submission[title] = submission[title].apply(lambda x: 1 if x>1 else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"665f722224b040f3b9d23ab1fa6fb4c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"6bd76db42f9647a181f3cab8a5c0e323":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8335945776cd4058bac8a4fcbc458813":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8717947a95254177937d94e83e29620c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e9e0fc7552b44cb9a9edd53b2fcf680":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8717947a95254177937d94e83e29620c","placeholder":"​","style":"IPY_MODEL_6bd76db42f9647a181f3cab8a5c0e323","value":" 476/? [00:15&lt;00:00, 30.85it/s]"}},"add1d5c06409409aaab3ec1fffbadc90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d69b4a1adf1645c1a062f6ed993144d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_add1d5c06409409aaab3ec1fffbadc90","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_665f722224b040f3b9d23ab1fa6fb4c9","value":1}},"fbc75b52f6034979aa98f4f9cb341ffc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d69b4a1adf1645c1a062f6ed993144d0","IPY_MODEL_9e9e0fc7552b44cb9a9edd53b2fcf680"],"layout":"IPY_MODEL_8335945776cd4058bac8a4fcbc458813"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}