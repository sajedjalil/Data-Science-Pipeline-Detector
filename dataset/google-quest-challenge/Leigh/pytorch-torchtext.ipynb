{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Load libs and files"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchtext\nfrom torchtext import vocab, data\n\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.insert(0, '../input/glove-reddit-comments/')\nfrom clean_text import RegExCleaner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/google-quest-challenge'\nEMB_PATH = '../input/embeddings-glove-crawl-torch-cached'\nEMB_FILENAME = 'GloVe.Reddit.120B.512D.txt'\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nN_SPLITS = 4\nBATCH_SIZE = 128","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nsubm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='qa_id')\nsubm.loc[:, :] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset and loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just simple tokenizer\ntknz = RegExCleaner.reddits()\ndef tokenizer(text):\n    text = tknz(text).split()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## define the columns that we want to process and how to process\n\n## text field, We have 2 text field, so we must fixed by length\ntxt_field = data.Field(sequential=True, tokenize=tokenizer, fix_length=125,\n                       batch_first=False, include_lengths=False,  use_vocab=True)\n## Numeric fields\nnum_field = data.Field(sequential=False, dtype=torch.float,  use_vocab=False)\nidx_field = data.Field(sequential=False, dtype=torch.int,  use_vocab=False)\n## Fields which don't need preprocessing\nraw_field = data.RawField()\n\nlabels = df.columns[11:]\nnum_columns = list(zip(labels, [num_field]*len(labels)))\nbasic_columns = [\n    ('qa_id', idx_field),\n    ('question_title', raw_field),\n    ('question_body', txt_field),\n    ('question_user_name', raw_field),\n    ('question_user_page', raw_field),\n    ('answer', txt_field),\n    ('answer_user_name', raw_field),\n    ('answer_user_page', raw_field),\n    ('url', raw_field),\n    ('category', raw_field),\n    ('host', raw_field),\n]\ntrain_fields = basic_columns + num_columns\ntest_fields = basic_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading csv file\ntrain_ds = data.TabularDataset(path=os.path.join(DATA_PATH, 'train.csv'), \n                           format='csv',\n                           fields=train_fields, \n                           skip_header=True)\n\ntest_ds = data.TabularDataset(path=os.path.join(DATA_PATH, 'test.csv'), \n                           format='csv',\n                           fields=test_fields, \n                           skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example\nprint('Chunk of answer: '+' '.join(train_ds.examples[1].answer[:10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify the path to the localy saved vectors\nvec = vocab.Vectors(os.path.join(EMB_PATH, EMB_FILENAME), cache=EMB_PATH)\n# build the vocabulary using train and validation dataset and assign the vectors\ntxt_field.build_vocab(train_ds, test_ds, max_size=300000, vectors=vec)\n\nembs_vocab = train_ds.fields['question_body'].vocab.vectors\nprint('Embedding vocab size: ', embs_vocab.size()[0])\nvocab_size = embs_vocab.size()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Wrapper for loaders, which structured fields\nclass BatchWrapper:\n      def __init__(self, dataloader, mode='train'):\n            self.dataloader, self.mode = dataloader, mode\n     \n      def __iter__(self):\n            if self.mode =='test':\n                for batch in self.dataloader:\n                    yield (batch.qa_id, batch.question_body, batch.answer)\n            else:\n                for batch in self.dataloader:\n                    target = torch.stack([getattr(batch, label) for label in labels], dim=-1)\n                    yield (batch.question_body,  batch.answer, target)\n  \n      def __len__(self):\n            return len(self.dl)\n\ndef wrapper(ds, mode='train', **kwargs):\n    dataloader = data.BucketIterator(ds, device=DEVICE, **kwargs)\n    return BatchWrapper(dataloader, mode)\n\ndef splits_cv(dataset, cv, y=None, batch_size=BATCH_SIZE):\n    \"\"\"\n        Split dataset to train and validation used cross-validator and wrap loader\n    \"\"\"\n    for indices in cv.split(range(len(dataset)), y):\n        (train_data, valid_data) = tuple([dataset.examples[i] for i in index] for index in indices)\n        yield tuple(wrapper(data.Dataset(d, dataset.fields), batch_size=batch_size) for d in (train_data, valid_data) if d)\n        \ncv = KFold(n_splits=N_SPLITS, random_state=6699)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = wrapper(test_ds, batch_size=BATCH_SIZE, shuffle=False, repeat=False, mode='test')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN_QA(nn.Module):\n    def __init__(self, embs_vocab, hidden_size=64, layers=1,\n                 dropout=0., bidirectional=False, num_classes=30):\n        super().__init__()\n\n        coef = 2 if bidirectional else 1\n        dropout = dropout if layers > 1 else 0\n        self.emb = nn.Embedding.from_pretrained(embs_vocab, freeze=True)\n                \n        self.question = nn.LSTM(embs_vocab.size(1), hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        \n        self.answer = nn.LSTM(embs_vocab.size(1), hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        \n        self.classifier = nn.Sequential(\n                nn.Linear(2*hidden_size*coef, 64),\n                nn.ReLU(inplace=True),\n                nn.Linear(64, num_classes)\n            )\n                \n    def forward(self, q, a):\n        \n        q = self.emb(q)\n        a = self.emb(a)\n        \n        q_rnn, _ = self.question(q)\n        a_rnn, _ = self.answer(a)\n        \n        q_rnn, _ = q_rnn.max(dim=0, keepdim=False) \n        a_rnn, _ = a_rnn.max(dim=0, keepdim=False) \n        \n        out = torch.cat([q_rnn, a_rnn], dim=-1)\n        out = self.classifier(out).sigmoid()\n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train, oof prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def metric_fn(p, t):\n    score = 0\n    for i in range(p.shape[1]):\n        score += np.nan_to_num(spearmanr(p[:,i], t[:,i])[0])\n    score /= 30\n    return score\n\n@torch.no_grad()\ndef validation_fn(model, loader, loss_fn):\n    y_pred, y_true, tloss = [], [], []\n    for q, a, target in loader:\n        outputs = model(q, a)\n        loss = loss_fn(outputs, target)\n        tloss.append(loss.item())\n        y_true.append(target.detach().cpu().numpy())\n        y_pred.append(outputs.detach().cpu().numpy())\n        \n    tloss = np.array(tloss).mean()\n    y_pred = np.concatenate(y_pred)\n    y_true = np.concatenate(y_true)\n    metric = metric_fn(y_pred, y_true)\n    return tloss, metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Table for results\nheader = r'''\n           Train       Validation\nEpoch | Loss |Spearm| Loss |Spearm| Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:6.3f}'*4 + '\\u2502{:6.2f}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def oof_preds(train_ds, test_loader, embs_vocab, epochs = 4):\n\n    for loader, vloader in splits_cv(train_ds, cv):\n        \n        model = RNN_QA(embs_vocab, hidden_size=128, dropout=0.1, bidirectional=True).to(DEVICE)\n        \n        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), 1e-3,\n                                                     betas=(0.75, 0.999), weight_decay=1e-3)\n        loss_fn = torch.nn.BCELoss()\n        print(header)\n        for epoch in range(1,epochs+1):      \n            y_pred, y_true = [], []\n            start_time = time.time()\n            tloss = []          \n            model.train()\n            \n            for q, a, target in loader:\n                optimizer.zero_grad()\n                outputs = model(q, a)\n                loss = loss_fn(outputs, target)\n                tloss.append(loss.item())\n                loss.backward()\n                optimizer.step()\n                y_true.append(target.detach().cpu().numpy())\n                y_pred.append(outputs.detach().cpu().numpy())\n\n            tloss = np.array(tloss).mean()\n            y_pred = np.concatenate(y_pred)\n            y_true = np.concatenate(y_true)\n            tmetric = metric_fn(y_pred, y_true)\n\n            vloss, vmetric = validation_fn(model, vloader, loss_fn)\n            if epoch % 2 == 0:\n                print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)/60**1))\n\n       \n        # Get prediction for test set\n        qa_id, preds = [], [] \n        with torch.no_grad():\n            for qaids, q, a in test_loader:\n                outputs = model(q, a)\n                qa_id.append(qaids.cpu().numpy())\n                preds.append(outputs.detach().cpu().numpy())\n            \n        # Save prediction of test set\n        qa_id = np.concatenate(qa_id)\n        preds = np.concatenate(preds)\n        subm.loc[qa_id, labels]  =  subm.loc[qa_id, labels].values + preds / N_SPLITS\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_preds(train_ds, test_loader, embs_vocab, epochs = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}