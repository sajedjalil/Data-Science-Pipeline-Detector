{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install \"../input/kerasswa/keras-swa-0.1.2\"  > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport gc\nimport glob\nimport os\nimport sys\nimport string\nimport random\nfrom tqdm import tqdm_notebook\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder, minmax_scale, MultiLabelBinarizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom scipy.stats import spearmanr, rankdata\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1 = CPU only\n\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom gensim.models.doc2vec import TaggedDocument\nfrom gensim import utils\nimport torch\n\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport transformers\n\n\nimport tensorflow as tf\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)\n\nimport tensorflow_hub as hub\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nfrom keras import Model\nfrom swa.keras import SWA\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport pickle    \n\ndef save_obj(obj, name ):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\ndef load_obj(name ):\n    with open(name + '.pkl', 'rb') as f:\n        return pickle.load(f)\n                \n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"nltk.data.path.append(\"../input/nltk-data/nltk_data\")\n# thse are in nltk_data dataset \n# nltk.download('wordnet')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('punkt')\n# nltk.download('stopwords')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"#INPUT_PATH=\"/kaggle/input/\"\nINPUT_PATH=\"../input/\"\ntrain = pd.read_csv(INPUT_PATH+'google-quest-challenge/train.csv')\ntest = pd.read_csv(INPUT_PATH+'google-quest-challenge/test.csv')\nsubmission = pd.read_csv(INPUT_PATH+'google-quest-challenge/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title','question_body','answer']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#clean data\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n#         df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = clean_data(train, input_columns)\ntest = clean_data(test, input_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train['question_body'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def constructLabeledSentences(data):\n    sentences=[]\n    for index, row in data.iteritems():\n        sentences.append(TaggedDocument(utils.to_unicode(row).split(), ['Text' + '_%s' % str(index)]))\n    return sentences\n\ndef textClean(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n    \ndef cleanup(text):\n    text = textClean(text)\n    text= text.translate(str.maketrans(\"\",\"\", string.punctuation))\n    return text\n\ntrain_question_body_sentences = constructLabeledSentences(train['question_body'])\ntrain_question_title_sentences = constructLabeledSentences(train['question_title'])\ntrain_answer_sentences = constructLabeledSentences(train['answer'])\n\ntest_question_body_sentences = constructLabeledSentences(test['question_body'])\ntest_question_title_sentences = constructLabeledSentences(test['question_title'])\ntest_answer_sentences = constructLabeledSentences(test['answer'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from gensim.models import Doc2Vec\n\nall_sentences = train_question_body_sentences + \\\n                train_answer_sentences + \\\n                test_question_body_sentences + \\\n                test_answer_sentences\n\nText_INPUT_DIM=128\ntext_model = Doc2Vec(min_count=1, window=5, vector_size=Text_INPUT_DIM, sample=1e-4, negative=5, workers=4, epochs=5,seed=1)\ntext_model.build_vocab(all_sentences)\ntext_model.train(all_sentences, total_examples=text_model.corpus_count, epochs=text_model.iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\n\ndef normalize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN') or tag.startswith('PRP'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            continue\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos).lower())\n    return lemmatized_sentence\n\n#print(set(normalize_sentence(word_tokenize(train['question_body'][0]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def normalize_vectorize(df, columns: list):\n    for col in columns:\n        print(col)\n        df[col+'_norm'] = df[col].apply(lambda x: ' '.join(set(normalize_sentence(word_tokenize(x)))))\n        df[col+'_vec'] = df[col].apply(lambda x: text_model.infer_vector([x]))\n\n    return df\n\ntrain = normalize_vectorize(train, input_columns)\ntest = normalize_vectorize(test, input_columns)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#bert embedings\ntry:\n    pbe = load_obj(\"../input/questembeddings/precomputed_bert_embeddings\")\n    train_question_body_dense = pbe['train_question_body_dense']\n    train_answer_dense = pbe['train_answer_dense']\n    train_question_title_dense = pbe['train_question_title_dense']\n    test_question_body_dense = pbe['test_question_body_dense']\n    test_answer_dense = pbe['test_answer_dense']\n    test_question_title_dense = pbe['test_question_title_dense']\nexcept:\n    print(\"Load failed, build embedding\")\n    def sigmoid(x):\n        return 1 / (1 + math.exp(-x))\n\n    def chunks(l, n):\n        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n        for i in range(0, len(l), n):\n            yield l[i:i + n]\n\n    def fetch_vectors(string_list, batch_size=64):\n        # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n        DEVICE = torch.device(\"cuda\")\n        tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n        model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n        model.to(DEVICE)\n\n        fin_features = []\n        for data in tqdm_notebook(chunks(string_list, batch_size)):\n            tokenized = []\n            for x in data:\n                x = \" \".join(x.strip().split()[:300])\n                tok = tokenizer.encode(x, add_special_tokens=True)\n                tokenized.append(tok[:512])\n\n            max_len = 512\n            padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized], dtype='int64')\n            attention_mask = np.where(padded != 0, 1, 0)\n            input_ids = torch.tensor(padded).to(DEVICE)\n            attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n            with torch.no_grad():\n                last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n            features = last_hidden_states[0][:, 0, :].cpu().numpy()\n            fin_features.append(features)\n\n        fin_features = np.vstack(fin_features)\n        return fin_features\n\n    train_question_body_dense = fetch_vectors(train.question_body.values)\n    train_answer_dense = fetch_vectors(train.answer.values)\n    train_question_title_dense = fetch_vectors(train.question_title.values)\n\n\n    test_question_body_dense = fetch_vectors(test.question_body.values)\n    test_answer_dense = fetch_vectors(test.answer.values)\n    test_question_title_dense = fetch_vectors(test.question_title.values)\n\n    precomputed_bert_embeddings = {\n        'train_question_body_dense': train_question_body_dense,\n        'train_answer_dense': train_answer_dense,\n        'train_question_title_dense': train_question_title_dense,\n        'test_question_body_dense': test_question_body_dense,\n        'test_answer_dense': test_answer_dense,\n        'test_question_title_dense': test_question_title_dense,\n\n    }\n\n    #save_obj(precomputed_bert_embeddings,\"../input/questembeddings/precomputed_bert_embeddings\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\ntfquestion_title = tfidf.fit_transform(train[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test[\"question_title\"].values)\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test[\"question_body\"].values)\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train[\"answer\"].values)\ntfanswer_test = tfidf.transform(test[\"answer\"].values)\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.cuda.empty_cache() # release all gpu memory from pytorch","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# universal sentence encoder\n\ntry:\n    embeddings_train = load_obj(\"../input/questembeddings/use_embeddings_train\")\n    embeddings_test = load_obj(\"../input/questembeddings/use_embeddings_test\")\nexcept:\n    print(\"Load failed, build embedding\")\n    try:\n        module_url = INPUT_PATH+'universalsentenceencoderlarge4/'\n        embed = hub.load(module_url)\n        def UniversalEmbedding(x):\n            results = embed(tf.squeeze(tf.cast(x, tf.string)))[\"outputs\"]\n            return keras.backend.concatenate([results])\n    except:\n        module_url = INPUT_PATH+'universalsentenceencoderlarge3/'\n        embed = hub.Module(module_url)\n        def UniversalEmbedding(x):\n            results = embed(tf.squeeze(tf.cast(x, tf.string)), signature=\"default\", as_dict=True)[\"default\"]\n            return keras.backend.concatenate([results])\n\n    embeddings_train = {}\n    embeddings_test = {}\n    for text in input_columns:\n        print(text)\n        train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n        test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n\n        curr_train_emb = []\n        curr_test_emb = []\n        batch_size = 4\n        ind = 0\n        while ind*batch_size < len(train_text):\n            curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n            ind += 1\n\n        ind = 0\n        while ind*batch_size < len(test_text):\n            curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n            ind += 1    \n\n        embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n        embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n\n    del embed\n    K.clear_session()\n    gc.collect()\n    \n    #save_obj(embeddings_train,\"../input/questembeddings/use_embeddings_train\")\n    #save_obj(embeddings_test,\"../input/questembeddings/use_embeddings_test\")","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[40,51],"trusted":false},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\nfeatures_lrg = ['category', 'netloc', 'question_user_name','answer_user_name','host']\nfeatures_sml = ['category', 'netloc', 'host']\n    \nfeatures_train = []\nfeatures_test = []\nfor feature in features_sml:\n    merged = pd.concat([train[feature], test[feature]]).to_numpy().reshape(-1, 1).squeeze()\n    encoded_size = len(max(merged, key=len))\n    print(feature,encoded_size)\n    ctr = 0\n    column = []\n    for val in train[feature].values:\n        ctr +=1\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)/128. for i in val]\n        column.append(output)\n    features_train.append(column)\n        \n    column = []\n    for val in test[feature].values:\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)/128. for i in val]\n        column.append(output)\n    features_test.append(column)\n\nfeatures_train_lrg = []\nfeatures_test_lrg = []\nfor feature in features_lrg:\n    merged = pd.concat([train[feature], test[feature]]).to_numpy().reshape(-1, 1).squeeze()\n    encoded_size = len(max(merged, key=len))\n    print(feature,encoded_size)\n    ctr = 0\n    column = []\n    for val in train[feature].values:\n        ctr +=1\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)/128. for i in val]\n        column.append(output)\n    features_train.append(column)\n        \n    column = []\n    for val in test[feature].values:\n        output = np.zeros(encoded_size)\n        output[:len(val)] = [ord(i)/128. for i in val]\n        column.append(output)\n    features_test.append(column)\n    \n    \nl2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\ncos_dist = lambda x, y: (x*y).sum(axis=1)\nabs_dist = lambda x, y: np.abs(x-y).sum(axis=1)\nsum_dist = lambda x, y: (x+y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    l2_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    l2_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    l2_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    cos_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    cos_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    abs_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    abs_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    abs_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    abs_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    abs_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    sum_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    sum_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    sum_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.answer_vec.values])),\n    sum_dist(np.array([x for x in train.question_title_vec.values]), np.array([x for x in train.answer_vec.values])),\n    sum_dist(np.array([x for x in train.question_body_vec.values]), np.array([x for x in train.question_title_vec.values])),\n    l2_dist(train_question_body_dense, train_answer_dense),\n    cos_dist(train_question_body_dense, train_answer_dense),\n    abs_dist(train_question_body_dense, train_answer_dense),\n    sum_dist(train_question_body_dense, train_answer_dense),\n    l2_dist(train_question_body_dense, train_question_title_dense),\n    cos_dist(train_question_body_dense, train_question_title_dense),\n    abs_dist(train_question_body_dense, train_question_title_dense),\n    sum_dist(train_question_body_dense, train_question_title_dense),\n    l2_dist(train_answer_dense, train_question_title_dense),\n    cos_dist(train_answer_dense, train_question_title_dense),\n    abs_dist(train_answer_dense, train_question_title_dense),\n    sum_dist(train_answer_dense, train_question_title_dense),\n]).T\n\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    l2_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    l2_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    l2_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    cos_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    cos_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    abs_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    abs_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    abs_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    abs_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    abs_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    sum_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    sum_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    sum_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.answer_vec.values])),\n    sum_dist(np.array([x for x in test.question_title_vec.values]), np.array([x for x in test.answer_vec.values])),\n    sum_dist(np.array([x for x in test.question_body_vec.values]), np.array([x for x in test.question_title_vec.values])),\n    l2_dist(test_question_body_dense, test_answer_dense),\n    cos_dist(test_question_body_dense, test_answer_dense),\n    abs_dist(test_question_body_dense, test_answer_dense),\n    sum_dist(test_question_body_dense, test_answer_dense),\n    l2_dist(test_question_body_dense, test_question_title_dense),\n    cos_dist(test_question_body_dense, test_question_title_dense),\n    abs_dist(test_question_body_dense, test_question_title_dense),\n    sum_dist(test_question_body_dense, test_question_title_dense),\n    l2_dist(test_answer_dense, test_question_title_dense),\n    cos_dist(test_answer_dense, test_question_title_dense),\n    abs_dist(test_answer_dense, test_question_title_dense),\n    sum_dist(test_answer_dense, test_question_title_dense),\n]).T\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"possible_features_train = [\n    [item for k, item in embeddings_train.items()],\n    features_train,\n    features_train_lrg,\n    [ dist_features_train ],\n    [ [x for x in train.question_body_vec.values] ],\n    [ [x for x in train.question_title_vec.values] ],\n    [ [x for x in train.answer_vec.values] ],\n    [ train_question_body_dense ],\n    [ train_answer_dense ],\n    [ tfquestion_title ],\n    [ tfquestion_body ],\n    [ tfanswer ]\n    \n]\npossible_features_test = [\n    [item for k, item in embeddings_test.items()],\n    features_test,\n    features_test_lrg,\n    [ dist_features_test ],\n    [ [x for x in test.question_body_vec.values] ],\n    [ [x for x in test.question_title_vec.values] ],\n    [ [x for x in test.answer_vec.values] ],\n    [ test_question_body_dense ],\n    [ test_answer_dense ],\n    [ tfquestion_title_test ],\n    [ tfquestion_body_test ],\n    [ tfanswer_test ]\n]\n\ndef get_train_test(split=0.8):\n    total_len = len(possible_features_train)\n    r_idx = random.sample(range(total_len), int(total_len * split))\n    \n    train = [ train_question_title_dense ]\n\n    test =  [ test_question_title_dense ]\n\n    for i in r_idx:\n        train += possible_features_train[i]\n        test += possible_features_test[i]\n        \n    return np.hstack(train),np.hstack(test)\n\nX_train,X_test = get_train_test()\ny_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.losses import *\ndef bce(t,p):\n    return binary_crossentropy(t,p)\n\ndef custom_loss(true,pred):\n    bce = binary_crossentropy(true,pred)\n    return bce + logcosh(true,pred)\n\ndef swish(x):\n    return K.sigmoid(x) * x\n\ndef relu1(x):\n    return keras.activations.relu(x, alpha=0.0, max_value=1., threshold=0.0)\n\ndef create_model1(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(64, activation='elu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(128, activation='elu',kernel_initializer='lecun_normal')(x)\n    x = Dropout(0.4)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\n\ndef create_model2(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(256, activation='elu',\n              kernel_initializer='lecun_normal', \n              #kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\n\ndef create_model3(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(200, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              )(x)\n    x = Dropout(0.4)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\ndef create_model4(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu',\n              kernel_initializer='lecun_normal', \n              )(input1)\n    x = Dropout(0.2)(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\ndef create_model5(X_train):\n    input1 = Input(shape=(X_train.shape[1],))\n    x = Dense(4096, activation='selu',kernel_initializer='lecun_normal')(input1)\n    x = Dense(512, activation='selu',\n              kernel_initializer='lecun_normal', \n              kernel_regularizer=keras.regularizers.l2(0.01)\n             )(x)\n    output = Dense(len(targets),activation='sigmoid',name='output')(x)\n    model = Model(inputs=input1, outputs=output)\n    return model\n\ndef create_model(X_train):\n    model = random.choice([create_model1, create_model2, create_model3, create_model4, create_model5])(X_train)\n    model.summary()\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import gc\nprint(gc.collect())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def pearson_metric(y_true, y_pred):\n    y_true = K.clip(y_true, K.epsilon(), 1)\n    y_pred = K.clip(y_pred, K.epsilon(), 1)\n    # reshape stage\n#     y_true = K.reshape(y_true, shape=(-1, 224 * 224 * 3))\n#     y_pred = K.reshape(y_pred, shape=(-1, 224 * 224 * 3))\n    # normalizing stage - setting a 0 mean.\n    y_true -= K.mean(y_true)\n    y_pred -= K.mean(y_pred)\n    # normalizing stage - setting a 1 variance\n    y_true = K.l2_normalize(y_true, axis=-1)\n    y_pred = K.l2_normalize(y_pred, axis=-1)\n    # final result\n    pearson_correlation = K.sum(y_true * y_pred, axis=-1)\n    return 1-pearson_correlation\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"trusted":false},"cell_type":"code","source":"# Compatible with tensorflow backend\nerror_pred_y = None\nerror_y = None\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name, reload=False):\n        global noise\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n        self.reload = reload\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        noise = np.random.normal(0, 1e-7, y_pred_val.shape[0])\n        rho_val = np.mean([spearmanr(self.y_val[:, ind] + noise, y_pred_val[:, ind] + noise).correlation for ind in range(y_pred_val.shape[1])])\n        print('\\r  val_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n\n        if rho_val >= self.value:\n            self.model.save_weights(self.model_name)\n            self.value = rho_val\n        else:\n            self.model.load_weights(self.model_name)\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        global error_pred_y, error_y        \n        y_pred_val = self.model.predict(self.x_val)\n        noise = np.random.normal(0, 1e-7, y_pred_val.shape[0])\n        rho_val = np.mean([spearmanr(self.y_val[:, ind] + noise, y_pred_val[:, ind] + noise).correlation for ind in range(y_pred_val.shape[1])])\n        if np.isnan(rho_val):\n            print(\"Error\")\n            error_pred_y = y_pred_val\n            error_y = self.y_val\n            print(y_pred_val)\n            print(self.x_val)\n            print(self.y_val)\n            raise \"bogus error\"\n#         y_pred = self.model.predict(self.x)\n#         rho = np.mean([spearmanr(self.y[:, ind], y_pred[:, ind] + np.random.normal(0, 1e-7, y_pred.shape[0])).correlation for ind in range(y_pred.shape[1])])\n        if rho_val >= self.value:\n            self.model.save_weights(self.model_name)\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n#         if self.bad_epochs >= self.patience:\n#             print(\"Epoch %05d: early stopping Threshold\" % epoch)\n#             self.model.stop_training = True\n        print('\\r  val_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n#         print('\\rtrain_spearman-rho: %s' % (str(round(rho, 4))), end=100*' '+'\\n')\n        if self.reload:\n            print('  reload best: %s' % (str(round(self.value, 4))))\n            self.model.load_weights(self.model_name)\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"all_predictions = []\nmodel_idx =0 \ndef run_model():\n    global y_train,all_predictions, model_idx\n    X_train,X_test = get_train_test()\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n                                  patience=7, min_lr=1e-6, verbose=1)\n\n    early_stop = EarlyStopping(monitor='val_loss',\n                                  min_delta=0,\n                                  patience=15,\n                                  mode='auto')\n\n    kf = KFold(n_splits=5, random_state=random.randint(0,1000), shuffle=True)\n\n    for ind, (tr, val) in enumerate(kf.split(X_train)):\n        X_tr = X_train[tr]\n        y_tr = y_train[tr]\n        X_vl = X_train[val]\n        y_vl = y_train[val]\n        model_idx+=1\n        model = create_model(X_train)\n        optimizer = Adam(lr=5e-4,clipnorm=1.4)\n        model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n        model.fit(\n            X_tr, y_tr, \n            epochs=100, batch_size=64, \n            validation_data=(X_vl, y_vl), \n            verbose=True, \n            callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), \n                                           validation_data=(X_vl, y_vl),\n                                           patience=10, \n                                           model_name=f'best_model_batch{model_idx}.h5',\n                                           reload=True),\n                       reduce_lr,\n                       early_stop\n                      ]\n        )\n        optimizer = SGD(lr=0.01,clipnorm=1.1)\n        model.compile(optimizer=optimizer, loss=custom_loss, metrics=[bce,logcosh])\n        history = model.fit(\n            X_tr, y_tr, \n            epochs=50, batch_size=64, \n            validation_data=(X_vl, y_vl), \n            verbose=True, \n            callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), \n                                           validation_data=(X_vl, y_vl),\n                                           patience=10, \n                                           model_name=f'best_model_batch_sgd{model_idx}.h5'),\n                       reduce_lr,\n                       early_stop   \n                      ]\n        )\n        if min(history.history['val_bce']) < 0.375:\n            all_predictions.append(model.predict(X_test))\n\n    optimizer = Adam(lr=2e-4,clipnorm=1.9)\n    model.compile(optimizer=optimizer, loss=bce)\n    model.fit(\n        X_train, y_train, \n        epochs=30, batch_size=128, \n        validation_data=(X_train, y_train), \n        verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_train, y_train), \n                                       validation_data=(X_train, y_train),\n                                       patience=10, \n                                       model_name=f'best_model_batchoverfit{model_idx}.h5',\n                                       reload=False),\n                   reduce_lr,\n                   early_stop\n                  ]\n    )\n    all_predictions.append(model.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"all_predictions = []\nwhile len(all_predictions) < 20:\n    run_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(all_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# This is slow...\n# from sklearn.linear_model import MultiTaskElasticNet\n# \n# kf = KFold(n_splits=5, random_state=2019, shuffle=True)\n# for ind, (tr, val) in enumerate(kf.split(X_train)):\n#     X_tr = X_train[tr]\n#     y_tr = y_train[tr]\n#     X_vl = X_train[val]\n#     y_vl = y_train[val]\n    \n#     model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n#     model.fit(X_tr, y_tr)\n#     all_predictions.append(model.predict(X_test))\n    \n# model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n# model.fit(X_train, y_train)\n# all_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"K.clear_session()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Catboost didn't work well\n# # clear session and clear cuda memory for catboost\n# K.clear_session()\n# gc.collect()\n\n# from numba import cuda;\n# cuda.select_device(0);\n# cuda.close()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"# kf = KFold(n_splits=3, random_state=42, shuffle=True)\n# from xgboost import XGBClassifier\n# from sklearn.multiclass import OneVsRestClassifier\n# from sklearn.multioutput import MultiOutputRegressor\n# # all_predictions = []\n# for ind, (tr, val) in enumerate(kf.split(X_train)):\n#     X_tr = X_train[tr]\n#     y_tr = y_train[tr]\n#     X_vl = X_train[val]\n#     y_vl = y_train[val]\n\n#     print(X_tr.shape)\n#     print(y_tr.shape)\n\n\n    \n#     gs1 = MultiOutputRegressor(\n#         cb.CatBoostRegressor(iterations=400,\n#                             learning_rate=0.7,\n#                             depth=9,\n#                             verbose = 20,\n#                             task_type = 'GPU',\n#                             loss_function = 'MAE')\n        \n#     )\n#     gs1.fit(X_tr, y_tr);    \n\n#     all_predictions.append(gs1.predict(X_test))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds/max_val + 1e-12","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.read_csv(INPUT_PATH+'google-quest-challenge/sample_submission.csv')\nsubmission[targets] = test_preds\nsubmission.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index = False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}