{"cells":[{"metadata":{},"cell_type":"markdown","source":"## If you like the kernel, consider upvoting it and the associated datasets:\n\nhttps://www.kaggle.com/abhishek/transformers\n\nhttps://www.kaggle.com/abhishek/sacremoses\n\nhttps://www.kaggle.com/abhishek/distilbertbaseuncased"},{"metadata":{},"cell_type":"markdown","source":"### Most of the code in this kernel comes directly from:\n\nhttps://www.kaggle.com/abazdyrev/use-features-oof\n\nPlease consider upvoting it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n\nimport os\nimport sys\nimport glob\nimport torch\n\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fetch_vectors(string_list, batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/google-quest-challenge/train.csv\").fillna(\"none\")\ndf_test = pd.read_csv(\"../input/google-quest-challenge/test.csv\").fillna(\"none\")\n\nsample = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\ntarget_cols = list(sample.drop(\"qa_id\", axis=1).columns)\n\ntrain_question_body_dense = fetch_vectors(df_train.question_body.values)\ntrain_answer_dense = fetch_vectors(df_train.answer.values)\n\ntest_question_body_dense = fetch_vectors(df_test.question_body.values)\ntest_answer_dense = fetch_vectors(df_test.answer.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\n\nseed(42)\ntf.random.set_seed(42)\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_dir = '../input/google-quest-challenge/'\ntrain = pd.read_csv(path_join(data_dir, 'train.csv'))\ntest = pd.read_csv(path_join(data_dir, 'test.csv'))\nprint(train.shape, test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]\n\ninput_columns = ['question_title', 'question_body', 'answer']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"find = re.compile(r\"^[^.]*\")\n\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\nfeatures = ['netloc', 'category']\nmerged = pd.concat([train[features], test[features]])\nohe = OneHotEncoder()\nohe.fit(merged)\n\nfeatures_train = ohe.transform(train[features]).toarray()\nfeatures_test = ohe.transform(test[features]).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"../input/universalsentenceencoderlarge4/\"\nembed = hub.load(module_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_train = {}\nembeddings_test = {}\nfor text in input_columns:\n    print(text)\n    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n    \n    curr_train_emb = []\n    curr_test_emb = []\n    batch_size = 4\n    ind = 0\n    while ind*batch_size < len(train_text):\n        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1\n        \n    ind = 0\n    while ind*batch_size < len(test_text):\n        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n        ind += 1    \n        \n    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \ndel embed\nK.clear_session()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n]).T\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T\n\nX_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train, dist_features_train])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test, dist_features_test])\ny_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.hstack((X_train, train_question_body_dense, train_answer_dense))\nX_test = np.hstack((X_test, test_question_body_dense, test_answer_dense))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compatible with tensorflow backend\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    inps = Input(shape=(X_train.shape[1],))\n    x = Dense(512, activation='elu')(inps)\n    x = Dropout(0.2)(x)\n    x = Dense(y_train.shape[1], activation='sigmoid')(x)\n    model = Model(inputs=inps, outputs=x)\n    model.compile(\n        optimizer=Adam(lr=1e-4),\n        loss=['binary_crossentropy']\n    )\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_predictions = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = create_model()\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=True, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    all_predictions.append(model.predict(X_test))\n    \nmodel = create_model()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))\n    \nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    \n    model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n    model.fit(X_tr, y_tr)\n    all_predictions.append(model.predict(X_test))\n    \nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\nmodel.fit(X_train, y_train)\nall_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds/max_val + 1e-12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(path_join(data_dir, 'sample_submission.csv'))\nsubmission[targets] = test_preds\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}