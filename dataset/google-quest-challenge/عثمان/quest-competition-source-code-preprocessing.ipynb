{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re, html\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source code had a large coverage in the SO data. In the write-ups I've seen so far, most people did not first further train (masked language model) their Bert models on the StackExchange data prior to finetuning their classifiers, which would mean that the models wouldn't really have a clear understanding of what's happening in the source. I didn't have the time or compute to do a SX LM either. Sometimes there would be blocks of source code that 10, 20, even 40 lines long. I felt that this would cause overfitting. Even more so in cases like RoBERTa that include whitespace in the tokenizers. Due to this, early on I invested some time in creating some parsers that would identify source code."},{"metadata":{},"cell_type":"markdown","source":"Once source is identified, you have some options. For example, if you have a link, do you just want to replace it with URL? Would you rather replace it with a special token, e.g. [URL]? Or would you rather do further regex parsing, such as convert it to the domain name."},{"metadata":{},"cell_type":"markdown","source":"Some code is easy to identify such as php blocks, or javascript embedded into HTML. Other code is very difficult to identify because there are no opening and closing tags and people just start typing code right into their sentences. Do handle this, I split the text into lines and then run a bunch of regex statements on each line to see if the line should be 'classified' as code or not. If there are multiple concurrent source code lines then we do something. In the example below, any additional lines after a threshold number of lines is truncated, as an example to curve overfitting. This could be extended by, e.g. replacing the entire code block with a special token."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# allow first line not to be counted as source(?)\nmultiline = re.compile('\\n{2,}')\n\ncode_html = re.compile(\n    r'(\\wclass\\s*=)|(\\wid\\s*=)|(\\whref\\s*=)|(^\\s*<[a-z]+\\w.*[a-z]>\\s*$)|(</a>)|(</ul>)|(</li>)|(<hmtl)|(</div>)|(</span>)|(<[a-z]+.*[a-z]+\\s*=.*>)', re.IGNORECASE\n)\ncode_python = re.compile(\n    r'(\"\\s*,\\s*\")|(\\'\\s*,\\s*\\')|((\\'|\")\\s*:)|([\\'\"][\\])])|((\\[|\\()(\\'|\"))|(^\\s*[{}\\(\\)\\[\\]]\\s*$)|(=\\s*\")|(=\\s*\\')|(=\\s*\\()|(=\\s*{)|(=\\s*\\[)|(^\\s*return\\b)|(\\[\\s*[0-9xyzijk\\-]+\\s*\\])|([a-z_0-9]+\\.[a-z_0-9]+\\()|(^\\s*else\\s*:)|(=\\s*[{\\[(])|(^\\s*[a-z_0-9]+\\s*\\.[a-z_0-9]+\\s*\\()|(^\\s*[a-z_0-9]+\\s*\\.[a-z_0-9]+\\s*[=+/*])|(^\\s*#+)|(^\\s*def\\s*[a-z_0-9]+)|(\\welif\\w)|(\\s*if.*:\\s*$)|(^\\s*class [a-z_0-9\\(\\)]:\\s*$)|(for\\s+.*\\s+in\\s+.*:\\s*$)|(^\\s*[a-z_0-9]+\\(.*\\)\\s*$)|(^\\s*def\\s*[a-z_0-9]*\\s*\\()|([a-z_0-9]+\\.[a-z_0-9]+\\s*[=+/\\-])', re.IGNORECASE\n)\ncode_php = re.compile(\n    # We get 99% of c/c++/php for free by catching ending on \";\"\n    r'(\\(\\s*[\\'\"])|([\\'\"]\\s*\\))|({\\s*$)|(\\)\\s*{)|(;\\s*$)|(=>)|(else\\s*[\\({])|(\\$[a-z]+)|([a-z0-9_]{3,}\\()|(^\\s*(//|/\\*))|(\\*/$)', re.IGNORECASE\n)\ncode_java = re.compile(\n    r'(^\\s*(var|const)\\b)|(optional<|list<|set<|map<|queue<)|(\\(\\s*{|}\\s*\\))|(^\\s*\\@[a-z])', re.IGNORECASE\n)\n\ndef detsrc(line):\n    strip_len = len(line.strip())\n\n    if strip_len==0: return True    \n    if code_html.search(line): return True\n    \n    if strip_len > 150: return False\n    if code_python.search(line): return True\n    if code_php.search(line): return True\n    if code_java.search(line): return True\n    return False\n\n# @numba.jit\ndef strip_code(text):\n    lowertext = text.lower()\n \n    # Strip HTML first, because it can encapsulate JS and PHP\n    while True:\n        pos1 = lowertext.find('<html')\n        if pos1 == -1: break\n            \n        pos2 = lowertext.find('/html>', pos1+5)\n        if pos2 == -1: break\n        \n        # Add the token\n        codelen = pos2-pos1-5\n        tag = f' sample code  '\n        text = text[:pos1] + tag + text[pos2+6:]\n        lowertext = text.lower()\n        \n    # Strip PHP next cause it's easy\n    while True:\n        pos1 = lowertext.find('<?php')\n        if pos1 == -1: break\n            \n        pos2 = lowertext.find('?>', pos1+5)\n        if pos2 == -1: break\n        \n        # Add the token\n        codelen = pos2-pos1-5\n        tag = f' sample code '\n        text = text[:pos1] + tag + text[pos2+2:]\n        lowertext = text.lower()\n        \n    # Strip Script next cause it's easy\n    while True:\n        pos1 = lowertext.find('<script')\n        if pos1 == -1: break\n            \n        pos2 = lowertext.find('</script>', pos1+7)\n        if pos2 == -1: break\n        \n        # Add the token\n        codelen = pos2-pos1-7\n        tag = f' sample code '\n        text = text[:pos1] + tag + text[pos2+9:]\n        lowertext = text.lower()\n        \n    # Strip links\n    while True:\n        pos1 = lowertext.find('<a ')\n        if pos1 == -1: break\n            \n        pos2 = lowertext.find('</a>', pos1+3)\n        if pos2 == -1: break\n        \n        # TODO: Add the domain\n        tag = f' url '\n        text = text[:pos1] + tag + text[pos2+4:]\n        lowertext = text.lower()\n        \n    # Strip images\n    while True:\n        pos1 = lowertext.find('<img')\n        if pos1 == -1: break\n            \n        pos2 = lowertext.find('>', pos1+4)\n        if pos2 == -1: break\n        \n        # TODO: Add the alt if present, otherwise the domain\n        tag = f' image '\n        text = text[:pos1] + tag + text[pos2+1:]\n        lowertext = text.lower()\n\n    # Detect and strip specific languages\n    # Replace multi-whitespace with single whitespace using multiline\n    # We do this hear rather than above for various reasons:\n    codelines = []\n    lines = multiline.sub('\\n', text).split('\\n')\n    \n    stretch = 0\n    for line in lines:\n        if detsrc(line):\n            stretch += 1\n        else:\n            stretch = 0\n        codelines.append(stretch)\n\n    # TODO: IMPORTANT\n    # IF WE FILTER ALL LINES, THEN WE HAVE A PROBLEM.\n    # WE SHOULD HAVE AT LEAST 1 NON-CODE LINE, OTHERWISE, DROP FILTERING\n    \n    return '\\n'.join([\n        line\n        for idx_line, line in enumerate(lines)\n        if codelines[idx_line]<5\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ntest  = pd.read_csv(\"../input/google-quest-challenge/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.question_body = train.question_body.apply(html.unescape).apply(strip_code)\ntrain.answer        = train.answer.apply(html.unescape).apply(strip_code)\ntest.question_body  = test.question_body.apply(html.unescape).apply(strip_code)\ntest.answer         = test.answer.apply(html.unescape).apply(strip_code)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above code isn't optimized at all but runs fast enough on the competition data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['question_body','answer']].to_csv('train_clean.csv')\ntest[['question_body','answer']].to_csv('test_clean.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}