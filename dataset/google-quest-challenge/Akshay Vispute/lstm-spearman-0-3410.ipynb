{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing essentials\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading dataset\ndf_train = pd.read_csv('../input/google-quest-challenge/train.csv')\ndf_test = pd.read_csv('../input/google-quest-challenge/test.csv')\ndf_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# download pretrained glove vectors : https://nlp.stanford.edu/projects/glove/\n!wget --header=\"Host: downloads.cs.stanford.edu\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" \"http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\" -c -O 'glove.6B.zip'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir './glove/'\n!mv './glove.6B.zip' './glove/'\n!unzip './glove/glove.6B.zip' -d './glove/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining a function to remove stop_words\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nstop_words.remove('no'); stop_words.remove('not'); stop_words.remove('nor')\n\ndef stopwrd_removal(sent):\n  lst = []\n  for wrd in sent.split():\n    if wrd not in stop_words:\n      lst.append(wrd)\n  return \" \".join(lst)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessor(column, remove_stopwords = False, remove_specialchar = False):\n  \"\"\"pass any column with Text in it from df_train | Note: returns nothing makes inplace changes in df_train\"\"\"\n  # 1. remove html tags, html urls, replace html comparison operators\n  # text = df_train[column].values\n  df_train[column] = [re.sub('<.*?>', ' ', i) for i in df_train[column].values]\n  df_train[column] = df_train[column].str.replace('&lt;', '<')\\\n                                          .str.replace('&gt;', '>')\\\n                                          .str.replace('&le;', '<=' )\\\n                                          .str.replace('&ge;', '>=')\n\n  # 2. remove latex i,e., if there is any formulas or latex we have to remove it\n  df_train[column] = [re.sub('\\$.*?\\$', ' ', i) for i in df_train[column].values]\n\n  # 3. all lowercase \n  df_train[column] = df_train[column].str.lower()\n\n  # 4. decontractions\n  df_train[column] = df_train[column].str.replace(\"won't\", \"will not\").str.replace(\"can\\'t\", \"can not\").str.replace(\"n\\'t\", \" not\").str.replace(\"\\'re\", \" are\").str.\\\n                                                replace(\"\\'s\", \" is\").str.replace(\"\\'d\", \" would\").str.replace(\"\\'ll\", \" will\").str.\\\n                                                replace(\"\\'t\", \" not\").str.replace(\"\\'ve\", \" have\").str.replace(\"\\'m\", \" am\")\n  \n  # 5. removing non-english or hebrew characters\n  df_train[column] = [i.encode(\"ascii\", \"ignore\").decode() for i in df_train[column].values]\n\n  # 6. remove all special-characters other than alpha-numericals\n  if remove_specialchar == True:\n    df_train[column] = [re.sub('[^A-Za-z0-9]+', ' ', i) for i in df_train[column].values]\n\n  # 7. separating special chars from alphanumerics\n  all_sc = [re.findall('[^ A-Za-z0-9]', i) for i in df_train[column].values]\n  special_char = np.unique([j for i in all_sc for j in i])\n  replace_char = [' '+i+' ' for i in special_char]\n  for i,j in zip(special_char, replace_char):\n   df_train[column] = df_train[column].str.replace(i, j)\n\n  # 8. Stop_word removal\n  if remove_stopwords == True:\n    df_train[column] = [stopwrd_removal(i) for i in df_train[column].values]\n\n  # 9. remove all white-space i.e., \\n, \\t, and extra_spaces\n  df_train[column] = df_train[column].str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.rstrip()\n  df_train[column] = [re.sub('  +', ' ', i) for i in df_train[column].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['clean_title'] = df_train['question_title']\ndf_train['clean_body'] = df_train['question_body']\ndf_train['clean_answer'] = df_train['answer']\ntext_preprocessor('clean_title',  remove_stopwords = False, remove_specialchar = False)\ntext_preprocessor('clean_body',  remove_stopwords = False, remove_specialchar = False)\ntext_preprocessor('clean_answer',  remove_stopwords = False, remove_specialchar = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. setting up target features\nquestion_tar = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n       \nanswer_tar = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntar_features = question_tar + answer_tar\nlen(tar_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. splitting dataset train_test_split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_cv, y_train, y_cv = train_test_split(df_train[['clean_title', 'clean_body', 'clean_answer']], df_train[tar_features], test_size = 0.12, random_state = 42)\nX_train.shape, X_cv.shape, y_train.shape, y_cv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. creating training features : title + body = title_body | answer_train | title + body + answer = title_body_answer\ntitle_train = X_train['clean_title'].values\nbody_train = X_train['clean_body'].values\nanswer_train = X_train['clean_answer'].values\n\ntitle_cv = X_cv['clean_title'].values\nbody_cv = X_cv['clean_body'].values\nanswer_cv = X_cv['clean_answer'].values\n\n# train data\ntitle_body_train = [i+' '+j for i,j in zip(title_train, body_train)]\n\n# cv data\ntitle_body_cv = [i+' '+j for i,j in zip(title_cv, body_cv)]\n\nlen(title_body_train), len(answer_train), len(title_body_cv), len(answer_cv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.1. Basic Modelling : converting i/o to machine readable inputs and outputs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nfrom tensorflow.keras.layers import Input, Softmax, GRU, LSTM, Conv1D, Embedding, Dense, RepeatVector, TimeDistributed, Bidirectional, Dropout, Concatenate\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. tokenizing\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nmax_words = 20000\n\n# M1 : title_body\ntitle_body_token = Tokenizer(num_words = max_words, filters = '')\ntitle_body_token.fit_on_texts(title_body_train)\ntitle_body_seq_train = title_body_token.texts_to_sequences(title_body_train)\ntitle_body_seq_cv = title_body_token.texts_to_sequences(title_body_cv)\n# 3. bulding vocab\ntitle_body_vocab = title_body_token.word_index\n\n# M2 : answer\nanswer_token = Tokenizer(num_words = max_words, filters = '')\nanswer_token.fit_on_texts(answer_train)\nanswer_seq_train = answer_token.texts_to_sequences(answer_train)\nanswer_seq_cv = answer_token.texts_to_sequences(answer_cv)\n# 3. bulding vocab\nanswer_vocab = answer_token.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. building vocab\nprint('Total no.of words in title_body vocab =', len(title_body_vocab))\nprint('Total no.of words in answer vocab =', len(answer_vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.1. padding : max lengths\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntitle_body_max_len = max([len(i) for  i in title_body_seq_train])\nanswer_max_len = max([len(i) for  i in answer_seq_train])\nprint('MAX seq_len in title_body sentences = {}\\nMAX seq_len in answer sequences = {}'.format(title_body_max_len, answer_max_len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.2. padding : setting up sequence max_len threshold using percentile method (for padding the seq)\nlen_lst_1 = [len(i) for  i in title_body_seq_train]\nfor i in np.arange(80, 101, 1):\n  print('percentile = {} | seq_len = {} | no.of datapts NOT covered = {}'.format(round(i, 2), round(np.percentile(len_lst_1, i)), round(len(len_lst_1) - (i*len(len_lst_1)*0.01))))\n\nprint('\\n')\nlen_lst_2 = [len(i) for  i in answer_seq_train]\nfor i in np.arange(80, 101, 1):\n  print('percentile = {} | seq_len = {} | no.of datapts NOT covered = {}'.format(round(i, 2), round(np.percentile(len_lst_2, i)), round(len(len_lst_2) - (i*len(len_lst_2)*0.01))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.3. padding : setting up sequence max_len threshold using elbow_method\nlst_1 = []\nlst_2 = []\nlst_i = []\nfor i in np.arange(60, 100, 1):\n  lst_i.append(round(i, 2))\n  lst_1.append(round(np.percentile(len_lst_1, i)))\n  lst_2.append(round(np.percentile(len_lst_2, i)))\n\nplt.figure(figsize= (24, 5.5))\nplt.subplot(1,2,1)\nplt.plot(lst_1)\nplt.grid()\nplt.title('title_body len vs percentiles')\nplt.xlabel('percentiles')\nplt.ylabel('title_body seq length')\nplt.xticks(ticks = range(0, 40), labels = lst_i)\n\nplt.subplot(1,2,2)\nplt.plot(lst_2)\nplt.grid()\nplt.title('answer len vs percentiles')\nplt.xlabel('percentiles')\nplt.ylabel('answer sequence length')\nplt.xticks(ticks = range(0, 40), labels = lst_i)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4.4. padding : padding the train and test sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntitle_body_seq_train = pad_sequences(title_body_seq_train, maxlen = 300, padding = 'post', truncating='post')\ntitle_body_seq_cv = pad_sequences(title_body_seq_cv, maxlen = 300, padding = 'post', truncating='post')\n\nanswer_seq_train = pad_sequences(answer_seq_train, maxlen = 300, padding = 'post', truncating='post')\nanswer_seq_cv = pad_sequences(answer_seq_cv, maxlen = 300, padding = 'post', truncating='post')\n\ntitle_body_seq_train.shape, title_body_seq_cv.shape, answer_seq_train.shape, answer_seq_cv.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. initializing Embedding Matrix using glove_dict\n# 5.1 glove_dict\nfile = open('./glove/glove.6B.100d.txt', encoding='utf8')\nlst = []\nfor i in file:\n    word, vec = i.split(maxsplit=1)\n    vec = np.fromstring(vec, 'f', sep = ' ')\n    lst.append(tuple([word, vec]))\nglove_dict = dict(lst)\nfile.close()\nprint('Total no. of words :', len(glove_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5.2 Embedding Matrix : question\nmax_words = 20000\nembedding_matrix_text_body = np.random.normal(loc = 0, scale = 0.3, size = (max_words+1, 100))\nfor word, i in title_body_vocab.items():\n    vector = glove_dict.get(word)\n    if vector is not None and i <= max_words:\n        embedding_matrix_text_body[i] = glove_dict[word]\nprint(embedding_matrix_text_body.shape)\n\n# 5. Embedding Matrix : answer\nembedding_matrix_answer = np.random.normal(loc = 0, scale = 0.3, size = (max_words+1, 100))\nfor word, i in answer_vocab.items():\n    vector = glove_dict.get(word)\n    if vector is not None and i <= max_words:\n        embedding_matrix_answer[i] = glove_dict[word]\nprint(embedding_matrix_answer.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.2. Modelling : Constructing a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. Constructing a model\ntf.keras.backend.clear_session()\ndef get_model(embedding_matrix_text_body, embedding_matrix_answer):\n  # Creating 'Embedding layer'\n  seed = 42\n  title_body_embedding_layer = Embedding(input_dim = max_words+1, output_dim= 100, weights = [embedding_matrix_text_body],\n                                        mask_zero = True, trainable = True, name = 'title_body_embed')\n\n  answer_embedding_layer = Embedding(input_dim = max_words+1, output_dim= 100, weights = [embedding_matrix_answer],\n                                    mask_zero = True, trainable = True, name = 'answer_embed') \n  # Title_body model\n  tb_inputs = Input(name = 'title_body_seq', shape = (300,))\n  tb_embed = title_body_embedding_layer(tb_inputs)\n  tb_lstm, tb_hidden, tb_cell, tb_hidden_back, tb_cell_back = Bidirectional(LSTM(name = 'TB_ENCODER', units = 128, dropout = 0.25, return_sequences = True, return_state=True))(tb_embed)\n\n  tb_dense_1 = Dense(units = 128, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal(seed = seed))(tf.concat([tb_hidden, tb_hidden_back], axis = -1))\n  tb_dropout_1 = Dropout(rate = 0.2, seed = seed)(tb_dense_1)\n  tb_out = Dense(units = 21, activation = 'sigmoid', kernel_initializer = tf.keras.initializers.GlorotNormal(seed = seed))(tb_dropout_1)\n\n  # answer model\n  ans_inputs = Input(name = 'ans_seq', shape = (300,))\n  ans_embed = answer_embedding_layer(ans_inputs)\n  ans_lstm, ans_hidden, ans_cell, ans_hidden_back, ans_cell_back = Bidirectional(LSTM(name = 'TB_ENCODER', units = 128, dropout = 0.2, return_sequences = True, return_state=True))(ans_embed)\n  ans_dense_1 = Dense(units = 64, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal(seed = seed))(tf.concat([ans_hidden, ans_hidden_back], axis = -1))\n  ans_dropout_1 = Dropout(rate = 0.2, seed = seed)(ans_dense_1)\n  ans_out = Dense(units = 9, activation = 'sigmoid', kernel_initializer = tf.keras.initializers.GlorotNormal(seed = seed))(ans_dropout_1)\n\n  concat_1 = Concatenate(axis = -1)([tb_out, ans_out])\n\n  model = Model(inputs = [tb_inputs, ans_inputs] , outputs = concat_1)\n  return model\nmodel = get_model(embedding_matrix_text_body, embedding_matrix_answer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# post processing : binning\ndef return_bins(arr):\n  val = np.unique(arr)\n  bins = []\n  for i in range(len(val)):\n    if i > 0:\n      bins.append((val[i-1] + val[i])/2)\n  return bins\n  \nunique_val_30 = [np.unique(df_train[tar_features].values[:, i]) for i in range(30)]\nbins_30 = [return_bins(df_train[tar_features].values[:, i]) for i in range(30)]\n\ndef binned_out(y_pred):\n  col = y_pred.shape[1]\n  final_pred = np.zeros(y_pred.shape)\n  for i in range(col):\n    idx = np.digitize(y_pred[:, i], bins_30[i])\n    final_pred[:, i] = unique_val_30[i][idx]\n  return final_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining callbacks\n# !rm -r './saved model/'\n# !rm -r './logs/'\n!mkdir './saved model/'\n!mkdir './logs/'\n\n# tensorboard callback\nimport datetime\nlog_dir=\"./logs/\" + datetime.datetime.now().strftime(\"%Y-%m-%d %H_%M_%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n\n# spearman function\nfrom scipy.stats import pearsonr, spearmanr\ndef compute_spearman(y_true, y_pred, final_pred):\n  col = y_true.shape[1]\n  lst = []\n  for i in range(col):\n    # p = round(spearmanr(y_true[:, i], y_pred[:, i])[0], 5)\n    p = round(spearmanr(y_true[:, i], final_pred[:, i])[0], 5)\n    p = round(p, 5)\n    if np.isnan(p):\n      p = round(spearmanr(y_true[:, i], y_pred[:, i])[0], 5)\n    lst.append(p)\n  return np.array(lst), round(sum(lst)/len(lst), 5)\n\n# Custom spearman metric\nclass print_spearman(tf.keras.callbacks.Callback):\n    def __init__(self, train_data, validation_data):\n        super(tf.keras.callbacks.Callback, self).__init__()\n        self.x, self.y = train_data\n        self.val_x, self.val_y = validation_data\n    \n    def on_train_begin(self, logs={}):\n        self.all_feat_spearman = []\n        self.spearman_dict = {'train_spearman' :[], 'val_spearman' :[]}\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.epoch = epoch\n        # 1. Test_set evaluation\n        print('\\nspearman :')\n        y_pred = self.model.predict(x = self.x)\n        y_pred_val = self.model.predict(x = self.val_x)\n\n        final_pred = binned_out(y_pred)\n        final_pred_val = binned_out(y_pred_val)\n\n        train_spear_lst, train_spearman = compute_spearman(self.y, y_pred, final_pred)\n        val_spear_lst, val_spearman = compute_spearman(self.val_y, y_pred_val, final_pred_val)\n\n        self.all_feat_spearman.append({'train_spearman' : train_spear_lst, 'val_spearman' : val_spear_lst})\n\n        self.spearman_dict['train_spearman'].append(train_spearman)\n        self.spearman_dict['val_spearman'].append(val_spearman)\n        prev_epoch_lr  = tf.keras.backend.eval(self.model.optimizer.lr)\n        print(\"train_spearman : {} | val_spearman : {} | Learning_Rate : {}\".format(train_spearman, val_spearman, round(prev_epoch_lr, 6)))\n        print('train_spear_lst : ', train_spear_lst, '\\n' 'val_spear_lst :', val_spear_lst)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor= 'val_loss', factor=np.sqrt(0.1), patience=4, verbose=1)\n\ncheckpt = tf.keras.callbacks.ModelCheckpoint('./saved model/{epoch:1d}', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=True)\n\nprint_spearman_fn = print_spearman(train_data = ([title_body_seq_train, answer_seq_train], y_train.values),\n                                 validation_data = ([title_body_seq_cv, answer_seq_cv], y_cv.values))\ncallbacks = [print_spearman_fn, reduce_lr, checkpt, tensorboard_callback]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM : training a model\ntf.keras.backend.clear_session()\nopt = tf.keras.optimizers.Adam(learning_rate = 0.001)\nrmse = tf.keras.metrics.RootMeanSquaredError()\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = opt,  metrics = [rmse])\nhistory = model.fit(x = [title_body_seq_train, answer_seq_train], y =  y_train.values,\n                    validation_data = ([title_body_seq_cv, answer_seq_cv], y_cv.values),\n                    batch_size = 32, epochs = 20, callbacks = callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting model graphs\nplt.figure(figsize= (24, 5.5))\nplt.subplot(1,3,1)\nplt.plot(history.history['loss'], label = 'training loss')\nplt.plot(history.history['val_loss'], label = 'validation loss')\nplt.title('epochs vs loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.xticks(range(0, 20), range(1, 21))\nplt.legend()\nplt.grid()\n\nplt.subplot(1,3,2)\nplt.plot(history.history['root_mean_squared_error'], label = 'training rmse')\nplt.plot(history.history['val_root_mean_squared_error'], label = 'validation rmse')\nplt.title('epochs vs rmse')\nplt.xlabel('epochs')\nplt.ylabel('rmse')\nplt.xticks(range(0, 20), range(1, 21))\nplt.legend()\nplt.grid()\n\nplt.subplot(1,3,3)\nplt.plot(print_spearman_fn.spearman_dict['train_spearman'], label = 'training spearman')\nplt.plot(print_spearman_fn.spearman_dict['val_spearman'], label = 'validation spearman')\nplt.title('epochs vs spearman')\nplt.xlabel('epochs')\nplt.ylabel('spearman')\nplt.xticks(range(0, 20), range(1, 21))\nplt.legend()\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessor(column, df, remove_stopwords = False, remove_specialchar = False):\n  \"\"\"pass any column with Text in it from df | Note: returns nothing makes inplace changes in df\"\"\"\n  # 1. remove html tags, html urls, replace html comparison operators\n  # text = df[column].values\n  df[column] = [re.sub('<.*?>', ' ', i) for i in df[column].values]\n  df[column] = df[column].str.replace('&lt;', '<')\\\n                                          .str.replace('&gt;', '>')\\\n                                          .str.replace('&le;', '<=' )\\\n                                          .str.replace('&ge;', '>=')\n\n  # 2. remove latex i,e., if there is any formulas or latex we have to remove it\n  df[column] = [re.sub('\\$.*?\\$', ' ', i) for i in df[column].values]\n\n  # 3. all lowercase \n  df[column] = df[column].str.lower()\n\n  # 4. decontractions\n  df[column] = df[column].str.replace(\"won't\", \"will not\").str.replace(\"can\\'t\", \"can not\").str.replace(\"n\\'t\", \" not\").str.replace(\"\\'re\", \" are\").str.\\\n                                                replace(\"\\'s\", \" is\").str.replace(\"\\'d\", \" would\").str.replace(\"\\'ll\", \" will\").str.\\\n                                                replace(\"\\'t\", \" not\").str.replace(\"\\'ve\", \" have\").str.replace(\"\\'m\", \" am\")\n  \n  # 5. removing non-english or hebrew characters\n  df[column] = [i.encode(\"ascii\", \"ignore\").decode() for i in df[column].values]\n\n  # 6. remove all special-characters other than alpha-numericals\n  if remove_specialchar == True:\n    df[column] = [re.sub('[^A-Za-z0-9]+', ' ', i) for i in df[column].values]\n\n  # 7. separating special chars from alphanumerics\n  all_sc = [re.findall('[^ A-Za-z0-9]', i) for i in df[column].values]\n  special_char = np.unique([j for i in all_sc for j in i])\n  replace_char = [' '+i+' ' for i in special_char]\n  for i,j in zip(special_char, replace_char):\n   df[column] = df[column].str.replace(i, j)\n\n  # 8. Stop_word removal\n  if remove_stopwords == True:\n    df[column] = [stopwrd_removal(i) for i in df[column].values]\n\n  # 9. remove all white-space i.e., \\n, \\t, and extra_spaces\n  df[column] = df[column].str.replace(\"\\n\", \" \").str.replace(\"\\t\", \" \").str.rstrip()\n  df[column] = [re.sub('  +', ' ', i) for i in df[column].values]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. text preprocessing\ndf_test['clean_title'] = df_test['question_title']\ndf_test['clean_body'] = df_test['question_body']\ndf_test['clean_answer'] = df_test['answer']\ntext_preprocessor('clean_title',df = df_test,  remove_stopwords = False, remove_specialchar = False)\ntext_preprocessor('clean_body',df = df_test,  remove_stopwords = False, remove_specialchar = False)\ntext_preprocessor('clean_answer',df = df_test,  remove_stopwords = False, remove_specialchar = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. preparing input data\ntitle_test = df_test['clean_title'].values\nbody_test = df_test['clean_body'].values\nanswer_test = df_test['clean_answer'].values\n\ntitle_body_test = [i+' '+j for i,j in zip(title_test, body_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. tokenizing\ntitle_body_seq_test = title_body_token.texts_to_sequences(title_body_test) # title + body\nanswer_seq_test = answer_token.texts_to_sequences(answer_test) # answer\n\n# 4. padding\ntitle_body_seq_test = pad_sequences(title_body_seq_test, maxlen = 300, padding = 'post', truncating='post')\nanswer_seq_test = pad_sequences(answer_seq_test, maxlen = 300, padding = 'post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. loading best model weights \nbest_epoch = np.argmax(print_spearman_fn.spearman_dict['val_spearman'])+1\nmodel.load_weights('./saved model/'+ str(best_epoch))\n\n# 6. predicting unseen test set \ny_pred_test = model.predict([title_body_seq_test, answer_seq_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7. post_processing : binning\nfinal_pred = binned_out(y_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8. submission file \npred_csv =  pd.DataFrame(final_pred, columns = tar_features)\nid_df = pd.DataFrame(df_test['qa_id'].values, columns =['qa_id'])\nsubmission_csv = pd.concat([id_df, pred_csv], axis=1)\nsubmission_csv.to_csv('submission.csv', index=False)\nsubmission_csv.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}