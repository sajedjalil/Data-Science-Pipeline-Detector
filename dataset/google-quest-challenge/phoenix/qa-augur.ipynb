{"cells":[{"metadata":{},"cell_type":"markdown","source":"##### credits,  \nhttps://www.kaggle.com/mobassir, \nhttps://www.kaggle.com/abhishek, \nhttps://www.kaggle.com/ldm314,   \nhttps://www.kaggle.com/christofhenkel/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#most of the credits goes to: https://www.kaggle.com/mobassir\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gc\nimport os\nimport warnings\nimport operator\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom tqdm.notebook import tqdm\nimport transformers\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom nltk import ngrams\nfrom collections import Counter\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport nltk\nfrom gensim import corpora, models\nimport pyLDAvis#for interactive topic model visualization\nimport pyLDAvis.gensim\nfrom keras.preprocessing.text import Tokenizer\n\npyLDAvis.enable_notebook()\nnp.random.seed(2018)\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GroupKFold\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom tensorflow.keras.models import load_model\nimport math\n\nimport re\n\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\nimport glob\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda, Flatten\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import MultiTaskElasticNet\nimport torch\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_columns = ['question_title', 'question_body', 'answer']\ntargets = [\n        'question_asker_intent_understanding',\n        'question_body_critical',\n        'question_conversational',\n        'question_expect_short_answer',\n        'question_fact_seeking',\n        'question_has_commonly_accepted_answer',\n        'question_interestingness_others',\n        'question_interestingness_self',\n        'question_multi_intent',\n        'question_not_really_a_question',\n        'question_opinion_seeking',\n        'question_type_choice',\n        'question_type_compare',\n        'question_type_consequence',\n        'question_type_definition',\n        'question_type_entity',\n        'question_type_instructions',\n        'question_type_procedure',\n        'question_type_reason_explanation',\n        'question_type_spelling',\n        'question_well_written',\n        'answer_helpful',\n        'answer_level_of_information',\n        'answer_plausible',\n        'answer_relevance',\n        'answer_satisfaction',\n        'answer_type_instructions',\n        'answer_type_procedure',\n        'answer_type_reason_explanation',\n        'answer_well_written'    \n    ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv',index_col='qa_id')\ntest = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv',index_col='qa_id')\nsubmission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\ntrain.shape,test.shape,submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a=train.groupby(by=['category'])\n# train=pd.concat([train.iloc[a.indices['STACKOVERFLOW']],train.iloc[a.indices['TECHNOLOGY']],train.iloc[a.indices['SCIENCE']],train.iloc[a.indices['LIFE_ARTS']],train.iloc[a.indices['CULTURE']]])\n# b=test.groupby(by=['category'])\n# test=pd.concat([test.iloc[b.indices['STACKOVERFLOW']],test.iloc[b.indices['TECHNOLOGY']],test.iloc[b.indices['SCIENCE']],test.iloc[b.indices['LIFE_ARTS']],test.iloc[b.indices['CULTURE']]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for colname in tqdm(input_columns):\n#     preprocess(colname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# train, _ = train_test_split(train, test_size=0.3,random_state=42,shuffle=True)\n# train.shape,test.shape,submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##checking the distributions of targets(all 30)\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n# for col in targets:\n#     plt.hist(train[col])\n#     plt.title(col)\n#     plt.show()\n#plt.hist(train['question_body_critical'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling - stage1"},{"metadata":{},"cell_type":"markdown","source":"## 1.Distil BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# import torch\n# import sys\n# def fetch_vectors(string_list, batch_size=64):\n#     # credits: https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n#     DEVICE = torch.device(\"cuda\")\n#     tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"../input/distilbertbaseuncased/\")\n#     model = transformers.DistilBertModel.from_pretrained(\"../input/distilbertbaseuncased/\")\n#     model.to(DEVICE)\n#     fin_features = []\n#     for data in chunks(string_list, batch_size):\n#         tokenized = []\n#         for x in data:\n#             x = \" \".join(x.strip().split()[:300])\n#             tok = tokenizer.encode(x, add_special_tokens=True)\n#             tokenized.append(tok[:512])\n\n#         max_len = 512\n#         padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n#         attention_mask = np.where(padded != 0, 1, 0)\n#         input_ids = torch.tensor(padded).to(DEVICE)\n#         attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n#         with torch.no_grad():\n#             last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n#         features = last_hidden_states[0][:, 0, :].cpu().numpy()\n#         fin_features.append(features)\n\n#     fin_features = np.vstack(fin_features)\n#     return fin_features\n\n# def chunks(l, n):\n#     \"\"\"Yield successive n-sized chunks from l.\"\"\"\n#     for i in range(0, len(l), n):\n#         yield l[i:i + n]\n# sys.path.insert(0, \"../input/transformers/transformers-master/\")\n# train_question_body_dense = fetch_vectors(train.question_body.values)\n# train_answer_dense = fetch_vectors(train.answer.values)\n# test_question_body_dense = fetch_vectors(test.question_body.values)\n# test_answer_dense = fetch_vectors(test.answer.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.BERT uncased(tuned on GCP TPUs),  \ncredits:https://www.kaggle.com/abhishek  for demonstrating it on youtube"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport torch\nimport sys\n\ndef fetch_vectors(string_list, batch_size=64):\n    # credits: https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    DEVICE = torch.device(\"cuda\")\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bertbaseuncased/bert-base-uncased/\")\n    model = transformers.BertModel.from_pretrained(\"../input/bertbaseuncased/bert-base-uncased/\")\n    model.to(DEVICE)\n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n\n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\nsys.path.insert(0, \"../input/transformers/transformers-master/\")\ntrain_question_body_dense = fetch_vectors(train.question_body.values)\ntrain_answer_dense = fetch_vectors(train.answer.values)\ntest_question_body_dense = fetch_vectors(test.question_body.values)\ntest_answer_dense = fetch_vectors(test.answer.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.Manual features"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport re\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfind = re.compile(r\"^[^.]*\")\n\ntrain['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\nfeatures = ['netloc', 'category']\nmerged = pd.concat([train[features], test[features]])\nohe = OneHotEncoder()\nohe.fit(merged)\n\nfeatures_train = ohe.transform(train[features]).toarray()\nfeatures_test = ohe.transform(test[features]).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. USE-QA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv',index_col='qa_id')\n# test = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv',index_col='qa_id')\n# submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n# train.shape,test.shape,submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport sys\nsys.path.insert(0, \"/kaggle/input/tftext/tensorflow_text/\")\nembed = hub.load(\"/kaggle/input/useqa3/USEQA3/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembeddings_train = {}#sentence_embeddings\nembeddings_test = {}#sentence_embeddings\nprint(\"preparing embeddings for train data....\")\ntrain['question_title']=train['question_title'].apply(lambda x:x.strip('\\n'))\ntrain['question_body']=train['question_body'].apply(lambda x:x.strip('\\n'))\ntrain['answer']=train['answer'].apply(lambda x:x.strip('\\n'))\ntrain_ans_emb = []\ntrain_questitle_emb = []\ntrain_quesbody_emb = []\nfor q_t,q_b,a in tqdm(zip(list(train['question_title']),list(train['question_body']),list(train['answer']))):\n    question_title=[q_t]\n    question_body=[q_b]\n    responses=[a]\n    response_contexts = responses\n    question_title_embeddings = embed.signatures['question_encoder'](tf.constant(question_title))['outputs']\n    question_body_embeddings = embed.signatures['question_encoder'](tf.constant(question_body))['outputs']\n    response_embeddings = embed.signatures['response_encoder'](input=tf.constant(responses),context=tf.constant(response_contexts))['outputs']\n    train_ans_emb.append(response_embeddings.numpy())\n    train_questitle_emb.append(question_title_embeddings.numpy())\n    train_quesbody_emb.append(question_body_embeddings.numpy())\n#Stacking the sentence embeddings for all len(train)\nembeddings_train['answer_embedding'] = np.vstack(train_ans_emb)\nembeddings_train['question_body_embedding'] = np.vstack(train_quesbody_emb)\nembeddings_train['question_title_embedding'] = np.vstack(train_questitle_emb)\n\nprint(\"preparing embeddings for test data....\")\ntest['question_title']=test['question_title'].apply(lambda x:x.strip('\\n'))\ntest['question_body']=test['question_body'].apply(lambda x:x.strip('\\n'))\ntest['answer']=test['answer'].apply(lambda x:x.strip('\\n'))\ntest_ans_emb = []\ntest_questitle_emb = []\ntest_quesbody_emb = []\nfor q_t,q_b,a in tqdm(zip(list(test['question_title']),list(test['question_body']),list(test['answer']))):\n    question_title=[q_t]\n    question_body=[q_b]\n    responses=[a]\n    response_contexts = responses\n    question_title_embeddings = embed.signatures['question_encoder'](tf.constant(question_title))['outputs']\n    question_body_embeddings = embed.signatures['question_encoder'](tf.constant(question_body))['outputs']\n    response_embeddings = embed.signatures['response_encoder'](input=tf.constant(responses),context=tf.constant(response_contexts))['outputs']\n    test_ans_emb.append(response_embeddings.numpy())\n    test_questitle_emb.append(question_title_embeddings.numpy())\n    test_quesbody_emb.append(question_body_embeddings.numpy())\n#Stacking the sentence embeddings for all len(test)\nembeddings_test['answer_embedding']  = np.vstack(test_ans_emb)\nembeddings_test['question_body_embedding'] = np.vstack(test_quesbody_emb)\nembeddings_test['question_title_embedding'] = np.vstack(test_questitle_emb)\n\n\ndel embed\nK.clear_session()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# import torch\n# model = torch.load(\"../input/mt-dnn-largept/mt_dnn_large.pt\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.USE-large"},{"metadata":{},"cell_type":"markdown","source":"### 4.1)loading embeddings"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# %%time\n# module_url = \"../input/universalsentenceencoderlarge4/\"\n# embed = hub.load(module_url)\n# embeddings_train = {}#sentence_embeddings\n# embeddings_test = {}#sentence_embeddings\n\n# for text in tqdm(['question_title']):\n#     print(text)\n#     train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n#     test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n\n#     curr_train_emb = []\n#     curr_test_emb = []\n#     batch_size = 4\n#     ind = 0\n#     while ind*batch_size < len(train_text):\n#         curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n#         ind += 1\n        \n#     ind = 0\n#     while ind*batch_size < len(test_text):\n#         curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n#         ind += 1    \n        \n#     embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n#     embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n    \n# embeddings_train['question_body_embedding'] = train_quesbody_emb_stacked\n# embeddings_train['answer_embedding']        = train_ans_emb_stacked\n# embeddings_test['question_body_embedding']  = test_quesbody_emb_stacked\n# embeddings_test['answer_embedding']         = test_ans_emb_stacked\n\n# del embed\n# K.clear_session()\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2)Using embeddings from 4.1 to compute ${l}_2,cos$ distances.  \n${l}_2,cos$ are used as features"},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n\ncos_dist = lambda x, y: (x*y).sum(axis=1)\n#embeddings_train is a sentence embedding dictionary\ndist_features_train = np.array([\n    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n]).T\n\ndist_features_test = np.array([\n    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n]).T\nX_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train,dist_features_train])\nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test,dist_features_test])\ny_train = train[targets].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Universal sentence encoder embedding\ntry:\n    print(embeddings_train['question_title_embedding'].shape,embeddings_train['question_body_embedding'].shape,embeddings_train['answer_embedding'].shape)\n    print(embeddings_test['question_title_embedding'].shape,embeddings_test['question_body_embedding'].shape,embeddings_test['answer_embedding'].shape)\nexcept:\n    print(\"Error due to print statement\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking USE,manual features,Distil BERT\n"},{"metadata":{},"cell_type":"markdown","source":"* train_question_body_dense,train_answer_dense are from **Distil-BERT**  \n* X_train is from manual features & USE"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.hstack((X_train, train_question_body_dense, train_answer_dense))\nX_test = np.hstack((X_test, test_question_body_dense, test_answer_dense))\nX_train.shape,X_test.shape,y_train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(X_train).to_csv('X_train_USEQA_BERTuncased.csv')\npd.DataFrame(y_train).to_csv('y_train_USEQA_BERTuncased.csv')\npd.DataFrame(X_test).to_csv('X_test_USEQA_BERTuncased.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, MaxPooling2D, Flatten, Activation\ndef create_model(n_dense1=256,dropout1=0.30,lr_rate=0.00003):\n    model = Sequential()\n    model.add(Dense(n_dense1, input_dim=X_train.shape[1], activation='elu'))\n    model.add(Dropout(dropout1))\n    model.add(Dense(y_train.shape[1], activation='sigmoid'))    \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr_rate),loss=tf.keras.losses.binary_crossentropy,metrics=['accuracy'])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nall_predictions = []\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    model = create_model()\n    print( X_tr.shape,y_tr.shape,X_vl.shape,y_vl.shape)\n    model.fit(\n        X_tr, y_tr, epochs=100, batch_size=32, validation_data=(X_vl, y_vl), verbose=False, \n        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_vl, y_vl),\n                                       patience=5, model_name=f'best_model_batch{ind}.h5')]\n    )\n    \n    all_predictions.append(model.predict(X_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.fit(X_train, y_train, epochs=33, batch_size=32, verbose=False)\nall_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nkf = KFold(n_splits=5, random_state=2019, shuffle=True)\nfor ind, (tr, val) in enumerate(kf.split(X_train)):\n    X_tr = X_train[tr]\n    y_tr = y_train[tr]\n    X_vl = X_train[val]\n    y_vl = y_train[val]\n    model = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\n    model.fit(X_tr, y_tr)\n    all_predictions.append(model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wall time: 18min 22s for 40% of train data, on GPU!  \n**MultiTaskElasticNet** is trained with L1/L2 mixed-norm as regularizer  \n\n**l1_ratio(param)**, $\\epsilon\\text{ }[0,1]$.  \nl1_ratio = 1 the penalty is an L1/L2 penalty.  \nl1_ratio = 0 it is an L2 penalty.   \n0 < l1_ratio < 1, penalty is a combination of L1/L2 and L2.\n"},{"metadata":{},"cell_type":"markdown","source":"**len(all_predictions)=11**.....HOW?????  \nAns: 5-fold with keras model +  1-usual fit with keras  + 5-fold with MultiElasticNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(all_predictions)):\n    print(i+1,\":\",all_predictions[i].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel = MultiTaskElasticNet(alpha=0.001, random_state=42, l1_ratio=0.5)\nmodel.fit(X_train, y_train)\nall_predictions.append(model.predict(X_test))\nlen(all_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\nmax_val = test_preds.max() + 1\ntest_preds = test_preds/max_val + 1e-12","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''\nExpansion of \ntest_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_predictions]).mean(axis=0)\n\ntemp1=[]\nfor p in all_predictions:\n    temp2=[]\n    for c in p.T:\n        temp2.append(rankdata(c))\n    temp1.append(np.array(temp2).T)\ntest_preds2=np.array(temp1).mean(axis=0)\nmax_val2 = test_preds2.max() + 1\ntest_preds2 = test_preds2/max_val2 + 1e-12\n'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling-stage2"},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nimport re\nimport unicodedata\nimport six\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n\n    # The casing has to be passed in by the user and there is no explicit check\n    # as to whether it matches the checkpoint. The casing information probably\n    # should have been stored in the bert_config.json file, but it's not, so\n    # we have to heuristically detect it to validate.\n\n    if not init_checkpoint:\n        return\n\n    m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n    if m is None:\n        return\n\n    model_name = m.group(1)\n\n    lower_models = [\n        \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n        \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n    ]\n\n    cased_models = [\n        \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n        \"multi_cased_L-12_H-768_A-12\"\n    ]\n\n    is_bad_config = False\n    if model_name in lower_models and not do_lower_case:\n        is_bad_config = True\n        actual_flag = \"False\"\n        case_name = \"lowercased\"\n        opposite_flag = \"True\"\n\n    if model_name in cased_models and do_lower_case:\n        is_bad_config = True\n        actual_flag = \"True\"\n        case_name = \"cased\"\n        opposite_flag = \"False\"\n\n    if is_bad_config:\n        raise ValueError(\n            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n            \"However, `%s` seems to be a %s model, so you \"\n            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n            \"how the model was pre-training. If this error is wrong, please \"\n            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n                                              model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef printable_text(text):\n    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it's a Unicode string and in the other it's a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(\"utf-8\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True):\n        \"\"\"Constructs a BasicTokenizer.\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BERT Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks, segments for BERT\"\"\"\n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]#including special BERT tokens\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [np.asarray(input_ids, dtype=np.int32), np.asarray(input_masks, dtype=np.int32), np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        rho_val = compute_spearmanr(self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        self.test_predictions.append(self.model.predict(self.test_inputs, batch_size=self.batch_size))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_bert_model(input_units,output_units):\n    input_word_ids = tf.keras.layers.Input((input_units,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input((input_units,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input((input_units,), dtype=tf.int32, name='input_segments')\n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(output_units, activation=\"sigmoid\",name=\"dense_output\")(x)\n    model = tf.keras.models.Model(inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    return model    \n        \n\ndef train_and_predict(model, train_data, valid_data, test_data, learning_rate, epochs, batch_size, loss_function, fold):  \n    custom_callback = CustomCallback((valid_data[0], valid_data[1]),test_data,batch_size,fold=None)\n    optimizer = tf.keras.optimizers.Adam(learning_rate)\n    model.compile(loss_function,optimizer)\n    model.fit(train_data[0], train_data[1],epochs,batch_size, callbacks=[custom_callback])\n    return custom_callback","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BERT_PATH = '/kaggle/input/berthub'\n# tokenizer = FullTokenizer(BERT_PATH+'/assets/vocab.txt', True)\n# target_cols = list(submission.columns)\n# design_cols = list(set(train.columns)-set(target_cols))#totally 10 in nos of which many are gratuitous.\n# Y_cols = target_cols\n# X_cols = ['question_title','question_body','answer']\n# len(X_cols),len(Y_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gkf = GroupKFold(n_splits=5).split(X=train.question_body, groups=train.question_body)\n# bert_dimension=512\n# #outputs = compute_output_arrays(train, Y_cols)\n# #inputs = compute_input_arays(train, X_cols, tokenizer, bert_dimension)\n# test_inputs = compute_input_arays(test, X_cols, tokenizer, bert_dimension)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\n# #Using pretrained models\n# from tqdm import tqdm\n# tqdm.pandas()\n# models = []\n# bert_dimension = 512#dimension of bert embedding\n# for i in tqdm(range(5)):\n#     weights_path = f'../input/bertuned-f{i}/bertuned_f{i}.h5'\n#     model = train_bert_model(input_units=bert_dimension,output_units=len(Y_cols)-1)\n#     model.load_weights(weights_path)\n#     models.append(model)\n# weights_path = f'../input/bertf1e15/Full-0.h5'\n# model = train_bert_model(input_units=bert_dimension,output_units=len(Y_cols)-1)\n# model.load_weights(weights_path)\n# models.append(model)\n# len(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %%time\ntest_predictions = []\n# BATCH_SIZE_FOR_INFERENCE = 8#was 8 earlier\n# from tqdm import tqdm\n# tqdm.pandas()\n# test_predictions = []\n# for model in tqdm(models):\n#     test_predictions.append(model.predict(test_inputs, batch_size=BATCH_SIZE_FOR_INFERENCE))\ntest_predictions.append(test_preds)#appending DistillBert,USEs,handfeatured\n#print(\"test_predictions shape\",test_predictions[i].shape)\nfinal_predictions = np.mean(test_predictions, axis=0)\nfinal_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.iloc[:,1:] = final_predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# output1 = pd.read_csv(\"../input/output1/output1_USEDistBertoof.csv\",index_col='qa_id')\n# output1.drop(columns='Unnamed: 0',inplace=True)\n# output2 = pd.read_csv(\"../input/2layerednn/2layeredNN.csv\",index_col='qa_id')\n\n\n# submission = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n# submission.iloc[:,1:] =(output2.to_numpy()+final_predictions)/2\n# submission.head()\n# submission.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# submission.iloc[:, 1:] = final_predictions\n# if len(submission)==476:\n#     submission.to_csv(\"submission.csv\",index=False)\n# else:\n#     temp = submission[~submission.qa_id.isin(submission.qa_id.values)]\n#     sub = pd.concat([submission,temp],ignore_index=True)\n#     sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6079(entire train data) instead of 1823(sample used)\n### After vertical stacking of embeddings\n   embeddings_train['question_title_embedding'].shape,embeddings_train['question_body_embedding'].shape,embeddings_train['answer_embedding'].shape\n    embeddings_test['question_title_embedding'].shape,embeddings_test['question_body_embedding'].shape,embeddings_test['answer_embedding'].shape\n \n### Output:  \n(1823, 512) (1823, 512) (1823, 512)  \n(476, 512) (476, 512) (476, 512)\n\n\n### After horizontal stacking of embeddings along with manual features\nX_train = np.hstack([item for k, item in embeddings_train.items()] + [features_train,dist_features_train])  \nX_test = np.hstack([item for k, item in embeddings_test.items()] + [features_test,dist_features_test])  \nX_train.shape,X_test.shape\n\n### Output:  \n((1823, 1605), (476, 1605))  \nwhere, features_train.shape=(1823, 63), dist_features_train.shape=(1823, 6)  \n1605 = 512*3 + 63 + 6\n\n### After horizontal stacking of X_train along with Distil BERT Output  \n\nX_train = np.hstack((X_train, train_question_body_dense, train_answer_dense))  \nX_test = np.hstack((X_test, test_question_body_dense, test_answer_dense))  \nX_train.shape,X_test.shape,y_train.shape  \n### Output:  \n((1823, 3141), (476, 3141), (1823, 30))  \nwhere, train_question_body_dense.shape=(1823, 768), train_answer_dense.shape=(1823, 768)  \n3141 =                 1605                            +     768    +    768 (768 is the dim of DistilBert)  \n     =( 512*3 +        63         +      6       )     +   768      +    768  \n     =  3USEs +  manualfeatures   +features(3USEs) + DistilBert(questionbody)+DistilBert(answer)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}