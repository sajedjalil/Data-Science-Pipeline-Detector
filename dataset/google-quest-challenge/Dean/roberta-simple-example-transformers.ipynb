{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport time\nimport random\nimport os\nimport torch\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom math import floor, ceil\nfrom transformers import AdamW,BertForSequenceClassification\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\ntrain = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\n\nMAX_SEQUENCE_LENGTH = 512","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train shape =', train.shape)\nprint('test shape =', test.shape)\n\noutput_categories = list(train.columns[11:])\ninput_categories = list(train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modified [inital code](https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer) to RoBERTa input format"},{"metadata":{"trusted":true},"cell_type":"code","source":"## credit to https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer\n\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\n## modified inital code to RoBERTa format\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [1] * (max_seq_length-len(token_ids))\n    return input_ids\n\n## modified inital code to RoBERTa format\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=238, a_max_len=238):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+6) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+6 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+6)))\n        \n        if t_len > t_new_len:\n            ind1 = floor(t_new_len/2)\n            ind2 = ceil(t_new_len/2)\n            t = t[:ind1]+t[-ind2:]\n        else:\n            t = t[:t_new_len]\n\n        if q_len > q_new_len:\n            ind1 = floor(q_new_len/2)\n            ind2 = ceil(q_new_len/2)\n            q = q[:ind1]+q[-ind2:]\n        else:\n            q = q[:q_new_len]\n\n        if a_len > a_new_len:\n            ind1 = floor(a_new_len/2)\n            ind2 = ceil(a_new_len/2)\n            a = a[:ind1]+a[-ind2:]\n        else:\n            a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = ['<s>'] + title + ['</s>','</s>'] + question + ['</s>','</s>'] + answer + ['</s>']\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, x_train, idxs, targets=None):\n        self.input_ids = x_train[0][idxs]\n        self.input_masks = x_train[1][idxs]\n        self.input_segments = x_train[2][idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n\n    def __getitem__(self, idx):\n#         x_train = self.x_train[idx]\n        input_ids =  self.input_ids[idx]\n        input_masks = self.input_masks[idx]\n        input_segments = self.input_segments[idx]\n\n        target = self.targets[idx]\n\n        return input_ids, input_masks, input_segments, target\n\n    def __len__(self):\n        return len(self.input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer, get_cosine_with_hard_restarts_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_weights = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(pretrained_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = compute_input_arays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ny_train = compute_output_arrays(train, output_categories)\nx_test = compute_input_arays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_config = RobertaConfig.from_pretrained(pretrained_weights) \nbert_config.num_labels = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NFOLDS = 5\nBATCH_SIZE = 4\nEPOCHS = 1\nSEED = 7345\nnum_warmup_steps = 100\nlr = 3e-5\n\n\ngradient_accumulation_steps = 1\nseed_everything(SEED)\n\nmodel_list = list()\n\n\ny_oof = np.zeros((len(train), 30))\ntest_pred = np.zeros((len(test), 30))\n\n\ny_oof = np.zeros((len(train), 30))\ntest_pred = np.zeros((len(test), 30))\n\nkf = KFold(n_splits=NFOLDS, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=BATCH_SIZE, shuffle=False)\n\n\nfor i, (train_idx, valid_idx) in enumerate(kf.split(x_train[0])):\n    \n    \n    print(f'fold {i+1}')\n\n    ## loader\n    train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train_idx, y_train),batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(TextDataset(x_train, valid_idx, y_train),batch_size=BATCH_SIZE, shuffle=False)\n    \n\n    t_total = len(train_loader)//gradient_accumulation_steps*EPOCHS\n\n\n    net = RobertaForSequenceClassification.from_pretrained(pretrained_weights, config=bert_config)\n    net.cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    optimizer = AdamW(net.parameters(), lr = lr)\n    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)  # PyTorch scheduler\n\n\n    for epoch in range(EPOCHS):  \n\n        start_time = time.time()\n        avg_loss = 0.0\n        net.train()\n\n\n        for step, data in enumerate(train_loader):\n\n            # get the inputs\n            input_ids, input_masks, input_segments, labels = data\n\n\n            pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                            )[0]\n            \n            \n            loss = loss_fn(pred, labels.cuda())\n        \n            avg_loss += loss.item()\n            loss = loss / gradient_accumulation_steps\n            loss.backward()\n\n            if (step + 1) % gradient_accumulation_steps == 0:\n\n                # Calling the step function on an Optimizer makes an update to its parameters\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n            \n        avg_val_loss = 0.0\n\n        valid_preds = np.zeros((len(valid_idx), 30))\n        true_label = np.zeros((len(valid_idx), 30))\n        \n        for j,data in enumerate(val_loader):\n\n            # get the inputs\n            input_ids, input_masks, input_segments, labels = data\n            pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                            )[0]\n\n            loss_val = loss_fn(pred, labels.cuda())\n            avg_val_loss += loss_val.item()\n            \n            pred = torch.sigmoid(pred)\n            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = pred.cpu().detach().numpy()\n            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n\n\n        elapsed_time = time.time() - start_time \n\n        score = 0\n        for i in range(30):\n          s = np.nan_to_num(\n                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation / 30)\n          score += s\n\n        \n\n        print('Epoch {}/{} \\t loss={:.4f}\\t val_loss={:.4f}\\t spearmanr={:.4f}\\t time={:.2f}s'.format(epoch+1, EPOCHS, avg_loss/len(train_loader),avg_val_loss/len(val_loader),score, elapsed_time))\n\n    model_list.append(net)\n    y_oof[valid_idx] = valid_preds\n\n\n    result = list()\n    with torch.no_grad():\n        for data in test_loader:\n            input_ids, input_masks, input_segments, labels = data\n            y_pred = net(input_ids = input_ids.long().cuda(),\n                                labels = None,\n                                attention_mask = input_masks.cuda(),\n                            )[0]\n\n            y_pred = torch.sigmoid(y_pred)\n            result.extend(y_pred.cpu().detach().numpy())\n            \n    test_pred += np.array(result)/NFOLDS\n\n\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.loc[:, output_categories] = test_pred\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}