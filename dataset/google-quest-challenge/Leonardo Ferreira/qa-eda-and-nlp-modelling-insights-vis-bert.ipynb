{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to my new Kernel to NLP tasks\n\nI'm starting to focus on NLP tasks and I decided to find for good challenges on Kaggle, and for luck, I discovered this competition.\n\nI hope it can be useful to the other fellows I'm sure that I will learn a lot in this amazing world of NLP. \n\nCome with me and let's discover some interesting questions about this data.\n"},{"metadata":{},"cell_type":"markdown","source":"# Description\n\nComputers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.\n\nHumans are better at addressing subjective questions that require a deeper, multidimensional understanding of context - something computers aren't trained to do well…yet.. Questions can take many forms - some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.\n\n<img src=\"https://storage.googleapis.com/kaggle-media/competitions/google-research/human_computable_dimensions_1.png\" alt=\"QA Google\">\n\nUnfortunately, it’s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That’s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.\n\nIn this competition, you’re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!\n\nDemonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like."},{"metadata":{},"cell_type":"markdown","source":"# INITIAL QUESTIONS: \n- What's the intersection between users that do the question and the who don't. \n- In general (mean), how many different topics an user do questions?\n- How many questions and/or answers an users usually do? \n- We can see clear patterns in different topics? (for instance, well writen in literacture topic...)\n- How many different topics we have?\n- The size of questions and answers are same in all categories?\n- We can detect the sentiment of QA of different categories?\n- What's the most frequent words by categories? \n- How many different topics we can detect in the QA topics? \n- And much more questions that probably will raise when I put my hands-on it! \n\n\n\n## After that we got the answers I will create some features, preprocess the data and model an algorithm that can solve this problem;\n\n# Enjoy and stay tuned to the next updates"},{"metadata":{},"cell_type":"markdown","source":"## Importing the needed libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib_venn import venn2, venn3, venn2_circles, venn3_circles #to create intersection graphs\nimport matplotlib.pyplot as plt #to plot show the charts\nimport seaborn as sns\nfrom scipy import stats\n\nfrom nltk import word_tokenize\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport os \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"## Importing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/google-quest-challenge/train.csv\")\ndf_test = pd.read_csv(\"../input/google-quest-challenge/test.csv\")\ndf_sub = pd.read_csv(\"../input/google-quest-challenge/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating func to resume the dataset"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resumetable(df_train)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! It's a good start to meet our data; \nWe have **6079 Answers Unique ID's** of QA Labeled. <br>\nThe resume also shows that **3215 different users did questions** and **4114 different users answered** questions\n\nWe can note that the Entropy of the values are very high and we don't have missing values; \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Test dataset"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"resumetable(df_test)[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 476 examples in the test dataset."},{"metadata":{},"cell_type":"markdown","source":"## Shape of the Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of submission: {df_sub.shape}\")\ntarget_cols = df_train[df_train.columns[df_train.columns.isin(df_sub.columns[1:])]].columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First look in \"the face\"(haha) of our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we are ready to start exploring the data to try get a good insight that can help us to build the Algorithm;<br>\nWe can see that we have many different metrics that was included only on training data... It will be useful to we better understand the categories."},{"metadata":{},"cell_type":"markdown","source":"## UNIQUE QUESTIONS\n- Let's start by a more generalist understanding and how the questions are distributed between the different categories and hosts"},{"metadata":{"trusted":true},"cell_type":"code","source":"## I will use the host column, split and get the first string\nfor i in range(len(df_train)):\n    df_train.loc[i,'host_cat'] = df_train.host.str.split('.')[i][0]\ndf_train.drop('host', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"host = df_train.groupby(['host_cat'])['url'].nunique().sort_values(ascending=False)\ncategory = df_train.groupby(['category'])['url'].nunique().sort_values(ascending=False)\n\nplt.figure(figsize=(16,12))\nplt.suptitle('Unique URL by Host and Categories', size=22)\n\nplt.subplot(211)\ng0 = sns.barplot(x=category.index, y=category.values, color='blue')\ng0.set_title(\"Unique Answers by category\", fontsize=22)\ng0.set_xlabel(\"Category Name\", fontsize=19)\ng0.set_ylabel(\"Total Count\", fontsize=19)\n#g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g0.patches:\n    height = p.get_height()\n    g0.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(height/category.sum()*100),\n            ha=\"center\",fontsize=11) \n\nplt.subplot(212)\ng1 = sns.barplot(x=host[:20].index, y=host[:20].values, color='blue')\ng1.set_title(\"TOP 20 HOSTS with more UNIQUE questions\", fontsize=22)\ng1.set_xlabel(\"Host Name\", fontsize=19)\ng1.set_ylabel(\"Total Count\", fontsize=19)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 3,\n            '{:1.1f}%'.format(height/host.sum()*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.3, top = 0.90)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First chart:\nThe most common category in Stackexchange fórums are:<br>\n1 - Technology(41.8%)<br>\n2 - Stackoverflow (21.2%)<br>\n3 - Culture (14.7%)<br>\n4 - Science (11.2)<br>\n5 - Life Arts(10.9)<br>\nIt's not so unbalanced, so it can be useful to train the model. \n\nSecond Chart:\nThe most common category is the Stackoverflow with 20.6% of the total open topics(questions);<br>\nAfter the StackOverflow we can see that only 7 categories have a ratio highest than 2%, we have in total 59;\n\nThe group of Stackoverflow, eletronics, superuser, serverfault, english, math, tex, physics and askubuntu together has **almost 46% of the total** questions. \n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Questions and Answer Unique Users and some values\nAs we saw in the Resume table, \n> Total Unique Users in 'Question User Name': 3215 <br>Total Unique Users in 'Answer User Name': 4114 <br>\n    \nLet's take a first look on the users intersection between who ask something and who try to answer. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total Unique Users in 'Question User Name': {df_train['question_user_name'].nunique()}\")\nprint(f\"Total Unique Users in 'Answer User Name': {df_train['answer_user_name'].nunique()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ploting the Venn Diagram."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\n\nvenn2([set(df_train['question_user_name'].value_counts(dropna=False).index), \n       set(df_train['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), alpha=.5)\nplt.title('Comparison of Question and Answer Users Intersection\\n', fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's very interesting. <br>\nWe can see that a small part of total users has done both actions on the platform. <br>\nI thought it had a more balanced ratio in users that do questions and who answer it;\n\n"},{"metadata":{},"cell_type":"markdown","source":"##  QA USERS Intersection by CATEGORY feature"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec # to do the grid of plots\n\ngrid = gridspec.GridSpec(3, 3)\nplt.figure(figsize=(16,3*4))\n\nplt.suptitle('Intersection QA USERS \\nQuestions and Answers by different CATEGORIES', size=20)\n\nfor n, col in enumerate(df_train['category'].value_counts().index):\n    ax = plt.subplot(grid[n])\n    venn2([set(df_train[df_train.category == col]['question_user_name'].value_counts(dropna=False).index), \n           set(df_train[df_train.category == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's interesting, we can see some difference in the intersection ratio of the USERS in the different categories;<br>\nTake a look on Stackoverflow, it has a big difference between people who ask and people who answer questions; "},{"metadata":{},"cell_type":"markdown","source":"## QA USERS Intersection by Host feature"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"grid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,4.5*4))\n\nplt.suptitle('Intersection QA USERS - TOP 15 \\nQuestions and Answers by different HOSTS', size=20)\ntop_host = df_train['host_cat'].value_counts()[:15].index\nfor n, col in enumerate(top_host):\n    ax = plt.subplot(grid[n])\n    venn2([set(df_train[df_train.host_cat == col]['question_user_name'].value_counts(dropna=False).index), \n           set(df_train[df_train.host_cat == col]['answer_user_name'].value_counts(dropna=False).index)],\n      set_labels=('Question Users', 'Answer Users'), )\n    ax.set_title(str(col), fontsize=15)\n    ax.set_xlabel('')\n    #plt.subplots_adjust(top = 0.98, wspace=.9, hspace=.9)\n    \nplt.subplots_adjust(top = 0.9, hspace=.1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! I find it very insightful hahah<br>\nIt's interesting to note that programmers host has almost 1:3 ratio between QA Users, and only one in the intersection.<br>\nOnly for curiosity, we can take a look on one some of the intersection users to see what type of questions was done by them"},{"metadata":{},"cell_type":"markdown","source":"# Title and Body Lenghts of questions \n- I will start counting the Number of words in each question and set it to a new column\n- After I will plot some distributions about the titles and the bodies of the questions\n- The idea is to see te difference by categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize each item in the review column\nword_tokens = [word_tokenize(question) for question in df_train.question_body]\n\n# Create an empty list to store the length of the reviews\nlen_tokens = []\n\n# Iterate over the word_tokens list and determine the length of each item\nfor i in range(len(word_tokens)):\n     len_tokens.append(len(word_tokens[i]))\n\n# Create a new feature for the lengh of each review\ndf_train['question_n_words'] = len_tokens","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"grid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nplt.suptitle('Title and Question Lenghts by Different Categories \\nThe Mean in RED - Also 5% and 95% lines', size=20)\ncount=0\ntop_cats=df_train['category'].value_counts().index\nfor n, col in enumerate(top_cats):\n    for i, q_t in enumerate(['question_title', 'question_body', 'question_n_words']):\n        ax = plt.subplot(grid[count])\n        if q_t == 'question_n_words':\n            sns.distplot(df_train[df_train['category'] == col][q_t], bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\nQuestion #Total Words Distribution\", fontsize=15)\n            ax.axvline(df_train[df_train['category'] == col][q_t].quantile(.95))\n            ax.axvline(df_train[df_train['category'] == col][q_t].quantile(.05))\n            mean_val = df_train[df_train['category'] == col][q_t].mean()\n            ax.axvline(mean_val, color='red' )\n            ax.set_xlabel('')            \n        else:\n            sns.distplot(df_train[df_train['category'] == col][q_t].str.len(), bins = 50, \n                         color='g', label=\"RED - 50%\") \n            ax.set_title(f\"Distribution of {str(col)} \\n{str(q_t)}\", fontsize=15)\n            ax.axvline(df_train[df_train['category'] == col][q_t].str.len().quantile(.95))\n            ax.axvline(df_train[df_train['category'] == col][q_t].str.len().quantile(.05))\n            mean_val = df_train[df_train['category'] == col][q_t].str.len().mean()\n            ax.axvline(mean_val, color='red' )\n            #ax.text(x=mean_val*1.1, y=.02, s='Holiday in US', alpha=0.7, color='#334f8d')\n            ax.set_xlabel('')\n        count+=1\n        \nplt.subplots_adjust(top = 0.90, hspace=.4, wspace=.15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! <br>\nThe quantiles of the distributions are very similar in all categories in both title and question lenghts.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Exploring Target Features\n- Let's explore the target features to find some clear pattern that can be useful.\n- I will apply some techniques of dimensionality reduction and verify the explained variability and f"},{"metadata":{},"cell_type":"markdown","source":"## Distribution of all Target Features\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"grid = gridspec.GridSpec(10, 3)\n\nplt.figure(figsize=(16,8*4))\ncount=0\nplt.suptitle('Distribution of QA metrics (Target Features)', size=20)\n# top_host = df_train['host_cat'].value_counts()[:15].index\nfor n, col in enumerate(target_cols):\n    #if df_train[target_cols].std()[col] > .15:\n    ax = plt.subplot(grid[count])\n    sns.boxplot(x='category', y=col, data=df_train)\n    ax.set_title(str(col), fontsize=13)\n    ax.set_xlabel('')\n    ax.set_ylabel(' ')\n    count+=1\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n\nplt.subplots_adjust(top = 0.95, hspace=.9, wspace=.2)\n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool. It's a informative chart where we can get the difference between the categories to each target feature.\n- I think that worth to spent a time to analyze it in depth\nFor instance: \n- \"question body critical\" has an interesting distribution between the different categories. \n- \"question well writen\" shows that Life Arts and Culture has highest quality in the written. \n- And many other distributions that can be meaninful.\n\nAlso, we have some features taht we can't see a clear difference between the categories. \n\nLet's try to extract the PCA of all this features and see what it can shows to us. "},{"metadata":{},"cell_type":"markdown","source":"## Principal Component Analyzis - N components"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pca = PCA(n_components=3, random_state=42)\n\nprincipalComponents = pca.fit_transform(df_train[target_cols])\n\nprincipalDf = pd.DataFrame(principalComponents)\n\n# df.drop(cols, axis=1, inplace=True)\nprefix='Target_PCA'\nprincipalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\ndf_train = pd.concat([df_train, principalDf], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"TOP 3 PCA Explanability: \")\n[print(str(f\"{i+1} - {round(pca*100,3)}%\")) for i, pca in enumerate(pca.explained_variance_ratio_[:3])]\nprint(f\"Sum of 3 Principal components: {round(pca.explained_variance_ratio_[:3].sum()*100,3)}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's not so much, but it's a important value that means that we have some different patterns. \n- Let's create some charts to explain"},{"metadata":{},"cell_type":"markdown","source":"# Ploting PCA of Target\n- ploting the first and second principal components"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\ng = sns.scatterplot(x='Target_PCA0', y='Target_PCA1', data=df_train, hue='category')\ng.set_title(\"PCA Components Distribution by Categories\", fontsize=22)\ng.set_xlabel(\"TARGET PCA 0\", fontsize=16)\ng.set_ylabel(\"TARGET PCA 1\", fontsize=16)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool!!! It shows us that the questions in Technology and Stackoverflow has different pattern than the culture, science, life arts, what make a lot of sense. <br>\n\nBelow, I will take a look in the difference between the hosts. "},{"metadata":{},"cell_type":"markdown","source":"# PCA of target BY HOSTS\nploting pca to the top 15 hosts with more unique questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df_train[df_train.host_cat.isin(top_host)], col='host_cat',\n                  col_wrap=3, height=3, aspect=1.5, hue='category')\n\ng.map(sns.scatterplot, \"Target_PCA0\", \"Target_PCA1\", alpha=.5 ).add_legend();\ng.set_titles('{col_name}', fontsize=17)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GREAT! We can clearly note different patterns in the target PCA. \nFor instance, take a look on askubuntu, stackoverflow, tex and culture, English, physics and so on... It's a insightful information."},{"metadata":{},"cell_type":"markdown","source":"# Finding for Correlation in Target Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(df_train[target_cols].corr(),vmin=-1,cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can note that some features are high correlated.<br>\nFor instance: Critical questions and questions interestingness self seems more well written. "},{"metadata":{},"cell_type":"markdown","source":"# Questions and Answers Texts\nLet's start ploting some WordClouds by Categories. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\nnewStopWords = ['amp', 'gt', 'lt', 'div', 'id',\n                'fi', 'will', 'use', 'one', 'nbsp', 'need']\nstopwords.update(newStopWords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"grid = gridspec.GridSpec(5, 2)\n\nplt.figure(figsize=(16,7*4))\n\nplt.suptitle('Word Cloud OF CATEGORY FEATURE', size=20)\n\nfor n, col in enumerate(df_train['category'].value_counts().index):\n    ax = plt.subplot(grid[n])  \n    \n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(df_train[df_train['category'] == col]['answer'].astype(str)))\n\n    #print(wordcloud)\n\n    plt.imshow(wordcloud)\n    plt.title(f\"Category: {col}\",fontsize=18)\n    plt.axis('off')\nplt.subplots_adjust(top = 0.95, hspace=.2, wspace=.1 )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice I can see some interesting words in the different categories, but only stackoverflow seems more clear. Let's explore it in further;"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"markdown","source":"# WordCloud by HOSTS\n- TOP 10 hosts with more unique questions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#newStopWords = ['fruit', \"Drink\", \"black\"]\n\n#stopwords.update(newStopWords)\n\nimport matplotlib.gridspec as gridspec # to do the grid of plots\ngrid = gridspec.GridSpec(5, 2)\n\nplt.figure(figsize=(16,7*4))\n\nplt.suptitle('Answers Word Cloud \\nTOP 10 hosts with more questions', size=20)\n\nfor n, col in enumerate(df_train['host_cat'].value_counts()[:10].index):\n    ax = plt.subplot(grid[n])   \n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=250,\n        max_font_size=100, \n        width=400, height=280,\n        random_state=42,\n    ).generate(\" \".join(df_train[df_train['host_cat'] == col]['answer'].astype(str)))\n\n    #print(wordcloud)\n\n    plt.imshow(wordcloud)\n    plt.title(f\"Host: {col}\",fontsize=18)\n    plt.axis('off')\n    \nplt.subplots_adjust(top = 0.95, hspace=.2, wspace=.1 )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Cool! \nThe wordcloud of the hosts are very meaningful of the different communities"},{"metadata":{},"cell_type":"markdown","source":"# Answers Sentiment analysis "},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ndf_train['ans_polarity']= df_train['answer'].apply(pol)\ndf_train['ans_subjectivity']= df_train['answer'].apply(sub)\n\ndf_train[['answer', 'category', 'ans_polarity', 'ans_subjectivity']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! Now that we got some metrics about the posible sentiment of the answers, let get some quantitative analysis of it."},{"metadata":{},"cell_type":"markdown","source":"## Ploting the polarity x the subjetictivy"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(16,5))\n\ng = sns.scatterplot(x='ans_polarity', y='ans_subjectivity', \n                    data=df_train, hue='category')\ng.set_title(\"Sentiment Analyzis (Polarity x Subjectivity) by 'Category' Feature\", fontsize=21)\ng.set_xlabel(\"Polarity distribution\",fontsize=18)\ng.set_ylabel(\"Subjective \",fontsize=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can't see a clear difference in the answers. But seems that the answers tends to have a more positive value in polarity <br>\nLet's find other way to access this informations in a better and more objective way."},{"metadata":{},"cell_type":"markdown","source":"## Getting some statistics of Polarity and Subjectivity by the Category feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"polarity_answers = df_train.groupby('category')['ans_polarity', 'ans_subjectivity'].describe().reset_index()\n\npolarity_answers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De degree of subjectivity of the Stackoverflow is smaller than the other categories. <br>\nWe can also note a smaller level of polarity in the answer. In opposite of it, Life Arts has the highest polarity and subjectivity what make sense."},{"metadata":{},"cell_type":"markdown","source":"# Trigrams by TOP 10 HOSTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.update(['amp', 'lt', 'gt', 'frac'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import re\ncontraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\",\n                    \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n                    \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n                    \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                    \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n                    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                    \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n                    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \n                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n                    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n                    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n                    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n                    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n                    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                    \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n                    \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n                    \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n                    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                    \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                    \"you'll've\": \"you will have\", \"you're\":\"you are\", 'you re':\"you are\",'youre': \"you are\", \"you've\": \"you have\", }\n\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>',\n          '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£','·', '_', '{', '}', '©', '^', '®', '`', \n          '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n          '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n          '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',  '▒', '：', '¼', '⊕', '▼', '▪', '†', '■',\n          '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n          '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬',\n          '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', #'lt', 'gt', 'amp', 'div', 'ex', 'le', 'http', 'www', 'vo', '\\n'\n         ]\n\ndef clean_text(x):\n    x = str(x)\n    \n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, '')\n    return x\n\ndef _get_contractions(contraction_dict):\n    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n    return contraction_dict, contraction_re\n\ncontractions, contractions_re = _get_contractions(contraction_dict)\n\ndef replace_contractions(text):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, text)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Example of Questions + Answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, the order is important\ndf_train['answer'] = df_train['answer'].apply(replace_contractions)\ndf_train['answer'] = df_train['answer'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ngrid = gridspec.GridSpec(5, 3)\nplt.figure(figsize=(16,6*4))\n\nfor n, cat in enumerate(top_host[:9]):\n    \n    ax = plt.subplot(grid[n])   \n    # print(f'PRINCIPAL WORDS CATEGORY: {cat}')\n    # vectorizer = CountVectorizer(ngram_range = (3,3)) \n    # X1 = vectorizer.fit_transform(df_train[df_train['host_cat'] == cat]['answer'])  \n \n    min_df_val = round(len(df_train[df_train['host_cat'] == cat]) - len(df_train[df_train['host_cat'] == cat]) * .99)\n    max_df_val = round(len(df_train[df_train['host_cat'] == cat]) - len(df_train[df_train['host_cat'] == cat]) * .3)\n    \n    # Applying TFIDF \n    vectorizer = TfidfVectorizer(ngram_range = (2,2), min_df=5, stop_words='english',\n                                 max_df=.5) \n    X2 = vectorizer.fit_transform(df_train[df_train['host_cat'] == cat]['answer']) \n    features = (vectorizer.get_feature_names()) \n    scores = (X2.toarray()) \n\n    # Getting top ranking features \n    sums = X2.sum(axis = 0) \n    data1 = [] \n    \n    for col, term in enumerate(features): \n        data1.append( (term, sums[0,col] )) \n\n    ranking = pd.DataFrame(data1, columns = ['term','rank']) \n    words = (ranking.sort_values('rank', ascending = False))[:10]\n    \n    sns.barplot(x='term', y='rank', data=words, ax=ax, \n                color='blue', orient='v')\n    ax.set_title(f\"Top rank Trigram of: {cat}\")\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n    ax.set_ylabel(' ')\n    ax.set_xlabel(\" \")\n\nplt.subplots_adjust(top = 0.95, hspace=.9, wspace=.1)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It needs a more appropriate text preprocessing but it give us a good first understand about the words in different categories"},{"metadata":{},"cell_type":"markdown","source":"# Start Modellling\n- Preprocessing\n- Implement a simple solution BERT using TfIdf as baseline \n\n> I will implement the amazing pythonic preprocessing of @pavelvpster kernel, you can access <a href=\"https://www.kaggle.com/pavelvpster/google-q-a-labeling-tf-idf-pytorch\">here</a>"},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_cols = ['host_cat', 'question_n_words', 'Target_PCA0',\n              'Target_PCA1', 'Target_PCA2', 'ans_polarity', 'ans_subjectivity']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting the X and y"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train[target_cols].copy()\nX_train = df_train.drop(list(extra_cols) + list(target_cols), axis=1)\ndel df_train\n\nX_test = df_test.copy()\ndel df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Start Modeling\n- I will start by a simple model to use as baseline. As it's my first time with a QA task, I searched for good solutions. \n- The QA model implementation below, I saw in @hamditarek amazing kernel about LDA and LSA topic modelling. Kernel Link below on the resources links; \n\nVersion 2: <br>\nI'm now testing the solutions of akensert"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Shell \n!pip install ../input/sacremoses > /dev/null\n\nimport sys\nsys.path.insert(0, \"../input/transformers/\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n# import tensorflow_hub as hub\nimport tensorflow as tf\n# import bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting thePaths to the pre trained models and "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.set_printoptions(suppress=True)\n\nfrom transformers import *\n\nBERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n\nMAX_SEQUENCE_LENGTH = 512\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions to preprocess the data to BERT archtecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"## The function to creat the masks using to the title, question and answer\ndef _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    \n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1, str2,\n            add_special_tokens=True,\n            max_length=length,\n            truncation_strategy=truncation_strategy)\n        \n        input_ids =  inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n        \n        return [input_ids, input_masks, input_segments]\n    \n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        title + ' ' + question, None, 'longest_first', max_sequence_length)\n    \n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        answer, None, 'longest_first', max_sequence_length)\n    \n    return [input_ids_q, input_masks_q, input_segments_q,\n            input_ids_a, input_masks_a, input_segments_a]\n\n# Computing the inputs\ndef compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    \n    for _, instance in tqdm(df[columns].iterrows()):\n        \n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n        \n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n        \n    return [np.asarray(input_ids_q, dtype=np.int32), \n            np.asarray(input_masks_q, dtype=np.int32), \n            np.asarray(input_segments_q, dtype=np.int32),\n            np.asarray(input_ids_a, dtype=np.int32), \n            np.asarray(input_masks_a, dtype=np.int32), \n            np.asarray(input_segments_a, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Creating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n## Computing the error metric to the model optimization\ndef compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef create_model():\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n    \n    config = BertConfig() # print(config) to see settings\n    config.output_hidden_states = False # Set to True to obtain hidden states\n    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n    \n    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n    # pretrained model has been downloaded manually and uploaded to kaggle. \n    bert_model = TFBertModel.from_pretrained(\n        BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n    \n    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n    \n    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n    \n    x = tf.keras.layers.Concatenate()([q, a])\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n\n    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Processing the inputs and outputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"outputs = compute_output_arrays(y_train, y_train.columns)\ninputs = compute_input_arrays(X_train, X_train.columns, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arrays(X_test, X_test.columns, tokenizer, MAX_SEQUENCE_LENGTH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trainning the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Creating Kfold with 5 splits \ngkf = GroupKFold(n_splits=5).split(X=X_train.question_body, groups=X_train.question_body)\n\n## to receive predictions\nvalid_preds = []\ntest_preds = []\n\n## Looping throught the folds\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 2 folds (out of 5) to manage < 2h\n    if fold in [0 , 2, 4]:\n        \n        ## Train index from Kfold \n        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n        train_outputs = outputs[train_idx]\n        ## Valid index from Kfold \n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        \n        K.clear_session()\n        \n        ## Instantiating the Bert Model\n        model = create_model()\n        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n        \n        ## Fiting the model\n        model.fit(train_inputs, train_outputs, epochs=2, batch_size=6)\n        \n        # model.save_weights(f'bert-{fold}.h5')\n        valid_preds.append(model.predict(valid_inputs))\n        # predicting the test set and appending to test_preds\n        test_preds.append(model.predict(test_inputs))\n        \n        # Calculating the error in the valid set\n        rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n        print('validation score = ', rho_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataframe to CSV \n- Saving the prediction file in csv extension"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n\ndf_sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# NOTE: THIS KERNEL IS NOT FINISHED"},{"metadata":{},"cell_type":"markdown","source":"### Stay tuned and if this kernel was useful for you, please upvote the kernel"},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"Some of sources/references that I used in this kernel:<br>\nhttps://www.kaggle.com/abhinand05/bert-for-humans-tutorial-baseline-version-2 <br>\nhttps://www.kaggle.com/pavelvpster/google-q-a-labeling-tf-idf-pytorch <br>\nhttps://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools <br>\nhttps://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/ <br>\nhttps://www.kaggle.com/hamditarek/get-started-with-nlp-lda-lsa <br>\nhttps://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer<br><br>\nThe books:<br>\n- Natural Language Processing and Computational Linguistics. A practical Guide to Text Analysis with Python, Gensim, spaCy and Keras (Packt-2018) - Bhargav Srinivasa<br>\n- Python Natural Language Processing (Packt-2017) - Jalaj Thanaki"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}