{"cells":[{"metadata":{},"cell_type":"markdown","source":"# QUEST BERT-base Pytorch minimalistic (training)\n\nThis is a PyTorch version of @akensert's brilliant TF kernel. Here I train all 5 folds, and have a separate inference kernel over [here]().\n\nFeel free to comment if you have any questions!"},{"metadata":{},"cell_type":"markdown","source":"Versions:\n\nV19: Fixed f-strings, trying linear scheduler with warmup of 0.05, commonly used for fine-tuning BERT. Also using AdamW, also commonly used for fine-tuning BERT.\n\nV18: separated CV display into different code cell, because of output overflow/cutoff issues.\n\nV17: Updated to train all 5-folds with GroupKFold, print all fold scores and CV score at end, cleaned up code so it's a little clearer."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nos.system('pip install ../input/sacremoses/sacremoses-master/ > /dev/null')\nos.system('pip install ../input/transformers/transformers-master/ > /dev/null')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers, sys, os, gc\nimport numpy as np, pandas as pd, math\nimport torch, random, os, multiprocessing, glob\nimport torch.nn.functional as F\nimport torch, time\n\nfrom sklearn.model_selection import GroupKFold\n# from ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold <--- if you want to use Neuron Engineer's CV\nfrom scipy.stats import spearmanr\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    BertTokenizer, BertModel, BertPreTrainedModel, BertForSequenceClassification, BertConfig,\n    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n    get_cosine_schedule_with_warmup,\n)\n\nfrom tqdm import tqdm_notebook\nprint(transformers.__version__)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 42\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/google-quest-challenge/train.csv\",)\ntest = pd.read_csv(\"../input/google-quest-challenge/test.csv\",)\n\ntarget_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice',\n               'question_type_compare', 'question_type_consequence',\n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful',\n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helpers"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# helpers\n\n# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n    \n    #293+239+30 = 508 + 4 = 512\n    \n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm_notebook(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the below cell to see how to add colors to your output! (thanks to my teammate @ecdrid)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Ref From Stack Overflow\n# Make sure to end with bcolors.ENDC otherwise color is used in the rest of the terminal\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PyTorch Dataset, training, validation, testing functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset and other utils..\n\nclass QuestDataset(torch.utils.data.Dataset):\n    def __init__(self, inputs, lengths, labels = None):\n        \n        self.inputs = inputs\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        self.lengths = lengths\n\n    def __getitem__(self, idx):\n        \n        input_ids       = self.inputs[0][idx]\n        input_masks     = self.inputs[1][idx]\n        input_segments  = self.inputs[2][idx]\n        lengths         = self.lengths[idx]\n        if self.labels is not None: # targets\n            labels = self.labels[idx]\n            return input_ids, input_masks, input_segments, labels, lengths\n        return input_ids, input_masks, input_segments, lengths\n\n    def __len__(self):\n        return len(self.inputs[0])\n\ndef train_model(train_loader, optimizer, criterion, scheduler=None):\n    \n    model.train()\n    avg_loss = 0.\n    tk0 = tqdm_notebook(enumerate(train_loader))\n    \n    for idx, batch in tk0:\n        \n        input_ids, input_masks, input_segments, labels, _ = batch\n        input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)            \n        \n        output_train = model(input_ids = input_ids.long(),\n                             labels = None,\n                             attention_mask = input_masks,\n                             token_type_ids = input_segments,\n                            )\n        logits = output_train[0] #output preds\n        \n        loss = criterion(logits, labels)\n        loss.backward()\n        \n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n        optimizer.zero_grad()\n        \n        avg_loss += loss.item() / len(train_loader)\n        del input_ids, input_masks, input_segments, labels\n\n    torch.cuda.empty_cache()\n    gc.collect()\n    return avg_loss\n\ndef val_model(val_loader, val_shape, batch_size=8):\n\n    avg_val_loss = 0.\n    model.eval() # eval mode\n    \n    valid_preds = np.zeros((val_shape, 30))\n    original = np.zeros((val_shape, 30))\n    \n    tk0 = tqdm_notebook(enumerate(val_loader))\n    with torch.no_grad():\n        \n        for idx, batch in tk0:\n            input_ids, input_masks, input_segments, labels, _ = batch\n            input_ids, input_masks, input_segments, labels = input_ids.to(device), input_masks.to(device), input_segments.to(device), labels.to(device)            \n            \n            output_val = model(input_ids = input_ids.long(),\n                             labels = None,\n                             attention_mask = input_masks,\n                             token_type_ids = input_segments,\n                            )\n            logits = output_val[0] #output preds\n            \n            avg_val_loss += criterion(logits, labels).item() / len(val_loader)\n            valid_preds[idx*batch_size : (idx+1)*batch_size] = logits.detach().cpu().squeeze().numpy()\n            original[idx*batch_size : (idx+1)*batch_size]    = labels.detach().cpu().squeeze().numpy()\n        \n        score = 0\n        preds = torch.sigmoid(torch.tensor(valid_preds)).numpy()\n        \n        spear = []\n        for i in range(preds.shape[1]):\n            spear.append(np.nan_to_num(spearmanr(original[:, i], preds[:, i]).correlation))\n        score = np.mean(spear)\n    return avg_val_loss, score\n\ndef predict_result(model, test_loader, batch_size=32):\n    \n    test_preds = np.zeros((len(test), 30))\n    \n    model.eval();\n    tk0 = tqdm_notebook(enumerate(test_loader))\n    for idx, x_batch in tk0:\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                           )\n            predictions = outputs[0]\n            test_preds[idx*batch_size : (idx+1)*batch_size] = predictions.detach().cpu().squeeze().numpy()\n\n    output = torch.sigmoid(torch.tensor(test_preds)).numpy()        \n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT model configuration"},{"metadata":{},"cell_type":"markdown","source":"The below hidden code cell contains code to add a pooling layer to the model if you desire to do so, as according to @ecdrid's discussion post over [here](). "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# class BertForSequenceClassification(BertPreTrainedModel):\n#     r\"\"\"\n#         **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n#             Labels for computing the sequence classification/regression loss.\n#             Indices should be in ``[0, ..., config.num_labels - 1]``.\n#             If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n#             If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n#     Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n#         **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n#             Classification (or regression if config.num_labels==1) loss.\n#         **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n#             Classification (or regression if config.num_labels==1) scores (before SoftMax).\n#         **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n#             list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n#             of shape ``(batch_size, sequence_length, hidden_size)``:\n#             Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n#         **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n#             list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n#             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n#     Examples::\n\n#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n#         model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n#         input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n#         labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n#         outputs = model(input_ids, labels=labels)\n#         loss, logits = outputs[:2]\n\n#     \"\"\"\n#     def __init__(self, config):\n#         super(BertForSequenceClassification, self).__init__(config)\n#         self.num_labels = config.num_labels\n\n#         self.bert = BertModel(config)\n#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n#         self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n#         self.init_weights()\n\n#     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n#                 position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n\n#         outputs = self.bert(input_ids,\n#                             attention_mask=attention_mask,\n#                             token_type_ids=token_type_ids,\n#                             position_ids=position_ids,\n#                             head_mask=head_mask,\n#                             inputs_embeds=inputs_embeds)\n\n#         sequence_output = outputs[0]\n#         pooled_output = torch.mean(sequence_output,axis=1) # Global Average Pooling (batch_size,hidden_size) \n\n#         pooled_output = self.dropout(pooled_output)\n#         logits = self.classifier(pooled_output)\n\n#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n#         if labels is not None:\n#             if self.num_labels == 1:\n#                 #  We are doing regression\n#                 loss_fct = MSELoss()\n#                 loss = loss_fct(logits.view(-1), labels.view(-1))\n#             else:\n#                 loss_fct = CrossEntropyLoss()\n#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n#             outputs = (loss,) + outputs\n\n#         return outputs  # (loss), logits, (hidden_states), (attentions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below is the BERT configuration code, which includes setting up the tokenizer class, and the configuration class. We are using a `bert-base-uncased` model, with 30 labels as associated with this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\")\ninput_categories = list(train.columns[[1,2,5]]); input_categories\n\nbert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\nbert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = 30\n\nbert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\noutput_model_file = 'bert_pytorch.bin'\n\nprint(do_lower_case, bert_model, device, output_model_file)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and validation code"},{"metadata":{},"cell_type":"markdown","source":"Below, we define the folds, and go through the folds, train and validate the model. The final validation fold scores are printed out at the end and the models are saved for later inference."},{"metadata":{"_uuid":"45167d81-7984-40f7-aa93-cbb4e1d68e57","_cell_guid":"02fbbdbf-1eac-4951-a500-3b41eb7a31c6","trusted":true},"cell_type":"code","source":"from transformers import AdamW\nNUM_FOLDS = 5  \nBATCH_SIZE = 8\nepochs = 4    # change this\n\n# Define CV\nkf = GroupKFold(n_splits = NUM_FOLDS)\nscores = []\n\ny_train = train[target_cols].values # dummy\n\nprint(bcolors.FAIL, f\"For Every Fold, Train {epochs} Epochs\", bcolors.ENDC)\n\nfor fold, (train_index, val_index) in enumerate(kf.split(X=train.question_body, groups=train.question_body)):\n        \n    print(bcolors.HEADER, \"Current Fold:\", fold, bcolors.ENDC)\n\n    train_df, val_df = train.iloc[train_index], train.iloc[val_index]\n    print(\"Train and Valid Shapes are\", train_df.shape, val_df.shape)\n    \n    print(bcolors.HEADER, \"Preparing train datasets....\", bcolors.ENDC)\n    \n    inputs_train = compute_input_arays(train_df, input_categories, tokenizer, max_sequence_length=512)\n    outputs_train = compute_output_arrays(train_df, columns = target_cols)\n    outputs_train = torch.tensor(outputs_train, dtype=torch.float32)\n    lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n    lengths_train[lengths_train == 0] = inputs_train[0].shape[1]\n    \n    print(bcolors.HEADER, \"Preparing Valid datasets....\", bcolors.ENDC)\n    \n    inputs_valid = compute_input_arays(val_df, input_categories, tokenizer, max_sequence_length=512)\n    outputs_valid = compute_output_arrays(val_df, columns = target_cols)\n    outputs_valid = torch.tensor(outputs_valid, dtype=torch.float32)\n    lengths_valid = np.argmax(inputs_valid[0] == 0, axis=1)\n    lengths_valid[lengths_valid == 0] = inputs_valid[0].shape[1]\n   \n    \n    print(bcolors.HEADER, \"Preparing Dataloaders....\", bcolors.ENDC)\n\n    train_set    = QuestDataset(inputs=inputs_train, lengths=lengths_train, labels=outputs_train)\n    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n    \n    valid_set    = QuestDataset(inputs=inputs_valid, lengths=lengths_valid, labels=outputs_valid)\n    valid_loader = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n    \n    # Create model\n    model = BertForSequenceClassification.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config);\n    model.zero_grad();\n    model.to(device);\n    torch.cuda.empty_cache()\n    model.train();\n    \n    best_score      = -1.\n    best_param_score = None\n    best_param_epoch = None\n\n    optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-7, correct_bias=False)\n    criterion = nn.BCEWithLogitsLoss()\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0.05, num_training_steps= epochs*len(train_loader))\n    #scheduler=None\n    \n    print(\"Training....\")\n    \n    for epoch in tqdm_notebook(range(epochs)):\n        \n        torch.cuda.empty_cache()\n        \n        start_time   = time.time()\n        avg_loss     = train_model(train_loader, optimizer, criterion, scheduler)\n        avg_val_loss, score = val_model(valid_loader, val_shape=val_df.shape[0])\n        elapsed_time = time.time() - start_time\n\n        print(bcolors.WARNING, 'Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(\n            epoch + 1, epochs, avg_loss, avg_val_loss, score, elapsed_time),\n        bcolors.ENDC\n        )\n\n        if best_score < score:\n            best_score = score\n            best_param_score = model.state_dict()\n            best_epoch_score = epoch\n\n    print(bcolors.WARNING, f'Best model came from Epoch {best_epoch_score} with score of {best_score}',bcolors.ENDC)\n    model.load_state_dict(best_param_score)\n    torch.save(best_param_score, 'best_param_score_{}.pt'.format(fold+1))\n    scores.append(best_score)\n\n    del train_df, val_df, model, optimizer, criterion, scheduler\n    torch.cuda.empty_cache()\n    del valid_loader, train_loader, valid_set, train_set\n    torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    torch.cuda.empty_cache()\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(bcolors.HEADER,'Individual Fold Scores:',bcolors.ENDC)\nprint(scores)\nprint(bcolors.HEADER,f'QUEST BERT-base {NUM_FOLDS}-fold CV score: ',np.mean(scores),bcolors.ENDC)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}