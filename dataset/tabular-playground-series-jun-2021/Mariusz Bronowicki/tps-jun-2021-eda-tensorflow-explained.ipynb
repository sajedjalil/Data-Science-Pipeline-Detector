{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hi Kagglers!\nI was wondering how am I supposed to aproach this month competition, what can we do what hasn't been done yet. I decided that this time I focus on creating model in Tensorflow with a deep understanding of workflow and what should be done to create robust model with hope of improving metrics. I would also try to create some nice plots in EDA part.","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“¡Import Libraries and Datasets","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nimport missingno as no\n\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:11.473106Z","iopub.execute_input":"2021-06-30T16:35:11.473597Z","iopub.status.idle":"2021-06-30T16:35:14.378959Z","shell.execute_reply.started":"2021-06-30T16:35:11.473562Z","shell.execute_reply":"2021-06-30T16:35:14.376868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:14.384223Z","iopub.execute_input":"2021-06-30T16:35:14.384591Z","iopub.status.idle":"2021-06-30T16:35:14.389299Z","shell.execute_reply.started":"2021-06-30T16:35:14.384559Z","shell.execute_reply":"2021-06-30T16:35:14.388306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/test.csv\")\nsample_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jun-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:14.390969Z","iopub.execute_input":"2021-06-30T16:35:14.391284Z","iopub.status.idle":"2021-06-30T16:35:17.138623Z","shell.execute_reply.started":"2021-06-30T16:35:14.391251Z","shell.execute_reply":"2021-06-30T16:35:17.137579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  ðŸ“Š Exploratory Data Analysis\n\nAlthough this data set is similar to the previous one and we could start creating model strait away, it is a good practice to look at the properties of a dataset we are working on.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:17.140682Z","iopub.execute_input":"2021-06-30T16:35:17.141186Z","iopub.status.idle":"2021-06-30T16:35:17.18378Z","shell.execute_reply.started":"2021-06-30T16:35:17.141136Z","shell.execute_reply":"2021-06-30T16:35:17.182781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no.matrix(train_df, figsize=(18,4));","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:17.185034Z","iopub.execute_input":"2021-06-30T16:35:17.185527Z","iopub.status.idle":"2021-06-30T16:35:21.481902Z","shell.execute_reply.started":"2021-06-30T16:35:17.185475Z","shell.execute_reply":"2021-06-30T16:35:21.480771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values in training and test datasets.","metadata":{}},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:21.483355Z","iopub.execute_input":"2021-06-30T16:35:21.483647Z","iopub.status.idle":"2021-06-30T16:35:21.490899Z","shell.execute_reply.started":"2021-06-30T16:35:21.483618Z","shell.execute_reply":"2021-06-30T16:35:21.489673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop('id', axis=1).describe().T.style.bar(subset=['mean'], color=px.colors.qualitative.Pastel[4])\\\n                                        .background_gradient(subset=['std'], cmap='Greens')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:21.493015Z","iopub.execute_input":"2021-06-30T16:35:21.493722Z","iopub.status.idle":"2021-06-30T16:35:22.243244Z","shell.execute_reply.started":"2021-06-30T16:35:21.49364Z","shell.execute_reply":"2021-06-30T16:35:22.242144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ðŸŽ¯ Our target labels","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.countplot(x=train_df['target'], palette='coolwarm')\nplt.title(\"Distribution of target labels\", fontdict={'fontsize':24})\nplt.xlabel('Target', fontdict={'fontsize':16})\nplt.ylabel('Count', fontdict={'fontsize':16});","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:22.246632Z","iopub.execute_input":"2021-06-30T16:35:22.247079Z","iopub.status.idle":"2021-06-30T16:35:22.688051Z","shell.execute_reply.started":"2021-06-30T16:35:22.247032Z","shell.execute_reply":"2021-06-30T16:35:22.686834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_ratio = 100 * train_df['target'].value_counts() / len(train_df)\n\nplt.figure(figsize=(12,5))\nsns.barplot(x=class_ratio.index,y=class_ratio.values, palette='coolwarm')\nplt.title(\"Target labels Percentage in training dataset\", fontdict={'fontsize':24})\nplt.ylabel(\"Percentage %\")\nplt.xlabel(\"Target label\");","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:22.689974Z","iopub.execute_input":"2021-06-30T16:35:22.69031Z","iopub.status.idle":"2021-06-30T16:35:23.008632Z","shell.execute_reply.started":"2021-06-30T16:35:22.690258Z","shell.execute_reply":"2021-06-30T16:35:23.007456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, we have inbalance problem again where majority of it is in class 6 and class 8 so I will apply StratifiedKFold method to help me deal with it. Importance of knowing whether we have balanced or unbalanced target labels is also when it comes to evaluate the model performance. In this case more reliable metrics are f1, precision and recall instead of accuracy.","metadata":{}},{"cell_type":"markdown","source":"### Correlation between features and target.","metadata":{}},{"cell_type":"code","source":"lb = LabelEncoder()\ntrain_df['num_target'] = lb.fit_transform(train_df['target'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:23.010046Z","iopub.execute_input":"2021-06-30T16:35:23.010427Z","iopub.status.idle":"2021-06-30T16:35:23.087553Z","shell.execute_reply.started":"2021-06-30T16:35:23.010391Z","shell.execute_reply":"2021-06-30T16:35:23.086332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(28,16))\ncorr_mat = train_df.drop([\"id\", 'target'], axis=1).corr()\nmask = np.zeros_like(corr_mat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr_mat, mask=mask, square=True, ax=ax, linewidths=0.1,center=0, cmap='coolwarm_r');","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:23.089319Z","iopub.execute_input":"2021-06-30T16:35:23.089787Z","iopub.status.idle":"2021-06-30T16:35:31.591861Z","shell.execute_reply.started":"2021-06-30T16:35:23.089726Z","shell.execute_reply":"2021-06-30T16:35:31.590756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see features correlation to the target.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,5))\nsns.barplot(y=corr_mat['num_target'].values[:-1],x=corr_mat['num_target'].index[:-1], palette='coolwarm')\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:31.593514Z","iopub.execute_input":"2021-06-30T16:35:31.594181Z","iopub.status.idle":"2021-06-30T16:35:34.868527Z","shell.execute_reply.started":"2021-06-30T16:35:31.594129Z","shell.execute_reply":"2021-06-30T16:35:34.867784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these plots we can tell that all features have weak correlation to the target column and the correlation is positive. Feature 20 has no correlation to the target column.","metadata":{}},{"cell_type":"markdown","source":"### Features distribution in training and test dataset.","metadata":{}},{"cell_type":"code","source":"feat_cols = [col for col in train_df.columns if col not in ['target','num_target','id']]\n\nfig = plt.figure(figsize=(20,40))\nfor i,col in enumerate(feat_cols[:12]):\n    temp_df = train_df[[col,'target']].groupby('target').mean()\n    plt.subplot(25,3,i+1)\n    sns.barplot(x=temp_df.index[:12],y=temp_df[col][:12], palette='coolwarm')\n    plt.ylabel(f\"feature_{i} mean\")\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:34.869759Z","iopub.execute_input":"2021-06-30T16:35:34.870257Z","iopub.status.idle":"2021-06-30T16:35:39.548871Z","shell.execute_reply.started":"2021-06-30T16:35:34.870221Z","shell.execute_reply":"2021-06-30T16:35:39.547775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,40))\n\nfor i, col in enumerate(feat_cols):\n    plt.subplot(25,3, i+1)\n    sns.kdeplot(train_df[col], fill=True, color='red')\n    sns.kdeplot(test_df[col], fill=True, color='blue')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:35:39.550242Z","iopub.execute_input":"2021-06-30T16:35:39.550594Z","iopub.status.idle":"2021-06-30T16:37:38.405714Z","shell.execute_reply.started":"2021-06-30T16:35:39.55056Z","shell.execute_reply":"2021-06-30T16:37:38.402809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training and test dataset distribution are virtually the same with lots of outliers. In normal case scenario we would have to deal with it, but in this case were the dataset is created syntheticaly using CGAN I found very little difference in performance of the model, therefore I will not deal with outliers in this notebook.Let's have a closer look into few features with different distribution and create some nice plots and have some fun with it.","metadata":{}},{"cell_type":"code","source":"# Create fig and gridspec\nfig = plt.figure(figsize=(16,10),dpi=80)\ngrid = plt.GridSpec(4,4, hspace=0.5,wspace=0.2)\n\n# Define the axes\nax_main = fig.add_subplot(grid[:-1,:-1])\nax_right = fig.add_subplot(grid[:-1,-1], xticklabels=[],yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1,0:-1],xticklabels=[],yticklabels=[])\n\n# Scatterplot on main ax\nax_main.scatter(x='feature_12', y='feature_39',c='num_target',data=train_df,alpha=.9,cmap=\"coolwarm\")\n\n# Boxplot on the right\nax_right.boxplot(x=train_df['feature_39'])\n\n# boxplot on the bottom\nax_bottom.boxplot(x=train_df['feature_12'],vert=False, )\n\n# Decorations\nax_main.set(title='Scatterplot with Boxplot \\n feature_39 vs. feature_12', xlabel='feature_39', ylabel='feature_12');","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:37:38.407643Z","iopub.execute_input":"2021-06-30T16:37:38.408107Z","iopub.status.idle":"2021-06-30T16:37:42.82792Z","shell.execute_reply.started":"2021-06-30T16:37:38.408061Z","shell.execute_reply":"2021-06-30T16:37:42.826737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('white')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:37:42.829269Z","iopub.execute_input":"2021-06-30T16:37:42.829647Z","iopub.status.idle":"2021-06-30T16:37:42.834681Z","shell.execute_reply.started":"2021-06-30T16:37:42.829611Z","shell.execute_reply":"2021-06-30T16:37:42.833431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_agg = train_df.loc[:,['feature_39','target']].groupby('target')\nvals = [df['feature_39'].values.tolist() for i,df in df_agg]\nplt.figure(figsize=(16,9),dpi=80)\n# create color list\ncolors = [plt.cm.coolwarm(i/float(len(vals)-1)) for i in range(len(vals))]\n# plot histogram\nn, bins, patches = plt.hist(vals,30,stacked=True,density=False,color=colors[:len(vals)])\nplt.xlim(0,15)\n# decorations\nplt.legend({group:col for group,col in zip(np.unique(train_df['target']).tolist(),colors[:len(vals)])})\nplt.title('Stacked histogram of fearure_39 colored by class', fontsize=22);","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:37:42.836139Z","iopub.execute_input":"2021-06-30T16:37:42.836571Z","iopub.status.idle":"2021-06-30T16:37:44.560023Z","shell.execute_reply.started":"2021-06-30T16:37:42.836535Z","shell.execute_reply":"2021-06-30T16:37:44.55884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Density Plot\nplt.figure(figsize=(10,5), dpi= 80)\nfor i in range(len(train_df['target'].unique())):\n    sns.kdeplot(train_df.loc[train_df['target'] == f'Class_{i}', \"feature_39\"], shade=False, color=colors[i], alpha=.3, fill=None)\nplt.title(\"Density plot of feature_39\", fontsize=22)\nplt.legend({group:val for group,val in zip(train_df['target'].unique(),colors[:9])});","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:37:44.56178Z","iopub.execute_input":"2021-06-30T16:37:44.562136Z","iopub.status.idle":"2021-06-30T16:37:46.44897Z","shell.execute_reply.started":"2021-06-30T16:37:44.562102Z","shell.execute_reply":"2021-06-30T16:37:46.448114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install joypy\nimport joypy","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T16:37:46.45025Z","iopub.execute_input":"2021-06-30T16:37:46.450779Z","iopub.status.idle":"2021-06-30T16:37:55.021657Z","shell.execute_reply.started":"2021-06-30T16:37:46.450732Z","shell.execute_reply":"2021-06-30T16:37:55.020424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,10),dpi=80)\nfig, axes = joypy.joyplot(train_df, \n                          column=['feature_39','feature_10','feature_67'],\n                          by='target',\n                          figsize=(14,10),\n                          legend=True,\n                          color=['g','r','b'])\nplt.title(\"Chosen features distribution per class\",fontsize=22);","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:37:55.023541Z","iopub.execute_input":"2021-06-30T16:37:55.024018Z","iopub.status.idle":"2021-06-30T16:38:10.535797Z","shell.execute_reply.started":"2021-06-30T16:37:55.023967Z","shell.execute_reply":"2021-06-30T16:38:10.534577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It's time to create a base ANN (Artificial Neural Network) model. ðŸ¥ðŸŽº\n\nIn this notebook I would like to create a robust model using Tensorflow. First I will create a base model and than I will try to imrove the model.There are few things to be done in order to make neural network working:\n* All data needs to be numerical.\n* data should be presented in tensors (tensorflow also works great with arrays)\n* Scaled the data (a model performes much better ones a data is normalized)\n\nTo create a base model I will split data with train_test_split. Later, when we try to imrove our model performace and make our model more robust I will use one of the cross validation methods to split the data.","metadata":{}},{"cell_type":"code","source":"# Drop unwanted columns\ntrain_df.drop([\"id\",\"target\"], axis=1, inplace=True)\ntest_df.drop(\"id\", axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:38:10.537264Z","iopub.execute_input":"2021-06-30T16:38:10.537589Z","iopub.status.idle":"2021-06-30T16:38:10.6122Z","shell.execute_reply.started":"2021-06-30T16:38:10.537557Z","shell.execute_reply":"2021-06-30T16:38:10.610898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:38:10.613685Z","iopub.execute_input":"2021-06-30T16:38:10.614052Z","iopub.status.idle":"2021-06-30T16:38:10.630348Z","shell.execute_reply.started":"2021-06-30T16:38:10.614017Z","shell.execute_reply":"2021-06-30T16:38:10.629245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Split our data into X & y\nX = train_df.iloc[:,:-1].values\ny = train_df.iloc[:,-1].values\n\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=y)\n\n# Normalize the data\nsc = MinMaxScaler(feature_range=(0,1))\nX_train_norm = sc.fit_transform(X_train)  # First we fit and transform train set and than transform test set to avoid data leakage\nX_test_norm = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:38:10.636877Z","iopub.execute_input":"2021-06-30T16:38:10.637374Z","iopub.status.idle":"2021-06-30T16:38:11.258094Z","shell.execute_reply.started":"2021-06-30T16:38:10.637333Z","shell.execute_reply":"2021-06-30T16:38:11.256998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of our datasets\nX_train_norm.shape, X_test_norm.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:38:11.261925Z","iopub.execute_input":"2021-06-30T16:38:11.262393Z","iopub.status.idle":"2021-06-30T16:38:11.269567Z","shell.execute_reply.started":"2021-06-30T16:38:11.262355Z","shell.execute_reply":"2021-06-30T16:38:11.268199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base ANN model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:38:11.271524Z","iopub.execute_input":"2021-06-30T16:38:11.272048Z","iopub.status.idle":"2021-06-30T16:38:17.639352Z","shell.execute_reply.started":"2021-06-30T16:38:11.272003Z","shell.execute_reply":"2021-06-30T16:38:17.638195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a base model\nbase_model = tf.keras.Sequential([\n    tf.keras.layers.Dense(75, activation='relu'),\n    tf.keras.layers.Dense(75, activation='relu'),\n    tf.keras.layers.Dense(9, activation='softmax')  # we have multi-class classification problem\n])\n\n# Compile the base model\nbase_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # expect labels to provided as integers\n                   optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                   metrics=[\"accuracy\"])\n\n# Fit the base model\nbase_history = base_model.fit(X_train_norm, \n                              y_train, \n                              epochs=20,\n                              validation_data=(X_test_norm, y_test))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T16:38:17.641244Z","iopub.execute_input":"2021-06-30T16:38:17.641818Z","iopub.status.idle":"2021-06-30T16:41:17.391196Z","shell.execute_reply.started":"2021-06-30T16:38:17.641777Z","shell.execute_reply":"2021-06-30T16:41:17.389913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate our base model","metadata":{}},{"cell_type":"code","source":"# Create a data frame\nbase_history_df = pd.DataFrame(base_history.history)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:17.393223Z","iopub.execute_input":"2021-06-30T16:41:17.393615Z","iopub.status.idle":"2021-06-30T16:41:17.400682Z","shell.execute_reply.started":"2021-06-30T16:41:17.393569Z","shell.execute_reply":"2021-06-30T16:41:17.399681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:17.401823Z","iopub.execute_input":"2021-06-30T16:41:17.402246Z","iopub.status.idle":"2021-06-30T16:41:17.416334Z","shell.execute_reply.started":"2021-06-30T16:41:17.402212Z","shell.execute_reply":"2021-06-30T16:41:17.415472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_history(df, fold=1):\n    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(18,5))\n    \n    fig.suptitle(f\"FOLD={fold}\")\n\n    # First plot\n    df[[\"loss\",\"val_loss\"]].plot(ax=ax[0])\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Loss\")\n    ax[0].set_title(\"Training and Validation Loss\")\n\n    # Second plot\n    df[[\"accuracy\",\"val_accuracy\"]].plot(ax=ax[1])\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Accuracy\")\n    ax[1].set_title(\"Training and Validation Accuracy\")\n    \nplot_history(base_history_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:17.417493Z","iopub.execute_input":"2021-06-30T16:41:17.41793Z","iopub.status.idle":"2021-06-30T16:41:17.99063Z","shell.execute_reply.started":"2021-06-30T16:41:17.417899Z","shell.execute_reply":"2021-06-30T16:41:17.989495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model.evaluate(X_train_norm, y_train)\nbase_model.evaluate(X_test_norm,y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:17.992399Z","iopub.execute_input":"2021-06-30T16:41:17.992847Z","iopub.status.idle":"2021-06-30T16:41:24.144375Z","shell.execute_reply.started":"2021-06-30T16:41:17.992799Z","shell.execute_reply":"2021-06-30T16:41:24.14341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ”‘ **Note:** As we can see on the first plot above, the training loss started with ~1.80 value and have gone down to ~1.72 and it looks like it could go a little bit more if we train for more epochs. On the other hand loss on test data have gone down from ~1.78 to ~1.77 than sort of leveling and as we increase number of epochs loss is going up. Looks like the base model is overfitting. We will have to apply regularization to tackle this issue. Accuracy also increase on training data but is decreasing on test data as we can see on second plot above\n\n\nTo evaluate classification model we can use other metrics as:\n1. Precision (Specificity) - is the ratio of True Positives to all positives predicted by a model(low precision: the more false positive model predicts, the lower the precision).\n2. Recall (also known as Sensitivity) - is the ratio of True Positives to all positives in your data\n3. F1-score - in case we want to find ideal blend of precision and recall\n\nAlongside visualizing our model results as much as possible, there are handfull evaluation methods we should be familiar with. To main ones we can include:\n* Confusion matrix\n* Classification report\n* Receiver Operating Characteristic (ROC) curve\n\nLet's make predictions and try these methods to evaluate our model. We have to remember that our predictions array come out in prediction probability form... to standard output form the sigmoid or softmax activation function.","metadata":{}},{"cell_type":"code","source":"# Make predictions\nbase_y_pred = base_model.predict(X_test_norm)\n# Convert all of the prediction probabilities into integers\nbase_y_pred_int = base_y_pred.argmax(axis=1)\nbase_y_pred_int[:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:24.145744Z","iopub.execute_input":"2021-06-30T16:41:24.146052Z","iopub.status.idle":"2021-06-30T16:41:25.061101Z","shell.execute_reply.started":"2021-06-30T16:41:24.146023Z","shell.execute_reply":"2021-06-30T16:41:25.060336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classification report\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, base_y_pred_int))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:25.062155Z","iopub.execute_input":"2021-06-30T16:41:25.062556Z","iopub.status.idle":"2021-06-30T16:41:25.13998Z","shell.execute_reply.started":"2021-06-30T16:41:25.062527Z","shell.execute_reply":"2021-06-30T16:41:25.138945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion matrix","metadata":{}},{"cell_type":"code","source":"classes_names = {l:i for (i,l) in enumerate(lb.classes_)}","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:25.142524Z","iopub.execute_input":"2021-06-30T16:41:25.14299Z","iopub.status.idle":"2021-06-30T16:41:25.149149Z","shell.execute_reply.started":"2021-06-30T16:41:25.14294Z","shell.execute_reply":"2021-06-30T16:41:25.147133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef make_confusion_matrix(y_true, y_preds, classes=None, figsize=(15,15),text_size=15):\n    \"\"\"\n    Plots confusion matrix for given true labels and model predictions.\n    \"\"\"\n    # Create the confusion matrix\n    cm = confusion_matrix(y_true, y_preds)\n    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:,np.newaxis] # normalize out confusion matrix\n    n_classes = cm.shape[0]\n    \n    # Create a matrix plot\n    fig, ax = plt.subplots(figsize=figsize)\n    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n    fig.colorbar(cax)\n    \n    # Set labels to be classes\n    if classes:\n        labels=classes\n    else:\n        labels=np.arange(cm.shape[0])\n        \n    # Label the axes\n    ax.set(title=\"Confusion matrix\",\n           xlabel=\"Predicted labels\",\n           ylabel=\"True labels\",\n           xticks=np.arange(n_classes),\n           yticks=np.arange(n_classes),\n           xticklabels=labels,\n           yticklabels=labels)\n    \n    # Set x-axis labels to bottom\n    ax.xaxis.set_label_position(\"bottom\")\n    ax.xaxis.tick_bottom()\n    \n    # Adjust label size\n    ax.yaxis.label.set_size(text_size)\n    ax.xaxis.label.set_size(text_size)\n    ax.title.set_size(text_size)\n    \n    # Set threshold to different colors\n    threshold = (cm.max() + cm.min()) / 2.\n    \n    # Plot the text on each cell\n    for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, f\"{cm[i,j]} ({cm_norm[i,j]*100:.1f})\",\n        horizontalalignment='center',\n        color='white' if cm[i,j] > threshold else 'black',\n        size=text_size)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:25.150766Z","iopub.execute_input":"2021-06-30T16:41:25.151314Z","iopub.status.idle":"2021-06-30T16:41:25.166252Z","shell.execute_reply.started":"2021-06-30T16:41:25.151094Z","shell.execute_reply":"2021-06-30T16:41:25.165244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, base_y_pred_int)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:25.167816Z","iopub.execute_input":"2021-06-30T16:41:25.168438Z","iopub.status.idle":"2021-06-30T16:41:25.248767Z","shell.execute_reply.started":"2021-06-30T16:41:25.168387Z","shell.execute_reply":"2021-06-30T16:41:25.247606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a confusion matrix plot\nmake_confusion_matrix(y_true=y_test, \n                      y_preds=base_y_pred_int, \n                      classes=classes_names.keys(), \n                      figsize=(18,15),\n                      text_size=15)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:25.250178Z","iopub.execute_input":"2021-06-30T16:41:25.250541Z","iopub.status.idle":"2021-06-30T16:41:26.463111Z","shell.execute_reply.started":"2021-06-30T16:41:25.250506Z","shell.execute_reply":"2021-06-30T16:41:26.461232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### AUC-ROC curve\n\nðŸ”‘ **Note:** AUC ROC Curve is a performance measurement for classification problem with various thresholds settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between the classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve, auc, roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:26.464821Z","iopub.execute_input":"2021-06-30T16:41:26.465309Z","iopub.status.idle":"2021-06-30T16:41:26.473211Z","shell.execute_reply.started":"2021-06-30T16:41:26.465241Z","shell.execute_reply":"2021-06-30T16:41:26.471496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = len(classes_names)\n\ndef roc_auc_plot(y_true, y_preds, n_classes):\n    \"\"\"\n    Compute ROC Curve and ROC Area for each class than create a plot.\n    \"\"\"\n    # Compute roc and auc for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    y_true = tf.one_hot(y_true, depth=n_classes)\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_true[:,i],y_preds[:,i])\n        roc_auc[i] = auc(fpr[i],tpr[i])\n        \n    # Plot Roc curve\n    linestyles = ['-', '--', '-.', ':','-', '--', '-.', ':','-']\n\n    plt.figure(figsize=(12,10))\n    for i in range(n_classes):\n        plt.plot(fpr[i], \n                 tpr[i], \n                 label='ROC curve of class {0} (area={1:0.2f})'.format(i+1, roc_auc[i]),\n                 linestyle=linestyles[i])\n        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic to multi-class')\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:26.475626Z","iopub.execute_input":"2021-06-30T16:41:26.476071Z","iopub.status.idle":"2021-06-30T16:41:26.490368Z","shell.execute_reply.started":"2021-06-30T16:41:26.47602Z","shell.execute_reply":"2021-06-30T16:41:26.488759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc_auc_plot(y_test, base_y_pred, n_classes)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:26.492445Z","iopub.execute_input":"2021-06-30T16:41:26.492876Z","iopub.status.idle":"2021-06-30T16:41:26.973787Z","shell.execute_reply.started":"2021-06-30T16:41:26.492831Z","shell.execute_reply":"2021-06-30T16:41:26.973028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How we can improve our model?\n\nTo improve our model we can:\n* Increase hidden layers\n* Add more neurons in hidden layer \n* Change non-linear activation function\n* Find the ideal learning rate\n* Weight initialization\n* Change optimizer\n* Cross-Validate our data\n* Normalizing/Scaling data\n* Batch Normalization\n\nTo avoid overfitting we can:\n* use dropout method\n* set early stopping\n\nIt's a good idea to change one thing at a time and see if our model improves. I'm going to create a function where I will try these techniques to build a robust model.","metadata":{}},{"cell_type":"code","source":"def ann_model(X_train, X_test, y_train, y_true):\n    \n    # Create Early Stopping\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',patience=4,min_delta=0,verbose=1,\n        mode='max',baseline=0,restore_best_weights=True)\n    \n    # Create a model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(75,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(100,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(100,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(75, activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(9, activation='softmax')  # We have multi-class classification problem\n    ])\n    \n    # Compile the model\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # expect labels to be provide as integer\n                  optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                  metrics=['accuracy']\n                 )\n    \n    # Fit the model\n    history = model.fit(X_train, \n                        y_train, \n                        epochs=60,\n                        validation_data=(X_test, y_true),\n                        callbacks=[early_stop]) \n    \n    model.evaluate(X_test,y_true)\n    \n    return history, model","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:41:26.974823Z","iopub.execute_input":"2021-06-30T16:41:26.975246Z","iopub.status.idle":"2021-06-30T16:41:26.985352Z","shell.execute_reply.started":"2021-06-30T16:41:26.975215Z","shell.execute_reply":"2021-06-30T16:41:26.984476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nhistory, model_ann = ann_model(X_train=X_train_norm,\n                     X_test=X_test_norm,\n                     y_train=y_train,\n                     y_true=y_test)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-30T16:41:26.986542Z","iopub.execute_input":"2021-06-30T16:41:26.986984Z","iopub.status.idle":"2021-06-30T16:45:38.819834Z","shell.execute_reply.started":"2021-06-30T16:41:26.986935Z","shell.execute_reply":"2021-06-30T16:45:38.818718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot_history(history_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:45:38.821426Z","iopub.execute_input":"2021-06-30T16:45:38.821736Z","iopub.status.idle":"2021-06-30T16:45:39.30624Z","shell.execute_reply.started":"2021-06-30T16:45:38.821705Z","shell.execute_reply":"2021-06-30T16:45:39.304934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ðŸ”‘**Note:** Looks like we can increase number of epochs as val_loss is still lover than training set and accuracy of training and test set stays on the same level (in this case they are beautifully interwine together).","metadata":{}},{"cell_type":"code","source":"def find_ideal_lr(X_train, X_test, y_train, y_test):\n    \n    # Create the learning rate callback to find ideal learning rate\n    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n    \n    # Create a model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(75,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(100,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(100,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(75, activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(9, activation='softmax')  # We have multi-class classification problem\n    ])\n    \n    # Compile the model\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # expect labels to be provide as integer\n                  optimizer=tf.keras.optimizers.Adam(),\n                  metrics=['accuracy']\n                 )\n    \n    # Fit the model\n    history = model.fit(X_train, \n                        y_train, \n                        epochs=60,\n                        validation_data=(X_test, y_test),\n                        callbacks=[lr_scheduler])\n    \n    # Evaluate the model\n    model.evaluate(X_test,y_test)\n    \n    return history, model","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:45:39.308071Z","iopub.execute_input":"2021-06-30T16:45:39.308544Z","iopub.status.idle":"2021-06-30T16:45:39.319614Z","shell.execute_reply.started":"2021-06-30T16:45:39.308496Z","shell.execute_reply":"2021-06-30T16:45:39.31852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nhistory, _ = find_ideal_lr(X_train=X_train_norm,\n                           X_test=X_test_norm,\n                           y_train=y_train,\n                           y_test=y_test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T16:45:39.321133Z","iopub.execute_input":"2021-06-30T16:45:39.321468Z","iopub.status.idle":"2021-06-30T16:56:11.026478Z","shell.execute_reply.started":"2021-06-30T16:45:39.321435Z","shell.execute_reply":"2021-06-30T16:56:11.025273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_lr_history_df = pd.DataFrame(history.history)\nplot_history(find_lr_history_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:56:11.028462Z","iopub.execute_input":"2021-06-30T16:56:11.028927Z","iopub.status.idle":"2021-06-30T16:56:11.539621Z","shell.execute_reply.started":"2021-06-30T16:56:11.028877Z","shell.execute_reply":"2021-06-30T16:56:11.538432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the learning rate decay curve\nlrs = 1e-4 * (10**(tf.range(60)/20))\nplt.figure(figsize=(12,7))\nplt.semilogx(lrs, history.history['loss'])\nplt.axvline(x=1e-2, linestyle='--', color='red')\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"Loss\")\nplt.title(\"Finding the ideal learning rate\")\nplt.grid()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:56:11.540999Z","iopub.execute_input":"2021-06-30T16:56:11.541316Z","iopub.status.idle":"2021-06-30T16:56:12.571164Z","shell.execute_reply.started":"2021-06-30T16:56:11.541259Z","shell.execute_reply":"2021-06-30T16:56:12.57039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ann_model(X_train, X_test, y_train, y_test):\n    \n    # Create Early Stopping\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',patience=4,min_delta=0,verbose=1,\n        mode='max',baseline=0,restore_best_weights=True)\n    \n    # Create a model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(75,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(100,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(100,activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(75, activation='sigmoid'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(9, activation='softmax')  # We have multi-class classification problem\n    ])\n    \n    # Compile the model\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # expect labels to be provide as integer\n                  optimizer=tf.keras.optimizers.Adam(lr=0.01),  # ideal learning rate\n                  metrics=['accuracy']\n                 )\n    \n    # Fit the model\n    history = model.fit(X_train, \n                        y_train, \n                        epochs=60,\n                        validation_data=(X_test, y_test),\n                        callbacks=[early_stop]) \n    \n    # Evaluate the model\n    model.evaluate(X_test,y_test)\n    \n    return history, model\n\ntf.random.set_seed(42)\nhistory, ann = ann_model(X_train=X_train_norm,\n                         X_test=X_test_norm,\n                         y_train=y_train,\n                         y_test=y_test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T16:56:12.572701Z","iopub.execute_input":"2021-06-30T16:56:12.573335Z","iopub.status.idle":"2021-06-30T16:58:43.992666Z","shell.execute_reply.started":"2021-06-30T16:56:12.573268Z","shell.execute_reply":"2021-06-30T16:58:43.991646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot_history(history_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:58:43.994223Z","iopub.execute_input":"2021-06-30T16:58:43.99459Z","iopub.status.idle":"2021-06-30T16:58:44.497082Z","shell.execute_reply.started":"2021-06-30T16:58:43.994557Z","shell.execute_reply":"2021-06-30T16:58:44.496335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see our model has improved with less epochs,means using less comutation time. Now, I will try weights initialization to find out if this could improve my model. I'm also going to change metric which we're going to monitor as accuracy is not our goal, hence by observing accuracy we might not necessary get the best loss.","metadata":{}},{"cell_type":"code","source":"SEED=45\nEPOCHS=100\nBATCH_SIZE=512\nN_FOLDS=10\nN_CLASS=9","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:58:44.498228Z","iopub.execute_input":"2021-06-30T16:58:44.498566Z","iopub.status.idle":"2021-06-30T16:58:44.503057Z","shell.execute_reply.started":"2021-06-30T16:58:44.498532Z","shell.execute_reply":"2021-06-30T16:58:44.501913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_metric(y_true,y_pred):\n    cce = tf.keras.losses.SparseCategoricalCrossentropy()\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-06-30T16:58:44.50442Z","iopub.execute_input":"2021-06-30T16:58:44.504724Z","iopub.status.idle":"2021-06-30T16:58:44.516541Z","shell.execute_reply.started":"2021-06-30T16:58:44.504695Z","shell.execute_reply":"2021-06-30T16:58:44.515401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ann_model_2(X_train, X_test, y_train, y_test):\n    \n    # Create Early Stopping\n    es = tf.keras.callbacks.EarlyStopping(\n         monitor='val_custom_metric',patience=42,min_delta=0.0001,verbose=1,\n         mode='min',baseline=0,restore_best_weights=False)\n    \n    # Create weights initializer\n    weights_initializer = tf.keras.initializers.GlorotUniform(seed=SEED)\n    \n    # Create plateau\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',factor=0.04, patience=3,verbose=1,mode='min',cooldown=1)\n    \n    # Create a model\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(75,activation='sigmoid', kernel_initializer=weights_initializer),\n        tf.keras.layers.Dropout(0.2),\n        # tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(100,activation='sigmoid', kernel_initializer=weights_initializer),\n        tf.keras.layers.Dropout(0.2),\n        # tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(100,activation='sigmoid', kernel_initializer=weights_initializer),\n        tf.keras.layers.Dropout(0.2),\n        # tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(75, activation='sigmoid', kernel_initializer=weights_initializer),\n        tf.keras.layers.Dropout(0.2),\n        # tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(9, activation='softmax'), # We have multi-class classification problem\n    ])\n    \n    # Compile the model\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), # expect labels to be provide as integer\n                  optimizer=tf.keras.optimizers.Adam(lr=0.01),  # ideal learning rate\n                  metrics=[\"accuracy\",custom_metric]\n                 )\n    \n    # Fit the model\n    history = model.fit(X_train, \n                        y_train, \n                        epochs=EPOCHS,\n                        batch_size=BATCH_SIZE,\n                        validation_data=(X_test, y_test),\n                        callbacks=[plateau,es],\n                        verbose=1) \n    \n    # Evaluate the model\n    # model.evaluate(X_test,y_test)\n    \n    return history, model","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T16:58:44.518149Z","iopub.execute_input":"2021-06-30T16:58:44.518555Z","iopub.status.idle":"2021-06-30T16:58:44.531973Z","shell.execute_reply.started":"2021-06-30T16:58:44.518517Z","shell.execute_reply":"2021-06-30T16:58:44.530738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nhistory, ann = ann_model_2(X_train=X_train_norm, # use X_train when using BatchNormalization\n                           X_test=X_test_norm,\n                           y_train=y_train,\n                           y_test=y_test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T16:58:44.533391Z","iopub.execute_input":"2021-06-30T16:58:44.533708Z","iopub.status.idle":"2021-06-30T17:00:42.314742Z","shell.execute_reply.started":"2021-06-30T16:58:44.533681Z","shell.execute_reply":"2021-06-30T17:00:42.313843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nplot_history(history_df)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:00:42.316139Z","iopub.execute_input":"2021-06-30T17:00:42.316622Z","iopub.status.idle":"2021-06-30T17:00:42.791595Z","shell.execute_reply.started":"2021-06-30T17:00:42.31658Z","shell.execute_reply":"2021-06-30T17:00:42.790164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submision","metadata":{}},{"cell_type":"code","source":"# Scale our test data first\ntest_df_norm = sc.transform(test_df) # we only need to transform the data as we already trained our scaler\n# Make predictions\nann_y_preds = ann.predict(test_df_norm)\n# Paste the prediction for each class into sample df\nfor i,col in enumerate(classes_names):\n    sample_df[col] = ann_y_preds[:,i]\n    \nsample_df.to_csv(\"submission7.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:00:42.793443Z","iopub.execute_input":"2021-06-30T17:00:42.79393Z","iopub.status.idle":"2021-06-30T17:00:46.647006Z","shell.execute_reply.started":"2021-06-30T17:00:42.793879Z","shell.execute_reply":"2021-06-30T17:00:46.64577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:00:46.649213Z","iopub.execute_input":"2021-06-30T17:00:46.649584Z","iopub.status.idle":"2021-06-30T17:00:46.656679Z","shell.execute_reply.started":"2021-06-30T17:00:46.649548Z","shell.execute_reply":"2021-06-30T17:00:46.655447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\ntrain_df[\"kfold\"] = -1\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(X=train_df, y=train_df['num_target'])):\n    train_df.loc[ts_idx,\"kfold\"] = fold","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:00:46.658405Z","iopub.execute_input":"2021-06-30T17:00:46.658762Z","iopub.status.idle":"2021-06-30T17:00:46.750112Z","shell.execute_reply.started":"2021-06-30T17:00:46.658726Z","shell.execute_reply":"2021-06-30T17:00:46.749104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"kfold\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:00:46.75144Z","iopub.execute_input":"2021-06-30T17:00:46.751751Z","iopub.status.idle":"2021-06-30T17:00:46.763186Z","shell.execute_reply.started":"2021-06-30T17:00:46.75172Z","shell.execute_reply":"2021-06-30T17:00:46.762059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ann_model_kfolds(train, test):\n    \n    \"\"\"\n    Function performs cross-validation using StratifyKFolds.\n    \"\"\"  \n    \n    # Create place holders for our predictions\n    oof_train = np.zeros(shape=(train_df.shape[0], N_CLASS))\n    oof_preds = np.zeros(shape=(test_df.shape[0], N_CLASS))\n    history_dict = {}\n    test_fold_preds = {}\n    folds_acc = []\n    folds_loss = []\n    \n    for fold in range(N_FOLDS):\n        print(f\"=========FOLD_{fold+1}=========\")\n        t_df = train[train.kfold !=fold].reset_index(drop=True)\n        v_df = train[train.kfold ==fold].reset_index(drop=True)\n        \n        # Split into training and testing set\n        xtrain = t_df.drop([\"num_target\",'kfold'], axis=1).values\n        xvalid = v_df.drop([\"num_target\",\"kfold\"],axis=1).values\n        ytrain = t_df[\"num_target\"].values\n        yvalid = v_df[\"num_target\"].values\n        \n        # Normalize datasets\n        sc = MinMaxScaler()\n        xtrain_norm = sc.fit_transform(xtrain)\n        xvalid_norm = sc.transform(xvalid)\n        test_norm = sc.transform(test.values)\n        \n        # Time for our model\n        history, ann_model = ann_model_2(X_train=xtrain_norm,\n                                         X_test=xvalid_norm,\n                                         y_train=ytrain,\n                                         y_test=yvalid)\n        \n        # Save history for a model in specific fold\n        history_dict[f\"Fold_{fold+1}\"] = history\n        \n        # Make predictions for our model in a fold split \n        fold_y_preds = ann_model.predict(xvalid_norm)\n        fold_y_pred_test = ann_model.predict(test_norm)\n        \n        # Evaluate our model\n        model_eval = ann_model.evaluate(xvalid_norm, yvalid)\n        \n        # Print our our results\n        print(f\"Fold_{fold+1} Validation Accuracy={model_eval[1]}\")\n        print(f\"Fold_{fold+1} Validation Loss={model_eval[0]}\")\n        print(\"\\n\")\n        \n        # Save our predictions\n        folds_acc.append(model_eval[1])\n        folds_loss.append(model_eval[0])\n        \n        oof_train[v_df.index] = fold_y_preds\n        oof_preds += fold_y_pred_test\n        test_fold_preds[f\"fold_{fold+1}\"] = oof_preds\n        \n    return folds_acc, folds_loss, history_dict, oof_train, oof_preds, test_fold_preds","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T17:00:46.7647Z","iopub.execute_input":"2021-06-30T17:00:46.765076Z","iopub.status.idle":"2021-06-30T17:00:46.779457Z","shell.execute_reply.started":"2021-06-30T17:00:46.765042Z","shell.execute_reply":"2021-06-30T17:00:46.778184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        \nacc, loss, folds_history, train_pred, test_pred, test_fold_dict = ann_model_kfolds(train_df, test_df)\nprint(\"\\n\")\nprint(\"============Final Models Metrics==============\")\nprint(f\"Mean Accuracy after {N_FOLDS}_folds: {np.mean(acc):.2f}%\")\nprint(f\"Mean Loss after {N_FOLDS}_folds: {np.mean(loss)}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T17:00:46.781239Z","iopub.execute_input":"2021-06-30T17:00:46.781612Z","iopub.status.idle":"2021-06-30T17:23:00.31286Z","shell.execute_reply.started":"2021-06-30T17:00:46.781578Z","shell.execute_reply":"2021-06-30T17:23:00.311869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1,N_FOLDS):\n    temp_df = pd.DataFrame(folds_history[f\"Fold_{i}\"].history)\n    plot_history(temp_df,i)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:23:00.314223Z","iopub.execute_input":"2021-06-30T17:23:00.314561Z","iopub.status.idle":"2021-06-30T17:23:04.982493Z","shell.execute_reply.started":"2021-06-30T17:23:00.314528Z","shell.execute_reply":"2021-06-30T17:23:04.980579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit our n_folds model","metadata":{}},{"cell_type":"code","source":"test_pred = np.clip((test_pred / N_FOLDS), 1e-15, 1-1e-15)\nsub_id_df = pd.DataFrame(sample_df['id'], columns=['id'])\nsub_df = pd.DataFrame(test_pred, columns=lb.classes_)\nsub_concat_df = pd.concat([sub_id_df,sub_df], axis=1)\nsub_concat_df.to_csv(f\"sub_({N_FOLDS})_folds.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:23:04.98446Z","iopub.execute_input":"2021-06-30T17:23:04.984947Z","iopub.status.idle":"2021-06-30T17:23:06.95878Z","shell.execute_reply.started":"2021-06-30T17:23:04.984895Z","shell.execute_reply":"2021-06-30T17:23:06.957578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K.clear_session()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:23:06.9601Z","iopub.execute_input":"2021-06-30T17:23:06.960433Z","iopub.status.idle":"2021-06-30T17:23:06.966728Z","shell.execute_reply.started":"2021-06-30T17:23:06.960399Z","shell.execute_reply":"2021-06-30T17:23:06.965826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Different aproach","metadata":{}},{"cell_type":"code","source":"cat_tr_df = train_df.drop(['num_target','kfold'], axis=1).astype('category')\ncat_ts_df = test_df.astype('category')\ncat_tr_df['train'] = 1\ncat_ts_df['train'] = 0\n\ntr_ts_df = pd.concat([cat_tr_df, cat_ts_df])\ndummy_tr_ts_df = pd.get_dummies(tr_ts_df, drop_first=True)\n\ndummy_tr_df = dummy_tr_ts_df[dummy_tr_ts_df['train']==1]\ndummy_ts_df = dummy_tr_ts_df[dummy_tr_ts_df['train']==0]\n\ndummy_tr_df = pd.concat([dummy_tr_df,train_df[['num_target','kfold']]], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:23:06.967916Z","iopub.execute_input":"2021-06-30T17:23:06.968206Z","iopub.status.idle":"2021-06-30T17:23:15.181282Z","shell.execute_reply.started":"2021-06-30T17:23:06.968177Z","shell.execute_reply":"2021-06-30T17:23:15.180137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_COMPONENTS = 75\n\nfrom sklearn.decomposition import PCA, SparsePCA","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:23:15.182808Z","iopub.execute_input":"2021-06-30T17:23:15.18315Z","iopub.status.idle":"2021-06-30T17:23:15.315945Z","shell.execute_reply.started":"2021-06-30T17:23:15.183115Z","shell.execute_reply":"2021-06-30T17:23:15.314859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=N_COMPONENTS, random_state=SEED).fit(dummy_tr_df.drop(['num_target','kfold'],axis=1))\n#sparse_pca = SparsePCA(n_components=N_COMPONENTS, random_state=SEED).fit(dummy_tr_df.drop(['num_target','kfold'],axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:23:15.317311Z","iopub.execute_input":"2021-06-30T17:23:15.317742Z","iopub.status.idle":"2021-06-30T17:24:07.646736Z","shell.execute_reply.started":"2021-06-30T17:23:15.317709Z","shell.execute_reply":"2021-06-30T17:24:07.645202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create training and test datasets with features from PCA\npca_tr_df = pd.DataFrame(pca.transform(dummy_tr_df.drop(['num_target','kfold'], axis=1)),\n                         columns=[f\"feature_{i}\" for i in range(N_COMPONENTS)])\n\npca_ts_df = pd.DataFrame(pca.transform(dummy_ts_df),\n                         columns = [f\"feature_{i}\" for i in range(N_COMPONENTS)])\n\n# Create training and test dataset with features from SparsePCA\n#spca_tr_df = pd.DataFrame(sparse_pca.transform(dummy_tr_df.drop(['num_target','kfold'], axis=1)),\n                          #columns=[f\"feature_{i}\" for i in range(N_COMPONENTS)])\n\n#spca_ts_df = pd.DataFrame(sparse_pca.transform(dummy_ts_df)),\n                          #columns=[f\"feature_{i}\" for i in range(N_COMPONENTS)])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:24:07.648557Z","iopub.execute_input":"2021-06-30T17:24:07.648889Z","iopub.status.idle":"2021-06-30T17:24:18.182181Z","shell.execute_reply.started":"2021-06-30T17:24:07.648853Z","shell.execute_reply":"2021-06-30T17:24:18.180982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_tr_df['kfold'] = -1\npca_tr_df['num_target'] = train_df['num_target'].values\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(X=pca_tr_df, y=pca_tr_df['num_target'])):\n    pca_tr_df.loc[ts_idx,'kfold'] = fold","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:24:18.183883Z","iopub.execute_input":"2021-06-30T17:24:18.184707Z","iopub.status.idle":"2021-06-30T17:24:18.312721Z","shell.execute_reply.started":"2021-06-30T17:24:18.184648Z","shell.execute_reply":"2021-06-30T17:24:18.311898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_tr_df.shape, pca_ts_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:24:18.319835Z","iopub.execute_input":"2021-06-30T17:24:18.320534Z","iopub.status.idle":"2021-06-30T17:24:18.327659Z","shell.execute_reply.started":"2021-06-30T17:24:18.320476Z","shell.execute_reply":"2021-06-30T17:24:18.326689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc, loss, folds_history, train_pred, test_pred, test_fold_dict = ann_model_kfolds(pca_tr_df, \n                                                                                   pca_ts_df)\nprint(\"\\n\")\nprint(\"============Final Metrics for a new aproach ==============\")\nprint(f\"Mean Accuracy after {N_FOLDS}_folds: {np.mean(acc):.2f}%\")\nprint(f\"Mean Loss after {N_FOLDS}_folds: {np.mean(loss)}\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-30T17:24:18.329407Z","iopub.execute_input":"2021-06-30T17:24:18.330089Z","iopub.status.idle":"2021-06-30T17:46:45.573652Z","shell.execute_reply.started":"2021-06-30T17:24:18.330051Z","shell.execute_reply":"2021-06-30T17:46:45.572545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = test_pred / N_FOLDS\nsub_id_df = pd.DataFrame(sample_df['id'], columns=['id'])\nsub_df = pd.DataFrame(test_pred, columns=lb.classes_)\nsub_concat_df = pd.concat([sub_id_df,sub_df], axis=1)\nsub_concat_df.to_csv(f\"sub_({N_FOLDS})_folds_dummy_and_pca.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T17:46:45.574948Z","iopub.execute_input":"2021-06-30T17:46:45.575222Z","iopub.status.idle":"2021-06-30T17:46:47.518042Z","shell.execute_reply.started":"2021-06-30T17:46:45.575194Z","shell.execute_reply":"2021-06-30T17:46:47.516863Z"},"trusted":true},"execution_count":null,"outputs":[]}]}