{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c481ecb2-3a73-01a2-332a-df8ee82d5cd6"},"source":"From this [great forum post][1] by Achal we noticed that there is quite some correlation between the numerical features. Therefore I wanted to explore to how many components we could reduce the feature subspace without losing too much of the explained variance.\n\n\n  [1]: https://www.kaggle.com/achalshah/allstate-claims-severity/allstate-feature-analysis-python"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7aaf62a2-4cc9-5c00-860c-2b7c6e38a69f"},"outputs":[],"source":"%matplotlib inline\nimport numpy as np \nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3224761-6012-e4b7-a846-ebbe9f2202b3"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"399de919-911a-39e6-55c4-45b4942b5cf5"},"source":"**Checking how many numercial and categorical features + putting the colnames in a list per data type**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c24570a7-14f2-fb5e-eae5-96eab7ce49a7"},"outputs":[],"source":"numFeatures = []\ncatFeatures = []\n\nfor col, val in train.iloc[0,:].iteritems():\n    if type(val) is not str:\n        numFeatures.append(col)\n    elif type(val) is str:\n        catFeatures.append(col)\n        \n# Remove id and loss from the numFeatures\nnumFeatures.remove('id')\nnumFeatures.remove('loss')\n        \nprint(len(numFeatures), 'Numerical Features:', numFeatures, \"\\n\")\nprint(len(catFeatures), 'Categorical Features:', catFeatures)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d5dfe05-e83e-99fb-c18f-100dd8b781aa"},"source":"**Standardizing the numerical features before performing PCA**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"405591e5-9771-459e-5e9e-aeccf11b91c2"},"outputs":[],"source":"sc = StandardScaler()\ntrain_nums_std = sc.fit_transform(train[numFeatures])"},{"cell_type":"markdown","metadata":{"_cell_guid":"a9dae0ed-04c8-caa1-0553-2f067fe6aa60"},"source":"**PCA**<br>\nSet n_components to None to keep all principal components and their explained variance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18f4573a-6eca-7c11-1e74-35366dfb60db"},"outputs":[],"source":"pca = PCA(n_components=None)\ntrain_nums_pca = pca.fit_transform(train_nums_std)\nvarExp = pca.explained_variance_ratio_"},{"cell_type":"markdown","metadata":{"_cell_guid":"b531e383-b64b-2736-1214-d8bad7a70952"},"source":"**Plot the cumulative explained variance as a function of the number of components**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa3fec90-c2fc-6d43-3ff3-de7d5a231daf"},"outputs":[],"source":"cumVarExplained = []\nnb_components = []\ncounter = 1\nfor i in varExp:\n    cumVarExplained.append(varExp[0:counter].sum())\n    nb_components.append(counter)\n    counter += 1\n\nplt.subplots(figsize=(8, 6))\nplt.plot(nb_components, cumVarExplained, 'bo-')\nplt.ylabel('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.ylim([0.0, 1.1])\nplt.xticks(np.arange(1, len(nb_components), 1.0))\nplt.yticks(np.arange(0.0, 1.1, 0.10))"},{"cell_type":"markdown","metadata":{"_cell_guid":"13c1861d-84eb-37ae-c4c9-d43565e0fc05"},"source":"With 7 components we already explain more than 90% of all variance in the features. So we could reduce the number of features to half of the original numerical features."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}