{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"12bf2fd3-c34a-d53f-240a-5dbd4feab113"},"source":"This is a take on Scirpus' MCMC notebook. Most of it is similar until the end in which I compare it with OOF stacking methods."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71120554-87aa-e461-d0d3-d91040b2c482"},"outputs":[],"source":"import numpy as np\nimport pymc3 as pm\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"78056b59-0436-6f9f-3f42-39d58be4ae61"},"source":"The following cell creates the two models with noise based on a target.\nOne should note that the first model has more noise than the second model so one would expect model 1 to perform worse than model 2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfb499a7-5137-637a-04c1-c423f1786764"},"outputs":[],"source":"size = 500\ntrue_intercept = 1\ntrue_slope = 2\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\nmodel1 = true_regression_line + np.random.normal(scale=.5, size=size) #Noisy\nmodel2 = true_regression_line + np.random.normal(scale=.2, size=size) #Less Noisy"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0e79a4d-483a-c265-1e60-3c9465546af6"},"outputs":[],"source":"np.random.seed = 0\npermutation_set = np.random.permutation(size)\ntrain_set = permutation_set[0:size//2]\ntest_set = permutation_set[size//2:size]"},{"cell_type":"markdown","metadata":{"_cell_guid":"d32a7bf8-87a9-4a08-0b8b-c30ed073bfe7"},"source":"Let us see what the MAE looks like"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e290451b-e23d-d38f-4308-9a96a4cd08a9"},"outputs":[],"source":"print(mean_absolute_error(true_regression_line[test_set],model1[test_set]))\nprint(mean_absolute_error(true_regression_line[test_set],model2[test_set]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd672844-7d55-03b1-7afa-c686db58a406"},"source":"As expected the noisier model does worse"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c14c03a-35c4-d4b4-e70c-cb7cae364e67"},"source":"Now let us look at the straight average"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4dd744e-f03c-21ac-bdc2-3408a678dd1a"},"outputs":[],"source":"print(mean_absolute_error(true_regression_line[test_set],(model1*.5+model2*.5)[test_set]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ceb449e7-e977-7f0f-98eb-b55ac402722d"},"source":"As one can see this isn't as good as our top model"},{"cell_type":"markdown","metadata":{"_cell_guid":"ebcb3903-ed60-a403-51f2-30a94120d5d3"},"source":"Now comes the cool part.  We are going to use MCMC to draw samples from our data and get stats on how we can obtain a model that gets the best out of our raw models.\n\nImportant:  Please look at the documentation [here][1] (https://pymc-devs.github.io/pymc3/index.html) for details\n\n\n  [1]: https://pymc-devs.github.io/pymc3/index.html"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a23b1bda-4853-a214-c2d9-eb8bdc30fe1a"},"outputs":[],"source":"data = dict(x1=model1[train_set], x2=model2[train_set], y=true_regression_line[train_set])\nwith pm.Model() as model:\n    # specify glm and pass in data. The resulting linear model, its likelihood and \n    # and all its parameters are automatically added to our model.\n    pm.glm.glm('y ~ x1 + x2', data)\n    step = pm.NUTS() # Instantiate MCMC sampling algorithm\n    trace = pm.sample(2000, step, progressbar=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b0e9af59-4534-ec33-a358-03f40ad0419b"},"source":"It takes a while - now is time to look at what goodness it gives to us"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ea87feb-96d1-ac52-09b2-11a90707ad5d"},"outputs":[],"source":"pm.traceplot(trace, figsize=(7,7))\nplt.tight_layout();"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1d921bb-9c51-9faf-8608-b07494936936"},"source":"One can see that for every drawn sample it gives the parameter values for the intercept, x1 and x2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3afeac54-5894-738d-5d6c-93a8f87e321f"},"outputs":[],"source":"intercept = np.median(trace.Intercept)\nprint(intercept)\nx1param = np.median(trace.x1)\nprint(x1param)\nx2param = np.median(trace.x2)\nprint(x2param)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fe9804b1-df81-a387-1780-a7f325cee91a"},"source":"I created a quick imitation of test/train split in order to compare of OOF ensembling methods."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2e33ede-bcbe-3b59-6ec3-cb9485de7bb4"},"outputs":[],"source":"model1_train = model1[train_set]\nmodel2_train = model2[train_set]\nx_train = np.vstack((model1_train, model2_train)).T\n\nmodel1_test = model1[test_set].T\nmodel2_test = model2[test_set].T\nx_test = np.vstack((model1_test, model2_test)).T\n\ny = true_regression_line[train_set]"},{"cell_type":"markdown","metadata":{"_cell_guid":"59637cab-3cda-d32e-e64f-1bf87e3de9bf"},"source":"Now to check if Linear Regression  finds a similar solution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"113ad939-6418-d61a-f199-5227d8a1842e"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nclfLR = LinearRegression()\nclfLR.fit(x_train, y)\ny_pred_LR = clfLR.predict(x_test)\nprint(clfLR.intercept_)\nprint(clfLR.coef_[0])\nprint(clfLR.coef_[1])"},{"cell_type":"markdown","metadata":{"_cell_guid":"c7b85438-ea19-cb2a-c6c7-c581f934f048"},"source":"And a simple neural net."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"25a11197-3208-a64a-2d7e-f8fdfe78503e"},"outputs":[],"source":"from sklearn.neural_network import MLPRegressor\nclfMLP = MLPRegressor()\nclfMLP.fit(x_train, y)\ny_pred_MLP = clfMLP.predict(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c2b94f4e-fb9d-69e7-16d5-c0f4ee80a0a5"},"source":"And a GBM."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f6314985-8183-32a4-a865-ad00ac975a7e"},"outputs":[],"source":"from sklearn.ensemble import GradientBoostingRegressor\nclfGBR = GradientBoostingRegressor(random_state=0)\nclfGBR.fit(x_train, y)\ny_pred_GBR = clfGBR.predict(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d4c5b45-5e05-1d32-5625-1ba880839cb0"},"source":"Now let's compare:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3f7493b-2612-b30b-af02-6dd774e3b292"},"outputs":[],"source":"print('Model 1:',mean_absolute_error(true_regression_line[test_set],model1[test_set]))\nprint('Model 2:', mean_absolute_error(true_regression_line[test_set],model2[test_set]))\nprint('Average:',mean_absolute_error(true_regression_line[test_set],(model1*.5+model2*.5)[test_set]))\nprint('MCMC:',mean_absolute_error(true_regression_line[test_set],\n                                  (intercept+x1param*model1+x2param*model2)[test_set]))\nprint('LR:',mean_absolute_error(true_regression_line[test_set], y_pred_LR))\nprint('MLP:',mean_absolute_error(true_regression_line[test_set], y_pred_MLP))\nprint('GBM:',mean_absolute_error(true_regression_line[test_set], y_pred_GBR))"},{"cell_type":"markdown","metadata":{"_cell_guid":"4a54d6c3-5642-8490-9480-5c73f42965fd"},"source":"Looks like MCMC did not outperform linear regression, however it was pretty close. Both of them come up with similar coefficients. Additionally, MCMC gives you a good sense of the standard deviation, although it runs significantly slower. Seems like a good tool to use the in the ensembling tool belt! Thanks Scirpus!"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}