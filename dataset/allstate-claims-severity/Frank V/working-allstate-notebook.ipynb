{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90c03c15-926e-a4c4-9924-0c6a460e8a19","_active":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39dad488-13ef-c471-13dc-44d52903c98e","_active":false},"outputs":[],"source":"#../input/test.csv\n#../input/train.csv\n\ndata_train = pd.read_csv('../input/train.csv')\ndata_train.head()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab8a1d5b-e515-d78a-9b65-217f429f23e9","_active":false},"outputs":[],"source":"# Print out all the feature names\nlist(data_train.columns.values)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37cf06f2-2936-f00b-5fb7-609fe41c97d4","_active":false},"outputs":[],"source":"# cat1 - cat116 & cont1 - cont14 (and the \"loss\" column)\n\n# check what kind of data for each category\nprint (data_train.cat1.head(1))\nprint (data_train.cat116.head(1))\nprint (data_train.cont1.head(1))\nprint (data_train.cont14.head(1))","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"986addfd-6bba-210e-0f3a-bf5070ed30c8","_active":false},"outputs":[],"source":"# For testing syntax - delete this cell\nx = \"cat1\"\nfor test in data_train[x]:\n    pass\n    #print (test)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3044f7bb-9557-bed5-c5e2-3e22ee3d75f0","_active":false},"outputs":[],"source":"# Check Disparity of Categories between cat1 - cat116\nfor x in range(1,117):\n    disparity = []\n    cat = \"cat\"\n    proper_string = (str(cat)+str(x))\n    for test in data_train[proper_string]:\n        if test in disparity:\n            pass\n        else:\n            disparity.append(test)\n    print (\"Len of List For: \",proper_string,\"is \", len(disparity))        \n            \n        ","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93e505d8-a124-dbeb-cea0-0f8b012edf62","_active":false},"outputs":[],"source":"# From prior cell, note that cat1-cat72 are binary, then disparity increases as we increment up the \n# list of features\n\n# Now lets print cat116 again for good measure\nprint (data_train.cat116)\n\n# Do we want to convert these letters to numbers and run some linear regressions?\n# The categories do not seem to increment (i.e. the 27th catergory would be aa, then ab, then ac...etc)\n# So the question becomes what is the importance of this lettering convention\n# also, is it the same amongst the other columns of data?\n# For now we will skip this puzzle, and move on to the numeric data","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3301dd4c-e9eb-8aaa-b3bf-736623d5dda0","_active":false},"outputs":[],"source":"# Check Disparity of Categories between cont1 - cont14\nfor x in range(1,15):\n    disparity = []\n    cat = \"cont\"\n    proper_string = (str(cat)+str(x))\n    for test in data_train[proper_string]:\n        if test in disparity:\n            pass\n        else:\n            disparity.append(test)\n    print (\"Len of List For: \",proper_string,\"is \", len(disparity))    \n    \n    # I would not have thought there would be so little disparity, considering this data is \n    # six digits. Might be a programming rounding error, need to dig deeper in next cell","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36943ab4-539f-8660-c108-691150001a73","_active":false},"outputs":[],"source":"# cont 3 had the least disparity, so lets examine it\n\nprint (data_train['cont2'])\n\n#for x in data_train['cont2']:\n    #print (x)\n    \n    #### Seems to pass the smell test ","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e4ed5c5-0cb7-e766-5bef-bdf9cd3ea484","_active":false},"outputs":[],"source":"# Now lets print a linear regression\n\nimport seaborn as sns\nsns.jointplot(data_train['cont1'], data_train['loss'], kind='reg', size=6)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccd877f8-fae5-6ca6-9414-62eb151a1799","_active":false},"outputs":[],"source":"# ok not much to discern from cont1...lets look at the loss data a little\n\nprint (data_train['loss'].mean(), \"Mean\")\nprint (data_train['loss'].median(), \"Median\")\nprint (data_train['loss'].max(), \"Maximum\")\nprint (data_train['loss'].min(), \"Minimum\")","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95a1d872-d482-ebf6-db17-de27d092e5ec","_active":false},"outputs":[],"source":"import numpy as np\n# Heat Maps?\n# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..cont14 into deciles\ndata_train['cont14_decile'] = pd.qcut(data_train['cont14'], 10, labels=False) + 1\n#data_train.head()\n\nfrom collections import defaultdict\n#BE_ME_Portfolio = [x for x in np.arange(1, 10, 1)]\n#ME_Portfolio = [x for x in np.arange(1, 10, 1)]\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont14_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the monthly return values\n#monthly_returns = defaultdict(dict)\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n#for decile_BE in BE_ME_Portfolio:\n    for decile_cont14 in Cont14_Data:\n    #for decile_ME in ME_Portfolio:    \n        #Portfolio = results.loc[(results['BE_ME_Decile'] == decile_BE) & (results['ME_Decile'] == decile_ME)]\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont14_decile'] == decile_cont14)]\n        #monthly_return = np.mean(Portfolio['Month'])\n        Average_Loss = np.sum(New_Frame['loss'])\n        #monthly_returns[decile_BE][decile_ME] = monthly_return\n        Claimed_Loss_Average[decile_loss][decile_cont14] = Average_Loss\n        \n#monthly_returns = pd.DataFrame(monthly_returns)\nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\n#monthly_returns.index.name = \"ME Decile\"\nClaimed_Loss_Average.index.name = \"cont14 decile\"\n#monthly_returns.columns.name = \"BE_ME Decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n#print monthly_returns.head()\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont14 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n    #savefig('sample.png')\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print monthly_returns\n\n### Note that I used the sum of the losses claimed in the respective deciles\n\n# Conclusion - It looks like the tails of cont14 cause the most variance in claims","execution_state":"idle"},{"metadata":{"_cell_guid":"946d7e6a-2250-f960-bd78-6ac70a2fd265","_active":false,"collapsed":false},"source":"import numpy as np\n# Heat Maps?\n# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont1_decile'] = pd.qcut(data_train['cont1'], 10, labels=False) + 1\n#data_train.head()\n\nfrom collections import defaultdict\n#BE_ME_Portfolio = [x for x in np.arange(1, 10, 1)]\n#ME_Portfolio = [x for x in np.arange(1, 10, 1)]\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont1_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the monthly return values\n#monthly_returns = defaultdict(dict)\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n#for decile_BE in BE_ME_Portfolio:\n    for decile_cont1 in Cont1_Data:\n    #for decile_ME in ME_Portfolio:    \n        #Portfolio = results.loc[(results['BE_ME_Decile'] == decile_BE) & (results['ME_Decile'] == decile_ME)]\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont1_decile'] == decile_cont1)]\n        #monthly_return = np.mean(Portfolio['Month'])\n        Average_Loss = np.sum(New_Frame['loss'])\n        #monthly_returns[decile_BE][decile_ME] = monthly_return\n        Claimed_Loss_Average[decile_loss][decile_cont1] = Average_Loss\n        \n#monthly_returns = pd.DataFrame(monthly_returns)\nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\n#monthly_returns.index.name = \"ME Decile\"\nClaimed_Loss_Average.index.name = \"cont1 decile\"\n#monthly_returns.columns.name = \"BE_ME Decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n#print monthly_returns.head()\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont1 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n    #savefig('sample.png')\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print monthly_returns\n\n### Note that I used the sum of the losses claimed in the respective deciles\n\n# Conclusion - It looks like the tails of cont14 cause the most variance in claims","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"2db1f589-02c2-cbb0-ae0c-9fb4ea866542","_active":false,"collapsed":false},"source":"import numpy as np\n# Heat Maps?\n# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont2_decile'] = pd.qcut(data_train['cont2'], 10, labels=False) + 1\n#data_train.head()\n\nfrom collections import defaultdict\n#BE_ME_Portfolio = [x for x in np.arange(1, 10, 1)]\n#ME_Portfolio = [x for x in np.arange(1, 10, 1)]\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont2_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the monthly return values\n#monthly_returns = defaultdict(dict)\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n#for decile_BE in BE_ME_Portfolio:\n    for decile_cont2 in Cont2_Data:\n    #for decile_ME in ME_Portfolio:    \n        #Portfolio = results.loc[(results['BE_ME_Decile'] == decile_BE) & (results['ME_Decile'] == decile_ME)]\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont2_decile'] == decile_cont2)]\n        #monthly_return = np.mean(Portfolio['Month'])\n        Average_Loss = np.sum(New_Frame['loss'])\n        #monthly_returns[decile_BE][decile_ME] = monthly_return\n        Claimed_Loss_Average[decile_loss][decile_cont1] = Average_Loss\n        \n#monthly_returns = pd.DataFrame(monthly_returns)\nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\n#monthly_returns.index.name = \"ME Decile\"\nClaimed_Loss_Average.index.name = \"cont2 decile\"\n#monthly_returns.columns.name = \"BE_ME Decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n#print monthly_returns.head()\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont2 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n    #savefig('sample.png')\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print monthly_returns\n\n### Note that I used the sum of the losses claimed in the respective deciles\n\n# Conclusion - It looks like the tails of cont14 cause the most variance in claims","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"1e589e0d-cfe0-a245-83aa-7be96d1418db","_active":false,"collapsed":false},"source":"import numpy as np\n# Heat Maps?\n# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont3_decile'] = pd.qcut(data_train['cont3'], 10, labels=False) + 1\n#data_train.head()\n\nfrom collections import defaultdict\n#BE_ME_Portfolio = [x for x in np.arange(1, 10, 1)]\n#ME_Portfolio = [x for x in np.arange(1, 10, 1)]\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont3_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the monthly return values\n#monthly_returns = defaultdict(dict)\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n#for decile_BE in BE_ME_Portfolio:\n    for decile_cont3 in Cont3_Data:\n    #for decile_ME in ME_Portfolio:    \n        #Portfolio = results.loc[(results['BE_ME_Decile'] == decile_BE) & (results['ME_Decile'] == decile_ME)]\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont3_decile'] == decile_cont3)]\n        #monthly_return = np.mean(Portfolio['Month'])\n        Average_Loss = np.sum(New_Frame['loss'])\n        #monthly_returns[decile_BE][decile_ME] = monthly_return\n        Claimed_Loss_Average[decile_loss][decile_cont1] = Average_Loss\n        \n#monthly_returns = pd.DataFrame(monthly_returns)\nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\n#monthly_returns.index.name = \"ME Decile\"\nClaimed_Loss_Average.index.name = \"cont3 decile\"\n#monthly_returns.columns.name = \"BE_ME Decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n#print monthly_returns.head()\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont3 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n    #savefig('sample.png')\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print monthly_returns\n\n### Note that I used the sum of the losses claimed in the respective deciles\n\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e3fe9309-f32c-9ae5-f017-f71812db78d1","_active":false,"collapsed":false},"source":"import numpy as np\n# Heat Maps?\n# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont4_decile'] = pd.qcut(data_train['cont4'], 10, labels=False) + 1\n#data_train.head()\n\nfrom collections import defaultdict\n#BE_ME_Portfolio = [x for x in np.arange(1, 10, 1)]\n#ME_Portfolio = [x for x in np.arange(1, 10, 1)]\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont4_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the monthly return values\n#monthly_returns = defaultdict(dict)\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n#for decile_BE in BE_ME_Portfolio:\n    for decile_cont4 in Cont4_Data:\n    #for decile_ME in ME_Portfolio:    \n        #Portfolio = results.loc[(results['BE_ME_Decile'] == decile_BE) & (results['ME_Decile'] == decile_ME)]\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont4_decile'] == decile_cont4)]\n        #monthly_return = np.mean(Portfolio['Month'])\n        Average_Loss = np.sum(New_Frame['loss'])\n        #monthly_returns[decile_BE][decile_ME] = monthly_return\n        Claimed_Loss_Average[decile_loss][decile_cont1] = Average_Loss\n        \n#monthly_returns = pd.DataFrame(monthly_returns)\nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\n#monthly_returns.index.name = \"ME Decile\"\nClaimed_Loss_Average.index.name = \"cont4 decile\"\n#monthly_returns.columns.name = \"BE_ME Decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n#print monthly_returns.head()\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont4 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n    #savefig('sample.png')\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print monthly_returns\n\n### Note that I used the sum of the losses claimed in the respective deciles\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"2ef421a3-687a-9713-0de2-8910148fe297","_active":false,"collapsed":false},"source":"\n\nfrom collections import defaultdict\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont5_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the loss claim values\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n    for decile_cont5 in Cont5_Data:\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont5_decile'] == decile_cont4)]\n        Average_Loss = np.sum(New_Frame['loss'])\n        Claimed_Loss_Average[decile_loss][decile_cont1] = Average_Loss\n        \nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\nClaimed_Loss_Average.index.name = \"cont4 decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont4 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print Claimed_Loss_Average\n\n### Note that I used the sum of the losses claimed in the respective deciles","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"a12ad6be-9b02-d57a-6ccb-7c056ae00bd8","_active":false,"collapsed":false},"source":"# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont6_decile'] = pd.qcut(data_train['cont6'], 10, labels=False) + 1\n\nfrom collections import defaultdict\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont6_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the loss claim values\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n    for decile_cont6 in Cont6_Data:\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont6_decile'] == decile_cont6)]\n        Average_Loss = np.sum(New_Frame['loss'])\n        Claimed_Loss_Average[decile_loss][decile_cont6] = Average_Loss # Source of error (fix this in prior cells)\n        \nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\nClaimed_Loss_Average.index.name = \"cont6 decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont6 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print Claimed_Loss_Average\n\n### Note that I used the sum of the losses claimed in the respective deciles\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"91b8a3f1-ec64-d027-ee07-9c5a62ce9c53","_active":false,"collapsed":false},"source":"# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont7_decile'] = pd.qcut(data_train['cont7'], 10, labels=False) + 1\n\nfrom collections import defaultdict\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont7_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the loss claim values\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n    for decile_cont7 in Cont7_Data:\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont7_decile'] == decile_cont7)]\n        Average_Loss = np.sum(New_Frame['loss'])\n        Claimed_Loss_Average[decile_loss][decile_cont7] = Average_Loss # Source of error (fix this in prior cells)\n        \nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\nClaimed_Loss_Average.index.name = \"cont7 decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont7 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print Claimed_Loss_Average\n\n### Note that I used the sum of the losses claimed in the respective deciles\n","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"92c52173-5254-968d-a025-2fb4f84d6382","_active":false,"collapsed":false},"source":"# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont8_decile'] = pd.qcut(data_train['cont8'], 10, labels=False) + 1\n\nfrom collections import defaultdict\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont8_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the loss claim values\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n    for decile_cont8 in Cont8_Data:\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont8_decile'] == decile_cont8)]\n        Average_Loss = np.sum(New_Frame['loss'])\n        Claimed_Loss_Average[decile_loss][decile_cont8] = Average_Loss # Source of error (fix this in prior cells)\n        \nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\nClaimed_Loss_Average.index.name = \"cont8 decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont8 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print Claimed_Loss_Average\n\n### Note that I used the sum of the losses claimed in the respective deciles","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"69092dfc-1b62-b786-8a41-89b4daec10ed","_active":false,"collapsed":false},"source":"# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont9_decile'] = pd.qcut(data_train['cont9'], 10, labels=False) + 1\n\nfrom collections import defaultdict\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont9_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the loss claim values\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n    for decile_cont9 in Cont9_Data:\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont9_decile'] == decile_cont9)]\n        Average_Loss = np.sum(New_Frame['loss'])\n        Claimed_Loss_Average[decile_loss][decile_cont9] = Average_Loss # Source of error (fix this in prior cells)\n        \nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\nClaimed_Loss_Average.index.name = \"cont9 decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont9 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print Claimed_Loss_Average\n\n### Note that I used the sum of the losses claimed in the respective deciles","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e4efd373-8c2e-ec89-b7c6-3ae60c4d6800","_active":true,"collapsed":false},"source":"# First split the loss data into deciles\ndata_train['loss_decile'] = pd.qcut(data_train['loss'], 10, labels=False) + 1\n# Now split ..continous data into deciles\ndata_train['cont10_decile'] = pd.qcut(data_train['cont10'], 10, labels=False) + 1\n\nfrom collections import defaultdict\nLoss_Data = [x for x in np.arange(1, 10, 1)]\nCont10_Data = [x for x in np.arange(1, 10, 1)]\n\n#: Create a dictionary to hold all the loss claim values\nClaimed_Loss_Average = defaultdict(dict)\n\nfor decile_loss in Loss_Data:\n    for decile_cont10 in Cont10_Data:\n        New_Frame = data_train.loc[(data_train['loss_decile'] == decile_loss) & (data_train['cont10_decile'] == decile_cont10)]\n        Average_Loss = np.sum(New_Frame['loss'])\n        Claimed_Loss_Average[decile_loss][decile_cont10] = Average_Loss # Source of error (fix this in prior cells)\n        \nClaimed_Loss_Average = pd.DataFrame(Claimed_Loss_Average)\nClaimed_Loss_Average.index.name = \"cont10 decile\"\nClaimed_Loss_Average.columns.name = \"Loss Decile\"\n\n\nimport matplotlib.pyplot as pyplot\n\ndef heat_map(df):\n    fig = pyplot.figure()\n    ax = fig.add_subplot(111)\n    axim = ax.imshow(df.values,cmap = pyplot.get_cmap('RdYlGn'), interpolation = 'nearest')\n    ax.set_xlabel(df.columns.name)\n    ax.set_xticks(np.arange(len(df.columns)))\n    ax.set_xticklabels(list(df.columns))\n    ax.set_ylabel(df.index.name)\n    ax.set_yticks(np.arange(len(df.index)))\n    ax.set_yticklabels(list(df.index))\n    ax.set_title(\"Cont10 impact on Claim Losses \")\n    pyplot.colorbar(axim)\n\n    \n#: Plot our heatmap\nheat_map(Claimed_Loss_Average)\n#print Claimed_Loss_Average\n\n### Note that I used the sum of the losses claimed in the respective deciles","execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e8f0218b-65d0-2fac-4ff3-0b112f560d5e","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"8601231b-ae0d-a97b-6723-419a9df953ab","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"a980ba28-38f7-afb6-5ea1-26fe8763e02d","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"f1c0a0f4-4180-d61a-42d5-39049d879f00","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"42ac23f9-6341-43bb-b6ff-e53bc7776741","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"cc413bdd-b092-5867-bbb6-577b6d6a1dc6","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"787b6183-6eab-ed3c-00be-eea3dcf633e0","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e101b66c-2a81-dbf8-87fa-c52dbc9bc1e5","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"d1d2a97e-6450-5dbb-c87b-537453ab87be","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"29eee20f-03f2-ae12-e299-7739a6050308","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"13538346-d2a2-542d-84c1-05ebbc89016c","_active":false,"collapsed":false},"source":null,"execution_count":null,"cell_type":"code","outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e29d7463-ace0-3d56-335a-fb6d74698c69","_active":false},"outputs":[],"source":"# Now we want to test all the data at once, a neural network is probably a good idea\n\n# to do for NN\n\n# Convert \"cat\" data to numbers\n\n\n\n\n# Convert loss data to 0-1\n# Fix bug in tensorflow","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"993b0d0c-0a58-7dfd-306c-565d04bb1532","_active":false},"outputs":[],"source":"# Data sets\nIRIS_TRAINING = \"../input/train.csv\"\nIRIS_TEST = \"../input/test.csv\"\n\nwith open(\"../input/test.csv\",'r') as f:\n    with open(\"updated_test.csv\",'w') as f1:\n        f.next() # skip header line\n        for line in f:\n            f1.write(line)\n            \n            ","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"705c721a-a666-04f3-88a9-0c3604e2bc88","_active":false},"outputs":[],"source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\n# Data sets\nIRIS_TRAINING = \"../input/train.csv\"\nIRIS_TEST = \"../input/test.csv\"\n\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING,\n                                                                  target_dtype=np.int,\n                                                                  features_dtype=np.float32,\n                                                                  target_column=-1)\ntest_set     = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST,\n                                                                  target_dtype=np.int,\n                                                                  features_dtype=np.float32,\n                                                                  target_column=-1)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1e87200-07a9-9518-4bed-cebb4b9185ee","_active":false},"outputs":[],"source":"# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=132)]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=131,\n                                            model_dir=\"/tmp/iris_model\")\n\n# Fit model.\nclassifier.fit(x=training_set.data, \n               y=training_set.target, \n               steps=2000)\n\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(x=test_set.data,\n                                     y=test_set.target)[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n\n# Classify two new flower samples.\n#new_samples = np.array(\n#    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n#y = classifier.predict(new_samples)\n#print('Predictions: {}'.format(str(y)))","execution_state":"idle"}]}