{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2eb0bba1-6d62-4acc-b16a-fbf7c7762a22"},"source":"I have covered a very simple approach in my previous Notebook where I simply changed the categorical variables into numeric with using only the number of category levels and made 1 more alteration on the output to make it symmetric:\n[Just an easy solution][1]"},{"cell_type":"markdown","metadata":{"_cell_guid":"683efc54-6eaf-6545-82f6-c09bb9ee02d9"},"source":"In this solution I will try to do a better job on categorical variables. This can be really important as the XGBoost (which I will use again) does splits based on relational operators: in each step it splits the dataset to \"<\" and \">=\" of a given value. If the target variable is totally independent from the value of the feature then the internal decision tree might not find the relevant rules easily and will go towards the goal in small steps by just selecting the tails of the feature. So let's help the decision tree and give it meaningful inputs!\n\n\n  [1]: https://www.kaggle.com/guyko81/allstate-claims-severity/just-an-easy-solution"},{"cell_type":"markdown","metadata":{"_cell_guid":"4207bed5-6e73-3ae2-ffa6-87ce57f26741"},"source":"The first part will be the same as in [Just an easy solution][1]."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2c37794-4006-67c6-e0b6-b3ffd06f9e05"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb # XGBoost implementation\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# read data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nfeatures = [x for x in train.columns if x not in ['id','loss']]\n#print(features)\n\ncat_features = [x for x in train.select_dtypes(include=['object']).columns if x not in ['id','loss']]\nnum_features = [x for x in train.select_dtypes(exclude=['object']).columns if x not in ['id','loss']]\nprint(cat_features)\nprint(num_features)\n\ntrain['log_loss'] = np.log(train['loss'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d348a4c-dbd6-cabb-5411-189bc50af280"},"source":"I'm going to use the average value of the target (log_loss in our case) for each category. Let's see how it changes the 'cat1' variable:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3629eb3a-6b33-ecf7-a4e1-2f538425322d"},"outputs":[],"source":"train_x = train[features]\na = pd.DataFrame(train['log_loss'].groupby([train['cat1']]).mean())\na['cat1'] = a.index\ntrain_x['cat1'] = pd.merge(left=train_x, right=a, how='left', on='cat1')['log_loss']\ntrain_x.head(n=20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a2fd0c0-562e-f5ed-6235-e5b36caae5be"},"source":"Nice, just perfect! Hopefully it will help and worth the work. \nLet's do it for all of the variables! \n(don't forget the test dataset)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c6c0bec6-3a9a-c202-1cfc-c3cfa354a307"},"outputs":[],"source":"train_x = train[features]\ntest_x = test[features]\nfor c in range(len(cat_features)):\n    a = pd.DataFrame(train['log_loss'].groupby([train[cat_features[c]]]).mean())\n    a[cat_features[c]] = a.index\n    train_x[cat_features[c]] = pd.merge(left=train_x, right=a, how='left', on=cat_features[c])['log_loss']\n    test_x[cat_features[c]] = pd.merge(left=test_x, right=a, how='left', on=cat_features[c])['log_loss']\n\ntrain_x.head(n=20)"},{"cell_type":"markdown","metadata":{"_cell_guid":"75bc4a66-835d-ac21-b5a3-ee8db8a6191e"},"source":"Come XGBoost, do it for us :)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b80ae45-41d7-0c1d-a37b-6382f0899c4d"},"outputs":[],"source":"xgdmat = xgb.DMatrix(train_x, train['log_loss']) # Create our DMatrix to make XGBoost more efficient\n\nparams = {'eta': 0.01, 'seed':0, 'subsample': 0.5, 'colsample_bytree': 0.5, \n             'objective': 'reg:linear', 'max_depth':6, 'min_child_weight':3} \n\nnum_rounds = 1000\nbst = xgb.train(params, xgdmat, num_boost_round = num_rounds)"},{"cell_type":"markdown","metadata":{"_cell_guid":"43fad280-7ded-1c49-4a12-d1e5ada6aad6"},"source":"And the feature importance. Will it differ from the previous?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90aeb2eb-3840-f547-ee87-0300cbe3137b"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport operator\n\ndef ceate_feature_map(features):\n    outfile = open('xgb.fmap', 'w')\n    i = 0\n    for feat in features:\n        outfile.write('{0}\\t{1}\\tq\\n'.format(i, feat))\n        i = i + 1\n\n    outfile.close()\n    \nceate_feature_map(features)\n\nimportance = bst.get_fscore(fmap='xgb.fmap')\nimportance = sorted(importance.items(), key=operator.itemgetter(1))\n\ndf = pd.DataFrame(importance, columns=['feature', 'fscore'])\ndf['fscore'] = df['fscore'] / df['fscore'].sum()\n\nplt.figure()\ndf.plot()\ndf.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nplt.xlabel('relative importance')\nplt.gcf().savefig('feature_importance_xgb.png')\n\ndf"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a08f2fc-54c4-0840-54b2-2987acb1dbce"},"source":"It definitely differs. Seems like more categorical variables reached the top 10 in the importance list: 5 vs 2 in previous model."},{"cell_type":"markdown","metadata":{"_cell_guid":"42daab23-6d86-7833-e4be-9e5fc075ac24"},"source":"Ok, model prediction again."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03a1c5d2-757f-81ed-851f-616d56c66098"},"outputs":[],"source":"test_xgb = xgb.DMatrix(test_x)\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nsubmission.iloc[:, 1] = np.exp(bst.predict(test_xgb))\nsubmission.to_csv('xgb_starter.cat_mean.csv', index=None)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e022c611-7072-0972-51e0-e10ab0d3001e"},"source":"Hmm, the result is better but not that much: 1138.56"},{"cell_type":"markdown","metadata":{"_cell_guid":"c592b4de-4a6a-d153-7136-6a63e0526080"},"source":"I guess that 1000 trees can solve the issue of not having ordered inputs. But at least we made some progression. "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}