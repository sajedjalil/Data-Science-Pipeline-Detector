{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"09c5db0a-1ccd-76cb-4060-8d7bfba9d5f4"},"source":"This is a draft. \nBut basically, it ranks the features using a random forest then cross validate the number of features to use and predict with the corresponding random forest."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab99a504-e90f-7572-40d4-63f4a45ede1f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport subprocess\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\n#from sklearn.cross_validation import KFold\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nimport time\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"16c10b2b-ca6b-748c-a5e5-f41ed9486f73"},"source":"#Download data "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"225cb0a4-8759-4c93-fa2c-43e032598629"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ny = train['loss']\nshift = 200\nlog_y = np.log(y+shift)\nprint(np.mean(y))\nprint(np.median(y))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ba20fe49-506c-398c-a94e-dab1d9ab0fa6"},"source":"#Preprocessing\nHere the categorical/continuous variables are named as cati/contj so we encode the categorical labels easily. But maybe different label encodings improve the regression? Also the data is very clean so no missing data, etc. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3bd15121-9f51-a78c-9313-37381788758d"},"outputs":[],"source":"def preprocess(df):\n    \n    #Assume categorical/continuous variables named as cati/contj\n    #Assume no missing data, etc. \n    cat_headers = [ x for x in df.columns.values if \"cat\" in x] ; #print cat_headers\n    cont_headers = [ x for x in df.columns.values if \"cont\" in x] ; #print cont_headers\n    df3 = df[cat_headers].apply(preprocessing.LabelEncoder().fit_transform) \n    #print(df[['cat1','cat2','cont1']].head(5))\n    return pd.concat([df[cont_headers],df3],axis=1)\n\ntrain = preprocess(train)\nx = train.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3688d84a-78a1-fbfd-0c02-2159dc209c33"},"outputs":[],"source":"def scorer(x,y):\n    return mean_absolute_error(np.exp(x),np.exp(y))\n\nfrom sklearn.metrics import fbeta_score, make_scorer\ncustom_scorer = make_scorer(scorer,greater_is_better=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f36b44cb-b6bf-8993-5edf-da70a21264dd"},"outputs":[],"source":"from sklearn import tree\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom numpy.random import RandomState\nprng = RandomState(1234567890)\n\ncv_tree = False\ncv_rf = True\n\nif cv_tree:\n    for max in [None,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]:\n        t0 = time.time()\n        reg = tree.DecisionTreeRegressor(max_depth=max)\n        scores = cross_val_score(reg, x,log_y, cv=3,scoring=custom_scorer)\n        scores10 = cross_val_score(reg, x,log_y, cv=10,scoring=custom_scorer)\n        d = time.time()-t0\n        print(\"tree:\",max,abs(scores.mean()),abs(scores10.mean()),d)\n\nif cv_rf:\n    t0 = time.time()\n    reg = RandomForestRegressor(n_estimators = 100,criterion='mse',random_state=prng,n_jobs=-1)\n    scores = cross_val_score(reg, x,log_y, cv=3,scoring=custom_scorer)\n    #scores10 = cross_val_score(reg, x,log_y, cv=10,scoring=custom_scorer)\n    d = time.time()-t0\n    #print(\"rf:\",max,abs(scores.mean()),abs(scores10.mean()),d)\n    print(\"rf:\",max,abs(scores.mean()),d)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2072344b-b1fb-2bea-912f-87e8d6952a94"},"source":"#Features selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe790c91-2dcd-28ca-fd59-0237f2aba0c3"},"outputs":[],"source":"from numpy.random import RandomState\nprng = RandomState(1234567890)\n\nforest = RandomForestRegressor(n_estimators = 100,criterion='mse',random_state=prng,n_jobs=-1)\nforest = GradientBoostingRegressor(learning_rate=0.4,criterion='mse')\nregressors = [ GradientBoostingRegressor(learning_rate=rate,criterion='mse') for rate in [0.1,0.2,0.3,0.4,0.5]]\nfind_features = False"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9bf72245-7b7b-721e-42f5-17ffd53db4a4"},"outputs":[],"source":"if find_features:\n    t0 = time.time()\n    forest = forest.fit(x,log_y)\n    d = time.time()-t0 ; print(\"Time to fit\",d)\n    feature_importances =  sorted(zip(forest.feature_importances_,train.columns.values),reverse=True)\n    sorted_features = [ feature for score,feature in feature_importances] ; print(sorted_features)\nelse:\n    sorted_features = ['cat80', 'cont14', 'cat101', 'cont7', 'cont2', 'cat103', 'cat79', 'cat100', 'cat111', 'cat112', 'cont8', 'cat12', 'cont5', 'cat53', 'cont3', 'cont6', 'cont4', 'cont1', 'cat110', 'cont13', 'cat81', 'cont12', 'cont11', 'cont10', 'cont9', 'cat114', 'cat1', 'cat113', 'cat57', 'cat116', 'cat72', 'cat83', 'cat107', 'cat82', 'cat91', 'cat105', 'cat115', 'cat93', 'cat106', 'cat73', 'cat84', 'cat108', 'cat109', 'cat92', 'cat87', 'cat94', 'cat4', 'cat75', 'cat97', 'cat31', 'cat39', 'cat104', 'cat95', 'cat5', 'cat50', 'cat6', 'cat99', 'cat36', 'cat2', 'cat27', 'cat37', 'cat38', 'cat44', 'cat102', 'cat23', 'cat26', 'cat49', 'cat9', 'cat96', 'cat25', 'cat52', 'cat76', 'cat98', 'cat66', 'cat77', 'cat13', 'cat10', 'cat11', 'cat90', 'cat74', 'cat40', 'cat3', 'cat41', 'cat54', 'cat8', 'cat28', 'cat71', 'cat19', 'cat24', 'cat45', 'cat29', 'cat88', 'cat43', 'cat16', 'cat65', 'cat86', 'cat89', 'cat85', 'cat7', 'cat30', 'cat78', 'cat51', 'cat18', 'cat17', 'cat14', 'cat67', 'cat42', 'cat46', 'cat59', 'cat61', 'cat32', 'cat33', 'cat21', 'cat68', 'cat47', 'cat34', 'cat60', 'cat63', 'cat35', 'cat22', 'cat48', 'cat58', 'cat56', 'cat69', 'cat20', 'cat70', 'cat55', 'cat15', 'cat62', 'cat64']\n    print(sorted_features)"},{"cell_type":"markdown","metadata":{"_cell_guid":"849f9283-6079-b660-2d11-244bcb60276f"},"source":"#Training"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d0da8f8-fa40-933c-70a6-45344298f411"},"outputs":[],"source":"tune_params = False\nif tune_params:\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n\n    tuned_parameters = {'n_features':[60,65,70,75,80,85,90,95,100,105,110]}\n    tuned_parameters = {'n_features':[80]}\n\n    for key in tuned_parameters.keys():\n        for value in tuned_parameters[key]:\n            t0 = time.time()\n            features = sorted_features[0:value]\n            x_ = train[features].values\n            x_train, x_test, log_y_train, log_y_test = train_test_split(x_, log_y, test_size=0.5, random_state=prng)\n            forest.fit(x_train,log_y_train)\n            log_y_predict = forest.predict(x_test)\n            y_predict = np.exp(log_y_predict)-shift\n            y_true = np.exp(log_y_test)-shift\n            score = mean_absolute_error(y_predict,y_true)\n            d = (time.time()-t0)/60.0 \n            print(key,value,score,d)\n\n#n_features 80 1212..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8159c7cc-c3e3-84ce-c41c-4356ed659635"},"outputs":[],"source":"tune_shift = False\nif tune_shift:\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n    \n    shifts = [200]\n    features = sorted_features[0:80]\n    \n    for shift in shifts:\n        t0 = time.time()\n        x_ = train[features].values\n        log_y = np.log(y+shift)\n        x_train, x_test, log_y_train, log_y_test = train_test_split(x_, log_y, test_size=0.33, random_state=prng)\n        forest.fit(x_train,log_y_train)\n        log_y_predict = forest.predict(x_test)\n        y_predict = np.exp(log_y_predict)-shift\n        y_true = np.exp(log_y_test)-shift\n        score = mean_absolute_error(y_predict,y_true)\n        d = (time.time()-t0)/60.0 \n        print(shift,score,d)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe9c24a0-778b-39bd-24cf-30cf920ee525"},"outputs":[],"source":"find_regressor = False\nif find_regressor:\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import mean_absolute_error\n   \n    for reg in regressors:\n        t0 = time.time()\n        x_ = train[features].values\n        log_y = np.log(y+shift)\n        x_train, x_test, log_y_train, log_y_test = train_test_split(x_, log_y, test_size=0.33, random_state=prng)\n        reg.fit(x_train,log_y_train)\n        log_y_predict = reg.predict(x_test)\n        y_predict = np.exp(log_y_predict)-shift\n        y_true = np.exp(log_y_test)-shift\n        score = mean_absolute_error(y_predict,y_true)\n        d = (time.time()-t0)/60.0 \n        print(shift,score,d)"},{"cell_type":"markdown","metadata":{"_cell_guid":"691cb28a-3744-f5e3-119d-76f4f138a8b2"},"source":"#Predicting"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91b857d0-21f9-33f3-acbb-453af22eaaed"},"outputs":[],"source":"predict = False\nif predict:\n    top_features = sorted_features[0:80]\n    x2 = train[top_features].values\n    t0 = time.time()\n    forest = forest.fit(x2,log_y)\n    d = time.time()-t0 ; print(\"Time to fit\",d/60.0)\n    xx = preprocess(test)[top_features].values\n    log_yy = forest.predict(xx)\n    yy = np.exp(log_yy)-shift"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c56ef27-27ef-bb36-29fa-98e3b47427ec"},"source":"#Publish"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e4dbc83-f89c-8053-eff5-68c96fc73331"},"outputs":[],"source":"publish = False\nif publish:\n    sub_name = 'random_forest.csv'\n    sub = pd.DataFrame()\n    sub['id'] = test['id']\n    #sub['loss'] = [ int(res) for res in yy]\n    sub['loss'] = yy\n    sub.to_csv(sub_name, index=False)\n    print(sub.head(5))\n    sub_example = pd.read_csv('../input/sample_submission.csv')\n    print(sub_example.head(5))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}