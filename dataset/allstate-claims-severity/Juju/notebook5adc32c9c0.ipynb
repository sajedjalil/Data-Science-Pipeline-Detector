{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13b51871-7958-bf62-6af8-1d71dee8207a"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"markdown","metadata":{"_cell_guid":"75f4b07e-231d-7fe4-c51a-7c53004f92e5"},"source":"#Loss prediction\n\nThe problem is to regress categorical and continuous variables into loss predictions. The performance measure is the mean absolute error:\n$ MAE = \\sum | y^{true}_i - y^{predict}_i |$"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cfa9c0f2-8d02-8143-cd36-8f3282b60444"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport subprocess\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import KFold\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.externals import joblib\nfrom sklearn.metrics import fbeta_score, make_scorer\nfrom sklearn import tree\nfrom sklearn.cross_validation import cross_val_score\nimport time\nfrom numpy.random import RandomState\nprng = RandomState(1234567890)\nimport sklearn ; print(sklearn.__version__)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13efa161-a37c-9760-7d4b-fa4c7082315c"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"5cba53f1-d50f-ab87-9879-d5da1414ff3d"},"source":"#Preprocessing\nThe data is clean.\nWe need to encode the categorical labels (preprocess fct). We choose to ignore the categorical values only present in the training or testing set. \nWe need to transform the data such that mean and median are close such that we do not have to use MAE in the algos. Solving MAE is too slow because the median needs sorting which is $O(NlnN)$ while the mean is a sum so done in $O(N)$.\n\nFollow https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114/code"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90c72a0f-c943-5c37-4de0-9a6281cef34c"},"outputs":[],"source":"test['loss'] = np.nan #Add a Loss column with NaN to test DF\njoined = pd.concat([train, test])\n\nfor column in list(train.select_dtypes(include=['object']).columns):\n    # \n    if train[column].nunique() != test[column].nunique(): # could fail in theory\n        set_train = set(train[column].unique()) # Do we need unique since we have set\n        set_test = set(test[column].unique())\n        remove_train = set_train - set_test\n        remove_test = set_test - set_train\n        remove = remove_train.union(remove_test)\n        def filter_cat(x):\n            if x in remove:\n                return np.nan\n            return x\n\n        joined[column] = joined[column].apply(lambda x: filter_cat(x), 1)\n        \n    #pd.factorize encode the (sorted) values\n    joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n    \n#HACK We set test['loss'] = NaN to recognize it \ntrain = joined[joined['loss'].notnull()]\ntest = joined[joined['loss'].isnull()]\ny = train['loss']\ntrain = train.drop(['id','loss'],1)\ntest = test.drop(['id','loss'],1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf999213-bc1b-1bbf-6f68-a6b3372704f9"},"outputs":[],"source":"def transform(y,is_inverse=None):\n    shift = 200\n    if is_inverse:\n        return np.exp(y)-shift\n    return np.log(y+shift)\n\ndef scorer(x,y):\n    return mean_absolute_error(transform(x,True),transform(y,True))\n\ncustom_scorer = make_scorer(scorer,greater_is_better=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bdd01747-d787-6e08-10b3-f858cffd4c73"},"outputs":[],"source":"print('mean',np.mean(y),'median',np.median(y))\n#print(train.head(5))\n\nX = train.values\nX_test = test.values\nfy = transform(y)\nprint('mean',transform(np.mean(fy),True),'median',transform(np.median(fy),True))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6cefd875-3c82-b5b2-7db0-cd80015ec2bb"},"source":"#Training"},{"cell_type":"markdown","metadata":{"_cell_guid":"b72121b3-7601-e4cf-15b6-746032aed9a0"},"source":"##Regression Tree"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3acfc190-4fa2-a2b6-7f4c-51445fd3ab97"},"outputs":[],"source":"if False:\n    t0 = time.time()\n    reg = tree.DecisionTreeRegressor(max_depth=9,min_samples_split=2)\n    scores = cross_val_score(reg, X,fy, cv=3,scoring=custom_scorer)\n    d = time.time()-t0\n    print(scores.mean(),d)\n\n    #reg = tree.DecisionTreeRegressor()    \n    #reg = reg.fit(x,fy)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d90f23c3-637d-8006-e149-5109d14d736a"},"source":"##Random Forest"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1dee251a-0ba5-81b1-f2ad-4b43637de830"},"outputs":[],"source":"if False:\n    params = [5,10,25,50,100]\n    params = [5]\n    for param in params:\n        t0 = time.time()\n        forest = RandomForestRegressor(n_estimators = param,criterion='mse',n_jobs=-1,random_state=prng)\n        scores = cross_val_score(forest, X,fy, cv=3,scoring=custom_scorer)\n        d = time.time()-t0\n        print(param,scores.mean(),d)\n\n    #5 -1319.6123701 37.71500015258789\n    #10 -1266.15301356 102.43400001525879\n    #25 -1231.19439691 213.24000000953674\n    #50 -1219.07043097 292.1300001144409\n    #100 -1212.35944528 633.6279997825623\n\nif False:\n    forest = RandomForestRegressor(n_estimators = 100,criterion='mse',n_jobs=-1,random_state=prng)\n    forest = forest.fit(X,fy)\n    feature_impportances =  sorted(zip(forest.feature_importances_,train.columns.values),reverse=True)\n    #Print the top features\n    print(feature_impportances)\n    for a,b in feature_impportances:\n        print(a,b)\nfeatures = ['cat80','cont14','cat101','cont7','cont2','cat79','cat103','cat100','cat12','cat111',\n            'cat112','cont8','cont5','cont3','cat81','cont4','cont6','cat53','cont1','cat110',\n            'cont13','cont12','cont10','cont11','cat57','cat1','cont9','cat114','cat113']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8da39694-4467-a779-5a2e-6df479a78805"},"outputs":[],"source":"### XGBoost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6466a48-46b2-41ed-860a-40296f9b9e6a"},"outputs":[],"source":"import xgboost as xgb\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n\nRANDOM_STATE = 1234\n\nparams = {\n        'min_child_weight': 1,\n        'eta': 0.01,\n        'colsample_bytree': 0.5,\n        'max_depth': 12,\n        'subsample': 0.8,\n        'alpha': 1,\n        'gamma': 1,\n        'silent': 1,\n        'verbose_eval': True,\n        'seed': RANDOM_STATE\n    }\n\nxgtrain = xgb.DMatrix(X, label=y)\nxgtest = xgb.DMatrix(X_test)\n\nrun_cv=False\nrun_model=False\n\nif run_cv:\n    cv = xgb.cv(params, xgtrain, num_boost_round=10, nfold=5, stratified=False,\n         early_stopping_rounds=50, verbose_eval=1, show_stdv=True, feval=evalerror, maximize=False)\n\nif run_model:\n    model = xgb.train(params, xgtrain, int(2012 / 0.9), feval=evalerror)\n    prediction = np.exp(model.predict(xgtest)) - shift"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}