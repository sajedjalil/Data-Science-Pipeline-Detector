{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d739c63-b14a-10ef-1cdc-0c62a1b59070"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\ndf = pd.read_csv(\"../input/train.csv\") \ndf_test = pd.read_csv(\"../input/test.csv\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77e0df46-c679-0b1a-f563-0eaf11df47ea"},"outputs":[],"source":"#Print all rows and columns. Dont hide any\n##df.drop = ('id', axis = 1)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\ndf.head(10)"},{"cell_type":"markdown","metadata":{"_cell_guid":"de835963-c645-a74e-9787-c20728a2d0f1"},"source":"###Data Cleaning/Feature Engineering\nUsing a method from Alexandru's kernel, we identified the columns with the greatest number of unique categorical variables and applied factorization to them (in order to reduce memory demand and runtime). We then converted the columns with 20 or fewer categorical variables into dummies/indicator variables. However, running the model with this transformed data actually increases the error, so we do not incorporate this in the final models. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df43e977-1aff-bd3f-2147-f0a00c9fb61c"},"outputs":[],"source":"##identify the columns with the most parameters and apply factorization to them (code from Alexandru)\ndf.iloc[:,1:116].apply(lambda x: pd.unique(x).shape[0]).sort_values(ascending = False).head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bae4ab5e-f36f-3522-a40f-f3e2424329ac"},"outputs":[],"source":"train_cat_large = df[[\"cat116\", \"cat110\",\"cat109\",\"cat113\", \"cat112\", \"cat115\"]]\ndf_frac = df.drop(train_cat_large, axis = 1)\nX_int_cat = train_cat_large.apply(lambda x: pd.factorize(x)[0])\ndf = pd.concat([df_frac, X_int_cat], axis = 1)\n#new dataframe factorizes the columns with most variables"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fac9c6ba-700c-6444-14bf-2a0cc6d5a5bb"},"outputs":[],"source":"df.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"5d9d3182-1fd8-ddb5-2e6f-d97317a2cb09"},"source":"num_train = df.size\nall_data = pd.concat((df, df_test)).reset_index(drop=True)\nfeatures = df.columns\ncats = [feat for feat in features if \"cat\" in feat]\nfor feat in cats:\n    all_data[feat] = pd.factorize(all_data[feat])[0]\ndf_train = all_data.iloc[:num_train]\ndf_test = all_data.iloc[num_train:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eeeaee83-3f91-f4b2-cbdc-0ebeb8c1cb32"},"outputs":[],"source":"df = pd.get_dummies(df, dummy_na = False, drop_first = True)\ndf.tail()\nprint(df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03b50d29-137b-f188-49f4-3a54f27aedc8"},"outputs":[],"source":"col = list(df.columns)\n#numcol = col[:15]\ndf['log_loss'] = np.log1p(df['loss'])\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd64ec55-b1cb-f4a8-8950-8d21c5f57544"},"outputs":[],"source":"import seaborn as sns\nsns.lmplot('cont3' , 'log_loss', data = df)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ed875b78-b2cb-e0d2-cf4b-efd629f3c2d9"},"source":"Scatterplot for one of the continuous variables and the log-transformed loss. The best-fit line indicates that there may be a slight positive correlation. "},{"cell_type":"markdown","metadata":{"_cell_guid":"a5371c3f-8a53-1a44-f253-93ca632d05bc"},"source":"##Violin Plots for Categorical Variables\nThese plots show the distribution of the log-transformed losses with respect to each variable in the categorical columns. The distributions for different variables within each columns generally seem pretty similar, indicating that most attributes by themselves are not a good indicator of loss."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b553e87b-222b-36d7-888c-29f7054ae733"},"outputs":[],"source":"from sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model\nimport statsmodels.api as sm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15088b2e-6444-4d34-d05c-d5974d205c17"},"outputs":[],"source":"##Split data and log transform\nlocal_train, local_test = train_test_split(df,test_size=0.2,random_state=123)\ndf_y_train = np.logp1(local_train['loss'])\ndf_x_train = local_train.drop([\"loss\"],axis=1)\ndf_y_test = np.logp1(local_test['loss'])\ndf_x_test = local_test.drop([\"loss\"],axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"561f193e-f5d5-f9b5-f05c-fe45b22e9773"},"outputs":[],"source":"##Training on linear Regression\nclf = sm.OLS(df_y_train, df_x_train)\nresult = clf.fit()\npreds = result.predict(df_x_test)\npreds"},{"cell_type":"markdown","metadata":{"_cell_guid":"393a2a20-70ea-3155-61af-272e34f8e43c"},"source":"#Linear Regression Model\nAs a first run, this model returns an error value above the benchmark. It may be less accurate due to the high correlation between many of the continuous variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de3a2f49-f6a4-448e-ca06-06c54ebf4cea"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}