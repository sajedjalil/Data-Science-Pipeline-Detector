{"metadata":{"language_info":{"name":"python","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"source":"# Overview\nThe goal of this notebook is to make a simple neural network which uses a standard embedding followed by an LSTM for all the categorical variables and fully connected layers for the continuous ones (after a batch normalization step). The idea is a little absurd since the categories likely refer to very different things (A in cat1 is probably not the same as A in cat2), but the approach lets us see how much of this information can be captured by a single LSTM (bidirectional) and if it could be viable approach for unknown categorical data","metadata":{"_uuid":"db03d4ed38e4488b9910e356ea79810ca9c075a3","_cell_guid":"d4e0e354-61a7-45ff-86b0-ab1060f33e64"},"cell_type":"markdown"},{"source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.layers import Embedding, Dense, Input, MaxPooling1D, concatenate, Flatten, Dropout, BatchNormalization\nfrom keras.layers import LSTM, Bidirectional, TimeDistributed\nfrom keras.models import Model","outputs":[],"execution_count":null,"metadata":{"_uuid":"547d64ad5e01739133d7d1876ca42ae408d19d13","_cell_guid":"7ce2d1c3-6d8a-48f7-b010-f648bb921c5a"},"cell_type":"code"},{"source":"base_path = os.path.join('..', 'input')\ntrain_path = os.path.join(base_path, 'train.csv')\ntest_path = os.path.join(base_path, 'test.csv')\ntrain_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\ntrain_df.sample(3)","outputs":[],"execution_count":null,"metadata":{"_uuid":"4499e2b0b4f1e408bb67adb16c2c0da389fc2dea","_cell_guid":"37bd9a89-52b1-4800-8160-38c51597a0db"},"cell_type":"code"},{"source":"from sklearn.preprocessing import LabelEncoder\ncat_cols = [x for x in train_df.columns if 'cat' in x]\ncont_cols = [x for x in train_df.columns if 'cont' in x]\nle_cat_encoder = LabelEncoder()\n# fit the encoder based on training and test datasets\nle_cat_encoder.fit(np.concatenate([train_df[x] for x in cat_cols]+\n                                 [test_df[x] for x in cat_cols]))\nprint(len(le_cat_encoder.classes_), 'categories')\ny_col = 'loss'","outputs":[],"execution_count":null,"metadata":{"_uuid":"06a1343abdc2d2ffe8c910ff13fe609b6c14a470","_cell_guid":"3d15adc4-62ea-40bd-8a64-01efe6070484"},"cell_type":"code"},{"source":"all_emb_chan, all_inputs = [], []\nfor k in cat_cols:\n    in_val = Input(shape = (1,), name = k)\n    all_emb_chan +=[Embedding(len(le_cat_encoder.classes_)+1, 64)(in_val)]\n    all_inputs += [in_val]\nconcat_layer = concatenate(all_emb_chan, axis = 1) # concatenate all of the columns together\n\nnorm_concat_emb = BatchNormalization()(concat_layer)\nfeature_layer = TimeDistributed(Dense(32))(Dropout(0.5)(norm_concat_emb))\nfeature_lstm = Bidirectional(LSTM(16))(feature_layer)\n\ncont_input = Input(shape = (len(cont_cols),), name = 'continuous')\nbn_cont = BatchNormalization()(cont_input)\ncont_feature_layer = Dense(16)(Dropout(0.5)(bn_cont))\nfull_concat_layer = concatenate([feature_lstm, cont_feature_layer])\nfull_reduction = Dense(16)(full_concat_layer)\n\nout_layer = Dense(1, activation = 'tanh')(full_reduction)\nfull_model = Model(inputs = all_inputs+[cont_input], \n                   outputs = [out_layer], name = 'FullModel')\nfull_model.compile(optimizer = 'adam', loss = 'mae')\nprint('Using a model with:', full_model.count_params(), 'parameters, in', len(full_model.layers), 'layers')","outputs":[],"execution_count":null,"metadata":{"_uuid":"48fdfdbb09c366e442aa2175bdb99c672f1e369a","_cell_guid":"6cd262e6-bc2f-46ed-922e-d0c42a437805"},"cell_type":"code"},{"source":"y_vec = train_df[y_col].copy().values\nloss_mean, loss_std = y_vec.mean(), 3*y_vec.std()\ny_vec -= loss_mean\ny_vec /= loss_std\ntrain_df['loss_norm'] = y_vec.clip(-1,1)","outputs":[],"execution_count":null,"metadata":{"_uuid":"31df6e1f383d24a7070c12c5e3a200bfdd345a86","collapsed":true,"_cell_guid":"00bedcdc-d7ad-4796-a7a2-3a6eb563f7e4"},"cell_type":"code"},{"source":"from sklearn.model_selection import train_test_split\nt_split_df, v_split_df = train_test_split(train_df, \n                 test_size = 0.2,\n                 stratify = pd.qcut(train_df['loss'], 10),\n                                         random_state = 2017)\nprint(t_split_df.shape, v_split_df.shape)","outputs":[],"execution_count":null,"metadata":{"_uuid":"d210e349c6193a7bf6a6db4f4fec6a0d646e2585","_cell_guid":"09004ba0-bc9c-40c1-a9b0-079c0b48114d"},"cell_type":"code"},{"source":"def gen_samples(in_df, batch_size = None, loss_name = 'loss_norm'):\n    while True:\n        out_df = in_df if batch_size is None else in_df.sample(batch_size)\n        feed_dict = {c_name: le_cat_encoder.transform(out_df[c_name].values) for c_name in cat_cols}\n        feed_dict['continuous'] = out_df[cont_cols].values\n        yield feed_dict, out_df[loss_name].values","outputs":[],"execution_count":null,"metadata":{"_uuid":"dcc7c50fa339e41a37d184602ba4ffff79452a3f","collapsed":true,"_cell_guid":"acb7e243-2380-47a0-8445-0e7ae0b3d096"},"cell_type":"code"},{"source":"loss_history = []","outputs":[],"execution_count":null,"metadata":{"_uuid":"acbcc25ee9920500ca75ee257713a00b9ee4a53a","collapsed":true,"_cell_guid":"fa64be5f-9ac1-4f31-ad09-dc0439d4854d"},"cell_type":"code"},{"source":"for i in range(10):\n    loss_history += [full_model.fit_generator(gen_samples(t_split_df, 32), \n                         steps_per_epoch = 500,\n                         epochs = 1,\n                         validation_data = next(gen_samples(v_split_df))\n                         )]","outputs":[],"execution_count":null,"metadata":{"_uuid":"67f7c60db0a8b0694d18fc6378857cbd501f63d8","_cell_guid":"455b3968-b357-4d69-a366-8298843c5435"},"cell_type":"code"},{"source":"%%time\nvalid_vars, valid_loss = next(gen_samples(v_split_df, loss_name = 'loss'))\npred_loss = full_model.predict(valid_vars).ravel()*loss_std+loss_mean","outputs":[],"execution_count":null,"metadata":{"_uuid":"5bde5218de9c1a69c4bb9c88cef0f494bb3c5e91","collapsed":true,"_cell_guid":"f214b359-268d-4f55-be4e-d540a582c86c"},"cell_type":"code"},{"source":"fig, ax1 = plt.subplots(1,1)\nax1.hist(valid_loss-pred_loss)\nax1.set_title('Loss Error: MAE-%2.2f' % (np.mean(np.abs(valid_loss-pred_loss))))\nax1.set_xlabel('Actual - Predicted Loss')","outputs":[],"execution_count":null,"metadata":{"_uuid":"03a1d747104230b154fb79a617ddffb750a7bbe3","_cell_guid":"0e98aa60-69fe-457d-8cac-d35c2fc08dfe"},"cell_type":"code"},{"source":"# Make Prediction\nHere we load the test data and make a prediction","metadata":{},"cell_type":"markdown"},{"source":"%%time\ntest_vars, test_id = next(gen_samples(test_df, loss_name = 'id'))\npred_test_loss = full_model.predict(test_vars, verbose = 1).ravel()*loss_std+loss_mean","outputs":[],"execution_count":null,"metadata":{"_uuid":"d15955692371d00fef19fcda657f38085a7c6522","_cell_guid":"ad27e8e6-5940-419f-8801-462a4e3aae39"},"cell_type":"code"},{"source":"pd.DataFrame(dict(id = test_id, loss = pred_test_loss)).to_csv('prediction.csv', index = False)","outputs":[],"execution_count":null,"metadata":{"_uuid":"545f5f31444e6de2fb6566328692571806e661b7","collapsed":true,"_cell_guid":"ceab3def-44ea-4c96-bd3e-c2726e16c388"},"cell_type":"code"},{"source":"","outputs":[],"execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"}],"nbformat":4}