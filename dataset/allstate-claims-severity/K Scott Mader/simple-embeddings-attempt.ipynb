{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"1204b5c9b3e912c72dc6ad9561bf0af36d3d5d51","_cell_guid":"663eb8be-b6df-478d-830b-bdbc767a6857"},"source":"# Overview\nThe goal of this notebook is to make a simple neural network which uses embeddings for all of the categorical variables and fully connected layers for the continuous ones (after a batch normalization step). The idea is to see how well just having a simple formula for embeddings works on complicated datasets."},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.layers import Embedding, Dense, Input, MaxPooling1D, concatenate, Flatten, Dropout, BatchNormalization\nfrom keras.models import Model","metadata":{"collapsed":true,"_uuid":"547d64ad5e01739133d7d1876ca42ae408d19d13","_cell_guid":"7ce2d1c3-6d8a-48f7-b010-f648bb921c5a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = os.path.join('..', 'input')\ntrain_path = os.path.join(base_path, 'train.csv')\ntest_path = os.path.join(base_path, 'test.csv')\ntrain_df = pd.read_csv(train_path)\ntrain_df.sample(3)","metadata":{"collapsed":true,"_uuid":"4499e2b0b4f1e408bb67adb16c2c0da389fc2dea","_cell_guid":"37bd9a89-52b1-4800-8160-38c51597a0db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncat_cols = [x for x in train_df.columns if 'cat' in x]\ncont_cols = [x for x in train_df.columns if 'cont' in x]\nle_encoders = {x: LabelEncoder() for x in cat_cols}\nle_cols = {k: v.fit_transform(train_df[k]) for k,v in le_encoders.items()}\ny_col = 'loss'","metadata":{"collapsed":true,"_uuid":"06a1343abdc2d2ffe8c910ff13fe609b6c14a470","_cell_guid":"3d15adc4-62ea-40bd-8a64-01efe6070484"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_emb_chan, all_inputs = [], []\nfor k,v in le_cols.items():\n    in_val = Input(shape = (1,), name = k)\n    all_emb_chan +=[Flatten()(Embedding(v.max()+1, (v.max()+1)//2)(in_val))]\n    all_inputs += [in_val]\nconcat_layer = concatenate(all_emb_chan)\nnorm_concat_emb = BatchNormalization()(concat_layer)\nfeature_layer = Dense(16)(Dropout(0.5)(norm_concat_emb))\n\ncont_input = Input(shape = (len(cont_cols),), name = 'continuous')\nbn_cont = BatchNormalization()(cont_input)\ncont_feature_layer = Dense(16)(Dropout(0.5)(bn_cont))\nfull_concat_layer = concatenate([feature_layer, cont_feature_layer])\nfull_reduction = Dense(16)(full_concat_layer)\n\nout_layer = Dense(1, activation = 'tanh')(full_reduction)\nfull_model = Model(inputs = all_inputs+[cont_input], outputs = [out_layer], name = 'FullModel')\nfull_model.compile(optimizer = 'adam', loss = 'mae')\nprint('Using a model with:', full_model.count_params(), 'parameters, in', len(full_model.layers), 'layers')","metadata":{"collapsed":true,"_uuid":"48fdfdbb09c366e442aa2175bdb99c672f1e369a","_cell_guid":"6cd262e6-bc2f-46ed-922e-d0c42a437805"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_vec = train_df[y_col].copy().values\nloss_mean, loss_std = y_vec.mean(), 3*y_vec.std()\ny_vec -= loss_mean\ny_vec /= loss_std\ntrain_df['loss_norm'] = y_vec.clip(-1,1)","metadata":{"collapsed":true,"_uuid":"31df6e1f383d24a7070c12c5e3a200bfdd345a86","_cell_guid":"00bedcdc-d7ad-4796-a7a2-3a6eb563f7e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nt_split_df, v_split_df = train_test_split(train_df, \n                 test_size = 0.2,\n                 stratify = pd.qcut(train_df['loss'], 10),\n                                         random_state = 2017)\nprint(t_split_df.shape, v_split_df.shape)","metadata":{"collapsed":true,"_uuid":"d210e349c6193a7bf6a6db4f4fec6a0d646e2585","_cell_guid":"09004ba0-bc9c-40c1-a9b0-079c0b48114d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gen_samples(in_df, batch_size = None, loss_name = 'loss_norm'):\n    while True:\n        out_df = in_df if batch_size is None else in_df.sample(batch_size)\n        feed_dict = {c_name: le_encoders[c_name].transform(out_df[c_name].values) for c_name in cat_cols}\n        feed_dict['continuous'] = out_df[cont_cols].values\n        yield feed_dict, out_df[loss_name].values","metadata":{"collapsed":true,"_uuid":"dcc7c50fa339e41a37d184602ba4ffff79452a3f","_cell_guid":"acb7e243-2380-47a0-8445-0e7ae0b3d096"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_history = []","metadata":{"collapsed":true,"_uuid":"70f2f38d2908897292f73a2e746b5e8dd7b3f22b","_cell_guid":"6c8b4850-6c67-4493-9a21-73f7f8b53b5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    loss_history += [full_model.fit_generator(gen_samples(t_split_df, 32), \n                         steps_per_epoch = 500,\n                         epochs = 1,\n                         validation_data = next(gen_samples(v_split_df))\n                         )]","metadata":{"collapsed":true,"_uuid":"c0f3e0a2f7212a38f25c4ead52d6bfbc745ce004","_cell_guid":"5760cb04-f701-49e1-aeb3-ebca9f3896c4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_vars, valid_loss = next(gen_samples(v_split_df, loss_name = 'loss'))\npred_loss = full_model.predict(valid_vars).ravel()*loss_std+loss_mean","metadata":{"collapsed":true,"_uuid":"064b550524f7f4755939f472c48a8368b5c99361","_cell_guid":"3ae2fd0f-e680-4a93-8c98-61ed89f7e6d9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots(1,1)\nax1.hist(valid_loss-pred_loss)\nax1.set_title('Loss Error: MAE-%2.2f' % (np.mean(np.abs(valid_loss-pred_loss))))\nax1.set_xlabel('Actual - Predicted Loss')","metadata":{"collapsed":true,"_uuid":"0c559c4fd8f2e19478894aacf84613658c2afd40","_cell_guid":"17299d26-6e59-4aac-a7eb-883d23379c58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Using a model with:', full_model.count_params(), 'parameters')","metadata":{"collapsed":true,"_uuid":"a61ef47aa2b85b66730851c8605ee34b18773e57","_cell_guid":"197101ef-0bc7-4e68-848c-5fd045d119a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(test_path)\ntest_vars, test_id = next(gen_samples(test_df, loss_name = 'id'))\npred_test_loss = full_model.predict(test_vars, verbose = 1).ravel()*loss_std+loss_mean","metadata":{"collapsed":true,"_uuid":"f6fc8faa3894b29d5b45ee0c0d4ebb87e41ecc6b","_cell_guid":"ebad9506-8c4b-4fe0-932d-9b9f02395aa2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d58658fb6ae26f4c2ebdb5fa1b787920328f3fd7","_cell_guid":"0050354d-48ae-4d72-a149-15dfeeccc0a8"},"source":"# Out of scope\ngreat, the test dataset has labels we don't see in the training"},{"cell_type":"code","source":"","metadata":{"collapsed":true,"_uuid":"6536f8b9328ba8d030dbd3d3e1ea7b39a2b11479","_cell_guid":"89562300-fb2b-4e66-b8b1-1e04c6298966"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"file_extension":".py","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.6.3"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat_minor":1}