{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6d8ec764-b31f-13da-1487-a14c62b177a9"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0dd4eea5-de3c-936e-0cd0-bf1cf24124c4"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# loading required modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport math\nfrom sklearn import svm\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# setting pandas env variables to display max rows and columns\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows',1000)\n\n\n# load train and test dataset\nprint(\"Loading.....\")\ntrain = pd.read_csv(\"../input/train.csv\")\ntrain_y = train['loss']\ntrain.drop(['loss'], axis=1, inplace=True)\n\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Loaded.\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03b5c5d1-a65a-e0cb-52c8-934ec0939f64"},"outputs":[],"source":"#Recomended: log transform the label variable\ntrain['loss'] = np.log1p(train['loss'])\nprint(train['loss'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4397101f-286b-cfe9-1c53-65a07750411b"},"outputs":[],"source":"# sepearte the categorical and continous features\ncont_columns = []\ncat_columns = []\n\nfor i in train.columns:\n    if train[i].dtype == 'float':\n        cont_columns.append(i)\n    elif train[i].dtype == 'object':\n        cat_columns.append(i)\n\ncont_columns.remove('loss')\n\nprint(\"Continuous Valued Columns: \\n\", cont_columns)\nprint(\"\\n\\n\")\nprint(\"Categorical Valued Columns: \\n\", cat_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c03a52c-0406-c299-995f-d08181ce88c7"},"outputs":[],"source":"# Optional: Display info\n\n# printing train dataset information\ntrain.info()\n\nprint(\"\\n\\n\")\n\n# printing test dataset information\ntest.info()\n\nprint(\"\\n\\n\")\n\ntrain.describe(include = ['object'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"111fbec3-dd39-c2fc-5e50-f68cf1ed397a"},"outputs":[],"source":"#Plot the loss function\nax = sns.distplot(train['loss'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc3b2c7e-f717-f8b3-6d54-b3498cda8cdc"},"outputs":[],"source":"#Calculate the correlation between coninuous variables, loss and one another\ncorr = train[cont_columns+['loss']].corr()\n\n#Display the correlation between the continuous variables, loss, and one another\nsns.set(style=\"white\")\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\n#Use light/Dark palettes when looking at abs of corr\ncmap = sns.light_palette((260, 75, 50), input=\"husl\", as_cmap=True)\n#sns.dark_palette((260, 75, 60), input=\"husl\", as_cmap=True)\n#sns.light_palette(\"#2ecc71\", as_cmap=True)\n\n#Use diverging palette when looking at corr\n#sns.diverging_palette(240, 5, as_cmap=True)\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(\n    np.absolute(corr), \n    mask=mask, \n    cmap=cmap, \n    vmax=.3,\n    square=True, \n    linewidths=.5, \n    cbar_kws={\"shrink\": .5}, \n    ax=ax\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d324c108-74fd-fd84-7d86-7787f05f2386"},"outputs":[],"source":"#Plot the two most correlated continuous valued functions against each other\n(sns.jointplot(\n    x=\"cont11\", \n    y=\"cont12\", \n    data=train[cont_columns],\n    #kind=\"kde\", space=0, color=\"g\"\n)\n.plot_joint(sns.kdeplot, zorder=1, n_levels=6))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77ce16dc-10ae-759c-1630-0914c3f64412"},"outputs":[],"source":"#Plot continuous valued functions and loss against each other\nsns.pairplot(\n    train[cont_columns+['loss']], \n    vars=(cont_columns[8:14]+['loss']), \n    kind = 'scatter',\n    diag_kind='kde'\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76c43953-417f-c81a-96fd-7685c54ef2ed"},"outputs":[],"source":"#Count the number of options for each categorical variable\noptions_count = [(x, len(np.unique(train[x], return_counts=True)[1])) for x in train[cat_columns]]\n\nprint( options_count )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d09bd9c8-8742-31c3-aaa6-5f510938ede5"},"outputs":[],"source":"#Convert the categorical variables to binary\ncols = cat_columns[95:100]\ntrain_test = train.copy()\n\ntrain_cat_columns_new = pd.get_dummies(train_test[cols])\ncat_columns_new = list(train_cat_columns_new.columns.values)\n\n\ntrain_test.drop(cat_columns, axis=1, inplace=True)\n#train_test.drop(cont_columns, axis=1, inplace=True)\n#print(train_test)\n\nresult = pd.concat([train_test, train_cat_columns_new], axis=1, join_axes=[train_test.index])\nprint(result)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccaadcb0-1a97-8c2b-7521-1111521e0f73"},"outputs":[],"source":"#Calculate the correlation between categorical variables, loss and one another\ncorr_cat = result[cat_columns_new+['loss']].corr()\n\n#Display the correlation between categorical variables, loss and one another\nsns.set(style=\"white\")\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\n#Use light/Dark palettes when looking at abs of corr\ncmap = sns.light_palette((260, 75, 50), input=\"husl\", as_cmap=True)\n#sns.dark_palette((260, 75, 60), input=\"husl\", as_cmap=True)\n#sns.light_palette(\"#2ecc71\", as_cmap=True)\n\n#Use diverging palette when looking at corr\n#sns.diverging_palette(240, 5, as_cmap=True)\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr_cat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(\n    np.absolute(corr_cat), \n    mask=mask, \n    cmap=cmap, \n    vmax=.3,\n    square=True, \n    linewidths=.5, \n    cbar_kws={\"shrink\": .5}, \n    ax=ax\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1985a05c-4bf2-48f7-d8f8-98b2099305af"},"outputs":[],"source":"#Count the number of observations for each option of a categorical variable\nsns.countplot(\n    data=train[cat_columns+['loss']],\n    x=cat_columns[1],\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3488934c-cf10-c31e-ed55-d4c11eba409b"},"outputs":[],"source":"#Display the median loss (with error) for each category option\nsns.barplot(\n    data=train[cat_columns+['loss']],\n    x=cat_columns[1], \n    y=\"loss\"\n);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b64c3a3d-4f4e-3ac7-bd67-41b3f7328c76"},"outputs":[],"source":"#Display the violin plot of loss vs. category option (with error) for numerous categories\ng = sns.PairGrid(\n    train[cat_columns+['loss']],\n    x_vars=cat_columns[:6],\n    y_vars=[\"loss\"],\n)\ng.map(\n    sns.violinplot, \n    palette=\"pastel\", \n    split=False, \n    inner=\"stick\", \n    bw=.2\n);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06131b85-352f-a629-5988-c13fa7514a04"},"outputs":[],"source":"#"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c10ac80-7a28-4233-203f-7a9ea6fe5008"},"outputs":[],"source":"#Perform SVM\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e8eda19-e81a-0997-0608-b70fa2992db5"},"outputs":[],"source":"\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"478bcba0-7925-517c-9efd-902599603b75"},"outputs":[],"source":"#Probability plots of continuous variables\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\n\nplt.figure(figsize=(15,25))\ngs = gridspec.GridSpec(7, 2)\nfor i, cn in enumerate(train[cont_columns].columns):\n    ax = plt.subplot(gs[i])\n    stats.probplot(train[cn], dist = stats.lognorm, plot = ax)\n    ax.set_xlabel('')\n    ax.set_title('Probplot of feature: cont' + str(i+1))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b23a2339-1bf6-a562-9329-d8d8abf8aebd"},"outputs":[],"source":"#Skewness of continuous variables \nskewness_list = []\nfor cn in train[cont_columns].columns:\n    skewness_list.append(stats.skew(train[cn]))\n\nplt.figure(figsize=(10,7))\nplt.plot(np.absolute(skewness_list), 'bo-')\nplt.xlabel(\"continous features\")\nplt.ylabel(\"skewness\")\nplt.title(\"plotting skewness of the continous features\")\nplt.xticks(range(15), range(1,15,1))\nplt.plot([(0.25) for i in range(0,14)], 'r--')\nplt.text(6, .1, 'threshold = 0.25')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ef9544a-15b5-3f25-fb27-79c8264f9b93"},"outputs":[],"source":"#Consider only the highly skewed columns\nskewed_cont_columns = []\nskew_threshold = 0.25\nfor i, cn in enumerate(cont_columns):\n    if np.abs(skewness_list[i]) >= skew_threshold:\n        skewed_cont_columns.append(cn)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cabd74b-30ba-34f1-35c9-4c28ad1aa7db"},"outputs":[],"source":"#Display the highly skewed columns \nplt.figure(figsize=(15,25))\ngs = gridspec.GridSpec(6, 2)\nfor i, cn in enumerate(skewed_cont_columns):\n    ax = plt.subplot(gs[i])\n    sns.distplot(train[cn], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('hist plot of feature: ' + str(cn))\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4737cb5c-7348-dbc0-b0a1-eabcff993f5d"},"outputs":[],"source":"'''\nBelow function comes in handy in plotting the distribution and probability plot side by side and we look at\noriginal feature\ncustom transformed feature\nboxcox transformed feature\nin some cases custom transformation might be better than boxcox transformation, let's analyze\n'''\ndef examine_transform(original, transformed):\n    plt.figure(figsize=(15,10))\n    gs = gridspec.GridSpec(3,2, width_ratios=(1,2))\n    \n    ax = plt.subplot(gs[0])\n    sns.distplot(original, bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of orignal feature')\n    \n    ax = plt.subplot(gs[1])\n    prob = stats.probplot(original, dist = stats.norm, plot = ax)\n    ax.set_xlabel('')\n    ax.set_title('Probplot of original feature')\n    \n    ax = plt.subplot(gs[2])\n    sns.distplot(transformed, bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of transformed feature')\n    \n    ax = plt.subplot(gs[3])\n    prob = stats.probplot(transformed, dist = stats.norm, plot = ax)\n    ax.set_xlabel('')\n    ax.set_title('Probplot of transformed feature')\n    \n    # apply boxcox transformation\n    xt, _ = stats.boxcox(original)\n    ax = plt.subplot(gs[4])\n    sns.distplot(xt, bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of boxcox transformed feature')\n    \n    ax = plt.subplot(gs[5])\n    prob = stats.probplot(xt, dist = stats.norm, plot = ax)\n    ax.set_xlabel('')\n    ax.set_title('Probplot of boxcox transformed feature')\n    \n    \n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"32d87375-2bf5-5980-9288-5c9bb8897b2d"},"outputs":[],"source":"examine_transform(train.cont1, np.power(train.cont1,0.5))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}