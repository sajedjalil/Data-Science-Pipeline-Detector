{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"591a73e0-67eb-d958-7e0e-3a1319ade3dc"},"source":"What's the impact of the categorical features encoding on the model performance ?\n\n * let's start with the basic LabelEncoder\n   \n * then compare with a custom encoding, based on the impact of each category on the loss"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15576b23-07d8-18e9-b5f2-f9bc4dab98b7"},"outputs":[],"source":"%matplotlib inline\n\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.linear_model import SGDRegressor\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"492307e7-25e4-d7b9-7f1f-841e0a736c87"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"6010f1bd-d9b3-807c-5e01-3b20aee3bedc"},"source":"# 1- Extract the categories from `train` data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61d650a3-d1e5-5bd4-4ac1-f9751e885b49"},"outputs":[],"source":"catFeatureslist = []\nfor colName,x in train.iloc[1,:].iteritems():\n    if(str(x).isalpha()):\n        catFeatureslist.append(colName)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7a8e8aa7-b771-4f38-a1c5-d5b0cc961fb2"},"source":"# 2- Transform categories using LabelEncoder"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a46cb634-2630-0744-f261-ce72b144b425"},"outputs":[],"source":"for cf in catFeatureslist:\n    le = LabelEncoder()\n    le.fit(train[cf].unique())\n    train[cf] = le.transform(train[cf])"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d042037-4e7e-0e08-83ef-295c2cdd64d6"},"source":"Let's plot the relation between `cat100` and the `loss`"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a8458da-2944-f6ef-0d97-8e465d7c8864"},"outputs":[],"source":"ax = sns.violinplot(train.cat100, train.loss)\nax.axis([-1,15,0,6000])"},{"cell_type":"markdown","metadata":{"_cell_guid":"f88ea3d4-caa8-4e16-494e-1c6589cb478f"},"source":"As we can see, there is no trivial relation between the `cat100` values and the `loss`.\nWe can have the intuition that it will be harder for a linear regression model to find a curve matching that plot.\n\nBut what about the other models, giving better performance on the Allstate Claims Severity challenge, like the ensemble boosting models ?"},{"cell_type":"markdown","metadata":{"_cell_guid":"88f4cec0-8af0-fcf3-9bda-5c085dd78bc5"},"source":"# 3- Train and measure the MAE with LabelEncoder"},{"cell_type":"markdown","metadata":{"_cell_guid":"0683abd1-29f6-c6a0-f7df-a82758fb3b30"},"source":"For the comparison, I'm using 2 models, whose hyper parameters have already been optimized:\n\n* SGDRegressor\n* GradientBoostingRegressor\n\nThey are not the best models for this challenge, but the model fitting operation is fast."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"857a5e02-e42a-ea2d-3ac0-f701961ab96d"},"outputs":[],"source":"clf_gbr = GradientBoostingRegressor(\n    loss='ls',\n    learning_rate=0.1,\n    n_estimators=50,\n    max_depth=5,\n    max_features=0.12,\n    random_state=69,\n    subsample=0.5,\n    verbose=0)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3a300c7-20e3-616d-5493-64f06c49f520"},"outputs":[],"source":"clf_sgdr = SGDRegressor(\n    fit_intercept=False,\n    loss='squared_loss',\n    penalty='elasticnet',\n    alpha=0.03,\n    l1_ratio=0.7,\n    learning_rate='invscaling',\n    random_state=42,\n    shuffle=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44242f90-05c2-6693-ccb4-89c77fbf9bfd"},"outputs":[],"source":"def evaluateModelPerf(clf, train, Y, Y_scaler=None):\n    clf.fit(train, Y)\n    print(\"Coefficient of determination on training set:\", clf.score(train, Y))\n    \n    print(\"Score and Mean Absolute Error on the cross validation sets:\")\n    cv = KFold(n_splits=5, shuffle=True, random_state=33)\n    maes = []\n    scores = []\n    for _, test_index in cv.split(train):\n        Y_predict = clf.predict(train.iloc[test_index])\n        if Y_scaler is not None:\n            Y_scaled = Y[test_index] * Y_scaler.scale_ + Y_scaler.mean_\n            Y_predict_scaled = Y_predict * Y_scaler.scale_ + Y_scaler.mean_\n            mae = mean_absolute_error(Y_scaled, Y_predict_scaled)\n        else:\n            mae = mean_absolute_error(Y[test_index], Y_predict)\n        score = clf.score(train.iloc[test_index], Y[test_index])\n        print(\"score: {}, MAE: {}\".format(score, mae))\n        maes.append(mae)\n        scores.append(score)\n\n    print(\"Average score: {}\".format(np.average(scores)))\n    print(\"Average MAE: {}\".format(np.average(maes)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e41d92b8-3ac7-ea8f-2f11-2925bd92c6ee"},"outputs":[],"source":"Y = train.loss\ntrain = train.drop([\"id\", \"loss\"], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9d6fa2b-fd14-8ed9-93d9-aa4039037b72"},"outputs":[],"source":"evaluateModelPerf(clf_gbr, train, Y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"67387ace-f369-7d88-f9bf-64f7049b5e8e"},"source":"SGDRegressor is sensitive to scaling and normalization:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30826444-51fa-fea9-6566-571082adfd76"},"outputs":[],"source":"scaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled = pd.DataFrame(train_scaled)\ntrain_scaled.columns = train.columns\ntrain = train_scaled\n\nY = scaler.fit_transform(Y[:, None])[:, 0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"de2c131a-8ec3-bc86-3908-20e2dfb491d0"},"outputs":[],"source":"evaluateModelPerf(clf_sgdr, train, Y, Y_scaler=scaler)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5ebdb1d5-77a2-81a9-2cb8-7e68867a5179"},"source":"# 4- Tranform categories based on their ordered impact on the loss"},{"cell_type":"markdown","metadata":{"_cell_guid":"7b792cb2-47a3-0cd9-6a57-c92fb63a724b"},"source":"Let's restart the process with a custom label encoding"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81ad1e2a-070f-4ff7-cc0a-03843bab4d0d"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"65f645b4-4afd-f25f-ad28-f5258827a27a"},"outputs":[],"source":"categoriesOrder = {}\nfor cat in catFeatureslist:\n    medians = train.groupby([cat])['loss'].median()\n    ordered_medians = sorted(medians.keys(), key=lambda x: medians[x])\n    categoriesOrder[cat] = ordered_medians"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"626db879-9be1-3929-a8da-667d259907d2"},"outputs":[],"source":"def tranformCategories(X):\n    for cat, order in categoriesOrder.items():\n        class_mapping = {v: order.index(v) for v in order}\n        X[cat] = X[cat].map(class_mapping)\n    return X\n\nft = FunctionTransformer(tranformCategories, validate=False)\ntrain = ft.fit_transform(train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d401dc40-c3dc-56d4-9b52-b2f52fb0a82b"},"outputs":[],"source":"ax = sns.violinplot(train.cat100, train.loss)\nax.axis([-1,15,0,6000])"},{"cell_type":"markdown","metadata":{"_cell_guid":"18516328-7647-215c-076b-8b14131a74b8"},"source":"# 5- Train and measure the MAE with the custom encoder"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41037e2e-6583-6784-b682-390893af30e4"},"outputs":[],"source":"Y = train.loss\ntrain = train.drop([\"id\", \"loss\"], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4888e9d-dfca-abc8-ad04-cf8c5370136c"},"outputs":[],"source":"evaluateModelPerf(clf_gbr, train, Y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8b5bc97-cf05-32fe-1f5e-4c9b3d4e05df"},"outputs":[],"source":"scaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled = pd.DataFrame(train_scaled)\ntrain_scaled.columns = train.columns\ntrain = train_scaled\n\nY = scaler.fit_transform(Y[:, None])[:, 0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fdf3097-126b-00e3-9f3e-778576f98086"},"outputs":[],"source":"evaluateModelPerf(clf_sgdr, train, Y, Y_scaler=scaler)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0d9d4543-f905-19be-cfd9-513773b9ec57"},"source":"# 6- Conclusion"},{"cell_type":"markdown","metadata":{"_cell_guid":"2c047701-1645-4009-d5b2-46f45f617977"},"source":"We can notice that the categorized feature encoding method can have an impact on the model performance.\n\n* The impact will be higher on linear regression model than on ensemble boosting model.\n\n* The benefit will decrease as we increase the tree depth or the number of iterations.\n\nBut for fast analysis, in the early stage of the feature engineering process, it could be an interesting practice to consider."},{"cell_type":"markdown","metadata":{"_cell_guid":"5246ec8a-32fb-bc6b-28c2-8ce6b5dc6d30"},"source":"# 7- Notes"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ba5c739-377a-c69b-8f8c-93010a8fe6ef"},"source":"When using, such encoding method, there might be some category values in the `test` data, that are missing from the `train` data.\n\nFollowing method aims at identifying and replacing them with the median value for each category."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"378d3f78-97dd-26c2-33f8-51b7ab2a5104"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e87562f3-80fd-53d8-926b-18d8606e76ae"},"outputs":[],"source":"print(\"Categories found in test data, but not present in train data:\")\nfor cat in catFeatureslist:\n    testCat = set(test[cat].unique())\n    trainCat = set(train[cat].unique())\n    missing = testCat - trainCat\n    if missing:\n        nb_samples = 0\n        for m in missing:\n            nb_samples += test[test[cat] == m].shape[0]\n        print(\"Feature: {}. Missing categories: {}. Number of samples: {}\".format(cat, list(missing), nb_samples))"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd062dd7-f10e-c5f9-5b0b-582a0ecb4c30"},"source":"Example: the 3 `E` and `G` values for the `cat92` will be replaced by `NaN` during encoding:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"677cb7e9-8653-74b3-eb44-ca915faced2d"},"outputs":[],"source":"train = train.drop([\"id\", \"loss\"], axis=1)\ntest = test.drop([\"id\"], axis=1)\ntrain = ft.fit_transform(train)\ntest = ft.fit_transform(test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8ea8f5f-e579-2d32-f89f-4be65ef41dbf"},"outputs":[],"source":"test[np.isnan(test.cat92)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ffa9fc78-77bf-8956-4f82-c65c8e821187"},"outputs":[],"source":"imp = Imputer(missing_values='NaN', strategy='median', axis=0)\nimp.fit(train)\ntest = imp.transform(test)\ntest = pd.DataFrame(test)\ntest.columns = train.columns"},{"cell_type":"markdown","metadata":{"_cell_guid":"29084885-9652-6161-1e40-a898432d4786"},"source":"The `NaN` values have now been replaced by the median value for each category:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa1f1893-cc62-1f74-db69-c318077228df"},"outputs":[],"source":"test[np.isnan(test.cat92)]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f788ec0-fa17-3f82-5c1c-cb866c9b4ce8"},"outputs":[],"source":"all_labels = set()\nfor cat in catFeatureslist:\n    all_labels.update(train[cat].unique())\nle = LabelEncoder()\nle.fit(list(all_labels))\nfor cat in catFeatureslist:\n    train[cat] = le.transform(train[cat])"},{"cell_type":"markdown","metadata":{"_cell_guid":"923faa96-2426-e4c1-dbe9-ce7b2236606d"},"source":"Alternative to the median value: use the correlation between categories"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0fb1d2a6-e0ed-1ffa-77c2-8aeb52b0e58b"},"outputs":[],"source":"corr = train[catFeatureslist].corr()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8446fa6-fa44-5de0-28fb-ba5f4fdaa2fa"},"outputs":[],"source":"sns.heatmap(corr)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07327ff2-361f-0786-193e-73c1f15f7f0f"},"outputs":[],"source":"for cat in catFeatureslist:\n    corr_order = corr[cat].order()\n    print(\"{} gets maximum correlation factor with {} (corr={:.2})\".format(\n        cat,\n        corr_order.index[-2],\n        corr_order[-2]\n    ))"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}