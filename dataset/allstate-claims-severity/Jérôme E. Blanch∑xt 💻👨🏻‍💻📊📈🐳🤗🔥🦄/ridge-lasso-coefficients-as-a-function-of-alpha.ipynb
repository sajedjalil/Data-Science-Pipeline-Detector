{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ridge & Lasso coefficients as a function of the regularization Alpha parameter"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom scipy.stats import skew, boxcox\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom pandas import DataFrame\nfrom scipy.stats.stats import pearsonr\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport scipy.stats as stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data(file_path):    \n    print('Loading datasets...')\n    X_train = pd.read_csv(file_path + 'train.csv', sep=',')\n    print('Datasets loaded')\n    return X_train\nPATH = '../input/allstate-claims-severity/'\nDATA = read_data(PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_categorical_covariates = [x for x in DATA.select_dtypes(include=['object']).columns if x not in ['id','loss', 'log_loss', 'log_loss_+_200']]\ncontinuous_covariates = [x for x in DATA.select_dtypes(exclude=['object']).columns if x not in ['id','loss', 'log_loss', 'log_loss_+_200']]        \nbinary_categorical_covariates = [x for x in DATA.columns if len(DATA[x].unique()) == 2 and DATA[x].dtype == 'object']\nnon_bynary_categorical_covariates = [x for x in DATA.columns if len(DATA[x].unique()) > 2 and DATA[x].dtype == 'object']\n\nprint(\"List of Binary Categorical Covariates:\\n\")\nprint(binary_categorical_covariates)\nprint(\"\\n\")\nprint(\"List of Non Binary Categorical Covariates:\\n\")\nprint(non_bynary_categorical_covariates)\nprint(\"\\n\")\nprint(\"List of All Categorical Covariates:\\n\")\nprint(all_categorical_covariates)\nprint(\"\\n\")\nprint(\"List of Continuous Covariates:\\n\")\nprint(continuous_covariates)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in all_categorical_covariates:\n    lb = LabelEncoder()\n    lb.fit(DATA[x].unique())\n    DATA[x] = lb.transform(DATA[x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 1\n\ntrainx = DATA.columns[1:73]\ntrainy = DATA.columns[-2]\n\nX = DATA[trainx]\nY = DATA[trainy]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n\nmodel1 = LinearRegression(n_jobs=-1)\nresults1 = cross_val_score(model1, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression (Manual Tuning): ({0:.3f}) +/- ({1:.3f})\".format(-1*results1.mean(), results1.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = sm.add_constant(X)\nmodel = sm.OLS(Y, X2)\nmodel_ = model.fit()\nprint(model_.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = Ridge(alpha=1,random_state=seed)\nresults2 = cross_val_score(model2, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression Ridge (Manual Tuning): ({0:.3f}) +/- ({1:.3f})\".format(results2.mean(), results2.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.clock()\n\nclf = Ridge()\ncoefs = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X2, Y)\n    coefs.append(clf.coef_)\n\nplt.figure(figsize=(40, 20))\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at that point \\nand all weights would be shrinked to zero (see proof below)', \nxy=(0.1, -0.6), xytext=(0.1, -0.4), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.clock()\n\nclf = Ridge()\nerror = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    #clf.fit(X2, Y)\n    error.append(cross_val_score(clf, X2, Y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1).mean())\n\nplt.figure(figsize=(40, 20))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, error, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('mean absolute error')\nplt.title('Mean absolute error as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at that point \\nand all weights would be \\nshrinked to zero (see proof below)', \nxy=(0.1, -0.68), xytext=(0.1, -0.65), arrowprops=dict(facecolor='black'), color='black')\nplt.annotate('', \nxy=(100, -0.51), xytext=(100, -0.54), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.68,-0.48])\n#plt.xlim([-17.5,17.5])\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 = Lasso(alpha=0.0001,random_state=seed)\nresults3 = cross_val_score(model3, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression Lasso (Manual Tuning): ({0:.3f}) +/- ({1:.3f})\".format(results3.mean(), results3.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.clock()\n\nclf = Lasso()\ncoefs = []\nalphas = np.logspace(-6, 2, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X2, Y)\n    coefs.append(clf.coef_)\n\nplt.figure(figsize=(40, 20))\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Lasso coefficients as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('', \nxy=(0.1, -0.6), xytext=(0.1, -0.4), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.clock()\n\nclf = Lasso()\nerror = []\nalphas = np.logspace(-6, 9, 200)\n\nfor a in alphas:\n    clf.set_params(alpha=a)\n    #clf.fit(X2, Y)\n    error.append(cross_val_score(clf, X2, Y, cv=5, scoring='neg_mean_absolute_error', n_jobs=1).mean())\n\nplt.figure(figsize=(40, 20))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, error, color='b')\nax.set_xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('mean absolute error')\nplt.title('Mean absolute error as a function of the regularization Alpha parameter')\nplt.axis('tight')\n\nplt.annotate('Lasso is done at that point \\nand all weights are \\nshrinked to zero', \nxy=(0.1, -0.60), xytext=(0.1, -0.58), arrowprops=dict(facecolor='black'), color='black')\nplt.annotate('Ridge still perform good at that point', \nxy=(100, -0.49), xytext=(100, -0.51), arrowprops=dict(facecolor='black'), color='black')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.68,-0.48])\n#plt.xlim([-17.5,17.5])\nplt.show()\n\nend_time = time.clock()\nprint(\"\")\nprint(\"Total Estimation Running Time:\")\nprint(end_time - start_time, \"Seconds\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model4 = ElasticNet(alpha=0.0001,l1_ratio=0.5,random_state=seed)\nresults4 = cross_val_score(model4, X_train, y_train, cv=5, scoring='neg_mean_absolute_error', n_jobs=1)\nprint(\"Linear Regression Elastic Net (Manual Tuning): ({0:.3f}) +/- ({1:.3f})\".format(results4.mean(), results4.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha=0.0001)\nlasso.fit(X_train, y_train)\nridge = Ridge(alpha=1,random_state=seed)\nridge.fit(X_train, y_train)\nlinear = LinearRegression()\nlinear.fit(X_train, y_train)\nenet = ElasticNet(alpha=0.0001, l1_ratio=0.5)\nenet.fit(X_train, y_train)\n\nplt.figure(figsize = (40, 20))\nplt.plot(enet.coef_, color='red', linewidth=2,label='Elastic net coefficients with α = 0.0001 & l1 Ratio = 0.5')\nplt.plot(lasso.coef_, color='black', linewidth=2,label='Lasso coefficients with α = 0.0001')\nplt.plot(ridge.coef_, color='green', linewidth=2,label='Ridge coefficients with α = 1')\nplt.plot(linear.coef_, color='blue', linewidth=2,label='Linear coefficients with no regularization')\nplt.grid(color='black', linestyle='dotted')\nplt.ylim([-0.8,0.9])\nplt.xlim([-5,75])\nplt.legend(loc='best')\nplt.title('Coefficient Distribution According to the Linear Regression type')\nplt.xlabel('Binary Variable Ranked (cat1, cat2,...,cat72)')\nplt.ylabel('Estimated Coefficient Value')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}