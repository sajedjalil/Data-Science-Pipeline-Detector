{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"6f9b6ac6-3f48-f963-75df-21d037e13bd6"},"source":"A first try(for me) at stacking, with scikit and xgb. seems to be working well enough! \n\nlet me know the thoughts!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f37a5a61-e1e0-a3d6-57d0-7f6dd7ac9487"},"outputs":[],"source":"import os,sys,time,random,math,time\nimport tarfile, zipfile\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression,Ridge\n\nfrom sklearn import decomposition, datasets, ensemble\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score, mean_absolute_error\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, Image\n\nimport xgboost as xgb\n\n\nfrom subprocess import check_output\ndatadir=\"../input/\"\nprint(check_output([\"ls\", datadir]).decode(\"utf8\"))\n\n%matplotlib inline  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8cd6160-54d7-8cba-99f1-da89842d2912"},"outputs":[],"source":"def loadData(datadir,filename):\n    # Load the wholesale customers dataset\n    #data = pd.read_csv(filename)\n    data = ''\n    print (\"loading: \"+datadir+filename)\n    try:\n        if zipfile.is_zipfile(datadir+filename):\n            z = zipfile.ZipFile(datadir+filename)\n            filename = z.open(filename[:-4])\n        else:\n            filename=datadir+filename\n        data = pd.read_csv(filename, parse_dates=True)  \n        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n    except Exception as e:\n        print (\"Dataset could not be loaded. Is the dataset missing?\")\n        print(e)\n    return data\n\ndef writeData(data,filename):\n    # Load the wholesale customers dataset\n    try:\n        data.to_csv(filename, index=False)\n    except Exception as e:\n        print (\"Dataset could not be written.\")\n        print(e)\n    verify=[]\n    try:\n        with open(filename, 'r') as f:\n            for line in f:\n                verify.append(line)\n        f.closed\n        return verify[:5]\n    except IOError:\n        sys.std\n        \ndef LabelEncoder(data):\n    # lifted in parts from:\n    #https://www.kaggle.com/mmueller/allstate-claims-severity/yet-another-xgb-starter/code\n    features = data.columns\n    cats = [feat for feat in features if 'cat' in feat]\n    for feat in cats:\n        data[feat] = pd.factorize(data[feat], sort=True)[0]\n    return data\n\n# XGB!\n\ndef xgbfit(X_train,y_train):\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    \n\n    xgb_params = {\n        'seed': 0,\n        'colsample_bytree': 0.7,\n        'silent': 1,\n        'subsample': 0.7,\n        'learning_rate': 0.075,\n        'objective': 'reg:linear',\n        'max_depth': 6,\n        'num_parallel_tree': 1,\n        'min_child_weight': 1,\n        'eval_metric': 'mae',\n    }\n\n    start_time = time.time()\n    res = xgb.cv(xgb_params, dtrain, num_boost_round=750, nfold=4, seed=42, stratified=False,\n                 early_stopping_rounds=15, verbose_eval=100, show_stdv=True, maximize=False)\n    print(\"fit time:{}s\".format(round((time.time()-start_time), 3) ))\n\n    best_nrounds = res.shape[0] - 1\n    cv_mean = res.iloc[-1, 0]\n    cv_std = res.iloc[-1, 1]\n    print('CV-Mean: {0}+{1}'.format(cv_mean, cv_std))\n    # XGB Train!\n    start_time = time.time()\n    gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n    print(\"Train time:{}s\".format(round((time.time()-start_time), 3) ))\n    return gbdt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97e0a37a-bc6d-3211-3fbc-3be69f1ad7ca"},"outputs":[],"source":"data = loadData(datadir,'train.csv')\ndisplay(data.info())\ndisplay(data.head(5))\n\ntest_data= loadData(datadir,'test.csv') \ndisplay(test_data.info())\ndisplay(test_data.head(5))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7aff6688-a717-2fa5-62ed-6d23cc7e039e"},"source":"Pre-proccessing\n---------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad62fb7f-9860-e268-2761-21a3eea9dc1e"},"outputs":[],"source":"# combine the two frames so we can encode the labels!\ntest_data['loss']=0\n\nlengthofData=len(data)\nlengthoftest_data=len(test_data)\n\nprint(\"data:\",lengthofData)\nprint(\"test:\",lengthoftest_data)\n\ncombineddata=pd.concat([data,test_data])\nlengthofcombined=len(combineddata)\nprint(\"combined:\",lengthofcombined)\n\n# the categorical data that we need in a number format\ncombineddata=LabelEncoder(combineddata)\n\n# time to split the data back apart!\ndata=combineddata.iloc[:lengthofData].copy()\ntest_data=combineddata.iloc[lengthofData:].copy()\ntest_data.drop(['loss'],1,inplace=True) # didn't have this column before, make it go away!\n\n\nx_test = test_data.copy()\nx_test.drop(['id'],1,inplace=True)\n\n# we don't want the ID columns in X, and of course not loss either\nx=data.drop(['id','loss'],1)\n# loss is our label\ny=data['loss']\n\n#minmax scaler\nscaler= MinMaxScaler() \nx = scaler.fit_transform(x)\nx_test_data = scaler.fit_transform(x_test)\n\n#display(x[:5])\n#display(y.head(5))\n\nprint(\"Pre-Processing done\")\nprint(\"data:\",len(x))\nprint(\"labels:\",len(y))\nprint(\"test:\",len(x_test_data))"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e1ec6bf-6afb-8a5f-40fe-72d9a1adea13"},"source":"Stacking, Layer 1\n-----------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e57821a8-7d7c-961c-c70c-9f984c1ce56d"},"outputs":[],"source":"# OK let's actually do some ML\nregrList=[] # a list of regressions to use\n#regrList.append(LinearRegression())\nregrList.append(ExtraTreesRegressor())\nregrList.append(Ridge())\n    \nregrList.append(RandomForestRegressor(n_estimators=10,\n                                      #criterion = 'mae',\n                                      n_jobs =-1, \n                                      random_state=42))\nprint(\"number of scikitlearn regressors to use:\",len(regrList))"},{"cell_type":"markdown","metadata":{"_cell_guid":"d6e5d5b8-96d9-54c9-dfa0-8fe9a65a82f0"},"source":"Split the data into k-folds(divisions). train the regressors on each combination of k-1 folds, and then predict on the held-out fold. Preserve the prediction of each regressor for the next layer."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41847d2c-c981-92f3-2059-0d1894f1decc"},"outputs":[],"source":"#prepare the fold divisions\n\ndata_size=x.shape[0]\nprint(\"size of train data:\",data_size)\nfolds=[]\nnum_folds=5\nfold_start=0\nfor k in range(num_folds-1):\n    fold_end=int(((data_size/num_folds)*(k+1)))\n    folds.append((fold_start,fold_end))\n    fold_start=fold_end\nfolds.append((fold_start,data_size))\nprint(\"folds at:\",folds)\nprint(\"fold size:\", (data_size/num_folds))\nprint(\"train size:\",(data_size/num_folds)*(num_folds-1))\n\ncount=0\nfor i in folds:\n    count+=i[1]-i[0]\nprint(count)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09d3809c-6b74-ec93-e485-c72611a990cc"},"outputs":[],"source":"x_layer2=[]\nstart_time0 = time.time()\n\nfor fold_start,fold_end in folds:\n    print(\"Fold:\",fold_start,\"to\",fold_end,\"of\",data_size)\n    start_time1 = time.time()\n    fold_result=[]\n    \n    X_test = x[fold_start:fold_end].copy()\n    y_test = y[fold_start:fold_end].copy()\n    X_train=np.concatenate((x[:fold_start], x[fold_end:]), axis=0)\n    y_train=np.concatenate((y[:fold_start], y[fold_end:]), axis=0)\n    print(\"\\nfolding! len test {}, len train {}\".format(len(X_test),len(X_train)))\n    \n    for i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n        start_time = time.time()\n        regrList[i].fit(X_train,y_train)\n        print(\"\\nfit time:{}s\".format(round((time.time()-start_time), 3) ))\n\n        start_time = time.time()\n        print(regrList[i])\n        curr_predict=regrList[i].predict(X_test)\n        if fold_result == []:\n            fold_result = np.array(curr_predict.copy())\n        else:\n            fold_result = np.column_stack((fold_result,curr_predict))\n        \n        print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n        #show some stats on that last regressions run    \n        print(\"Mean abs error: {:.2f}\".format(np.mean(abs(curr_predict - y_test))))\n        print(\"Score: {:.2f}\".format(regrList[i].score(X_test, y_test)))\n    \n    #XGB -- it doesn't fit the pattern of scikit, so do it seperatly\n    #dtest = xgb.DMatrix(X_test)\n    #gbdt=xgbfit(X_train,y_train)\n\n    # now do a prediction and spit out a score(MAE) that means something\n    #start_time = time.time()\n    #curr_predict=gbdt.predict(dtest)\n    #fold_result = np.column_stack((fold_result,curr_predict))  \n    #print(\"XGB Mean abs error: {:.2f}\".format(np.mean(abs(curr_predict - y_test))))\n    #print(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n    \n    if x_layer2 == []:\n        x_layer2=fold_result\n    else:\n        x_layer2=np.append(x_layer2,fold_result,axis=0)\n        \n    print(\"--layer2 length:\",len(x_layer2))\n    print(\"--layer2 shape:\",np.shape(x_layer2))\n    print(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   \nprint(\"Full run time:{}s\".format(round((time.time()-start_time0), 3) ))   "},{"cell_type":"markdown","metadata":{"_cell_guid":"ebb6558b-a6a4-b08e-c513-296db5f2bd24"},"source":"train layer 2\n-------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b088b0fa-2d0f-1f28-5606-eae4fe9ec08f"},"outputs":[],"source":"print(len(x_layer2))\nprint(len(y))\n\n#  train/validation split\nX_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n                                                                                y,\n                                                                                test_size=0.25,\n                                                                                random_state=42)\nlayer2_regr=LinearRegression()\n\nlayer2_regr.fit(X_layer2_train,y_layer2_train)\n\nlayer2_predict=layer2_regr.predict(X_layer2_validation)\n\n#show some stats on that last regressions run    \nprint(\"Mean abs error: {:.2f}\".format(np.mean(abs(layer2_predict - y_layer2_validation))))\nprint(\"Score: {:.2f}\".format(layer2_regr.score(X_layer2_validation, y_layer2_validation)))\n\n\n#with LinearReg: Mean abs error: 1238.52"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50eea161-66b6-a7ca-331b-f0f7b96998b6"},"outputs":[],"source":"# The XGB version of layer 2\nprint(len(x_layer2))\nprint(len(y))\n\n#  train/validation split\nX_layer2_train, X_layer2_validation, y_layer2_train, y_layer2_validation = train_test_split( x_layer2,\n                                                                                y,\n                                                                                test_size=0.25,\n                                                                                random_state=42)\n#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\ndtest = xgb.DMatrix(X_layer2_validation)\n#layer2_gbdt=xgbfit(X_layer2_train,y_layer2_train)\n\n# now do a prediction and spit out a score(MAE) that means something\nstart_time = time.time()\n#print(\"XGB Mean abs error: {:.2f}\".format(np.mean(abs(layer2_gbdt.predict(dtest) - y_layer2_validation))))\nprint(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n#with LinearReg: XGB Mean abs error: 1205.77"},{"cell_type":"markdown","metadata":{"_cell_guid":"d68e95da-f8d3-e17a-acbf-c82d762ea286"},"source":"Predict layer 1 on test\n-----------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dad5cb1c-42e3-59b6-1c0f-247b80886d4e"},"outputs":[],"source":"x_layer2_test = []\nstart_time1 = time.time()\nfor i in range(len(regrList)): # for each of the regressions we use, fit/predict the data\n    start_time = time.time()\n    print(regrList[i])\n    curr_predict=regrList[i].predict(x_test_data)\n    print(\"predict time:{}s\".format(round((time.time()-start_time), 3) ))\n    \n    if x_layer2_test == []:\n        x_layer2_test = np.array(curr_predict.copy())\n    else:\n        x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n    print(curr_predict)\n\n#XGB -- it doesn't fit the pattern of scikit, so do it seperatly\ndtest = xgb.DMatrix(x_test_data)\n# now do a prediction and spit out a score(MAE) that means something\nstart_time = time.time()\n#curr_predict=gbdt.predict(dtest)\n#x_layer2_test = np.column_stack((x_layer2_test,curr_predict))\n#print(\"Mean abs error: {:.2f}\".format(np.mean(abs(cache[i+1] - y_test))))\nprint(\"XGB predict time:{}s\".format(round((time.time()-start_time), 3) ))\n\nprint(\"Fold run time:{}s\".format(round((time.time()-start_time1), 3) ))   "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5f61966-8e94-54bb-3e96-1fc76356ed2f"},"outputs":[],"source":"# some problems noted---fact finding below!\ndisplay(\"size of original test data:\",len(x_test_data))\ndisplay(\"Test shape:\",np.shape(x_layer2_test))\ndisplay(\"train shape:\",np.shape(x_layer2))\n\nprint(\"sample of layer2 test:\\n\",x_layer2_test[:4])\n\nprint(\"x_layer2_test mean:\",x_layer2_test.mean( axis=0))\nprint(\"x_layer2 mean:\",x_layer2.mean(axis=0))\ntrain_layer2_col0_mean=x_layer2.mean(axis=0)[0]\n\nprint(\"x_layer2_test std:\",x_layer2_test.std( axis=0)) \nprint(\"x_layer2 std:\",x_layer2.std(axis=0))\n\n# notice that column 0(linregresion) has a significantly higher mean and std\n# here's a hack to not fix that for now! \n\n# check which row in column 0 are significantly far from the mean\nproblem_column=x_layer2_test.T[0]\noutliers=[]\nfor i in range(len(problem_column)):\n    if problem_column[i]>30000:\n        outliers.append((i,problem_column[i]))\nprint(\"num outliers:\",len(outliers))\n\n#for each problem child, set them to the average value from the train set, to null the affect some\nfor o in outliers:\n    problem_column[o[0]]=train_layer2_col0_mean\n    \nprint(problem_column[o[0]])\n\n#check outliers again\nproblem_column=x_layer2_test.T[0]\noutliers=[]\nfor i in range(len(problem_column)):\n    if problem_column[i]>30000:\n        outliers.append((i,problem_column[i]))\nprint(\"num outliers:\",len(outliers))\n\nprint(x_layer2_test.T[0][o[0]]) # verify that the change made it all the way to the original"},{"cell_type":"markdown","metadata":{"_cell_guid":"955778bb-5c2f-be97-b660-46585824f5f0"},"source":"Predict Layer 2\n---------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1dd418a-429d-fe24-4887-7960127f623c"},"outputs":[],"source":"test_data['loss']=layer2_regr.predict(x_layer2_test)\n\nresult=test_data[['id','loss',]]\noutput_fname=\"result_submission_stack.csv\"\ndisplay(writeData(result,output_fname))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99dfb5d6-4bee-1062-ebd1-2338d4c63631"},"outputs":[],"source":"#the XGB version:\n\ndtest = xgb.DMatrix(x_layer2_test)\n#test_data['loss']=layer2_gbdt.predict(dtest)\n\nresult=test_data[['id','loss',]]\noutput_fname=\"result_submission_stack_xgb.csv\"\ndisplay(writeData(result,output_fname))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c51ff678-ad95-0f95-6c2c-abb0af307232"},"outputs":[],"source":"#let's have a look at the std of the result, as a cross check\nprint(\"result std:\",result.std(axis=0))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8ba82c66-b8f0-5d01-7634-2635e8e47384"},"source":"EOF\n---"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}