{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"0decc98a-62b5-38ad-92ad-c88fe3dbef70"},"source":"EDA Python.\n\nThis is my first kernel on Kaggle. Please share your views, to help me improve."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"998e5766-f27c-541b-3c2c-2d8fcf0a7b4b"},"outputs":[],"source":"#Please go through the comments in each cell.\nimport pandas as pd\nimport numpy as np\nimport scipy\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import VarianceThreshold\nimport scipy.stats as stats"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c64996ed-8108-7356-a097-9e7612fce78a"},"outputs":[],"source":"#Fetching the training data as a pandas dataframe for visualization\ntrain_df = pd.read_csv(\"../input/train.csv\")\n#deleting the column 'id' from the dataframe as it is a unique and does not have any effect on the algorithm\ndel train_df['id']\n\"../input/train.csv\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d906098a-e1e2-a525-ff37-3d2297f5350f"},"outputs":[],"source":"#creating 2 seperate dataframes for categorical and continuous features.\ntrain_df_cat = pd.DataFrame()#training data frame with categorical features\ntrain_df_cont = pd.DataFrame() #training data with continuous features\ncat_list = []#list of categorical features\ncont_list = []#list of continuous features\n\n#populating the created data frames for categorical and continuous features\nfor each_column in train_df.columns:\n    if train_df[each_column].dtype == 'float':\n        cont_list.append(each_column)\ncont_list.remove('loss')\nfor each_column in train_df.columns:\n    if train_df[each_column].dtype == 'object':\n        cat_list.append(each_column)\n\nfor i in range(0,len(cat_list)):\n    train_df_cat[i] = train_df[cat_list[i] ]\ntrain_df_cat.columns =cat_list\nfor i in range(0,len(cont_list)):\n    train_df_cont[i] = train_df[cont_list[i] ]\ntrain_df_cont.columns =cont_list\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3879141d-b58b-41cf-a40d-4f5af2d7f134"},"outputs":[],"source":"\n#checking for missing values:\ntrain_df.isnull().sum().sum()\n#sum is 0 which indicates there are no misisng values in the training data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd468408-11ed-8cee-f054-e2bb3a32ce3d"},"outputs":[],"source":"#Univariate analysis:\n#It is method of analysis where each variable can be analysed independently.\n#For analysing the distributions of each feature, I have used  4 methods:\n    #1.Histogram\n    #2.Boxplot\n    #3.stats.skew to measure the skewness in each continuous feature\n#For the first three methods, I have used the plot() function with just changing the arguements for respective plots\n#ex: df.plot(kind = 'hist') for histogram. I have customized the plot using other arguements to the plot function such\n #as (subplot = True) for plotting each column (each features) independently.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# change outlier point symbols\nhist_dist = train_df_cont.plot(kind='hist', subplots=True,layout = (5,3),figsize = (15,15),sharex = False)\n\n#On analysing the histogram of all the continuous features, the features cont1,cont5,cont7,cont8,cont9,cont13 seems to be \n#skewed. Let us confirm, if that is the case with the remaining methods.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01f55ade-31c3-924a-0c17-1b97d6ba7d32"},"outputs":[],"source":"#Method 2: Density plots\ndensity_plot_cont = train_df_cont.plot(kind='density', subplots=True,layout = (5,3),figsize = (15,15),sharex = False)\n\n#On analysing the density plots, it almost presents a similar picture as the histogram. The same set of features \n #cont1,cont5,cont7,cont8,cont9,cont13 seems to be skewed, while the other features seems to be fairly symmetrical.\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6abd5030-9dee-a048-128f-1ac080673930"},"outputs":[],"source":"#Method 3: Box plots\n\ncolor = dict(boxes='DarkGreen', whiskers='DarkOrange',medians='DarkBlue', caps='Gray')\nbox_plot_cont = train_df_cont.plot(kind='box', subplots=True,layout = (5,3),color=color, sym='r+',figsize = (15,15),\n                                   sharex = False,showfliers=True)\n\n#After all we are learning machine learning and avoiding redundancy is a key component hence would end the analysis\n#with just stating similar results. But one key point to be highlighted in this case, is the presence of outliers \n#in the features cont 7, cont 9, cont 10.  eliminating outliers is a one of the important pre processing step before \n#applying the algorithm."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8afae7e-05e7-4591-4e27-f1acf5a71de8"},"outputs":[],"source":"#Method 4: stats.skew\n#Using the general thumb rule\n#If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n#If the skewness is between -1 and â€“ 0.5 or between 0.5 and 1, the data are moderately skewed\n#Hence fetching the list of features with skewness above and below the specified limit.\n\nimport scipy.stats as stats\nskew_list = []\nfor each_column in train_df_cont.columns:\n    skew_list.append(round(scipy.stats.skew(train_df_cont[each_column],bias = False),2))\nskew_dict = dict(zip(train_df_cont.columns,skew_list))\nprint (\"The skewness in each continuous features are:\",skew_dict)\n#Fetching the list of continuous features to be normalized\nto_be_normalized = []\nfor keys,value in skew_dict.items():\n    if  not((-0.5 < value < 0.5)):\n        to_be_normalized.append(keys)\n\nprint ('\\t')\nprint (\"The continuous features to be normalized are\",to_be_normalized)\n\nprint (train_df_cont.head())\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df7a4080-c159-f608-24c7-1a31808026a1"},"outputs":[],"source":"#Feature pre processing of continuous features.\n#Now that we have an idea of continuous features, let us get into preprocessing\n\n#We are going to try Log transformation on the continuous features to reduce skewness.\n\nfor each_column in to_be_normalized:\n    print (\"The skew in\", each_column ,\"before applying the transformation:\",train_df_cont[each_column].skew())\n \n    train_df_cont[each_column] = np.log1p(train_df_cont[each_column])\n    print (\"The skew in\", each_column ,\"after applying the transformation:\",train_df_cont[each_column].skew())\n    print ('\\t')\n    \n#You can see the reduction is skewness, but still the features 'cont5','cont7','cont9' have high skewness"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24717e9c-ef30-74c6-5c1b-732ef2f8b92e"},"outputs":[],"source":"#Multivariate Data Analysis:\n#lets analyze and intrepret a key factor in the training data set, correlation between the features.\n#Stating the general intitution, for better performance of a machine learning algorithm, the features has to be highly\n#correalted with the class variable and ideally uncorrealted with the other features in the feature set.\n#Lets perform the correaltion analysis to check if the above mentioned condition holds for our training data.\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#creating a dataframe for storing the correaltion values between each continuous features. Could use any other data \n#structure as well, using pandas the code for analysis could be concise.\ntrain_df_cont_corr = pd.DataFrame()\n#populating the created dataframe\ntrain_df_cont_corr = train_df_cont.corr()\nplt.subplots(figsize=(21, 11))\nsns.heatmap(train_df_cont_corr,annot=True)\n\nplt.show()\n\n#Now that we have a visual image of the correaltion between the continuous features, lets get the list of columns \n#which has correaltion above the specified limit of 0.6 in the heatmap.\n\nfor index,rows in train_df_cont_corr.iterrows():\n    for each_column in train_df_cont_corr.columns:\n        if rows[each_column] > 0.6 and rows[each_column] != 1:\n            print (\"The correaltion of continous features:\", index, \"and\",each_column,\"is\", rows[each_column])\n\n#Now we have the list of features which has inter feature correaltion above 0.6. I have added the condition '!= 1'\n#beacuse it is clear from the heat map that a feature has a correlation of 1 only with itself, hence removing the \n#redundant information"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21c90d00-3c59-07c5-2666-a224145dc0ac"},"outputs":[],"source":"#If a feature's value are all same, it cannot give us extra information. Hence finding the vatriance in data for \n#all the continuous features\nvt = VarianceThreshold()\nxt = vt.fit_transform(train_df_cont)\nvariance_dict = dict(zip(train_df_cont.columns,vt.variances_))\nprint (variance_dict)\nprint (min(variance_dict, key = variance_dict.get))\n\n#Result: Found that the feature 'cont7' has the least variance."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3218c514-05ee-250b-327b-8f523b25ade3"},"outputs":[],"source":"#Let us analyse the feature importance of the continuous features.\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\ntrain_df_cont['loss'] = train_df['loss']\narray = train_df_cont.values\n\n\nX = array[:,0:14]\nY = array[:,14].astype(int)\n\ntest = SelectKBest(score_func=f_regression, k=4)\nfit = test.fit(X, Y)\nprint (\"The feature importance for the various continuous features are:\")\nd = dict(zip(train_df_cont.columns,fit.scores_))\nsorted(d.items(), key=lambda x: (-x[1], x[0]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d16083a-0396-8d7b-529e-c7676f22b0d0"},"outputs":[],"source":"#Now that, we have pre processed the continuous features lets analyze the categorical data.\n#Intially we will see the number of categories in each categorical feature\n\ncategory_list = []\nfor each_column in train_df_cat.columns:\n    print (\"The number of categories in the feature\",each_column,\"are:\",len(train_df_cat[each_column].unique()))\n\n#Now we know the number of categories in each categorical feature, for better performance of the algorithm \n#we will encode categories with numerical value"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}