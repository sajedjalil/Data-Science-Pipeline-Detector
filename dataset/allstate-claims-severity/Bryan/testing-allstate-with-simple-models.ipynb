{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"74b1d89f-834c-2d65-af3a-955335e05170"},"source":"Examine the columns and convert the categorical data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c74cca73-833e-02d9-ffb4-da782668ac5f"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n%matplotlib inline\n\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6391f8b-31c5-a603-e8be-a12baa1b8e14"},"outputs":[],"source":"data_train_raw = pd.read_csv('../input/train.csv')\ndata_test_raw = pd.read_csv('../input/test.csv')\n# print(data_train_raw.dtypes)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2fd68f19-8721-4171-02aa-9143242096b7"},"source":"How many unique values are in each categorical column?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac861346-424e-cc35-3a9a-5483154f7bd0"},"outputs":[],"source":"col_uniques=[]\nfor col in data_train_raw.columns:\n    if (col.find('cat') !=-1):\n        col_uniques.append([col, len(data_train_raw[col].unique())])\nprint(col_uniques)"},{"cell_type":"markdown","metadata":{"_cell_guid":"dcc00d4a-6b9e-2231-244f-95250f4efe08"},"source":"Now convert the data using the label encoder."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c06bf03-0a64-3cd9-1d9f-9ccf2a8848a7"},"outputs":[],"source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nfor col in data_train_raw.columns:\n    if (col.find('cat') !=-1):\n      #  print(col)\n        data_train_raw[str(col+'_numerical')]=le.fit_transform(data_train_raw[col])\n        data_test_raw[col] = data_test_raw[col].map(lambda s: '<unknown>' if s not in le.classes_ else s)\n        le.classes_ = np.append(le.classes_, '<unknown>')\n        data_test_raw[str(col+'_numerical')]=le.transform(data_test_raw[col])\nprint(data_train_raw.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f6bf1ec9-18dc-fea4-32a8-97bfecb59429"},"source":"There are many different columns now, so it will be difficult to view all of the data. While we could loop through all of the categories and look for interesting things, lets first do a principle component analysis. First I will normalize all of the numerical columns (after splitting into a training and validation sample)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57cf5300-3286-e1ab-8f41-307c75c21451"},"outputs":[],"source":"XCols =[0]\ndatacols=data_train_raw.columns\nfor c in range(len(datacols)):\n    if(datacols[c].find('cont')!=-1 or datacols[c].find('numerical')!=-1):\n        XCols.append(c)\nX_total = data_train_raw[XCols]\nY_total = data_train_raw['loss']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b24e0fa8-d957-99dc-b51d-f9f6a4a4cd53"},"outputs":[],"source":"XColst =[0]\ndatacols=data_test_raw.columns\nfor c in range(len(datacols)):\n    if(datacols[c].find('cont')!=-1 or datacols[c].find('numerical')!=-1):\n        XColst.append(c)\nX_test = data_test_raw[XColst]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fe35fe5-1ce8-e34a-3506-fc8e7e2aef47"},"outputs":[],"source":"# data_test_raw[XColst]"},{"cell_type":"markdown","metadata":{"_cell_guid":"63635e81-188b-9849-1318-be1768b7f8ce"},"source":"Let's look at the loss column. This will show that it will be better to predict the log of the loss and then take the exponent of the prediction later."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06cd607e-bdb2-4f27-3fcc-407cc98ab566"},"outputs":[],"source":"plt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.hist(Y_total,100)\nplt.title('loss')\n\nplt.subplot(1,2,2)\nplt.hist(np.log(Y_total),100)\nplt.title('log(loss)')\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e06763d-f0e2-c9b2-7802-0474897ff71d"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_total, Y_total, test_size=0.4, random_state=0)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0b86deff-508e-b58a-b96a-aabd258e9c06"},"source":"Now we normalize all of the columns, we will save the transformation parameters so they can be used by the validation and test samples. Note that the 'cont' columns are already in the range (0,1)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8adf8866-9943-61f8-6d50-3e1d50362bc3"},"outputs":[],"source":"stds=[1]\nmeans=[0]\nxcols=list(X_train.columns)\n\n\nfor c in range(1,len(xcols)):\n    mm = X_train[xcols[c]].mean()\n    ss = X_train[xcols[c]].std()\n    \n    means.append(mm)\n    stds.append(ss)\n    \n#    print(xcols[c],r)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8f6a226-ce95-d998-1bb9-23630b71cc98"},"outputs":[],"source":"X_train = (X_train[xcols] - means) / stds\nX_valid = (X_valid[xcols] - means) / stds\nX_test = (X_test[xcols] - means) / stds\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb9508ab-eb68-d4a0-ebc6-8d048002bfe5"},"outputs":[],"source":"xcols.remove('id')\n\nprint(\"Train\")\nprint(X_train[xcols[100]].describe())\nprint(\"Valid\")\nprint(X_valid[xcols[100]].describe())\nprint(\"Test\")\nprint(X_test[xcols[100]].describe())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5660f66-4675-4a42-530c-a669903c1ab4"},"outputs":[],"source":"from sklearn.cluster import KMeans\nkm = KMeans(n_clusters=8,n_jobs=-1)\nkm.fit(X_train[xcols])\n\nX_train['km']=km.predict(X_train[xcols])\nX_valid['km']=km.predict(X_valid[xcols])\nX_test['km']=km.predict(X_test[xcols])\nxcols.append('km')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57fb8d89-1e7d-f5fb-5820-8ad61016d632"},"outputs":[],"source":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(X_train[xcols])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b368412e-d2b6-2f99-0975-4e46ad84d357"},"outputs":[],"source":"X_train_transformed = pca.transform(X_train[xcols])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14dac1be-b4f3-a17b-d839-547eca07e047"},"outputs":[],"source":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.scatter(X_train_transformed[:,0],y_train)\nplt.title('First Axis')\nplt.ylabel('loss')\nplt.yscale('log')\n\nplt.subplot(1,2,2)\nplt.scatter(X_train_transformed[:,1],y_train)\nplt.title('Second Axis')\nplt.yscale('log')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0cbd855-57ef-1030-1065-643ea65d9da4"},"outputs":[],"source":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_train_transformed[:,0], X_train_transformed[:,1], np.log(y_train))\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1be098c3-e7ea-4d75-8f3a-2f2a7c5e91c3"},"source":"There may be some correlations in the data here, but it appears that the PCA here is loosing much of the variance."},{"cell_type":"markdown","metadata":{"_cell_guid":"16b4dfc0-4953-f621-e62d-6c8f48c60cce"},"source":"## Attempt some simple models"},{"cell_type":"markdown","metadata":{"_cell_guid":"cb319f56-f5c3-f41b-f16a-643d364dc7f8"},"source":"### Random forrest"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73d5afd7-e8ff-aa90-d8d6-5d672eeb8547"},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrfr = RandomForestRegressor(n_estimators= 400, n_jobs=-1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"786fc95a-f763-19a6-0ebc-b5da2d42a402"},"outputs":[],"source":"rfr.fit(X_train[xcols],np.log(y_train))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f3f6509c-0408-676c-0b79-3f7c474c58df"},"outputs":[],"source":"X_train['rfr']=np.exp(rfr.predict(X_train[xcols]))\nX_valid['rfr']=np.exp(rfr.predict(X_valid[xcols]))\nX_test['rfr']=np.exp(rfr.predict(X_test[xcols]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6a01dc0-f700-09b0-4b52-26274e9c48e5"},"outputs":[],"source":"trainscore=mean_squared_error(X_train['rfr'],y_train)\nvalidscore=mean_squared_error(X_valid['rfr'],y_valid)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a91cbf6-d073-7823-1a8a-592b7da79376"},"outputs":[],"source":"plt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.title(r'Training score='+str(trainscore))\nplt.scatter(X_train['rfr'], y_train)\nplt.xlabel('Prediction')\nplt.ylabel('Truth')\n\nplt.subplot(1,2,2)\nplt.title(r'Validation score='+str(validscore))\nplt.scatter(X_valid['rfr'], y_valid)\nplt.xlabel('Prediction')\nplt.ylabel('Truth')\n\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0e38306c-8deb-4c08-d2d4-d0c088e52af7"},"source":"We see that the validation set is preforming much worse with the default random forrest hyper parameters. We could try fixing this for overtraining. However, lets first run this on the test set to get a first score."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73034f60-1f0b-bfa1-32b3-9d8328c3d981"},"outputs":[],"source":"X_test.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26be6d02-2b20-53d7-1358-a2444d5eea85"},"outputs":[],"source":"rfrpred=pd.DataFrame(list(zip(X_test['id'],X_test['rfr'])),columns=('id','loss'))\nrfrpred['id']=rfrpred['id'].astype('int')\n                     "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"923af531-5f33-d5f2-14b8-0b4db6880fd2"},"outputs":[],"source":"rfrpred.to_csv('submit_RFR_' +str(validscore) +'.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2299599d-71a8-e9ea-6554-02ab4f48c9ae"},"outputs":[],"source":"list(enumerate(sorted(list(zip(xcols,rfr.feature_importances_)), key=lambda l:l[1], reverse=True)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e6fa67f-514d-b20d-4646-71023f8987cc"},"source":"### Linear Regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8c54655-9785-986a-18bf-42d6b51307dd"},"outputs":[],"source":"from sklearn.linear_model import Ridge"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3f5838b-fd8b-eb3d-1b6a-ed998dbd515b"},"outputs":[],"source":"rid = Ridge(alpha=1e-6, fit_intercept=True, normalize=False, \n                  copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87b5e1a4-1e45-9a70-f57d-322894a62e9b"},"outputs":[],"source":"rid.fit(X_train[xcols],np.log(y_train))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7fa638a-a547-c5fc-8946-8ae17a7b7543"},"outputs":[],"source":"X_train['rid'] = np.exp(rid.predict(X_train[xcols]))\nX_valid['rid'] = np.exp(rid.predict(X_valid[xcols]))\nX_test['rid'] = np.exp(rid.predict(X_test[xcols]))\n\ntrainscore=mean_squared_error(X_train['rid'],y_train)\ntestscore=mean_squared_error(X_valid['rid'],y_valid)\nprint(trainscore,testscore)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"069d971f-cddd-578f-82e5-33bb94606e36"},"outputs":[],"source":"plt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.title(r'Training score='+str(trainscore))\nplt.scatter(X_train['rid'], y_train)\nplt.xlabel('Prediction')\nplt.ylabel('Truth')\n\nplt.subplot(1,2,2)\nplt.title(r'Validation score='+str(testscore))\nplt.scatter(X_valid['rid'], y_valid)\nplt.xlabel('Prediction')\nplt.ylabel('Truth')\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5bab303-1cfd-1acc-2366-0e8cc2717b05"},"outputs":[],"source":"ridpred=pd.DataFrame(list(zip(X_test['id'],X_train['rid'])),columns=('id','loss'))\nridpred['id']=ridpred['id'].astype('int')\nridpred.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66b3ca34-9fb4-f777-cfd2-3de01207556a"},"outputs":[],"source":"rid.feature_importances_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2082c85a-deca-27a9-c748-a0a7193cd502"},"outputs":[],"source":"ridpred.to_csv('submit_ridge_' +str(testscore) +'.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ebf3770-db80-95c3-d18f-9a78440db44f"},"source":"## Basic NN"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05f3e44f-7962-df98-6171-986c0095e506"},"outputs":[],"source":"from sklearn.neural_network import MLPRegressor\n\nmlpnnR = MLPRegressor(hidden_layer_sizes=(int(X_train.shape[1]/2),int(X_train.shape[1]/2), int(X_train.shape[1]/2),int(X_train.shape[1]/2)), \n                       activation='logistic', \n                       solver='adam', \n                       alpha=0.1, \n                       batch_size='auto',\n#                        learning_rate='adaptive',\n                       learning_rate_init=0.0001,\n                       power_t=0.5, max_iter=200,\n                       shuffle=True, \n                       random_state=None, \n                       tol=0.0001, \n                       verbose=True,\n                       warm_start=False,\n                       momentum=0.9,\n                       nesterovs_momentum=True, early_stopping=False, \n                       validation_fraction=0.1, beta_1=0.9, beta_2=0.999, \n                       epsilon=1e-08)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db43529c-11a1-997e-067c-7c2951bbb9d2"},"outputs":[],"source":"mlpnnR.fit(X_train[xcols],np.log(y_train))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9176968f-6b10-70e7-ce21-399fe70ce91b"},"outputs":[],"source":"X_train['nn'] = np.exp(mlpnnR.predict(X_train[xcols]))\nX_valid['nn'] = np.exp(mlpnnR.predict(X_valid[xcols]))\nX_test['nn'] = np.exp(mlpnnR.predict(X_test[xcols]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"300b1605-785d-a33a-b141-d0a6cb420958"},"outputs":[],"source":"trainnnpred=X_train['nn']\nvalidnnpred=X_valid['nn']\n\nnnscoret=mean_squared_error(trainnnpred,y_train)\nnnscorev=mean_squared_error(validnnpred,y_valid)\n\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nplt.title(r'Training score='+str(nnscoret))\nplt.scatter(trainnnpred, y_train)\nplt.xlabel('Prediction')\nplt.ylabel('Truth')\n\nplt.subplot(1,2,2)\nplt.title(r'Validation score='+str(nnscorev))\nplt.scatter(validnnpred, y_valid)\nplt.xlabel('Prediction')\nplt.ylabel('Truth')\n\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d152cea0-2d91-be68-1012-e181190d6267"},"outputs":[],"source":"mlpout=pd.DataFrame(\n    list(zip(X_test['id'],X_test['nn'])),\n    columns=('id','loss'))\nmlpout['id']=mlpout['id'].astype('int')\nmlpout.head()\nmlpout.to_csv('submit_nnet_' +str(nnscorev) +'.csv', \n               index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"002679bf-d8aa-33d2-53ec-386e1237ae63"},"source":"## Make an average"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d88cdca-73bf-4fe7-6838-c208bb621bae"},"outputs":[],"source":"Ave2=pd.DataFrame()\nAve2['id']=ridpred['id']\nAve2['loss']=(1/testscore*ridpred['loss'] + 1/validscore*rfrpred['loss'] + 1/nnscorev*mlpout['loss'])/(1/testscore+1/validscore+1/nnscorev)\n\nAve2.head()\nAve2.to_csv('Average.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}