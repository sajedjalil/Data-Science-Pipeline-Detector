{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of contents:\n0. Dependencies \n1. Data\n2. Class definition for the estimator\n3. Logistic regression models\n4. SVC (*to be continued*)\n5. Adaboost (*to be continued*)\n6. .. other estimators"},{"metadata":{"id":"J2NlEZD5HEF6","colab_type":"text"},"cell_type":"markdown","source":"In this tutorial, I'll try some techniques for feature selection, applied to different optimizers in order to increase LB score on the test dataset. For this purpose, I'll explore predicted probabilities distribution on test data set, with the aim to understand how CV score and LB score are correlated and predict which model can give high submission result before submitting.\n\nIt this tutorial *I will not* provide EDA.\n\nHere, I'll not promise to show you best results of LB, but I'll try my best, I promise ;) Currently I've raeched public LB 0.847 and private LB 0.839.\n\nAnother important goal is to find **correlation between CV score on train dataset and LB score on test dataset**, and determine how you can evaluate your results without actual test targets.\n"},{"metadata":{"id":"lkBDQP3CKbI-","colab_type":"text"},"cell_type":"markdown","source":"# 0. Dependencies"},{"metadata":{"id":"g2ZyOIvuKfG-","colab_type":"text"},"cell_type":"markdown","source":"## 0.1 Import"},{"metadata":{"id":"4no3ws3lz8Yc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve    \nfrom sklearn import linear_model \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nimport IPython\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"ehO-2PCgIW81","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\nsubm_df = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"E1Luc-olKhAO","colab_type":"text"},"cell_type":"markdown","source":"## 0.2 Pandas settings"},{"metadata":{"id":"aGaTghxyKjle","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"pd.set_option('max_rows', 105)\npd.set_option('max_columns', 105)\npd.set_option('max_colwidth', 150)\npd.options.display.float_format = '{:.3f}'.format","execution_count":null,"outputs":[]},{"metadata":{"id":"a--OMm2TKkHu","colab_type":"text"},"cell_type":"markdown","source":"# 1. Data"},{"metadata":{"id":"hsDVABOf0Lzl","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"X_train = train_df.drop(['id', 'target'], axis=1)\ny_train = train_df['target']\nX_test = test_df.drop(['id'], axis=1)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nn_fold = 20\nfolds = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"rl1IObpMKQg-","colab_type":"text"},"cell_type":"markdown","source":"# 2. Class definition for estimator"},{"metadata":{"id":"lTJQCmxoItmM","colab_type":"text"},"cell_type":"markdown","source":"For easier training and searching for hyperparameters process, I created estimator class, where I've wrapped several main sklearn functions."},{"metadata":{"id":"5CVEgzFSKTO3","colab_type":"text"},"cell_type":"markdown","source":"### 2.1 Estimator class"},{"metadata":{"id":"TEp8p2ZAKWzu","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class MyEstimator:    \n\n    def __init__(self, estimator, param_grid, folds):\n        self.grid = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring='roc_auc', cv=folds, verbose=0, n_jobs=-1)\n        self.estimator = estimator\n\n\n    def get_best_estimator(self):\n\n        return self.grid.best_estimator_\n\n\n    def best_params(self):\n\n        return self.grid.best_params_\n\n\n    def fit_grid(self, X, y, verbose=False):\n        self.grid.fit(X, y)\n        if verbose:\n            print('Best Parameters', self.grid.best_params_)\n\n\n    def train_estimator(self, X, X_test, y=y_train, folds=folds, verbose=False):\n\n        scores = []\n\n        for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n\n            X_train, X_valid = X[train_index], X[valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n\n            self.grid.best_estimator_.fit(X_train, y_train)\n            y_pred_valid = self.grid.best_estimator_.predict(X_valid).reshape(-1)\n\n            scores.append(roc_auc_score(y_valid, y_pred_valid))\n\n            if verbose:\n                print('CV mean AUC score (on train/valid set): {0:.4f}, std: {1:.4f}'.format(np.mean(scores), np.std(scores)))  \n\n        return scores\n\n\n    def predict_probabilities(self, X, target_value=1):\n\n        return self.grid.best_estimator_.predict_proba(X)[:, target_value]\n\n\n    def predict_targets(self, X):\n\n        return self.grid.best_estimator_.predict(X)\n\n\n    def plot_probabilities(self, preds_proba, num_rows=1, num_cols=2, i=1, y_scale=1000, title_sbp='Hist', verbose=False):\n\n        if verbose:\n            print('Proba mean: {0:5f}'.format(preds_proba.mean()))\n\n        plt.subplot(num_rows, num_cols, i, facecolor='slategrey')\n        plt.hist(preds_proba, bins=40, color='powderblue')\n        plt.xticks(np.arange(0, 1.1, 0.1))\n        plt.yticks(range(0, y_scale, 100))\n        plt.grid(color='royalblue', linestyle=':', lw=1.3)\n        plt.title(title_sbp)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_Gk_3EKiKyQW","colab_type":"text"},"cell_type":"markdown","source":"### 2.2 Functions"},{"metadata":{"id":"kldMoJr2K5lf","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# this dataframe should be used for sumbission. Such a way we'll not mess up with the original dataset.\nsubmit_df = pd.DataFrame()\nsubmit_df['id'] = subm_df['id']","execution_count":null,"outputs":[]},{"metadata":{"id":"bM32ZDyiK8AI","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# to convert to CSV, enable argument `to_csv`\n# as parameters for estimator should be used k_dict\n# currently function supports only logreg and elastic net estimators\ndef run_and_submit(k_dict, to_csv=False, estimator_type=linear_model.LogisticRegression(), X_train=X_train, y_train=y_train, X_test=X_test, folds=folds, submit_df=submit_df):\n\n    for k_val in k_dict.keys():\n\n        # if we have unique k values in dict, they will be int. \n        # But if there are several runs in the dict with the same k_value - we need\n        # to diffirintiate them, converting in str type\n        if type(k_val) == str:\n            k_ind = int(k_val[:2]) # we're not gonna use more than 99 features here\n        else:\n            k_ind = k_val\n\n        selector = SelectKBest(f_classif, k=k_ind)\n        X_train_K = selector.fit_transform(X_train, y_train.values)\n        X_test_K = selector.transform(X_test)\n\n        params_ = k_dict[k_val][0]\n\n        estimator = MyEstimator(estimator_type, params_, folds)\n        estimator.fit_grid(X_train_K, y_train)\n        scores = estimator.train_estimator(X_train_K, X_test_K) \n\n        if to_csv:\n\n            if isinstance(estimator_type, linear_model.LogisticRegression):\n                submit_df['target'] = estimator.predict_probabilities(X_test_K)\n                name_csv = \"subm_LR_Kbst{}_C{}_{}_CV{}.csv\".format(str(k_ind), \n                                                                   str(estimator.grid.best_params_['C'])[1:],\n                                                                   str(k_dict[k_val][0]['penalty'])[2:4],\n                                                                   str(np.mean(scores))[1:6])\n                submit_df.to_csv(name_csv, index=False)\n\n\n            elif isinstance(estimator_type, linear_model.ElasticNet):\n\n                submit_df['target'] = estimator.predict_targets(X_test_K)\n\n                Lasso = False\n                if isinstance(estimator_type, linear_model.Lasso):\n                    Lasso = True\n\n                if Lasso:\n                    name_csv = \"subm_Lasso_Kbst{}_alpha{}_CV{}.csv\".format(str(k_ind), \n                                                                           str(estimator.grid.best_params_['alpha'])[1:],\n                                                                           str(np.mean(scores))[1:6])\n                else:\n                    name_csv = \"subm_ElNet_Kbst{}_alpha;l1_{};{}_CV{}.csv\".format(str(k_ind), \n                                                                                  str(estimator.grid.best_params_['alpha'])[1:],\n                                                                                  str(estimator.grid.best_params_['l1_ratio'])[1:],\n                                                                                  str(np.mean(scores))[1:6])\n\n                submit_df.to_csv(name_csv, index=False)\n\n\n            else:\n                print('Submission for this type of estimator is not avialiable yet')\n        ","execution_count":null,"outputs":[]},{"metadata":{"id":"69rNVXs7K9p-","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# as parameters for estimator should be used k_dict\n# currently function supports only logreg and elastic net estimators\ndef plot_results(k_dict, result_cols, estimator_type=linear_model.LogisticRegression(), figsize=(22, 12), rows=3, cols=3, y_scale=1000, X_train=X_train, y_train=y_train, X_test=X_test, folds=folds):\n\n    # function will plot probabilities for certain parameters of estimator\n    # returns dataframe with results\n    i = 1\n    result_df = pd.DataFrame(columns=result_cols)\n    plt.figure(figsize=figsize)\n\n    for k_val in k_dict.keys():\n    \n        # if we have unique k values in dict, they will be int. \n        # But if there are several runs in the dict with the same k_value - we need\n        # to diffirintiate them, converting in str type\n        if type(k_val) == str:\n            k_ind = int(k_val[:2])\n        else:\n            k_ind = k_val\n      \n        selector = SelectKBest(f_classif, k=k_ind)\n        X_train_K = selector.fit_transform(X_train, y_train.values)\n        X_test_K = selector.transform(X_test)\n\n        params_ = k_dict[k_val][0]\n\n        estimator = MyEstimator(estimator_type, params_, folds)\n        estimator.fit_grid(X_train_K, y_train)\n        scores = estimator.train_estimator(X_train_K, X_test_K)\n    \n    \n        if isinstance(estimator_type, linear_model.LogisticRegression):\n            result_df.loc[len(result_df)] = [k_ind, k_dict[k_val][0]['penalty'][0], k_dict[k_val][0]['C'][0], np.mean(scores), k_dict[k_val][1], k_dict[k_val][0]]\n            estimator.plot_probabilities(estimator.predict_probabilities(X_test_K), \n                                       num_rows=rows, num_cols=cols, i=i, y_scale=y_scale, \n                                       title_sbp='k = {}, C = {}, {}    CV = {:.3f}, LB = {:.3f} '.format(k_ind, k_dict[k_val][0]['C'][0], k_dict[k_val][0]['penalty'][0], \n                                                                                                          np.mean(scores), k_dict[k_val][1]))\n        elif isinstance(estimator_type, linear_model.ElasticNet):\n      \n            Lasso = False\n            if isinstance(estimator_type, linear_model.Lasso):\n                Lasso = True\n\n            if Lasso:\n                result_df.loc[len(result_df)] = [k_ind, k_dict[k_val][0]['alpha'][0], np.mean(scores), k_dict[k_val][1], k_dict[k_val][0]]\n                estimator.plot_probabilities(estimator.predict_targets(X_test_K), \n                                             num_rows=rows, num_cols=cols, i=i, y_scale=y_scale,\n                                             title_sbp='k = {}, alpha = {},    CV = {:.3f}, LB = {:.3f} '.format(k_ind, k_dict[k_val][0]['alpha'][0],\n                                                                                                                 np.mean(scores), k_dict[k_val][1]))\n            else:\n                result_df.loc[len(result_df)] = [k_ind, k_dict[k_val][0]['alpha'][0], k_dict[k_val][0]['l1_ratio'][0], np.mean(scores), k_dict[k_val][1], k_dict[k_val][0]]\n                estimator.plot_probabilities(estimator.predict_targets(X_test_K), \n                                             num_rows=rows, num_cols=cols, i=i, y_scale=y_scale,\n                                             title_sbp='k = {}, alpha/l1 = {}/{},    CV = {:.3f}, LB = {:.3f} '.format(k_ind, k_dict[k_val][0]['alpha'][0], k_dict[k_val][0]['l1_ratio'][0], \n                                                                                                                       np.mean(scores), k_dict[k_val][1]))      \n        else:\n            print('This type of estimator is not supported yet')\n        \n        i += 1\n\n    return result_df","execution_count":null,"outputs":[]},{"metadata":{"id":"IT7NlPSaKtX1","colab_type":"text"},"cell_type":"markdown","source":"# 3. Logistic regression Models"},{"metadata":{"id":"QDQDnzpOLBOw","colab_type":"text"},"cell_type":"markdown","source":"## 3.1 Logreg, l1 best from all results."},{"metadata":{"id":"5pZjYJkmLhce","colab_type":"text"},"cell_type":"markdown","source":"For feature selecton, we'll use SelectKBest function from sklearn, with f_classif. As I tried SelectPercentile function with f_classif and mutual_info_classif as score function, but they didn't provide LB score higher than 0.84. \nSo here, I'll not stop at evaluating the performance of the function for feature selection.\n\nAs some brute-force attempt for searching hyperparameters I've iterated through all features quantity from 3 features to 149 features, in each iteration used logistic regression model, with GridSearch cross validation (StratifiedKFold with n_splits=20), separately for l1 and l2 regularization and C from 0.01 to 0.5 (step 0.01). That attempt took some time, as it was 14406 iterations, but didn't give some reasonable results.\n\nIn the final file, I can select parameters with the highest CV score on train dataset, but this is not much useful if we didn't know the correlation between CV and LB score.\n\nAs an example, below there are 7 best results with the highest CV from brute force, and they will give poor LB score."},{"metadata":{"id":"St-GRW_pLD-3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":716},"outputId":"484f3ab3-a6c8-4118-f326-bcfac266c950","trusted":true},"cell_type":"code","source":"result_cols = ['k', 'penalty', 'C', 'CV', 'LB', 'params']\n# key of the dict - number of features to be selected\n# second value in tuple of dict values (0.736, 0.734, etc) is the LB score for that parameters set.\nk_dict_all_l1 = {39: ({'C': [0.46], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.736),\n                 40: ({'C': [0.46], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.734),\n                 49: ({'C': [0.46], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.735),\n                 46: ({'C': [0.46], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.729),\n                 47: ({'C': [0.46], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.730),\n                 52: ({'C': [0.46], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.731),\n                 43: ({'C': [0.45], 'class_weight': [None], 'fit_intercept': [False], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.731) }\n\nresult_all_l1 = plot_results(k_dict_all_l1, result_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_all_l1","execution_count":null,"outputs":[]},{"metadata":{"id":"GXXA5lKIOwcU","colab_type":"text"},"cell_type":"markdown","source":"But from that attempt we can take 2 important conclusions:  \n* high number of features (more than 20) will lead to overfitting;\n* this picture of the distribution of predicted probabilities on test dataset means that we'll have low LB score.\n\nAbout the distribution picture, I should say, that I tried at least hundreds of them with different models, parameters and selected features. And I have some sense how this distribution should look like for high LB score. *Good* distributions are shown below, and they're exactly opposite from pictures above."},{"metadata":{"id":"oyWH9W1HLEmJ","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.1 Logreg, l1 my selection"},{"metadata":{"id":"beDP0hO1QOHb","colab_type":"text"},"cell_type":"markdown","source":"In this attempt, I've selected a number of features and C value by careful LB probing. This technique showed better results.\nAs you can also see, here provided only models with 'l1' regularization because 'l2' with these parameters shows higher overfitting."},{"metadata":{"id":"4ii-ozRlLR6o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1151},"outputId":"592c6c70-0329-4721-9fad-23c862a0f908","trusted":true},"cell_type":"code","source":"result_cols = ['k', 'penalty', 'C', 'CV', 'LB', 'params']\nk_dict_myl1 = {'15r1':  ({'C': [0.336], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.839),\n               '15r2':  ({'C': [0.276], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.840),\n               '15r3':  ({'C': [0.2], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.842),\n               '15r4':  ({'C': [0.19], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.843),\n               '15r5':  ({'C': [0.18], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.843),\n               '15r6':  ({'C': [0.17], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.844),\n               '15r7':  ({'C': [0.16], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.844),\n               '15r8':  ({'C': [0.15], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.845),\n               '15r9':  ({'C': [0.14], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.845),\n               '15r10': ({'C': [0.13], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r11': ({'C': [0.125], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r12': ({'C': [0.12], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r13': ({'C': [0.11], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r14': ({'C': [0.1], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.845),\n               '15r15': ({'C': [0.097], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.845)}\nresult_myl1_df = plot_results(k_dict_myl1, result_cols, figsize=(22, 20), rows=5, cols=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"AbDxhcVBQzIR","colab_type":"text"},"cell_type":"markdown","source":"Pictures above should give you some perception, how final distribution should look like."},{"metadata":{"trusted":true},"cell_type":"code","source":"result_myl1_df","execution_count":null,"outputs":[]},{"metadata":{"id":"BbwX9E3sLGyJ","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.2 Logreg, l2 my selection"},{"metadata":{"id":"aOioGZJzRDVD","colab_type":"text"},"cell_type":"markdown","source":"Carefull LB probing with 'l2' regularization."},{"metadata":{"id":"_w-FRjafLUAn","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"result_cols = ['k', 'penalty', 'C', 'CV', 'LB', 'params']\nk_dict_myl2 = {'15r1':  ({'C': [0.8390], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.833),\n               '15r2':  ({'C': [0.276], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r3':  ({'C': [0.2], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r4':  ({'C': [0.19], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r5':  ({'C': [0.18], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r6':  ({'C': [0.17], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r7':  ({'C': [0.16], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r8':  ({'C': [0.15], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r9':  ({'C': [0.14], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r10': ({'C': [0.13], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r11': ({'C': [0.125], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r12': ({'C': [0.12], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r13': ({'C': [0.11], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r14': ({'C': [0.1], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n               '15r15': ({'C': [0.097], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.831),\n                    20: ({'C': [0.097], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.802),\n                    25: ({'C': [0.02], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.797),\n                    34: ({'C': [0.02], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.792),\n                    36: ({'C': [0.01], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.785) }\nresult_myl2_df = plot_results(k_dict_myl2, result_cols, figsize=(22, 28), rows=7, cols=3)","execution_count":null,"outputs":[]},{"metadata":{"id":"-Ntf6gLERMY-","colab_type":"text"},"cell_type":"markdown","source":"As you can see, distributions look a bit different than with 'l1', and this will give overfitting. Distributions should have some bulge shape, with a mean value close to the center, but biased a bit towards 1. If it is biased too much to 1, you'll have overfitting. If it's biased to zero, you'll have a lot of overfitting (LB score will be less than 0.5, but as already mentioned in the discussion section, it's not too much a problem, as you can just flip probabilities)."},{"metadata":{"trusted":true},"cell_type":"code","source":"result_myl2_df","execution_count":null,"outputs":[]},{"metadata":{"id":"8ZpzT0ysLIbx","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.3 Best l1 and l2"},{"metadata":{"id":"CqaIT08yS4od","colab_type":"text"},"cell_type":"markdown","source":"Here 3 best distributions for 'l1' and for 'l2'."},{"metadata":{"id":"frkAocBSLXIA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":499},"outputId":"aaca292f-587f-48fc-a98b-803ce418f6e2","trusted":true},"cell_type":"code","source":"k_dict_l1l2 = {'15r1': ({'C': [0.13], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r2': ({'C': [0.12], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r3': ({'C': [0.11], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l1'], 'solver': ['liblinear']}, 0.846),\n               '15r4':  ({'C': [0.2], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r5':  ({'C': [0.276], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.832),\n               '15r6':  ({'C': [0.8390], 'class_weight': ['balanced'], 'fit_intercept': [True], 'penalty': ['l2'], 'solver': ['liblinear']}, 0.833)}\n_ = plot_results(k_dict_l1l2, result_cols, figsize=(22, 8), rows=2, cols=3)                          ","execution_count":null,"outputs":[]},{"metadata":{"id":"u22Zz55zLZFA","colab_type":"text"},"cell_type":"markdown","source":"### 3.1.4 LB and CV graph"},{"metadata":{"id":"-KagFqEoTI-l","colab_type":"text"},"cell_type":"markdown","source":"Also worth mentioning, that with logistic regression,  15 features and rather strong regularization, our CV score is lower than resulting LB score. This tendency was constant while number of features is smaller than 20. \nGraphs below show how LB and CV changes with C value with 15 features of logreg model.\n\nWeaker regularization will increase CV, but decrease LB."},{"metadata":{"id":"1f0IOzEZLfmh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":405},"outputId":"3c4762bf-7b28-4cb5-a650-f4205f02b630","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 6), facecolor='lightgrey')\n\nplt.subplot(121, facecolor='slategrey')\n_ = plt.plot(result_myl1_df[result_myl1_df['k'] == 15]['C'], result_myl1_df[result_myl1_df['k'] == 15]['CV'], c='salmon', ls='--')\n_ = plt.plot(result_myl1_df[result_myl1_df['k'] == 15]['C'], result_myl1_df[result_myl1_df['k'] == 15]['LB'], c='navy')\nplt.xlabel('C value')\nplt.ylabel('AUC score')\nplt.xticks(np.arange(.1, .35, .05))\nplt.yticks(np.arange(.765, .850, .005))\nplt.grid(color='midnightblue', linestyle=':', lw=.7)\n_ = plt.legend(('CV', 'LB'), loc='lower right')\n_ = plt.title('[LB and CV] versus [C value] for l1, k = 15')\n\n\nplt.subplot(122, facecolor='slategrey')\n_ = plt.plot(result_myl2_df[(result_myl2_df['k'] == 15)]['C'], \n             result_myl2_df[(result_myl2_df['k'] == 15)]['CV'], c='salmon', ls='--') \n\n_ = plt.plot(result_myl2_df[result_myl2_df['k'] == 15]['C'], \n             result_myl2_df[result_myl2_df['k'] == 15]['LB'], c='navy')\nplt.xlabel('C value')\nplt.ylabel('AUC score')\nplt.xticks(np.arange(.1, 0.8, .05))\nplt.yticks(np.arange(.765, .850, .005))\nplt.grid(color='midnightblue', linestyle=':', lw=.7)\n_ = plt.legend(('CV', 'LB'), loc='lower right')\n_ = plt.title('[LB and CV] versus [C value] for l2, k = 15')","execution_count":null,"outputs":[]},{"metadata":{"id":"oRoizyU4LJvh","colab_type":"text"},"cell_type":"markdown","source":"## 3.2 Lasso [LB 0.847 / 0.839]"},{"metadata":{"id":"5JaXB5FTUXND","colab_type":"text"},"cell_type":"markdown","source":"Attempt with lasso estimator. This estimator shouldn't give much different result, as basically, it's a linear model with l1 regularisation. Parameters below were resulted from the iteration process with a small amount of features."},{"metadata":{"id":"59D6PiicZWRl","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"dict_lasso = {15: ({'alpha': [0.03], 'tol': [1e-7], 'fit_intercept': [True]}, 0.847), \n              16: ({'alpha': [0.03], 'tol': [1e-7], 'fit_intercept': [False]}, 0.842), \n              13: ({'alpha': [0.03], 'tol': [1e-2], 'fit_intercept': [True]}, 0.839), \n              14: ({'alpha': [0.03], 'tol': [1e-7], 'fit_intercept': [False]}, 0.842)}","execution_count":null,"outputs":[]},{"metadata":{"id":"8wS_tH-cZUES","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"run_and_submit(dict_lasso, to_csv=False, estimator_type=linear_model.Lasso())","execution_count":null,"outputs":[]},{"metadata":{"id":"iaztK32xVA1r","colab_type":"text"},"cell_type":"markdown","source":"Model with 15 features, alpha = 0.3 gave currently my highest LB = 0.847 and 0.839 private."},{"metadata":{"id":"LkF28MMPVjSR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":499},"outputId":"db1e61a2-816e-4252-c308-78cb271a1165","trusted":true},"cell_type":"code","source":"result_cols = ['k', 'alpha', 'CV', 'LB', 'params']\ndf_lasso = plot_results(dict_lasso, result_cols, estimator_type=linear_model.Lasso(), figsize=(16, 8), rows=2, cols=2, y_scale=1500)","execution_count":null,"outputs":[]},{"metadata":{"id":"mqNIvsZudvPd","colab_type":"text"},"cell_type":"markdown","source":"Distribution has negative values because for prediction used predict function of lasso, instead of predict_proba from logistic regression."},{"metadata":{"id":"KRp_x3mSd7m-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"e6b17fba-37f9-4567-a085-e367fe761e62","trusted":true},"cell_type":"code","source":"df_lasso","execution_count":null,"outputs":[]},{"metadata":{"id":"8Ldqb0HPCteT","colab_type":"text"},"cell_type":"markdown","source":"## 3.3 Elastic Net, search best CV"},{"metadata":{"id":"1forDe5peg56","colab_type":"text"},"cell_type":"markdown","source":"#### Brute force for elastic net"},{"metadata":{"id":"dk10faDTFXgW","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# Reset before new run\nlist_k = []\nlist_mean = []\nlist_std = []\nlist_CV = []\nlist_C = []\nlist_params = []\n\n\nresdict = {'k': list_k,\n           'mean': list_mean,\n           'std': list_std,\n           'CV': list_CV,\n           'alpha/l1_ratio': list_C,\n           'params': list_params}\n\na = False # takes time to run\n\nif a:\n    for k_val in range(10, 21):\n        selector = SelectKBest(f_classif, k=k_val)\n        X_train_K = selector.fit_transform(X_train, y_train.values.astype(int))\n        X_test_K = selector.transform(X_test)\n\n        for alpha in range(5, 31):\n            alpha /= 100\n\n            grid_elnet = {'alpha': [alpha],\n                          'l1_ratio': [0.25, 0.5, 0.75],\n                          'fit_intercept': [True, False], \n                          'tol': [1e-5, 1e-4, 1e-3, 1e-1], \n                          'selection': ['cyclic', 'random']}\n\n            elnet_estim = MyEstimator(linear_model.ElasticNet(), grid_elnet, folds)\n            elnet_estim.fit_grid(X_train_K, y_train)\n\n            scores = elnet_estim.train_estimator(X_train_K, X_test_K, verbose=False)\n            preds = elnet_estim.predict_targets(X_test_K)\n\n            list_k.append(k_val)\n            list_mean.append(preds.mean())\n            list_std.append(np.std(scores))\n            list_CV.append(np.mean(scores))\n            list_C.append([elnet_estim.grid.best_params_['alpha'], elnet_estim.grid.best_params_['l1_ratio']])\n            list_params.append(str(elnet_estim.grid.best_params_)[1:len(str(elnet_estim.grid.best_params_))-1])\n\n            print(k_val, alpha) # to track the process\n            \n    result_df = pd.DataFrame(resdict) # results\n\n    ### filter results\n\n    filter_df = result_df.sort_values('CV', ascending=False).head(50)\n    #filter_df.to_csv('best50_elnet.csv', index=False)\n    #filter_df.head(20)\n  ","execution_count":null,"outputs":[]},{"metadata":{"id":"2UZHBVFxe4Gi","colab_type":"text"},"cell_type":"markdown","source":"As we can see, brute force can give us a high CV, unfortunately, LB is a bit overfitted and didn't give a better result than lasso or logreg which was carefully selected. "},{"metadata":{"id":"IteyiNwAfNYT","colab_type":"text"},"cell_type":"markdown","source":"#### Choose best params of elastic net (highest CV)"},{"metadata":{"id":"-9uVtSzgfjSw","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"# LB score (second value of the tuple) taken after submission\ndict_elnet = {  '14r1': ({'alpha': [0.05], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.839),\n                '18r2': ({'alpha': [0.06], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.828),\n                '18r3': ({'alpha': [0.08], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.831),\n                '18r4': ({'alpha': [0.07], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.830),\n                '20r5': ({'alpha': [0.05], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-1]}, 0.821),\n                '18r6': ({'alpha': [0.09], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.832),\n                '16r7': ({'alpha': [0.05], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.834),\n                '18r8': ({'alpha': [0.05], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.828),\n                '15r9': ({'alpha': [0.09], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.843),\n                '15r10': ({'alpha': [0.05], 'fit_intercept': [True], 'l1_ratio': [0.50], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.845),               \n                '15r11': ({'alpha': [0.06], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.841),\n                '15r12': ({'alpha': [0.08], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-1]}, 0.843),\n                '16r13': ({'alpha': [0.09], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.840),\n                '14r14': ({'alpha': [0.07], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-1]}, 0.841),\n                '15r15': ({'alpha': [0.10], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.844),\n                '14r16': ({'alpha': [0.08], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.842),\n                '14r17': ({'alpha': [0.06], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.841),\n                '19r18': ({'alpha': [0.08], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-3]}, 0.828),\n                '19r19': ({'alpha': [0.09], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.830),\n                '17r20': ({'alpha': [0.07], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.836)}              ","execution_count":null,"outputs":[]},{"metadata":{"id":"1Wepg_ocEz9I","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"run_and_submit(dict_elnet, to_csv=False, estimator_type=linear_model.ElasticNet())","execution_count":null,"outputs":[]},{"metadata":{"id":"dZ6W2Q1ACwh7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1586},"outputId":"81658645-a33d-498c-9f0d-6343d9b34f10","trusted":true},"cell_type":"code","source":"result_cols = ['k', 'alpha', 'l1_ratio', 'CV', 'LB', 'params']\nelnet_df = plot_results(dict_elnet, result_cols, estimator_type=linear_model.ElasticNet(), figsize=(22, 28), rows=7, cols=3, y_scale=1500)                          ","execution_count":null,"outputs":[]},{"metadata":{"id":"9PcL2F5IyHYM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":228},"outputId":"32cd8f32-7dbc-45e4-de1a-1c8e0fbd4c5c","trusted":true},"cell_type":"code","source":"elnet_df.sort_values('LB', ascending=False).head(6)","execution_count":null,"outputs":[]},{"metadata":{"id":"9tVCdXGWx4RK","colab_type":"text"},"cell_type":"markdown","source":"### 3.3.1 ElasticNet, show best"},{"metadata":{"id":"elQNUuDbgJN6","colab_type":"text"},"cell_type":"markdown","source":"Here shown best models with Elastic Net.\nAlso, in that case CV is much higher than LB, so if we find model with high CV, high LB wil not be guaranteed. "},{"metadata":{"id":"BpxB77x_x8qD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":499},"outputId":"2a31759e-83b7-4fae-ccfd-1e2fc7a754e0","trusted":true},"cell_type":"code","source":"dict_elnet = {  '15r1': ({'alpha': [0.05], 'fit_intercept': [True], 'l1_ratio': [0.50], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.845),\n                '15r2': ({'alpha': [0.10], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.844),\n                '15r3': ({'alpha': [0.08], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-1]}, 0.843),\n                '15r4': ({'alpha': [0.09], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.843),\n                '14r5': ({'alpha': [0.08], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['cyclic'], 'tol': [1e-5]}, 0.842),\n                '15r6': ({'alpha': [0.06], 'fit_intercept': [True], 'l1_ratio': [0.25], 'selection': ['random'], 'tol': [1e-1]}, 0.841)}    \nresult_cols = ['k', 'alpha', 'l1_ratio', 'CV', 'LB', 'params']\n_ = plot_results(dict_elnet, result_cols, estimator_type=linear_model.ElasticNet(), figsize=(22, 8), rows=2, cols=3, y_scale=1500)  ","execution_count":null,"outputs":[]},{"metadata":{"id":"Su1jyyJngoAG","colab_type":"text"},"cell_type":"markdown","source":"Also, we can see that distribution is bulge shaped. Note, that there isn't shown probabilities, as used function predicts of the elastic net, not predict_proba (so values on X axis > 1). Still, this distribution gives us understanding, what LB score we'll achieve with these predictions."},{"metadata":{"id":"cZ6-pofPzi3y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":550},"outputId":"9467ce50-e0f1-4a55-8b03-dc248387089e","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 6), facecolor='lightgrey')\n\nplt.subplot(121, facecolor='slategrey')\n_ = plt.plot(elnet_df.sort_values('alpha')[elnet_df['k'] == 15]['alpha'], elnet_df.sort_values('alpha')[elnet_df['k'] == 15]['CV'], c='salmon', ls='--')\n_ = plt.plot(elnet_df.sort_values('alpha')[elnet_df['k'] == 15]['alpha'], elnet_df.sort_values('alpha')[elnet_df['k'] == 15]['LB'], c='navy')\nplt.xlabel('alpha')\nplt.ylabel('AUC score')\nplt.xticks(np.arange(.05, .10, .01))\nplt.yticks(np.arange(.840, .915, .005))\nplt.grid(color='midnightblue', linestyle=':', lw=.7)\n_ = plt.legend(('CV', 'LB'), loc='lower right')\n_ = plt.title('[LB and CV] versus [alpha] for ElNet, k = 15')","execution_count":null,"outputs":[]},{"metadata":{"id":"7ZkeLoTghOnP","colab_type":"text"},"cell_type":"markdown","source":"**Conclusion**\n\nAs a target, I wanted to find some clue to determine if the prediction will be overfitted or not. \nFrom pictures above can be seen what shape of distribution you should search. Of course to describe distribution statistical parameters can be used, but I wanted to have some visual representation.\n\nAlso, I've understood that brute-force parameters searching will not help with this task, as CV score can differ from LB a lot. \nFor now, 15 features selected with KBest can definitely show LB higher than 0.85. In future, I'll try other optimizers."}],"metadata":{"colab":{"name":"my_solution.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["g2ZyOIvuKfG-","E1Luc-olKhAO","5CVEgzFSKTO3","QDQDnzpOLBOw","oyWH9W1HLEmJ","BbwX9E3sLGyJ","8ZpzT0ysLIbx","u22Zz55zLZFA","oRoizyU4LJvh","1fobSvwwDE89","tgIT32LfDMQN"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}