{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"#### 이 것은 안드레이룩야넨코의 노트북을 한글화 한 것입니다\n\n## General information\n\nDon't Overfit II 에서는 바이너리 클래시피케이션을 합니다\n\n300열 250행의 훈련 샘플에 테스트 샘플은 79배나 됩니다\t\n\n이런 상황에서 오버핏하기 쉬운데 훈련세트가 작기 때문입니다\t\n\n오버피팅의 의미가 생각 안 나시면 간단히 말해서 트레이닝 데이터에는 잘 맞으나 테스트 데이터에는 잘 맞지 않는다는 이야기 입니다\t\n\n우리는 오버피팅하지 않는 모델을 만들어야 합니다\n\n이 노트에서는 다음을 합니다\n\n* 인사이트를 얻기위해 항목에 대한 EDA를 합니다\n* 퍼뮤테이션 임포턴스를 사용하여 가장 영향력 있는 항목을 찾습니다\t\n* 여러 모델을 비교합니다 - 베이스 클래시피케이션, 리니어 모델, 트리기반 모델 등을 해봅니다\n* 여러 종류의 feature selection 방법을 해봅니다 - ELI5 및 SHAP을 포함합니다\n* 모델 하이퍼 파라미터 최적화를 해봅니다\n* 항목 생성을 해봅니다\n* 그리고 다른 여러가지를 해보지요\n\n![](https://cdn-images-1.medium.com/max/1600/1*vuZxFMi5fODz2OEcpG-S1g.png)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\npd.set_option('max_columns', None)\nimport json\nimport ast\nimport time\nimport datetime\nimport os\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n# 정우일님의 관련 블로그 https://wooiljeong.github.io/python/python_plotly/\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFECV\n\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c971fbc1a1b8249045b120924058402a036e665"},"cell_type":"code","source":"path = '/kaggle/input/dont-overfit-ii'\ntrain = pd.read_csv(f'{path}/train.csv')\ntest = pd.read_csv(f'{path}/test.csv')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"구글 콜랩에서 사용하실 때는 컴퓨터에 첨부된 트레인 및 테스트 csv 파일을 컴퓨터에 다운로드 한 후 아래 코드를 실행하여 다시 그 파일들을 불러올 수 있게 됩니다.\n\n    from google.colab import files\n    uploaded = files.upload()\n\n그런 다음 아래 코드를 통해서 csv를 데이터프레임으로 바꿀 수 있게 됩니다.\n\n    import io\n    test = pd.read_csv(io.BytesIO(uploaded['test.csv']))\n    train = pd.read_csv(io.BytesIO(uploaded['train.csv']))"},{"metadata":{"_uuid":"7b40dd26ead2705ebf0058b0aa528c89a18aedbe"},"cell_type":"markdown","source":"<a id=\"de\"></a>\n## Data exploration"},{"metadata":{"trusted":true,"_uuid":"67a26174603f6a28709b7e0431aa431832ae608d"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* id 열, target 열 및 300개의 항목이 있음을 알 수 있습니다\n* 익명으로 처리되었으므로 그 의미를 모릅니다\n* 열들에 대해 이해하려고 노력해보지요"},{"metadata":{"trusted":true,"_uuid":"504b609f61494c21f8b078182e06191581ecb2a6"},"cell_type":"code","source":"train[train.columns[2:]].std().plot('hist');\nplt.title('Distribution of stds of all columns');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7b0f65d2d5c999a44a3f71e013b1b6a6ff08980"},"cell_type":"code","source":"train[train.columns[2:]].mean().plot('hist');\nplt.title('Distribution of means of all columns');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 모든 열의 평균 값이 -0.2와 0.15 사이임을 알 수 있습니다\t\n* 표준 편차는 매우 작습니다\n* 우리는 항목이 서로 매우 비슷하다고 말할 수 있습니다"},{"metadata":{"trusted":true,"_uuid":"d205e01b009224a3189903e1858dd592fb222d2d"},"cell_type":"code","source":"# we have no missing values\ntrain.isnull().any().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30e64cca712542d662201263914d8fc25496563e"},"cell_type":"code","source":"print('Distributions of first 28 columns')\nplt.figure(figsize=(26, 24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7, 4, i + 1)\n    plt.hist(train[col])\n    plt.title(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227daacd39977e5658c7e27db2686d8f65fdff3c"},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89e9ed49ceff33d27cd1888336c3c46a38c5c8aa"},"cell_type":"markdown","source":"이 개요에서 우리는 다음을 볼 수 있습니다\n\n* 대상은 이진이며 약간의 불균형이 있습니다\n* 샘플의 26.8 %가 0 클래스에 속합니다\n* 열의 값은 다소 비슷합니다"},{"metadata":{"_uuid":"06df27b43428261da7daf02e708b934519d78ac2"},"cell_type":"markdown","source":"이제 상관 관계를 살펴 봅시다!"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train[train.columns[2:]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"열이 너무 많아서 위에서 도저히 읽을 수가 없습니다.\n\ntop correlated features를 보겠습니다."},{"metadata":{"trusted":true,"_uuid":"ae63462aa70238f0a2858de687dc7d2ae319589a"},"cell_type":"code","source":"corrs = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs = corrs[corrs['level_0'] != corrs['level_1']]\ncorrs.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2d921a5d3bf606b88853988c10acad020685334"},"cell_type":"markdown","source":"항목 간의 상관 관계가 0.3보다 낮고 target과 가장 관련이 높은 피처의 상관 관계는 03.3입니다\t\n\n따라서 제거 할 수 있는 상관 관계가 높은 항목이 없으며\ttarget과 상관 관계가 거의 없는 일부 열을 삭제할 수 있습니다"},{"metadata":{},"cell_type":"markdown","source":"## Prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4f28e1e3c847e2fe165034dd870154afb7fe939"},"cell_type":"markdown","source":"## Basic modelling\n\n기본 모델링을합니다\n\n훈련 모델과 폴드 예측에 익숙해지기를 바랍니다\t\n\n이것이 왜 유용할까요?\n\nsklearn의 cross_val_score는 모델의 점수를 계산하기에 충분하지만, 자세히 배울수록 충분하지는 않다는 것을 알게 될 것입니다\tbut as you learn more, you'll realize it isn't always enough\n\n왜냐하면\t\n\n* 예측을 제공하지 않습니다\n* 폴드로부터의 예측은 제공하지 않습니다\n* 특정 변환을 적용할 수 없습니다\n* lgbm, catboost, xgboost와 같은 그래디언트 부스팅 모델은 cross_val_score로 전달할 수 없는 추가 매개 변수를 필요로 합니다\t\n\n이 것은 어렵지 않고 논리는 다음과 같습니다\n\n```\nfor fold in folds:\n    get train and validation data\n    apply some transformations (if necessary)\n    train model\n    predict on validation data\n    calculate train and validation metrics\n    predict on test data\n```\n\n간단한 로지스틱 리그레션을 사용하여 단계별로 코드를 작성해 봅시다"},{"metadata":{},"cell_type":"markdown","source":"[](http://i.imgur.com/QBuDOjs.jpg)"},{"metadata":{},"cell_type":"markdown","source":"먼저 몇 가지 사항을 정의 해 보겠습니다\t\n\n* Prediction은 우리의 예측이 될 것입니다\n* scores_train, scores_valid은 점수 리스트입니다\n* fold란 데이터를 나누는 방법입니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = np.zeros(len(X_test))\nscores_train = []\nscores_valid = []\nfolds = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 폴드를 한 번 사용하여 트레인 데이터를 훈련 및 검증으로 분할합니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n    X_train_fold, X_valid_fold = X_train.loc[train_index], X_train.loc[valid_index]\n    y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index]\n    break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 모델을 훈련시키고 메트릭을 계산할 수 있습니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nmodel.fit(X_train_fold, y_train_fold)\ny_pred_train = model.predict(X_train_fold).reshape(-1,)\ntrain_score = roc_auc_score(y_train_fold, y_pred_train)\n\ny_pred_valid = model.predict(X_valid_fold).reshape(-1,)\nvalid_score = roc_auc_score(y_valid_fold, y_pred_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Train auc: {train_score:.4}. Valid auc: {valid_score:.4}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"함수를 만들어 봅니다\n\n이는 테스트데이터에 대한 예측도 만들 수 있는 것입니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(X_train, y_train, X_test, folds=folds, model=None):\n    prediction = np.zeros(len(X_test))\n    scores_train = []\n    scores_valid = []\n    \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n        X_train_fold, X_valid_fold = X_train[train_index], X_train[valid_index]\n        y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index]\n        \n        model.fit(X_train_fold, y_train_fold)\n        y_pred_train = model.predict(X_train_fold).reshape(-1,)\n        train_score = roc_auc_score(y_train_fold, y_pred_train)\n        scores_train.append(train_score)\n        \n        y_pred_valid = model.predict(X_valid_fold).reshape(-1,)\n        valid_score = roc_auc_score(y_valid_fold, y_pred_valid)\n        scores_valid.append(valid_score)\n\n        y_pred = model.predict_proba(X_test)[:, 1]\n        prediction += y_pred\n\n    prediction /= folds.get_n_splits()\n    \n    print(f'Mean train auc: {np.mean(scores_train):.4f}, std: {np.std(scores_train):.4f}.')\n    print(f'Mean valid auc: {np.mean(scores_valid):.4f}, std: {np.std(scores_valid):.4f}.')\n    \n    return scores_valid, prediction\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds,  model=model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Different ways of splitting data into folds\n\n데이터를 폴드로 나누는 방법에는 여러 가지가 있습니다\n* 가장 간단한 방법은 무작위로 나누는 것입니다: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n* 일반적으로 분류에는 더 나은 방법이 있긴 합니다 - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html \n\nStratifiedKFold는 계층화 된 폴드를 반환하는 k-폴드의 변형입니다\n\n각 세트에는 각 세트의 샘플이 전체 세트와 거의 같은 비율로 포함되어 있습니다\n\n* StratifiedKFold와 비슷한 RepeatedStratifiedKFold도 있는데 https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html 이는 그 안에서 여러번 반복됩니다\n\n우리는 RepeatedStratifiedKFold로 확인하겠습니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"repeated_folds = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=repeated_folds, model=model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"mean auc가 증가한 것을 볼 수 있습니다"},{"metadata":{},"cell_type":"markdown","source":"## Approaches to feature selection\n\nfeature selection이 무엇이고 왜 중요한지 설명하겠습니다"},{"metadata":{"_uuid":"600682b545014ae67e19a8b04724e75767be6014"},"cell_type":"markdown","source":"\n### ELI5\n\nELI5는 ML 모델에 대한 설명을 제공하는 패키지입니다\t\n\n선형 모델뿐만 아니라 트리 기반 알고리즘에 대해서도 이를 수행 할 수 있습니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(model, top=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f16f5fee606cb48d35c0cb95e123c7542aacac28"},"cell_type":"code","source":"(model.coef_ != 0).sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c738de31f86152ced6cb35ddb8d3569e7b49a6e"},"cell_type":"markdown","source":"가중치가 매우 높은 항목과 가중치가 마이너스인 더 많은 항목이 있음을 알 수 있습니다\t\n\n실제로 ELI5에 따르면 중요한 항목은 32개만 있습니다\t\n\n이 항목들만 사용하여 모델을 구축해 봅시다"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Important information about ELI5:\n\n실제로 매우 간단하게 작동됩니다\t\nlogistic regression와 같은 모델의 model coefficient를 보여주거나 랜덤 포레스트와 같은 모델의 feature importance를 보여 줍니다\t\n\nELI5의 결과를 model coefficient와 비교해 봅니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, coef in enumerate(model.coef_[0]):\n    if coef != 0:\n        print(f'Feature {X_train.columns[i]} has coefficient {coef:.4f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"여기에 중요한 결론이 있습니다\n\n모델에 계수 또는 항목 중요도가 없는 경우 ELI5가 작동하지 않습니다\t\n\nSVC가 그런 예입니다\t"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"모델이 훨씬 좋아졌다는 것을 알 수 있습니다\n\n중요한 교훈은 때로는 영양가 없는 것은 없는 것이 좋다는 것이죠"},{"metadata":{"_uuid":"fbee2b85bae60cf0607b977692306eb380582e5c"},"cell_type":"markdown","source":"<a id=\"eli5p\"></a>\n### Permutation importance\n\nELI5를 잘 이용하는 다른 방법이 하나 더 있습니다\n\nPermutation Feature Importance는 데이터가 테이블 형식일 때 훈련된 estimator에 사용할 수 있는 모델 검사 기술입니다\t \n\nPermutation Importance는 다음과 같은 방식으로 작동합니다\t\n\n* 모델을 훈련 시킵니다\n* 하나의 유효성 검사 데이터 열을 무작위로 섞고 점수를 계산합니다\n* 점수가 크게 떨어지면 항목이 중요하다는 의미입니다\n\n링크를 클릭하시면 추가적으로 내용을 볼 수 있습니다: https://www.kaggle.com/dansbecker/permutation-importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=repeated_folds,  model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(f'{path}/sample_submission.csv')\nsubmission['target'] = prediction\nsubmission.to_csv('submission_3.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"242439bf4036359fc07864ac41d0bccac6f1d9c6"},"cell_type":"code","source":"perm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, top=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.formatters.as_dataframe.explain_weights_df(perm).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.formatters.as_dataframe.explain_weights_df(perm).loc[eli5.formatters.as_dataframe.explain_weights_df(perm)['weight'] != 0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_weights = eli5.formatters.as_dataframe.explain_weights_df(perm).loc[eli5.formatters.as_dataframe.explain_weights_df(perm)['weight'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26dd053929c39014a3244d8d2471e8b2cdb5ac0f"},"cell_type":"code","source":"top_features = [i[1:] for i in selected_weights.feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcbc3e88b57bab0d1220c41d222540d42a445622"},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a010cfbba09f2846f05211678657f87614c219c4"},"cell_type":"markdown","source":"\n### SHAP\n\n또 다른 흥미로운 도구는 SHAP입니다\t\n\n다양한 모델에 대한 설명을 제공합니다"},{"metadata":{"trusted":true,"_uuid":"5869cb071f42e20e9e420c0a9d1ef584dd2b2417","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=repeated_folds, model=model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33c4481d2b911208738c7ffc7042259108bb5736","scrolled":false},"cell_type":"code","source":"explainer = shap.LinearExplainer(model, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd6ac9eda7175a382b6b0adf4e5fe95ecd02e553"},"cell_type":"markdown","source":"\n이 플롯을 처음 볼 때 해석하기 어려울 수 있습니다\n\n항목이 예측에 미치는 영향을 보여줍니다\t\n\n각 행은 각 항목을 나타냅니다\t\n\n색상은 실제 항목 값입니다\t\n\n예를 들어 파란색  항목 18의 낮은 값은 모형 예측에 부정적인 영향을 미칩니다 (1이냐 0이냐에서 0이 되겠지요)\t\n\n빨간색인 높은 값은 긍정적인 영향을 미칩니다 (1이냐 0이냐에서 1이 되겠지요)\n\n항목 176은 반대 영향이 있습니다 \t\n\n낮은 값은 긍정적인 영향을 미치며 높은 값은 부정적인 영향을 미칩니다\t\n\n불행히도 항목을 수동으로 선택해야합니다 \t\n\n그 작업을 해주는 라이브러리를 사용하겠습니다\t"},{"metadata":{},"cell_type":"markdown","source":"### Recursive feature elimination\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model_with_feature_selection(X_train, y_train, X_test, folds=folds, model=None, feature_selector=None):\n    prediction = np.zeros(len(X_test))\n    scores_train = []\n    scores_valid = []\n    \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X_train, y_train)):\n        X_train_fold, X_valid_fold = X_train[train_index], X_train[valid_index]\n        y_train_fold, y_valid_fold = y_train[train_index], y_train[valid_index]\n        # so that we don't transform the original test data\n        X_test_copy = X_test.copy()\n        \n        feature_selector.fit(X_train_fold, y_train_fold)\n        X_train_fold = feature_selector.transform(X_train_fold)\n        X_valid_fold = feature_selector.transform(X_valid_fold)\n        X_test_copy = feature_selector.transform(X_test_copy)\n        \n        model.fit(X_train_fold, y_train_fold)\n        y_pred_train = model.predict(X_train_fold).reshape(-1,)\n        train_score = roc_auc_score(y_train_fold, y_pred_train)\n        scores_train.append(train_score)\n        \n        y_pred_valid = model.predict(X_valid_fold).reshape(-1,)\n        valid_score = roc_auc_score(y_valid_fold, y_pred_valid)\n        scores_valid.append(valid_score)\n\n        y_pred = model.predict_proba(X_test_copy)[:, 1]\n        prediction += y_pred\n\n    prediction /= folds.get_n_splits()\n    \n    print(f'Mean train auc: {np.mean(scores_train):.4f}, std: {np.std(scores_train):.4f}.')\n    print(f'Mean valid auc: {np.mean(scores_valid):.4f}, std: {np.std(scores_valid):.4f}.')\n    \n    return scores_valid, prediction\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우리 버전의 수정된 버전을 작성해 봅니다\t\n\n여기 교차 교차 데이터 내에 RFECV를 추가합니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nfeature_selector = RFECV(model, min_features_to_select=10, scoring='roc_auc', step=0.1, verbose=0, cv=repeated_folds, n_jobs=-1)\nscores, prediction = train_model_with_feature_selection(X_train.values, y_train, X_test, folds=repeated_folds, model=model, feature_selector=feature_selector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing models\n\n다른 모델을 비교할 수 있습니다 \n\n기본 매개 변수가있는 모델이 제대로 작동하지 않을 수 있으므로 최적화 된 모델을 비교할 가치가 있다고 생각합니다\t\n\n다음과 같이 할 것입니다:\n\n* default parameter로 모델을 학습하고 기본 점수를 확인합니다\t \n* best feature들을 선택합니다\t \n* grid search를 실행합니다 \n* best model을 훈련시키고 다시 점수를 봅니다\t\n\n또한 각 모델에 대한 feature selection을 해봅니다  \n\n그리고 훈련을 더 빠르게 하기 위해 반복하지 않는 간단한 폴드를 사용할 것입니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\nlr = linear_model.LogisticRegression(max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.05, 0.08, 0.01, 0.1, 1.0, 10.0],\n                  'solver': ['liblinear']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_logreg, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"모델을 최적화하면 실제로 auc 점수가 향상됩니다!"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = AdaBoostClassifier()\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\n\nabc = AdaBoostClassifier()\n\nparameter_grid = {'n_estimators': [5, 10, 20, 50, 100],\n                  'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]\n                 }\n\ngrid_search = GridSearchCV(abc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_abc, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001, loss='modified_huber')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\nsgd = linear_model.SGDClassifier(eta0=1, max_iter=1000, tol=0.0001)\n\nparameter_grid = {'loss': ['log', 'modified_huber'],\n                  'penalty': ['l1', 'l2', 'elasticnet'],\n                  'alpha': [0.001, 0.01, 0.1, 0.5],\n                  'l1_ratio': [0, 0.15, 0.5, 1.0],\n                  'learning_rate': ['optimal', 'invscaling', 'adaptive']\n                 }\n\ngrid_search = GridSearchCV(sgd, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_sgd, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"계수나 항목 중요도 등이 없기 때문에 SVC에서는 Permutation Importance를 사용합니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SVC(probability=True, gamma='scale')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\nperm = PermutationImportance(model, random_state=1).fit(X_train, y_train)\nselected_weights = eli5.formatters.as_dataframe.explain_weights_df(perm).loc[eli5.formatters.as_dataframe.explain_weights_df(perm)['weight'] != 0]\ntop_features = [i[1:] for i in selected_weights.feature if 'BIAS' not in i]\nX_train_selected = train[top_features]\ny_train = train['target']\nX_test_selected = test[top_features]\n\nsvc = SVC(probability=True, gamma='scale')\n\nparameter_grid = {'C': [0.01, 0.1, 1.0, 10.0, 100.0],\n                  'kernel': ['linear', 'poly', 'rbf'],\n                 }\n\ngrid_search = GridSearchCV(svc, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_svc, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2335b870061080430fc25c9e99111357088824b8"},"cell_type":"code","source":"plt.figure(figsize=(12, 8));\nscores_df = pd.DataFrame({'LogisticRegression': scores_logreg})\nscores_df['AdaBoostClassifier'] = scores_abc\nscores_df['SGDClassifier'] = scores_sgd\nscores_df['SVC'] = scores_svc\n\nsns.boxplot(data=scores_df);\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f81c4775a537def6405793464109d7e80f32697"},"cell_type":"markdown","source":"logistic regression가 대부분의 다른 모델보다 우수하다는 것을 알 수 있습니다 \n\n다른 모델은이 작은 데이터 세트에서 과적합하거나 작동하지 않는 것 같습니다 "},{"metadata":{"_uuid":"03af3b65d3f687689051ac93662029c4e2f58600"},"cell_type":"markdown","source":"## Feature engineering\n\n항목 생성에는 여러 접근 방식이 있습니다\t\n\n익명화되고 비슷한 항목들이 있으면 행을 기준으로 항목을 계산할 수 있습니다\n\n예를 들어 행 별 평균값 같은 것을 말합니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['mean'] = X_train.mean(axis=1)\nX_train['kurt'] = X_train.kurt(axis=1)\nX_train['mad'] = X_train.mad(axis=1)\nX_train['median'] = X_train.median(axis=1)\nX_train['max'] = X_train.max(axis=1)\nX_train['min'] = X_train.min(axis=1)\nX_train['skew'] = X_train.skew(axis=1)\nX_train['sem'] = X_train.sem(axis=1)\n\nX_test['mean'] = X_test.mean(axis=1)\nX_test['kurt'] = X_test.kurt(axis=1)\nX_test['mad'] = X_test.mad(axis=1)\nX_test['median'] = X_test.median(axis=1)\nX_test['max'] = X_test.max(axis=1)\nX_test['min'] = X_test.min(axis=1)\nX_test['skew'] = X_test.skew(axis=1)\nX_test['sem'] = X_test.sem(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = itemgetter([int(i[1:]) for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i])(X_train.columns)\nX_train_selected = X_train[top_features]\ny_train = train['target']\nX_test_selected = X_test[top_features]\n\nlr = linear_model.LogisticRegression(max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.05, 0.08, 0.01, 0.1, 1.0, 10.0],\n                  'solver': ['liblinear']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_logreg, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CV 점수를 향상시키지 않았으므로 이러한 항목이 크게 도움이 되지 않은 것 같습니다"},{"metadata":{},"cell_type":"markdown","source":"## Scaling the data\n\n마지막 요령은 데이터의 크기를 조정하는 것입니다\n\n일반적으로 다음과 같은 접근 방식이 있습니다\n\n* 각 폴드에서 데이터를 트레인과 검증용으로 나눈 다음 \n* 트레인데이터 및 검증데이터에 스케일러를 적용한 후 \n* 다시 검증 및 테스트에 적용하는 것입니다\t\n* 앞처럼 다시 진행해 봅니다\t\n\n그러나 Kaggle에서는 테스트 데이터를 즉시 적용할 수 있는 독특한 상황입니다\t\n\n따라서 사용 가능한 모든 데이터에 스케일러를 적용해보기도 합니다\t\n\n데이터를 다시 준비하고 stanard scaler를 사용합니다"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\ndata = StandardScaler().fit_transform(np.concatenate((X_train, X_test), axis=0))\nX_train.iloc[:, :] = data[:250]\nX_test.iloc[:, :] = data[250:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nprint('Default scores')\nscores, prediction = train_model(X_train.values, y_train, X_test, folds=folds, model=model)\nprint()\ntop_features = itemgetter([int(i[1:]) for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i])(X_train.columns)\nX_train_selected = X_train[top_features]\ny_train = train['target']\nX_test_selected = X_test[top_features]\n\nlr = linear_model.LogisticRegression(max_iter=1000)\n\nparameter_grid = {'class_weight' : ['balanced', None],\n                  'penalty' : ['l2', 'l1'],\n                  'C' : [0.001, 0.05, 0.08, 0.01, 0.1, 1.0, 10.0],\n                  'solver': ['liblinear']\n                 }\n\ngrid_search = GridSearchCV(lr, param_grid=parameter_grid, cv=folds, scoring='roc_auc', n_jobs=-1)\ngrid_search.fit(X_train_selected, y_train)\nprint(f'Best score of GridSearchCV: {grid_search.best_score_}')\nprint(f'Best parameters: {grid_search.best_params_}')\n\nprint()\nscores_logreg, prediction = train_model(X_train_selected.values, y_train, X_test_selected, folds=repeated_folds, model=grid_search.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(f'{path}/sample_submission.csv')\nsubmission['target'] = prediction\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"점수가 조금 증가했습니다!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}