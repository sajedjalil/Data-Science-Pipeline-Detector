{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis competition asks us to build a model of k = 300 features with only N = 250 observations. This resembles the situation often faced by scientists in the real world: we have limited observations and many possible explanations. \n\nIn a normal scientific context, we could use domain knowledge to help navigate this situation: we would likely know something about these variables' meanings, the accuracy with which they are measured, their temporal orderings, or best of all, their causal relationships.  But, here we have a purely _data science_ context. We have no domain knowledge about these features at all, so we have only our data-science tools to help sort out this mess.\n\nFirst, I'll get my environment ready, load the data, and define some custom helper functions."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"!pip install scikit-misc\n\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pymc3 as pm\nimport random\nimport matplotlib\nimport plotnine\nfrom plotnine import ggplot, aes, geom_point, geom_jitter, geom_smooth, geom_histogram, geom_line, geom_errorbar, stat_smooth, geom_ribbon\nfrom plotnine.facets import facet_wrap\nimport seaborn as sns\nimport theano.tensor as tt\nfrom scipy.special import expit\nfrom scipy.special import logit\nfrom scipy.stats import cauchy\nfrom sklearn.metrics import roc_auc_score\nfrom skmisc.loess import loess\nimport warnings\nwarnings.filterwarnings(\"ignore\") # plotnine is causing all kinds of matplotlib warnings\n\n\n## Define custom functions\n\ninvlogit = lambda x: 1/(1 + tt.exp(-x))\n\ndef trace_predict(trace, X):\n    y_hat = np.apply_along_axis(np.mean, 1, expit(trace['alpha'] + np.dot(X, np.transpose(trace['beta']) )) )\n    return(y_hat)\n\n\n# Define prediction helper function\n# for more help see: https://discourse.pymc.io/t/how-to-predict-new-values-on-hold-out-data/2568\ndef posterior_predict(trace, model, n=1000, progressbar=True):\n    with model:\n        ppc = pm.sample_posterior_predictive(trace,n, progressbar=progressbar)\n    \n    return(np.mean(np.array(ppc['y_obs']), axis=0))\n\n\n## I much prefer the syntax of tidyr gather() and spread() to pandas' pivot() and melt()\ndef gather( df, key, value, cols ):\n    id_vars = [ col for col in df.columns if col not in cols ]\n    id_values = cols\n    var_name = key\n    value_name = value\n    return pd.melt( df, id_vars, id_values, var_name, value_name )\n\n\ndef spread( df, index, columns, values ):\n    return df.pivot(index, columns, values).reset_index(level=index).rename_axis(None,axis=1)\n\n\n## define custom plotting functions\n\ndef fit_loess(df, transform_logit=False):\n    l = loess(df[\"value\"],df[\"target\"])\n    l.fit()\n    pred_obj = l.predict(df[\"value\"],stderror=True)\n    conf = pred_obj.confidence()\n    \n    yhat = pred_obj.values\n    ll = conf.lower\n    ul = conf.upper\n    \n    df[\"loess\"] = np.clip(yhat,.001,.999)\n    df[\"ll\"] = np.clip(ll,.001,.999)\n    df[\"ul\"] = np.clip(ul,.001,.999)\n    \n    if transform_logit:\n        df[\"logit_loess\"] = logit(df[\"loess\"])\n        df[\"logit_ll\"] = logit(df[\"ll\"])\n        df[\"logit_ul\"] = logit(df[\"ul\"])\n    \n    return(df)\n\n\ndef plot_loess(df, features):\n    \n    z = gather(df[[\"id\",\"target\"]+features], \"feature\", \"value\", features)\n    z = z.groupby(\"feature\").apply(fit_loess, transform_logit=True)\n    z[\"feature\"] = pd.to_numeric(z[\"feature\"])\n\n    plot = (\n        ggplot(z, aes(\"value\",\"logit_loess\",ymin=\"logit_ll\",ymax=\"logit_ul\")) + \n        geom_point(alpha=.5) + \n        geom_line(alpha=.5) + \n        geom_ribbon(alpha=.33) + \n        facet_wrap(\"~feature\")\n    )\n    return(plot)\n\n\n## Load data\n\ndf = pd.read_csv(\"../input/train.csv\")\ny = np.asarray(df.target)\nX = np.array(df.ix[:, 2:302])\ndf2 = pd.read_csv('../input/test.csv')\ndf2.head()\nX2 = np.array(df2.ix[:, 1:301])\n\nprint(\"training shape: \", X.shape)\nprint(\"test shape: \", X2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis\n\nBefore we begin modeling, let's do some exploratory data analysis. First, I'll pick 12 random features and make a pairs plot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(432532) # comment out for new random samples\n\nrand_feats = [str(x) for x in random.sample(range(0,300), 12)]\ndfp = gather(df[[\"id\",\"target\"]+rand_feats], \"feature\", \"value\", rand_feats)\nsns.set(style=\"ticks\")\n\nsns.pairplot(spread(dfp, \"id\", \"feature\", \"value\").drop(\"id\",1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the histograms, it appears all features are more-or-less normally-distributed and based on the scatterplots they appear largely uncorrelated with one another. That means we won't have much luck using any dimensionality-reduction techniques to combine or reduce redundant features--we're stuck dealing with all 300. (Indeed, another kaggler who tried using an autoencoder to reduce the dimensionality of the problem and found no joy).\n\n## Plotting features and target\n\nNext, I'll poke around a bit and examine the possible relationships between features and the target using the same 12 randomly-selected features. For each, I fit a loess curve and plot the relationship transformed in the logit space (an idea I took from [this blog post](https://thestatsgeek.com/2014/09/13/checking-functional-form-in-logistic-regression-using-loess/)). This will let me inspect whether any of the features might exhibit a non-linear relationship with the target."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotnine.options.figure_size = (12,9)\n\nrandom.seed(432532) # comment out for new random samples\n\nrand_feats = [str(x) for x in random.sample(range(0,300), 12)]\nplot_loess(df,rand_feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I've looked through a number of random feature plots like this and tried to get a sense of the relationships. Most features seem more-or-less unrelated to the target. Among those with potentially meaningful relationsips, some appeared linear and some appeared potentially non-linear. The original challenge was solvable by a hyperplane, so a non-linear answer seems like a logical next step for the challenge creators. I'll comment more on that below (in the \"appendix\"), but for now I will stick to looking for a linear solution.\n"},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n## Bayesian Logistic Regression with PyMC3\n\nThe crux of this challenge is managing the bias-variance trade-off. Because we have more predictor features than observations, even a simple classifier like logistic regression will have too much variance and will overfit (indeed, it will have no unique solutions). So to overcome this, we need to induce some bias.\n\nI'm going to take a Bayesian approach, inducing bias with strong priors. Specifically, I'm going to use a Bayesian logistic regression classifier to predict the targets, implementing the Bayesian inference using [PyMC3](https://docs.pymc.io/). Because we have more features than observations, I'm going to place a strongly skeptical, or regularizing, prior over the coefficients of the model. Another example of this approach can be found in the [Bayesian Spike-and-Slab in PyMC3](https://www.kaggle.com/melondonkey/bayesian-spike-and-slab-in-pymc3) kernel. A spike-and-slab prior embodies the idea that only a small subset of the features are meaningful, so that most coefficients should be set to zero. To do this, it combines a \"spike\" at zero with a \"slab\" acros all other coefficient values. \n\nAn issue with the spike-and-slab prior is that it relies on discrete parameter values (binary variable indicating presence/absence of each coefficient), which prevents us from using the superior NUTS sampler in PyMC3. So intead, I'm going to employ a continuous analogue to the spike-and-slab model by using a [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution) prior. Keeping things continuous will allow the use of the NUTS sampler in PyMC3 (and would also allow this to be implemented in Stan).\n\nBelow is the specification of the model in PyMC3, wrapped in a function to create the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(X, y, cauchy_scale):\n    model = pm.Model()\n\n    with model:\n\n        # Priors for unknown model parameters\n        alpha = pm.Normal('alpha', mu=0, sd=3)\n        beta = pm.Cauchy('beta', alpha=0, beta=cauchy_scale, shape=X.shape[1])\n        mu = pm.math.dot(X, beta)\n\n        # Likelihood (sampling distribution) of observations\n        y_obs = pm.Bernoulli('y_obs', p=invlogit(alpha + mu),  observed=y)\n    \n    model.name = \"linear_c_\"+str(cauchy_scale)\n    return(model)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### Cauchy scale parameter ($\\gamma$)\n\nWe have essentially one tuning parameter available in our model: the scale parameter for the cauchy distribution ($\\gamma$). This is analogous to setting the size of the zero \"spike\" in the spike-and-slab model, essentially capturing how many of the features we think might actually be useful.\n\nReading around on the forums, most people seem to suspect that there are actually very few meaningful variables. Informally inspecting my loess plots above suggest a similar conclusion. So for starters, let's imagine that only 10% of variables are likely to be meaningful. We could approximate that with a cauchy scale parameter of .0175, which would give us a reasonable prior for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"shape_par = .0175\n\nprior_df = pd.DataFrame({\"value\": np.arange(-2,2,.01)})\nprior_df = prior_df.assign(dens = cauchy.pdf(prior_df[\"value\"],0,shape_par))\ncauchy_samples = cauchy.rvs(0, shape_par, 10000)\n\nprint(\"percent non-zero coefs :\", 1-np.mean((cauchy_samples < .1) & (cauchy_samples > -.1)))\n\nplotnine.options.figure_size = (8,6)\nggplot(prior_df, aes(x=\"value\", y=\"dens\")) + geom_line()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We could stick with that, which would be a sort of \"proper\" Bayesian way to do things, where that prior would really represent our subjective beliefs about the likely coefficient values. But, to be fair, I formed these prior beliefs in a pretty causal way---I wouldn't necessarily go to the mat for them.\n\nSo instead, we could treat the prior as a tuneable parameter, and use a measure of model performance to select the \"best\" cauchy scale parameter value for our model. \n\n## Model Comparison\n\nCross validation is a bit tricky with so little data, and mcmc methods are a bit slow to fit. So instead I'll use LOO, a Bayesian approximation of Leave-One-Out cross-validation, to assess different models' performance. I'll perform a search in the region of parameter values around our best initial guess of .0175. I'll look at parameters from .01 to .05 and I'll also throw in .10 to illustrate what a more extreme value does to things.\n\nI should acknowledge and point interested readers to Richard McElreath and his amazing book, [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/), which greatly informs this approach."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"cauchy_scale_pars = [.01,.015,.0175,.020,.0225,.025,.03,.04,.05, .1]\n\nmodels = []\ntraces = []\nmodel_dict = dict()\n\nfor scale_val in cauchy_scale_pars:\n    model = make_model(X,y, scale_val)\n    with model:\n        trace = pm.sample(1000,\n                          tune = 500,\n                          init= \"adapt_diag\", \n                          cores = 1, \n                          progressbar = False, \n                          chains = 2, \n                          nuts_kwargs=dict(target_accept=.95),\n                          random_seed = 12345\n                         )\n    \n    traces.append(trace)\n    models.append(model)\n    model_dict[model] = trace\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compare models with LOO\ncomp = pm.stats.compare(model_dict, ic=\"LOO\", method='BB-pseudo-BMA')\n\n# generate posterior predictions for original data\nfor i in range(0,len(traces)):\n    y_hat = trace_predict(traces[i], X)\n    print(\"scale = \",cauchy_scale_pars[i],\", training AUCROC:\",roc_auc_score(y,y_hat))\n    \n# print comparisons\ncomp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These LOO values need to be taken with a grain of salt: according to LOO the best model is one using a cauchy scale paramter of .10, a model that is completely overfit with an ROCAUC of 1.0. The `shape_warn` column indicates that LOO is having issues with all these models, which is all the more reason to be cautious. If we had more data and more time, we should use k-fold cross validation instead of LOO.\n\nIn addition to the LOO score for each model, we can see the effects of regularization in the pLOO value, our measure of model complexity. We can interpret this value as the effective number of parameters in our model. Consider the model with $\\gamma$ = .025: although the model has 301 parameter values (300 betas + 1 alpha), our use of a strong prior makes the model only as flexible as if it had ~50 parameters.\n\nBelow I'm going to throw out the two clearly overfit models ($\\gamma$ =.10 and .05) and compare the remaining plausible models."},{"metadata":{"trusted":true},"cell_type":"code","source":"# can throw out .1 and .05 as way out-of-bounds\ncomp_abridged = pm.stats.compare(dict(zip(models[0:-2], traces[0:-2])), ic=\"LOO\", method='BB-pseudo-BMA')\ncomp_abridged","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the difference in LOO scores (dLOO) and the standard error of the difference (dSE), we see that the model with $\\gamma$ = .03 model isn't appreciably worse than the model with scale parameter .04 (less than standard error difference) and the $\\gamma$ = .025 model is borderline. But, the .0175 model we started with is coming up as worse than the $\\gamma$ =.04 model. Being somewhat conservative, and considering that AUCROC indicates overfitting for all models, the $\\gamma$ = .025 or .03 models might be reasonable solutions. \n\nLet's inspect the $\\gamma$ = .025 model."},{"metadata":{},"cell_type":"markdown","source":"## Inspecting coefficients\n\nLet's see which coefficients are in our model are largest and/or confidently non-zero. I'll pull out the coefficients and sort them by their posterior mean. I'll also identify the coefficients for which the highest posterior density does not include zero. I'm using a 90% HPDI, but that's arbitrary.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = models[5]\ntrace1 = traces[5]\nmodel1.name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = pm.summary(trace1, varnames=[\"beta\"], alpha=.10)\n\ntop_coefs = (coefs\n             .assign(abs_est = abs(coefs[\"mean\"]), non_zero = np.sign(coefs[\"hpd_5\"]) == np.sign( coefs[\"hpd_95\"]))\n             .sort_values(\"abs_est\", ascending=False)\n            ).head(20)\n\ntop_coefs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, I'll plot the top 20 coefficients' loess curves in logit space for visual inspection. Features 33 and 65 are the strongest features (consistent with what others have found) and that's pretty evident from the plots."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotnine.options.figure_size = (12,9)\n\nregex = re.compile(\"__(.*)\")\ntop_feats = [regex.search(x)[1] for x in list(top_coefs.index)]\n\nplot_loess(df, top_feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nFinally, I'll generate predictions for the leaderboard for all these models.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # generate test predictions and create submission file\ndef generate_submission(trace, file_suffix=\"\"):\n\n    test_predictions = trace_predict(trace, X2)\n\n    submission  = pd.DataFrame({'id':df2.id, 'target':test_predictions})\n    submission.to_csv(\"submission_\"+file_suffix+\".csv\", index = False)\n    return(None)\n\nfor model in model_dict.keys():\n    generate_submission(model_dict[model], model.name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To hedge bets further, we can also generate predictions using model averaging, averaging over the models that seemed reasonable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# grab potentially reasonable models\ncomp_MA = pm.stats.compare(dict(zip(models[0:-3], traces[0:-3])), ic=\"LOO\", method='BB-pseudo-BMA')\n\n# do prediction from averaged model\nppc_w = pm.sample_posterior_predictive_w(traces[0:-3], 4000, [make_model(X2,np.zeros(19750),c) for c in cauchy_scale_pars[0:-3]],\n                        weights=comp_MA.weight.sort_index(ascending=True),\n                        progressbar=True)\n                        \ny_hatMA = np.mean(np.array(ppc_w['y_obs']), axis=0)\nsubmission  = pd.DataFrame({'id':df2.id, 'target':y_hatMA})\nsubmission.to_csv('submission_MA.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How did we do? I've tested some of these models on the public leaderboard. The winners are the $\\gamma$ = .025 and the model averaged predictions, with a public leaderboard score of .860. Overall, the models' actual out-of-sample performance largely accords with LOO, with the exception of the overfit $\\gamma$ = .05 model. Some of the differences are subtle, but largely so are the differences in LOO.\n\nHere's a table manually reproducing the scores for a selection of models alongside public leaderboard scores.\n\n|      model      |   LOO  |  pLOO | training AUC | LB AUC |\n|:---------------:|:------:|:-----:|:------------:|:------:|\n|  $\\gamma$ = .01 | 250.75 | 38.35 |     .955     |  .847  |\n| $\\gamma$ =.0175 | 238.78 | 47.69 |     .978     |  .859  |\n|  $\\gamma$ =.025 | 236.63 | 55.65 |     .988     |__.860__|\n|  $\\gamma$ =.04  | 229.64 | 65.64 |     .997     |  .857  |\n|  $\\gamma$ =.05  |  220.5 | 68.08 |     .999     |  .856  |\n|     Averaged    |   n/a  |  n/a  |              |__.860__|\n\n# Conclusion\n\nThis Bayesian logistic regression model with regularizing cauchy priors performs quite well on the public leaderboard, with a very respectable score of .860.\n\nIn addition:\n* We were able to use all predictors in our model, despite having more predictors than observations.\n* Using subjective intuitions and some visual diagnostics, it was relatively easy to come up with sensible starting values for our prior\n* Using LOO let us approximate cross validation in a Bayesian setting and with very little data\n* Using a continuous prior let us use the sophisticated NUTS sampler in PyMC3\n* Using PyMC3 let us do all of this at a high level of abstraction: creating, fitting, and predicting from a custom model in less than 20 lines of code\n\nThis was my first time using PyMC3 and I've come away pretty impressed!"},{"metadata":{},"cell_type":"markdown","source":"\n----------------------------------------\n\n# Appendix\n\nTo me, the spirit of this competition is broken if you test too much based on submissions to the public leaderboard. Still, it's interesting to get feedback on how different approaches are working, and whether the methods deployed to compare models are translating properly into the real world.\n\n## Non-linearities?\n\nI still have some suspicions there could be non-linear relationships in the dataset. I believe the last verion of this contest was solvable with a hyperplane, so a natural extension for this second version would be to add non-linear relationships. \n\nBelow I've picked out 10 candidates I observed for non-linear relationships, plus 65 and 33, two feaures that others have found to have strong linear relationships. If they really are non-linear, I think most of these could be captured by a 3rd-order polynomial."},{"metadata":{"trusted":true},"cell_type":"code","source":"non_lin_feats = [\"276\",\"91\",\"240\",\"246\",\"253\",\"255\",\"268\",\"118\",\"240\",\"7\",\"167\",\"65\",\"33\"]\n\nplot_loess(df, non_lin_feats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite these plots, I've played around with adding polynomial terms to the model but never had any success improving leaderboard scores. I just don't think there's enough data to fit these more complex models. \n\nStill, leaving the PyMC3 model here for posterity."},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_polymodel(X, y):\n    \n    with pm.Model() as model:\n        \n        # Priors for unknown model parameters\n        alpha = pm.Normal('alpha', mu=0, sd=3)\n        beta1 = pm.Cauchy('beta', alpha=0, beta=.07, shape=X.shape[1])\n        beta2 = pm.Cauchy('beta^2', alpha=0, beta=.07, shape=X.shape[1])\n        beta3 = pm.Cauchy('beta^3', alpha=0, beta=.07, shape=X.shape[1])\n        \n        mu1 = pm.math.dot(X, beta1)\n        mu2 = pm.math.dot(np.power(X,2), beta2)\n        mu3 = pm.math.dot(np.power(X,3), beta3)\n        \n        p = invlogit(alpha + mu1 + mu2 + mu3)\n        \n        # Likelihood (sampling distribution) of observations\n        y_obs = pm.Bernoulli('y_obs', p=p,  observed=y)\n        \n    return(model)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}