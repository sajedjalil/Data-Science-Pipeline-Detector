{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1a1b68b5-f272-d015-4355-e080a09a9888"},"source":"**Overview** This notebook provides basic data exploration and wrangling. Prediction model is based on gradient boosting decision tree (from scikit-learn) with additional consideration of abnormal peaks in target variable, regression is tuned with randomized search cross validation.  At the end of notebook is unfinished attempt to apply adversarial validation. \n\nBased on kernels:\n\n[Simple data exploration](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-sberbank) \n\n[Adversarial validation](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b901964-0ff8-07c0-e77c-041553083c81"},"outputs":[],"source":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import  precision_score,recall_score,average_precision_score,roc_auc_score\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 10, 10\ncolor = sns.color_palette()"},{"cell_type":"markdown","metadata":{"_cell_guid":"00c4fd34-2696-73ed-e5f8-eb1951533012"},"source":"### 1.Data exploration"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b579fd5-fa46-16ca-7567-ebf6da77be22"},"outputs":[],"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\nid_test = test_df.id\n\nprint('train_df shape:',train_df.shape)\nprint('test_df shape:',test_df.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb7b7782-3a4d-a259-7834-6867311b4c6e"},"outputs":[],"source":"dtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\nprint('Variables data type:')\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6a0b4c5d-00e1-b597-c430-739d4d01f171"},"source":"### Data quality"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edcb865f-6273-25f4-1b79-65a8b6e4105d"},"outputs":[],"source":"train_df.describe().round(1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a345d76c-ae5d-8b68-ae6c-99d68f7b0c97"},"source":"build_year column contain outlier greater by few orders of magnitude than rest of data, probably second half of number (another year) belong to next row of data.\nAlso maximal value in state is out of scale (1-4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee8a8724-9e34-5ad0-e22c-942a9f05c16a"},"outputs":[],"source":"print(train_df.loc[train_df['build_year'] == 20052009].id)\nprint(train_df.loc[train_df['state'] == 33].id)\nprint('build_year:',train_df.ix[10090].build_year)\nprint('state:',train_df.ix[10090].state)\n\ntrain_df.loc[train_df['id'] == 10092, 'build_year'] = 2007\ntrain_df.loc[train_df['id'] == 10092, 'state'] = 3\ntrain_df.loc[train_df['id'] == 10093, 'build_year'] = 2009"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"729b636c-f52d-c094-7de3-0cd9f424b0fd"},"outputs":[],"source":"train_df.describe().round(1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"41d46be6-be6c-defb-4622-0aa00219d487"},"source":"### Missing data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1baecda-382d-482b-782a-aa7f2c5b4f1d"},"outputs":[],"source":"train_na = (train_df.isnull().sum() / len(train_df)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nsns.barplot(y=train_na.index, x=train_na,color=color[0])\nplt.xlabel('% missing')"},{"cell_type":"markdown","metadata":{"_cell_guid":"73528b4c-b012-0eeb-6cea-f85f3af10905"},"source":"Encoding categorical data to numerical"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71b6e2e4-fe89-751c-969e-4c205f5bb42d"},"outputs":[],"source":"for f in train_df.columns:\n    if train_df[f].dtype=='object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \nfor c in test_df.columns:\n    if test_df[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(test_df[c].values)) \n        test_df[c] = lbl.transform(list(test_df[c].values))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ba96b990-57be-a4b7-4b2a-61b3e120e952"},"source":"Variables like life_sq or kitch_sq are important in prediction (see below Features importance), and because they are linked with full_sq it is better to fill missing values with ratio of full_sq than median or mean.\n\nRest of missing values is filled with median of respected feature."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8947de4-df6c-beee-fc26-be22a1e318b8"},"outputs":[],"source":"kitch_ratio = train_df['full_sq']/train_df['kitch_sq']\ntrain_df['kitch_sq']=train_df['kitch_sq'].fillna(train_df['full_sq'] /kitch_ratio.median())\ntest_df['kitch_sq']=test_df['kitch_sq'].fillna(test_df['full_sq'] /kitch_ratio.median())\n\nlifesq_ratio = train_df['full_sq']/train_df['life_sq']\ntrain_df['life_sq']=train_df['life_sq'].fillna(train_df['full_sq'] /lifesq_ratio.median())\ntest_df['life_sq']=test_df['life_sq'].fillna(test_df['full_sq'] /lifesq_ratio.median())\n\ntrain_df=train_df.fillna(train_df.median(),inplace=True)\ntest_df=test_df.fillna(test_df.median(),inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f95a9141-a29d-05f9-4504-2c7a5fd1be50"},"source":"### Exploration of target variable"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35e5edcf-e859-c17f-c19f-df9904c19296"},"outputs":[],"source":"sns.distplot(train_df.price_doc.values, kde=None)\nplt.xlabel('price')"},{"cell_type":"markdown","metadata":{"_cell_guid":"b54b73ec-bf89-633c-70ee-ba0d1eb25973"},"source":"Our target variable is spread across few orders of magnitude, it's more suitable to work with log10 of this value.\nAlso it's reasonable to 'coarse grind' our data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f442408-115e-3ac5-60ed-9338243e88b0"},"outputs":[],"source":"ulimit = np.percentile(train_df.price_doc.values, 99)\nllimit = np.percentile(train_df.price_doc.values, 1)\ntrain_df.loc[train_df['price_doc'] >ulimit, 'price_doc'] = ulimit\ntrain_df.loc[train_df['price_doc'] <llimit, 'price_doc'] = llimit\n\nsns.distplot(np.log(train_df.price_doc.values),  bins=50,kde=None)\nplt.xlabel('price')\n\ntrain_df['price_doc_log'] = np.log1p(train_df['price_doc'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"04485af3-e399-fcca-2c62-1a746780bb27"},"source":"Log of our data have distribution close to normal, with exception to two abnormal peaks on left side.  Lets mark this values for further 'investigation'."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da2a9559-1d43-5b6c-5589-4810194485e4"},"outputs":[],"source":"print(train_df['price_doc'].value_counts().head(10))\n\ntrain_df['label_value'] = 0\ntrain_df.loc[train_df['price_doc'] == 1000000, 'label_value'] = 1\ntrain_df.loc[train_df['price_doc'] == 2000000, 'label_value'] = 2"},{"cell_type":"markdown","metadata":{"_cell_guid":"60bc6669-98ae-a214-3582-6c7e5765104b"},"source":"### 2.Model evaluation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9068ff9f-d032-429e-0e82-9e6ad9cfdf53"},"outputs":[],"source":"data_X = train_df.drop([\"id\",\"timestamp\",\"price_doc\",\"price_doc_log\",'label_value'],axis=1)\ndata_y = train_df['price_doc_log']"},{"cell_type":"markdown","metadata":{"_cell_guid":"906cd41e-6863-5446-2520-bbba2bb43a58"},"source":"Gradient boosted tree was tuned with randomized cross validation. Because it's time consuming to compute, results are listed below."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93a1821f-a1bc-9461-c9a2-e8fcd13980fc"},"outputs":[],"source":"# GBmodel = GradientBoostingRegressor()\n# param_dist = {\"learning_rate\": np.linspace(0.05, 0.15,5),\n#               \"max_depth\": range(3, 5),\n#               \"min_samples_leaf\": range(3, 5)}\n\n# rand = RandomizedSearchCV(GBmodel, param_dist, cv=7,n_iter=10, random_state=5)\n# rand.fit(data_X,data_y)\n# rand.grid_scores_\n\n# print(rand.best_score_)\n# print(rand.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"48b25cff-3526-5d4d-5749-2345df24b133"},"source":"best score: 0.38834623226916964 \n\nbest parameters: {'min_samples_leaf': 4, 'learning_rate': 0.1, 'max_depth': 4}"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf2637d5-455d-a910-b973-b46b9b3d7684"},"outputs":[],"source":"GBmodel = GradientBoostingRegressor(min_samples_leaf= 4, learning_rate= 0.1, max_depth= 4)\nGBmodel.fit(data_X,data_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"692b05be-f62c-fb63-7370-329c6e9d6d27"},"outputs":[],"source":"sns.distplot(GBmodel.predict(data_X),kde=None)"},{"cell_type":"markdown","metadata":{"_cell_guid":"10c2517f-a330-9a4b-3003-3b78285f7344"},"source":"## Classification"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ce5ddb5-c701-3dca-6a0d-499b1f6da6fb"},"source":"Because our regression model doesn't predict briefly mentioned peaks let's create classification model for them. After classification respected values will be assigned."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1df0bfcf-c534-3e2c-7c82-b8e26423ec0e"},"outputs":[],"source":"clfdata_X = train_df.drop(['id','timestamp','label_value','price_doc_log','price_doc'],axis=1)\nclfdata_y = train_df['label_value']\n\nclfX_train, clfX_test, clfY_train, clfY_test = train_test_split(clfdata_X, clfdata_y, test_size=0.30,random_state=31)\n\nGBclf= GradientBoostingClassifier()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e97dc158-47dc-1f46-0730-2384a3e35397"},"outputs":[],"source":"GBclf.fit(clfX_train,clfY_train)\nGBclf.score(clfX_test,clfY_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26156c05-4c69-be2d-6440-d09c240dfb15"},"outputs":[],"source":"print(precision_score(GBclf.predict(clfX_test),clfY_test.values,average='macro'))\nprint(recall_score(GBclf.predict(clfX_test),clfY_test.values,average='macro'))\n\nprint(precision_score(GBclf.predict(clfX_test),clfY_test.values,average='micro'))\nprint(recall_score(GBclf.predict(clfX_test),clfY_test.values,average='micro'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d709ae18-47a0-2a48-383b-e40ee3488666"},"outputs":[],"source":"pred = GBmodel.predict(data_X)\nlab = GBclf.predict(clfdata_X)\npred_Y = pd.DataFrame({'pred': np.expm1(pred), 'label':lab})\n\n\n\npred_Y.loc[pred_Y['label'] == 1, 'pred'] = 1000000\npred_Y.loc[pred_Y['label'] == 2, 'pred'] = 2000000\nsns.distplot(np.log(pred_Y.pred),kde=None)\nsns.distplot(train_df.price_doc_log.values,kde=None)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ffa71cdb-3ba8-2d51-6984-b3cfb190bcec"},"source":"Unfortunately prediction performance is rather awful, model needs further development."},{"cell_type":"markdown","metadata":{"_cell_guid":"078dfec0-eb74-d335-9123-006a185fabc3"},"source":"## Features importance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16f949b1-5599-df90-65fd-a09cfcb32559"},"outputs":[],"source":"importances = GBmodel.feature_importances_\nimportances_by_trees=[tree[0].feature_importances_ for tree in GBmodel.estimators_]\nstd = np.std(importances_by_trees,axis=0)\nindices = np.argsort(importances)[::-1]\n\n\nsns.barplot(importances[indices][:20],data_X.columns[indices[:20]].values)\nplt.title(\"Feature importances - regression\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ff3bff6-cb43-cb2b-94c8-8b357326994b"},"outputs":[],"source":"clf_importances = GBclf.feature_importances_\nclf_importances_by_trees=[tree[0].feature_importances_ for tree in GBclf.estimators_]\nclf_std = np.std(clf_importances_by_trees,axis=0)\nclf_indices = np.argsort(clf_importances)[::-1]\n\n\nsns.barplot(clf_importances[clf_indices][:20],clfdata_X.columns[clf_indices[:20]].values)\nplt.title(\"Feature importances - classification\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"6c6b9041-a0ae-73b2-06dd-69dd62db7ca9"},"source":"### 3.Prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80da5f87-c777-834f-fac9-bef099bd7bf0"},"outputs":[],"source":"predict = GBmodel.predict(test_df.drop([\"id\", \"timestamp\"],axis=1))\nlabel = GBclf.predict(test_df.drop(['id','timestamp'],axis=1))\noutput = pd.DataFrame({'id': id_test, 'price_doc': np.expm1(predict), 'label':label})\n\n\n\noutput.loc[output['label'] == 1, 'price_doc'] = 1000000\noutput.loc[output['label'] == 2, 'price_doc'] = 2000000\noutput = output.drop(['label'],axis=1)\noutput.to_csv('output.csv', index=False)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"57898282-e02e-6c62-cf43-601d24210f57"},"source":"### Adversarial validation (unfinished)\n\n\nAccording to [this kernel](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms) our test data is easy distinguished from training data. In this case it is better to validate our model with set familiar to test data. Below is unfinished attempt in implementing this method."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7405906-8e76-1d11-7b2a-1dd4c9506498"},"outputs":[],"source":"train_dfadv = train_df.drop([\"timestamp\",\"price_doc\",\"price_doc_log\"],axis=1)\ntest_dfadv = test_df\ntrain_dfadv['istrain'] = 1\ntest_dfadv['istrain'] = 0\nwhole_df = pd.concat([train_dfadv, test_dfadv], axis = 0)\nwhole_df = whole_df.fillna(whole_df.median())\nvalY = whole_df['istrain']\nvalX = whole_df.drop(['istrain',\"id\", \"timestamp\"],axis=1)\n\nX_vtrain, X_vtest, y_vtrain, y_vtest = train_test_split(valX.values, valY.values, test_size=0.20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"646b74dd-83d2-1c35-9153-533b301cbb96"},"outputs":[],"source":"GBclf= GradientBoostingClassifier()\nGBclf.fit(X_vtrain,y_vtrain)\nvpred_y = GBclf.predict(X_vtest)\nroc_auc_score(vpred_y,y_vtest)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c581f134-641f-9f5b-2f65-72b3bacf3fdb"},"outputs":[],"source":"importances = GBclf.feature_importances_\nimportances_by_trees=[tree[0].feature_importances_ for tree in GBclf.estimators_]\nstd = np.std(importances_by_trees,axis=0)\nindices = np.argsort(importances)[::-1]\n\n\nsns.barplot(importances[indices][:20],valX.columns[indices[:20]].values)\nplt.title(\"Feature importances\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e615d41c-789f-73dc-ad53-2a5dca71770c"},"outputs":[],"source":"X=train_df.drop([\"id\", \"timestamp\", \"price_doc\",\"price_doc_log\"], axis=1)\ny=train_df.price_doc_log.values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1ea58b9-105f-10d2-b06c-06dc99f22f8c"},"outputs":[],"source":"val_prob = GBclf.predict_proba(X)\nadversarial_set = train_df\nadversarial_set['prob'] = val_prob.T[1]\n\nadversarial_set=adversarial_set.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n\nadversarial_set_length =int(adversarial_set.shape[0]*0.20)\nadversarial_set = adversarial_set.sort_values(by='prob')\nvalidation_set = adversarial_set[:adversarial_set_length] \ntrain_set = adversarial_set[adversarial_set_length:]\n\ntrainY  =train_set['price_doc_log'].values\ntrainX = train_set.drop(['price_doc_log','prob'],axis=1).values\n\nvalidationY  =validation_set['price_doc_log'].values\nvalidationX = validation_set.drop(['price_doc_log','prob'],axis=1).values"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}