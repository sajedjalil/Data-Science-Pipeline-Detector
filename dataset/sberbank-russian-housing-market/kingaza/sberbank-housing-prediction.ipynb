{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"1d10f51c-1e0f-d50b-8edc-d247c32d633d"},"source":"# This notebook will contain a quick walk-through, going from original data to outcome.\n## It contains limited EDA, which is contained in other notebooks.\n\nI hope you will find some useful information in the notebook. Fork away if you find it useful. Please also leave comments if you have suggestions of what can be improved or what you'd like to see added.\n\n### The original data set is broken up into a number of testing data sets, namely\n\n 1. Original: The original data set, where NaN data will be imputed\n    (30,500 rows, 292 features)\n 2. NA_Row: A data set with all rows containing NaN data removed (6,000\n    rows, 292 features)\n 3. NA_Col: A data set with all features containing NaN data removed\n    (30,500 rows, 241 features)\n 4. Clean: A data set with features containing > 500 NaNs removed,\n    followed by removal of those rows that contain NaNs (30,000 rows,\n    251 features)\n 5. Dummies: The clean data set, with dummy variables created for the\n    object features (30,000 rows, 391 features)\n\n### As a first algorithm, Gradient Boosting Regression is used to test on these data sets. I will be adding additional algorithms as time goes on."},{"cell_type":"markdown","metadata":{"_cell_guid":"d4c2362b-fa68-c352-f01a-863ecc7f43f4"},"source":"# Importing the main packages as well as training sets.\nI have done a minimal bit of pre-processing on some of the columns."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21c744c5-f09a-3588-77cc-0bc4bb77a886"},"outputs":[],"source":"# Importing main packages and settings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7729b99e-f49a-62fe-f7fb-164d8b82ebf1"},"outputs":[],"source":"# Loading the training dataset\ndf = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n\n# Adding feature for yearmonth of purchase and removing ID and timestamp\ndf['yearmonth'] = df['timestamp'].dt.year*100 + df[\"timestamp\"].dt.month\ndf = df.drop(['id','timestamp'], axis=1)\n\n# Adding log price for use as target variable\ndf['log_price_doc'] = np.log1p(df['price_doc'].values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6c0614b4-940f-6284-0096-254e9c0caff0"},"source":"# Creation of testing data sets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"096b566c-6458-50a6-4621-5f73231bf00c"},"outputs":[],"source":"# df with all rows containing NA removed\ndf_na_row = df.dropna(axis=0)\n\n# df with all features containing NA removed\ndf_na_col = df.dropna(axis=1)\n\n# First 3 dataframes\nprint(\"Original DataFrame: {}\".format(df.shape))\nprint(\"DataFrame After Dropping All Rows with Missing Values: {}\".format(df_na_row.shape))\nprint(\"DataFrame After Dropping All Features with Missing Values: {}\".format(df_na_col.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"585d1fec-c017-657d-d93e-85efee0d52e8"},"outputs":[],"source":"# courtesy of SRK's notebook - to determine features with missing data\nmissing_df = df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count']>0]\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(8,18))\nrects = ax.barh(ind, missing_df.missing_count.values)\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b02e168-94dd-19e1-9580-8ce4d5bd5487"},"source":"## Creation of a data set that only removes features with lots of missing data and with added dummies variables.\nDropping only features with >500 NaNs and subsequently dropping rows with missing data results in preserving most of the rows and more features. Dummy variables are subsequently created for the object type features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c250bbe-8ee0-bde7-ebb4-00ffd354f100"},"outputs":[],"source":"missing500_df = missing_df.ix[missing_df['missing_count']>500]\nmissing500_cols = missing500_df['column_name'].values\ndf_clean = df.drop(missing500_cols, axis=1).dropna(axis=0)\nprint(\"DataFrame After Dropping Features >500 NaN And Then Rows: {}\".format(df_clean.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0c3f8639-2702-5b49-dfb5-ea328e94e5c0"},"outputs":[],"source":"df_dummies = pd.get_dummies(df_clean, drop_first=True)\nprint(\"Clean DataFrame With Dummy Variables: {}\".format(df_dummies.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8050ef70-2f2d-c30d-3155-0cd470c55e80"},"source":"## Creation of X features and y targets based on the data sets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a3d22c0-bc57-7f91-49ad-73bd253b42c0"},"outputs":[],"source":"# X and y based on original df, removing object type features\nX_orig = df.drop(['price_doc', 'log_price_doc'], axis=1).select_dtypes(exclude=['object', 'datetime64']).values\ny_orig = df['price_doc'].values\n\n# X and y based on df with rows containing NaN dropped, removing object type features\nX_row = df_na_row.drop(['price_doc', 'log_price_doc'], axis=1).select_dtypes(exclude=['object', 'datetime64']).values\ny_row = df_na_row['price_doc'].values\n\n# X and y based on df with features containing NaN dropped, removing object type features\nX_col = df_na_col.drop(['price_doc', 'log_price_doc'], axis=1).select_dtypes(exclude=['object', 'datetime64']).values\ny_col = df_na_col['price_doc'].values\n\n# X and y based on df with dummy variables for object features\nX_dummies = df_dummies.drop(['price_doc', 'log_price_doc'], axis=1).values\ny_dummies = df_dummies['price_doc'].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06391a85-4727-cf4d-f30c-56b4e227ce5d"},"outputs":[],"source":"# Import the relevant sklearn packages\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingRegressor"},{"cell_type":"markdown","metadata":{"_cell_guid":"3d1d7b2b-abe6-826d-6b04-24c721b779f5"},"source":"## Initial test using Gradient Boosting Regressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"28653c99-029c-dfdf-3107-a48fda5cbcb3"},"outputs":[],"source":"# removing warning just for now - will need to look into this\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# instantiating\nimp = Imputer(missing_values='NaN', strategy='median', axis=0)\nscaler = StandardScaler()\ngbr = GradientBoostingRegressor()\n\n# setting up steps for the pipeline, with and without imputating\nsteps_exclimp = [('scaler', scaler),\n        ('GradientBoostingRegressor', gbr)]\n\nsteps_inclimp = [('imputation', imp),\n        ('scaler', scaler),\n        ('GradientBoostingRegressor', gbr)]\n\n# instantiating the pipeline\npipe = Pipeline(steps_exclimp)\npipe_imp = Pipeline(steps_inclimp)\n\n# creating train ang test sets using train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_dummies, y_dummies, test_size=0.3, random_state=42)\n\n# fitting and predicting\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(pipe.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e0b91b2d-d125-afa0-d211-e54496fe3444"},"source":"## Cross Validation Scores for the different data sets using Gradient Boosting Regressor"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5e7b882-e98c-b818-3994-121df6698806"},"outputs":[],"source":"# Compute 3-fold cross-validation scores: cv_scores\ncv_scores_dummies = cross_val_score(pipe, X_dummies, y_dummies, cv=3)\n\n# Print the 3-fold cross-validation scores\nprint(cv_scores_dummies)\n\nprint(\"Average 3-Fold CV Score: {}\".format(np.mean(cv_scores_dummies)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12ca52b3-49e0-2a33-8a89-bd6833695d74"},"outputs":[],"source":"# Compute 3-fold cross-validation scores: cv_scores\ncv_scores_na_row = cross_val_score(pipe, X_row, y_row, cv=3)\n\n# Print the 3-fold cross-validation scores\nprint(cv_scores_na_row)\n\nprint(\"Average 3-Fold CV Score: {}\".format(np.mean(cv_scores_na_row)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7994d85-5074-6a04-8b54-e45bb4363c99"},"outputs":[],"source":"# Compute 3-fold cross-validation scores: cv_scores\ncv_scores_na_col = cross_val_score(pipe, X_col, y_col, cv=3)\n\n# Print the 3-fold cross-validation scores\nprint(cv_scores_na_col)\n\nprint(\"Average 3-Fold CV Score: {}\".format(np.mean(cv_scores_na_col)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f012b833-4d70-ce49-cc1d-0aa7ed54a2ff"},"outputs":[],"source":"# Compute 3-fold cross-validation scores: cv_scores\ncv_scores_orig = cross_val_score(pipe_imp, X_orig, y_orig, cv=3)\n\n# Print the 3-fold cross-validation scores\nprint(cv_scores_orig)\n\nprint(\"Average 3-Fold CV Score: {}\".format(np.mean(cv_scores_orig)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1fbaae72-0857-cc32-980f-574ab173f790"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}