{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9c9dd449-7187-2428-5cd7-4237942c4620"},"source":"Using XGB and some basic features this single model scored me a 0.325 on LB.\n\nThis is one of my first kernels and only second competition so please feel free to point out anything I could do better or any flaws in my work so far.   \n\nI drew inspiration for a bunch of features from previous Kaggle competitions but also from a few very useful notebooks.  \n\nMost notably:\n- https://www.kaggle.com/sudalairajkumar/feature-engineering-validation-strategy\n-  https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\n\nThank you!"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52617b5c-dd86-e2ba-4fa5-e4dc663ada2e"},"outputs":[],"source":"import numpy as np \nimport pandas as pd "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4443d20b-9d9c-595b-1ed8-d0a101d1a946"},"outputs":[],"source":"#Load Data\ntrain_raw = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\ntest_raw = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\nmacro_raw = pd.read_csv('../input//macro.csv', parse_dates=['timestamp']) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2dc5293f-d0b2-8c9d-cd9f-0e01e2a8c156"},"outputs":[],"source":"#Join macro-economic data\ntrain_full = pd.merge(train_raw, macro_raw, how='left', on='timestamp')\ntest_full = pd.merge(test_raw, macro_raw, how='left', on='timestamp')"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5557637-037e-c4a1-064d-0899925e0ead"},"source":"## Feature Engineering\n#### First encode categorical features\nQuestion - I have seen pd.get_dummies(), pd.factorize, one hot encoding, and label encoder.  I understand how one hot encoding works, but is there a difference or a reason why get_dummies, factorize, and label encoder should be used over another?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5003ecc6-2e7d-dc6f-6f50-ab27c893286e"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_object_features(train, test):\n    '''(DataFrame, DataFrame) -> DataFrame, DataFrame\n    \n    Will encode each non-numerical column.\n    '''\n    train = pd.DataFrame(train)\n    test = pd.DataFrame(test)\n    cols_to_encode = train.select_dtypes(include=['object'], exclude=['int64', 'float64']).columns\n    for col in cols_to_encode:\n        le = LabelEncoder()\n        #Fit encoder\n        le.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n        #Transform\n        train[col] = le.transform(list(train[col].values.astype('str')))\n        test[col] = le.transform(list(test[col].values.astype('str')))\n    \n    return train, test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79dd6eb2-b9e4-3e82-b38d-343c58284b06"},"outputs":[],"source":"train_df, test_df = encode_object_features(train_full, test_full)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c9b9d4a8-abb6-9969-4e6c-be642e23dd5c"},"source":"Add new features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1be2f93a-5633-d0fd-ae3e-15635d5fd767"},"outputs":[],"source":"def add_date_features(df):\n    '''(DataFrame) -> DataFrame\n    \n    Will add some specific columns based on the date\n    of the sale.\n    '''\n    #Convert to datetime to make extraction easier\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    #Extract features\n    df['month'] = df['timestamp'].dt.month\n    df['day'] = df['timestamp'].dt.day\n    df['year'] = df['timestamp'].dt.year\n    \n    #These features inspired by Bruno's Notebook at https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\n    #Month-Year\n    month_year = df['timestamp'].dt.month + df['timestamp'].dt.year * 100\n    month_year_map = month_year.value_counts().to_dict()\n    df['month_year'] = month_year.map(month_year_map)\n    #Week-Year\n    week_year = df['timestamp'].dt.weekofyear + df['timestamp'].dt.year * 100\n    week_year_map = week_year.value_counts().to_dict()\n    df['week_year'] = week_year.map(week_year_map)\n    df.drop('timestamp', axis=1, inplace=True)\n    return df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11c18330-9817-5845-1abe-44f547c2f693"},"outputs":[],"source":"def add_state_features(df):\n    '''(DataFrame) -> DataFrame\n    \n    Add's features, meant to be used for both train and test df's.\n    Does some operations to the state grouping\n    '''\n    #Get median of full sq by state\n    df['state_median_full_sq'] = df['full_sq'].groupby(df['state']).transform('median')\n    #Build features from full sq median by state\n    df['full_sq_state_median_diff'] = df['full_sq'] - df['state_median_full_sq']\n    df['life_sq_state_median_full_diff'] = df['life_sq'] - df['state_median_full_sq']\n    #Drop helper columns\n    df.drop('state_median_full_sq', axis=1, inplace=True)\n    \n    return df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f987756-0d39-a2b4-c7ef-9c7ae498df26"},"outputs":[],"source":"def add_features(df):\n    '''(DataFrame) -> DataFrame\n    \n    Add's features, meant to be used for both train and test df's.\n    '''\n    #Floor\n    df['floor_ratio'] = df['floor'] / df['max_floor'].astype(float)\n    df['floor_from_top'] = df['max_floor'] - df['floor']\n    #Sq areas\n    df['kitch_sq_ratio'] = df['kitch_sq'] / df['full_sq'].astype(float)\n    df['life_sq_ratio'] = df['life_sq'] / df['full_sq'].astype(float)\n    df['full_sq_per_room'] = df['full_sq'] / df['num_room'].astype(float)\n    df['life_sq_per_room'] = df['life_sq'] / df['num_room'].astype(float)\n    df['full_living_sq_diff'] = df['full_sq'] - df['life_sq']\n    #df['full_sq_per_floor'] = df['full_sq'] / df['max_floor'].astype(float) #No value added\n    df = add_date_features(df)\n    df = add_state_features(df)\n    df['build_year_vs_year_diff'] = df['build_year'] - df['year']  #no change\n    \n    #Drop Id -> Made it worse\n    #df.drop('id', axis=1, inplace=True)\n    \n    #School Variables -> Made it worse\n    #df['preschool_quota_ratio'] = df[\"children_preschool\"] / df[\"preschool_quota\"].astype(\"float\")\n    #df['school_quota_ratio'] = df[\"children_school\"] / df[\"school_quota\"].astype(\"float\")\n    return df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a93dfb6-9cdf-b494-cfb2-dbcefab4462e"},"outputs":[],"source":"train_df = add_features(train_df)\ntest_df = add_features(test_df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bef5c91-5ee6-50a7-739e-a138f996ccf0"},"outputs":[],"source":"train_df.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"8981f25a-dd66-889d-1a70-123883c0ae31"},"source":"## Cross-Validate\n\nHere I use cross-validation to test my new features. After, I also train a model to take a look at the feature_importances as determined by the XGB algorithm. These importances can give you ideas of which features to focus on for further feature engineering."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9db67b0d-0bb7-a943-2ccf-fc8d41c55a11"},"outputs":[],"source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\n#Get Data\n#Y_train = train_df['price_doc'].values\nY_train = np.log1p(train_df['price_doc'].values)\nX_train = train_df.ix[:, train_df.columns != 'price_doc'].values\nX_test = test_df.values\n\n#Initialize Model\nxgb = XGBRegressor()\n#Create cross-validation\ncv = TimeSeriesSplit(n_splits=5)\n#Train & Test Model\ncross_val_results = cross_val_score(xgb, X_train, Y_train, cv=cv, scoring='neg_mean_squared_error')\nprint(cross_val_results.mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8cde2d22-dca9-4af4-9ae6-54de738ab264"},"outputs":[],"source":"model = xgb.fit(X_train, Y_train)\nmodel.feature_importances_;\n#Python 3 error with code below, need to fix\n#importances = zip(model.feature_importances_, train_df.ix[:, train_df.columns != 'price_doc'].columns)\n#importances = pd.DataFrame(importances, columns=['importance', 'feature'])\n#importances.sort_values('importance', ascending=False).head(30)"},{"cell_type":"markdown","metadata":{"_cell_guid":"daf78057-e0fc-3664-9690-6bb94df8c148"},"source":"## Train Model & Submit\n\nHere I train my final model with some tuned parameters to the XGB."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df53acc9-969d-cc97-5fe1-9dcfc05d5e3a"},"outputs":[],"source":"from xgboost import XGBRegressor\n\n#Get Data\nY_train = train_df['price_doc'].values\nX_train = train_df.ix[:, train_df.columns != 'price_doc'].values\nX_test = test_df.values\n#Init Model\nxgb = XGBRegressor(learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.7)\n#Train Model\nmodel = xgb.fit(X_train, Y_train)\n#Make Predictions\npredictions = xgb.predict(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b32130e7-b17d-af55-e6f9-4ef721ab5a25"},"outputs":[],"source":"#Make Submission File\nsubmission_df = pd.DataFrame({'id':test_full['id'], 'price_doc':predictions})\nsubmission_df.to_csv('xgb-added_features.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6699ebf3-a372-362c-59da-b77aa30b437f"},"source":"## Next Steps\n\nCurrently my best submission as me at 53% on the leaderboard.  \n\nSome next steps I want to take are:  \n-Engineer more features - play with groupby (sub_area, more state features, etc)  \n-Remove some features - try PCA? Learn more about when to remove features that aren't adding new info, how to tell  \n-NaN's - there is a lot of missing data and wrong data - should I remove some/all? Correct some (ie. incorrect years)?  \n-ID field seems to improve results - why is this? should I remove it anyway?  \n-Optimize XGB using GridSearch and Build Ensemble learner  \n\nPlease feel free to let me know what I could do better! "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a45346e0-ceee-6c03-ac3a-c1730c08ba18"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}