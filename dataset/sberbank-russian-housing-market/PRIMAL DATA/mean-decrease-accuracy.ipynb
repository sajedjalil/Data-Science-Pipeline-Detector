{"cells":[{"outputs":[],"source":"getting mean accuracy decrease for features using variety of regressors","cell_type":"markdown","execution_count":null,"metadata":{"_uuid":"b4972cffa2d478196d66bfdf4b2576a0ed8a7281","_execution_state":"idle","_cell_guid":"7284af26-2931-4da9-a207-52cbd36d3f0f","collapsed":false}},{"outputs":[],"source":"import pandas as pd\nimport numpy as np","cell_type":"code","execution_count":null,"metadata":{"_uuid":"371ec8fa1edf20cbb8d6fd6d6d0a6c19f64a2845","_execution_state":"idle","_cell_guid":"f04dc8bc-336a-4907-b804-aefd439ce532","trusted":false,"collapsed":false}},{"outputs":[],"source":"df = pd.read_csv(\"../input/train.csv\")\ndf_x = df.drop(labels=[\"price_doc\"], axis=1)\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain = df_x\n#data cleaning\nbad_index = train[train.life_sq > train.full_sq].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\ntest.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\nbad_index = test[test.life_sq > test.full_sq].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.life_sq < 5].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = test[test.life_sq < 5].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.full_sq < 5].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[test.full_sq < 5].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ntrain.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = train[train.kitch_sq >= train.life_sq].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[test.kitch_sq >= test.life_sq].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = train[train.life_sq > 300].index\ntrain.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\nbad_index = test[test.life_sq > 200].index\ntest.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ntrain.product_type.value_counts(normalize= True)\ntest.product_type.value_counts(normalize= True)\nbad_index = train[train.build_year < 1500].index\ntrain.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = test[test.build_year < 1500].index\ntest.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = train[train.num_room == 0].index \ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = test[test.num_room == 0].index \ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [3174, 7313]\ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\ntrain.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = train[train.floor == 0].index\ntrain.ix[bad_index, \"floor\"] = np.NaN\nbad_index = train[train.max_floor == 0].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.max_floor == 0].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = train[train.floor > train.max_floor].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.floor > test.max_floor].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\ntrain.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ntrain.ix[bad_index, \"floor\"] = np.NaN\ntrain.material.value_counts()\ntest.material.value_counts()\ntrain.state.value_counts()\nbad_index = train[train.state == 33].index\ntrain.ix[bad_index, \"state\"] = np.NaN\ntest.state.value_counts()\n\n\ndf_x = train \ncombined = pd.concat([df_x,test], ignore_index=True, axis=0)\nobj_col = combined.select_dtypes(include=[object]).columns\n\nfrom sklearn.preprocessing import LabelEncoder\nfor name in obj_col:\n    if name != \"timestamp\" and name != \"product_type\":\n        print(name)\n        encoder = LabelEncoder()\n        combined[name] = encoder.fit_transform(combined[name].fillna(value=99).values)\n        \n#investment is 1 while occupier 0        \ncombined[\"product_type\"] = combined.product_type.map({\"Investment\":1, \n                                                      \"OwnerOccupier\":0, np.nan:99}).values\n\nfilled= combined.groupby(by=\"sub_area\").fillna(99)\ncombined = filled.merge(combined[[\"id\", \"sub_area\"]], on=\"id\")\n\n#add ratio and age vars\ncombined[\"age\"] = pd.to_datetime(combined[\"timestamp\"]).dt.year - combined.build_year\ncombined[\"ratio\"] = (combined[\"full_sq\"]/combined[\"life_sq\"]).fillna(value=99)\ncombined = combined.replace([-np.inf, np.inf], 99)\n\n\n\n\n\n# add month, monthyear, ratio of green to industry, ofice count over leisure, \n#muslim to christ, ratio of close to far big chuches, mean distance to the art attractions\n#increase in buildings from old_times, ratio of children to youngens, and ratio of young male to female \ncombined[\"timestamp\"] = pd.to_datetime(combined.timestamp)\ncombined = combined.assign(month=combined.timestamp.dt.month)\ncombined = combined.assign(month_year=combined.timestamp.dt.year.astype(str) + combined.month.astype(str))\\\n           .assign(green_industry=combined[\"green_part_3000\"]/combined[\"prom_part_3000\"])\\\n           .assign(work_or_play=combined[\"office_count_1500\"]/(combined[\"sport_count_1500\"]+combined[\"leisure_count_1500\"]))\\\n           .assign(islam_or_christ=combined[\"mosque_count_500\"]/ combined[\"church_count_500\"])\\\n           .assign(church_appeal=combined[\"big_church_count_500\"]/combined[\"big_church_count_1500\"])\\\n           .assign(mean_km_art=combined[[\"museum_km\", \"exhibition_km\", \"catering_km\", \"theater_km\", \"park_km\"]].mean(axis=1))\\\n           .assign(new_to_old_count=combined[\"build_count_after_1995\"]/combined[\"build_count_1971-1995\"])\\\n           .assign(new_to_older_count=combined[\"build_count_after_1995\"]/combined[\"build_count_1946-1970\"])\\\n        .assign(young_to_old=combined[\"0_13_all\"]/combined[\"16_29_all\"])\\\n        .assign(male_to_femal_young=combined[\"young_male\"]/combined[\"young_female\"])\n        \ncombined[\"month_year\"] = combined.month_year.astype(np.float64)\ncombined = combined.replace([-np.inf, np.inf], 99)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"63e6c56e9f52d7535e80a58ec5eca5e611d128c3","_execution_state":"idle","_cell_guid":"e7bc31ff-5ff6-4c72-9757-dac949e52eb3","trusted":false,"collapsed":false}},{"outputs":[],"source":"combined = combined.drop(labels=[\"id\", \"timestamp\"], axis=1, errors=\"ignore\").fillna(value=1)\nx_traindf = combined[:30471].copy()\nx_testdf = combined[30471:]\nprice =  df.price_doc.values","cell_type":"code","execution_count":null,"metadata":{"_uuid":"611af084ca918025c7066fa308edb98d7b1ab987","_execution_state":"idle","_cell_guid":"7ef1a9c1-d42d-4124-bd64-3f83a0c122fd","trusted":false,"collapsed":false}},{"outputs":[],"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x_traindf.values, price, train_size=0.7, test_size=0.3)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"e2874d43c341899dfd40db5eb2bc6194a24ce0ac","_execution_state":"idle","_cell_guid":"4f7f4ad0-c80d-4c44-a20d-66c80d83decd","trusted":false,"collapsed":false}},{"outputs":[],"source":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, BayesianRidge, \\\nPassiveAggressiveRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom collections import defaultdict\n\nregressors = {\"linreg\" : LinearRegression(), \n              \"sdg\" : SGDRegressor(l1_ratio=0, alpha=0.01), \n              \"bayes\" : BayesianRidge(), \n              \"pasagg\" : PassiveAggressiveRegressor(), \n              \"neural\": MLPRegressor(), \n             \"gtr\":GradientBoostingRegressor()}\n\n\ncolumns = x_traindf.columns\n ","cell_type":"code","execution_count":null,"metadata":{"_uuid":"0fa8ec5dd176b295fe1c3c089de773632397b542","_execution_state":"idle","_cell_guid":"169b6b3d-1e5c-4260-943a-22ec5b11c332","trusted":false,"collapsed":false}},{"outputs":[],"source":"splitter = ShuffleSplit(test_size=0.2, n_splits=10)\ndef mean_accuracy(estimator, train, target, reg=None):\n    \"\"\"\n    train and targets should be numpy arrays\n    \"\"\"\n    scores = []\n    decreases = defaultdict(list)\n    for train_index, test_index in splitter.split(X=train, y=target):\n        train_x, train_y = train[train_index, :], target[train_index]\n        test_x, test_y = train[test_index, :], target[test_index]\n    \n        estimator.fit(X=train_x, y=train_y)\n        score = estimator.score(X=test_x, y=test_y)\n        scores.append(score)\n        for i, name in enumerate(columns):\n            test_copy = np.copy(test_x)\n            np.random.shuffle(test_copy[:, i])\n            try:\n                test_copy = estimator.transform(test_copy)\n            except AttributeError:\n                pass\n            score_i = estimator.score(X=test_copy, y=price[test_index])\n            decrease = (score_i - score)/score\n            decreases[name].append(decrease)\n    print(reg)\n    print(np.median(scores))\n    return pd.DataFrame(data=decreases).mean()\n\n","cell_type":"code","execution_count":null,"metadata":{"_uuid":"28d94e1c7935b2418c9454d1a9cbea7881fadfa2","_execution_state":"idle","_cell_guid":"a394e3f0-55fd-439c-b474-7b710c40c013","trusted":false,"collapsed":false}},{"outputs":[],"source":"gtb_imp = mean_accuracy(regressors[\"gtr\"], X_train, Y_train, reg=\"gtr\").nlargest(100).index.to_list()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"fc9b989cfe6f66394e4e2e045309d8946d54a218","_execution_state":"idle","trusted":false,"collapsed":false}},{"outputs":[],"source":"#important = {}\n#for k in regressors.keys():\n#    important[k] = mean_accuracy(regressors[k], X_train, Y_train, reg=k).nlargest(100).index.to_list()","cell_type":"code","execution_count":null,"metadata":{"_uuid":"b0824b87abb8eae8c9faaf07c99e681a0458de86","_execution_state":"idle","_cell_guid":"f6ae8cf0-4e15-4c63-b6c2-2600e0510b9c","trusted":false,"collapsed":false}},{"outputs":[],"source":"from sklearn.decomposition import KernelPCA\ndecomposer = KernelPCA(n_components=20, kernel=\"poly\", copy_X=False)\ncol_indices = pd.Series(data=range(len(columns)), index=columns)\n#performance = {}\ndef validate(reg, col_imp):\n    \"\"\"giv fit regressor and its important columns\"\"\"\n    col_index = col_indices.loc[col_imp].values\n    for_pca = set(columns).difference(col_imp)\n    pca_index = col_indices.loc[for_pca].values\n    X_train_copy = X_train.copy()\n    X_test_copy = X_test.copy()\n    decomposer.fit_transform(X_train_copy[:, pca_index])\n    X_test_copy[:, ] = decomposer.transform(X_test_copy[:, pca_index])\n    reg.fit(X=X_train_copy, y=Y_train)\n    score = regressors[k].score(X=X_test_copy, y=Y_test)  \n    return score","cell_type":"code","execution_count":null,"metadata":{"_uuid":"804c31ea4b190f48840f2565871776f4682155e2","_execution_state":"idle","_cell_guid":"4d0c70c3-2361-44b5-b970-e9df3c8b1eae","trusted":false,"collapsed":false}},{"outputs":[],"source":"validate(regressors[\"gtr\"], gtb_imp)","cell_type":"code","execution_count":null,"metadata":{"_uuid":"b9a8aea5fd04d1feaf7cbb670b68f5dfbd59ff7a","_execution_state":"idle","_cell_guid":"829658d6-b2d1-45e8-96e0-f4212f196032","trusted":false,"collapsed":false}},{"outputs":[],"source":"","cell_type":"code","execution_count":null,"metadata":{"_uuid":"d8129049ae720f5a4b1a159badcdc0ed06e29e1d","_execution_state":"idle","_cell_guid":"87f5e58c-fc85-4267-8cb3-4b1e5bd8bdcc","trusted":false,"collapsed":false}}],"nbformat":4,"nbformat_minor":0,"metadata":{"_change_revision":0,"language_info":{"file_extension":".py","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","name":"python"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"_is_fork":false}}