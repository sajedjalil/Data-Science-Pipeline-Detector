{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c3eab30-b1b1-3b6f-658d-568fe5d5c062"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.model_selection import cross_val_score, train_test_split\n%matplotlib inline\nimport xgboost as xgb\nimport seaborn as sns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8702b68f-d722-5672-29bd-100fa384fc2a"},"outputs":[],"source":"total = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nmacro = pd.read_csv('../input/macro.csv')\n\ndf_total = pd.merge(total, macro, on='timestamp', how='left')\ndf_total.drop('id', axis = 1, inplace = True)\ndf_total['price_doc'] = np.log1p(df_total['price_doc'])\n\ndf_test = pd.merge(test, macro, on='timestamp', how='left')\ndf_test.drop('id', axis = 1, inplace = True)\ndf_all = pd.concat([df_total,df_test], keys = ['total','test'])\n\nprint ('total: ', total.shape)\n# print total.head()\nprint ('test: ', test.shape)\n# print test.head()\nprint ('macro: ', macro.shape)\n# print macro.head()\nprint ('all: ', df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94e2e5ab-0bce-0170-dbac-0cf366ee3fb8"},"outputs":[],"source":"def missingPattern(df):\n    numGroup = list(df._get_numeric_data().columns)\n    catGroup = list(set(df.columns) - set(numGroup))\n    print('Total categorical/numerical variables are %s/%s' % (len(catGroup), len(numGroup)))\n    \n    #missing data\n    n = df.shape[0]\n    count = df.isnull().sum()\n    percent = 1.0 * count / n\n    dtype = df.dtypes\n    # correlation\n    missing_data = pd.concat([count, percent,dtype], axis=1, keys=['Count', 'Percent', 'Type'])\n    missing_data.sort_values('Count', ascending = False, inplace = True)\n    missing_data = missing_data[missing_data['Count'] > 0]\n    print ('Total missing columns is %s' % len(missing_data))\n\n    return numGroup, catGroup, missing_data\n\nnumGroup, catGroup, missing_data = missingPattern(df_all)\nmissing_data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d980b5e-4274-341d-1c16-091cbebff727"},"outputs":[],"source":"import operator\ndef getCorr(df, numGroup, eps, *verbose):\n    corr = df[numGroup].corr()\n#     plt.figure(figsize=(8, 6))\n#     plt.pcolor(corr, cmap=plt.cm.Blues)\n#     plt.show()\n    corr.sort_values([\"price_doc\"], ascending = False, inplace = True)\n    highCorrList = list(corr.price_doc[abs(corr.price_doc)>eps].index)\n    if verbose:\n        print (\"Find most important features relative to target\")\n        print (corr.price_doc[abs(corr.price_doc)>eps])\n    return corr, highCorrList\ncorr, highCorrList = getCorr(df_all.ix['total',:], numGroup, 0.4, True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3f601cf-b561-4e3a-6a7f-5b08cdef69da"},"outputs":[],"source":"# for numerical variable, draw scatter plot(x vs y) and histogram plot(total vs test)    \ndef scatterplotNum(df, varNum, ax):\n    plt.scatter(df[varNum], df['price_doc'])\n    plt.xlabel(varNum)\n    plt.ylabel('Price_doc')\n\ndef hishplotNum(df, varNum, ax):\n    plt.hist(df.ix['total',varNum], bins = 50, alpha = 0.4)\n    plt.hist(df.ix['test',varNum], bins = 50, color = 'r', alpha = 0.4)\n    plt.xlabel(varNum)\n    plt.ylabel('Frequency')\n    plt.legend(('total','test'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"995eeecb-9773-ad56-1fe2-43fd1ce369aa"},"outputs":[],"source":"high_missing_data = missing_data[missing_data['Percent'] > 0.5]\nprint (high_missing_data.index)\nXYcorr = corr['price_doc'].to_dict()\n\nfor i in XYcorr:\n    if i != 'price_doc' and XYcorr[i] > -1 and i in high_missing_data.index:\n        fig = plt.figure(i)\n        ax1 = fig.add_subplot(1,1,1)\n        scatterplotNum(df_all.ix['total'], i, ax1)\n        plt.title('correlation is %.4f' %(XYcorr[i]))\n\n#         ax2 = fig.add_subplot(1,2,2)\n#         hishplotNum(pd.concat([total,test],keys = ['total','test']), i, ax2)\n#         plt.title('correlation is %.4f' %(XYcorr[i]))\n        plt.gcf().set_size_inches(6, 4)\n        plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a88d23b-55d2-d31c-4ef3-359cda3489ee"},"outputs":[],"source":"# remove the heavy missing features\nfor i in high_missing_data.index:\n    df_all.drop(i, axis = 1, inplace = True)\n\nprint('all: ', df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6cd46cd-960e-c0c2-72e7-2127b3432a16"},"outputs":[],"source":"# total missing\n# macro missing\nbasic_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(total.columns))\nmacro_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(macro.columns))\nprint('missing in basic: ', len(basic_missing))\nprint('missing in macro: ', len(macro_missing))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db71d342-a989-e933-6dd6-eb6b4ea78389"},"outputs":[],"source":"### for macro info, look at the missing value info(mean, std) groupby yr, year_month\ndf_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\ndf_all['year'] = df_all.timestamp.dt.year\ndf_all['year_month'] = df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"634c299b-701a-8048-d4a0-d0c5008f9cc3"},"outputs":[],"source":"# life_sq and full_sq are highly related to price_doc\n# life_sq <= full_sq and full_sq has no missing value\n\n# life_sq or full_sq <= 5\ndf_all['life_sq'][df_all['life_sq']<=5] = df_all['full_sq'][df_all['life_sq']<=5]\ndf_all['full_sq'][df_all['full_sq']<=5] = df_all['life_sq'][df_all['full_sq']<=5]\n\n\n# # life_sq or full_sq > 200 \ndf_all['life_sq'].ix['total'][1084] = 28.1\ndf_all['life_sq'].ix['total'][4385] = 42.6\ndf_all['life_sq'].ix['total'][9237] = 30.1\ndf_all['life_sq'].ix['total'][9256] = 45.8\ndf_all['life_sq'].ix['total'][9646] = 80.2\ndf_all['life_sq'].ix['total'][13546] = 74.78\ndf_all['life_sq'].ix['total'][13629] = 25.9\ndf_all['life_sq'].ix['total'][21080] = 34.9\ndf_all['life_sq'].ix['total'][26342] = 43.5\n\ndf_all['life_sq'].ix['test'][601] = 74.2\ndf_all['life_sq'].ix['test'][1896] = 36.1\ndf_all['life_sq'].ix['test'][2031] = 23.7\ndf_all['life_sq'].ix['test'][2791] = 86.9\ndf_all['life_sq'].ix['test'][5187] = 28.3\n\ndf_all['full_sq'].ix['total'][1478] = 35.3\ndf_all['full_sq'].ix['total'][1610] = 39.4\ndf_all['full_sq'].ix['total'][2425] = 41.2\ndf_all['full_sq'].ix['total'][2780] = 72.9\ndf_all['full_sq'].ix['total'][3527] = 53.3\ndf_all['full_sq'].ix['total'][5944] = 63.4\ndf_all['full_sq'].ix['total'][7207] = 46.1\n\n\n# life_sq > full_sq\ndf_all['life_sq'][df_all.life_sq > df_all.full_sq] = df_all['full_sq'][df_all.life_sq > df_all.full_sq]\n\n# kitch_sq > full_sq\n\ndf_all['kitch_sq'][df_all.kitch_sq > df_all.full_sq] = \\\n            df_all['full_sq'][df_all.kitch_sq > df_all.full_sq] - df_all['life_sq'][df_all.kitch_sq > df_all.full_sq]\n\n\n# else\n# floor > max_floor\ndf_all['max_floor'][df_all.floor > df_all.max_floor] = \\\n        df_all['floor'][df_all.floor > df_all.max_floor] + df_all['max_floor'][df_all.floor > df_all.max_floor]\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae194c79-52e3-4e8f-7ce7-6687fc98a475"},"outputs":[],"source":"# fill the missing value in train and test\ndef basicmissingFill(df):\n    # num variables\n    # pre-processing\n    n = df.shape[0]\n    \n    \n    df_all['life_sq'][df_all.life_sq.isnull()] = df_all['full_sq'][df_all.life_sq.isnull()]\n\n\n    df['state'] = df['state'].replace({33:3})\n    df['build_year'][df['build_year'] == 20052009] = 2005\n    df['build_year'][df['build_year'] == 4965] = float('nan')\n    df['build_year'][df['build_year'] == 0] = float('nan')\n    df['build_year'][df['build_year'] == 1] = float('nan')\n    df['build_year'][df['build_year'] == 3] = float('nan')\n    df['build_year'][df['build_year'] == 71] = float('nan')\n    df['build_year'][df['build_year'] == 20] = 2000\n    df['build_year'][df['build_year'] == 215] = 2015\n    df['build_year'].ix['total'][13117] = 1970\n\n\n    \n    # zero-filling count feature \n    zero_fil = ['build_count_brick','build_count_block','build_count_mix','build_count_before_1920',\\\n               'build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995',\\\n               'build_count_monolith','build_count_slag','build_count_wood','build_count_panel','build_count_frame',\\\n               'build_count_foam','preschool_quota']\n    for i in zero_fil:\n        df[i] = df[i].fillna(0)\n    \n    # mode-filling: count feature and ID\n    mode_fil = ['state','ID_railroad_station_walk','build_year','material','num_room']\n    for i in mode_fil:\n        df[i] = df[i].fillna(df[i].mode()[0]) \n\n    # mean-filling\n    mean_fil = ['cafe_avg_price_500','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000',\\\n               'cafe_avg_price_3000','cafe_avg_price_5000','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg',\\\n               'cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg','cafe_sum_1500_max_price_avg',\\\n               'cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg',\\\n               'cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_5000_max_price_avg',\\\n               'cafe_sum_5000_min_price_avg','railroad_station_walk_min','railroad_station_walk_km',\\\n               'school_quota','raion_build_count_with_material_info','prom_part_5000',\\\n               'raion_build_count_with_builddate_info','green_part_2000','metro_km_walk','metro_min_walk',\\\n               'hospital_beds_raion']\n    for i in mean_fil:\n        grouped = df[['year',i]].groupby('year')\n        df[i] = grouped.transform(lambda x: x.fillna(x.mean()))\n        \n    # exception: 'kitch_sq','floor','max_floor'\n    df['kitch_sq'][df.kitch_sq.isnull()] = df['full_sq'][df.kitch_sq.isnull()] - df['life_sq'][df.kitch_sq.isnull()]\n    df['floor'] = df['floor'].fillna(df['floor'].mean())\n    df['max_floor'][df.max_floor.isnull()] = df['floor'][df.max_floor.isnull()]\n    \n    #================\n    # Cat. variables\n    df['product_type'] = df['product_type'].fillna(df['product_type'].mode()[0])\n    \n    return df\n\ndf_all = basicmissingFill(df_all)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dac84269-50f5-4a55-9c19-cce9ca33823e"},"outputs":[],"source":"print('basic_missing filling finished: ', df_all[basic_missing].isnull().sum().sum() == 7662)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"addb7c67-0c54-5004-251a-a1351edaaa8c"},"outputs":[],"source":"# for Cat features in macro_missing\nmacro_missing_obj = []\nfor i in macro_missing:\n    if df_all[i].dtype == object:\n        grouped = df_all[['year',i]].groupby(['year',i])\n        print (grouped.agg(len))\n        macro_missing_obj.append(i)\n        print (missing_data.ix[i])\n        print ('\\n')\n# consider to drop macro_missing_obj\nfor i in macro_missing_obj:\n    df_all.drop(i, axis = 1, inplace = True)\n    macro_missing.remove(i)\n\nprint('macro missing features count: ', len(macro_missing) )\nprint ('df_all shape: ', df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d7312ed-5117-c9ff-9818-a52bc49517a6"},"outputs":[],"source":"# for num features in macro_missing\n# filling strategy: for each feature->if 2015 is not null: fillna the mean(2015) else: fillna the mean(2014)\ndef macromissingFill(df):\n    for i in macro_missing:\n        fill2014 = np.nanmean(df[i][df['year']==2014])\n        fill2015 = np.nanmean(df[i][df['year']==2015])\n        # income_per_cap: the only macro_missing feature which is not agg by year\n        if ~np.isnan(fill2015):\n            df[i] = df[i].fillna(fill2015)\n        else:\n            df[i] = df[i].fillna(fill2014)\n\n    return df\n\ndf_all = macromissingFill(df_all)\nprint ('macro_missing filling finished: ', df_all[macro_missing].isnull().sum().sum() == 0)\n\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e945eec-80f0-dbac-797c-aa39f4e4be30"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb8c7e8d-e37a-3bc0-8032-fee3945524b7"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad55a930-a574-3ed0-7f4c-92f023e28f67"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e8f315c-5eda-2cdc-8283-67e1c310f7da","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af451fd1-0bfb-0f2e-a2a1-2ecaeed2fde6"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91c401e8-320d-0dbd-9ad0-8aa930d3f9f2"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98ad4274-580c-08a5-d9f5-a4744588a2f6"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6733dd8f-7e80-6cba-b4f8-3ec3ed3c8910"},"outputs":[],"source":"\nYtotal = df_total_num['price_doc'].as_matrix()\ntestId = list(df_test_num['id'])\ndf_total_num.drop('price_doc', axis = 1, inplace = True)\ndf_test_num.drop('id', axis = 1, inplace = True)\nprint 'Current training numerical variables count is %d '  %(df_total_num.shape[1])\nprint 'Current training categorical variables count is %d '  %(df_total_cat.shape[1])\nprint 'Current test numerical variables count is %d '  %(df_test_num.shape[1])\nprint 'Current test categorical variables count is %d '  %(df_test_cat.shape[1])\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"35cd478c-f7dc-51f7-a580-781da318beaf"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4a636d5-6c73-642c-eb1a-d4e29f7477d4"},"outputs":[],"source":"# numerical variables mean-variance normalization\n# merge numerical & categorical variables\nfrom sklearn import preprocessing\n\n# Xtotal = np.hstack((df_total_num.as_matrix(),df_total_cat.as_matrix()))\n# Xtest = np.hstack((df_test_num.as_matrix(),df_test_cat.as_matrix()))\n# Xtotal = df_total_num.as_matrix()\n# Xtest = df_test_num.as_matrix()\n\n\n# scaler = preprocessing.StandardScaler().fit(df_total_num)\n# # Xtotal = np.hstack((scaler.transform(df_total_num.as_matrix()),df_total_cat.as_matrix()))\n# # Xtest = np.hstack((scaler.transform(df_test_num.as_matrix()),df_test_cat.as_matrix()))\n# Xtotal = scaler.transform(df_total_num.as_matrix())\n# Xtest = scaler.transform(df_test_num.as_matrix())\n\nXtr, Xval, Ytr, Yval = train_test_split(Xtotal, Ytotal, test_size = 0.25, random_state = 200)\nprint \"Xtr : \" , Xtr.shape\nprint \"Xval : \" , Xval.shape\nprint \"Ytr : \" , Ytr.shape\nprint \"Yval : \" , Yval.shape\nprint \"Xtest : \", Xtest.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"201ce529-5d51-78ca-795b-43e33f63f21d"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"023c809d-389b-1e9c-5418-4f0432ce6b3e","collapsed":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12ea9c3c-acfb-0e28-5716-9a900bbb833c","collapsed":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"f8594316-c95d-3382-bfbf-dd3809fadbc1"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51a91556-bdc5-0773-7394-a7551836cf98","collapsed":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"98c00807-de04-fa08-90c3-1e96601dcb6a"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"100f3e27-a35f-dd63-1de2-1bdfca8d71cd","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}