{"metadata":{"language_info":{"name":"python","file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1","mimetype":"text/x-python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"e618a4f9493fa933f1f80630a6ec4df445c6edaf"},"execution_count":null,"outputs":[],"cell_type":"markdown","source":" Our solution was a stack of Neural-Nets, RandomForest and Linear Models such us:\n RidgeRegression, HuberRegression, RansacRegression. Finally, we used XGB as meta-model.\nWe did extensive feature engineering similar to some solutions already presented. The most \"original\" code that we created was a sample_weight function that penalized samples with more missing values. This function penalizes more the samples that miss the features directly related with the house, such us: full_sq, life_sq, floor, etc.\nThis sample_weight was used in the RandomForest model.\n   \n"},{"metadata":{"_cell_guid":"88df7bd9-96cb-45e3-8146-606ebd0097e7","_execution_state":"idle","_uuid":"a2440d7b1e1e2820c0d256df5ce4bca4b83a50eb","trusted":false},"execution_count":null,"cell_type":"code","outputs":[],"source":"import numpy as np \nimport pandas as pd\n\ntrain = pd.read_csv('../input/train.csv')\n\nnulls = train.isnull().sum()\n\nimportance = train.shape[0]*[None]\nfor i in range(train.shape[0]):\n    nulls = train.loc(i)[i].isnull().sum()\n    imp_nulls = train.loc(i)[i].isnull()[: 12].sum()\n    importance[i] = np.exp((1/(nulls+1)*(1/(imp_nulls+1))))/np.exp(1)\n\n#imp_nulls are the nulls directly related to the house\n#We decide to use the sqrt to reduce the amplitude. The model performed better in LB and local CV\nimportance = np.sqrt(importance)\n"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"d0ee613e7ce99020cd7796fff60698ffe23478cc"},"execution_count":null,"outputs":[],"cell_type":"markdown","source":"Before we fit the Linear Models with the train data, we removed some outliers based on price per square meter. \n"},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"066e2cb515257128391cdbc0fcbe77b87d13b201"},"execution_count":null,"outputs":[],"cell_type":"code","source":"train['price_per_sq'] = np.array(train.price_doc)/np.array(train.full_sq)\ntrain = train[(train.price_per_sq > 60000) & (train.price_per_sq < 500000)]\n#The values 60000 and 500000 were defined based on price_per_sq histogram."},{"metadata":{"_execution_state":"idle","collapsed":false,"_uuid":"1c81c76b5c983a910a71953fa8d3dd2aef7adf9b"},"execution_count":null,"outputs":[],"cell_type":"markdown","source":"I think this two small ideas were the most interesting part of our solution.\nHope you find it useful."}],"nbformat_minor":0,"nbformat":4}