{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ac06cd79-9189-6d2b-9a83-012d19327beb"},"source":"I decided to study the effects of different 35/65% random splits on hold out data and determined the differences were potentially significant.  To demo this, I forked Reynaldo's kernel..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d7179ccb-7e6d-7bec-46fe-80394a3ca067"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn import model_selection, preprocessing, metrics\nimport xgboost as xgb\nimport datetime\n#now = datetime.datetime.now()\n\notrain = pd.read_csv('../input/train.csv')\n\ntrain = otrain.iloc[0:24000].copy()\ntest = otrain.iloc[24000:].copy()\n\nid_test = test.id\n\n\ny_train = train[\"price_doc\"]\nx_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\nx_test = test.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n\nlbl1 = {}\nlbl2 = {}\n\nfor c in x_train.columns:\n    if x_train[c].dtype == 'object':\n        lbl1[c] = preprocessing.LabelEncoder()\n        lbl1[c].fit(list(x_train[c].values)) \n        x_train[c] = lbl1[c].transform(list(x_train[c].values))\n        #x_train.drop(c,axis=1,inplace=True)\n        \nfor c in x_test.columns:\n    if x_test[c].dtype == 'object':\n        lbl2[c] = preprocessing.LabelEncoder()\n        lbl2[c].fit(list(x_test[c].values)) \n        x_test[c] = lbl2[c].transform(list(x_test[c].values))\n        #x_test.drop(c,axis=1,inplace=True)        \n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\ncv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=True)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04d55ef1-8010-57aa-43d0-4015eed54cf3"},"outputs":[],"source":"num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)\n\ny_predict = model.predict(dtest)\noutput = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n#output.head()\n\ndef rmsle(tgt, preds):\n    return np.sqrt(sklearn.metrics.mean_squared_log_error(tgt, preds))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d8755e94-a689-ba27-a0ed-7e681f27114e"},"outputs":[],"source":"# oops, but i don't feel like rerunning that code ;)\nimport sklearn.metrics\n\n# now split the held-back predicted output and run several different 35/65% splits on it...\n\nsp = int(len(test) * .35)\n\nfor seeds in [0, 1337, 31337, 71331, 12345, 54321]:\n    ind = output.index.copy()\n    np.random.seed(seeds)\n    ind = np.random.permutation(ind)\n    \n    tmp = test[['price_doc', 'id']].copy()\n    tmp['preds'] = output.price_doc\n    \n    tmp = tmp.loc[ind]\n    \n    print(rmsle(tmp.iloc[0:sp].price_doc, tmp.iloc[0:sp].preds), rmsle(tmp.iloc[sp:].price_doc, tmp.iloc[sp:].preds))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c101a5c3-020f-9f84-f4a9-1624031f1f11"},"outputs":[],"source":"# let's also take the entire HB area for comparison...\nprint(rmsle(tmp.price_doc, tmp.preds))"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c7f6ad1-a052-8338-a248-ab1f11bbbfde"},"source":"As you can see, the 35/65 scores are rarely close.  It is very likely (but NOT certain) that the LB shakeup will be significant at the end!\n\nA run on a home system running the same docker container, given xgb seed of 12345 gave me these results:\n\n    0.397478374522 0.420745197489\n    0.41342785749 0.412390991909\n    0.399848227388 0.419535052541\n    0.424963481769 0.406031620705\n    0.406878538719 0.415881614054\n    0.408207325049 0.41518027333\n    0.412754054895\n\nThis indicates that there is a ~25% chance a model with slightly better HB score (i.e. CV) will perform worse on the private leaderboard."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7bafeff8-570d-300c-0273-a667342206c0"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}