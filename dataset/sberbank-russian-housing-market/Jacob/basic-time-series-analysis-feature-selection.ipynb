{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b808f82a-c85d-3245-cf3f-f2eb0097ef74"},"source":"# Basic Time Series Analysis & Feature Selection"},{"cell_type":"markdown","metadata":{"_cell_guid":"426783f2-6c04-f06c-0ede-5f8ca2d0dbd8"},"source":"sentences starting with a dot (as below) is my personal opinion. \n\n* have fun!`enter code here`"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3466e8e0-83f2-1480-a1aa-38535d41b3ca"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport datetime\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing\ncolor = sns.color_palette()\n%matplotlib inline\n# result transform\ndef num2log(arr):\n    return np.log(arr + 1)\n# result retransform\ndef log2num(arr):\n    return np.exp(arr) - 1\n# treat date\ndef trans_date(arr): \n    return datetime.datetime.strptime(arr, '%Y-%m-%d').date()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3c498425-1cdc-01c6-cdbf-338d51880913"},"source":"## Input data & have a quick look"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b212c75-e2d2-4e5a-9d4b-b41590955dbf"},"outputs":[],"source":"train_df = pd.read_csv('../input/train.csv')\nprint('In trainset, there are {} rows and {} columns'.format(train_df.shape[0],train_df.shape[1]))\ntrain_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a0fcbd4-3ea1-d9e5-8e8d-5f79dd4976e1"},"outputs":[],"source":"print('check if id is unique: ' + str(len(pd.unique(train_df.id)) == train_df.shape[0]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"8fa81415-7bec-e2be-f079-d6c1b41c6104"},"source":"### have a look at the distribution of log price"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b74a786b-8e42-880d-1ff1-cca836d4f49c"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nsns.distplot(num2log(train_df.price_doc.values), bins=100, kde=False)\nplt.xlabel('price', fontsize=14)\nplt.title('Distribution of log price_doc', fontsize = 18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"94132ad5-d139-2ae0-b871-26505b68d904"},"source":"* not bad"},{"cell_type":"markdown","metadata":{"_cell_guid":"129b126f-e79a-7f00-696a-c3d63eafbaca"},"source":"## Time Analysis\nderive column `month#` and `month` to see the trend and periodic charateristics"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7c77719-573d-fe73-62bd-d1cda17ae0b4"},"outputs":[],"source":"train_df['date'] = train_df['timestamp'].apply(trans_date)\ntrain_df['year'] = train_df.date.apply(lambda x: x.year)\ntrain_df['month_num'] = train_df.date.apply(lambda x: x.month + 12*(x.year-2011))\ntrain_df['month'] = train_df.date.apply(lambda x: x.month)\nstart_date = train_df.date.min()\ntrain_df['diff_date'] = train_df['date'].apply(lambda x: (x - start_date).days)\ntrain_df['log_price'] = num2log(train_df.price_doc.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eecfd3e6-88c9-2b89-a5c0-9a01ec431cc4"},"outputs":[],"source":"month_num_count = train_df.groupby('month_num').count().reset_index()\nmonth_num_count = train_df.groupby('month_num').size().reset_index()\nmonth_num_count.columns = ['month_num', 'Count']\nplt.figure(figsize=(12,8))\nsns.boxplot(x = 'month_num', y = 'log_price', data = train_df)\nplt.ylabel('Log Price', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Boxplot of log_price on Month # from 2011-01', fontsize = 18)\nplt.show()\nplt.figure(figsize=(12,8))\nsns.distplot(month_num_count.Count.values, bins=50, kde=False)\nplt.xlabel('count of samples per month_num', fontsize=14)\nplt.title('Distribution of count', fontsize = 18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa4bfd94-cf06-31a2-2f23-124c82f72892"},"source":"* there is no obvious increasing or decreasing trend.\n\nthe second plot is to show the confidence of the conclusion based on the count of samples."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d82b2b71-5f4b-6e56-73eb-dc35dd597d82"},"outputs":[],"source":"plt.figure(figsize=(12,8))\nsns.boxplot(x = 'month', y = 'log_price', data = train_df)\nplt.ylabel('Log Price', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Boxplot of log_price on Month', fontsize = 18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"12250fac-64bc-163c-5e37-6104e4088b85"},"source":"* there is no obvious seasonal periodic characteristics"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b6189ad-bde0-06fa-276f-8f2ec182df5f"},"outputs":[],"source":"date_count = train_df.groupby('date').count().reset_index()\ndate_count = train_df.groupby('date').size().reset_index()\ndate_count.columns = ['date', 'Count']\nplt.figure(figsize=(12,8))\nsns.boxplot(x = 'date', y = 'log_price', data = train_df)\nplt.ylabel('Log Price', fontsize=12)\nplt.xlabel('Date', fontsize=12)\nplt.title('Boxplot of log_price on Date', fontsize = 18)\nplt.show()\nplt.figure(figsize=(12,8))\nsns.distplot(date_count.Count.values, bins=100, kde=False)\nplt.xlabel('count of samples per date', fontsize=14)\nplt.title('Distribution of count', fontsize = 18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ca4b81db-40ff-4d19-eeb0-c357177cb27a"},"source":"I will not make any conclusion due to the low counts for most dates"},{"cell_type":"markdown","metadata":{"_cell_guid":"446bbccc-99ac-5c03-a0ef-6905dbddb9a2"},"source":"## Missing values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"474e916c-ae1a-b5c4-e156-1e3ece750f9a"},"outputs":[],"source":"missing = train_df.isnull().sum(0).reset_index()\nmissing.columns = ['column', 'count']\nmissing = missing.sort_values(by = 'count', ascending = False).loc[missing['count'] > 0]\nmissing['percentage'] = missing['count'] / float(train_df.shape[0]) * 100\nind = np.arange(missing.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,18))\nrects = ax.barh(ind, missing.percentage.values, color='r')\nax.set_yticks(ind)\nax.set_yticklabels(missing.column.values, rotation='horizontal')\nax.set_xlabel(\"Precentage of missing values %\", fontsize = 14)\nax.set_title(\"Number of missing values in each column\", fontsize = 18)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"972504ad-c3b7-521a-e0b1-8d258cd4e01e"},"source":"* delete columns with more than 20% missing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3b74477-a43a-32f8-c353-899d27d1c2c3"},"outputs":[],"source":"delete_col = missing.loc[missing['percentage'] >= 20].column.values\ntrain_set = train_df.drop(delete_col, axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6e1f22f9-027e-240b-14d3-b335e63ce15d"},"source":"## Feature selection by correlation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6555c64c-aead-d5e4-8d07-363f555ad2e0"},"outputs":[],"source":"for f in train_set.columns:\n    if train_set[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_set[f].values)) \n        train_set[f] = lbl.transform(list(train_set[f].values))\ndtype_df = train_set.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"192df624-408a-08d4-19b2-9e27497da94b"},"outputs":[],"source":"corrmat = train_set.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1).corr(method='pearson', min_periods=1000)\n# plot absolute values\ncorrmat = np.abs(corrmat)\nsns.set(context=\"paper\", font=\"monospace\")\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True, xticklabels = False, yticklabels = False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e47988a-d67a-03f7-5f91-649b565a3eb3"},"source":"* OMG, how beautiful is it!!\n* Calm down!\n* It shows that there are many features has high correlations, maybe we can reduce them."},{"cell_type":"markdown","metadata":{"_cell_guid":"4e9b1159-7180-3507-591d-7fbcb5be918d"},"source":"### Correlation between target and log_price"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b1420e7-86e0-7488-be18-f33bdd6c517b"},"outputs":[],"source":"remain_num = 50\ncorr_target = corrmat['log_price'].reset_index()[:-2]\ncorr_target.columns = ['feature','abs_corr']\ncorr_target = corr_target.sort_values(by = 'abs_corr', ascending = True)[:remain_num].loc[corr_target['abs_corr'] >0.01]\nind = np.arange(corr_target.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,18))\nrects = ax.barh(ind, corr_target.abs_corr.values, color='r')\nax.set_yticks(ind)\nax.set_yticklabels(corr_target.feature.values, rotation='horizontal')\nax.set_xlabel(\"absolute corr\", fontsize = 14)\nax.set_title(\"Correlations between features and log_price \", fontsize = 18)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c44b8ca7-a59e-5cd5-fe62-d29c9a2fd9a5"},"outputs":[],"source":"corr_target_f = list(corr_target.feature.values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"2e14ce94-8494-ac69-212d-33f3839d8cb3"},"source":"### correlation between features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1975a92-5298-0733-2a2a-73079f3bbf2e"},"outputs":[],"source":"corr_target_f2 = corr_target_f\ncorr_target_f2.append('log_price')\nhigh_corr = train_set[corr_target_f2].corr(method='pearson', min_periods=1000)\nhigh_corr = np.abs(high_corr)*100\nf, ax = plt.subplots(figsize=(11, 11))\nsns.heatmap(high_corr, cbar=False, annot=True, square=True, fmt='.0f', \n            annot_kws={'size': 8})\nplt.title('High-corrlation Features')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"efceb227-7ff2-a990-64a8-05df385047b1"},"source":"* delete duplicated features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"239cd791-845e-341f-f5ba-3d8433b75f7d"},"outputs":[],"source":"remove_list = ['cafe_sum_5000_max_price_avg','16_29_male', 'female_f','full_all', 'male_f']\ncorr_target_f = [x for x in corr_target_f if x not in remove_list]"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d244392-a993-d892-9b6a-e3d2c25109d9"},"source":"## Feature selection by xgb"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1562298a-1134-0bec-c264-50ffdaa2a932"},"outputs":[],"source":"train_y = train_set.log_price.values\ntrain_X = train_set.drop([\"id\", \"timestamp\", \"price_doc\", \"log_price\"], axis=1)\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 10,\n    'subsample': 1.0,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)\nremain_num = 99"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da926b65-4827-3d98-7109-324308675e5e"},"outputs":[],"source":"fig, ax = plt.subplots(figsize=(10,18))\nxgb.plot_importance(model, max_num_features=remain_num, height=0.8, ax=ax)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"34b9f92c-2db3-f5b7-e081-17785c08b757"},"source":"* Even there is no obvious charateristics on time, derived columns`month` and `date` still have relatively high importance, that's interesting."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03b875b1-a61f-5ff7-e388-7fdd649f6eb7"},"outputs":[],"source":"importance = model.get_score(importance_type='weight')\ntuples = sorted([(k, importance[k]) for k in importance], key=lambda x: x[1], reverse=True)[:remain_num]\nxgb_imp_f = [x[0] for x in tuples]"},{"cell_type":"markdown","metadata":{"_cell_guid":"e08683f8-0dbe-b118-0f45-68e52ba0d05a"},"source":"## Summary above"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75f4e711-4e6f-cbf2-2e42-fadacc62606f"},"outputs":[],"source":"total_f = list(set(xgb_imp_f) | set(corr_target_f))\ncommon_f = list(set(xgb_imp_f) & set(corr_target_f))\nxgb_top_f = xgb_imp_f[:10]\ncorr_top_f = corr_target_f[:10]\nprint('there are {} features chosen in total'.format(len(total_f)))\nprint('there are {} features chosen in common'.format(len(common_f)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"e8c1715c-77e0-f48c-a540-8fba95249665"},"source":"## Simple test by total chosen features"},{"cell_type":"markdown","metadata":{"_cell_guid":"54955af1-9b56-f3c3-a3e0-37b2ee76cdba"},"source":"### normalize and impute missing values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9199f49a-db5e-1b8d-a988-ea467ac11993"},"outputs":[],"source":"train_col = total_f\nif 'log_price' not in train_col:\n    train_col.append('log_price')\ntest_col = [x for x in train_col if x != 'log_price']\nmissing_imputer = preprocessing.Imputer(missing_values='NaN', strategy='most_frequent', axis=0, verbose=0, copy=True)\nmissing_imputer.fit(train_set[test_col])\ntrain_data = pd.DataFrame(missing_imputer.transform(train_set[test_col]),columns = test_col)\ntrain_data = pd.DataFrame(preprocessing.normalize(train_data, norm='l2', axis=0, \n                                                  copy=True, return_norm=False),columns = test_col)\ntrain_data['log_price'] = train_set['log_price']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5395e67-135b-842d-4f90-55627b85c816"},"outputs":[],"source":"kf = model_selection.KFold(3,shuffle =True)\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 1.0,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\ndef cv_xgb(val_train_X,val_train_Y,val_val_X,val_val_Y):\n    dtrain = xgb.DMatrix(val_train_X, val_train_Y, feature_names=val_train_X.columns)\n    dval = xgb.DMatrix(val_val_X, val_val_Y, feature_names=val_val_X.columns)\n    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n                           early_stopping_rounds=50, verbose_eval=20)\n    num_boost_round = partial_model.best_iteration\n    return(num_boost_round,partial_model.best_score)\nfor val_train, val_val in kf.split(train_data):\n    val_train_X = train_data.ix[val_train].drop('log_price',axis=1)\n    val_train_Y = train_data.ix[val_train].log_price\n    val_val_X = train_data.ix[val_val].drop('log_price',axis=1)\n    val_val_Y = train_data.ix[val_val].log_price\n    print(\"%s %s %s %s\" % (val_train_X.shape, val_train_Y.shape, val_train.shape, val_val.shape))\n    print(cv_xgb(val_train_X,val_train_Y,val_val_X,val_val_Y))"},{"cell_type":"markdown","metadata":{"_cell_guid":"c10fad5a-bdac-5d2d-3219-e633ada7198e"},"source":"* Actually, it's not as good as what I expected, but not bad, we can play around a lot, to get better result."},{"cell_type":"markdown","metadata":{"_cell_guid":"aa0cbf82-147c-af4c-dae1-a8ee1ff92f75"},"source":"### let's look at the top xgb_chosen features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99df0327-0a82-a0b4-77c6-719990e84b86"},"outputs":[],"source":"for f in xgb_top_f:\n    print(f)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"419459ad-bb53-e953-08c8-8d7cb6b1613a"},"outputs":[],"source":"ulimit = np.percentile(train_set.log_price.values, 99.5)\nllimit = np.percentile(train_set.log_price.values, 0.5)\ntrain_set['log_price'].loc[train_set['log_price']>ulimit] = ulimit\ntrain_set['log_price'].loc[train_set['log_price']<llimit] = llimit\nfor f in [x for x in xgb_top_f if x not in ['date', 'month']]:\n    ulimit = np.percentile(train_set[f].values, 99.5)\n    llimit = np.percentile(train_set[f].values, 0.5)\n    train_set[f].loc[train_set[f]>ulimit] = ulimit\n    train_set[f].loc[train_set[f]<llimit] = llimit\n    plt.figure(figsize=(12,12))\n    sns.jointplot(y=train_set[f].values, x=train_set.log_price.values, size=10)\n    plt.xlabel('Log of Price', fontsize=12)\n    plt.ylabel(f, fontsize=12)\n    plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f68f610-0f4c-b8b9-86ba-3fdc6b830909"},"source":"* Some plots are very interesting! we may get some good predictors after some transformation."},{"cell_type":"markdown","metadata":{"_cell_guid":"9a97499c-7fa8-c182-5156-d9f68fdf93a8"},"source":"### All data preparation process on test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5667336-b9cf-cb52-75c5-3e404a229bb4"},"outputs":[],"source":"test_df = pd.read_csv('../input/test.csv')\ntest_id = test_df['id']\n# derive columns about time\ntest_df['date'] = test_df['timestamp'].apply(trans_date)\ntest_df['year'] = test_df.date.apply(lambda x: x.year)\ntest_df['month_num'] = test_df.date.apply(lambda x: x.month + 12*(x.year-2011))\ntest_df['month'] = test_df.date.apply(lambda x: x.month)\ntest_df['diff_date'] = test_df['date'].apply(lambda x: (x - start_date).days)\n# drop columns with too much NAs\ntest_set = test_df.drop(delete_col, axis=1)\n# transform types\nfor f in test_set.columns:\n    if test_set[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(test_set[f].values)) \n        test_set[f] = lbl.transform(list(test_set[f].values))\n# impute missing values and normalize\ntest_data = pd.DataFrame(missing_imputer.transform(test_set[test_col]),columns = test_col)\ntest_data = pd.DataFrame(preprocessing.normalize(test_data, norm='l2', axis=0, \n                                                  copy=True, return_norm=False),columns = test_col)\n\n\nDtest = xgb.DMatrix(test_data, feature_names=test_col)"},{"cell_type":"markdown","metadata":{"_cell_guid":"efb5bccb-872a-e35a-d93f-a72805d2099e"},"source":"more is coming..."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}