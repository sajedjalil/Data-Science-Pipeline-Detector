{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"8304bf25-cd25-600c-a14f-a5de10b5be4c"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e6290ab-f33f-5065-e26b-a5ffb8d523dd"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import linear_model\nfrom sklearn import neural_network\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import mean_squared_error\n\n\nimport matplotlib.pyplot as plt\n# from sklearn.utils import shuffle\n# from sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nfrom sklearn import preprocessing\n\n\n# Input data files are available in the 'input/' directory.\n\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"3711d1f6-0619-cb03-e389-d653e5295bd0"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc2042cd-ee5c-359a-1848-97f4b5a0a12b"},"outputs":[],"source":"realty_drop_cols = ['male_f', 'female_f', 'young_female', 'work_all', 'work_female', \n                   'railroad_station_walk_min', 'railroad_station_avto_km', 'railroad_station_avto_min',\n                  'sadovoe_km', 'bulvar_ring_km', 'kremlin_km']\nmacro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\", 'cpi', 'brent', \n\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]\n\ndf_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n\ndf_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'], usecols=['timestamp'] + macro_cols)\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60ee019d-fee2-5f69-9a57-dd2087ac336e"},"outputs":[],"source":"# ylog will be log(1+y), as suggested by https://github.com/dmlc/xgboost/issues/446#issuecomment-135555130\nylog_train_all = np.log1p(df_train['price_doc'].values)\nid_test = df_test['id']\n\ndf_train.drop(['id', 'price_doc'], axis=1, inplace=True)\ndf_test.drop(['id'], axis=1, inplace=True)\n\n# Build df_all = (df_train+df_test).join(df_macro)\nnum_train = len(df_train)\ndf_all = pd.concat([df_train, df_test])\ndf_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\ndf_all.drop(realty_drop_cols, axis=1, inplace=True)\n# print(df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69685d58-859f-502e-66fb-4ab8a92150e1"},"outputs":[],"source":"# price_ulimit = np.log1p(1E8)\n# df_all =  df_all.loc[df_all['price_doc'] < price_ulimit,:] \n\nfull_sq_ulimit = 250\nlife_sq_ulimit = 250\nfull_sq_llimit = 10\nlife_sq_llimit = 5\ndf_all.loc[df_all['full_sq']>full_sq_ulimit, 'full_sq'] = np.nan\ndf_all.loc[df_all['full_sq']<full_sq_llimit, 'full_sq'] = np.nan\ndf_all.loc[df_all['life_sq']>life_sq_ulimit, 'life_sq'] = np.nan\ndf_all.loc[df_all['life_sq']<life_sq_llimit, 'life_sq'] = np.nan\n\ndf_all['life_full_ratio'] = df_all['life_sq'] / df_all['full_sq']\n\ndf_all.loc[df_all['life_full_ratio'] > 0.85, 'life_sq'] = np.nan\n\ndf_all.loc[df_all['floor'] == 0, 'floor'] = np.nan\ndf_all.loc[df_all['max_floor'] == 0, 'max_floor'] = np.nan\ndf_all.loc[df_all['max_floor'] < df_all['floor'], ['floor', 'max_floor']] = np.nan\ndf_all['floor_ratio'] = df_all['floor'] / df_all['max_floor']\n\ndf_all.loc[df_all['build_year'] > 2017, 'build_year'] = np.nan\ndf_all.loc[df_all['build_year'] < 1900, 'build_year'] = np.nan\n\n\ndf_all.loc[df_all['num_room'] == 0, 'num_room'] = np.nan\ndf_all.loc[df_all['num_room'] >= 10, 'num_room'] = np.nan\n\ndf_all.loc[df_all['kitch_sq'] <= 3.0 , 'kitch_sq'] = np.nan\ndf_all.loc[df_all['full_sq'] - df_all['kitch_sq'] <= 5.0 , 'kitch_sq'] = np.nan\n\ndf_all.loc[df_all['state'] == 33 , 'state'] = 3"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9030832f-5cb8-5ca1-0997-2a23a0e7847d"},"outputs":[],"source":"# Add month-year\nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf_all['month'] = df_all.timestamp.dt.month\ndf_all['dow'] = df_all.timestamp.dt.dayofweek\n\n# Other feature engineering\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\ndf_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n# Remove timestamp column (may overfit the model in train)\ndf_all.drop(['timestamp'], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab629c24-2da4-b89a-973c-06bfea34acad"},"outputs":[],"source":"for f in df_all.columns:\n    if df_all[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_all[f].values.astype('str')) )\n        df_all[f] = lbl.transform(list(df_all[f].values.astype('str')))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e7d080b9-4967-6074-6a74-67176321ee72"},"outputs":[],"source":"# train_df.fillna(-99, inplace=True)\n# test_df.fillna(-99, inplace=True)\n\n\ndf_all= df_all.fillna(df_all.median())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2b1cf10-7fe8-a898-1e17-0f79a6ae0c6b"},"outputs":[],"source":"# Convert to numpy values\nX_all = df_all.values\nX_all = preprocessing.normalize(X_all, norm='l1', axis=0, copy=True, return_norm=False)\n\n\n# print(X_all.shape)\n\n# Create a validation set, with last 20% of data\nnum_val = int(num_train * 0.2)\n\nX_train_all = X_all[:num_train]\nX_train = X_all[:num_train-num_val]\nX_val = X_all[num_train-num_val:num_train]\nylog_train = ylog_train_all[:-num_val]\nylog_val = ylog_train_all[-num_val:]\n\nX_test = X_all[num_train:]\n\ndf_columns = df_all.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8511102e-693f-ab35-b194-8edc51a4f71a"},"outputs":[],"source":"clf = SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',\n    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\nclf.fit(X_train, ylog_train)\n\ntraining_loss = np.sqrt(mean_squared_error(ylog_train, clf.predict(X_train)))\nvalidation_loss = np.sqrt(mean_squared_error(ylog_val, clf.predict(X_val)))\n\nprint('Training loss is {}'.format(training_loss))\nprint('Validation loss is {}'.format(validation_loss))\n\ntest_y_SVR = np.exp(clf.predict(X_test)) - 1\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': test_y_SVR})\ndf_sub.to_csv('Predict_SVR.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"02dbc44c-49e5-326d-1f1d-18ba9ec599ec"},"outputs":[],"source":"regNN = neural_network.MLPRegressor(hidden_layer_sizes = (100, 100, 100, 100, 100))\nregNN.fit (X_train, ylog_train)\ntraining_loss = np.sqrt(mean_squared_error(ylog_train, regNN.predict(X_train)))\nvalidation_loss = np.sqrt(mean_squared_error(ylog_val, regNN.predict(X_val)))\n\nprint('Training loss is {}'.format(training_loss))\nprint('Validation loss is {}'.format(validation_loss))\n\ntest_y_regNN = np.exp(regNN.predict(X_test)) - 1\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': test_y_regNN})\ndf_sub.to_csv('Predict_train_SVR.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fb8d6e7-3fee-65a4-6a78-a7ba0dc8ce10"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c34ee63-254c-95a6-24b1-4dccab547825"},"outputs":[],"source":"rfr = RandomForestRegressor(n_estimators = 200, max_depth = 40, min_samples_split = 20)\nrfr.fit (X_train, ylog_train)\ntraining_loss = np.sqrt(mean_squared_error(ylog_train, rfr.predict(X_train)))\nvalidation_loss = np.sqrt(mean_squared_error(ylog_val, rfr.predict(X_val)))\n\nprint('Training loss is {}'.format(training_loss))\nprint('Validation loss is {}'.format(validation_loss))\n\n\ntest_y_rfr = np.exp(rfr.predict(X_test)) - 1\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': test_y_rfr})\ndf_sub.to_csv('Predict_train_rfr.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f78446c-d71b-a910-c69b-d07159bcad0d"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f0b8bd6-7023-9d92-1271-1bda3413e68c"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f5bb18d-bb7b-9f00-f9b7-7dc0849c554d"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5115016-f314-aaad-38f2-77431fa06753"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}