{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1511075-2040-1acf-08f8-db4ab6bed4e1"},"source":"A quick extension. It was reported in here on the forums that the first 10k rows in the train set have more missing values(likely some part of the data source missing for these?). Removing these from the comparison might be useful. Thanks to Gerard Toonstra for pointing this out in this comment https://www.kaggle.com/c/sberbank-russian-housing-market/discussion/32312#179239\n\nSecond thing that seems interesting is to see if the first 35% of the test set can be differentiated from the last 65% which correspond to the public and private leaderboard split. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcbf5e1a-6442-47ba-5435-7053db612903"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63a18771-010e-db9f-0292-1b8e1b8787d5"},"outputs":[],"source":"# We start by loading the training / test data and combining them with minimal preprocessing necessary\n# Most of the data preparation is taken from here: \n# https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\nxtrain = pd.read_csv('../input/train.csv')\nid_train = xtrain['id']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a07cccc-0fe8-fa65-a022-7315160a2733"},"outputs":[],"source":"xtrain.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ee8172e-4816-482d-84cc-0d81dff4963c"},"outputs":[],"source":"xtrain['num_room'].isnull().plot()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"574566fd-793b-e5dc-407c-8136160fb0f5"},"outputs":[],"source":"# lazy way\nxtrain.loc[11000:,'num_room'].isnull().value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36a127f8-0a2f-183c-c8dd-74060f2c65a4"},"outputs":[],"source":"xtrain = xtrain.iloc[11000:,:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf0144e8-d3f3-3b80-4127-4de732e368ed"},"outputs":[],"source":"time_train = xtrain['timestamp']\nytrain = xtrain['price_doc']\nxtrain.drop(['id', 'timestamp', 'price_doc'], axis = 1, inplace = True)\nxtrain.fillna(-1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d1733b2-d704-33ae-c3c9-528aa284b474"},"outputs":[],"source":"xtest = pd.read_csv('../input/test.csv')\nid_test = xtest['id']            \ntime_test = xtest['timestamp']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f18df6ff-59e2-2747-c213-844d1d01c1e6"},"outputs":[],"source":"xtest.isnull().sum().sum() # still nulls in test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4a7338a-30db-4ada-de34-babca852e566"},"outputs":[],"source":"#fillna same way as train in the test set\nxtest.fillna(-1, inplace = True)\nxtest.drop(['id', 'timestamp'], axis = 1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f4ffc92-a776-6715-b7f3-925f1547d4fc"},"outputs":[],"source":"# add identifier and combine\nxtrain['istrain'] = 1\nxtest['istrain'] = 0\nxdat = pd.concat([xtrain, xtest], axis = 0)\n\n# convert non-numerical columns to integers\ndf_numeric = xdat.select_dtypes(exclude=['object'])\ndf_obj = xdat.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \nxdat = pd.concat([df_numeric, df_obj], axis=1)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0d09367-f024-ae7e-82ce-e75d53780274"},"source":"Define a split and the model (xgboost, what else :-)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"891a4cdc-d085-b67f-45e7-dcdc3ca0f8b2"},"outputs":[],"source":"skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 44)\nxgb_params = {\n        'learning_rate': 0.05, 'max_depth': 4,'subsample': 0.9,\n        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n        'silent': 1, 'n_estimators':100, 'gamma':1,\n        'min_child_weight':4\n        }   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)     "},{"cell_type":"markdown","metadata":{"_cell_guid":"e103b1ab-479c-777d-ee19-a9f3a572a7c4"},"source":"Calculate the AUC for each fold"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"223a095a-712f-fa90-707a-ea34fa091fc0"},"outputs":[],"source":"for train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))"},{"cell_type":"markdown","metadata":{"_cell_guid":"394561c3-a9d5-d3ad-31a0-801a8e7e2c42"},"source":"compared to full train set only a small change in auc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd473d7d-bb0c-0ef7-e4f3-36e0567ebb41"},"outputs":[],"source":"split_ind = int(xtest.shape[0] * 0.35)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0527b67-f18d-2a09-1cc7-af9df449d4b6"},"outputs":[],"source":"# I have not varified that the public/private split is time based. It should be though.\npublic_test = xtest.iloc[:split_ind,:]\nprivate_test = xtest.iloc[split_ind:,:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a71d7b8d-0eb0-a4de-b216-49263bc7d2ad"},"outputs":[],"source":"xdat = pd.concat([xtrain, public_test], axis = 0)\n\n# convert non-numerical columns to integers\ndf_numeric = xdat.select_dtypes(exclude=['object'])\ndf_obj = xdat.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \nxdat = pd.concat([df_numeric, df_obj], axis=1)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)\n\nfor train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9634f46a-1361-9480-a642-c5f7505cf8e6"},"outputs":[],"source":"# now compare train to private test set\nxdat = pd.concat([xtrain, private_test], axis = 0)\n\n# convert non-numerical columns to integers\ndf_numeric = xdat.select_dtypes(exclude=['object'])\ndf_obj = xdat.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \nxdat = pd.concat([df_numeric, df_obj], axis=1)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)\n\nfor train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76d38ded-ca1a-dfac-f4e5-3db989492d9d"},"outputs":[],"source":"public_test['istrain'] = 1\n\n# compare public and private test\nxdat = pd.concat([public_test, private_test], axis = 0)\n\n# convert non-numerical columns to integers\ndf_numeric = xdat.select_dtypes(exclude=['object'])\ndf_obj = xdat.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \nxdat = pd.concat([df_numeric, df_obj], axis=1)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)\n\nfor train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"55857315-1200-8647-7f86-6053c94bd4e9"},"source":"Some of the variation is very likely accounted for by differences in the size(and content of the folds) of the datasets in the each case. Checking against similar sized subsets of the full train set would be a good additional validation. As well as replicating with additional seeds, would be helpful."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}