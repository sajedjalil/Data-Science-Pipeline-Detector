{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e237dac-aa0a-f73e-117f-334dea01a347"},"outputs":[],"source":"df_train[\"full_sq\"].median()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b858a145-1abf-8a7f-ffb6-352b041b4573"},"source":"# Naive XGB with Imputer"},{"cell_type":"markdown","metadata":{"_cell_guid":"20161004-01f6-2e1d-dab0-a1fc10888fc0"},"source":"### Modification from https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317 (Thx!)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2926683d-ed45-aeb2-ae9f-e4e4f45701ad"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60beec8e-7821-671d-2f67-5bf4ebfdd0dc"},"outputs":[],"source":"# using most significant feature from macro. ref: https://www.kaggle.com/yitzhakr/moscow-houses-prices-analysis\n#macro_cols = ['oil_urals', 'gdp_quart_growth', 'cpi', 'usdrub', \\\n#                'salary_growth', 'unemployment', 'mortgage_rate', \\\n#                 'deposits_rate','rent_price_3room_bus']\n\n# From here: https://www.kaggle.com/robertoruiz/sberbank-russian-housing-market/dealing-with-multicollinearity/notebook\nmacro_cols = [\"balance_trade\", \"balance_trade_growth\", \"eurrub\", \"average_provision_of_build_contract\",\n\"micex_rgbi_tr\", \"micex_cbi_tr\", \"deposits_rate\", \"mortgage_value\", \"mortgage_rate\",\n\"income_per_cap\", \"rent_price_4+room_bus\", \"museum_visitis_per_100_cap\", \"apartment_build\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"46680c61-53ca-587f-d5c3-22847fc22e2d"},"outputs":[],"source":"df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndf_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'], usecols=['timestamp'] + macro_cols)\n#df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3127f8f7-3ca2-6e68-f488-eb01ca2935f3"},"outputs":[],"source":"ax = df_train['price_doc'].hist(bins=50)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2fbc0a3-7422-704a-cb21-035308132e9a"},"outputs":[],"source":"df_train[\"full_sq\"].median()"},{"cell_type":"markdown","metadata":{"_cell_guid":"acbb7abc-a4d1-7257-0352-e077d673e3c3"},"source":"## FEATURE ENGINEERING BELOW"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18f36673-9bcc-2246-328d-2b58389ecd76"},"outputs":[],"source":"# ylog will be log(1+y), as suggested by https://github.com/dmlc/xgboost/issues/446#issuecomment-135555130\nylog_train_all = np.log1p(df_train['price_doc'].values)\nid_test = df_test['id']\n\ndf_train.drop(['id', 'price_doc'], axis=1, inplace=True)\ndf_test.drop(['id'], axis=1, inplace=True)\n\n# Build df_all = (df_train+df_test).join(df_macro)\nnum_train = len(df_train)\ndf_all = pd.concat([df_train, df_test])\ndf_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\nprint(df_all.shape)\n\n#full_sq have the most deterministic factor for scoring. \n#some full_sq have noise value that may need some moderation. NaN have better effect on the result\ndf_all.loc[df_all['full_sq']>250, 'full_sq'] = np.nan\ndf_all.loc[df_all['full_sq']<10, 'full_sq'] = np.nan\n\n#some build year have noise value that may need some moderation. NaN have better effect on the result\n#df_all.loc[df_all['build_year'] > 2017, 'build_year'] = np.nan\n\n#change floor with 0 value. NaN have better effect on the result\ndf_all.loc[df_all['floor'] == 0, 'floor'] = np.nan\ndf_all.loc[df_all['max_floor'] == 0, 'max_floor'] = np.nan\n\n# Add month-year\nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf_all['month'] = df_all.timestamp.dt.month\ndf_all['dow'] = df_all.timestamp.dt.dayofweek\n\n# Other feature engineering\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\ndf_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n## Population feature engineering source : https://www.kaggle.com/philippsp/a-collection-of-new-features\n\n#df_all['young_proportion'] = df_all['young_all']/df_all['full_all'].astype(float)\n#df_all['work_proportion'] = df_all['work_all']/df_all['full_all'].astype(float)\n#df_all['retire_proportion'] = df_all['ekder_all']/df_all['full_all'].astype(float)\n\n#df_all['ratio_preschool'] = df_all['children_preschool'] / df_all['preschool_quota'].astype(float)\n#df_all['ratio_school'] = df_all['children_school'] / df_all['school_quota'].astype(float)\n\n# Remove timestamp column (may overfit the model in train)\ndf_all.drop(['timestamp'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"980bda24-d103-bb80-3a2f-abacef25bfd3"},"source":"## -- END FEATURE ENGINEERING"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"048c594d-b06c-8d9f-71d5-6f5bc55895a3"},"outputs":[],"source":"df_all"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"580473a2-dc6b-6399-891a-88adbad6a542"},"outputs":[],"source":"#df_all = df_all.dropna(axis=1,thresh=25000)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bd9ad38-8336-bf0c-e6aa-693d67abab48"},"outputs":[],"source":"# Deal with categorical values\ndf_numeric = df_all.select_dtypes(exclude=['object'])\ndf_obj = df_all.select_dtypes(include=['object']).copy()\n\nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n\ndf_values = pd.concat([df_numeric, df_obj], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4886fe75-1b0d-8d39-4116-5f041cd9877e"},"outputs":[],"source":"# remove all infinity from feature engineering\nfrom numpy import inf, nan\n\ndf_values[df_values == inf] = nan"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c14663b-d466-cca5-a0c5-83d225158254"},"outputs":[],"source":"#from fancyimpute import SimpleFill,SoftImpute\n\n#from sklearn.preprocessing import StandardScaler, RobustScaler\n\n#df_values_temp = pd.DataFrame(SimpleFill(fill_method=\"median\").complete(df_values))\n\n#df_values_temp.columns = df_values.columns\n#df_values_temp.index = df_values.index\n\n#df_values = df_values_temp"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"84dd1a62-cb4d-56e1-ceb2-f688e7c00701"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bc7567e-ddc7-3ced-179a-5c6124545f87"},"outputs":[],"source":"df_values.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45aa3bf1-1a7f-5de4-e01e-9b02c4660090"},"outputs":[],"source":"# Convert to numpy values\nX_all = df_values.values\nprint(X_all.shape)\n\n# Create a validation set, with last 20% of data\nnum_val = int(num_train * 0.2)\n\nX_train_all = X_all[:num_train]\nX_train = X_all[:num_train-num_val]\nX_val = X_all[num_train-num_val:num_train]\nylog_train = ylog_train_all[:-num_val]\nylog_val = ylog_train_all[-num_val:]\n\nX_test = X_all[num_train:]\n\ndf_columns = df_values.columns\n\nprint('X_train_all shape is', X_train_all.shape)\nprint('X_train shape is', X_train.shape)\nprint('y_train shape is', ylog_train.shape)\nprint('X_val shape is', X_val.shape)\nprint('y_val shape is', ylog_val.shape)\nprint('X_test shape is', X_test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e8ddb6e-7741-390c-1f5b-bdfd3d33c3ef"},"outputs":[],"source":"dtrain_all = xgb.DMatrix(X_train_all, ylog_train_all, feature_names=df_columns)\ndtrain = xgb.DMatrix(X_train, ylog_train, feature_names=df_columns)\ndval = xgb.DMatrix(X_val, ylog_val, feature_names=df_columns)\ndtest = xgb.DMatrix(X_test, feature_names=df_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aad7ac34-b03c-a2d7-09da-849c41172d1b"},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 1,\n    'colsample_bytree': 0.7,\n    'min_child_weight': 5,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1,\n    'n_estimators' : 250\n}\n\n# Uncomment to tune XGB `num_boost_rounds`\npartial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n                       early_stopping_rounds=30, verbose_eval=30)\n\nnum_boost_round = partial_model.best_iteration"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7facb76-c815-9344-fb47-3f7d180cb830"},"outputs":[],"source":"fig, ax = plt.subplots(1, 1, figsize=(8, 16))\nxgb.plot_importance(partial_model, max_num_features=50, height=0.5, ax=ax)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fed17001-f36b-4b71-b1cb-761d4acebc51"},"outputs":[],"source":"num_boost_round = partial_model.best_iteration"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1d8dfb9-96eb-2f1f-a0db-1b50aa5bb5c0"},"outputs":[],"source":"model = xgb.train(dict(xgb_params, silent=0), dtrain_all, num_boost_round=num_boost_round)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2a6ccd-1a80-2178-32b4-9728d3c38cf0"},"outputs":[],"source":"fig, ax = plt.subplots(1, 1, figsize=(8, 16))\nxgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5eda1477-cfa3-b48e-1dbb-74902f75bb8d"},"outputs":[],"source":"#stacking with random forest\n\n#from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\n\n#ABR = AdaBoostRegressor(base_estimator = RandomForestRegressor(max_depth = 3,n_estimators = 5),\n#                        n_estimators = 100,random_state = 777, learning_rate = 0.05)\n\n#ABR.fit(X_train, ylog_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0190c69c-5c99-16d5-b6e6-6b2eb6748309"},"outputs":[],"source":"##ABR.score(X_val, ylog_val)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"73681ad4-a0a6-b950-9c3a-6db5b354a15c"},"outputs":[],"source":"#meta_feature1 = model.predict(dtrain)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e6a8a28-8cb7-a335-9ce9-bddabe904dca"},"outputs":[],"source":"ylog_pred = model.predict(dtest)\ny_pred = np.exp(ylog_pred) - 1\n\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\ndf_sub.to_csv('output.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}