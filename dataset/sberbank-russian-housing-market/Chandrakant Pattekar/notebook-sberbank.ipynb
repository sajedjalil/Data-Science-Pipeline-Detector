{"nbformat":4,"nbformat_minor":0,"metadata":{"_is_fork":false,"_change_revision":0,"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","file_extension":".py","version":"3.6.0"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"f2b5ddd3596a708d235fda7632356cb9e3d23bea","_cell_guid":"daf836e3-98fe-f2be-f6c6-ec61d50f3150"},"source":"In this exploration notebook, we shall try to uncover the basic information about the dataset which will help us build our models / features.\n\nLet us start with importing the necessary modules."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"2b353d1b1c441356513779eed920c0c07fad4e13","_cell_guid":"010c0e14-3312-826e-ecd6-b7db41d41f94"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\n\nfrom sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n                              GradientBoostingClassifier)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n\ncolor = sns.color_palette()\n\n#%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.set_option('display.max_columns', 500)\nrun1 = False\nrun2 = False\nrun3 = False"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"0d8381813ce37dc0638ac352540cfd5820a30cf5","_cell_guid":"78b69b00-2ca9-ec50-0e64-df78c0698238"},"source":"First let us import the train file and get some idea about the data."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"08e4bb552d2316d2b2d8ac197e84d565e68b03ca","_cell_guid":"cf59f829-b0bd-fe3c-cd3b-c9c176c678ea"},"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.shape"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ccfca48549fa39dcdda73eabc43b8f96113ee205","_cell_guid":"094c89d2-2c4a-49a4-5536-7d26173f2909"},"source":"train_df.head()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"72a5caf02d894eb03311e51883b6343835c2f02f","_cell_guid":"4d035267-d1d5-e27c-1f36-887d9bc0359e"},"source":"There are quite a few variables in this dataset. \n\nLet us start with target variable exploration - 'price_doc'. First let us do a scatter plot to see if there are any outliers in the data."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"982d05eaa1a6a0c4d583cd2c53e14e1ffeb66a78","_cell_guid":"06c00812-d557-7797-7b4b-0d28632c39d9"},"source":"print(train_df.shape[0])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"a187fdb5fa8c73b8ae61016150287b93ca53617d","_cell_guid":"3b2b42e1-a5e8-79ff-af6b-91b40664e13e"},"source":"plt.figure(figsize=(8,6))\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.price_doc.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('price', fontsize=12)\nplt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"f1c0920ced8587df0d1a86fd4de47a204c6a4fc8","_cell_guid":"d975201d-d9c3-f456-4dac-4964e964d96b"},"source":"Looks okay to me. Also since the metric is RMSLE, I think it is okay to have it as such. However if needed, one can truncate the high values. \n\nWe can now bin the 'price_doc' and plot it."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"077550ad15fdb54c6cb9a724a2067662db733de8","_cell_guid":"03e5a2f2-c68b-9965-33dd-b698775c11cf"},"source":"plt.figure(figsize=(12,8))\ndfsnsplt = pd.DataFrame(train_df.price_doc.values.astype(int))\nprint(type(dfsnsplt))\nsns.distplot(dfsnsplt, bins=60, kde=True)\n#sns.distplot(train_df.price_doc.values)\nplt.xlabel('price', fontsize=12)\nplt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"0eaf1b1c86ed72a170453dfb042950f4a61bae58","_cell_guid":"205d91d5-47ce-f88b-154e-b441c043898b"},"source":"Certainly a very long right tail. Since our metric is Root Mean Square **Logarithmic** error, let us plot the log of price_doc variable."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"989e016746ad78c49e3dad40d57960e1c86d9138","_cell_guid":"27b08122-7009-d756-e695-c463a5a27d60"},"source":"plt.figure(figsize=(12,8))\nsns.distplot(np.log(train_df.price_doc.values), bins=50, kde=True)\nplt.xlabel('price', fontsize=12)\nplt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"07e2feebdd983f233c47ce748ace20b7cc7ef8b6","_cell_guid":"d80989f8-eae8-bede-f088-4d7c6c8cd7fb"},"source":"This looks much better than the previous one. \n\nNow let us see how the median housing price change with time. "},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"80abf7a55695230fcf84cb2886d8b9f1a33f7070","_cell_guid":"c2cc67da-dd73-7fb7-473f-3987f1a5ecb8"},"source":"train_df['yearmonth'] = train_df['timestamp'].apply(lambda x: x[:4]+x[5:7])\ngrouped_df = train_df.groupby('yearmonth')['price_doc'].aggregate(np.median).reset_index()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5a97dddccf48cf1ac11675d50c97ac80b78a7448","_cell_guid":"18daf0fc-7fc4-f117-adb6-b4d3c2483412"},"source":"plt.figure(figsize=(12,8))\nsns.barplot(grouped_df.yearmonth.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\nplt.ylabel('Median Price', fontsize=12)\nplt.xlabel('Year Month', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"87b98c1a2f57693fcba6482051446c53fcf6319e","_cell_guid":"4a23d010-ccde-e93a-56f3-4b1d1774091b"},"source":"There are some variations in the median price with respect to time. Towards the end, there seems to be some linear increase in the price values.\n\nNow let us dive into other variables and see. Let us first start with getting the count of different data types. "},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"3357ad49e806179f84e75c6aa7e573f671f50c1a","_cell_guid":"4a510255-4568-c574-0a56-1baeb94e38d1"},"source":"train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"819841cc31c20dbf0fe44105b64ee461eb5e2661","_cell_guid":"636d0880-b0df-cb3c-6a26-633643b03046"},"source":"So majority of them are numerical variables with 15 factor variables and 1 date variable.\n\nLet us explore the number of missing values in each column."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"a4a5b6772045134dff7936381fc0c747bd6c2355","_cell_guid":"a4693174-8f48-e979-9c2c-b018e50b6394"},"source":"missing_df_temp = train_df.isnull()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5cce275d427ddedb2a4ef05ac281b01f98a24d66","_cell_guid":"1bd91585-c40d-96ee-8031-5f6cff98f451"},"source":"missing_df_temp.head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ba001bd82c1ea27c8bb55b5b0f5d0d30ffb2553d","_cell_guid":"c2c98c5e-b912-094f-2b8a-630fe1b363b2"},"source":"missing_df_temp.sum(axis=0).reset_index().head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"b4132b37ce21e71f7eb863d7dfd97a68f4abbb42","_cell_guid":"2e68a733-1357-4adb-af07-53b7440913c9"},"source":"missing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.ix[missing_df['missing_count']>0]\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='y')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"cea5edfa0eea1e4feabb682a79d252c296fc85c5","_cell_guid":"0f893fb6-3526-3fd1-9780-db189ca5addf"},"source":"Seems variables are found to missing as groups.\n\nSince there are 292 variables, let us build a basic xgboost model and then explore only the important variables."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"729ef085100173bf09c4de3dab62185936ffd6b1","collapsed":false},"source":"df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\nid_test = list(df_test['id'])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"08bae5da97122ec8da93e146439a8e8c9bf480f1","collapsed":false},"source":"for f in df_test.columns:\n    if df_test[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_test[f].values)) \n        df_test[f] = lbl.transform(list(df_test[f].values))"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"0109ac3ded3e3de975797e5bec2f3acce065815c","collapsed":false},"source":"#clean data\nbad_index = train_df[train_df.life_sq > train_df.full_sq].index\ntrain_df.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\ndf_test.ix[equal_index, \"life_sq\"] = df_test.ix[equal_index, \"full_sq\"]\nbad_index = df_test[df_test.life_sq > df_test.full_sq].index\ndf_test.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train_df[train_df.life_sq < 5].index\ntrain_df.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = df_test[df_test.life_sq < 5].index\ndf_test.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train_df[train_df.full_sq < 5].index\ntrain_df.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = df_test[df_test.full_sq < 5].index\ndf_test.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ntrain_df.ix[kitch_is_build_year, \"build_year\"] = train_df.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = train_df[train_df.kitch_sq >= train_df.life_sq].index\ntrain_df.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = df_test[df_test.kitch_sq >= df_test.life_sq].index\ndf_test.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train_df[(train_df.kitch_sq == 0).values + (train_df.kitch_sq == 1).values].index\ntrain_df.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = df_test[(df_test.kitch_sq == 0).values + (df_test.kitch_sq == 1).values].index\ndf_test.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train_df[(train_df.full_sq > 210) & (train_df.life_sq / train_df.full_sq < 0.3)].index\ntrain_df.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = df_test[(df_test.full_sq > 150) & (df_test.life_sq / df_test.full_sq < 0.3)].index\ndf_test.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = train_df[train_df.life_sq > 300].index\ntrain_df.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\nbad_index = df_test[df_test.life_sq > 200].index\ndf_test.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ntrain_df.product_type.value_counts(normalize= True)\ndf_test.product_type.value_counts(normalize= True)\nbad_index = train_df[train_df.build_year < 1500].index\ntrain_df.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = df_test[df_test.build_year < 1500].index\ndf_test.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = train_df[train_df.num_room == 0].index \ntrain_df.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = df_test[df_test.num_room == 0].index \ndf_test.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\ntrain_df.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [3174, 7313]\ndf_test.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = train_df[(train_df.floor == 0).values * (train_df.max_floor == 0).values].index\ntrain_df.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = train_df[train_df.floor == 0].index\ntrain_df.ix[bad_index, \"floor\"] = np.NaN\nbad_index = train_df[train_df.max_floor == 0].index\ntrain_df.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = df_test[df_test.max_floor == 0].index\ndf_test.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = train_df[train_df.floor > train_df.max_floor].index\ntrain_df.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = df_test[df_test.floor > df_test.max_floor].index\ndf_test.ix[bad_index, \"max_floor\"] = np.NaN\ntrain_df.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ntrain_df.ix[bad_index, \"floor\"] = np.NaN\ntrain_df.material.value_counts()\ndf_test.material.value_counts()\ntrain_df.state.value_counts()\nbad_index = train_df[train_df.state == 33].index\ntrain_df.ix[bad_index, \"state\"] = np.NaN\ndf_test.state.value_counts()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"45bdb4414db2f56c04a8eb4dc2373b05e85f8520","collapsed":false},"source":"ulimit = np.percentile(train_df.price_doc.values, 99.5)\nllimit = np.percentile(train_df.price_doc.values, 0.5)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\ntrain_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n\ncol = \"full_sq\"\nulimit = np.percentile(train_df[col].values, 99.5)\nllimit = np.percentile(train_df[col].values, 0.5)\ntrain_df[col].ix[train_df[col]>ulimit] = ulimit\ntrain_df[col].ix[train_df[col]<llimit] = llimit\n"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"50120a37b2025d7de66006cc00fe0c9a6e533a02","collapsed":false},"source":"\n# Add month-year\n\"\"\"\nmonth_year = (train_df.timestamp.dt.month.astype(int) + (train_df.timestamp.dt.year * 100).astype(int))\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ntrain_df['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\nmonth_year = (df_test.timestamp.dt.month.astype(int) + (df_test.timestamp.dt.year * 100).astype(int))\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_test['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n\n# Add week-year count\nweek_year = (train_df.timestamp.dt.weekofyear.astype(int) + (train_df.timestamp.dt.year * 100).astype(int))\nweek_year_cnt_map = week_year.value_counts().to_dict()\ntrain_df['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\nweek_year = (df_test.timestamp.dt.weekofyear.astype(int) + (df_test.timestamp.dt.year * 100).astype(int))\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_test['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n\n# Add month and day-of-week\ntrain_df['month'] = train_df.timestamp.dt.month\ntrain_df['dow'] = train_df.timestamp.dt.dayofweek\n\ndf_test['month'] = df_test.timestamp.dt.month\ndf_test['dow'] = df_test.timestamp.dt.dayofweek\n\n\n# Other feature engineering\ntrain_df['rel_floor'] = train_df['floor'] / train_df['max_floor'].astype(float)\ntrain_df['rel_kitch_sq'] = train_df['kitch_sq'] / train_df['full_sq'].astype(float)\n\ndf_test['rel_floor'] = df_test['floor'] / df_test['max_floor'].astype(float)\ndf_test['rel_kitch_sq'] = df_test['kitch_sq'] / df_test['full_sq'].astype(float)\n\ntrain_df.apartment_name=train_df.sub_area + train_df['metro_km_avto'].astype(str)\ndf_test.apartment_name=df_test.sub_area + train_df['metro_km_avto'].astype(str)\n\ntrain_df['room_size'] = train_df['life_sq'] / train_df['num_room'].astype(float)\ndf_test['room_size'] = df_test['life_sq'] / df_test['num_room'].astype(float)\n\"\"\""},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e622b7b39b94d02cfc3c8b6a52b27962c7a21f97","collapsed":false},"source":"for f in train_df.columns:\n    if train_df[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n        \ntrain_y = train_df.price_doc.values\ntrain_X = train_df.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e7ffc622f2bd590c128dc8525a0b347ae249a7d9","collapsed":false},"source":"train_df.columns.values"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"59ae08a1a3443d7847ed6827fa8cc1071615bcf0","collapsed":false},"source":"print(train_X.shape)\nprint(train_y.shape)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ba9fe6328a9b0fe11892c85441551c550464a834","collapsed":false},"source":"#model gradient boost\n\n#clfdata_X = pd.DataFrame(np.nan_to_num(train_df.drop(['id','timestamp','price_doc'],axis=1)))\n#clfdata_y = pd.DataFrame(np.nan_to_num(train_df['price_doc']))\n\n#train_X, X_val, train_y, y_val = train_test_split(clfdata_X, clfdata_y, test_size=0.30,random_state=21)\n\ntrain_X1 = np.nan_to_num(train_X[:25000]).astype(int)\nval_X = np.nan_to_num(train_X[25000:]).astype(int)\ntrain_y1 = np.nan_to_num(train_y[:25000]).astype(int)\nval_y = np.nan_to_num(train_y[25000:]).astype(int)\n"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9f25d8fb777965116788ec52c36b36893157a848","collapsed":false},"source":"GBclf= GradientBoostingClassifier(max_depth=4,min_samples_leaf=2)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"4da316d50e9144508fd7f538dab51ad27b03acd3","collapsed":false},"source":"clfX_train.head(1)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"f7cfa4dec023f2e52d639585dc93e74d060065ad","collapsed":false},"source":"train_y1[0]"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"3037d6ad53f9c1763f5c6a1f120cf8d7901f3101","collapsed":false},"source":"#train_X, X_val, train_y, y_val\n\nGBclf.fit(train_X1,train_y1)\nGBclf.score(val_X,val_y)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"5854d758ef6a45b742ee0d226543a251a1907baf","collapsed":false},"source":"#predict = GBmodel.predict(test_df.drop([\"id\", \"timestamp\"],axis=1))\npredict = GBclf.predict(test_df.drop(['id','timestamp'],axis=1))\noutput = pd.DataFrame({'id': id_test, 'price_doc': np.expm1(predict)})\n#output['price_doc'] = lab\n"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"0905c853a6fca6e8c3d59ff5165e2c4af783e0fc","collapsed":false},"source":"output.to_csv('Sberbank_GBclf.csv', index=False)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"3e7eb8af761a89d9213aea009cefc0d9bb4d64be","_cell_guid":"31677ea2-0321-f709-dac8-120a2d02278f"},"source":"if run1 == True:\n\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 6,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1\n    }\n    dtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)\n\n    # plot the important features #\n    fig, ax = plt.subplots(figsize=(12,18))\n    xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n    plt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"d0e40ffae4774d0972a338387936e0d26577a1b1","_cell_guid":"7f10f4aa-05e8-0818-505e-9a5166e6a52b"},"source":"So the top 5 variables and their description from the data dictionary are:\n\n 1. full_sq - total area in square meters, including loggias, balconies and other non-residential areas\n 2. life_sq - living area in square meters, excluding loggias, balconies and other non-residential areas\n 3. floor - for apartments, floor of the building\n 4. max_floor - number of floors in the building\n 5. build_year - year built\n\nNow let us see how these important variables are distributed with respect to target variable.\n\n**Total area in square meters:**"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"d6e189c8c8b03f60e6515c28447480634d962282","collapsed":false},"source":"if run1 == True:\n    print(id_test[:10])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"c0709046ec0bf8dcdb08ae46850293480e6f01df","collapsed":false},"source":"if run1 == True:\n    df_test.drop([\"id\", \"timestamp\"], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e5ed31f2fee5e46cfbfe6566a214fab826ce1970","collapsed":false},"source":"if run1 == True:\n    dtest = xgb.DMatrix(df_test, feature_names=train_X.columns)\n\n    y_pred = model.predict(dtest)\n\n    y_pred = np.round(y_pred * 1.008)\n    df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\n    df_sub.to_csv('Sberbank_0.csv', index=False)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8e14f8985273da8b45decf7d8f2e09e257d6bd55","collapsed":false},"source":"df_sub.head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8d9c3311d8a53b8867d73c0296d92b6973b1131b","_cell_guid":"407b8469-fcc2-5d41-44de-2b3833523006"},"source":"if run2 == True:\n    ulimit = np.percentile(train_df.price_doc.values, 99.5)\n    llimit = np.percentile(train_df.price_doc.values, 0.5)\n    train_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\n    train_df['price_doc'].ix[train_df['price_doc']<llimit] = llimit\n\n    col = \"full_sq\"\n    ulimit = np.percentile(train_df[col].values, 99.5)\n    llimit = np.percentile(train_df[col].values, 0.5)\n    train_df[col].ix[train_df[col]>ulimit] = ulimit\n    train_df[col].ix[train_df[col]<llimit] = llimit\n\n    plt.figure(figsize=(12,12))\n    sns.jointplot(x=np.log1p(train_df.full_sq.values), y=np.log1p(train_df.price_doc.values), size=10)\n    plt.ylabel('Log of Price', fontsize=12)\n    plt.xlabel('Log of Total area in square metre', fontsize=12)\n    plt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"8b85d4f4689420f66ac2ceaeaf1f2eb9eb597fa6","_cell_guid":"c6ec261b-ba31-57a5-999a-5efcead8dbff"},"source":"**Living area in square meters:**"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"be45ff1aa95a76a03ef92d8840db6674d39c9236","_cell_guid":"e4e720e7-075c-0bed-2982-61c3a97775b0"},"source":"if run2 == True:\n    col = \"life_sq\"\n    train_df[col].fillna(0, inplace=True)\n    ulimit = np.percentile(train_df[col].values, 95)\n    llimit = np.percentile(train_df[col].values, 5)\n    train_df[col].ix[train_df[col]>ulimit] = ulimit\n    train_df[col].ix[train_df[col]<llimit] = llimit\n\n    plt.figure(figsize=(12,12))\n    sns.jointplot(x=np.log1p(train_df.life_sq.values), y=np.log1p(train_df.price_doc.values), \n              kind='kde', size=10)\n    plt.ylabel('Log of Price', fontsize=12)\n    plt.xlabel('Log of living area in square metre', fontsize=12)\n    plt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"18dd56b77665d5d91a178940e3dd9fd90788b2ae","_cell_guid":"038f8a9b-0b95-633d-6a8d-0bbfa10116f3"},"source":"**Floor:**\n\nWe will see the count plot of floor variable."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"392f35c55ac6551ed3eed3ec2207b51efa09df69","_cell_guid":"0c89affd-44b0-6c6c-f616-8bb2c518957a"},"source":"if run2 == True:\n    plt.figure(figsize=(12,8))\n    sns.countplot(x=\"floor\", data=train_df)\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"48a1c4fd7c60e7d8d95a4846c495e189bef0e876","_cell_guid":"0e11ee5f-88bb-b092-e522-64c91cee25b0"},"source":"The distribution is right skewed. There are some good drops in between (5 to 6, 9 to 10, 12 to 13, 17 to 18). Now let us see how the price changes with respect to floors."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"48302704fd2146918f0ac6cef842062ad6b69666","_cell_guid":"a0cae509-6568-0b07-ae4c-f5adf825f709"},"source":"if run2 == True:\n    grouped_df = train_df.groupby('floor')['price_doc'].aggregate(np.median).reset_index()\n    plt.figure(figsize=(12,8))\n    sns.pointplot(grouped_df.floor.values, grouped_df.price_doc.values, alpha=0.8, color=color[2])\n    plt.ylabel('Median Price', fontsize=12)\n    plt.xlabel('Floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"902e2078d0784b7d48331d4c7d65fa5072b5a300","_cell_guid":"af624cc2-1563-02e1-c87b-47b0bfbaacfd"},"source":"This shows an overall increasing trend (individual houses seems to be costlier as well - check price of 0 floor houses). \nA sudden increase in the house price is also observed at floor 18.\n\n**Max floor:**\n\nTotal number of floors in the building is one another important variable. So let us plot that one and see."},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"dffc078d6d4406260dc34047f8d8b86645670431","_cell_guid":"7bd45c6e-b9e2-c95c-fe31-52e307e2b1ca"},"source":"if run2 == True:\n    plt.figure(figsize=(12,8))\n    sns.countplot(x=\"max_floor\", data=train_df)\n    plt.ylabel('Count', fontsize=12)\n    plt.xlabel('Max floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"},{"cell_type":"markdown","execution_count":null,"outputs":[],"metadata":{"_uuid":"207709020a12338d3f8c137d6a97610b79be04c5","_cell_guid":"39981f77-9e03-79d5-33c9-91e4325d8ab6"},"source":"We could see that there are few tall bars in between (at 5,9,12,17 - similar to drop in floors in the previous graph). May be there are some norms / restrictions on the number of maximum floors present(?). \n\nNow let us see how the median prices vary with the max floors. "},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"c5a437ea8ba8c7dc93bcfc8ba4a83cef0c0b9ab9","_cell_guid":"5087aae6-a3e9-fef6-9d05-e8240716a9c1"},"source":"if run2 == True:\n    plt.figure(figsize=(12,8))\n    sns.boxplot(x=\"max_floor\", y=\"price_doc\", data=train_df)\n    plt.ylabel('Median Price', fontsize=12)\n    plt.xlabel('Max Floor number', fontsize=12)\n    plt.xticks(rotation='vertical')\n    plt.show()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9800607de67725e9cb9e54105a59e420c5a4e931","_cell_guid":"bed7cbff-146d-91cc-d7d0-45321542bf99"},"source":"if run3 == True:\n    import numpy as np\n    import pandas as pd\n    import xgboost as xgb\n    import matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9063f65e02219f46da84cbd716f7161d1b16c870","_cell_guid":"28bf4b7f-742c-450f-cd3c-67d29e975b78"},"source":"if run3 == True:\n    df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n    df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n    df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"598b57899d177a82eb77d899590831086ba6a670","_cell_guid":"d227967e-1880-4c24-2bbc-87572577fbad"},"source":"if run3 == True:\n    df_train.head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ca26b4913c1e07db0a113cdd943865e226e7bffb","_cell_guid":"caeb0606-e028-6bd8-c2c5-a72cce548282"},"source":"if run3 == True:\n    # cleanup\n    print(df_train.shape)\n    df_train.loc[df_train.full_sq == 0, 'full_sq'] = 30\n    df_train = df_train[df_train.price_doc/df_train.full_sq <= 600000]\n    df_train = df_train[df_train.price_doc/df_train.full_sq >= 10000]\n    print(df_train.shape)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"de44e3b90f51ed5318a1adbdc423cf4f3959eb2c","_cell_guid":"045214cb-205f-6f1d-54b8-e936b9899877"},"source":"if run3 == True:\n    #print(df_train.loc[df_train.full_sq == 30, 'full_sq'])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8a21c01ed2c058927d29c1f1103dd63fa68bb29b","_cell_guid":"57f20de4-5715-c6b9-5666-ed35bb1ad469"},"source":"if run3 == True:\n    print(df_test.shape+df_train.shape)\n    #print(df_train.shape)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"98a21b699fb16f84618bd3cbec545bce69531e5e","_cell_guid":"601fa6a6-c1f3-e94f-1b4e-f7abbef2f5ed"},"source":"if run3 == True:\n    #print(df_test['id'][:10])\n    #df_test.head()\n    #df_train.head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"1622a921f41db4ac121115afbd0dd766e9170821","_cell_guid":"19c60f35-d786-b9cf-d52e-71922644c262"},"source":"if run3 == True:\n    #print(df_train.loc[df_train.full_sq == 30, 'price_doc'],  df_train.loc[df_train.full_sq == 30, 'full_sq'])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"47c497960142e14ba8c77f7a8ae9e14c02ddb717","_cell_guid":"96d3b42e-adc8-e062-d765-64977b487398"},"source":"if run3 == True:\n    y_train = df_train['price_doc'].values\n    id_test = df_test['id']\n\n    df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n    df_test.drop(['id'], axis=1, inplace=True)\n\n    # Build df_all = (df_train+df_test).join(df_macro)\n    num_train = len(df_train)\n    df_all = pd.concat([df_train, df_test])\n    df_all = df_all.join(df_macro, on='timestamp', rsuffix='_macro')\n    print(df_all.shape)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"8c1e6c58a869bb189d1d5d60ebe4c89faf4d0f8b","_cell_guid":"df0a2031-4c31-3d44-b9dc-82d1c5f308af"},"source":"if run3 == True:\n    #df_all.describe"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"c8e7290be6767e7f51d56b1d393b9a2ab81dc85c","_cell_guid":"b67e1515-a5a8-0429-3550-61e22f0098e8"},"source":"if run3 == True:\n    # Add month-year\n    #year= df_all.timestamp.dt.year\n    #year_cnt_map = year.value_counts().to_dict()\n    #df_all['year_cnt'] = year.map(year_cnt_map)\n    #df_all['Age_building']=2018-df_all['build_year']\n\n    # Other feature engineering\n    df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n    df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n    # Remove timestamp column (may overfit the model in train)\n    df_all.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"f50a1362205ea90d4b304e4fbdaf18088e671d6c","_cell_guid":"2ce28202-fa7a-aa37-1d61-109bb8014b68"},"source":"if run3 == True:\n    factorize = lambda t: pd.factorize(t[1])[0]"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"fd4364c1b859625b5ccc33ac795ac26c09670736","_cell_guid":"6ce1902d-076d-edbe-728b-2c5de0243289"},"source":"if run3 == True:\n    df_obj = df_all.select_dtypes(include=['object'])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"019560f2a795ea7a247f6ed246edebedcfdeb52e","_cell_guid":"d481411f-926e-67fc-f802-49cec928e479"},"source":"if run3 == True:\n    df_obj.shape"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ab02ef737ce65698ecb5cb365a05d1c2f17d884b","_cell_guid":"0427f6a2-b523-66de-2342-826cc15cde9c"},"source":"if run3 == True:\n    z = np.array(list(map(factorize, df_obj.iteritems()))).T"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"36661b908e634053f1da5a5836c3a7e377928e9e","_cell_guid":"3b6b89ca-9ce4-ccfc-2d81-5288f1eabea0"},"source":"if run3 == True:\n    q = np.array([x for x in df_obj.iteritems()]).T"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"d83b16a1fee40c1b13ce939f9c4e759d333b24bf","_cell_guid":"eba96579-82eb-88bb-9640-84eed42b5f21"},"source":"if run3 == True:\n    print(q[:10])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"2aaee46b7bf869defe864279f48e84e673a09b47","_cell_guid":"e52b372b-5a10-7f14-5727-d895a13da240"},"source":"if run3 == True:\n    #df_obj = df_all.select_dtypes(include=['object'])\n\n    #X_all = np.c_[\n    #    df_all.select_dtypes(exclude=['object']).values,\n    #    np.array(list(map(factorize, df_obj.iteritems()))).T\n    #]\n    #print(X_all.shape)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"ee407a94038e471a348b19849d738c6430f99c34","_cell_guid":"e530b589-83b9-7159-84f2-8a07ce19f4d1"},"source":"if run3 == True:\n    #X_train = X_all[:num_train]\n    #X_test = X_all[num_train:]"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"f5f993671057a942fdd449c859b7c12df8ac21d6","_cell_guid":"1b74da38-3e8e-d106-f802-2cd425cfc6e2"},"source":"if run3 == True:\n    # Deal with categorical values\n    df_numeric = df_all.select_dtypes(exclude=['object'])\n    df_obj = df_all.select_dtypes(include=['object']).copy()\n\n    for c in df_obj:\n        df_obj[c] = pd.factorize(df_obj[c])[0]\n\n    df_values = pd.concat([df_numeric, df_obj], axis=1)\n\n\n    # Convert to numpy values\n    X_all = df_values.values\n    print(X_all.shape)\n\n    X_train = X_all[:num_train]\n    X_test = X_all[num_train:]\n\n    df_columns = df_values.columns\n    X_train=np.nan_to_num(X_train)\n    X_test=np.nan_to_num(X_test)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e43cf725ed11938530154bb7916cdfcafcfc06d4","_cell_guid":"2b7fc4fd-2c94-c3cd-f002-e181dce7a816"},"source":"if run3 == True:\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 5,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1\n    }\n\n    dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n    dtest = xgb.DMatrix(X_test, feature_names=df_columns)\n"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"3326ea690dc05736ff65d3ef2ee679312712420e","_cell_guid":"61ff35f7-6137-de13-80e6-680b3166f552"},"source":"if run3 == True:\n    # Uncomment to tune XGB `num_boost_rounds`\n\n    #cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    #    verbose_eval=True, show_stdv=False)\n    #cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()\n    #num_boost_rounds = len(cv_result)\n\n    num_boost_round = 489\n\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"491f484ae52cfb9e7af020109c1a369b59fcdec6","_cell_guid":"07c0ea31-b20f-cdc3-e073-35fd04931f4f"},"source":"if run3 == True:\n    fig, ax = plt.subplots(figsize=(12,18))\n    xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n    plt.show()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"048f7f95e78527b19199775cdd7bb09123a4e8b8","_cell_guid":"6ab9b9db-060c-2f2a-fc24-3cdd5a932434"},"source":"if run3 == True:\n    #fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n    #xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)\n\n    y_pred = model.predict(dtest)\n    y_pred = np.round(y_pred * 1.008)\n    df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\n    df_sub.to_csv('Sberbank_1.csv', index=False)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"2b83067bc97e2eb0640882240cb5de1bb8862005","_cell_guid":"770e179f-15e9-f8a4-e7c9-01ab642c777e"},"source":"if run3 == True:\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn import model_selection, preprocessing\n    import xgboost as xgb\n    import datetime"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"54bf825fa723a44e0b2d65ab0bf168b4d0b9f1bc","_cell_guid":"240371ae-87a1-8182-00de-7b8f1f385d2a"},"source":"if run3 == True:\n    #load files\n    train = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n    test = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n    macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\n    id_test = test.id\n\n    #multiplier = 0.969"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"fbb96fc660ab6c634befefeeea1a242be87e5821","_cell_guid":"40b21bff-8026-90aa-c250-dd47ea913ef5"},"source":"if run3 == True:\n    #clean data\n    bad_index = train[train.life_sq > train.full_sq].index\n    train.ix[bad_index, \"life_sq\"] = np.NaN\n    equal_index = [601,1896,2791]\n    test.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\n    bad_index = test[test.life_sq > test.full_sq].index\n    test.ix[bad_index, \"life_sq\"] = np.NaN\n    bad_index = train[train.life_sq < 5].index\n    train.ix[bad_index, \"life_sq\"] = np.NaN\n    bad_index = test[test.life_sq < 5].index\n    test.ix[bad_index, \"life_sq\"] = np.NaN\n    bad_index = train[train.full_sq < 5].index\n    train.ix[bad_index, \"full_sq\"] = np.NaN\n    bad_index = test[test.full_sq < 5].index\n    test.ix[bad_index, \"full_sq\"] = np.NaN\n    kitch_is_build_year = [13117]\n    train.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\n    bad_index = train[train.kitch_sq >= train.life_sq].index\n    train.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = test[test.kitch_sq >= test.life_sq].index\n    test.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\n    train.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\n    test.ix[bad_index, \"kitch_sq\"] = np.NaN\n    bad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\n    train.ix[bad_index, \"full_sq\"] = np.NaN\n    bad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\n    test.ix[bad_index, \"full_sq\"] = np.NaN\n    bad_index = train[train.life_sq > 300].index\n    train.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n    bad_index = test[test.life_sq > 200].index\n    test.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\n    train.product_type.value_counts(normalize= True)\n    test.product_type.value_counts(normalize= True)\n    bad_index = train[train.build_year < 1500].index\n    train.ix[bad_index, \"build_year\"] = np.NaN\n    bad_index = test[test.build_year < 1500].index\n    test.ix[bad_index, \"build_year\"] = np.NaN\n    bad_index = train[train.num_room == 0].index \n    train.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = test[test.num_room == 0].index \n    test.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\n    train.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = [3174, 7313]\n    test.ix[bad_index, \"num_room\"] = np.NaN\n    bad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\n    train.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\n    bad_index = train[train.floor == 0].index\n    train.ix[bad_index, \"floor\"] = np.NaN\n    bad_index = train[train.max_floor == 0].index\n    train.ix[bad_index, \"max_floor\"] = np.NaN\n    bad_index = test[test.max_floor == 0].index\n    test.ix[bad_index, \"max_floor\"] = np.NaN\n    bad_index = train[train.floor > train.max_floor].index\n    train.ix[bad_index, \"max_floor\"] = np.NaN\n    bad_index = test[test.floor > test.max_floor].index\n    test.ix[bad_index, \"max_floor\"] = np.NaN\n    train.floor.describe(percentiles= [0.9999])\n    bad_index = [23584]\n    train.ix[bad_index, \"floor\"] = np.NaN\n    train.material.value_counts()\n    test.material.value_counts()\n    train.state.value_counts()\n    bad_index = train[train.state == 33].index\n    train.ix[bad_index, \"state\"] = np.NaN\n    test.state.value_counts()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"544679fc283f94fb89cce0741f5fc08f6e24ee1e","_cell_guid":"eb830d0a-b8ac-7e95-7a74-ac341de9d099"},"source":"if run3 == True:\n    # brings error down a lot by removing extreme price per sqm\n    train.loc[train.full_sq == 0, 'full_sq'] = 50\n    train = train[train.price_doc/train.full_sq <= 600000]\n    train = train[train.price_doc/train.full_sq >= 10000]"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"2f705dda371dea4c6c2c595f5e33acb9d1d348f1","_cell_guid":"77b4a188-6e3d-4a23-b4c0-4c6069551135"},"source":"if run3 == True:\n    # Add month-year\n    month_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\n    month_year_cnt_map = month_year.value_counts().to_dict()\n    train['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n    month_year = (test.timestamp.dt.month + test.timestamp.dt.year * 100)\n    month_year_cnt_map = month_year.value_counts().to_dict()\n    test['month_year_cnt'] = month_year.map(month_year_cnt_map)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"4c9b29e6d628b941f8d7c301b0eb5e7836c61ac3","_cell_guid":"d5a3ec13-ca69-ba1b-7c43-c71960f1810b"},"source":"if run3 == True:\n    # Add week-year count\n    week_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\n    week_year_cnt_map = week_year.value_counts().to_dict()\n    train['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n    week_year = (test.timestamp.dt.weekofyear + test.timestamp.dt.year * 100)\n    week_year_cnt_map = week_year.value_counts().to_dict()\n    test['week_year_cnt'] = week_year.map(week_year_cnt_map)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"300165f3052b377460070558aa5fc057edd86ca8","_cell_guid":"9f0ec436-0517-1142-ff31-2232c1a7e627"},"source":"if run3 == True:\n    # Add month and day-of-week\n    train['month'] = train.timestamp.dt.month\n    train['dow'] = train.timestamp.dt.dayofweek\n\n    test['month'] = test.timestamp.dt.month\n    test['dow'] = test.timestamp.dt.dayofweek"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"e160d3f381ea12704ca06a3d02cb1fcf72360401","_cell_guid":"aa245a0d-ae75-8a01-fc9e-a9b374ebb5e4"},"source":"if run3 == True:\n    # Other feature engineering\n    train['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\n    train['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n\n    test['rel_floor'] = test['floor'] / test['max_floor'].astype(float)\n    test['rel_kitch_sq'] = test['kitch_sq'] / test['full_sq'].astype(float)\n\n    train.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\n    test.apartment_name=test.sub_area + train['metro_km_avto'].astype(str)\n\n    train['room_size'] = train['life_sq'] / train['num_room'].astype(float)\n    test['room_size'] = test['life_sq'] / test['num_room'].astype(float)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7a67bc962e4eee7cc6134e6921721ed06b714168","_cell_guid":"107c8731-16cc-730e-4c46-3864a8f368d7"},"source":"if run3 == True:\n    y_train = train[\"price_doc\"]\n    x_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n    x_test = test.drop([\"id\", \"timestamp\"], axis=1)\n\n    for c in x_train.columns:\n        if x_train[c].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(x_train[c].values)) \n            x_train[c] = lbl.transform(list(x_train[c].values))\n        #x_train.drop(c,axis=1,inplace=True)\n        \n    for c in x_test.columns:\n        if x_test[c].dtype == 'object':\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(x_test[c].values)) \n            x_test[c] = lbl.transform(list(x_test[c].values))\n            #x_test.drop(c,axis=1,inplace=True)  "},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"7dc030604ea401709caaca86175cc8f7273302a1","_cell_guid":"32b41275-2ad5-07ae-9db9-8b26d0ac217c"},"source":"if run3 == True:\n    xgb_params = {\n        'eta': 0.05,\n        'max_depth': 5,\n        'subsample': 0.7,\n        'colsample_bytree': 0.7,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'silent': 1\n    }\n\n    dtrain = xgb.DMatrix(x_train, y_train)\n    dtest = xgb.DMatrix(x_test)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"0afa70f27c09555faee6b1fb7e1ed138760d9598","_cell_guid":"38814d59-0557-00ec-313e-39d9282518dc"},"source":"if run3 == True:\n    #cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    #    verbose_eval=50, show_stdv=False)\n    #cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n\n    #num_boost_rounds = len(cv_output)\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= 350)\n\n    #fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n    #xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)\n\n    y_predict = model.predict(dtest)\n    y_predict = np.round(y_predict * 0.99)\n    output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n    output.head()\n"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_execution_state":"idle","_uuid":"9576c64f3a88932617de084986037a737dc05a55","_cell_guid":"224db05c-65fc-4b5d-3782-1d339fede7bd"},"source":"if run3 == True:\n    output.to_csv('Sberbank_2.csv', index=False)"}]}