{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fbcd76f-c899-bf50-b95d-1597c3f876dd"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05313156-727d-d4ec-e5da-0ea3a7d5ea3a"},"outputs":[],"source":"df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndf_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n\ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d1825a5b-d578-cda3-bfba-5a07d3bd944f"},"outputs":[],"source":"# let us perform a linear regression - start with a few features "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e92599d4-dd86-df0b-17cc-6a8f3f455b1e"},"outputs":[],"source":"def feature_summary(data):\n    n_row=data.shape[0]\n    features=pd.DataFrame()\n    features_names=[]\n    features_type = []\n    features_counts=[]\n    features_missing=[]\n    names=data.columns.values\n    for i in names:\n        features_names.append(i)\n        features_type.append(type(data.ix[1,i]))\n        features_counts.append(data[i].value_counts().count())\n        features_missing.append(data[data[i].isnull()].shape[0])\n    features['name']=features_names\n    features['type'] = features_type\n    features['value counts']=features_counts\n    features['missing']=features_missing\n    features['percentage_missing']=features['missing']/n_row\n    return (features)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ac5247f5-a622-7aaf-c46a-1400bf352305"},"outputs":[],"source":"feature_summ = feature_summary(df_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5dd4011f-ed1a-36a0-900c-5da39993fa1d"},"outputs":[],"source":"column_names = feature_summ[feature_summ['percentage_missing'] < 0.20]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"51882c18-a82d-1c21-dc9c-0532f4abfc1f"},"outputs":[],"source":"column_names"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"75f2eae0-24b3-c82e-613f-2fd9fe16de17"},"outputs":[],"source":"df_train_quant = df_train.select_dtypes(include=['int64', 'floating'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66c55b92-f5b3-b1ef-f9ef-88a99fb0061e"},"outputs":[],"source":"df_train_quant.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c59a5e3-a817-29ad-15e7-372e69ccddfd"},"outputs":[],"source":"corr_mat = pd.DataFrame()\ncolumn_names = np.arange(0)\ncorrelation_values = np.arange(0)\nfor i in df_train_quant.columns.values:\n    column_names = np.append(column_names, i)\n    correlation_values = np.append(correlation_values,df_train_quant[i].corr(df_train_quant['price_doc']))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f141436-58be-103e-e1d3-4b98664025f5"},"outputs":[],"source":"corr_mat['columns'] = column_names\ncorr_mat['value'] = correlation_values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c35044a2-e0fb-8cbf-ae7b-9b3bb60af098"},"outputs":[],"source":"corr_mat"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e49a98b-a27c-e1f6-9b06-7853d867bb82"},"outputs":[],"source":"corr_mat.plot(kind = 'bar')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1532911-5daf-c7ad-b8e3-e499e2310b17"},"outputs":[],"source":"# using the above poorly constructed plot, and if you look closely you'll see why i chose 0.20\n# as a theshold\ncorr_mat_select = corr_mat[corr_mat['value'].abs() > 0.20 ]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fa137323-85e2-99b3-2435-9a051ab0bbc3"},"outputs":[],"source":"corr_mat_select.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e385ad4e-3121-7148-318f-3dcab5e96033"},"outputs":[],"source":"# Implementing Linear Regression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"922963e1-977a-ebb2-518b-a7c063dcdb0c"},"outputs":[],"source":"# let us use linear regression to solve this\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model as lm "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0976aaa8-287b-d019-dc0e-5a4d59c16148"},"outputs":[],"source":"df_train_quant = df_train[corr_mat_select['columns']]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4b178df-a4a2-0496-6c67-2be9c4ab5e29"},"outputs":[],"source":"df_train_quant.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"175bd16a-96fa-e1a8-8183-60c2d660ee17"},"outputs":[],"source":"#handling missing values using mean values? we can explore more methods for this at a later\n#stage\n#df_train_quant = df_train_quant.drop('timestamp', 1)\nfor i in range(1,len(df_train_quant.columns)):\n       df_train_quant.iloc[:,i] = df_train_quant.iloc[:,i].fillna(df_train_quant.iloc[:,i].mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0977359-4c7f-cc14-0481-7aac40a157b7"},"outputs":[],"source":"y = df_train_quant['price_doc']\ndf_train_quant = df_train_quant.drop('price_doc', 1)\nX_train,X_test,y_train,y_test = train_test_split(df_train_quant,y,test_size=0.20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ae01f4c-f166-77bc-1f05-a3875c906562"},"outputs":[],"source":"model = lm.LinearRegression()\nmodel.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b73b0ac0-8fdb-43e5-33d9-b59cbeb76313"},"outputs":[],"source":"predy = model.predict(X_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e38d1b27-ee52-8aff-a37a-70d2a040712c"},"outputs":[],"source":"residuals = (predy - y_train) * (predy - y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60f712ed-0f31-d526-74d1-154b4c20a8fd"},"outputs":[],"source":"import matplotlib.pyplot as plt\nmodel_plot = pd.DataFrame()\nmodel_plot['y'] = y_train\nmodel_plot['residuals'] = residuals\nplt.plot(y_train, residuals)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f12f308a-04f3-11e1-ca01-654a1c7acab2"},"outputs":[],"source":"# We can see that the errors start increasing after the value is 17500. Maybe\n# we can set this as a threshold."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da60dd63-0213-35c3-ed21-fefaa0a4a629"},"outputs":[],"source":"predy = model.predict(X_test)\nfrom sklearn.metrics import r2_score\nr2_score(y_test,predy)\ncorr_mat_select = corr_mat_select.drop(275)\ntest = df_test[corr_mat_select['columns']]\nfor i in range(1,len(test.columns)):\n       test.iloc[:,i] = test.iloc[:,i].fillna(test.iloc[:,i].mean())\nresult = model.predict(test)\nresult = pd.DataFrame(result)\nresult['id'] = df_test['id']\nresult['price_doc'] = result.iloc[:,0].abs()\nresult = result.drop(0,1)\nresult['price_doc'] = result['price_doc'].round(2)\nresult.to_csv(\"output_10.csv\", index = False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66911f73-adfa-0443-a823-7f30f4f9fa75"},"outputs":[],"source":"# The above error plot gives us some insight. \n# The increasing value of error as y value increases shows us that we need to transform y values\n# or break it down.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"824861d2-4576-1993-e5b5-823509c1216e"},"outputs":[],"source":"# Let us separate our models by breaking our dataframes based on y values on the set threshold"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"447cc7e7-640e-3e50-b880-efeee9a0b318"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n    # Input data files are available in the \"../input/\" directory.\n    # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom subprocess import check_output\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model as lm \nfrom sklearn.metrics import r2_score\n# Any results you write to the current directory are saved as output.\n\ndf_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndf_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n\ndef feature_summary(data):\n    n_row=data.shape[0]\n    features=pd.DataFrame()\n    features_names=[]\n    features_type = []\n    features_counts=[]\n    features_missing=[]\n    names=data.columns.values\n    for i in names:\n        features_names.append(i)\n        features_type.append(type(data.ix[1,i]))\n        features_counts.append(data[i].value_counts().count())\n        features_missing.append(data[data[i].isnull()].shape[0])\n    features['name']=features_names\n    features['type'] = features_type\n    features['value counts']=features_counts\n    features['missing']=features_missing\n    features['percentage_missing']=features['missing']/n_row\n    return (features)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77a6a2c7-a973-7933-0c74-a99cf2a647b8"},"outputs":[],"source":"def normalize(test_series):\n    normalized = np.log(test_series)\n    return normalized\ndef denormalize(test_series):\n    denormalized = np.exp(test_series)\n    return denormalized"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8710879-50ea-c947-fa3b-1a4524902a6b"},"outputs":[],"source":"y_log = normalize(df_train['price_doc'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ede2540-bbb9-dc33-0e57-aeb664012ab1"},"outputs":[],"source":"#column_names = feature_summ[feature_summ['percentage_missing'] < 0.20]\ndf_train_quant = df_train.select_dtypes(include=['int64', 'floating'])\ncorr_mat = pd.DataFrame()\ncolumn_names = np.arange(0)\ncorrelation_values = np.arange(0)\nfor i in df_train_quant.columns.values:\n    column_names = np.append(column_names, i)\n    correlation_values = np.append(correlation_values,df_train_quant[i].corr(df_train_quant['price_doc']))\ncorr_mat['columns'] = column_names\ncorr_mat['value'] = correlation_values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed952248-36ef-b93a-bf38-7c7cb99d9c1c"},"outputs":[],"source":"# using the above poorly constructed plot, and if you look closely you'll see why i chose 0.20\n# as a theshold\ncorr_mat_select = corr_mat[corr_mat['value'].abs() > 0.20 ]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42bd2f62-e888-f46c-b1b3-2eb2caffe488"},"outputs":[],"source":"# let us use linear regression to solve this\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model as lm \ndf_train_quant = df_train[corr_mat_select['columns']]\n#handling missing values using mean values? we can explore more methods for this at a later\n#stage\n#df_train_quant = df_train_quant.drop('timestamp', 1)\nfor i in range(1,len(df_train_quant.columns)):\n       df_train_quant.iloc[:,i] = df_train_quant.iloc[:,i].fillna(df_train_quant.iloc[:,i].mean())\ny_normalized = normalize(df_train_quant['price_doc'])\ny_original = df_train_quant['price_doc']\ndf_train_quant = df_train_quant.drop('price_doc', 1)\nX_train,X_test,y_train,y_test = train_test_split(df_train_quant,y_normalized,test_size=0.20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f157951-fc93-bf39-d268-6554d9875768"},"outputs":[],"source":"model = lm.LinearRegression()\nmodel.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30009bb9-f9a0-a094-0d5f-73b3b2fa5c0d"},"outputs":[],"source":"predy = model.predict(X_train)\nfrom sklearn.metrics import r2_score\nr2_score(denormalize(predy), denormalize(y_train))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7922c663-5c41-5f9f-8446-519971235852"},"outputs":[],"source":"denormalize(predy)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d03398d-3482-4e01-601b-5582c8547fa6"},"outputs":[],"source":"denormalize(y_train)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}