{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7a4c8ae0-f11b-388f-eda2-c3655fe38301"},"source":"Briefly look into our data to see how these attributes correlate and find some interesting insight."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0eaa294b-d8b9-4a5b-3710-33820b7654dd"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\npd.options.mode.chained_assignment = None  # default='warn'"},{"cell_type":"markdown","metadata":{"_cell_guid":"0ab19395-eaaa-4432-09bf-c0ada74f225d"},"source":"Let see how our data looks like.  \nThere are around 300 attributes!!! That's a lot!  \nAnd our predicting target, price_doc, has max 1.111e+08 with mean 7.12e+06"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"883be516-42f1-dfae-ae8c-e365935e0a9d"},"outputs":[],"source":"train_df = pd.read_csv(\"../input/train.csv\")\ntrain_df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c89ad34d-7b2b-b96b-55a7-ea97e5c6de88"},"source":"Besides data directly related to individual house, we also have macro economic data which is time-series data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a6d43a8-8e56-7dd3-1bb9-c06497c406fd"},"outputs":[],"source":"macro_df = pd.read_csv(\"../input/macro.csv\")\nmacro_df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"06bcc616-dccb-d0f5-cd44-3b93f5aab515"},"source":"Let's start from our target -> price_doc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ad2f79f-5c18-25b0-ca9f-e013c99b2519"},"outputs":[],"source":"price = train_df['price_doc']\nplt.figure(figsize=(8,4))\nsns.distplot(price, kde=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9b279f77-1ff0-b6fa-94d7-7c6dc5ebe294"},"source":"Looks our max outlier show up, let's remove it first."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e1465823-494b-b3be-bd2e-bb36f0ba5821"},"outputs":[],"source":"ulimit = np.percentile(train_df.price_doc.values, 99)\ntrain_df['price_doc'].ix[train_df['price_doc']>ulimit] = ulimit\nprice = train_df['price_doc']\nplt.figure(figsize=(8,4))\nsns.distplot(price, kde=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f041eea1-35a7-c98e-40f1-2c8d125ca1f1"},"source":"Looks better.  \nNext, we use heatmap to see top 15 correlated attributes with price_doc."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"54b9d396-1d0c-5e99-624b-2c98c8efeb8c"},"outputs":[],"source":"corrmat = train_df.corr()\nn = 15\ncols = corrmat.nlargest(n, 'price_doc')['price_doc'].index\ncm_df = train_df[cols].corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(cm_df, square=True, annot=True, fmt='.2f', annot_kws={'size':10}, cbar=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0b6c7c51-97ab-b4f1-cbff-6aa6d485141c"},"source":"Number 1 is full_sq with 0.51 score. Hmm... not very much.  \nAnd you can see other attributes below are correlated to themself.  \nLike sport_count_5000, sport_count_3000, sport_count_2000, basically those three are hight correlated with each other.  \nWe can just choose one of them for later trainning at first.  \n  \nNow we can look into full_sql and price_doc's relationship.  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"17840b5e-dd69-3d26-87ab-3aa2e3a823c2"},"outputs":[],"source":"var = 'full_sq'\ndata = pd.concat([train_df['price_doc'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='price_doc')\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"def0c95a-c9f7-3a37-42ef-e84037bdb71d"},"source":"There's a obvious outlier there.  \nLet's remove it's outlier including upside and downside."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80d83941-f21c-1dc9-e78b-716b5b88455d"},"outputs":[],"source":"ulimit = np.percentile(train_df['full_sq'].values, 99.9)\ntrimmed_df = train_df.drop(train_df[train_df['full_sq']>ulimit].index)\n\ndata = pd.concat([trimmed_df['price_doc'], trimmed_df[var]], axis=1)\ndata.plot.scatter(x=var, y='price_doc')\ntrain_df = trimmed_df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df583ca5-e58c-9f5c-adeb-c62b14461f6e"},"outputs":[],"source":"dlimit = np.percentile(train_df['full_sq'].values, 0.1)\ntrimmed_df = train_df.drop(train_df[train_df['full_sq']<dlimit].index)\n\ndata = pd.concat([trimmed_df['price_doc'], trimmed_df[var]], axis=1)\ndata.plot.scatter(x=var, y='price_doc')\ntrain_df = trimmed_df"},{"cell_type":"markdown","metadata":{"_cell_guid":"d5c1b912-e994-1ef0-e2d7-dae0f7fe6412"},"source":"Okay, so far we deal with only full_sq.  \nWe can continue to doing all this cleaning process further.  \nHowever, let's check the missing data first."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3f790e0-8f1c-e349-b2e3-d746d7c26e82"},"outputs":[],"source":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Precent'])\nmissing_data.head(50)"},{"cell_type":"markdown","metadata":{"_cell_guid":"37d30253-198f-c6bc-31b3-5eedb3cbe013"},"source":"A great part of attributes have missing data.  \nWe can remove the attributes which have certain percentage of data missing like Percent>0.2.  \nAnd for rest of attributes, we have to figure out a way to fill in the missing data if you really want to keep this attributes as a feature."},{"cell_type":"markdown","metadata":{"_cell_guid":"7f60dd6e-ef4f-6864-18b8-52a5f4180a3d"},"source":"Next, let's check some category attributes.  \nFirst is product_type:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30c66bf5-f5db-61c4-261d-e10bdbdd9060"},"outputs":[],"source":"g_p_type = train_df.groupby('product_type').mean()['price_doc']\nplt.figure(figsize=(8,4))\nsns.barplot(g_p_type.index, g_p_type.values)\nplt.ylabel('price_doc')\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd245417-f299-bc05-e8d7-9bde81cc841b"},"source":"The mean of price of investment  type is slightly higher than OwnerOccupier's.  \nHow about the number?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"142b29ea-ade1-78c2-a530-8b86945a3586"},"outputs":[],"source":"g_p_type = train_df['product_type'].value_counts()\nplt.figure(figsize=(8,4))\nsns.barplot(g_p_type.index, g_p_type.values)\nplt.ylabel('Number of Occurrences')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31437bcd-1d4e-05e3-f3ef-986d3d22fee8"},"outputs":[],"source":"Wow! People like to invest in real estate.  \nHow about the sub_area?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af9586dd-3f1a-0929-e217-c635140ece2a"},"outputs":[],"source":"sub_area_list = train_df.groupby('sub_area').mean()['price_doc'].sort_values(ascending=False)[:15]\nplt.figure(figsize=(8,4))\nsns.barplot(sub_area_list.index, sub_area_list.values)\nplt.ylabel('price_doc')\nplt.xticks(rotation=70)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b43c344-7b68-80d2-0af8-39e45c3feffa"},"outputs":[],"source":"sub_area_list = train_df.groupby('sub_area').mean()['price_doc'].sort_values(ascending=True)[:15]\nplt.figure(figsize=(8,4))\nsns.barplot(sub_area_list.index, sub_area_list.values)\nplt.ylabel('price_doc')\nplt.xticks(rotation=70)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"162c94bf-7c80-a085-c8e5-b09d8610554e"},"source":"Interesting!  \nLocation, Location, Location!  \nHope all of these are helpful.  \nLet's dig into the macro data next time."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}