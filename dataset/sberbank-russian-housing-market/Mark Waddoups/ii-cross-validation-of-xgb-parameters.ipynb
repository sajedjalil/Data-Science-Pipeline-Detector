{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f37583e8-de55-31f3-5c4c-ff3824cdd0bb"},"source":"# Cross-validation of XGB parameters\n\nI'm going to use the same workflow I used in the first kernel. For a better explanation of that, see [the previous notebook here.][1]\n\nBut as a refresher:\n\n1. We process X by removing columns with high amounts of NaN values and a basic dummy of text variables.\n2. We define the RMSLE metric for scoring.\n3. We create a pipeline to impute missing values and scale the frame.\n4. XGBoost performed the best, so we will validate based on that model.\n\nSo step 1: process the data.\n\n\n  [1]: https://www.kaggle.com/mwaddoups/sberbank-russian-housing-market/i-regression-workflow-various-models/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bffb4ed3-73fb-270f-44e2-e73d7189b043"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain = pd.merge(train, macro, how='left', on='timestamp')\n\n# Process to get X and y variables\npercent_null = train.isnull().mean(axis=0) > 0.20\ndf = train.loc[:, ~percent_null]\ndf = df.drop(['id', 'price_doc'], axis=1)\ndf['timestamp'] = pd.to_numeric(pd.to_datetime(df['timestamp'])) / 1e18\ndf = pd.get_dummies(df).astype(np.float64)\nX = df\ny = np.log(train['price_doc'])\n\n# Make our train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ec584f4d-735c-18ec-94ea-7bb89e86e5ea"},"source":"Step 2: Define the metric."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b87750cc-cd5c-3247-481b-2a9865171c8b"},"outputs":[],"source":"from sklearn.metrics import make_scorer\n\ndef rmsle_exp(y_true_log, y_pred_log):\n    y_true = np.exp(y_true_log)\n    y_pred = np.exp(y_pred_log)\n    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n\ndef score_model(model, pipe):\n    train_error = rmsle_exp(y_train, model.predict(pipe.transform(X_train)))\n    test_error = rmsle_exp(y_test, model.predict(pipe.transform(X_test)))\n    return train_error, test_error\n\nrmsle_exp_scorer = make_scorer(rmsle_exp)"},{"cell_type":"markdown","metadata":{"_cell_guid":"fee79d9d-8bc9-8c9b-39d7-d1d8738eb2f0"},"source":"Step 3: Define the preprocessing pipeline."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b54866af-3761-9db4-0873-c2ede23e1036"},"outputs":[],"source":"\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Make a pipeline that transforms X\npipe = make_pipeline(Imputer(), StandardScaler())\npipe.fit(X_train)\npipe.transform(X_train)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5a7e6d7e-017c-59d4-2b90-b47d53eec18e"},"source":"Step 4: Showcase the base performance of XGBoost on the test set."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d61327a7-9a6a-d458-c915-700337cc5201"},"outputs":[],"source":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nxgb.fit(pipe.transform(X_train), y_train)\n\nprint(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(xgb, pipe)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"a35f6dfc-f52c-1478-6c1a-60f91c6f16a8"},"source":"## Cross-validation\nWe will now use the routines available in sklearn to cross-validate this regressor. We'll use random search, since it's usually faster and converges better than grid search."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67e64649-f6a4-b69f-d341-bea968901951"},"outputs":[],"source":"from scipy.stats import randint, uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    'max_depth': randint(3, 6),\n    'learning_rate': uniform(0.01, 0.45),\n    'n_estimators': randint(50,200),\n    'min_child_weight': randint(1, 50),\n    'gamma': uniform(0, 1),\n}\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=param_dist, \n                                   n_iter=8, cv=3, scoring=rmsle_exp_scorer, \n                                   verbose=1, n_jobs=-1, refit=False)\nrandom_search.fit(pipe.transform(X_train), y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3a5896a-8c0d-bd9a-4453-c2b760c97331"},"outputs":[],"source":"print(random_search.best_params_)\npd.DataFrame(random_search.cv_results_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"269c6c19-a77c-bada-dc2c-853c225a929a"},"outputs":[],"source":"# Refit the model on everything, including our held-out test set.\npipe.fit(X)\nxgb.fit(pipe.transform(X), y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5949ba0d-5e50-007d-b072-90fae8887e09"},"outputs":[],"source":"# Apply the same steps to process the test data\ntest_data = pd.merge(test, macro, how='left', on='timestamp')\ntest_data['timestamp'] = pd.to_numeric(pd.to_datetime(test_data['timestamp'])) / 1e18\ntest_data = pd.get_dummies(test_data).astype(np.float64)\n\n# Make sure it's in the same format as the training data\ndf_test = pd.DataFrame(columns=df.columns)\nfor column in df_test.columns:\n    if column in test_data.columns:\n        df_test[column] = test_data[column]\n    else:\n        df_test[column] = np.nan\n\n# Make the predictions\npredictions = np.exp(xgb.predict(pipe.transform(df_test)))\n\n# And put this in a dataframe\npredictions_df = pd.DataFrame()\npredictions_df['id'] = test['id']\npredictions_df['price_doc'] = predictions\npredictions_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1294eb7f-70e7-5915-271e-6a9b5eb61347"},"outputs":[],"source":"# Now, output it to CSV\npredictions_df.to_csv('predictions.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}