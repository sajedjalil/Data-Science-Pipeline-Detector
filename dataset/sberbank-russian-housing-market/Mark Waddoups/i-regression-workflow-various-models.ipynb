{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"63211500-9e48-cc38-1ff7-815b45dd81b5","_active":false},"source":"# Regression preparation with basic methods\n\nWe're just going to take the simplest approach to combining all the data. This rough and ready approach will hopefully help guide what the most fruitful avenue to pursue is.\n\nComments on workflow are appreciated! \n\n## Data processing\n\nWe'll pull in all the data, combine it, take a look at some stats and do very basic preparation (with no real 'feature engineering').","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"98a6ef83-288b-ad53-58f0-fe0fff52807a","_active":true},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"defe2aba-49df-a1da-6744-3bb2afc49b15","_active":false},"outputs":[],"source":"train = pd.merge(train, macro, how='left', on='timestamp')\nprint(train.shape)\ntrain.head()","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"118d6a20-7887-ae8d-6e86-fd6853d8c8e1","_active":false},"source":"### Processing the target variable\n\nFirst let's sort out our predicted value. Often in housing price datasets there is a lot of skewness in the value to predict and taking a log gives a more normal distribution. This tends to lead to less bias in the regression. Let's examine that value here.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"170fbd78-e2cf-87e9-330f-b4ab9c2750b3","_active":false},"outputs":[],"source":"target = train['price_doc']\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n\ntarget.plot(ax=axes[0], kind='hist', bins=100)\nnp.log(target).plot(ax=axes[1], kind='hist', bins=100, color='green', secondary_y=True)\nplt.show()","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"6ae321d1-c237-1b2c-8e02-82bed8f43f76","_active":false},"source":"Looks like the same thing is present in this dataset, so we will aim to predict log(house price).","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fc349d2-f937-9035-f5a6-2eeca8406a5e","_active":false},"outputs":[],"source":"y = np.log(target)","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"30f2c8ba-3a6d-8f00-4c7d-4d947d0f0593","_active":false},"source":"\n\n### Processing the input features\n\nBased on just the top of the dataset we can see there are plenty of missing values in some columns. Let's see how bad the problem is. Since we have 490 features currently, it's likely we will be able to discard some of the features with many missing values.\n\nWe can always come up with ways to add them in later.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"284c96d8-cfeb-08b2-2298-06f205e5da29","_active":false},"outputs":[],"source":"percent_null = train.isnull().mean(axis=0) > 0.20\nprint(\"{:.2%} of columns have more than 20% missing values.\".format(np.mean(percent_null)))","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"a3be8743-d3ea-01d5-e8e9-0d947449fb05","_active":false},"source":"I'm happy to lose 5% of the features and not have to worry about a proper imputation strategy. We'll also pull out the uninformative columns.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9123954-9212-8b0b-cd4a-452ff2f86311","_active":false},"outputs":[],"source":"df = train.loc[:, ~percent_null]\ndf = df.drop(['id', 'price_doc'], axis=1)\n\nprint(df.dtypes.value_counts())\nnp.array([c for c in df.columns if df[c].dtype == 'object'])","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"595bebbb-1c0c-e74b-42ac-1f9e881e33fd","_active":false},"source":"The next question is how to handle the object data types. I'm going to make the basic assumption that all floating and integer valued columns can be treated as one-dimensional values, so there is no need to dummy these.\n\nIt's possible that you could do better processing on some of the object columns - for example, 'sub_area' could be replaced with a 2D co-ordinate vector of the area, so distance metrics make sense between each class. However, I'm just going to dummy every object variable except 'timestamp'. I'll replace 'timestamp' with a numeric value, since it makes sense to treat this as 1-dimensional and the distance is well-defined.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99ac7dc0-8abc-59f9-cb50-67b514ab1ba5","_active":false},"outputs":[],"source":"df['timestamp'] = pd.to_numeric(pd.to_datetime(df['timestamp'])) / 1e18\nprint(df['timestamp'].head())\n\n# This automatically only dummies object columns\ndf = pd.get_dummies(df).astype(np.float64)\nprint(df.shape)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"749e5732-22d6-bcbd-b98e-953aa5b975c7","_active":false},"outputs":[],"source":"X = df","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"ae7b05d8-fa34-c12b-b55c-4daacdb9a9d9","_active":false},"source":"So we have 636 features and 30,471 total observations at this point. Since the number of features is low, the challenge is going to be finding a model with high capacity, rather than necessarily adding lots of regularization at this stage. I'm not worried about overfitting yet.\n\nNow let's prepare the data for learning. To do this, we'll follow the basic steps:\n\n1. Make a train/test split.\n2. Impute values for the missing values - we will replace with the mean. \n3. Scale every value by mean and standard deviation.\n\nN.B. We are going to use the imputer class from sklearn - this doesn't support (afaik) different imputing methods for different columns. Ideally for the [0, 1] values we converted from strings, you would use the mode, but for the continuous you would use the mean. You could write a class to implement this, but again this is just a rough and ready approach.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ad819685-1587-5584-17a7-5e08e3ea50f8","_active":false},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5a3fccc-3529-8352-d484-3def0b882eee","_active":false},"outputs":[],"source":"from sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Make a pipeline that transforms X\npipe = make_pipeline(Imputer(), StandardScaler())\npipe.fit(X_train)\npipe.transform(X_train)","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"98887b63-0e65-68ce-1ab5-ffa7e494a73f","_active":false},"source":"Now finally we want a single real-valued metric for comparing our models and implementations. Handily, Kaggle already tells us what that should be - the RMSLE. I don't think a library implementation in sklearn exists for this, but it's easy to define yourself.\n\nAlso, for convenience, I will take the exponential in this function, since our model is working in log(house price).","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"844288e2-ec59-10e3-b741-27287c39af88","_active":false},"outputs":[],"source":"from sklearn.metrics import make_scorer\n\ndef rmsle_exp(y_true_log, y_pred_log):\n    y_true = np.exp(y_true_log)\n    y_pred = np.exp(y_pred_log)\n    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n\ndef score_model(model, pipe):\n    train_error = rmsle_exp(y_train, model.predict(pipe.transform(X_train)))\n    test_error = rmsle_exp(y_test, model.predict(pipe.transform(X_test)))\n    return train_error, test_error","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"d972f774-9501-c1ae-6292-b61291a80fb9","_active":false},"source":"We now have everything we need for making some predictions. Let's fit a basic linear model. I expect this to underfit the data.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0bb9d672-f223-3942-29d7-55720da4c454","_active":false},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression(fit_intercept=True)\nlr.fit(pipe.transform(X_train), y_train)\n\nprint(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(lr, pipe)))","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"0dd53edc-8f89-390d-dd7b-f10b07eb8641","_active":false},"source":"So it looks like the benchmark level for the competition was set with linear regression - it's close to our test error. \n\nI'll use a few more \"out-of-the-box\" methods here, and we can proceed from there. Specifically I'll try SVR, random forests and everyone's favourite, XGBoost.\n\nSVR allows for nice non-linearities if you use the Gaussian kernel. The downside is it takes a long time to fit the data. You also should run cross-validation on the parameters C (the regularization parameter) and what sklearn calls 'gamma', the standard deviation of the kernel. Here, I'll just use the defaults.\n\n**Note: Due to high runtime, I have disabled this run of SVR. Suffice to say, it performs worse than the forests but better than LR, with train/test errors of around 0.48.**","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e635ecf0-d8c7-9773-85ec-75d730380bc3","_active":false},"outputs":[],"source":"#from sklearn.svm import SVR\n\n#svr = SVR()\n#svr.fit(pipe.transform(X_train), y_train)\n\n#print(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(svr, pipe)))\nprint(\"Train error: ~0.48, Test error: ~0.48\")","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"934aa30f-867b-4229-91bd-d71dc97e6942","_active":false},"source":"The next two will be tree models. I'll use the same n_estimators with both. Random forests will overfit unless you set min_samples_leaf to a reasonable value, so I picked 50 relatively arbitrarily. Again, cross validation can help determine better than default settings for this and many other parameters.","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a68050b1-2b52-8b51-d22e-0fa3d1caf708","_active":false},"outputs":[],"source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100, min_samples_leaf=50, n_jobs=-1)\nrfr.fit(pipe.transform(X_train), y_train)\n\nprint(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(rfr, pipe)))","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e5caa58-e914-b3a6-4ae2-431469bb07a3","_active":false},"outputs":[],"source":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nxgb.fit(pipe.transform(X_train), y_train)\n\nprint(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(xgb, pipe)))","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"09d7ea3e-0e3f-b172-e542-d00b8833bf09","_active":false},"source":"So in just base implementations, XGBoost is (very slightly) the winner. Shocking, I know.\n\nOf course, there are many steps you can take to improve any of these models. For example:\n\n1. Engineer better features from the data.\n2. Use some of the features we threw out at the start.\n3. Cross-validate on the many parameters the more complicated models have.\n4. Try a model we haven't used yet (deep network, polynomial features in regression)\n\nBut this notebook should serve as a baseline workflow!\n\nAnd finally, basic code for submission below:","outputs":[],"execution_count":null,"execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01a267cd-2501-c392-a740-a2962269480f","_active":false},"outputs":[],"source":"# Refit the model on everything, including our held-out test set.\npipe.fit(X)\nxgb.fit(pipe.transform(X), y)","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27dee4b2-71f8-fa4b-577c-6923e8ee25f5","_active":false},"outputs":[],"source":"# Apply the same steps to process the test data\ntest_data = pd.merge(test, macro, how='left', on='timestamp')\ntest_data['timestamp'] = pd.to_numeric(pd.to_datetime(test_data['timestamp'])) / 1e18\ntest_data = pd.get_dummies(test_data).astype(np.float64)\n\n# Make sure it's in the same format as the training data\ndf_test = pd.DataFrame(columns=df.columns)\nfor column in df_test.columns:\n    if column in test_data.columns:\n        df_test[column] = test_data[column]\n    else:\n        df_test[column] = np.nan\n\n# Make the predictions\npredictions = np.exp(xgb.predict(pipe.transform(df_test)))\n\n# And put this in a dataframe\npredictions_df = pd.DataFrame()\npredictions_df['id'] = test['id']\npredictions_df['price_doc'] = predictions\npredictions_df.head()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bf2df46-1a50-300e-304f-21b2ed1294dc","_active":false},"outputs":[],"source":"# Now, output it to CSV\npredictions_df.to_csv('predictions.csv', index=False)","execution_state":"idle"}]}