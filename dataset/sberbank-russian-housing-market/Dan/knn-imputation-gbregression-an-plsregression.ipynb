{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d9e53332-bc26-4c84-e665-c2010ecb13f3"},"source":"This is a quick kernel with kNN imputation and a GradientBoostingRegressor that achieved ~.33 on the LB. \n\nNOTE: I am having some issues running this on Kaggle so I apologize if errors are currently visible. \nFancyImpute is easier to work with offline."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a3b685a1-d350-5af1-cc71-816867dfe038"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\npd.set_option(\"display.max_columns\",len(test))\ntrain.shape,test.shape\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33147a90-6b7b-3ab2-fc24-0ee910f28cfe"},"outputs":[],"source":"missing_vals = pd.concat([train.isnull().sum()/len(train),test.isnull().sum()/len(test)],axis=1, keys=['Train','Test'])\nmissing_vals.sort_values(ascending=False,by=\"Train\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ee75ff64-88bf-e39f-4120-07c38a93979d"},"outputs":[],"source":"import datetime\n\ndef time_stamp(df):\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    df[\"year\"] = df[\"timestamp\"].dt.year\n    df[\"month\"] =  df[\"timestamp\"].dt.month\n    df[\"day\"] = df[\"timestamp\"].dt.day \n    del df[\"timestamp\"]\n\ntime_stamp(train)\ntime_stamp(test)\n\n\n\ntrain_cont = [x for x in train.columns if train.dtypes[x] != 'object' and (x not in ['day','month','year'])]\ntest_cont = [x for x in test.columns if test.dtypes[x] != 'object' and (x not in ['day','month','year'])]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab8a9f47-ff79-dfeb-fd4e-385add113146"},"outputs":[],"source":"\"\"\"Examining Correlations\"\"\"\npd.DataFrame(train.corr()[\"price_doc\"].sort_values(ascending=False))"},{"cell_type":"markdown","metadata":{"_cell_guid":"fcf88bdc-e953-9c15-89ca-e17032b00687"},"source":"Based on missing values and correlations, the following variables were removed to reduce noise."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af35edab-357a-fc54-1aed-66d913f0ba09"},"outputs":[],"source":"del_low_corr = [\"trc_sqm_5000\",\"prom_part_500\",\"build_count_1971-1995\",\n               \"school_quota\",\"ID_railroad_station_walk\",\"cemetery_km\",\"water_km\",\n               \"big_church_count_500\",\"cafe_sum_3000_max_price_avg\",\n               \"cafe_avg_price_3000\",\"cafe_sum_3000_min_price_avg\",\n               \"build_count_1921-1945\",\"16_29_male\",\n               \"female_f\",\"full_all\",\"ID_bus_terminal\",\"ID_railroad_station_avto\",\n               \"ID_big_road1\",\"ID_big_road2\",\"trc_count_500\",\"trc_count_1000\",\n               \"trc_sqm_1500\",\"cafe_count_500_price_4000\",\"cafe_count_500_price_2500\",\n               \"market_count_5000\",\"hospital_beds_raion\",\n               \"cafe_avg_price_500\",\"cafe_sum_500_max_price_avg\",\n               \"cafe_sum_500_min_price_avg\",\"preschool_quota\",\"cafe_count_1500\"]\nfor i in del_low_corr:\n    del train[i]\n    del test[i]\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68eee4d7-4731-6ca3-1f74-1f420b644462"},"outputs":[],"source":"for i in train.skew().keys():\n    if abs(train.skew()[i]) > .5 and (i!= 'price_doc'):\n        train[i] = np.log1p(train[i])\n        test[i] = np.log1p(test[i])\n        print(\"Just finished {} with skew {}\".format(str(i), str(train.skew()[i])))\n        #Distribution plots if needed\n        #sns.distplot(train[i].dropna())\n        #plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21b07987-711e-262c-2219-bf5c8bbad48a"},"outputs":[],"source":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)\n\nsns.distplot(train.price_doc)\nplt.title(\"price\")\nplt.show()\n\n\ntrain.price_doc = np.log1p(train.price_doc)\n\nsns.distplot(train.price_doc)\nplt.title(\"log transformed price\")\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ebfbe12-5e83-123f-c357-b47649398c21"},"source":"Slight improvements are seen in the distribution after the transformation from skew  > 1.7 to -.88\nThis should not heavily effect GBR model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4266e8fe-8b39-ad78-87da-a44d28ee13a2"},"outputs":[],"source":"\"\"\"from fancyimpute import  KNN\n\ntrain.material = train.material.fillna(1) #As none are missing in test.\n\ntrain_columns = list(train) #fancyimpute removes var names\ntest_columns = list(test)\n\ntrain = pd.DataFrame(KNN(k=3).complete(train))\ntest = pd.DataFrame(KNN(k=3).complete(test))\n\ntrain.columns = train_columns \ntest.columns = test_columns\n\n#Make material an object again. \ntrain.material = train.material.astype(\"object\")\ntest.material = test.material.astype(\"object\")\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)\"\"\""},{"cell_type":"markdown","metadata":{"_cell_guid":"e85b8b7d-9aa4-d738-dd0f-53ae64fccf99"},"source":"Since the data set has high multicollinearity, a PLSRegression model will be fit to the data, followed by a GBR"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ae8ba45f-50c6-0072-4514-c1cd71f5bead"},"outputs":[],"source":"\"\"\"my_ids = train['id']\ntest_id = test['id']\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.preprocessing import scale\nX = np.array(train.drop([\"price_doc\",\"id\"],axis=1))\ny = np.array(train[\"price_doc\"])\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .25,random_state=1)\n\npls = PLSRegression(n_components=20,scale=False)\npls.fit(X_train,y_train)\ny_pred = pls.predict(X_test)\nr2_score(y_test,y_pred),mean_squared_error(y_test,y_pred)\"\"\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dd05f53d-b287-1776-b025-24cf916e4fc6"},"outputs":[],"source":"from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(loss='huber',learning_rate=.05,n_estimators=500,\n                              max_features='sqrt',min_samples_leaf=10)\n                                #max_features = 'sqrt' for multicollinearity.\n\"\"\"gb.fit(X_train,y_train)\ngb_pred = gb.predict(X_test)\nr2_score(y_test,gb_pred),mean_squared_error(y_test,gb_pred)\"\"\""},{"cell_type":"markdown","metadata":{"_cell_guid":"ea430526-f203-df4f-0160-3e14dfdbe5c6"},"source":"the fancyimpute kNN class does not seem to be working on Kaggle, but un-commenting the above code will work as a starter implementation for those wishing to implement this in Python."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ba2dce9-ef96-4943-bd94-1544c72d1230"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}