{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"code","execution_count":17,"metadata":{"_cell_guid":"328a17d7-e6d0-3e76-eacb-dd5145bb2b5c","_active":false,"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as pl\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport xgboost as xgb\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n# Pretty display for notebooks\n%matplotlib inline\n\n# Any results you write to the current directory are saved as output.","execution_state":"idle"},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"aea04415-07e4-4ff7-f2df-cf01529d1b52","_active":false},"outputs":[],"source":"data = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\nprices = data['price_doc']\nfeatures_raw = data.drop(['price_doc','id'], axis =1)","execution_state":"idle"},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"9b0f0e81-670e-8d8d-07a5-7aac7b99bc69","_active":false},"outputs":[],"source":"data.info()","execution_state":"idle"},{"cell_type":"code","execution_count":40,"metadata":{"_cell_guid":"fd56772c-b47e-495b-e4f5-3550b79bf5bf","_active":false,"collapsed":false},"outputs":[],"source":"# Import sklearn.preprocessing.StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfeatures_raw.fillna(0, inplace =True)\n# Initialize a scaler, then apply it to the features\nscaler = MinMaxScaler((0,1e6))\nnumerical = features_raw.select_dtypes(include=['float64','int64']).keys()\nfeatures_raw[numerical] = scaler.fit_transform(features_raw[numerical])\n\n# Show an example of a record with scaling applied\nprint (features_raw.head(n = 1))","execution_state":"idle"},{"cell_type":"code","execution_count":41,"metadata":{"_cell_guid":"8e506b51-f2b6-a6dc-89ae-99c1e6015f85","_active":false,"collapsed":false},"outputs":[],"source":"#One-hot encode the 'features' data using pandas.get_dummies()\nobjects = features_raw.select_dtypes(include=['object']).keys()\nlbl = LabelEncoder()\nfor col in objects:\n        lbl.fit(list(features_raw[col].values)) \n        features_raw[col] = lbl.transform(list(features_raw[col].values))\n# remove boolean with _no\nfeatures = features_raw\nfeatures = features.drop(features.filter(regex='_no', axis=1),axis=1)\nfeatures.drop('timestamp',axis=1,inplace =True)\n# Print the number of features after one-hot encoding\nencoded = list(features.columns)\nprint (\"{} total features after one-hot encoding.\".format(len(encoded)))\n\n# Uncomment the following line to see the encoded feature names\nprint (encoded)","execution_state":"idle"},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"fd2698f2-037e-b827-6f14-b52ee41614b9","_active":false},"outputs":[],"source":"#Minimum price of the data\nminimum_price = np.min(prices)\n\n#Maximum price of the data\nmaximum_price = np.max(prices)\n\n#Mean price of the data\nmean_price = np.mean(prices)\n\n#Median price of the data\nmedian_price = np.median(prices)\n\n#Standard deviation of prices of the data\nstd_price = np.std(prices)\n\n# Show the calculated statistics\nprint (\"Statistics for russian housing dataset:\\n\")\nprint (\"Minimum price: Rub {:,.2f}\".format(minimum_price))\nprint (\"Maximum price: Rub {:,.2f}\".format(maximum_price))\nprint (\"Mean price: Rub {:,.2f}\".format(mean_price))\nprint (\"Median price Rub {:,.2f}\".format(median_price))\nprint (\"Standard deviation of prices: Rub {:,.2f}\".format(std_price))\n#added statistic for critica alfa = 0.05 double tailed cutover\n#calculating standard error using correction for sample size n = 100\nalfa_c = (std_price/99)*1.96\nprint (\"critical alfa score for sample size of 100: Rub {:,.2f}\".format(alfa_c))","execution_state":"idle"},{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"51b345e9-4d4a-d1dd-5d81-c61f593f6c85","_active":false},"outputs":[],"source":"from sklearn.metrics import r2_score\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    score = r2_score(y_true, y_predict) \n    return score","execution_state":"idle"},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"61021e34-754a-cc1b-c8b7-7ccb948f92cb","_active":false},"outputs":[],"source":null,"execution_state":"idle"},{"cell_type":"code","execution_count":22,"metadata":{"_cell_guid":"0d2ab7da-e11c-a9b3-4712-3a2ca565ec1b","_active":false},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n#Shuffle and split the data into training and testing subsets\nX_train, X_test, y_train, y_test = train_test_split(features,prices, test_size =0.2, random_state = 33)\nprint (\"Training and testing split was successful.\")","execution_state":"idle"},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"186fd04e-683d-022a-58ac-81166f3286cc","_active":false},"outputs":[],"source":"# Produce learning curves for varying training set sizes and maximum depths\nfrom sklearn.model_selection import ShuffleSplit, train_test_split,\nfrom sklearn.model_selection import learning_curve as curves\n\nfrom sklearn.tree import DecisionTreeRegressor\ndef ModelLearning(X, y):\n    \"\"\" Calculates the performance of several models with varying sizes of training data.\n        The learning and testing scores for each model are then plotted. \"\"\"\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Generate the training set sizes increasing by 50\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n\n    # Create the figure window\n    fig = pl.figure(figsize=(10,7))\n\n    # Create three different models based on max_depth\n    for k, depth in enumerate([1,3,6,10]):\n        \n        # Create a Decision tree regressor at max_depth = depth\n        regressor = DecisionTreeRegressor(max_depth = depth)\n\n        # Calculate the training and testing scores\n        sizes, train_scores, test_scores = curves(regressor, X, y, \\\n            cv = cv, train_sizes = train_sizes, scoring = 'r2')\n        \n        # Find the mean and standard deviation for smoothing\n        train_std = np.std(train_scores, axis = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n\n        # Subplot the learning curve \n        ax = fig.add_subplot(2, 2, k+1)\n        ax.plot(sizes, train_mean, 'o-', color = 'r', label = 'Training Score')\n        ax.plot(sizes, test_mean, 'o-', color = 'g', label = 'Testing Score')\n        ax.fill_between(sizes, train_mean - train_std, \\\n            train_mean + train_std, alpha = 0.15, color = 'r')\n        ax.fill_between(sizes, test_mean - test_std, \\\n            test_mean + test_std, alpha = 0.15, color = 'g')\n        \n        # Labels\n        ax.set_title('max_depth = %s'%(depth))\n        ax.set_xlabel('Number of Training Points')\n        ax.set_ylabel('Score')\n        ax.set_xlim([0, X.shape[0]*0.8])\n        ax.set_ylim([-0.05, 1.05])\n    \n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc='lower left', borderaxespad = 0.)\n    fig.suptitle('Decision Tree Regressor Learning Performances', fontsize = 16, y = 1.03)\n    fig.tight_layout()\n    fig.show()","execution_state":"idle"},{"cell_type":"markdown","metadata":{"_cell_guid":"c53d19ee-04b4-58ed-0ed6-0850e06409c5","_active":false},"source":"### Number of training points\nit looks that the number of training points is sufficient for the model since we can see it's plateauing ","outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"_cell_guid":"6a7caf93-9b41-161b-1b22-7670e5ef30d0","_active":false},"outputs":[],"source":"ModelLearning(features, prices)","execution_state":"idle"},{"cell_type":"code","execution_count":11,"metadata":{"_cell_guid":"b9e0e09c-1f55-b6bb-359e-6d7c8a08b9e1","_active":false},"outputs":[],"source":"from sklearn.model_selection import validation_curve\ndef ModelComplexity(X, y):\n    \"\"\" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. \"\"\"\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the max_depth parameter from 1 to 10\n    max_depth = np.arange(1,11)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = validation_curve(DecisionTreeRegressor(), X, y, \\\n        param_name = \"max_depth\", param_range = max_depth, cv = cv, scoring = 'r2')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    pl.figure(figsize=(7, 5))\n    pl.title('Decision Tree Regressor Complexity Performance')\n    pl.plot(max_depth, train_mean, 'o-', color = 'r', label = 'Training Score')\n    pl.plot(max_depth, test_mean, 'o-', color = 'g', label = 'Validation Score')\n    pl.fill_between(max_depth, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = 'r')\n    pl.fill_between(max_depth, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = 'g')\n    \n    # Visual aesthetics\n    pl.legend(loc = 'lower right')\n    pl.xlabel('Maximum Depth')\n    pl.ylabel('Score')\n    pl.ylim([-0.05,1.05])\n    pl.show()","execution_state":"idle"},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"a94c8c66-816e-c46b-367b-4efc9137578d","_active":false},"outputs":[],"source":"ModelComplexity(X_train, y_train)","execution_state":"idle"},{"cell_type":"code","execution_count":42,"metadata":{"_cell_guid":"fa521173-cecc-585e-3a03-be4f9be0b656","_active":false,"collapsed":false},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\ndtrain = xgb.DMatrix(features, prices)","execution_state":"idle"},{"metadata":{"_cell_guid":"0857f1c7-29e7-c632-f696-0f47d18d3430","_active":false,"collapsed":false},"source":"cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()","execution_count":43,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"522f97b6-cd7a-e981-180d-8b589427dc58","_active":false,"collapsed":false},"source":"num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)","execution_count":44,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"5172d7e2-7f96-365f-8475-45173982d5f5","_active":false,"collapsed":false},"source":"fig, ax = pl.subplots(1, 1, figsize=(8, 13))\nxgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)","execution_count":45,"cell_type":"code","outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":46,"metadata":{"_cell_guid":"451eca06-2f98-5208-9e9e-4480f3048567","_active":true,"collapsed":false},"outputs":[],"source":"#prepare test data:\ntest_data = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\nid_test = test_data.id\ntest_features_raw = test_data.drop(['id'], axis =1)\ntest_features_raw.fillna(0, inplace =True)\ntest_features_raw[numerical] = scaler.transform(test_features_raw[numerical])\nfor col in objects:\n        lbl.fit(list(test_features_raw[col].values)) \n        test_features_raw[col] = lbl.transform(list(test_features_raw[col].values))\n# remove boolean with _no\ntest_features = test_features_raw\ntest_features = test_features.drop(test_features.filter(regex='_no', axis=1),axis=1)\ntest_features.drop('timestamp',axis=1,inplace =True)\nTest_matrix = xgb.DMatrix(test_features)","execution_state":"idle"},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"e9a8d91f-db17-f1cd-4e4f-bc6a5ab2ce75","_active":false},"outputs":[],"source":"test_features.head()","execution_state":"idle"},{"cell_type":"code","execution_count":47,"metadata":{"_cell_guid":"6a336b42-908d-6214-c4e0-2e9f194f8154","_active":false,"collapsed":false},"outputs":[],"source":"y_pred = model.predict(Test_matrix)\n\ndf_submit = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\ndf_submit.to_csv('submit.csv', index=False)","execution_state":"idle"}]}