{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a366bffb-96aa-2377-ac58-ab1ceabea0b5"},"source":"# Exploratory Data Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1212742a-7365-a1dc-9f35-ea772943d1c6"},"outputs":[],"source":"#imports\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#helpers\nsigLev = 3\nsns.set_style(\"dark\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ac8c139-8434-f525-5b26-8b330d1aee9f"},"outputs":[],"source":"#load in dataset\ntrainFrame = pd.read_csv(\"../input/train.csv\")\ntestFrame = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"e083c758-ba27-94fd-43fb-b0baf7261d12"},"source":"# Metadata Analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a23a550-5a3f-09eb-778a-4b68c5d3629b"},"outputs":[],"source":"trainFrame.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ff7cb2d-94e1-0864-3277-0525423c0ded"},"source":"We see that we have about 292 features for each of our 30471 observations. this is a large feature set, and we may need to do some forms of dimensionality reduction in order to get this feature set into a more reasonable shape."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a2d9931-3572-c1bd-95d3-ba3c8f602627"},"outputs":[],"source":"trainFrame.dtypes"},{"cell_type":"markdown","metadata":{"_cell_guid":"8b93a676-73be-04b4-2b47-fac75b2ecfe6"},"source":"Thankfully, it looks like most of our variables are quantitative, which makes choosing a dimensionality reduction method relatively easier than having to deal with many interspersed categorical variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d92905ae-a45a-23ed-99fa-094085fdc23c"},"outputs":[],"source":"trainFrame.isnull().sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9c4e15c4-a2b3-1045-e9a1-98f81b822029"},"source":"We see that we have missing values for many components in our dataset. Let's see how our dimensionality would be reduced if we were to remove variables with missing values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e76647fa-b971-8454-442f-a69d4fcf0fe0"},"outputs":[],"source":"numMissing = trainFrame.isnull().sum()\nnumWithMissingObs = numMissing[numMissing > 0].shape[0]\nprint(numWithMissingObs)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cd6e0fe7-4668-f356-bb99-aa9d4662016b"},"source":"It looks like we only have 51 of the over 200 variables that would be removed from consideration if we were to not consider variables with missing values. For the sake of simplification, let us drop these variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9918013-d28c-a906-d2f5-115b65042143"},"outputs":[],"source":"colsWithMissingObs = numMissing[numMissing > 0].index\nfilteredTrainFrame = trainFrame.drop(colsWithMissingObs,axis = 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0cf17805-b12c-3440-87a4-d004e6b93390"},"outputs":[],"source":"filteredTrainFrame.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"bd017bfd-94cf-a6a0-d68e-15a1a97e8d80"},"source":"Let's now filter out variables that have little to no variation in our dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"336acd88-6cbc-a225-f1bd-a767ac98bc2e"},"outputs":[],"source":"sdVec = filteredTrainFrame.std()\nsdVec = sdVec.sort_values()\nsdVec"},{"cell_type":"markdown","metadata":{"_cell_guid":"bdbd0ada-937d-b696-a88f-0eef1b0917b2"},"source":"We see we have one variable that seems to have an unusually low amount of variance for our dataset. Given that it is a single variable, it doesn't entirely make sense to go through the trouble of removing it when there are other feature reduction methods that will likely reduce it anyway."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"738f14df-9056-bdf6-f1eb-a13cf16f1ace"},"outputs":[],"source":"timeCountFrame = filteredTrainFrame.groupby(\"timestamp\")[\"timestamp\"].count()\n#then plot\ntimeCountFrame.plot()\nplt.xlabel(\"Time Stamp\")\nplt.ylabel(\"Count\")\nplt.title(\"Observations over Time For Training Data\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"59d81b2d-f4a0-0210-74a0-22c0a787879f"},"source":"We do see a peak in observations around 2014, although given that there are tens of thousands of observations in this dataset, these peaks do not look too substantial. Thus, I wouldn't worry about a particular time bias unless our training set is substantially  different in time periods."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"990397ec-89d9-a3e9-0a1c-86a5460b48de"},"outputs":[],"source":"timeCountFrame = testFrame.groupby(\"timestamp\")[\"timestamp\"].count()\n#then plot\ntimeCountFrame.plot()\nplt.xlabel(\"Time Stamp\")\nplt.ylabel(\"Count\")\nplt.title(\"Observations over Time For Test Data\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc7caad4-d02c-0597-1ca5-d8c153113276"},"source":"We do see that the test data features time points that are much later than our current time points. Perhaps we will need to account for some time varying factors in order to predict future household properties."},{"cell_type":"markdown","metadata":{"_cell_guid":"b3fda0dd-97f4-b982-c95b-e38d9ee15e1b"},"source":"# Dimensionality Exploration\n\nIn order to best wield this large amount of data, it is very likely that we will need to filter this data into very key components. This data isn't quite large enough that an $L_1$ or $L_2$ regression is immediately necessary, and so I want to see how manageable a PCA can be on this dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ec82495-8dc2-a59d-04d8-296df40edbf0"},"outputs":[],"source":"filteredTrainFrame = filteredTrainFrame.drop(\"timestamp\",axis = 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ab00b9e0-1be3-ab33-2f79-b48683e66347"},"outputs":[],"source":"priceDocVec = filteredTrainFrame[\"price_doc\"]\nfilteredTrainFrame = filteredTrainFrame.drop(\"price_doc\",axis = 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30a12cdc-acf8-8dd5-5ae9-41441b6644f7"},"outputs":[],"source":"from sklearn.decomposition import PCA\ntestPCA = PCA()\ntestPCA.fit(filteredTrainFrame)"},{"cell_type":"markdown","metadata":{"_cell_guid":"30087786-2820-4462-687b-276b1f372741"},"source":"Despite my original assumptions, it looks like we have some categorical data in this dataset. This suggests to me that I need to edit my previous sections in order to account for this issue.\n\nNeed to fix:\n\n* Need to study how to re-encode strings in this dataset."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e20bd2c-cd59-5889-a633-f1bbbd126fea"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}