{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1511075-2040-1acf-08f8-db4ab6bed4e1"},"source":"Like probably everyone else in this contest, I've been scratching my head: what to do about validation? The usual approaches don't work (obviously regular cross validation is a bad idea due to the time dimension, the score on last year of data is nowhere near the LB). Solution? Adversarial validation, inspired by FastML:\n\n[Adversarial validation][1]\n\nThe general idea is to check the degree of similarity between training and tests in terms of feature distribution: if they are difficult to distinguish, the distribution is probably similar and the usual validation techniques should work. It does not seem to be the case, so we can suspect they are quite different. This intuition can be quantified by combining train and test sets, assigning 0/1 labels (0 - train, 1-test) and evaluating a binary classification task.\n\n\n  [1]: http://fastml.com/adversarial-validation-part-two/"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bcbf5e1a-6442-47ba-5435-7053db612903"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f4ffc92-a776-6715-b7f3-925f1547d4fc"},"outputs":[],"source":"# We start by loading the training / test data and combining them with minimal preprocessing necessary\n# Most of the data preparation is taken from here: \n# https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\nxtrain = pd.read_csv('../input/train.csv')\nid_train = xtrain['id']\ntime_train = xtrain['timestamp']\nytrain = xtrain['price_doc']\nxtrain.drop(['id', 'timestamp', 'price_doc'], axis = 1, inplace = True)\nxtrain.fillna(-1, inplace = True)\n\n\nxtest = pd.read_csv('../input/test.csv')\nid_test = xtest['id']            \ntime_test = xtest['timestamp']\nxtest.drop(['id', 'timestamp'], axis = 1, inplace = True)\n\n# add identifier and combine\nxtrain['istrain'] = 1\nxtest['istrain'] = 0\nxdat = pd.concat([xtrain, xtest], axis = 0)\n\n# convert non-numerical columns to integers\ndf_numeric = xdat.select_dtypes(exclude=['object'])\ndf_obj = xdat.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \nxdat = pd.concat([df_numeric, df_obj], axis=1)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d0d09367-f024-ae7e-82ce-e75d53780274"},"source":"Define a split and the model (xgboost, what else :-)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"891a4cdc-d085-b67f-45e7-dcdc3ca0f8b2"},"outputs":[],"source":"skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 44)\nxgb_params = {\n        'learning_rate': 0.05, 'max_depth': 4,'subsample': 0.9,\n        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n        'silent': 1, 'n_estimators':100, 'gamma':1,\n        'min_child_weight':4\n        }   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)     "},{"cell_type":"markdown","metadata":{"_cell_guid":"e103b1ab-479c-777d-ee19-a9f3a572a7c4"},"source":"Calculate the AUC for each fold"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"223a095a-712f-fa90-707a-ea34fa091fc0"},"outputs":[],"source":"for train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))"},{"cell_type":"markdown","metadata":{"_cell_guid":"394561c3-a9d5-d3ad-31a0-801a8e7e2c42"},"source":"As we can see, the separation is almost perfect - which strongly suggests that the train / test rows are very easy to distinguish even for an xgboost"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}