{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"dacabc83-ace5-9ac3-2224-b4942b5446e9"},"source":"# Exploration of Sberbank Housing Data, Part I\n\nThis notebook partially documents my thought process as I do some data cleaning and basic initial feature engineering."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67818350-8ed0-7526-e40d-68c447319780"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')\n\ndfa = pd.concat([train, test])\ndfa = dfa.merge(macro, on='timestamp', suffixes=['','_macro'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"8cfc9033-a013-b8b1-c27a-0211a2ae6a89"},"source":"I've combined the training and test sets, because any data cleaning we do will have to be done the same way in both sets, and it will have to deal with any problems that occur in either set.  I'll make use of the following function (based in part on code from [Mark Waddopus](https://www.kaggle.com/mwaddoups/sberbank-russian-housing-market/i-regression-workflow-various-models)) to examine variables: "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"141ea85a-dcca-1e99-5a3b-b6f7f1000333"},"outputs":[],"source":"def describe(varname=\"price_doc\", df=dfa, minval=-1e20, maxval=1e20, \n             addtolog=1, nlo=8, nhi=8, dohist=True, showmiss=True):\n  var = df[varname]\n\n  print(\"DESCRIPTION OF \", varname, \"\\n\")\n  if (showmiss):\n     print(\"Fraction missing = \", var.isnull().mean(), \"\\n\")\n  var = var[(var<=maxval) & (var>=minval)]\n  if (nlo > 0):\n    print(\"Lowest values:\\n\", var.sort_values().head(nlo).values, \"\\n\")\n  if (nhi > 0):\n    print(\"Highest values:\\n\", var.sort_values().tail(nhi).values, \"\\n\")\n\n  if (dohist):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\n\n    print(\"Histograms of raw values and logarithm\")\n    var.plot(ax=axes[0], kind='hist', bins=100)\n    np.log(var+addtolog).plot(ax=axes[1], kind='hist', bins=100, color='green', secondary_y=True)\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a5c4f56-9cfe-e5b1-f9fc-6ca3391ba47e"},"outputs":[],"source":"print(dfa.shape)\ndfa.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e79322a3-5c61-1c92-e79e-f164e5614d28"},"outputs":[],"source":"print(\"The names of all 392 columns:\\n\\n\", dfa.columns.values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"09628407-2855-e87b-b6e7-699b7ea4e101"},"source":"That's a lot.  Let's start with the target variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e95eeaa2-532d-631e-6f7c-6223555efe4a"},"outputs":[],"source":"describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"6c4623a4-0621-2139-5460-9d655328b837"},"source":"So price_doc looks pretty well-behaved.  There are spikes at round numbers (e.g., 13.8 is the natural log of 1e6), but that's not likely to be a problem.  I will use a logarithm (and I also intend to normalize by some macro variable like CPI, but that's a later issue)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23fcfc67-d722-f5da-5df0-7020b35ea334"},"outputs":[],"source":"describe(\"full_sq\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"65dbccdb-8a40-4807-149c-69818f5ced40"},"source":"There are ones and zeros, which are nonsense in principle but may have special meanings that we don't know about.  There's also one extreme outlier on the high end.  Let's see what the variable looks like without those strange values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21109571-36bc-a61f-018e-034fbf21e5d1"},"outputs":[],"source":"describe(\"full_sq\", minval=1.5, maxval=1000, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cc9bc94c-f3a7-5c75-01a1-3bf37ed90618"},"source":"I'm going to draw a line between 2 and 5.  Can you live in 2 square meters?  Maybe, but the main cluster of values seems to begin with 5, so I'm going to assume the 2 is a nonsense value.  Hence..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"506e1f69-f701-9dd7-e210-81cfc1110595"},"outputs":[],"source":"describe(\"full_sq\", minval=3, maxval=1000, nhi=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0a3fea58-2aab-05a4-5088-7077cfb9593d"},"source":"That logarithm histogram looks pretty nice.  Wrap it up.  I'll take it.  But what's that weird spike?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a04a440b-df55-6c83-ed6f-30fb5333fc2b"},"outputs":[],"source":"describe(\"full_sq\", minval=25, maxval=60, nhi=0, nlo=0, showmiss=False)\nprint(\"Mode is \", dfa.full_sq.mode().values[0] )"},{"cell_type":"markdown","metadata":{"_cell_guid":"48c068a3-1c2a-82fc-3900-b7419fde35cb"},"source":"Apparently the real answer to the ultimate question of life, the universe, and everything is 38.  And it's in units of square meters.  Other than that, I have no idea what it means.  But there's no particular reason for special processing of it.  On to the next variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5d385a0-fe15-be3f-c467-183c0a417f42"},"outputs":[],"source":"describe(\"life_sq\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c431339-2af2-50b8-25c0-c84a1e8cec33"},"source":"A lot of zeros (maybe commercial property with no living space?).  Also a lot of ones, as you can see from the log histogram.  (The function is actually log(x+1), and the natural logarithm of 2 is 0.69.)  Those make no sense, and the meaning is unclear, so I'll give them a dummy category.  There's also one extreme outlier on the right.  So let's look at the good part of the distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f6ae744-6a98-3270-6e30-8b14b2dfa338"},"outputs":[],"source":"describe(\"life_sq\", minval=1.5, maxval=1000, nhi=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1e5f2844-6a1b-7a2b-ae28-cd76db1cc115"},"source":"Again there's a 2, and I'm going to draw a line above it, so..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2703554-3f62-22a6-db10-c8864497fd30"},"outputs":[],"source":"describe(\"life_sq\", minval=2.5, maxval=1000, nhi=0, nlo=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b7da042b-f76a-ec6c-fb2f-b25230ba6a09"},"source":"Again, the log chart looks pretty reasonable but with some strange spikes.  Looking more closely,"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b43b164-abc5-c8c0-8e8f-e639182bba1b"},"outputs":[],"source":"describe(\"life_sq\", minval=15, maxval=35, nhi=0, nlo=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"05120f28-511e-1318-7010-7ac4d1eb3883"},"source":"19 and 30.  Who knows what they mean?  Ignore them.  On to the floor variable."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0189a9b4-4aed-98b4-5ec1-b94c9fa6c747"},"outputs":[],"source":"describe(\"floor\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"267ca829-baed-dd81-dc42-093313bb14e8"},"source":"Genrally looks reasonable.  Not sure what the zeros mean.  (There aren't enough of them for zero to mean \"not an apartment.\")  They'll get a dummy.  Also one is kind of a special case, so maybe should have its own dummy.  And 77 is not unreasonable, but it's far from the rest of the pack, so I'll give it a dummy too.  It's not obvious from the historgrams, but I think I will use a logarithm for floor:  intuitively, the difference between floor 2 and floor 3 is a lot more significant than the difference between floor 32 and floor 33.\n\nNext is max_floor."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"855d509c-a539-3f76-219d-8d98021f715d"},"outputs":[],"source":"describe(\"max_floor\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"8fe5616a-d214-446f-453a-c2bcbf37d4c5"},"source":"Looks pretty reasonable, except for the zeros.  Again one is a special case and should get a dummy.  And logs are clearly better than raw values.  There's also a big gap between 57 and 99, so maybe the last 4 points should get their own dummy.  (Also, 99 is a suspicious number, given that there are 3 of them.  Does it just mean \"a lot\"?  Is it a missing value code?)\n\nNext num_room."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"91c60f4e-d418-9272-319e-198de977646b"},"outputs":[],"source":"describe(\"num_room\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"cda04b96-0de4-3aea-43e5-82f68c94a244"},"source":"Looks good, except for the zeros, which will get a dummy.  Not clear whether raw values or logs are better, but my intuition says 1 room vs. 2 rooms is a bigger difference than 8 rooms vs. 9 rooms, so I'll go with logs.\n\nNext is kitch_sq"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c29a7d8c-3872-5822-3839-944d65dd3d0b"},"outputs":[],"source":"describe(\"kitch_sq\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"40f7f43e-9f78-6643-8517-a2a1fefe74cd"},"source":"That's an ugly one.  Let's first take a closer look at the low end."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b409392-62fe-752f-6942-52da36f3bb2a"},"outputs":[],"source":"describe(\"kitch_sq\", maxval=12, nhi=0, nlo=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0230a582-8ad0-3d92-61ae-53fa1ad8f911"},"source":"Lots of zeros and ones but nothing else strange.  One square meter is not implausibly small for a kitchenette, but it does look like a special case, so it will get a dummy.  Let's look at the high end."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"83a4772e-9ae2-0033-e5cb-66e34a6131cb"},"outputs":[],"source":"describe(\"kitch_sq\", minval=30, nhi=20, nlo=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8c4d6d78-318d-cf2c-f28b-ebd2146ef2a5"},"source":"Nothing completely insane here but the top part of the distribution is weird.  I'll put in a dummy for values greater than 400.  Let's look at the middle part of the distribution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"104aa162-4b64-b30c-cf9f-bb94fb7912c7"},"outputs":[],"source":"describe(\"kitch_sq\", minval=4, maxval=150, nhi=0, nlo=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f9b648e7-4fdb-1e7b-cd3e-56c1f7a37fe4"},"source":"Still looks werid.  Let's look at the part where it gets weird."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af8918f9-e01d-d258-9ba2-0b2c78ff8835"},"outputs":[],"source":"describe(\"kitch_sq\", minval=15, maxval=70, nhi=0, nlo=0, showmiss=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4ec287e9-2d2f-3a96-f020-a96d87b5f1bb"},"source":"So the weirdness is not just a few unusual cases.  Even with the log transformation, the overall distribution is quite skewed.  I'm tempted to take the cube root of the log.  Here's what that looks like for the full distribution and for just the non-extreme range of values (2 to 400)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"072454b8-0a17-6553-af07-2533fe3a87ce"},"outputs":[],"source":"var = dfa.kitch_sq\nvar2 = var[((var>1)&(var<400))]\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,3))\nnp.cbrt(np.log(var+1)).plot(ax=axes[0], kind='hist', bins=100, color='green', secondary_y=True)\nnp.cbrt(np.log(var2+1)).plot(ax=axes[1], kind='hist', bins=100, color='green', secondary_y=True)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7a93955-82fd-304b-93b4-718ce1ce4ddf"},"source":"To be continued..."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"660c857a-305a-59b3-4c43-e8331795e0c2"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}