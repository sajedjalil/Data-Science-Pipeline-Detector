{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"53ad210f-7122-c1f5-6e39-74f2cc2e0dcb"},"source":"# A Simple Macro Model To Predict Monthly House Prices"},{"cell_type":"markdown","metadata":{"_cell_guid":"99a93653-6246-cf95-373a-d02e2327e4eb"},"source":"## Introduction"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f36ba00-a66d-d98a-15eb-6029ff2c6ee6"},"source":"This model uses linear distrubuted lags (Almon lags of degree 1) of `balance_trade` and `mortgage_rate` to predict monthly real log house prices (monthly median log of `price_doc` divided by `cpi`).\n\nWhy Almon lags?  Because I like them.  They've never had a good theoretical justification, but they're easy for me to understand, and they're a convenient way to avoid overfitting by reducing the number of parameters without obfuscating the content of the model.  (An [Almon lag structure](http://davegiles.blogspot.com/2017/01/explaining-almon-distributed-lag-model.html) effectively constrains the regression coefficients on several lagged values of the same variable to lie on a polynomial curve -- in this case a 1st-degree polynomial, otherwise known as a straight line.  So I take 6 coefficients and force them to line up in a way that involves only 2 parameters.  It's feature engineering, 1960's style.)\n\nWhy this particular lag structure (linear from month 0 to month -5)?  Prelminary analysis indicated that this was a reasonable number of lags to include (given data limitations).  And then the structure had to be linear, or it would have too many parameters, which defeats the purpose of Almon lags.  Also, the choice to use the same number of lags for both variables is like a constraint that reduces overfitting, since \"number of lags\" is really an extra parameter hiding in the background.  That's also partly why I chose a total of 6 months -- half a year -- rather than something like 5 or 7, which would be a magic number.\n\nWhy mortgage rate?  Because it's the obvious macro variable that would affect housing prices with a lag.\n\nWhy trade balance?  Mostly becuase it fits really, really well.  When I fit it with several lags with no constraint on the lag structure, all the lags got coefficients with the same sign.  Hard to believe that would happen by chance.\n\nOn a theoretical level, trade balance is relevant to housing for Russia in particular because it's an indicator of how much excess savings Russia is generating.  (This doesn't work, for example, for the US, which is a net borrower.)  A large trade surplus corresponds to a lot of excess savings are going abroad.  In that situation, there are probably also a lot of savings going into domestic housing investment.\n\nAs I understand it, the amount of savings generated in Russia varies a great deal from one year to the next mostly becuase of energy prices.  When energy prices are high, producers save a lot of their income.  When prices are low, producers don't have much to save.  Presumably, when energy prices are high, producers don't keep all their saved income in foreign assets but bring some of it back to Russsia to buy housing."},{"cell_type":"markdown","metadata":{"_cell_guid":"7a0dbcc7-81bf-a5eb-7401-65f9be48ae15"},"source":"## Get the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1083c833-f238-c2c8-45ee-7ea9ab974db5"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc2664f3-e54f-e012-cbdc-197c06fa9ad8"},"outputs":[],"source":"macro = pd.read_csv('../input/macro.csv')\ntrain = pd.read_csv('../input/train.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60ac83d6-9007-ae4c-9c28-8af2afda116a"},"outputs":[],"source":"macro[\"timestamp\"] = pd.to_datetime(macro[\"timestamp\"])\nmacro[\"year\"]  = macro[\"timestamp\"].dt.year\nmacro[\"month\"] = macro[\"timestamp\"].dt.month\nmacro[\"yearmonth\"] = 100*macro.year + macro.month\nmacmeds = macro.groupby(\"yearmonth\").median()\nmacmeds.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a33d53a7-2bcc-ed7e-ff7e-d8494c9ee8a6"},"outputs":[],"source":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"year\"]  = train[\"timestamp\"].dt.year\ntrain[\"month\"] = train[\"timestamp\"].dt.month\ntrain[\"yearmonth\"] = 100*train.year + train.month\nprices = train[[\"yearmonth\",\"price_doc\"]]\np = prices.groupby(\"yearmonth\").median()\np.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"048383df-ab5b-a8f7-61a2-64346ef36208"},"outputs":[],"source":"df = macmeds.join(p)\n# Take a look at some of the data, just to make sure it's there:\ndf.loc[ [201109,201212,201403,201506],\n             [\"cpi\",\"balance_trade\",\"mortgage_rate\",\"year\",\"month\",\"price_doc\"]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"52de6eea-9f33-9042-5490-32956e700ecc"},"source":"## Functions to deal with Almon Lags"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cdc7985e-f680-9c63-2398-0a05fe9d6f1b"},"outputs":[],"source":"#  Adapted from code at http://adorio-research.org/wordpress/?p=7595\n#  Original post was dated May 31st, 2010\n#    but was unreachable last time I tried\n\nimport numpy.matlib as ml\n \ndef almonZmatrix(X, maxlag, maxdeg):\n    \"\"\"\n    Creates the Z matrix corresponding to vector X.\n    \"\"\"\n    n = len(X)\n    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n    for t in range(maxlag,  n):\n       #Solve for Z[t][0].\n       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n       for j in range(1, maxdeg+1):\n             s = 0.0\n             for i in range(1, maxlag+1):       \n                s += (i)**j * X[t-i]\n             Z[t-maxlag,j] = s\n    return Z\n\ndef almonXcof(zcof, maxlag):\n    \"\"\"\n    Transforms the 'b' coefficients in Z to 'a' coefficients in X.\n    \"\"\"\n    maxdeg  = len(zcof)-1\n    xcof    = [zcof[0]] * (maxlag+1)\n    for i in range(1, maxlag+1):\n         s = 0.0\n         k = i\n         for j in range(1, maxdeg+1):\n             s += (k * zcof[j])\n             k *= i\n         xcof[i] += s\n    return xcof"},{"cell_type":"markdown","metadata":{"_cell_guid":"e55aab0e-c0ce-8f8c-e75d-bf78dd53361d"},"source":"## Prepare data for macro model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eaa5430d-340a-13ba-c097-4d424964f1b5"},"outputs":[],"source":"y = df.price_doc.div(df.cpi).apply(np.log).loc[201108:201506]\nprint( y.head() )\ny.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f66f707-029e-249c-cf0f-9a243aa028b9"},"outputs":[],"source":"nobs = 47  # August 2011 through June 2015, months with price_doc data\ntblags = 5    # Number of lags used on PDL for Trade Balance\nmrlags = 5    # Number of lags used on PDL for Mortgage Rate\nztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\nzmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\ncolumns = ['tb0', 'tb1', 'mr0', 'mr1']\nz = pd.DataFrame( np.concatenate( (ztb, zmr), axis=1), y.index.values, columns )\nX = sm.add_constant( z )\nX.shape"},{"cell_type":"markdown","metadata":{"_cell_guid":"78e83e72-0004-2e60-471e-09a88d5ab68f"},"source":"## Fit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24363000-fe83-5348-91dd-eb20729f6c8a"},"outputs":[],"source":"eq = sm.OLS(y, X)\nfit = eq.fit()\nfit.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e6a11a2-272d-6254-66bc-177208ea53d0"},"source":"Here's what the fit looks like in-sample.  Pretty good for fitting 47 data points with only 5 parameters."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69afdbbc-df0c-f5cb-b055-cea47d01bdcc"},"outputs":[],"source":"%matplotlib inline\nplt.plot(y.values)\nplt.plot(pd.Series(fit.predict(X)).values)"},{"cell_type":"markdown","metadata":{"_cell_guid":"17c97f0f-df89-5eec-e5ce-1fceb0d10bc3"},"source":"## Predict"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d79905fc-7bdc-39e2-ef98-cd411e202082"},"outputs":[],"source":"test_cpi = df.cpi.loc[201507:201605]\ntest_index = test_cpi.index\nztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\nzmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\nz_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test), axis=1), test_index, columns )\nX_test = sm.add_constant( z_test )\npred_lnrp = fit.predict( X_test )\npred_p = np.exp(pred_lnrp) * test_cpi\npred_p.to_csv(\"monthly_macro_predicted.csv\")\npred_p"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c98c8ef-14a4-e5e3-67bc-db486026131d"},"outputs":[],"source":"print( \"Here's the average price predicted for the test period by the macro model: \\n\")\nprint( np.exp( pred_lnrp.mean() + np.log(test_cpi).mean() ) )\nprint( \"\\nDivide (logarithmic) average baseline micro model price prediction by this\")\nprint( \"   and use the result to justify multiplier for training prices in the micro model.\")"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}