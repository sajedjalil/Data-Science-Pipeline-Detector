{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ed13f694-197a-a581-e08e-5cd6e2e9bd04"},"source":"## Weighted Least Squares Fit including Heteroskedasticity Weights"},{"cell_type":"markdown","metadata":{"_cell_guid":"0401d9c4-acd5-a875-6ce4-044a3110b258"},"source":"This is the third checkpoint in my modeling progress, following \"[Sloppy OLS](https://www.kaggle.com/aharless/sloppy-mess-ols)\" and the [first cut at WLS](https://www.kaggle.com/aharless/weighted-least-squares-sberbank-data/notebook).  The main innovation is that I've added heteroskedasticity weights.  That is, I have a model (\"heteroskedasticity model\") to predict how well my prediction model will fit to the training data, and I use the heteroskedasticity model to weight the training cases when I fit the prediction model, giving greater weight to the cases that are predicted to fit better.\n\nIt's quick and dirty.  The heteroskedasticity model needs some regularization:  as it is, I just threw in the same features I used for the prediction model and added dummies for weird prices.  (Since I only predict the heteroskedasticity model on the training data, I can use features derived from the target variable, so \"dummies for weird prices\" are feasible.)  Perhaps I should use folds and get out-of-sample residuals for all the training cases instead of fitting the heteroskedasticity model to the squares of the in-sample residuals.  Anyhow, what I have is better than nothing at all.\n\nI've also changed the zero date for the time-weighting, just because I didn't think the original one was reasonable (even though it predicts better on the public leaderboard portion of the test data).  And I've eliminated (by commenting out) some features that didn't seem to fit well.  (Ideally one would want to use a method like LASSO to do this kind of thing, but right now the features are still in blocks -- for example, dummies for weird values of certain variables have to stay in or go out along with the variables to which they refer -- and I don't know how to tell a regularization method it has to keep them together.)\n\nAnd I've added a macro variable -- mortgage growth (actually in the last version of my old script).  Right now it's proxying for all the information in the macro data (except CPI, which I use to normalize the target).  Eventually I'll want to handle the macro data with more precision, but for now this is my best guess at the best single proxy.\n\nAnd I've added some interaction terms for the \"product_type\" variable.  The process that determines prices for investment properties may be quite different from the one that determines prices for owner-occupied properties."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8a907a5-8c65-786f-1103-083d47de3f6d"},"outputs":[],"source":"# Parameters\nuse_pipe = True  # Old version had collinearity that was somehow masked w/o the pipe transform\nweight_base = \"2010-07-01\""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1456923c-7537-1a64-e168-e83744128cbf"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec3712d3-6116-7103-d5b4-a2033cb577e0"},"outputs":[],"source":"dfa = pd.concat([train, test])  # \"dfa\" stands for \"data frame all\"\n# Eliminate spaces and special characters in area names\ndfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\ndfa = dfa.merge(macro, on='timestamp', suffixes=['','_macro'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"fb9fa044-b84c-cb96-a363-ce40800eeb07"},"source":"Note:  As a general principle, when there are values that seem invalid but are \"special\", such as zeros and ones in numeric fields that shouldn't be zero or one, or values that seem out of the reasonable range but aren't entirely impossible, my approach is to treat them as special cases rather than either missing or valid.  Dummy variables will take care of those special cases in OLS, though the situation may get complicated when fancy methods are involved.  One possibility is to recode them using OLS coefficients to replace the ugly values (perhaps coefficients from a baseline OLS with a sparse set of obvious features)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5bf28f1-ab8a-7612-75fd-5c2e1ddc9afe"},"outputs":[],"source":"dfa[\"fullzero\"] = (dfa.full_sq==0)\ndfa[\"fulltiny\"] = (dfa.full_sq<4)\ndfa[\"fullhuge\"] = (dfa.full_sq>2000)\ndfa[\"lnfull\"] = np.log(dfa.full_sq+1)\n\ndfa[\"nolife\"] = dfa.life_sq.isnull()\ndfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\ndfa[\"lifezero\"] = (dfa.life_sq==0)\ndfa[\"lifetiny\"] = (dfa.life_sq<4)\ndfa[\"lifehuge\"] = (dfa.life_sq>2000)\ndfa[\"lnlife\"] = np.log( dfa.life_sq + 1 )\n\ndfa[\"nofloor\"] = dfa.floor.isnull()\ndfa.floor = dfa.floor.fillna(dfa.floor.median())\ndfa[\"floor1\"] = (dfa.floor==1)\ndfa[\"floor0\"] = (dfa.floor==0)\ndfa[\"floorhuge\"] = (dfa.floor>50)\ndfa[\"lnfloor\"] = np.log(dfa.floor+1)\n\ndfa[\"nomax\"] = dfa.max_floor.isnull()\ndfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\ndfa[\"max1\"] = (dfa.max_floor==1)\ndfa[\"max0\"] = (dfa.max_floor==0)\ndfa[\"maxhuge\"] = (dfa.max_floor>80)\ndfa[\"lnmax\"] = np.log(dfa.max_floor+1)\n\ndfa[\"norooms\"] = dfa.num_room.isnull()\ndfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\ndfa[\"zerorooms\"] = (dfa.num_room==0)\ndfa[\"lnrooms\"] = np.log( dfa.num_room + 1 )\n\ndfa[\"nokitch\"] = dfa.kitch_sq.isnull()\ndfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\ndfa[\"kitch1\"] = (dfa.kitch_sq==1)\ndfa[\"kitch0\"] = (dfa.kitch_sq==0)\ndfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\ndfa[\"lnkitch\"] = np.log(dfa.kitch_sq+1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef2c0c29-247d-54a6-fab1-67b2446ae6ca"},"outputs":[],"source":"dfa[\"material0\"] = dfa.material.isnull()\ndfa[\"material1\"] = (dfa.material==1)\ndfa[\"material2\"] = (dfa.material==2)\ndfa[\"material3\"] = (dfa.material==3)\ndfa[\"material4\"] = (dfa.material==4)\ndfa[\"material5\"] = (dfa.material==5)\ndfa[\"material6\"] = (dfa.material==6)\n\n# \"state\" isn't explained but it looks like an ordinal number, so for now keep numeric\ndfa.loc[dfa.state>5,\"state\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\ndfa.state = dfa.state.fillna(dfa.state.median())\n\n# product_type gonna be ugly because there are missing values in the test set but not training\n# Check for the same problem with other variables\ndfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\ndfa.owner_occ.fillna(dfa.owner_occ.mean())\n\ndfa = pd.get_dummies(dfa, columns=['sub_area'], drop_first=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"433a71df-36f4-13f9-243d-8e9e5ee4fabd"},"outputs":[],"source":"# Build year is ugly\n# Can be missing\n# Can be zero\n# Can be one\n# Can be some ridiculous pre-Medieval number\n# Can be some invalid huge number like 20052009\n# Can be some other invalid huge number like 4965\n# Can be a reasonable number but later than purchase year\n# Can be equal to purchase year\n# Can be a reasonable nubmer before purchase year\n\ndfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\ndfa[\"nobuild\"] = dfa.build_year.isnull()\ndfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\ndfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\ndfa[\"futurebuild\"] = (dfa.sincebuild < 0)\ndfa[\"newhouse\"] = (dfa.sincebuild==0)\ndfa[\"tooold\"] = (dfa.sincebuild>1000)\ndfa[\"build0\"] = (dfa.build_year==0)\ndfa[\"build1\"] = (dfa.build_year==1)\ndfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\ndfa[\"lnsince\"] = dfa.sincebuild.mul(dfa.sincebuild>0).add(1).apply(np.log)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"522d7ca5-ce97-1d37-f12a-b241eb412b1e"},"outputs":[],"source":"# Note for later:\n# Want to check for valididty of relationships, e.g. kitch_sq < life_sq < full_sq\n# But this interacts with how variables are already processed, so that may have to be changed\n# For example, if kitch_sq is sometimes huge and there is a dummy to identify those huge cases,\n#  do we want a separate dummy to identify which of those cases are internally consistent?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f701aa4-ec19-4f58-e043-8a51fd9150db"},"outputs":[],"source":"# Interaction terms\ndfa[\"fullzero_Xowner\"] = dfa.fullzero.astype(\"float64\") * dfa.owner_occ\ndfa[\"fulltiny_Xowner\"] = dfa.fulltiny.astype(\"float64\") * dfa.owner_occ\ndfa[\"fullhuge_Xowner\"] = dfa.fullhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfull_Xowner\"] = dfa.lnfull * dfa.owner_occ\ndfa[\"nofloor_Xowner\"] = dfa.nofloor.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor0_Xowner\"] = dfa.floor0.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor1_Xowner\"] = dfa.floor1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfloor_Xowner\"] = dfa.lnfloor * dfa.owner_occ\ndfa[\"max1_Xowner\"] = dfa.max1.astype(\"float64\") * dfa.owner_occ\ndfa[\"max0_Xowner\"] = dfa.max0.astype(\"float64\") * dfa.owner_occ\ndfa[\"maxhuge_Xowner\"] = dfa.maxhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnmax_Xowner\"] = dfa.lnmax * dfa.owner_occ\ndfa[\"kitch1_Xowner\"] = dfa.kitch1.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitch0_Xowner\"] = dfa.kitch0.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitchhuge_Xowner\"] = dfa.kitchhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnkitch_Xowner\"] = dfa.lnkitch * dfa.owner_occ\ndfa[\"nobuild_Xowner\"] = dfa.nobuild.astype(\"float64\") * dfa.owner_occ\ndfa[\"newhouse_Xowner\"] = dfa.newhouse.astype(\"float64\") * dfa.owner_occ\ndfa[\"tooold_Xowner\"] = dfa.tooold.astype(\"float64\") * dfa.owner_occ\ndfa[\"build0_Xowner\"] = dfa.build0.astype(\"float64\") * dfa.owner_occ\ndfa[\"build1_Xowner\"] = dfa.build1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnsince_Xowner\"] = dfa.lnsince * dfa.owner_occ\ndfa[\"state_Xowner\"] = dfa.state * dfa.owner_occ"},{"cell_type":"markdown","metadata":{"_cell_guid":"5440698b-c748-7ab9-2375-c101363274bb"},"source":"OK, the data have been munged.  Now to make choose features for the fit."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63ea8770-89d8-3a7d-7833-fd543fceb5b9"},"outputs":[],"source":"# The normalized target variable:  log real sale price\ntraining = dfa[dfa.price_doc.notnull()]\ntraining.lnrp = training.price_doc.div(training.cpi).apply(np.log)\ny = training.lnrp\n\n# Features to use later in heteroskedasticity model\nmillion1 = (training.price_doc==1e6)\nmillion2 = (training.price_doc==2e6)\nmillion3 = (training.price_doc==3e6)\n\n# The features used for prediction\n# (Turns out norooms, nomax, and nokitch are all identical to the omitted material=NA category, \n#  so include only norooms, and then only if material dummies are excluded.)\nX = training[[\n       # Features derived from full_sq\n       \"fullzero\", \"fulltiny\", \"fullhuge\", \"lnfull\",\n       # Features derived from life_sq\n#  Omited due to \"visual regularization\"    \n#       \"nolife\", \"lifezero\", \"lifetiny\", \"lifehuge\", \"lnlife\",\n       # Features derived from floor\n       \"nofloor\", \"floor1\", \"floor0\", \"floorhuge\", \"lnfloor\",\n       # Features derived from max_floor\n       \"max1\", \"max0\", \"maxhuge\", \"lnmax\",\n       # Features derived from num_room\n#  Omited due to \"visual regularization\"    \n#       \"zerorooms\", \"lnrooms\",\n       # Features derived from kitch_sq\n       \"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\",\n       # Features derived from bulid_year\n       \"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n       \"build0\", \"build1\", \"untilbuild\", \"lnsince\",\n       # Feature derived from product_type\n       \"owner_occ\",\n       # Included \"state\" as is for now, but later will recode to more meaningful ratio scale\n       \"state\",\n       # Features (dummy set) derived from material\n       \"material1\", \"material2\", \"material3\", \"material4\", \"material5\", \"material6\",\n       # Interaction terms\n       \"fulltiny_Xowner\", \n#  Omited due to collinearity issue\n#       \"fullhuge_Xowner\", \"fullzero_Xowner\", \n       \"lnfull_Xowner\",\n       \"nofloor_Xowner\", \"floor0_Xowner\", \"floor1_Xowner\", \"lnfloor_Xowner\",\n#  Omited due to \"visual regularization\"    \n#       \"max1_Xowner\", \"max0_Xowner\", \"maxhuge_Xowner\", \"lnmax_Xowner\",\n       \"kitch1_Xowner\", \"kitch0_Xowner\", \"kitchhuge_Xowner\", \"lnkitch_Xowner\",\n#  Omited due to \"visual regularization\"    \n#       \"nobuild_Xowner\", \"newhouse_Xowner\", \"tooold_Xowner\", \n#       \"build0_Xowner\", \"build1_Xowner\", \"lnsince_Xowner\",\n       \"state_Xowner\",\n       # Features (dummy set) derived from sub_area\n       'sub_area_Akademicheskoe',\n       'sub_area_Alekseevskoe', 'sub_area_Altufevskoe', 'sub_area_Arbat',\n       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n       'sub_area_Caricyno', 'sub_area_Cheremushki',\n       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n       'sub_area_Krylatskoe', 'sub_area_Kuncevo', 'sub_area_Kurkino',\n       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n       'sub_area_Molzhaninovskoe', 'sub_area_MoskvorecheSaburovo',\n       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n       'sub_area_PoselenieFilimonkovskoe', 'sub_area_PoselenieKievskij',\n       'sub_area_PoselenieKlenovskoe', 'sub_area_PoselenieKokoshkino',\n       'sub_area_PoselenieKrasnopahorskoe',\n       'sub_area_PoselenieMarushkinskoe',\n       'sub_area_PoselenieMihajlovoJarcevskoe',\n       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n       'sub_area_PoselenieNovofedorovskoe',\n       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n       'sub_area_PoselenieRogovskoe', 'sub_area_PoselenieShhapovskoe',\n       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n       'sub_area_PoselenieVnukovskoe', 'sub_area_PoselenieVoronovskoe',\n       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n       'sub_area_Tverskoe', 'sub_area_Veshnjaki', 'sub_area_Vnukovo',\n       'sub_area_Vojkovskoe', 'sub_area_Vostochnoe',\n       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo',\n       'sub_area_Zjuzino',\n       # One macro variable to rule them all, for now\n       'mortgage_growth',\n       # Need to keep timestamp to calculate weights\n       'timestamp'\n             ]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"57d2fb54-474b-16d9-16eb-b73450e06a8a"},"source":"The first time around, I weight by time.  The second time around, I'll weight by time and inverse of predicted variance, as generated by the heteroskedsticity model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0001eb4-5937-7b78-d974-e353bc6d57a0"},"outputs":[],"source":"def get_weights(df):\n    # Weight cases linearly on time\n    # with later cases (more like test data) weighted more heavily\n    basedate = pd.to_datetime(weight_base).toordinal() # Basedate gets a weight of zero\n    wtd = pd.to_datetime(df.timestamp).apply(lambda x: x.toordinal()) - basedate\n    wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n    return wts"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d1ca19c-aa68-d94e-9cd9-87b306ffffdb"},"source":"Finish getting the data ready to fit.  And go through the charade of cross-validation.  (Eventually there will be some more respectable cross-validation, but maybe not for the baseline model.  What happens in this version doesn't really give a good estimate of prediction error, but I'm leaving the code in for possible later use.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"05cf3a15-f92b-f003-164b-ac8a2bfe0cef"},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nwts = get_weights(X_train)\nX_train = X_train.drop(\"timestamp\", axis=1)\nX_test = X_test.drop(\"timestamp\", axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96949aa5-c369-227b-3db4-71bd61ce9d8b"},"outputs":[],"source":"if use_pipe:\n    from sklearn.preprocessing import Imputer, StandardScaler\n    from sklearn.pipeline import make_pipeline\n\n    # Make a pipeline that transforms X\n    pipe = make_pipeline(Imputer(), StandardScaler())\n    pipe.fit(X_train)\n    pipe.transform(X_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"916b3f5c-9c7a-2f9e-9b00-74af0542e620"},"outputs":[],"source":"from sklearn.metrics import make_scorer\n\n# Need to fix this so it will weight the points using the same weight scheme as the fit.\n\ndef rmsle_exp(y_true_log, y_pred_log):\n    y_true = np.exp(y_true_log)\n    y_pred = np.exp(y_pred_log)\n    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n\ndef score_model(model, pipe=None):\n    if (pipe==None):\n        train_error = rmsle_exp(y_train, model.predict(X_train))\n        test_error = rmsle_exp(y_test, model.predict(X_test))\n    else:\n        train_error = rmsle_exp(y_train, model.predict(pipe.transform(X_train)))\n        test_error = rmsle_exp(y_test, model.predict(pipe.transform(X_test)))\n    return train_error, test_error"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"766553c7-751e-6097-9e3b-2d5b6f036dff"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression(fit_intercept=True)\nif use_pipe:\n    lr.fit(pipe.transform(X_train), y_train, sample_weight=wts)\n    print(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(lr, pipe)))\nelse:\n    lr.fit(X_train, y_train, sample_weight=wts)\n    print(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(lr)))"},{"cell_type":"markdown","metadata":{"_cell_guid":"7ea194df-4a28-a4a6-227e-f6cdd4d7517e"},"source":"Now fit to the whole training data.  (This is the first time around, to get a target for the heteroskedasticity model.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"171b40de-79bf-5f1c-b553-cd0c678118fc"},"outputs":[],"source":"wts = get_weights(X)\nX = X.drop(\"timestamp\", axis=1)\n\nif use_pipe:\n    pipe.fit(X)\n    pipe.transform(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d42815e2-dae2-3477-7350-e7ee26246105"},"outputs":[],"source":"lr = LinearRegression(fit_intercept=True)\nif use_pipe:\n    lr.fit(pipe.transform(X), y, sample_weight=wts)\nelse:\n    lr.fit(X, y, sample_weight=wts)"},{"cell_type":"markdown","metadata":{"_cell_guid":"78696bd0-539b-4e93-0b5f-5e6b0b24dfca"},"source":"Fit the heteroskedasticity model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52951928-64cc-954c-99ce-80c87c714a22"},"outputs":[],"source":"# Predict on the training set and take the residuals\nif use_pipe:\n    pred = lr.predict( pipe.transform(X) )\nelse:\n    pred = lr.predict(X)\nresids = y - pred\nresids2 = resids * resids"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a9ad37eb-4b18-dc90-0887-037453113554"},"outputs":[],"source":"# Add heteroskedasticity-related features\nXhetero = X.assign(million1=million1, million2=million2, million3=million3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5c15aae-a665-a3ce-b27c-ed910772e8b2"},"outputs":[],"source":"# For a visual look at the results, use statsmodels.WLS\nfrom statsmodels.regression.linear_model import WLS\nxdat = Xhetero.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = resids2.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=wts).fit()\nresult.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"371114fd-ede1-2b96-c8ae-1246c49eb6a5"},"outputs":[],"source":"# For prediction, use sklearn.linear_model.LinearRegression \nlr_hetero = LinearRegression(fit_intercept=True)\nif use_pipe:\n    newpipe = make_pipeline(Imputer(), StandardScaler())\n    newpipe.fit(Xhetero)\n    lr_hetero.fit(newpipe.transform(Xhetero), resids2, sample_weight=wts)\n    pred_res2 = lr_hetero.predict( newpipe.transform(Xhetero) )\nelse:\n    lr_hetero.fit(Xhetero, resids2, sample_weight=wts)\n    pred_res2 = lr_hetero.predict(Xhetero)"},{"cell_type":"markdown","metadata":{"_cell_guid":"54d43560-024a-0e47-fddc-2af0deda7309"},"source":"Revise the sample weights based on the results of the heteroskeasticity model.  (The fact that the `pred_res2` was sometimes negative -- hence the need for `np.max()` -- shows that the heteroskedasticity model still needs some work.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"01dcee29-34d5-21ea-a9a7-fe336308a6db"},"outputs":[],"source":"newwts = np.max( wts / pred_res2, 0 )"},{"cell_type":"markdown","metadata":{"_cell_guid":"31fbc74a-732b-99ae-4685-9c2708724a8e"},"source":"Fit the prediction model again using the new weights"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c798d8bf-4900-e9dc-880a-8072dec9a978"},"outputs":[],"source":"lr = LinearRegression(fit_intercept=True)\nif use_pipe:\n    lr.fit(pipe.transform(X), y, sample_weight=newwts)\nelse:\n    lr.fit(X, y, sample_weight=newwts)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4dd46dd-c421-4133-50ff-d76aea671ac5"},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"_cell_guid":"2635178a-2b6d-954f-c96e-3d31f6577c03"},"source":"Predict on the test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"706d866e-3290-372d-2e2f-17abdf4a5e6c"},"outputs":[],"source":"testing = dfa[dfa.price_doc.isnull()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cce0b518-c9e9-dba7-12a1-cef6979e6125"},"outputs":[],"source":"df_test = pd.DataFrame(columns=X.columns)\nfor column in df_test.columns:\n        df_test[column] = testing[column]        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"802d4db9-bf79-9a6e-406a-c30e47b0dcaa"},"outputs":[],"source":"# Make the predictions\nif use_pipe:\n    pred = lr.predict( pipe.transform(df_test) )\nelse:\n    pred = lr.predict(df_test)\npredictions = np.exp(pred)*testing.cpi\n\n# And put this in a dataframe\npredictions_df = pd.DataFrame()\npredictions_df['id'] = testing['id']\npredictions_df['price_doc'] = predictions\npredictions_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78b5fd2a-c236-40e7-1301-fc5bf0da9e89"},"outputs":[],"source":"predictions_df.to_csv('wls_predictions.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8bcbfce3-47c6-1978-4dda-db05b4fc726d"},"source":"Postlogue:  Look at model results"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b2a41526-4217-3ba7-6a23-b7213a27276d"},"outputs":[],"source":"# Check for ridiculous coefficients, likely indicating collinearity\nco = lr.coef_\nra = range(len(co))\nmask = np.abs(co)>1e4\nX.columns[mask].values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b6974889-325d-bb90-0d16-2bd5e5de5240"},"outputs":[],"source":"# Turn the model into a formula, so I can fit with statsmodels and look at the result\n# (Never mind, I'll pass data frames, but I'm keeping this code in case I need it later.)\n\n#fo = \"y ~ \"\n#i = 0\n#for col in X.columns.values:\n#    fo += col\n#    i += 1\n#    if (i<len(X.columns.values)):\n#        fo += \"+\"\n\n#import statsmodels.formula.api as sm\n#dfwls = X.copy()\n#dfwls[\"y\"] = y\n#result = sm.ols(formula=fo, data=dfwls).fit()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c57035a4-3049-03e9-0b1f-8af18ef981a4"},"outputs":[],"source":"from statsmodels.regression.linear_model import WLS\nxdat = X.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = y.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=newwts).fit()\nresult.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8db49201-ebf0-df43-9aab-82e2d49826e7"},"outputs":[],"source":"# Note that, if the model is run without the pipe transform, the coefficients below\n#  should be the same as those above.  Sometimes they are, sometimes not.\n#  If they're not the same, probably numerical instability due to collinearity.\npd.DataFrame(X.columns, co)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0d3c2bb3-e461-e632-cab9-550a0c1ce908"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b1216b3-72f2-c1e0-70a5-a24241869a12"},"outputs":[],"source":"# Diagnostic stuff maybe I will want to use again\n# pt = pipe.transform(X)\n# pip = pt[:,X.columns.get_loc(\"norooms\")]\n# from scipy import stats\n# print( stats.describe(pip) )\n# print( stats.describe(X.norooms))\n# pt.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c4107dae-140b-ffa3-cde3-59a51605bccd"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}