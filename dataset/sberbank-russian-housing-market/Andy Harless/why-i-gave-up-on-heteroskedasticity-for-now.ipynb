{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"132dd6d3-d2c8-0f4b-32ee-d0d0f6a21b5a"},"source":"## Why I gave up on heteroskedasticity (for now)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4c50d426-b706-cc02-567c-3ef9ee216f6c"},"source":"The intention when I started this notebook was to settle on [a set of sample weights](https://www.kaggle.com/aharless/sberbank-data-wls-with-heteroskedasticity-weights) to account for the predictably greater reliability of some data points in the Sberbank housing data.  I even drafted a pompous write-up about the importance of heteroskedasticity weighting.  The result of my out-of-sample testing, however, is that I could not find a set of weights that was an improvement over the weighting scheme I was already using (weighting by time alone, for a different reason).  Despite my disappointment, I'm uploading this script to document my analysis and serve as the fourth checkpoint in my modeling process.  And also to make it part of my code base in case I decide to copy some of it to do a similar kind of analysis in the future.\n\nAnother result of this analysis is that I decided to go back to my aggressive time weighting parameter -- actually an even more extreme version, which gives zero weight to the beginning of the data set.  This seems intuitively wrong to me, but it performs better both on out-of-sample testing within the training set and on the public leaderboard.  I sometimes do allow my intuition to overrule the data.  (After all, the ostensible conclusions of \"the data\" depend on the intuitive belief that the assumptions of the data analysis are correct, so an alternative intuition could be stronger.)  But in this case the data are strong enough to alter my priors.\n\nI decided to leave in the part where I got impressed by the robustness of the heteroskedasticity model.  Maybe a cautionary tale.  In the end it makes bad pudding."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aa3a0bbd-0a6a-3dd7-3edd-824bf0b9b923"},"outputs":[],"source":"# Parameters\nuse_pipe = True\nweight_base = \"2010-07-01\"  # Used for the initial analysis, but later I try other values"},{"cell_type":"markdown","metadata":{"_cell_guid":"d4ab2647-b816-cc7a-0e9d-3fc3e670c7d7"},"source":"### Read and munge the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfc8c288-9cf3-ef1e-f5d9-b111d330aebf"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2058db4b-cc96-4b95-7b25-f7b13850312b"},"outputs":[],"source":"dfa = pd.concat([train, test])  # \"dfa\" stands for \"data frame all\"\n# Eliminate spaces and special characters in area names\ndfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\ndfa = dfa.merge(macro, on='timestamp', suffixes=['','_macro'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"8948699c-9f28-6d5f-7c2c-c61f221551b6"},"source":"Note:  As a general principle, when there are values that seem invalid but are \"special\", such as zeros and ones in numeric fields that shouldn't be zero or one, or values that seem out of the reasonable range but aren't entirely impossible, my approach is to treat them as special cases rather than either missing or valid.  Dummy variables will take care of those special cases in OLS, though the situation may get complicated when fancy methods are involved.  One possibility is to recode them using OLS coefficients to replace the ugly values (perhaps coefficients from a baseline OLS with a sparse set of obvious features)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2aa0d7de-1c8e-4ee2-ab24-ffc700056d00"},"outputs":[],"source":"dfa[\"fullzero\"] = (dfa.full_sq==0)\ndfa[\"fulltiny\"] = (dfa.full_sq<4)\ndfa[\"fullhuge\"] = (dfa.full_sq>2000)\ndfa[\"lnfull\"] = np.log(dfa.full_sq+1)\n\ndfa[\"nolife\"] = dfa.life_sq.isnull()\ndfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\ndfa[\"lifezero\"] = (dfa.life_sq==0)\ndfa[\"lifetiny\"] = (dfa.life_sq<4)\ndfa[\"lifehuge\"] = (dfa.life_sq>2000)\ndfa[\"lnlife\"] = np.log( dfa.life_sq + 1 )\n\ndfa[\"nofloor\"] = dfa.floor.isnull()\ndfa.floor = dfa.floor.fillna(dfa.floor.median())\ndfa[\"floor1\"] = (dfa.floor==1)\ndfa[\"floor0\"] = (dfa.floor==0)\ndfa[\"floorhuge\"] = (dfa.floor>50)\ndfa[\"lnfloor\"] = np.log(dfa.floor+1)\n\ndfa[\"nomax\"] = dfa.max_floor.isnull()\ndfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\ndfa[\"max1\"] = (dfa.max_floor==1)\ndfa[\"max0\"] = (dfa.max_floor==0)\ndfa[\"maxhuge\"] = (dfa.max_floor>80)\ndfa[\"lnmax\"] = np.log(dfa.max_floor+1)\n\ndfa[\"norooms\"] = dfa.num_room.isnull()\ndfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\ndfa[\"zerorooms\"] = (dfa.num_room==0)\ndfa[\"lnrooms\"] = np.log( dfa.num_room + 1 )\n\ndfa[\"nokitch\"] = dfa.kitch_sq.isnull()\ndfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\ndfa[\"kitch1\"] = (dfa.kitch_sq==1)\ndfa[\"kitch0\"] = (dfa.kitch_sq==0)\ndfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\ndfa[\"lnkitch\"] = np.log(dfa.kitch_sq+1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dcd57ca6-b2c0-6007-8667-eea184d2f2c8"},"outputs":[],"source":"dfa[\"material0\"] = dfa.material.isnull()\ndfa[\"material1\"] = (dfa.material==1)\ndfa[\"material2\"] = (dfa.material==2)\ndfa[\"material3\"] = (dfa.material==3)\ndfa[\"material4\"] = (dfa.material==4)\ndfa[\"material5\"] = (dfa.material==5)\ndfa[\"material6\"] = (dfa.material==6)\n\n# \"state\" isn't explained but it looks like an ordinal number, so for now keep numeric\ndfa.loc[dfa.state>5,\"state\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\ndfa.state = dfa.state.fillna(dfa.state.median())\n\n# product_type gonna be ugly because there are missing values in the test set but not training\n# Check for the same problem with other variables\ndfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\ndfa.owner_occ.fillna(dfa.owner_occ.mean())\n\ndfa = pd.get_dummies(dfa, columns=['sub_area'], drop_first=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7a1a286f-91b6-547f-a5a2-ee4622f2f51c"},"outputs":[],"source":"# Build year is ugly\n# Can be missing\n# Can be zero\n# Can be one\n# Can be some ridiculous pre-Medieval number\n# Can be some invalid huge number like 20052009\n# Can be some other invalid huge number like 4965\n# Can be a reasonable number but later than purchase year\n# Can be equal to purchase year\n# Can be a reasonable nubmer before purchase year\n\ndfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\ndfa[\"nobuild\"] = dfa.build_year.isnull()\ndfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\ndfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\ndfa[\"futurebuild\"] = (dfa.sincebuild < 0)\ndfa[\"newhouse\"] = (dfa.sincebuild==0)\ndfa[\"tooold\"] = (dfa.sincebuild>1000)\ndfa[\"build0\"] = (dfa.build_year==0)\ndfa[\"build1\"] = (dfa.build_year==1)\ndfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\ndfa[\"lnsince\"] = dfa.sincebuild.mul(dfa.sincebuild>0).add(1).apply(np.log)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bcfb11d-a962-c628-f20d-84c753bdd7d2"},"outputs":[],"source":"# Note for later:\n# Want to check for valididty of relationships, e.g. kitch_sq < life_sq < full_sq\n# But this interacts with how variables are already processed, so that may have to be changed\n# For example, if kitch_sq is sometimes huge and there is a dummy to identify those huge cases,\n#  do we want a separate dummy to identify which of those cases are internally consistent?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"014fd13c-2d2a-8832-6c81-2898873b6324"},"outputs":[],"source":"# Interaction terms\ndfa[\"fullzero_Xowner\"] = dfa.fullzero.astype(\"float64\") * dfa.owner_occ\ndfa[\"fulltiny_Xowner\"] = dfa.fulltiny.astype(\"float64\") * dfa.owner_occ\ndfa[\"fullhuge_Xowner\"] = dfa.fullhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfull_Xowner\"] = dfa.lnfull * dfa.owner_occ\ndfa[\"nofloor_Xowner\"] = dfa.nofloor.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor0_Xowner\"] = dfa.floor0.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor1_Xowner\"] = dfa.floor1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfloor_Xowner\"] = dfa.lnfloor * dfa.owner_occ\ndfa[\"max1_Xowner\"] = dfa.max1.astype(\"float64\") * dfa.owner_occ\ndfa[\"max0_Xowner\"] = dfa.max0.astype(\"float64\") * dfa.owner_occ\ndfa[\"maxhuge_Xowner\"] = dfa.maxhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnmax_Xowner\"] = dfa.lnmax * dfa.owner_occ\ndfa[\"kitch1_Xowner\"] = dfa.kitch1.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitch0_Xowner\"] = dfa.kitch0.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitchhuge_Xowner\"] = dfa.kitchhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnkitch_Xowner\"] = dfa.lnkitch * dfa.owner_occ\ndfa[\"nobuild_Xowner\"] = dfa.nobuild.astype(\"float64\") * dfa.owner_occ\ndfa[\"newhouse_Xowner\"] = dfa.newhouse.astype(\"float64\") * dfa.owner_occ\ndfa[\"tooold_Xowner\"] = dfa.tooold.astype(\"float64\") * dfa.owner_occ\ndfa[\"build0_Xowner\"] = dfa.build0.astype(\"float64\") * dfa.owner_occ\ndfa[\"build1_Xowner\"] = dfa.build1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnsince_Xowner\"] = dfa.lnsince * dfa.owner_occ\ndfa[\"state_Xowner\"] = dfa.state * dfa.owner_occ"},{"cell_type":"markdown","metadata":{"_cell_guid":"4097d1e1-ae8a-ea2b-cff9-dc0732b0a782"},"source":"### Select features to fit\n(There's been a lot of back-and-forth off-camera here, mostly having to do with variables that caused collinearity problems -- sometimes, I think, because the scaler introduced imprecision in the context of bagging.  The scaling is applied to the whole training set, but models are fit to individual bags, and it sometimes seems to cause the model to mistakenly fit a coefficient for a dummy that should be omitted because it is constant in a particular bag.  That's an issue I have to think about.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"097694f9-8364-b232-bc34-70b8cbe8bedf"},"outputs":[],"source":"# Sets of features that go together\n\n# Features derived from full_sq\nfullvars = [\"fullzero\", \"fulltiny\",\n           # For now I'm going to drop the one \"fullhuge\" case. Later use dummy, maybe.\n           #\"fullhuge\",\n           \"lnfull\" ]\n\n# Features derived from floor\nfloorvars = [\"nofloor\", \"floor1\", \"floor0\",\n             # floorhuge isn't very important, and it's causing problems, so drop it\n             #\"floorhuge\", \n             \"lnfloor\"]\n\n# Features derived from max_floor\nmaxvars = [\"max1\", \"max0\", \"maxhuge\", \"lnmax\"]\n\n# Features derived from kitch_sq\nkitchvars = [\"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\"]\n\n# Features derived from bulid_year\nbuildvars = [\"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n             \"build0\", \"build1\", \"untilbuild\", \"lnsince\"]\n\n# Features (dummy set) derived from material\nmatervars = [\"material1\", \"material2\",  # material3 is rare, so lumped in with missing \n             \"material4\", \"material5\", \"material6\"]\n\n# Features derived from interaction of floor and product_type\nfloorXvars = [\"nofloor_Xowner\", \"floor1_Xowner\", \"lnfloor_Xowner\"]\n\n# Features derived from interaction of kitch_sq and product_type\nkitchXvars = [\"kitch1_Xowner\", \"kitch0_Xowner\", \"lnkitch_Xowner\"]\n\n# Features (dummy set) derived from sub_area\nsubarvars = [\n       'sub_area_Akademicheskoe',\n       'sub_area_Alekseevskoe', 'sub_area_Altufevskoe', 'sub_area_Arbat',\n       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n       'sub_area_Caricyno', 'sub_area_Cheremushki',\n       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n       'sub_area_Krylatskoe', 'sub_area_Kuncevo', 'sub_area_Kurkino',\n       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n       'sub_area_Molzhaninovskoe', 'sub_area_MoskvorecheSaburovo',\n       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n       'sub_area_PoselenieFilimonkovskoe', \n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieKievskij',\n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieKlenovskoe', \n       'sub_area_PoselenieKokoshkino',\n       'sub_area_PoselenieKrasnopahorskoe',\n       'sub_area_PoselenieMarushkinskoe',\n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieMihajlovoJarcevskoe',\n       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n       'sub_area_PoselenieNovofedorovskoe',\n       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n       'sub_area_PoselenieRogovskoe', \n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieShhapovskoe',\n       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n       'sub_area_PoselenieVnukovskoe', 'sub_area_PoselenieVoronovskoe',\n       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n       'sub_area_Tverskoe', 'sub_area_Veshnjaki', 'sub_area_Vnukovo',\n       'sub_area_Vojkovskoe', 'sub_area_Vostochnoe',\n       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo', 'sub_area_Zjuzino',   \n       ]\n\n\n# Lump together small sub_areas\ndfa = dfa.assign( sub_area_PoselenieSmall =\n   dfa.sub_area_PoselenieMihajlovoJarcevskoe +\n   dfa.sub_area_PoselenieKievskij +\n   dfa.sub_area_PoselenieKlenovskoe +\n   dfa.sub_area_PoselenieShhapovskoe )\n\n# For now eliminate case with ridiculous value of full_sq\ndfa = dfa[~dfa.fullhuge]\n\n    \n# Independent features\n\nindievars = [\"owner_occ\", \"state\", \"state_Xowner\", \"lnfull_Xowner\", \"mortgage_growth\"]\n\n\n# Complete list of features to use for fit\n\nallvars = fullvars + floorvars + maxvars + kitchvars + buildvars + matervars\nallvars += floorXvars + kitchXvars + subarvars + indievars\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"9565dc09-16e8-7f40-7622-91f4f2b865e8"},"source":"### Set up target variable, along with some related features for heteroskedasticity model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95a01ca0-d69e-fb05-3c09-31cf46ac1c43"},"outputs":[],"source":"# The normalized target variable:  log real sale price\ntraining = dfa[dfa.price_doc.notnull()]\ntraining.lnrp = training.price_doc.div(training.cpi).apply(np.log)\ny = training.lnrp\n\n# Features to use later in heteroskedasticity model\nmillion1 = (training.price_doc==1e6)\nmillion2 = (training.price_doc==2e6)\nmillion3 = (training.price_doc==3e6)\n\n# Create X matrix for fitting\nkeep = allvars + ['timestamp']  # Need to keep timestamp to calculate weights\nX = training[keep] "},{"cell_type":"markdown","metadata":{"_cell_guid":"a94d1ce7-66a9-56cf-8603-43d1d501b4e7"},"source":"### Set up (time-weights, imputation, scaling, etc.) for initial fit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e88893a1-db94-3d2d-d58a-15aad22bce19"},"outputs":[],"source":"def get_weights(df):\n    # Weight cases linearly on time\n    # with later cases (more like test data) weighted more heavily\n    basedate = pd.to_datetime(weight_base).toordinal() # Basedate gets a weight of zero\n    wtd = pd.to_datetime(df.timestamp).apply(lambda x: x.toordinal()) - basedate\n    wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n    return wts"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5094c157-11bf-8315-2249-e7a2cbfe962a"},"outputs":[],"source":"wts = get_weights(X)\nX = X.drop(\"timestamp\", axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"993b4393-0ef2-b1a1-00ee-0769d71474b3"},"outputs":[],"source":"if use_pipe:\n    from sklearn.preprocessing import Imputer, StandardScaler\n    from sklearn.pipeline import make_pipeline\n\n    # Make a pipeline that transforms X\n    pipe = make_pipeline(Imputer(), StandardScaler())\n    pipe.fit(X)\n    pipe.transform(X)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e5b72596-8f81-2965-859a-9cef6d13a155"},"source":"### Fit a bagged WLS and generate squared residuals"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"653d2512-1b26-71da-5d33-2a9c31691621"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import BaggingRegressor\nimport random\n\nrandom.seed = 100\nlr = LinearRegression(fit_intercept=True)\nbr = BaggingRegressor(lr)\nif use_pipe:\n    br.fit(pipe.transform(X), y, sample_weight=wts)\nelse:\n    br.fit(X, y, sample_weight=wts)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e57f7df2-4a6f-5a3d-e55f-0554a7ab3fa5"},"outputs":[],"source":"# Look for collinearity problems\nfor e in br.estimators_:\n    co = e.coef_\n    mask = np.abs(co)>1e4\n    print( X.columns[mask].values )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3875e84d-a48a-aa06-47ca-d4282b7cbeab"},"outputs":[],"source":"# Predict on the training set and take the residuals\nif use_pipe:\n    pred = br.predict( pipe.transform(X) )\nelse:\n    pred = br.predict(X)\nresids = y - pred\nresids2 = resids * resids"},{"cell_type":"markdown","metadata":{"_cell_guid":"62a72122-496c-a09a-b56d-d6fe9ee3de77"},"source":"### Fit the initial heteroskedsticity model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec8254af-e247-f499-45d8-f793bc707ca8"},"outputs":[],"source":"# Add heteroskedasticity-related features\nXhetero = X.assign(million1=million1, million2=million2, million3=million3, lnrp=y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"99e18986-4a1e-47f1-421f-c3cd2863cb33"},"source":"I tried this using raw squared residuals, but that model predicts a lot of negative values.  Given the distribution of the squared residuals, logs are more appropriate."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2f790dd5-1264-0093-b752-4e00bb4802ce"},"outputs":[],"source":"lnres2 = np.log(resids2)\nplt.hist(resids2, bins=20)\nplt.show()\nplt.hist(lnres2, bins=20)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36ef1ca5-d643-f001-3bef-d2fd7718a5ff"},"outputs":[],"source":"# For a visual look at the results of a rough model, use statsmodels.WLS\nfrom statsmodels.regression.linear_model import WLS\nxdat = Xhetero.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = lnres2.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=wts).fit()\nresult.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"8894cd12-6676-13d9-216e-22ff3b226fa1"},"source":"That's a regression sorely in need of regularization, so I'm going to fit a LASSO.  But SKLearn's version of LASSO doesn't allow for sample weights, so I first want to check the unweighted regression to see if the results are similar."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e0d7585-5c8e-8897-3fb4-ca5451623da1"},"outputs":[],"source":"from statsmodels.regression.linear_model import OLS\nresult = OLS(ydat, xdat).fit()\nresult.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"84dd2c6f-4e15-04e1-0ea2-e815b4aa1fda"},"source":"Close enough for a heteroskedasticity model, I think.  OK, let's do LASSO."},{"cell_type":"markdown","metadata":{"_cell_guid":"4410d585-0381-eb01-5978-db9d19794bf5"},"source":"### Regularize\n\nI experimented with various values of the LASSO parameter to see what gave reasonable-looking results.  Here's a range of possibilities:"},{"cell_type":"markdown","metadata":{"_cell_guid":"345040c2-aab2-5536-cee4-70ebf9b7e5e0"},"source":"#### First LASSO:  alpha=0.05"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f74f231-0226-c242-c98b-91c120054f6c"},"outputs":[],"source":"from sklearn.linear_model import Lasso\nla_hetero = Lasso(alpha=5e-2)\nif use_pipe:\n    newpipe = make_pipeline(Imputer(), StandardScaler())\n    newpipe.fit(Xhetero)\n    la_hetero.fit(newpipe.transform(Xhetero), lnres2)\n    pred_res2 = np.exp( la_hetero.predict( newpipe.transform(Xhetero) ) )\nelse:\n    la_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( la_hetero.predict(Xhetero) )\nprint( np.min(pred_res2) )\nprint( np.max(pred_res2) )\npd.DataFrame(Xhetero.columns, la_hetero.coef_)[np.abs(la_hetero.coef_)>1e-5]"},{"cell_type":"markdown","metadata":{"_cell_guid":"b44e2f40-1a4f-5c74-c1ec-cdedd1a8be10"},"source":"#### Second LASSO:  alpha=0.1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49710a52-9ef2-aa0b-a063-f1ffc4f3f686"},"outputs":[],"source":"la_hetero = Lasso(alpha=1e-1)\nif use_pipe:\n    la_hetero.fit(newpipe.transform(Xhetero), lnres2)\n    pred_res2 = np.exp( la_hetero.predict( newpipe.transform(Xhetero) ) )\nelse:\n    la_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( la_hetero.predict(Xhetero) )\nprint( np.min(pred_res2) )\nprint( np.max(pred_res2) )\npd.DataFrame(Xhetero.columns, la_hetero.coef_)[np.abs(la_hetero.coef_)>1e-5]"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f7cdaca-8eab-cb8d-4e90-d860bad1041f"},"source":"#### Third LASSO:  alpha=0.2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ed3f2a9-1a59-4937-7435-6aa3b484e5a2"},"outputs":[],"source":"la_hetero = Lasso(alpha=2e-1)\nif use_pipe:\n    la_hetero.fit(newpipe.transform(Xhetero), lnres2)\n    pred_res2 = np.exp( la_hetero.predict( newpipe.transform(Xhetero) ) )\nelse:\n    la_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( la_hetero.predict(Xhetero) )\nprint( np.min(pred_res2) )\nprint( np.max(pred_res2) )\npd.DataFrame(Xhetero.columns, la_hetero.coef_)[np.abs(la_hetero.coef_)>1e-5]"},{"cell_type":"markdown","metadata":{"_cell_guid":"d283657c-b09d-595e-2245-eb243c6f4442"},"source":"#### Fourth LASSO:  alpha=0.4"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e61d2328-9cad-e873-67bd-e12df980b3ee"},"outputs":[],"source":"la_hetero = Lasso(alpha=4e-1)\nif use_pipe:\n    la_hetero.fit(newpipe.transform(Xhetero), lnres2)\n    pred_res2 = np.exp( la_hetero.predict( newpipe.transform(Xhetero) ) )\nelse:\n    la_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( la_hetero.predict(Xhetero) )\nprint( np.min(pred_res2) )\nprint( np.max(pred_res2) )\npd.DataFrame(Xhetero.columns, la_hetero.coef_)[np.abs(la_hetero.coef_)>1e-5]"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf9c5464-84a5-f8f7-a573-97c2c3ca63c1"},"source":"Initially, I decided to fit an OLS, and then a WLS, to a slightly altered version of the variable set chosen by the third LASSO."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eee70398-680e-533a-bab6-1246dd0202b0"},"outputs":[],"source":"ls_hetero = LinearRegression(fit_intercept=True)\nxh = Xhetero[[\"lnfloor_Xowner\", \"sub_area_Nekrasovka\", \"owner_occ\",\n              \"state_Xowner\", \"million1\", \"million2\", \"million3\"]]\nif use_pipe:\n    newerpipe = make_pipeline(Imputer(), StandardScaler())\n    newerpipe.fit(xh)\n    ls_hetero.fit(newerpipe.transform(xh), lnres2)\n    pred_res2 = np.exp( ls_hetero.predict( newerpipe.transform(xh) ) )\nelse:\n    ls_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( ls_hetero.predict(Xhetero) )\nprint( np.min(pred_res2) )\nprint( np.max(pred_res2) )\nprint( np.mean(pred_res2) )\nprint( np.std(resids2) )\nprint( np.min(resids2) )\nprint( np.max(resids2) )\nprint( np.mean(resids2) )\nprint( np.std(resids2) )\n\npd.DataFrame(xh.columns, ls_hetero.coef_)[np.abs(ls_hetero.coef_)>1e-5]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8651a6a8-12d5-8f0e-6606-47236571d68a"},"outputs":[],"source":"xdat = xh.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = lnres2.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=wts).fit()\nresult.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"a8cf6858-8ecd-0c02-3aa9-8198ab81dae1"},"source":"Here's what impressed me:  I fit a bagged version of the WLS, using 20% random samples, and the coefficients are amazingly robust across bags -- I mean, about as robust as one could hope given the amount of random variation one expects in the data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a31a1ef-74fa-b2ad-f9ee-6a7a0a0650c5"},"outputs":[],"source":"random.seed = 200\nxin = xh\nlr = LinearRegression(fit_intercept=True)\nbr = BaggingRegressor(lr, max_samples=0.2)\nbr.fit(xin, lnres2, sample_weight=wts)\npd.DataFrame(data=[e.coef_ for e in br.estimators_], columns=xin.columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"65d153ff-92d2-f7dc-79ac-14d480d80137"},"source":"Just to make the point (and, at the time, to check that the samples were what I intended):  these are quite small samples relative to the total training data set, with only the randomly expected amount of overlap.  You should be able to see why I was impressed with the robustness."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f56e200-9db7-9453-d083-be5073f0ac07"},"outputs":[],"source":"sums = [sum(s) for s in br.estimators_samples_]\nprint( sums )\nprint (np.mean(sums))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e2a00dfa-0a23-f6ec-9de3-fc0936effe3a"},"outputs":[],"source":"print( xin.shape[0] )\nbr"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e86f029-55e5-a1b1-4975-2b8e0c71cee8"},"outputs":[],"source":"sum(br.estimators_samples_[0]*br.estimators_samples_[1])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"964fba6b-fee5-bad9-8f54-9c3aad96932d"},"outputs":[],"source":"br.estimators_samples_"},{"cell_type":"markdown","metadata":{"_cell_guid":"a8d6ca6e-bd53-6cca-bafc-9dbd2043c387"},"source":"But one really should also do a division by time.  A 50/50 split sample test showed less robustness and led me to omit some variables which had dramatically different coefficients on the sample halves (also because I was suspicious of them to begin with)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4ba981fb-6824-48ae-734b-b59ca1286a89"},"outputs":[],"source":"xin_top = xin[0:15235]\nr_top = lnres2[0:15235]\nxin_bot = xin[15236:30470]\nr_bot = lnres2[15236:30470]\nxdat = xin_top.copy().astype(np.float64)\nxdat[\"constant\"]=1\nrdat = r_top.copy().astype(np.float64)\nprint( OLS(rdat, xdat).fit().summary() )\nxdat = xin_bot.copy().astype(np.float64)\nxdat[\"constant\"]=1\nrdat = r_bot.copy().astype(np.float64)\nprint( OLS(rdat, xdat).fit().summary() )"},{"cell_type":"markdown","metadata":{"_cell_guid":"22fc7a84-a9e5-90dd-6036-a0c04564987d"},"source":"So here's what, at the time, was my \"final\" model, which I fit first as an exploratory WLS and then as a bagged WLS that was intended to generate my heteroskedasticity weights."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ff11bdf-dbdd-6394-a72c-8e1f2d9289ce"},"outputs":[],"source":"xh = Xhetero[[\"sub_area_Nekrasovka\", \"owner_occ\",\n              \"million1\", \"million2\", \"million3\"]]\nxdat = xh.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = lnres2.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=wts).fit()\nresult.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"689932dc-758e-d854-a896-c2da9198791f"},"outputs":[],"source":"random.seed = 200\nxin = xh\nlr = LinearRegression(fit_intercept=True)\nbr = BaggingRegressor(lr, max_samples=0.5)\nbr.fit(xin, lnres2, sample_weight=wts)\npd.DataFrame(data=[e.coef_ for e in br.estimators_], columns=xin.columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37c10420-fe08-b97a-39d3-1dffc0c98149"},"outputs":[],"source":"pred_res2 = np.exp( br.predict(xin) )\nprint()\nprint( np.min(pred_res2) )\nprint( np.min(resids2) )\nprint()\nprint( np.max(pred_res2) )\nprint( np.max(resids2) )\nprint()\nprint( np.mean(pred_res2) )\nprint( np.mean(resids2) )\nprint()\nprint( np.std(pred_res2) )\nprint( np.std(resids2) )"},{"cell_type":"markdown","metadata":{"_cell_guid":"2325c41f-4cec-ffd0-f736-22302c14f167"},"source":"Since all the predictors are binary and the model is linear, there are a small number of possible output values.  But the range of values is weird.  If I were to go with standard procedure and weight by the reciprocal of the predicted squared residuals (thereby equalizing the expected variance), I would, rather ridiculously, have to give 28 times as much weight to owner-occupied sales in Nekraskova as to typical (investment) sales.  And a price of 3 million rubles -- which, based on my past inspection of the data, is not necessarily far out of line and probably very often just represents a round figure for an otherwise reasonable price -- would further reduce the weight by a factor of 11."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c02857b-461b-8412-169b-3544ee8496cd"},"outputs":[],"source":"predvals = pd.Series(pred_res2).value_counts().sort_index()\npredvals"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e379aec-7e71-c10b-125d-8ececd245379"},"outputs":[],"source":"print( predvals.index.values[3]/predvals.index.values[0] )\nprint( predvals.index.values[8]/predvals.index.values[3] )"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c9013e9-5e66-d6ea-4d9d-f4c3fa696ffd"},"source":"So I tried using a \"waterdown\" parameter, where I add a constant to the predicted residuals and use the reciprocal of that as my sample weight.  The results were disappointing.  I won't go through everything I did, but I eventually decided to give up on my intuitively chosen bagged WLS and go back to LASSO, letting out-of-sample RMSLE choose the penalty coefficient and the \"waterdown\" parameter, as well the parameter for my time weights."},{"cell_type":"markdown","metadata":{"_cell_guid":"6f6ececc-1e70-0b87-dfec-884f30ddf4fc"},"source":"### Evaluate"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb731917-4160-e6e7-86e1-d153950b3fb2"},"outputs":[],"source":"from sklearn.metrics import make_scorer\n\ndef rmsle_exp(y_true_log, y_pred_log):\n    y_true = np.exp(y_true_log)\n    y_pred = np.exp(y_pred_log)\n    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n\ndef score_model(model, pipe=None):\n    if (pipe==None):\n        train_error = rmsle_exp(y_train, model.predict(X_train))\n        test_error = rmsle_exp(y_test, model.predict(X_test))\n    else:\n        train_error = rmsle_exp(y_train, model.predict(pipe.transform(X_train)))\n        test_error = rmsle_exp(y_test, model.predict(pipe.transform(X_test)))\n    return train_error, test_error"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f59add00-5a69-4960-ff98-57144e4e4ec5"},"outputs":[],"source":"len(X)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a0f9194a-74a5-71b3-93e7-c77995dea321"},"source":"Split the training data by time into a 75% training sample and a 25% test sample.  (Note that the data were already given to us in order of time.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0594a29-959b-8085-2cad-23eaae8f4b8e"},"outputs":[],"source":"split = int(.75*len(X))\nX_train = X[0:split]\nX_test = X[split:len(X)]\ny_train = y[0:split]\ny_test = y[split:len(y)]\nwts_train = wts[0:split]"},{"cell_type":"markdown","metadata":{"_cell_guid":"81eab13d-84f5-24d8-56be-904cf0cd766f"},"source":"Do a grid search to choose the most successful combination of LASSO penalty coefficient, waterdown parameter for predicted squared residuals, and zero date for time weighting.  (Note that I have given up on bagging now.  Originally I did some runs which used bagging, and the bagged versions almost always did worse out of sample.  So these just use plain WLS.)\n\n*This takes too long to run on Kaggle, so I've replaced the grid definitions with abbreviated ones.  The originals are still in the code, though, but they're superseded*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c78599b4-82fb-e8a1-93e6-46c35802901d"},"outputs":[],"source":"from sklearn.linear_model import Lasso\nnewpipe = make_pipeline(Imputer(), StandardScaler())\nnewpipe.fit(Xhetero)\n\nminval = 100.\nwhere_min = [\"Never\",0.,0.]\nresults = pd.DataFrame()\nalphas = [5e-1,2e-1,1e-1,5e-2,2e-2,1e-2,5e-3,2e-3]\n# DELETE NEXT LINE TO RUN FULL VERSION\nalphas = [5e-1,5e-2,2e-3]\nfor alpha in alphas:\n    print( \"alpha=\", alpha)\n    la_hetero = Lasso(alpha=alpha)\n    la_hetero.fit(newpipe.transform(Xhetero), lnres2)\n    pred_res2 = np.exp( la_hetero.predict( newpipe.transform(Xhetero) ) )\n    la_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( la_hetero.predict(Xhetero) )\n    wbvalues = [\"2010-01-01\", \"2010-04-01\", \"2010-07-01\",\"2010-10-01\",\n                \"2011-01-01\", \"2011-04-01\", \"2011-07-01\"]\n# DELETE NEXT LINE TO RUN FULL VERSION\n    wbvalues = [\"2010-01-01\", \"2011-01-01\", \"2011-07-01\"]\n    for wbase in wbvalues:\n        print( \"    wbase=\", wbase)\n        basedate = pd.to_datetime(wbase).toordinal() # Basedate gets a weight of zero\n        wtd = pd.to_datetime(training.timestamp).apply(lambda x: x.toordinal()) - basedate\n        wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n        row = []\n        wdvalues = [0,1,2,4,8,16,32,64,128,256,512]\n# DELETE NEXT LINE TO RUN FULL VERSION\n        wdvalues = [1,32,512]\n        for waterdown in wdvalues:\n            pred_wd = pred_res2 + waterdown\n            wts_train = (wts * (pred_wd)) [0:split]\n            lr.fit(X_train, y_train, sample_weight=wts_train)\n            test_error = rmsle_exp(y_test, lr.predict(X_test))\n            if test_error < minval:\n                minval = test_error\n                where_min = [wbase, alpha, waterdown]\n            row = row + [test_error]\n        index = pd.MultiIndex.from_tuples([(wbase,alpha)], names=['wbase', 'alpha'])\n        dfrow = pd.DataFrame( index=index, data=[row], columns=wdvalues)\n        results = results.append( dfrow )\nprint( where_min )\nprint( minval )\nresults"},{"cell_type":"markdown","metadata":{"_cell_guid":"0fa08442-6b78-d27e-772a-fe379ee8af36"},"source":"The choice was 512 (my proxy for infinity) for the waterdown parameter, so the LASSO penalty coefficient is irrelevant.  And the results favor the most aggressive version of time weighting.  I did another grid search below, which is a combination of fine tuning and hoping the data would change their mind.\n\n*Again, I've abbreviated the grid.*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2857f6af-0bc2-6267-a149-4d7c8a9fce0f"},"outputs":[],"source":"minval = 100.\nwhere_min = [\"Never\",0.,0.]\nresults = pd.DataFrame()\nalphas = [2e-1,1e-1,5e-2,2e-2,1e-2]\n# DELETE NEXT LINE TO RUN FULL VERSION\nalphas = [2e-1,1e-2]\nfor alpha in alphas:\n    print( \"alpha=\", alpha)\n    la_hetero = Lasso(alpha=alpha)\n    la_hetero.fit(newpipe.transform(Xhetero), lnres2)\n    pred_res2 = np.exp( la_hetero.predict( newpipe.transform(Xhetero) ) )\n    la_hetero.fit(Xhetero, lnres2)\n    pred_res2 = np.exp( la_hetero.predict(Xhetero) )\n    wbvalues = [\"2011-06-01\", \"2011-06-15\", \"2011-07-01\",\"2010-07-15\",\n                \"2011-08-01\", \"2011-08-15\"]\n# DELETE NEXT LINE TO RUN FULL VERSION\n    wbvalues = [\"2011-06-15\", \"2011-08-15\"]\n    for wbase in wbvalues:\n        print( \"    wbase=\", wbase)\n        basedate = pd.to_datetime(wbase).toordinal() # Basedate gets a weight of zero\n        wtd = pd.to_datetime(training.timestamp).apply(lambda x: x.toordinal()) - basedate\n        wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n        row = []\n        wdvalues = [0,2,8,32,128,512]\n# DELETE NEXT LINE TO RUN FULL VERSION\n        wdvalues = [8,512]\n        for waterdown in wdvalues:\n            pred_wd = pred_res2 + waterdown\n            wts_train = (wts * (pred_wd)) [0:split]\n            lr.fit(X_train, y_train, sample_weight=wts_train)\n            test_error = rmsle_exp(y_test, lr.predict(X_test))\n            if test_error < minval:\n                minval = test_error\n                where_min = [wbase, alpha, waterdown]\n            row = row + [test_error]\n        index = pd.MultiIndex.from_tuples([(wbase,alpha)], names=['wbase', 'alpha'])\n        dfrow = pd.DataFrame( index=index, data=[row], columns=wdvalues)\n        results = results.append( dfrow )\nprint( where_min )\nprint( minval )\nresults            "},{"cell_type":"markdown","metadata":{"_cell_guid":"e2d449e3-9f20-c042-53a9-0cf8fe76923d"},"source":"Again, the data tell me to throw away the heteroskedasticity weights (water them down to nothing) and choose the most aggressive time weighting.  In subsequent analysis, I've arbitrarily chosen an even slightly more aggressive time weighting, with 2011-08-19 as the zero date, so the initial data points are given the smallest possible nonzero weight in my linear weighting scheme.  Call that OCD, or Occam's Razor: the simplest way to choose time weights is to start from zero and ramp up linearly."},{"cell_type":"markdown","metadata":{"_cell_guid":"9320f904-e019-bc1a-645a-739240dcc537"},"source":"## Ditch the heteroskedasticity weighting and just fit with time weights"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9d604f30-d00f-f7fe-75dc-be348dba38df"},"outputs":[],"source":"weight_base = \"2010-08-19\"\nwts = get_weights(training)\nnewwts = wts\nfrom sklearn.linear_model import LinearRegression"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b804d93-c529-09b0-acca-a67907db561a"},"outputs":[],"source":"lr = LinearRegression(fit_intercept=True)\nif use_pipe:\n    lr.fit(pipe.transform(X), y, sample_weight=newwts)\nelse:\n    lr.fit(X, y, sample_weight=newwts)"},{"cell_type":"markdown","metadata":{"_cell_guid":"90faa170-bf07-bd79-268a-70a426a1363a"},"source":"### Predict on the test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e87e218-113f-a94a-3468-2469cab9adfe"},"outputs":[],"source":"testing = dfa[dfa.price_doc.isnull()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18bd4e67-21cc-9e34-0b8f-d6948241e64c"},"outputs":[],"source":"df_test = pd.DataFrame(columns=X.columns)\nfor column in df_test.columns:\n        df_test[column] = testing[column]        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb9f6def-ce9a-9901-8d26-d5a24d18d705"},"outputs":[],"source":"# Make the predictions\nif use_pipe:\n    pred = lr.predict( pipe.transform(df_test) )\nelse:\n    pred = lr.predict(df_test)\npredictions = np.exp(pred)*testing.cpi\n\n# And put this in a dataframe\npredictions_df = pd.DataFrame()\npredictions_df['id'] = testing['id']\npredictions_df['price_doc'] = predictions\npredictions_df.head()\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7d9c8eb2-0ca3-c363-4a19-1f1c7fcc6489"},"outputs":[],"source":"predictions_df.to_csv('wls_predictions.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5baabd3-beed-13d5-6e42-5def51429eb1"},"source":"### Postlogue:  Inspect the model fit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c092117-0172-a7f4-2a18-85481181d794"},"outputs":[],"source":"# Check for ridiculous coefficients, likely indicating collinearity\nco = lr.coef_\nra = range(len(co))\nmask = np.abs(co)>1e4\nX.columns[mask].values\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89423444-3c47-f76b-1095-8e25e37b071a"},"outputs":[],"source":"from statsmodels.regression.linear_model import WLS\nxdat = X.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = y.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=newwts).fit()\nresult.summary()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d6bb63b-0a42-c958-4446-df6bb3e1d3c0"},"outputs":[],"source":"# Note that, if the model is run without the pipe transform, the coefficients below\n#  should be the same as those above.  Sometimes they are, sometimes not.\n#  If they're not the same, probably numerical instability due to collinearity.\npd.DataFrame(X.columns, co)"},{"cell_type":"markdown","metadata":{"_cell_guid":"84f8957d-8c65-ef30-de20-f029de1f181e"},"source":"### Frequencies of sub_areas (completely out of the blue)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8a8daa36-c023-6cb5-d7c3-831497863ac2"},"source":"I'm going to use these in the next analysis, to collapse sub_areas with fewer than 10 cases into geographically contiguous aggregates.  I had to do something to point forward after such a disappointing analysis."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79792a85-86fd-eb53-df75-18ce33461b30"},"outputs":[],"source":"train.sub_area.value_counts().tail(20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff1c58e3-4276-8c0c-2a23-da58a5b690a7"},"outputs":[],"source":"print(train.sub_area.sort_values().unique()[1:170])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6f16081-b6aa-2509-8a41-9caae88d55ca"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}