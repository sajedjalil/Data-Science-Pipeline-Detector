{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"eb797542-13a5-d3ae-06f7-7148178f3c74","_active":false,"collapsed":false},"source":"This version adjusts results using average predicted prices from a macro model.\n\nRight now it adjusts each of the 3 models (Jason, Reynaldo, Bruno) separately (because that's how I happened to write it), but that is kind of silly.  It would be more reasonable to combine the results first and then adjust the combined result.\n\nAlso there is a provision for applying a humility factor (again separately to each model, which is silly).  The humility factor adjusts individual predictions toward the mean.  The combination of the macro adjustment and the humility factor is equivalent to scaling down and adding a constant, which is what the magic numbers used to do.  Now I'm thinking I can add a macro humility factor too, and that will give enough degrees of freedom to get back the effect of the magic numbers, but the new magic numbers will be humility parameters instead of just numbers that make the public leaderboard score higher.  Of course I will adjust the humility parameters to maximize the public leaderboard score, but now with a stronger theoretical justification.  (This amounts to using the public test cases as a cross-validation set, which is not unreasonable.)\n\nAlso, setting humility parameters, while having an *actual* macro model, means that the overall model can be used to predict the future beyond May 2016.  Because there's a macro model, that model would be re-fit and used to predict future means, and the same humility parameters (or new ones, if one decides to change them based on private leaderboard performance) can be applied.  You'll get actual predictions instead of the nonsense you would get by applying an arbitrary linear transformation that just happened to work well for the original test period.","outputs":[]},{"metadata":{"_cell_guid":"a0320923-48e8-7105-9228-bee42fa1fc41","_active":true,"collapsed":false},"source":"# Get ready for lots of annoying deprecation warnings\nimport statsmodels.api as sm","execution_count":17,"cell_type":"code","outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":18,"metadata":{"_cell_guid":"31a987d3-1475-7b13-8da3-9f904856d915","_active":false,"collapsed":false},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\nimport datetime\n","execution_state":"idle"},{"metadata":{"_cell_guid":"3d6f23a0-8a54-edf6-a793-14d6cd965335","_active":false,"collapsed":false},"source":"## Fit the macro model and compute average prediction","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"_cell_guid":"1c611c87-b384-4b03-862f-917717df8c3e","_active":false,"collapsed":false},"source":"# Read data\nmacro = pd.read_csv('../input/macro.csv')\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Macro data monthly medians\nmacro[\"timestamp\"] = pd.to_datetime(macro[\"timestamp\"])\nmacro[\"year\"]  = macro[\"timestamp\"].dt.year\nmacro[\"month\"] = macro[\"timestamp\"].dt.month\nmacro[\"yearmonth\"] = 100*macro.year + macro.month\nmacmeds = macro.groupby(\"yearmonth\").median()\n\n# Price data monthly medians\ntrain[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"year\"]  = train[\"timestamp\"].dt.year\ntrain[\"month\"] = train[\"timestamp\"].dt.month\ntrain[\"yearmonth\"] = 100*train.year + train.month\nprices = train[[\"yearmonth\",\"price_doc\"]]\np = prices.groupby(\"yearmonth\").median()\n\n# Join monthly prices to macro data\ndf = macmeds.join(p)","execution_count":2,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"c7fef4e9-279a-dbc2-f80e-8ea82eec262f","_active":false,"collapsed":false},"source":"# Function to process Almon lags\n\nimport numpy.matlib as ml\n \ndef almonZmatrix(X, maxlag, maxdeg):\n    \"\"\"\n    Creates the Z matrix corresponding to vector X.\n    \"\"\"\n    n = len(X)\n    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n    for t in range(maxlag,  n):\n       #Solve for Z[t][0].\n       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n       for j in range(1, maxdeg+1):\n             s = 0.0\n             for i in range(1, maxlag+1):       \n                s += (i)**j * X[t-i]\n             Z[t-maxlag,j] = s\n    return Z","execution_count":3,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"47b5ffb5-90c5-3107-5922-957e48c2a9fe","_active":false,"collapsed":false},"source":"# Prepare data for macro model\ny = df.price_doc.div(df.cpi).apply(np.log).loc[201108:201506]\nlncpi = df.cpi.apply(np.log)\ntblags = 5    # Number of lags used on PDL for Trade Balance\nmrlags = 5    # Number of lags used on PDL for Mortgage Rate\ncplags = 5    # Number of lags used on PDL for CPI\nztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\nzmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\nzcp = almonZmatrix(lncpi.loc[201103:201506].as_matrix(), cplags, 1)\ncolumns = ['tb0', 'tb1', 'mr0', 'mr1', 'cp0', 'cp1']\nz = pd.DataFrame( np.concatenate( (ztb, zmr, zcp), axis=1), y.index.values, columns )\nX = sm.add_constant( z )\n\n# Fit macro model\neq = sm.OLS(y, X)\nfit = eq.fit()\n\n# Predict with macro model\ntest_cpi = df.cpi.loc[201507:201605]\ntest_index = test_cpi.index\nztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\nzmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\nzcp_test = almonZmatrix(lncpi.loc[201502:201605].as_matrix(), cplags, 1)\nz_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test, zcp_test), axis=1), \n                       test_index, columns )\nX_test = sm.add_constant( z_test )\npred_lnrp = fit.predict( X_test )\npred_p = np.exp(pred_lnrp) * test_cpi\n\n# Merge with test cases and compute mean for macro prediction\ntest[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\ntest[\"year\"]  = test[\"timestamp\"].dt.year\ntest[\"month\"] = test[\"timestamp\"].dt.month\ntest[\"yearmonth\"] = 100*test.year + test.month\ntest_ids = test[[\"yearmonth\",\"id\"]]\nmonthprices = pd.DataFrame({\"yearmonth\":pred_p.index.values,\"monthprice\":pred_p.values})\nmacro_mean = np.exp(test_ids.merge(monthprices, on=\"yearmonth\").monthprice.apply(np.log).mean())\nmacro_mean","execution_count":4,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"38ef4aa4-4510-c080-cf88-f398fd3f18fc","_active":false,"collapsed":false},"source":"## Fit Jason's model and adjust predictions","execution_count":16,"cell_type":"markdown","outputs":[],"execution_state":"idle"},{"cell_type":"code","execution_count":5,"metadata":{"_cell_guid":"20e58eb5-0375-6e87-1bae-aefbe1e136e0","_active":false,"collapsed":false},"outputs":[],"source":"# Jason/Gunja\n\n\n\n#load files\ntrain = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\nmacro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\nid_test = test.id\n\n#multiplier = 0.969\n\n#clean data\nbad_index = train[train.life_sq > train.full_sq].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\ntest.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\nbad_index = test[test.life_sq > test.full_sq].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.life_sq < 5].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = test[test.life_sq < 5].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.full_sq < 5].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[test.full_sq < 5].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ntrain.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = train[train.kitch_sq >= train.life_sq].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[test.kitch_sq >= test.life_sq].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = train[train.life_sq > 300].index\ntrain.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\nbad_index = test[test.life_sq > 200].index\ntest.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ntrain.product_type.value_counts(normalize= True)\ntest.product_type.value_counts(normalize= True)\nbad_index = train[train.build_year < 1500].index\ntrain.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = test[test.build_year < 1500].index\ntest.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = train[train.num_room == 0].index \ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = test[test.num_room == 0].index \ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [3174, 7313]\ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\ntrain.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = train[train.floor == 0].index\ntrain.ix[bad_index, \"floor\"] = np.NaN\nbad_index = train[train.max_floor == 0].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.max_floor == 0].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = train[train.floor > train.max_floor].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.floor > test.max_floor].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\ntrain.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ntrain.ix[bad_index, \"floor\"] = np.NaN\ntrain.material.value_counts()\ntest.material.value_counts()\ntrain.state.value_counts()\nbad_index = train[train.state == 33].index\ntrain.ix[bad_index, \"state\"] = np.NaN\ntest.state.value_counts()\n\n# brings error down a lot by removing extreme price per sqm\ntrain.loc[train.full_sq == 0, 'full_sq'] = 50\ntrain = train[train.price_doc/train.full_sq <= 600000]\ntrain = train[train.price_doc/train.full_sq >= 10000]\n\n# Add month-year\nmonth_year = (train.timestamp.dt.month + train.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ntrain['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\nmonth_year = (test.timestamp.dt.month + test.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ntest['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (train.timestamp.dt.weekofyear + train.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ntrain['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\nweek_year = (test.timestamp.dt.weekofyear + test.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ntest['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ntrain['month'] = train.timestamp.dt.month\ntrain['dow'] = train.timestamp.dt.dayofweek\n\ntest['month'] = test.timestamp.dt.month\ntest['dow'] = test.timestamp.dt.dayofweek\n\n# Other feature engineering\ntrain['rel_floor'] = train['floor'] / train['max_floor'].astype(float)\ntrain['rel_kitch_sq'] = train['kitch_sq'] / train['full_sq'].astype(float)\n\ntest['rel_floor'] = test['floor'] / test['max_floor'].astype(float)\ntest['rel_kitch_sq'] = test['kitch_sq'] / test['full_sq'].astype(float)\n\ntrain.apartment_name=train.sub_area + train['metro_km_avto'].astype(str)\ntest.apartment_name=test.sub_area + train['metro_km_avto'].astype(str)\n\ntrain['room_size'] = train['life_sq'] / train['num_room'].astype(float)\ntest['room_size'] = test['life_sq'] / test['num_room'].astype(float)\n\ny_train = train[\"price_doc\"]\nwts = 1 - .47*(y_train == 1e6)\nx_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\nx_test = test.drop([\"id\", \"timestamp\"], axis=1)\n\nfor c in x_train.columns:\n    if x_train[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(x_train[c].values)) \n        x_train[c] = lbl.transform(list(x_train[c].values))\n        #x_train.drop(c,axis=1,inplace=True)\n        \nfor c in x_test.columns:\n    if x_test[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(x_test[c].values)) \n        x_test[c] = lbl.transform(list(x_test[c].values))\n        #x_test.drop(c,axis=1,inplace=True)  \n\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train, weight=wts)\ndtest = xgb.DMatrix(x_test)\n\n#cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n#    verbose_eval=50, show_stdv=False)\n#cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n\n#num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=350)\n\n#fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n#xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)\n\ny_predict = model.predict(dtest)","execution_state":"idle"},{"metadata":{"_cell_guid":"23b82f48-8faf-4c79-b8a7-59ee20b5f0a0","_active":false,"collapsed":false},"source":"micro_mean = np.exp( np.log(y_predict).mean() )","execution_count":7,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"29b54f67-0535-15b3-7a54-aa040224d3f1","_active":false,"collapsed":false},"source":"y_predict = y_predict * macro_mean / micro_mean\nhumility_factor = 1.0  # Set less than 1.0 to make predictions humble\ny_predict = humility_factor * (y_predict - macro_mean) + macro_mean\ny_predict = np.round(y_predict)\ngunja_output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\ngunja_output.head()","execution_count":8,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"e6a6e979-dad7-3fb4-41d9-bd4da3dbf3c8","_active":false,"collapsed":false},"source":"## Fit Reynaldo's model and adjust predictions","execution_count":null,"cell_type":"markdown","outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"ef1a4a17-c328-b2eb-4861-ccee2e5c1b2d","_active":false,"collapsed":false},"outputs":[],"source":"# Reynaldo\n\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nid_test = test.id\n\ny_train = train[\"price_doc\"]\nx_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\nx_test = test.drop([\"id\", \"timestamp\"], axis=1)\n\nfor c in x_train.columns:\n    if x_train[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(x_train[c].values)) \n        x_train[c] = lbl.transform(list(x_train[c].values))\n        \nfor c in x_test.columns:\n    if x_test[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(x_test[c].values)) \n        x_test[c] = lbl.transform(list(x_test[c].values))\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 384  # This was the CV output, as earlier version shows\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)\n\ny_predict = model.predict(dtest)\n\n","execution_state":"idle"},{"metadata":{"_cell_guid":"a9b414f0-84a6-343d-1277-174a73fe100a","_active":false,"collapsed":false},"source":"micro_mean = np.exp( np.log(y_predict).mean() )","execution_count":10,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"3b9e7293-fc47-5f11-3d95-6934856446a0","_active":false,"collapsed":false},"source":"y_predict = y_predict * macro_mean / micro_mean\nhumility_factor = 1.0  # Set less than 1.0 to make predictions humble\ny_predict = humility_factor * (y_predict - macro_mean) + macro_mean\noutput = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\noutput.head()\n","execution_count":11,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"7cc5b5c7-0626-97e2-4164-3e7fe3321da5","_active":false,"collapsed":false},"source":"## Fit Bruno's model and adjust predictions","execution_count":null,"cell_type":"markdown","outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"f3792f8a-3c79-b307-c699-6295e0358443","_active":false,"collapsed":false},"outputs":[],"source":"# Bruno with outlier dropped\n\n\n\n# Any results you write to the current directory are saved as output.\ndf_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndf_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n\ndf_train.drop(df_train[df_train[\"life_sq\"] > 7000].index, inplace=True)\n\ny_train = df_train['price_doc'].values\nid_test = df_test['id']\n\ndf_train.drop(['id', 'price_doc'], axis=1, inplace=True)\ndf_test.drop(['id'], axis=1, inplace=True)\n\nnum_train = len(df_train)\ndf_all = pd.concat([df_train, df_test])\n# Next line just adds a lot of NA columns (becuase \"join\" only works on indexes)\n# but somewhow it seems to affect the result\ndf_all = df_all.join(df_macro, on='timestamp', rsuffix='_macro')\nprint(df_all.shape)\n\n# Add month-year\nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf_all['month'] = df_all.timestamp.dt.month\ndf_all['dow'] = df_all.timestamp.dt.dayofweek\n\n# Other feature engineering\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\ndf_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n# Remove timestamp column (may overfit the model in train)\ndf_all.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)\n\n\nfactorize = lambda t: pd.factorize(t[1])[0]\n\ndf_obj = df_all.select_dtypes(include=['object'])\n\nX_all = np.c_[\n    df_all.select_dtypes(exclude=['object']).values,\n    np.array(list(map(factorize, df_obj.iteritems()))).T\n]\nprint(X_all.shape)\n\nX_train = X_all[:num_train]\nX_test = X_all[num_train:]\n\n\n# Deal with categorical values\ndf_numeric = df_all.select_dtypes(exclude=['object'])\ndf_obj = df_all.select_dtypes(include=['object']).copy()\n\nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n\ndf_values = pd.concat([df_numeric, df_obj], axis=1)\n\n\n# Convert to numpy values\nX_all = df_values.values\nprint(X_all.shape)\n\nX_train = X_all[:num_train]\nX_test = X_all[num_train:]\n\ndf_columns = df_values.columns\n\n\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\ndtest = xgb.DMatrix(X_test, feature_names=df_columns)\n\n\nnum_boost_round = 489  # From Bruno's original CV, I think\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)\n\ny_pred = model.predict(dtest)\n\n","execution_state":"idle"},{"metadata":{"_cell_guid":"b192fc4a-5f01-4f22-d84e-e590c846d689","_active":false,"collapsed":false},"source":"micro_mean = np.exp( np.log(y_pred).mean() )","execution_count":13,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"cd024003-2065-9813-d691-1d61664ddc8a","_active":false,"collapsed":false},"source":"y_predict = y_pred * macro_mean / micro_mean\nhumility_factor = 1.0  # Set less than 1.0 to make predictions humble\ny_predict = humility_factor * (y_predict - macro_mean) + macro_mean\n\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\ndf_sub.head()","execution_count":14,"cell_type":"code","outputs":[],"execution_state":"idle"},{"metadata":{"_cell_guid":"4c2eefab-fa97-a1d4-1b9e-574522f3e3c7","_active":false,"collapsed":false},"source":"## Merge the results","execution_count":null,"cell_type":"markdown","outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"41c97a7b-f063-7501-039b-7d1d681821a5","_active":false},"outputs":[],"source":"# Merge\n\n\nfirst_result = output.merge(df_sub, on=\"id\", suffixes=['_louis','_bruno'])\nfirst_result[\"price_doc\"] = np.exp( .714*np.log(first_result.price_doc_louis) + \n                                    .286*np.log(first_result.price_doc_bruno) )  # multiplies out to .5 & .2\nresult = first_result.merge(gunja_output, on=\"id\", suffixes=['_follow','_gunja'])\n\nresult[\"price_doc\"] = np.exp( .7*np.log(result.price_doc_follow) + \n                              .3*np.log(result.price_doc_gunja) )\nresult.drop([\"price_doc_louis\",\"price_doc_bruno\",\"price_doc_follow\",\"price_doc_gunja\"],axis=1,inplace=True)\nresult.head()","execution_state":"idle"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"692204b8-f18b-7d7e-1028-0f3db8b52598","_active":false},"outputs":[],"source":"result.to_csv('sub.csv', index=False)"}]}