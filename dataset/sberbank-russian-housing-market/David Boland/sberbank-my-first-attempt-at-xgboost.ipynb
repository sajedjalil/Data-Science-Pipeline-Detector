{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"e78b05e7-6704-8793-b749-57897e7182e1"},"source":"##About XGBoost\nXGBoost is an advanced implementation of the gradient boosting algorithm. \n\nXGBoost includes regularisation to avoid overfitting and uses parallel computing to improve performance.\n\nHere I am using it as a standalone entity without any Exploratory Data Analysis - this is because it has an in-built routine to handle missing values. We supply a different value and pass it as a parameter then Xgboost will try different things as it encounters missing values on each node and learn which path to take for missing values in the future. \n\nXGBoost will make splits on nodes up to the max_depth parameter specified, then it will prune the tree backwards and remove splits beyond which there is no positive gain\n\nXGBoost has built-in cross-validation at each iteration of the boosting process. \n\nAs a comparison please see my other notebook with EDA and random forest"},{"cell_type":"markdown","metadata":{"_cell_guid":"a29fd27d-48f9-f1db-57b6-048e164d8d5c"},"source":"## Beginning of routine\nWe start by importing the various libraries we are going to use.\nWe just need four in this example. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59baa4a0-d90f-6c37-3b65-548bd0cfcae2"},"outputs":[],"source":"import numpy as np # mathematical library including linear algebra\nimport pandas as pd #data processing and CSV file input / output\nfrom sklearn import model_selection, preprocessing # sklearn is the machine learning library\nimport xgboost as xgb # this is the extreme gradient boosting library"},{"cell_type":"markdown","metadata":{"_cell_guid":"b9b60b37-a2ab-d3d3-5033-a9ed07d66ee3"},"source":"##Read in data\nNow we read in the training and test data. We also read in the macro economic variables. We are using Pandas \"read_csv\" function for this."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"453ab9a5-c6ed-ffb5-7557-59cdda0589b5"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nmacro = pd.read_csv('../input/macro.csv')\nid_test = test.id"},{"cell_type":"markdown","metadata":{"_cell_guid":"e19107bc-8b80-b97d-be5b-69633af3e4a3"},"source":"## Set our response variables and perform any data modifications\nWe set y_train to be the price_doc variable - our required prediction\nWe then drop id, timestamp and price_doc from the training set to use in the prediction\n\nTo be consistent we also drop id and timestamp from our test data set.\n\nNormally we would do both of these together by combining our train and test sets for data wrangling but in this instance this affects performance severely. \n\nThe modification to the training price_doc reflects movement in house prices \nbetween the times in the training set versus the test set - we try to have them consistent"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4143c3ec-79e0-2d3b-87e0-5594b06e57b7"},"outputs":[],"source":"y_train = train[\"price_doc\"] * .969 + 10\nx_train = train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\nx_test = test.drop([\"id\", \"timestamp\"], axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"972831f6-323b-cb31-eaa3-f4bded3d4970"},"source":"##Fitting the model\nWe run through each column in the training set and give it a label. We do this using the preprocessing.LabelEncoder() function from the sklearn library.\nThis function takes a list of values and transforms non-numerical labels to numerical values. We require our labels to have numerical values for use in most algorithms and in partcular, the XGBoost algorithm.\n\nWe then repeat the process for the test set - again we would normally do this on a combined test / train for consistency."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d67d70d7-cc8a-7053-c8b9-04299b74423a"},"outputs":[],"source":"for c in x_train.columns:\n    if x_train[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder() # set an instance of the label encoder\n        lbl.fit(list(x_train[c].values)) # fit it to the values of the training set column headers\n        x_train[c] = lbl.transform(list(x_train[c].values)) # Have them transformed to encoded labels\n        \nfor c in x_test.columns:\n    if x_test[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder() # set an instance of the label encoder\n        lbl.fit(list(x_test[c].values)) # fit it to the values of the test set column headers\n        x_test[c] = lbl.transform(list(x_test[c].values)) # Have them transformed to encoded labels"},{"cell_type":"markdown","metadata":{"_cell_guid":"358f782a-cfa9-01cf-06ae-92ff33dbf5e2"},"source":"#Set the parameters for xgboost as follows:\n\n##Booster parameters \nThese parameters are used to optimise the algorithm in terms of both accuracy and performance.\n\n**eta: 0.05**  - the default value for this parameter is 0.3. This is similar to the learning rate (alpha) in gradient descent. \nMakes the model more robust by shrinking the weights on each step. Typical final values range from 0.01-0.2\n\n**max_depth: 5** - the default here is 6. It sets the maximum depth of a tree and is used to control over-fitting as higher depth allows the model to learn relations very specific to a particular sample. We tune it using cross-validation. Typical values range from 3-10\n\n**subsample: 0.7** - the default here is 1. It denotes the fraction of obeservations to be randomly samples for each tree. Lower values make the algorithm conservative and prevent overfitting but too small and we may get under-fitting. Typical values range from 0-1\n\n**colsample_bytree: 0.7** - the default here is 1. It denotes the fraction of columns to be randomly samples for each tree. Typical values range from 0.5-1\n\n##Learning Task Parameters\nThese parameters are used to define the optimisation metric to be calculated at each step.\n\n**'eval_metric': 'rmse'** sets our evaluation metric to root mean squared error\n    This  evaluation metric used to score submissions in this competition is the log root mean squared error, however this option is not available to us within xgboost so this is the closest match.\n\n## General parameters\n**booster** - left at default by not setting it, which means we are using a tree-based model. It can also be set to use linear models.\n\n**silent: 1** - this defaults to 0 and is a binary switch. When set to 0 running messages will be printed which may help to understand the model. It can be set to 1 to suppress running messages."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"376534b5-54cd-ae75-0316-9ba476146980"},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}"},{"cell_type":"markdown","metadata":{"_cell_guid":"2f625037-f77c-9add-db35-30f4f4d30f9e"},"source":"##Import the train and test sets to XGBoost and create a cross-validation set\nFormat the train and test sets we modified above for use in xgboost \n(Dmatrix is the format required by the xgboost library)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"04c52bd1-428e-698f-0104-4be7cc7d62d6"},"outputs":[],"source":"dtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"74214476-3c46-d816-0292-b3c62cdff8ba"},"source":"Create a cross-validation set and define the early stopping criteria.\nThe num_boost_round parameter sets the number of iterations of the algorithm. \nHere it is set to just 200 to speed up the run but in practice \nwe should set it to something like 1000"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c3da6d1-0875-acac-4156-9dbc228a7a1c"},"outputs":[],"source":"cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=200, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d07d8ea7-ac57-f971-3b14-216b7f9e4e3e"},"source":"##Train the algorithm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e23b720-ebfe-5ecf-e58f-9a5119dfd2be"},"outputs":[],"source":"num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params), dtrain, num_boost_round= num_boost_rounds)"},{"cell_type":"markdown","metadata":{"_cell_guid":"76bc2ff8-f524-d937-2f4b-4ea02bdaa5f9"},"source":"##Now we can make a prediction of house prices in our test cases"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"733fe57d-a793-c47a-f987-d688d343855c"},"outputs":[],"source":"y_predict = model.predict(dtest)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"74d744a8-ba02-111c-aaf8-4e61e24f19c4"},"source":"##Store our predictions for submission\nWe need to submit our results in a prescribed format. Two columns containing the id and the price.\nFirst format the output and then write the formatted data to csv for submission."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a71e14b3-c842-153a-7b2b-696f0482e9f8"},"outputs":[],"source":"output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n\noutput.to_csv('xgbSub.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}