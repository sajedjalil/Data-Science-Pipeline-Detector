{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Modeling movements to predict Defensive Pass Interference"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\n# setting plots' configuration\nplt.rcParams[\"figure.figsize\"]= 50,30\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.patches as mpatches\nfrom matplotlib.patches import FancyArrowPatch\n\nimport scipy.special\nimport math\nimport random\nfrom scipy.stats import multivariate_normal\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplays = pd.read_csv('../input/nfl-big-data-bowl-2021/plays.csv')\ngames = pd.read_csv('../input/nfl-big-data-bowl-2021/games.csv')\n\nweek1 = pd.read_csv('../input/nfl-big-data-bowl-2021/week1.csv')\nweek2 = pd.read_csv('../input/nfl-big-data-bowl-2021/week2.csv')\nweek3 = pd.read_csv('../input/nfl-big-data-bowl-2021/week3.csv')\nweek4 = pd.read_csv('../input/nfl-big-data-bowl-2021/week4.csv')\nweek5 = pd.read_csv('../input/nfl-big-data-bowl-2021/week5.csv')\nweek6 = pd.read_csv('../input/nfl-big-data-bowl-2021/week6.csv')\nweek7 = pd.read_csv('../input/nfl-big-data-bowl-2021/week7.csv')\nweek8 = pd.read_csv('../input/nfl-big-data-bowl-2021/week8.csv')\nweek9 = pd.read_csv('../input/nfl-big-data-bowl-2021/week9.csv')\nweek10 = pd.read_csv('../input/nfl-big-data-bowl-2021/week10.csv')\nweek11 = pd.read_csv('../input/nfl-big-data-bowl-2021/week11.csv')\nweek12 = pd.read_csv('../input/nfl-big-data-bowl-2021/week12.csv')\nweek13 = pd.read_csv('../input/nfl-big-data-bowl-2021/week13.csv')\nweek14 = pd.read_csv('../input/nfl-big-data-bowl-2021/week14.csv')\nweek15 = pd.read_csv('../input/nfl-big-data-bowl-2021/week15.csv')\nweek16 = pd.read_csv('../input/nfl-big-data-bowl-2021/week16.csv')\nweek17 = pd.read_csv('../input/nfl-big-data-bowl-2021/week17.csv')\n\n# creating a unique dataset for all the tracking data\nframes = [week1,week2,week3,week4,week5,week6,week7,week8,week9,week10,week11,week12,week13,week14,week15,week16,week17]\nweeks = pd.concat(frames,axis=0).reset_index().drop('index',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Imagine being an **NFL coach** behind the desk of his/her office. With our staff's help, we are analyzing a bunch of several passing plays shown in the huge monitor in front of us. The goal? *Studying the defensive strategies that can hinder the opponent's strength we will have to face.* \n\nIt is **right at this moment** that the approach shown in this notebook can be useful. Namely, can we **predict** whether a **Defensive Pass Interference will be called** on the analyzed passing play, based on **the players' movements?** \n\nIn other words, let's imagine that the monitor in front of us visualizes this particular scenario:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# setting plot's configuration\nfig, (ax1) = plt.subplots(1, 1)\n\n# specific playID of the passing play taken into consideration\nplayID = 75\n\n# tracking data of the passing play taken into consideration\npossible_track = week1[ week1['playId'] == playID]\n\n# names of the players involved + the ball\nnames = possible_track[\"displayName\"].unique()\n\n\ndef arrow(x,y,ax,color, alpha):\n    \"\"\"\n    Function to draw the arrow of the movement\n    :param x: position on x-axis\n    :param y: position on y-axis\n    :param ax: plot's configuration\n    :param color: color of the arrows\n    :param alpha: color's intensity of the arrows\n    :return: arrows on the specific positions\n    \"\"\"\n    # distance between the arrows\n    ind = np.arange(len(x)-14,len(x),13)\n    \n    # computing of the arrows\n    for i in ind:\n        ar = FancyArrowPatch ((x[i-1],y[i-1]),(x[i],y[i]), \n                              arrowstyle='fancy', mutation_scale=50, color = color, alpha = alpha)\n        ax.add_patch(ar)\n\n\ndef draw(player_df, colorr, alpha):\n    \"\"\"\n    Function to draw the movement\n    :param player_df: tracking data of the player\n    :param colorr: color of the movement\n    :param alpha: color's intensity of the movement\n    :return: plot (proportional to the field) of the movement\n    \"\"\"\n    \n    # removing variables of no interest \n    Y = player_df.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)  \n    \n    # converting into a matrix\n    Y = Y.to_numpy()\n    Y = Y.astype(float)\n    \n    # plot's dimension proportional to the field\n    plt.xlim([0, 120])\n    plt.ylim([0, 53.3])\n    \n    # plot of the movement\n    plt.plot(Y[:,0], Y[:,1] , '-ok', color = colorr, alpha = alpha)\n    \n    # line of scrimmage\n    plt.axvline(x= 90, color =  'black')\n    plt.text(90.5, 2, 'line of scrimmage ',fontsize=42, color = 'black',weight = 'bold')\n    \n    # line of endzone\n    plt.axvline(x= 10, color =  'black')\n    plt.text(3, 20, 'ENDZONE ',fontsize=100, color = 'black', weight = 'bold', rotation='vertical')\n    \n    # plot of the arrows on the movement\n    arrow(Y[:,0],Y[:,1],ax1,colorr, alpha = alpha)\n\n    \n# loop over each name involved in the passing play\nfor i in range(0,len(names)):\n    \n    # dataframe of tracking data of that \"name\"  \n    df = possible_track[possible_track['displayName'] == names[i]]\n\n    # plot of tracking data if \"name\" is the ball\n    if df[\"team\"][df.index[0]] == 'football':\n        colorr = 'brown'\n        draw(df, colorr,1)\n        \n    # plot of tracking data if \"name\" is a member of the offensive team\n    elif df[\"team\"][df.index[0]] == 'away':\n        colorr = 'blue'\n        draw(df, colorr,1)\n    \n    # legend\n    blue_patch = mpatches.Patch(color='blue', label= 'Offensive Team')\n    brawn_patch = mpatches.Patch(color='brown', label= 'Ball')\n    plt.legend(handles=[blue_patch,brawn_patch], loc='upper right',prop={'size': 30}) \n    \n    # arrow of the direction of the attack\n    plt.arrow(65, 10, -26, 0, length_includes_head=True, head_width=2, head_length=3)\n    plt.text(43.5, 10.4, 'direction of attack ',fontsize=42, color = 'black')\n    \n    # title\n    plt.title(\"Possible opponent's strategy during a passing play\",size=95)\n    \n    # caption\n    plt.text(60, -5, 'Figure 1', ha='center',size=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a large number of intensive meetings with our staff, we have decided that the following strategy is the best way to countering this passing play."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# setting plot's configuration\nfig, (ax1) = plt.subplots(1, 1)\n\n# loop over each name involved in the passing play\nfor i in range(0,len(names)):\n    \n    # dataframe of tracking data of that \"name\"\n    df = possible_track[possible_track['displayName'] == names[i]]\n\n    # plot of tracking data if \"name\" is the ball\n    if df[\"team\"][df.index[0]] == 'football':\n        colorr = 'brown'\n        draw(df, colorr,1)\n    \n    # plot of tracking data if \"name\" is a member of the offensive team\n    elif df[\"team\"][df.index[0]] == 'away':\n        colorr = 'blue'\n        draw(df, colorr,1)\n    \n    # plot of tracking data if \"name\" is a member of the defensive team\n    elif df[\"team\"][df.index[0]] == 'home':\n        colorr = 'red'\n        draw(df, colorr,1)\n    \n\n    # legend\n    red_patch = mpatches.Patch(color='red', label= 'Defensive Team')\n    blue_patch = mpatches.Patch(color='blue', label= 'Offensive Team')\n    brown_patch = mpatches.Patch(color='brown', label= 'Ball')\n    plt.legend(handles=[red_patch,blue_patch,brown_patch], loc='upper right',prop={'size': 30}) \n    \n    # arrow of the direction of the attack\n    plt.arrow(65, 10, -26, 0, length_includes_head=True, head_width=2, head_length=3)\n    plt.text(43.5, 10.4, 'direction of attack ',fontsize=42, color = 'black')\n    \n    # title\n    plt.title(\"Defensive strategy\", size=95)\n    \n    # caption\n    plt.text(60, -5, 'Figure 2', ha='center',size=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[**Defensive Pass Interference (DPI)**](https://operations.nfl.com/the-rules/nfl-video-rulebook/defensive-pass-interference/#:~:text=It%20is%20pass%20interference%20by,opportunity%20to%20catch%20the%20ball) is a crucial aspect that **can shift the balance of the match.** Therefore, we would like to keep it into consideration during the analysis of the defensive strategies. Namely, by moving in this way in response to this passing play, **will we commit a DPI?** \n\nOnce we answer this question, we will be able **to avoid defensive strategies that could lead the team into a DPI.**\n\nBy keeping in mind this goal, let's move towards how to build an answer to this question."},{"metadata":{},"cell_type":"markdown","source":"# Datasets"},{"metadata":{},"cell_type":"markdown","source":"The first step is to get a closer look at the two datasets that we are going to **manipulate** mainly in this analysis:\n- *plays*: containing the information about all passing plays during the 2018 regular season."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"plays","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- *weeks*: containing all the passing plays' tracking data in *plays*."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"weeks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Problem"},{"metadata":{},"cell_type":"markdown","source":"As we can observe, **each passing play** in *plays* is uniquely identified by a **combination of two variables**: *playId* and *gameId*.\n\nTo answer whether a DPI will be called or not, the two datasets *weeks* and *plays* are manipulated, in order to obtain a **new dataset** in which **classification algorithms** can be applied.\n\nWe want to obtain a **new dataset** in which each row is a passing play characterized by a set of **new** features. These **new** features should **efficiently collect** compelling **information** from the dataset *weeks* and *plays*, for **each different** passing play. "},{"metadata":{},"cell_type":"markdown","source":"# How to gather information from tracking data"},{"metadata":{},"cell_type":"markdown","source":"Using the dataset *weeks* as it is, is **unthinkable**. We need a method that can **model** the movement for each player **without losing too much information.** \n\nThe technique used in this notebook is based on [Bézier curves](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/edit?disco=AAAAHfghZEE), which are parametric curves that can be seen from a statistical point of view. More information about this method's theory can be found in my [Final Bachelor’s Project](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/view); in this notebook, the key concepts are shown.\n\nBy applying this method, we will **estimate a set of points** (in two-dimensional Euclidean space) which can **uniquely describe the movement of interest.** Let's see an example."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# setting plot's configuration\nfig, (ax1) = plt.subplots(1, 1)\n\n# dataframe of the tracking data of the player of interest\nY = possible_track[ possible_track['displayName'] == 'Nate Gerry']\n\n# removing variables of no interest\nY = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)\n\n# converting into a matrix \nY = Y.to_numpy()\nY = Y.astype(float)\n\n# loop over each name involved in the passing play  \nfor i in range(0,len(names)):\n    \n    # dataframe of tracking data of that \"name\"\n    df = possible_track[possible_track['displayName'] == names[i]]\n    \n    # plot of tracking data if \"name\" is the ball\n    if df[\"team\"][df.index[0]] == 'football':\n        colorr = 'brown'\n        draw(df, colorr,0.1)\n        \n    # plot of tracking data if \"name\" is a member of the offensive team    \n    elif df[\"team\"][df.index[0]] == 'away':\n        colorr = 'blue'\n        draw(df, colorr,0.1)\n        \n    # plot of tracking data if \"name\" is a member of the defensive team      \n    elif df[\"team\"][df.index[0]] == 'home':\n        colorr = 'red'\n        draw(df, colorr,0.1)\n    \n\n    # legend\n    red_patch = mpatches.Patch(color='red', label= 'Defensive Team')\n    blue_patch = mpatches.Patch(color='blue', label= 'Offensive Team')\n    brawn_patch = mpatches.Patch(color='brown', label= 'Ball')\n    orange_patch = mpatches.Patch(color='darkorange', label= 'Player of interest')\n    plt.legend(handles=[red_patch,blue_patch,brawn_patch,orange_patch], loc='upper right',prop={'size': 30}) \n    \n    # arrow of the direction of the attack\n    plt.arrow(65, 10, -26, 0, length_includes_head=True, head_width=2, head_length=3)\n    plt.text(43.5, 10.4, 'direction of attack ',fontsize=42, color = 'black')\n    \n    # plot of tracking data of the player of interest\n    plt.plot(Y[:,0], Y[:,1] , '-ok', color = 'darkorange', alpha = 4 )\n    arrow(Y[:,0],Y[:,1],ax1,color='orange', alpha = 4)\n    \n    # title\n    plt.title(\"Focus on one single player\",size=95)\n    \n    # caption\n    plt.text(60, -5, 'Figure 3', ha='center',size=70)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph shows the same passing play that was seen before (Figure 2), but our focus now is on the movement of the player coloured in orange. Given this smooth trajectory, we can [estimate](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/edit?disco=AAAAHfghZGY) (exploiting Bézier curves [through a statistical point of view](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/edit?disco=AAAAHfghZDw)) a new set of points that **uniquely** describes this **movement.**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# setting plot's configuration\nfig, (ax1) = plt.subplots(1, 1)\n\n\ndef b(t,d):\n    \"\"\"\n    This function is necessay to compute the general formulation of Bézier curves in matrix format.\n    :param t: scalar input of Bézier curve that takes a value between 0 and 1 \n    :param d: order Bézier curve\n    :return: scalar number\n    \"\"\"\n    a = (t**d) * ( (1-t)**(n-d) ) * scipy.special.binom(n, d)\n    return a\n\n# Bézier curve's order\nn = 4  \n\n# number of movement's observations\nm = Y.shape[0]\n\n# matrix T (more information look at chap. 2.1.3 \"Statistical Modeling of Trajectories with Bézier Curves\")\nT = []\nT.insert(0, np.zeros((m,n+1)))\nt  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\np = 0\nfor h in range(0, n+1):\n    for j in range(0,T[0].shape[0]):\n        T[0][j,p] = b(float(t[j]),h)\n    p = p + 1\n\n# estimated points\ntheta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n\n\ndef Bezier_function(t, degree):\n    \"\"\"\n    This function computes the output of Bezier curve of a given order with the fixed theta.\n    :param t: scalar input of Bézier curve that takes a value between 0 and 1 \n    :param degree: order Bézier curve\n    :return: two-dimensional vector\n    \"\"\"\n    n = degree\n\n    a = 0\n    for i in range(0,n+1):\n       a =  scipy.special.binom(n, i) * (1-t)**(n-i) * t**i * theta_hat[i,:] + a\n    return a\n\n# computation Bézier curve's output\nt = np.linspace(start=0, stop=1, num=1000)\nA = np.zeros((1000,2))\nfor i in range(0,1000):\n    A[i,:] = Bezier_function(t[i],n)\n\n# plot of tracking data of the player of interest\nplt.scatter(Y[:,0], Y[:,1] ,c= 'darkorange')\narrow(Y[:,0],Y[:,1], ax1,color='darkorange',alpha=1)\n\n# plot estimated points\nplt.scatter(theta_hat[:,0], theta_hat[:,1], c='blueviolet', s=300)\n\n# plot's dimension proportional to the field\nplt.xlim([0, 120])\nplt.ylim([0, 53.3])\n\n# line of scrimmage\nplt.axvline(x= 90, color =  'black')\nplt.text(90.5, 2, 'line of scrimmage ',fontsize=42, color = 'black',weight = 'bold')\n    \n# line of endzone\nplt.axvline(x= 10, color =  'black')\nplt.text(3, 20, 'ENDZONE ',fontsize=100, color = 'black', weight = 'bold', rotation='vertical')\n\n# legend\nred_patch = mpatches.Patch(color='blueviolet', label= 'Estimated points')\norange_patch = mpatches.Patch(color='darkorange', label= 'Player of interest')\nplt.legend(handles=[red_patch, orange_patch], loc='upper right',prop={'size': 30})\n\n# title\nplt.title(\"Estimated points of this specific movement\",size=95)\n\n# caption\nplt.text(60, -5, 'Figure 4', ha='center',size=70);","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"In other words, instead of using all observations of the movement, **these five estimated points** (purple points) can **uniquely identify this particular movement!** \n\nWhether you would like to test if these estimated points actually refer uniquely to that specific movement, then let's observe that by computing the output of the Bézier curve of [fourth-order](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/edit?disco=AAAAHhgt1Ek) for 1000 values of $ t \\in [0,1] $,  by fixing $ p_0, p_1, p_2, p_3, p_4 $ equal to the estimated points computed."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# setting plot's configuration\nfig, (ax1) = plt.subplots(1, 1)\n\n# plot of tracking data of the player of interest\nplt.scatter(Y[:,0], Y[:,1], c='darkorange')\n\n# plot estimated points\nplt.scatter(theta_hat[:,0], theta_hat[:,1], c='blueviolet', s=300)\n\n# plot's dimension proportional to the field\nplt.xlim([0, 120])\nplt.ylim([0, 53.3])\n\n# line of scrimmage\nplt.axvline(x= 90, color =  'black')\nplt.text(90.5, 2, 'line of scrimmage ',fontsize=42, color = 'black',weight = 'bold')\n    \n# line of endzone\nplt.axvline(x= 10, color =  'black')\nplt.text(3, 20, 'ENDZONE ',fontsize=100, color = 'black', weight = 'bold', rotation='vertical')\n\n# plot output Bézier curve\nplt.scatter(A[:,0], A[:,1], c='green')\narrow(A[:,0],A[:,1], ax1,color='green', alpha =1)\n\n# legend\nred_patch = mpatches.Patch(color='blueviolet', label= 'Estimated points')\norange_patch = mpatches.Patch(color='darkorange', label= 'Player of interest')\ngreen_patch = mpatches.Patch(color='green', label= 'Output Bézier Curve')\nplt.legend(handles=[red_patch, orange_patch, green_patch], loc='upper right',prop={'size': 30})\n\n# title\nplt.title(\"Output Bézier curve of fourth-order\",size=95)\n\n#caption\nplt.text(60, -5, 'Figure 5', ha='center',size=70);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe, the curve obtained from the fourth-order Bézier curve is similar to the player's movement!\n\nIf we had chosen to adopt a higher number of estimated points, we would have obtained Bézier curves' output closer to that of actual movement. **The power** of seeing Bézier curves through a **statistical point of view** is that you can not only compute an estimate of points that uniquely describes the movement, [**but also identify the ”correct order” of the Bézier curve.**](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/edit?disco=AAAAHfghZG8) Namely, the order that can represent the movement **without losing the signal**, but at the same time, it can **avoid over-fitting.**\n\nIn this context, I decided to adopt the Bézier curve of **fourth-order** since it seemed a **good trade-off** by looking at the [AIC values](https://drive.google.com/file/d/1iNhRpEiCRHS9hBGmFIaWpwjpHlpKui5N/edit?disco=AAAAHipHaLM). "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Looking at the AIC values for each player is unthinkable, given the dimension of dataset. \n# Here we can compute the AIC values of 10 random defensive players.\n# Running this part of the code multiple times is possible in order to base our decision not only on one single sample of 10 elements!\n\n# random sampling of 10 passing plays\nsampled_list = random.sample(list(plays.index), 10)\n\n# looping over the 10 sampled passing plays\nfor fils in range(0,len(sampled_list)):\n\n    gameID = plays.iloc[fils,0]\n    playID = plays.iloc[fils,1]\n    \n    # filtering of plays with that specific values of \"gameID\" and \"playID\"\n    focus7 = plays[ plays['gameId'] == gameID ]\n    focus6 = focus7 [ focus7 ['playId'] == playID ].reset_index().drop('index',axis=1)\n    \n    # team in possession of the ball\n    possTeam = focus6.iloc[0,6]\n\n    # checking which team is the defender\n    focus5 = games[ games['gameId'] == gameID ].reset_index().drop('index',axis=1)\n    if focus5.iloc[0,3] == possTeam:\n        defend = \"away\"\n    else:\n        defend = \"home\"\n    \n    # filtering of weeks with that specific values of \"gameID\", \"playID\" and \"defend\"\n    focus4 = weeks[ weeks['playId'] == playID ]\n    focus3 = focus4[ focus4['team'] == defend ]\n    focus2 = focus3[ focus3['gameId'] == gameID].reset_index().drop('index',axis=1)\n    \n    # names of the players involved in that specific defensive passing play\n    names = focus2['displayName'].unique().astype(str)\n    \n    # random sampling of one of the players in \"names\"\n    name = random.sample(list(names), 1)\n    \n    # dataframe of traking data of the \"name\" sampled\n    Y = focus2[focus2['displayName'] == name[0]]\n        \n    # checking if the direction of the attack is from left to right\n    if(focus2.iloc[0,17] == 'right'):\n        \n        # if it is, we flip the passing play in order to have the same direction of the attack for each different passing play (from right to left)\n        xli = []\n        for l in range(0,Y.shape[0]):\n        \n            OldMax = 120\n            OldMin = 0\n            NewMax = 0\n            NewMin = 120\n            OldValue = Y.iloc[l,1]\n        \n            OldRange = (OldMax - OldMin)  \n            NewRange = (NewMax - NewMin)  \n            NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n            xli.append(NewValue)\n        \n        \n        yli = []\n        for z in range(0,Y.shape[0]):\n        \n            OldMax = 53.3\n            OldMin = 0\n            NewMax = 0\n            NewMin = 53.3\n            OldValue = Y.iloc[z,2]\n        \n            OldRange = (OldMax - OldMin)  \n            NewRange = (NewMax - NewMin)  \n            NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n            yli.append(NewValue)\n    \n        # dataset flipped\n        frames = [pd.DataFrame(xli), pd.DataFrame(yli)]\n        Y = pd.concat(frames,axis=1).reset_index().drop('index',axis=1)\n            \n            \n    if(focus2.iloc[0,17] != 'right'):\n        # removing variables of no interest\n        Y = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)\n            \n    #converting into a matrix            \n    Y = Y.to_numpy()\n    Y = Y.astype(float)\n\n    def vector(Matrix):\n        \"\"\"\n        This function computes the vectorization, namely a linear transformation which converts the matrix into a column vector.\n        :param Matrix: a matrix\n        :return: a column vector\n        \"\"\"\n        n = Matrix.shape[0]\n        p = Matrix.shape[1]\n\n        vett = np.zeros(n*p)\n        o = 0\n        for i in range(p):\n            for k in range(n):\n                vett[o] = Matrix[k,i]\n                o = o + 1\n        return vett\n\n    #AKAIKE\n    def Akaike(Y,theta_hat,observations,variables,parameters):\n        \"\"\"\n        This function computes the AIC value in a multivariate linear regression context.\n        :param Y: response variable that is a two-dimensional matrix\n        :param theta_hat: it contains the estimated points\n        :param observations: number of observations\n        :param variables: number of variables (in this context two)\n        :param parameters: Bézier curve's order\n        \"\"\"\n        p = variables\n        k = parameters\n        n = observations\n\n        # T is the design matrix\n\n        # projection matrix\n        P = np.dot(T[0], np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])), np.transpose(T[0])))\n\n        # Estimate of the error variance\n        S = np.dot(np.dot(np.transpose(Y), (np.eye(Y.shape[0]) - P)), Y) / (Y.shape[0] - p - 1)\n\n        # Computation of the Multivariate Normal distribution's mean\n        Y_t = vector(Y)\n        mean = vector(np.dot(T[0],theta_hat))\n\n        # Computation of the Multivariate Normal distribution's covariance matrix\n        var = np.kron(S,np.eye(Y.shape[0]))\n\n        # Computational solution for the positive covariance\n        min_eig = np.min(np.real(np.linalg.eigvals(var)))\n        if min_eig < 0:\n            var -= 10 * min_eig * np.eye(*var.shape)\n        # Transpose of Y's log-likelihood\n        L = math.log(multivariate_normal.pdf(Y_t,mean,var))\n\n        # Akaike's formula\n        return -2*L + 2*n*(p*k + p*(p+1)/2)/(n-(k+p+1))\n\n\n    max_order = 10\n    akaike = np.zeros(max_order)\n    c = 0\n    \n    # loop over the possible orders of the Bézier curve\n    for i in range(0,max_order):\n        i = i + 2\n        n = int(i) # Number of parameters\n        m = Y.shape[0]\n\n        # MATRIX T\n        T = []\n        T.insert(0, np.zeros((m,n+1)))\n        t  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\n        p = 0\n        for h in range(0, n+1):\n            for j in range(0,T[0].shape[0]):\n                T[0][j,p] = b(float(t[j]),h)\n            p = p + 1\n        \n        # estimated points\n        theta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n        \n        # computation of AIC value\n        akaike[c] = Akaike(Y,theta_hat,Y.shape[0],2,n)\n        c = c + 1\n\n    plt.plot(np.arange(1,max_order+1), akaike, '-o')\n    plt.title(\"AIC values for 10 defensive players chosen randomly\",size=75)\n    plt.xlabel(\"Bézier curve's order\",size=55)\n    plt.ylabel(\"AIC's values\",size=55)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, by extending the approach of the estimated points to each movement of this analyzed defensive strategy, we can obtain the following result:"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# dataframe of tracking data of all defensive players of the passing play that was taken into consideration\nalls = possible_track[possible_track['team']== 'home']\n\n# loop over all defensive players\nfor k in range(0, len(alls[\"displayName\"].unique())):\n    \n    # filtering tracking data of the passing play taken into consideration, with respect to unique 'displayName' of 'alls'\n    Y = possible_track[ possible_track['displayName'] == alls[\"displayName\"].unique()[k]]\n    # removing variables of no interest\n    Y = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)\n    \n    # converting into a matrix\n    Y = Y.to_numpy()\n    Y = Y.astype(float)\n    \n    # Bezier curve's order adopted\n    n = 4 \n    m = Y.shape[0]\n\n    # MATRIX T\n    T = []\n    T.insert(0, np.zeros((m,n+1)))\n    t  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\n    p = 0\n    for h in range(0, n+1):\n        for j in range(0,T[0].shape[0]):\n            T[0][j,p] = b(float(t[j]),h)\n        p = p + 1\n\n    # matrix of estimated points\n    theta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n\n    # computation Bézier curve's output\n    t = np.linspace(start=0, stop=1, num=1000)\n    A = np.zeros((1000,2))\n    for i in range(0,1000):\n        A[i,:] = Bezier_function(t[i],n)\n\n    # plot each defensive movements \n    plt.scatter(Y[:,0], Y[:,1], c='red')\n    \n    # plot estimated points\n    plt.scatter(theta_hat[:,0], theta_hat[:,1], c='blueviolet', s = 200)\n    \n    # plot's dimension proportional to the field\n    plt.xlim([0, 120])\n    plt.ylim([0, 53.3])\n    \n    # line of scrimmage\n    plt.axvline(x= 90, color =  'black')\n    plt.text(90.5, 2, 'line of scrimmage ',fontsize=42, color = 'black',weight = 'bold')\n    \n    # line of endzone\n    plt.axvline(x= 10, color =  'black')\n    plt.text(3, 20, 'ENDZONE ',fontsize=100, color = 'black', weight = 'bold', rotation='vertical')\n    \n    # plot Bézier curve's output\n    plt.scatter(A[:,0], A[:,1], c='green')\n\n# legend\nviolet_patch = mpatches.Patch(color='blueviolet', label= 'Estimated points')\nred_patch = mpatches.Patch(color='red', label= 'Defensive Team')\ngreen_patch = mpatches.Patch(color='green', label= 'Output Bézier Curves')\nplt.legend(handles=[violet_patch,red_patch,green_patch], loc='upper right',prop={'size': 30}) \n    \n# plot arrow of the direction of the attack\nplt.arrow(65, 10, -26, 0, length_includes_head=True, head_width=2, head_length=3)\nplt.text(43.5, 10.4, 'direction of attack ',fontsize=42, color = 'black')\n\n# title\nplt.title(\"Estimated points for all defensive movements of this defensive strategy\",size=95)\n\n# caption\nplt.text(60, -5, 'Figure 6', ha='center',size=70);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A $35 \\times 2$ matrix containing the estimated points has been obtained for this particular defensive strategy ( 5 estimated points for each of the seven moving players involved in this passing play).\n\nLet's see now how this matrix can be **effectively transformed** into a feature vector for this passing play."},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality reduction\n\nTo use the matrix of estimated points as the feature vector of this passing play, a technique of dimensionality reduction is necessary. In this context, the primary linear technique called Principal Component Analysis, is used. By applying PCA, the $35 \\times 2$ matrix can be **transformed** into a $35 \\times 1$ array. By doing so, the transformation can be applied as **the new feature vector of that specific passing play.**"},{"metadata":{},"cell_type":"markdown","source":"As you can imagine, each passing play has its defensive strategy in which a different number of moving defenders is involved. By restating what we implied before, we should define new variables that can accurately collect the relevant information from the tracking data (✅), **for each different passing play.**"},{"metadata":{},"cell_type":"markdown","source":"# Each passing play is different"},{"metadata":{},"cell_type":"markdown","source":"A different number of moving defensive players is involved in each passing play. This means that, depending on the personnel used into the defensive strategy, we have a **different number of estimated points.** So as observed before, we obtained a $35 \\times 2$ matrix because there were seven moving defensive players involved. But, if nine moving defensive players had been involved, you would have obtained a $45 \\times 2$ matrix.\n\nTo apply a classification algorithm, we need a dataset in which each observation has the same number of features. Once the dimensionality reduction is applied, the obtained feature vector might have a different dimension depending on the moving defensive players involved in that specific passing play. To avoid this problem, the [K-means](https://www.youtube.com/watch?v=4b5d3muPQmA&t=85s) technique is applied. In essence, **each** passing play's estimated points are **replaced** by a **fixed number** of **centroids** which is the **same** for each passing play. Therefore, the PCA is applied to **the centroids**. \n\n*The advantage?* Every passing play has a defensive strategy defined by **the same number** of points (**centroids obtained**) even though a different personnel is used.\n\n*The disadvantage?* You can lose some information after using the K-means technique because we represent fewer centroids as replacements for the estimated points.\n\nThis heuristic method gives us a **good trade-off** between representation and classification's practicability. "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# dataframe of tracking data of all defensive players of the passing play that was taken into consideration\nalls = possible_track[possible_track['team']== 'home']\n\n# list of the estimated points for each different passing play\nthetas = []\n\n# loop over all defensive players\nfor k in range(0, len(alls[\"displayName\"].unique())):\n    \n    # filtering tracking data of the passing play taken into consideration, with respect to unique 'displayName' of 'alls'\n    Y = possible_track[ possible_track['displayName'] == alls[\"displayName\"].unique()[k]]\n    \n    # removing variables of no interest\n    Y = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)\n    \n    # converting into a matrix\n    Y = Y.to_numpy()\n    Y = Y.astype(float)\n        \n    # Bezier curve's order adopted\n    n = 4 \n    m = Y.shape[0]\n\n    # MATRIX T\n    T = []\n    T.insert(0, np.zeros((m,n+1)))\n    t  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\n    p = 0\n    for h in range(0, n+1):\n        for j in range(0,T[0].shape[0]):\n            T[0][j,p] = b(float(t[j]),h)\n        p = p + 1\n    \n    # matrix of estimated points\n    theta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n    \n    thetas.append(theta_hat)\n            \n    # plot each defensive movements \n    plt.scatter(Y[:,0], Y[:,1], c='red')\n    \n    # plot estimated points of each movement\n    plt.scatter(theta_hat[:,0], theta_hat[:,1], c='blueviolet',s=300)\n    \n    # plot's dimension proportional to the field\n    plt.xlim([0, 120])\n    plt.ylim([0, 53.3])\n    \n    # line of scrimmage\n    plt.axvline(x= 90, color =  'black')\n    plt.text(90.5, 2, 'line of scrimmage ',fontsize=42, color = 'black',weight = 'bold')\n    \n    # line of endzone\n    plt.axvline(x= 10, color =  'black')\n    plt.text(3, 20, 'ENDZONE ',fontsize=100, color = 'black', weight = 'bold', rotation='vertical')\n        \nthetas = np.concatenate(thetas)\n\n# Applying K-means algorithm on the estimated points\nkmeans = KMeans(n_clusters=25)\nkmeans.fit_predict(thetas)\n\n# matrix of the centroids\ncentroids = kmeans.cluster_centers_\n    \n# plot centroids obtained\nplt.scatter(pd.DataFrame(centroids)[0], pd.DataFrame(centroids)[1], c='goldenrod', s = 100);\n\n# legend\nviolet_patch = mpatches.Patch(color='blueviolet', label= 'Estimated points')\nred_patch = mpatches.Patch(color='red', label= 'Defensive Team')\ngolden_patch = mpatches.Patch(color='goldenrod', label= 'Centroids')\nplt.legend(handles=[violet_patch,red_patch,golden_patch], loc='upper right',prop={'size': 30}) \n    \n# plot arrow of the direction of the attack\nplt.arrow(65, 10, -26, 0, length_includes_head=True, head_width=2, head_length=3)\nplt.text(43.5, 10.4, 'direction of attack ',fontsize=42, color = 'black')\n\n# title\nplt.title(\"Centroids obtained with K-Means for this passing play\",size=95)\n\n# caption\nplt.text(60, -5, 'Figure 7', ha='center',size=70);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What is the \"right\" number of centroids that you should adopt?"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In this specific context, we do not want to lose too much information when using K-means to define fewer centroids as replacements for each passing play's estimated points. Thus, we should use the **largest possible number** of centroids. The largest possible number of centroids corresponds to the minimum number of moving defensive players involved in a passing play multiplied by five.\n\nBy looking at the variable *personnelD* in *plays*, that shows personnel used by the defensive team, we can spot that all the passing plays have a minimum of 5 defenders in motion except for one single passing play. Therefore, I have chosen to delete this observation to increase the number of centroids (used for **each** passing play) from 20 to 25, **by enhancing the power of the centroids' representativeness!**"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pd.DataFrame(plays.groupby('personnelD').count()['isDefensivePI'])\n\n# I am going to drop out the only passing play with 4 defensive players in motion in the next cell.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defensive strategy of each passing play"},{"metadata":{},"cell_type":"markdown","source":"Before generalizing the method to each defensive strategy of each passing play, we need to clean the dataset *plays* since some passing plays may not have defensive strategy. The data frame obtained called *key* becomes the reference point since it collects *gameId*, *playId* and the *defender team* of each *clean* passing play."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# list of all clean passing plays\nkey = []\n\n# empty list for checking purpose\nempty = []\n\n# loop over each different passing play\nfor i in range(0,plays.shape[0]): \n\n    gameID = plays[\"gameId\"][i]\n    playID = plays[\"playId\"][i]\n    \n    # filtering of plays with that specific values of \"gameID\" and \"playID\"\n    focus7 = plays[ plays['gameId'] == gameID ]\n    focus6 = focus7[ focus7['playId'] == playID ].reset_index().drop('index',axis=1)\n    \n    # team in possession of the ball\n    possTeam = focus6.iloc[0,6]\n    \n    # checking which team is the defender\n    focus5 = games[ games['gameId'] == gameID ].reset_index().drop('index',axis=1)\n    if focus5.iloc[0,3] == possTeam:\n        defend = \"away\"\n    else:\n        defend = \"home\"\n    \n    # filtering of weeks with that specific values of \"gameID\", \"playID\" and \"defend\"\n    focus4 = weeks[ weeks['gameId'] == gameID ]\n    focus3 = focus4[ focus4['playId'] == playID ]\n    focus2 = focus3[ focus3['team'] == defend].reset_index().drop('index',axis=1)\n    \n    # names of the players involved in that specific defensive passing play\n    names = focus2['displayName'].unique().astype(str)\n    \n    # checking if the passing play in plays is without a defensive strategy\n    if (names.tolist() != empty ):\n        \n        key.append([gameID,playID,defend])\n\n# converting into a dataframe        \nkey = pd.DataFrame(key)\nkey.columns = ['gameId','playId','Defender']\n\n# index of the passing play with only 4 players in motion\na = key[key['playId'] == int(plays[ plays['personnelD'] == '7 DL, 3 LB, 1 DB' ]['playId'])]\nidx = a[a['gameId'] == int(plays[ plays['personnelD'] == '7 DL, 3 LB, 1 DB' ]['gameId'])].index\n\n# Let's remove the only passing play with 4 defensive players in motion\nkey = key.drop([idx[0]]).reset_index().drop('index',axis=1)\nkey","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the generalization, we obtain a dataset in which each passing play is characterized by dif$_1$, ..., dif$_{25}$. These values result from the PCA's application on the 25 centroids, which were obtained from the estimated points of each different passing play."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# list of centroids obtained from defensive strategy for each passing play, sorted by distance from the end zone (defensive)\nprotos =[]\n\n# loop over each clean passing play\nfor i in range(0,len(key)): \n\n    gameID = key.iloc[i,0]\n    playID = key.iloc[i,1]\n    \n    # filtering of plays with that specific values of \"gameID\" and \"playID\"\n    focus7 = plays[ plays['gameId'] == gameID ]\n    focus6 = focus7[ focus7['playId'] == playID ].reset_index().drop('index',axis=1)\n    \n    # team in possession of the ball\n    possTeam = focus6.iloc[0,6]\n    \n    # checking which team is the defender\n    focus5 = games[ games['gameId'] == gameID ].reset_index().drop('index',axis=1)\n    if focus5.iloc[0,3] == possTeam:\n        defend = \"away\"\n    else:\n        defend = \"home\"\n    \n    # filtering of weeks with that specific values of \"gameID\", \"playID\" and \"defend\"\n    focus4 = weeks[ weeks['playId'] == playID ]\n    focus3 = focus4[ focus4['team'] == defend ]\n    focus2 = focus3[ focus3['gameId'] == gameID].reset_index().drop('index',axis=1)\n    \n    # names of the players involved in that specific defensive passing play\n    names = focus2['displayName'].unique().astype(str)\n    \n    # list of the estimated points for each different passing play    \n    thetas = []\n    \n    # loop over each defensive player's name involved in 'i' passing play\n    for u in range(0, len(names)):\n        \n        # dataframe of tracking data of the \"name\" \n        Y = focus2[focus2['displayName'] == names[u]]\n        \n        # checking if the direction of the attack is from left to right\n        if(focus2.iloc[0,17] == 'right'):\n            \n            # if it is, we flip the passing play in order to have the same direction of the attack for each different passing play (from right to left)\n            xli = []\n            for l in range(0,Y.shape[0]):\n        \n                OldMax = 120\n                OldMin = 0\n                NewMax = 0\n                NewMin = 120\n                OldValue = Y.iloc[l,1]\n        \n                OldRange = (OldMax - OldMin)  \n                NewRange = (NewMax - NewMin)  \n                NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n                xli.append(NewValue)\n        \n        \n            yli = []\n            for z in range(0,Y.shape[0]):\n        \n                OldMax = 53.3\n                OldMin = 0\n                NewMax = 0\n                NewMin = 53.3\n                OldValue = Y.iloc[z,2]\n        \n                OldRange = (OldMax - OldMin)  \n                NewRange = (NewMax - NewMin)  \n                NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n                yli.append(NewValue)\n    \n            # dataset flipped\n            frames = [pd.DataFrame(xli), pd.DataFrame(yli)]\n            Y = pd.concat(frames,axis=1).reset_index().drop('index',axis=1)\n            \n            \n        if(focus2.iloc[0,17] != 'right'):\n            # removing variables of no interest\n            Y = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)\n            \n        # converting into a matrix        \n        Y = Y.to_numpy()\n        Y = Y.astype(float)\n    \n        # Bezier curve's order adopted\n        n = 4 \n        m = Y.shape[0]\n\n        # MATRIX T\n        T = []\n        T.insert(0, np.zeros((m,n+1)))\n        t  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\n        p = 0\n        for h in range(0, n+1):\n            for j in range(0,T[0].shape[0]):\n                T[0][j,p] = b(float(t[j]),h)\n            p = p + 1\n        \n        # matrix estimated points\n        theta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n    \n            \n        thetas.append(theta_hat)\n\n        \n    thetas = np.concatenate(thetas) \n    \n    # Applying K-means algorithm on the estimated points\n    kmeans = KMeans(n_clusters=25)\n    kmeans.fit_predict(thetas)\n    \n    # matrix of the centroids\n    centroids = kmeans.cluster_centers_ \n        \n    # computation of the distance from the end zone (defensive) for the centroids, in order to sort the centroids by distance\n    dist = []\n    for w in range(0,centroids.shape[0]):\n    \n            dist.append(abs( centroids[w][0] - 10 ))\n        \n    protos.append([centroids[qq] for qq in list(np.argsort(dist))])\n        \n        \n\n# PCA on the centroids obtained\n_dataset = []\nfor i in range(0,len(protos)):          \n    \n    pr = np.array(protos[i])\n            \n    pca = PCA(n_components=1, whiten= True)\n    X_pca = pca.fit_transform(pr)\n    \n    _dataset.append(X_pca.tolist())\n    \n\n# transforming into a dataframe \n_dataseT_dif = pd.DataFrame(_dataset)\n\nfor i in range(0,_dataseT_dif.shape[0]):\n    \n    for j in range(0,_dataseT_dif.shape[1]):\n        \n        if _dataseT_dif.iloc[i,j] != None:\n            _dataseT_dif.iloc[i,j] = float(_dataseT_dif.iloc[i,j][0])\n\n_dataseT_dif.columns = ['dif_1','dif_2','dif_3','dif_4','dif_5',\n                        'dif_6','dif_7','dif_8','dif_9','dif_10',\n                        'dif_11','dif_12','dif_13','dif_14','dif_15',\n                        'dif_16','dif_17','dif_18','dif_19','dif_20',\n                        'dif_21','dif_22','dif_23','dif_24','dif_25']\n\n_dataseT_dif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having seen how defining a set of **new variables** that can accurately **collect** the relevant information from the tracking data for the **defensive strategies**, let's apply the same approach to the **offensive strategies** and to the **ball's movements!**\n\nSomeone may wonder, why should we also consider these movements? On the other hand, the DPI is committed by the defensive team. \n\nTrue, the DPI is committed by the defensive team, but it is also affected by the offensive strategy and ball's movement. They are both parts of the passing play. Thus, to add this information on the dataset just obtained, the same approach is applied to offensive strategies and ball's movements. "},{"metadata":{},"cell_type":"markdown","source":"# Offensive strategies and ball's movements"},{"metadata":{},"cell_type":"markdown","source":"The only small differences from the method shown above are: \n\n- In the case of offensive strategies, we need to delete 4 passing plays to increase the representativeness's power by using 25 centroids. Namely, all the passing plays have a minimum of 5 moving offensive players except for 4 passing plays."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# empty list for checking purpose\nempty = []\n\n# list of all passing plays that do not have offensive strategy.\nnull = []\n\n# loop over each different passing play\nfor i in range(0,len(key)): \n\n    gameID = key.iloc[i,0]\n    playID = key.iloc[i,1]\n    \n    # filtering of plays with that specific values of \"gameID\" and \"playID\"\n    focus7 = plays[ plays['gameId'] == gameID ]\n    focus6 = focus7[ focus7['playId'] == playID ].reset_index().drop('index',axis=1)\n    \n    # team in possession of the ball\n    possTeam = focus6.iloc[0,6]\n    \n    # checking which team is the attacker\n    focus5 = games[ games['gameId'] == gameID ].reset_index().drop('index',axis=1)\n    if focus5.iloc[0,3] == possTeam:\n        attack = \"home\"\n    else:\n        attack = \"away\"\n        \n    # filtering of weeks with that specific values of \"gameID\", \"playID\" and \"attack\"\n    focus4 = weeks[ weeks['playId'] == playID ]\n    focus3 = focus4[ focus4['team'] == attack ]\n    focus2 = focus3[ focus3['gameId'] == gameID].reset_index().drop('index',axis=1)\n        \n    # names of the players involved in that specific offensive passing play\n    names = focus2['displayName'].unique().astype(str)\n    \n    # checking if the passing play in key is without a offensive strategy\n    if (names.tolist() == empty ):\n           null.append(i)\n            \n# Dropping passing plays whitout no offensive strategies\nkey = key.drop(null).reset_index().drop('index',axis=1)\n_dataseT_dif = _dataseT_dif.drop(null).reset_index().drop('index',axis=1)\n\n# list of the passing plays with 4 offensive players in motion\nmin_4 = []\n\n# list of centroids obtained from offensive strategy for each passing play, sorted by distance from the end zone (defensive)\nprotos = []\n\n# loop over each passing play in key\nfor i in range(0,len(key)): \n\n    gameID = key.iloc[i,0]\n    playID = key.iloc[i,1]\n    \n    # filtering of plays with that specific values of \"gameID\" and \"playID\"\n    focus7 = plays[ plays['gameId'] == gameID ]\n    focus6 = focus7[ focus7['playId'] == playID ].reset_index().drop('index',axis=1)\n    \n    # team in possession of the ball\n    possTeam = focus6.iloc[0,6]\n\n    # checking which team is the attacker\n    focus5 = games[ games['gameId'] == gameID ].reset_index().drop('index',axis=1)\n    if focus5.iloc[0,3] == possTeam:\n        attack = \"home\"\n    else:\n        attack = \"away\"\n        \n    # filtering of weeks with that specific values of \"gameID\", \"playID\" and \"attack\"\n    focus4 = weeks[ weeks['playId'] == playID ]\n    focus3 = focus4[ focus4['team'] == attack ]\n    focus2 = focus3[ focus3['gameId'] == gameID].reset_index().drop('index',axis=1)\n        \n    # names of the players involved in that specific offensive passing play\n    names = focus2['displayName'].unique().astype(str)\n    \n    # checking if the passing play has more than 4 offensive players in motion\n    if (len(names) != 4 ):\n        \n        # list of the estimated points for each different passing play  \n        thetas = []\n        \n        # loop over each offensive player's name involved in 'i' passing play\n        for u in range(0, len(names)):\n                     \n            # dataframe of tracking data of the \"name\"\n            Y = focus2[focus2['displayName'] == names[u]]\n            \n            # checking if the direction of the attack is from left to right\n            if(focus2.iloc[0,17] == 'right'):\n                \n                # if it is, we flip the passing play in order to have the same direction of the attack for each different passing play (from right to left)\n                xli = []\n                for l in range(0,Y.shape[0]):\n        \n                    OldMax = 120\n                    OldMin = 0\n                    NewMax = 0\n                    NewMin = 120\n                    OldValue = Y.iloc[l,1]\n        \n                    OldRange = (OldMax - OldMin)  \n                    NewRange = (NewMax - NewMin)  \n                    NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n                    xli.append(NewValue)\n        \n        \n                yli = []\n                for z in range(0,Y.shape[0]):\n        \n                    OldMax = 53.3\n                    OldMin = 0\n                    NewMax = 0\n                    NewMin = 53.3\n                    OldValue = Y.iloc[z,2]\n        \n                    OldRange = (OldMax - OldMin)  \n                    NewRange = (NewMax - NewMin)  \n                    NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n                    yli.append(NewValue)\n    \n                # dataset flipped\n                frames = [pd.DataFrame(xli), pd.DataFrame(yli)]\n                Y = pd.concat(frames,axis=1).reset_index().drop('index',axis=1)\n            \n            \n            if(focus2.iloc[0,17] != 'right'):\n                # removing variables of no interest\n                Y = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)\n            \n            # converting into a matrix      \n            Y = Y.to_numpy()\n            Y = Y.astype(float)\n    \n            # Bezier curve's order adopted\n            n = 4 \n            m = Y.shape[0]\n\n            # MATRIX T\n            T = []\n            T.insert(0, np.zeros((m,n+1)))\n            t  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\n            p = 0\n            for h in range(0, n+1):\n                for j in range(0,T[0].shape[0]):\n                    T[0][j,p] = b(float(t[j]),h)\n                p = p + 1\n\n            # matrix estimated points\n            theta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n    \n            \n            thetas.append(theta_hat)\n\n        \n        thetas = np.concatenate(thetas)\n        \n        # Applying K-means algorithm on the estimated points\n        kmeans = KMeans(n_clusters = 25)\n        kmeans.fit_predict(thetas)\n        \n        # matrix of the centroids\n        centroids = kmeans.cluster_centers_ \n    \n        # computation of the distance from the end zone (defensive) for the centroids, in order to sort the centroids by distance\n        dist = []\n        for w in range(0,len(centroids)):\n    \n            dist.append(abs( centroids[w][0] - 10 ))\n        \n        protos.append([centroids[qq] for qq in list(np.argsort(dist))])\n    \n    else:\n        min_4.append(i)\n\n\n# Let's remove the passing plays with 4 offensive players in motion from key and _dataseT_dif\nkey = key.drop(min_4).reset_index().drop('index',axis=1)\n_dataseT_dif = _dataseT_dif.drop(min_4).reset_index().drop('index',axis=1)\n\n# PCA on the centroids obtained\n_dataset_off = []\nfor i in range(0,len(protos)):          \n    \n    pr = np.array(protos[i])\n            \n    pca = PCA(n_components=1, whiten= True)\n    X_pca = pca.fit_transform(pr)\n    \n    _dataset_off.append(X_pca.tolist())\n    \n# transforming into a dataframe \n_dataseT_off = pd.DataFrame(_dataset_off)\n\nfor i in range(0,_dataseT_off.shape[0]):\n    \n    for j in range(0,_dataseT_off.shape[1]):\n        \n        if _dataseT_off.iloc[i,j] != None:\n            _dataseT_off.iloc[i,j] = float(_dataseT_off.iloc[i,j][0])\n\n_dataseT_off.columns = ['att_1','att_2','att_3','att_4',\n                        'att_5','att_6','att_7','att_8','att_9',\n                        'att_10','att_11','att_12','att_13','att_14',\n                        'att_15','att_16','att_17','att_18','att_19',\n                        'att_20','att_21','att_22','att_23','att_24','att_25']\n_dataseT_off","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- In the ball's movements, we do not need to use K-means technique since the ball's movement is not characterized by a personnel. We can incorporate the estimated points directly into our dataset (after transforming them into a feature vector by using PCA)."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# list of the estimated points for each different passing play\nthetas = []\n\n# empty list for checking purpose\nempty = []\n\n# loop over each clean passing play in key\nfor i in range(0,len(key)): \n\n    gameID = key.iloc[i,0]\n    playID = key.iloc[i,1]\n    \n    # filtering of weeks with that specific values of \"gameID\", \"playID\" and \"football\"\n    focus4 = weeks[ weeks['playId'] == playID ]\n    focus3 = focus4[ focus4['team'] == 'football' ]\n    focus2 = focus3[ focus3['gameId'] == gameID].reset_index().drop('index',axis=1)\n    \n    # names of the ball involved in that specific passing play \n    names = focus2['displayName'].unique().astype(str)\n    \n    # checking if the passing play in plays is without ball's movement\n    if (names.tolist() != empty ):\n        \n        # loop over each element in \"names\" for each 'i' passing play\n        for u in range(0, len(names)):\n            \n            # dataframe of tracking data of the ball \n            Y = focus2[focus2['displayName'] == names[u]]\n            \n            # checking if the direction of the attack is from left to right\n            if(focus2.iloc[0,17] == 'right'):\n                \n                # if it is, we flip the passing play in order to have the same direction of the attack for each different passing play (from right to left)\n                xli = []\n                for l in range(0,Y.shape[0]):\n        \n                    OldMax = 120\n                    OldMin = 0\n                    NewMax = 0\n                    NewMin = 120\n                    OldValue = Y.iloc[l,1]\n        \n                    OldRange = (OldMax - OldMin)  \n                    NewRange = (NewMax - NewMin)  \n                    NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n                    xli.append(NewValue)\n        \n        \n                yli = []\n                for z in range(0,Y.shape[0]):\n        \n                    OldMax = 53.3\n                    OldMin = 0\n                    NewMax = 0\n                    NewMin = 53.3\n                    OldValue = Y.iloc[z,2]\n        \n                    OldRange = (OldMax - OldMin)  \n                    NewRange = (NewMax - NewMin)  \n                    NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n    \n                    yli.append(NewValue)\n    \n                # dataset flipped\n                frames = [pd.DataFrame(xli), pd.DataFrame(yli)]\n                Y = pd.concat(frames,axis=1).reset_index().drop('index',axis=1)\n            \n            \n            if(focus2.iloc[0,17] != 'right'):\n                # removing variables of no interest\n                Y = Y.drop(['time','s','a','dis','o','dir','event','nflId','displayName','jerseyNumber','position','frameId','team','gameId','playId','playDirection','route'],axis=1)           \n            \n            # converting into a matrix     \n            Y = Y.to_numpy()\n            Y = Y.astype(float)\n    \n            # Bezier curve's order adopted\n            n = 4 \n            m = Y.shape[0]\n\n            # MATRIX T\n            T = []\n            T.insert(0, np.zeros((m,n+1)))\n            t  =  np.linspace(0, 1,T[0].shape[0], endpoint = True )\n\n            p = 0\n            for h in range(0, n+1):\n                for j in range(0,T[0].shape[0]):\n                    T[0][j,p] = b(float(t[j]),h)\n                p = p + 1\n            \n            # matrix estimated points\n            theta_hat = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(T[0]), T[0])),np.transpose(T[0])),Y)\n    \n         \n        thetas.append(theta_hat)\n            \n\n# PCA on the estimated points obtained        \n_dataset_ball = []\nfor i in range(0,len(thetas)):          \n    \n    pr = np.array(thetas[i])\n            \n    pca = PCA(n_components=1, whiten= True)\n    X_pca = pca.fit_transform(pr)\n    \n    _dataset_ball.append(X_pca.tolist())\n    \n# transforming into a dataframe \n_dataseT_ball = pd.DataFrame(_dataset_ball)\n\nfor i in range(0,_dataseT_ball.shape[0]):\n    \n    for j in range(0,_dataseT_ball.shape[1]):\n        \n        if _dataseT_ball.iloc[i,j] != None:\n            _dataseT_ball.iloc[i,j] = float(_dataseT_ball.iloc[i,j][0])\n\n_dataseT_ball.columns = ['ball_1','ball_2','ball_3','ball_4','ball_5']\n_dataseT_ball","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other useful features"},{"metadata":{},"cell_type":"markdown","source":"The dataset *weeks* also contains information regarding each movement, such as speed and acceleration. Incorporating these into the new dataset is crucial in **enhancing** the data’s **quality**.\n\nMoreover, to **contextualize** each passing play, adding the variable *epa* from *plays* is necessary. This variable estimates the average of the next scoring outcome given the **down**, **distance**, **yardline** and **time remaining**. By doing so, each passing play is also characterized by its **game characteristics**."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Each average information is a nested list. Each average[i] has as the fist element the information regarding the defensive movements,\n# the second element regarding the information about the offensive movements, the third element regarding the information about \n# the ball's movement. This is done for each passing play.\naverage_s = []\naverage_a = []\naverage_dis = []\naverage_o = []\naverage_dir = []\n\n# loop over each passing play in key\nfor i in range(0,len(key)): \n    \n    # filtering of weeks with that specific values of \"playID\" \n    a = weeks[weeks['playId'] == key.iloc[i,1]]\n    \n    #Defender\n    b = a[a[\"team\"] == key.iloc[i,2]]\n    c = b[b[\"gameId\"] == key.iloc[i,0]]\n    average_s.append([np.mean(c['s'])])\n    average_a.append([np.mean(c['a'])])\n    average_dis.append([np.mean(c['dis'])])\n    average_o.append([np.mean(c['o'])])\n    average_dir.append([np.mean(c['dir'])])\n    \n    \n    if key.iloc[i,2] == 'away':\n        rr = 'home'\n    else:\n        rr = 'away'\n    #Attacker\n    b = a[a[\"team\"] == rr  ]\n    c = b[b[\"gameId\"] == key.iloc[i,0]]\n    average_s[i].append(np.mean(c['s']))\n    average_a[i].append(np.mean(c['a']))\n    average_dis[i].append(np.mean(c['dis']))\n    average_o[i].append(np.mean(c['o']))\n    average_dir[i].append(np.mean(c['dir']))\n    \n    \n    #Ball\n    b = a[a[\"team\"] == 'football' ]\n    c = b[b[\"gameId\"] == key.iloc[i,0]]\n    average_s[i].append(np.mean(c['s']))\n    average_a[i].append(np.mean(c['a']))\n    average_dis[i].append(np.mean(c['dis'])) \n    \n\n# converting into a dataframe\nspeed = pd.DataFrame(average_s)\nspeed.columns = ['speed_dif','speed_att','speed_ball']\n\nacc = pd.DataFrame(average_a)\nacc.columns = ['acc_dif','acc_att','acc_ball']\n\ndis = pd.DataFrame(average_dis)\ndis.columns = ['dis_dif','dis_att','dis_ball']\n\no = pd.DataFrame(average_o)\no.columns = ['o_dif','o_att']\n\ndire = pd.DataFrame(average_dir)\ndire.columns = ['dire_dif','dire_att']\n\n\n\nadd_info = []\n\n# loop over each passing play in 'key'\nfor i in range(0,len(key)):\n    \n    # filtering of plays with that specific values of \"playID\" and 'gameID'\n    a = plays[plays['playId'] == key.iloc[i,1]]\n    b = a[a[\"gameId\"] == key.iloc[i,0]]\n    \n    # adding 'epa' for the passing play 'i'\n    add_info.append(b['epa'].reset_index().drop('index',axis=1).iloc[0,0]) \n    \nadd_info = pd.DataFrame(add_info)\nadd_info.columns = ['epa'] \nadd_info","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have now obtained a *new* dataset that can effectively represent the **relevant information** from the tracking data, and in which a **classification algorithm** can be applied to predicting DPI penalties.\n\nIn other words, we can now **predict** whether a Defensive Pass Interference will be called on the analyzed passing play, based on the **tracking data**!"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"frames = [_dataseT_dif,_dataseT_off,_dataseT_ball,speed,acc,dis,o,dire,add_info]\n__final_ = pd.concat(frames,axis=1)\n__final_","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# response variable\n_y = []\n\n# loop over each passing play in key\nfor i in range(0,len(key)):\n    \n    # filtering of plays with that specific values of \"gameID\" and \"playID\"\n    a = plays[plays['playId'] == key.iloc[i,1]]\n    b = a[a[\"gameId\"] == key.iloc[i,0]]\n    \n    # value of 'isDefensivePI' \n    _y.append([b['isDefensivePI'].reset_index().drop('index',axis=1).iloc[0,0]])\n    \n_y = pd.DataFrame(_y)\n_y.columns = ['isDefensivePI']\n_y","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting DPI"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can imagine, we have more passing plays that do not have DPI fouls than the passing plays that do have them. Meaning, we are working with an extremely unbalanced dataset. Specifically, the Imbalance Ratio (IR) is equal to 0.01."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"_y.isDefensivePI.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The \"standard\" classification algorithms, such as Random Forest or Linear SVC, do not work correctly with this type of dataset. Therefore, we will use algorithms that perform appropriately with an unbalanced dataset from the library [*imbalanced-learn*](https://github.com/scikit-learn-contrib/imbalanced-learn).\n\nWe will try to maintain this Imbalance Ratio when we split the dataset in train and test set, trying to keep the representativeness of the results."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\ny_encoder = ordinal_encoder.fit_transform(_y)\ny_encoder = np.transpose(y_encoder)\ny_encoder = y_encoder[0].astype(np.float64)\ny_encoder\n\nX = np.array(__final_).astype(np.float64)\n\n# Splitting into train and test maintaing the Imbalance Ratio\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits= 1, test_size=0.25)\nfor train_index, test_index in split.split(_y,_y['isDefensivePI']):\n    y_train = y_encoder[train_index]\n    y_test = y_encoder[test_index]\n    \n    X_train = X[train_index]\n    X_test = X[test_index]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Balanced Random Forest"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from imblearn.ensemble import BalancedRandomForestClassifier\n\n# fitting the model\nbrf = BalancedRandomForestClassifier()\nbrf.fit(X_train, y_train) \n\n# checking the scores\ny_pred = brf.predict(X_test)\n\nprint(f'The balanced accuracy is: {round(balanced_accuracy_score(y_test, y_pred),2)}')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {round(precision_score(y_test,y_pred),2)}')\nprint(f'The recall is: {round(recall_score(y_test,y_pred),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RUSBoost"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from imblearn.ensemble import RUSBoostClassifier\n\n# fitting the model\nrusboost = RUSBoostClassifier(algorithm='SAMME.R')\nrusboost.fit(X_train, y_train)  \n\n# checking the scores\ny_pred = rusboost.predict(X_test)\n\nprint(f'The balanced accuracy is: {round(balanced_accuracy_score(y_test, y_pred),2) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {round(precision_score(y_test,y_pred),2)}')\nprint(f'The recall is: {round(recall_score(y_test,y_pred),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Easy Ensemble classifier"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from imblearn.ensemble import EasyEnsembleClassifier\n\n# fitting the model\neec = EasyEnsembleClassifier()\neec.fit(X_train, y_train) \n\n# checking the scores\ny_pred = eec.predict(X_test)\n\nprint(f'The balanced accuracy is: {round(balanced_accuracy_score(y_test, y_pred),2) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {round(precision_score(y_test,y_pred),2)}')\nprint(f'The recall is: {round(recall_score(y_test,y_pred),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Balanced Bagging"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# fitting the model\nbbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(), sampling_strategy='auto',replacement=False)\nbbc.fit(X_train, y_train) \n\n# checking the scores\ny_pred = bbc.predict(X_test)\n\nprint(f'The balanced accuracy is: {round(balanced_accuracy_score(y_test, y_pred),2) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {round(precision_score(y_test,y_pred),2)}')\nprint(f'The recall is: {round(recall_score(y_test,y_pred),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear SVC"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\n# fitting the model\nsvm_clf = Pipeline([\n        (\"scaler\",StandardScaler()),\n        (\"linear_svc\",LinearSVC(C=10,loss=\"hinge\"))\n    ])\nsvm_clf.fit(X_train, y_train) \n\n# checking the scores\ny_pred = svm_clf.predict(X_test)\n\nprint(f'The balanced accuracy is: {balanced_accuracy_score(y_test, y_pred) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {precision_score(y_test,y_pred)}')\nprint(f'The recall is: {recall_score(y_test,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# fitting the model\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train) \n\n# checking the scores\ny_pred = rfc.predict(X_test)\n\nprint(f'The balanced accuracy is: {balanced_accuracy_score(y_test, y_pred) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {precision_score(y_test,y_pred)}')\nprint(f'The recall is: {recall_score(y_test,y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import xgboost\n\n# fitting the model\nxgb_clf = xgboost.XGBClassifier()\nxgb_clf.fit(X_train,y_train)\n\n# checking the scores\ny_pred = xgb_clf.predict(X_test)\n\nprint(f'The balanced accuracy is: {round(balanced_accuracy_score(y_test, y_pred),2) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {round(precision_score(y_test,y_pred),2)}')\nprint(f'The recall is: {round(recall_score(y_test,y_pred),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adaboost"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# fitting the model\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), \n                             algorithm='SAMME.R', learning_rate=0.5, n_estimators=200)\nada_clf.fit(X_train, y_train)\n\n# checking the scores\ny_pred = ada_clf.predict(X_test)\n\nprint(f'The balanced accuracy is: {round(balanced_accuracy_score(y_test, y_pred),2) }')\nprint(f'The confusion matrix is: \\n {confusion_matrix(y_test,y_pred)}')\nprint(f'The precision is: {round(precision_score(y_test,y_pred),2)}')\nprint(f'The recall is: {round(recall_score(y_test,y_pred),2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given the impact that DPI might have on the match, I would prefer to recognize as many DPI as possible to provide the treatment they deserve. Therefore, I would pick the model that has the **highest recall**, and the Easy Ensemble classifier is the right option.\n\nIn any case, each coach has his/her opinion. Someone may prefer to be sure about predicting that the passing play actually has a DPI foul, at the risk of not to detect all them. In this case, the XGboost can be adopted since it has the **highest precision**.\n\nThe results obtained can be improved by:\n- analyzing the optimal threshold\n- hyperparameter optimization\n- And ... **I am open to any suggestion!**"},{"metadata":{},"cell_type":"markdown","source":"[Mattia Arsendi](https://mattia-arsendi.netlify.app/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}