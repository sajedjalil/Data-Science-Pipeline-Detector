{"cells":[{"metadata":{},"cell_type":"markdown","source":"DESCRIPTION\n\nMore and more sports teams are trying to exploit the game data to their advantage. The NFL Next Gen Stats track the position and the speed of every player during each play. Any actionable insight from such massive data could provide coaches that extra yard needed to change the result of the game in their favor. Here a model is presented using Deep Learning techniques to predict the intended receiver before the quarterback releases the football.\n\nMETHODOLOGY\n- The model discussed here used the data from NFL Next Gen Stats available on kaggle website. The data has information about each play from every game played in 2018 season.\n\n- As a part of the data preparation for model training, the data from a frame (snapshot) just before the release of football from quarterback is extracted and used as an input to the model. The frame contains the position, speed and direction of offensive and defensive players at that moment.\n\n- Based on the commentary provided for each play, intended receiver for a pass from quarterback is obtained and used as an output of the model. For example, “J.Jones” is extraxted as intended receiver from the commentary “(2:01) M.Ryan pass deep left to J.Jones to …”. Offensive players are tagged before-hand to be able to predict intended reciever using classification approach.\n\n- Only pass plays (either complete or incomplete) of the games are considered."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\n#pd.set_option(\"display.max_columns\", None)\n#pd.set_option(\"display.max_rows\", 200)\n#pd.set_option('display.max_colwidth', None)\n\nDELETE = 1\npara_weeks_considered = 17\npara_frames_considered = 1\npara_frames_step = 1\ndata_csv = \"/kaggle/working/data_1_17_51.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plays data\ndf_plays = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2021/plays.csv\")\ndf_plays.head()\n#df_plays[(df_plays[\"gameId\"] == 2018091700) & (df_plays[\"playId\"] == 2560)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weeks data\ndf_weeks = list()\nfor week in range(1,para_weeks_considered+1):\n    df_week = pd.read_csv(\"/kaggle/input/nfl-big-data-bowl-2021/week\" + str(week) + \".csv\")\n    df_week.drop(df_week[df_week[\"displayName\"] == \"Football\"].index, inplace=True)     ## FIX\n    df_weeks.append(df_week)\n    del df_week\ndf_weeks = pd.concat(df_weeks)\ndf_weeks.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Pre-processing (Getting relevant frames)\ndf_w1 = df_weeks.copy()\nif (DELETE):\n    del df_weeks\n\n#df_w1 = df_w1[[\"x\",\"y\",\"event\",\"displayName\",\"position\",\"frameId\",\"gameId\",\"playId\"]]         # FIX\ndf_w1 = df_w1[[\"x\",\"y\",\"s\",\"o\",\"dir\",\"event\",\"displayName\",\"position\",\"frameId\",\"gameId\",\"playId\"]]\n\ndf_w1_pf = df_w1[(df_w1[\"position\"] == \"QB\") & (df_w1[\"event\"] == \"pass_forward\")][[\"gameId\",\"playId\",\"frameId\"]].copy()\ndf_w1_pf[\"min_frameId\"] = df_w1_pf[\"frameId\"] - 0         # FIX  0 or para_frames_considered\ndf_w1_pf[\"max_frameId\"] = df_w1_pf[\"frameId\"] - 0         # FIX  0 or 1\ndf_w1.drop(columns=[\"event\"],inplace=True)\n\ndf_w1.set_index([\"gameId\",\"playId\"],inplace=True)\ndf_w1_pf.set_index([\"gameId\",\"playId\"],inplace=True)\ndf_w1 = df_w1.join(df_w1_pf, on=[\"gameId\",\"playId\"], how='left', rsuffix='_play')\ndf_w1 = df_w1[(df_w1[\"min_frameId\"] <= df_w1[\"frameId\"]) & \\\n              (df_w1[\"frameId\"] <= df_w1[\"max_frameId\"]) & \\\n              ((df_w1[\"frameId\"]-df_w1[\"min_frameId\"]) % para_frames_step == 0)]\ndf_w1[\"new_frameId\"] = df_w1[\"frameId\"] - df_w1[\"min_frameId\"] + 1\ndf_w1.drop(columns=[\"frameId\",\"frameId_play\",\"min_frameId\",\"max_frameId\"], inplace=True)\n\ndf_w1[\"Side\"] = \"Side\"\ndf_w1.loc[df_w1[\"position\"].str.contains('SS|FS|MLB|CB|LB|OLB|DL|DB|ILB|NT|S|DE'),\"Side\"] = \"D\"\ndf_w1.loc[df_w1[\"position\"].str.contains('QB|WR|RB|TE|FB|HB'),\"Side\"] = \"O\"\nif ((df_w1[\"Side\"] == \"Side\").sum() > 0):\n    df_w1.drop(df_w1[df_w1[\"Side\"] == \"Side\"].index, inplace=True)\ndf_w1.reset_index(inplace=True)\n\nif (DELETE):\n    del df_w1_pf\ndf_w1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Pre-processing (Getting intended receiver from commentary)\ndf_p1 = df_plays.copy()\nif (DELETE):\n    del df_plays\ndf_p1 = df_p1[(df_p1[\"passResult\"] == \"C\") | (df_p1[\"passResult\"] == \"I\")]       # FIX\n#df_p1 = df_p1[(df_p1[\"passResult\"] == \"C\")]\ndf_p1[\"intendedReciever\"] = df_p1[\"playDescription\"].str.split(\" to \").str[1].str.split(\" \").str[0] \\\n                                    .str.replace(\".\",\" \").str.strip()\n\ndf_p1.drop(df_p1[df_p1[\"intendedReciever\"].isnull()].index, inplace=True)\ndf_p1 = df_p1[['gameId','playId','quarter','down','yardsToGo','preSnapVisitorScore','preSnapHomeScore', \\\n                'gameClock','absoluteYardlineNumber','intendedReciever']]\n\ndf_p1 = df_p1.drop(df_p1[df_p1['gameClock'].isnull()].index, axis=0)\ndf_p1_temp = df_p1['gameClock'].str.split(':', expand=True).astype(int)\ndf_p1['timeLeft'] = df_p1_temp[0] * 60 + df_p1_temp[1]\n\ndel df_p1_temp\ndf_p1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data pre-processing (Matching intended receiver with offensive players)\ndf_w2 = df_w1.copy()\nif (DELETE):\n    del df_w1\n\ndf_w2.set_index([\"gameId\",\"playId\"], inplace=True)\ndf_p1.set_index([\"gameId\",\"playId\"], inplace=True)\ndf_w2 = df_w2.join(df_p1[\"intendedReciever\"], how='inner', on=[\"gameId\",\"playId\"])\ndf_w2.reset_index(inplace=True)\n\ndf_w2[\"recieverMatch\"] = 0\ndf_w2.loc[(df_w2[\"displayName\"].str[0] == df_w2[\"intendedReciever\"].str[0]) & \\\n(df_w2[\"displayName\"].str.split(\" \").str[-1] == df_w2[\"intendedReciever\"].str.split(\" \").str[-1]), \\\n        \"recieverMatch\"] = 1\n\ndf_w2.drop(columns=[\"intendedReciever\"], inplace=True)\ndf_w2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data clean up for intended reciever\ndf_w2_1 = df_w2.drop(df_w2[(df_w2[\"recieverMatch\"] == 1) & (df_w2[\"position\"] == \"QB\")].index)\n\ndf_w2_1 = df_w2_1.groupby([\"gameId\",\"playId\"])[[\"recieverMatch\"]].sum()\ndf_w2_1.reset_index(inplace=True)\ndf_w2_1.drop(df_w2_1[df_w2_1[\"recieverMatch\"] == 0].index, inplace=True)\n\ndf_w2.set_index([\"gameId\",\"playId\"], inplace=True)\ndf_w2_1.set_index([\"gameId\",\"playId\"], inplace=True)\ndf_w2 = df_w2.join(df_w2_1, how=\"inner\", on=[\"gameId\",\"playId\"], rsuffix=\"_temp\")\ndf_w2.reset_index(inplace=True)\ndf_w2.drop(columns=df_w2.columns[df_w2.columns.str.contains(\"_temp\")], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tagging offensive and defensive players to be used for classification\ndf_w3 = df_w2.copy()\nif (DELETE):\n    del df_w2\n\ndf_w3 = df_w3.sort_values([\"gameId\",\"playId\",\"new_frameId\",\"Side\"], ascending=[True,True,True,False])\ndf_w3.reset_index(inplace=True)\ndf_w3[\"playerId\"] = -1\ndf_w3.loc[(df_w3[\"position\"] == \"QB\"),\"playerId\"] = 0\n\ng = df_w3.loc[0,\"gameId\"]\np = df_w3.loc[0,\"playId\"]\nf = df_w3.loc[0,\"new_frameId\"]\ns = df_w3.loc[0,\"Side\"]\nn = 1\nfor i in range(len(df_w3)):\n    if (df_w3.loc[i,\"position\"] == \"QB\"):\n        continue\n        \n    if ((df_w3.loc[i,\"gameId\"] == g) & (df_w3.loc[i,\"playId\"] == p) & \\\n        (df_w3.loc[i,\"new_frameId\"] == f) & (df_w3.loc[i,\"Side\"] == s)):\n        df_w3.loc[i,\"playerId\"] = n\n        n += 1\n    else:\n        g = df_w3.loc[i,\"gameId\"]\n        p = df_w3.loc[i,\"playId\"]\n        f = df_w3.loc[i,\"new_frameId\"]\n        s = df_w3.loc[i,\"Side\"]\n        n = 1\n        df_w3.loc[i,\"playerId\"] = n\n        n += 1  \n\ndf_w3[\"Side_playerId\"] =  df_w3[\"Side\"] + df_w3[\"playerId\"].astype(str)\ndf_w3.drop(columns=[\"index\",\"playerId\",\"Side\",\"displayName\",\"position\"], inplace=True)\ndf_w3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w3.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning\ndf_w4 = df_w3.copy()\nif (DELETE):\n    del df_w3\n\n# FIX\n#df_w4_bad = df_w4[(df_w4[\"Side_playerId\"] == \"O6\") | (df_w4[\"Side_playerId\"] == \"D8\")] \\\ndf_w4_bad = df_w4[(df_w4[\"Side_playerId\"] == \"O6\") | (df_w4[\"Side_playerId\"] == \"D10\")] \\\n                    [[\"gameId\",\"playId\"]].copy()\ndf_w4_bad[\"dummy\"] = 1\n\ndf_w4.set_index([\"gameId\",\"playId\"], inplace=True)\ndf_w4_bad.set_index([\"gameId\",\"playId\"], inplace=True)\ndf_w4 = df_w4.join(df_w4_bad, on=[\"gameId\",\"playId\"], how='left', rsuffix='_bad')\ndf_w4.drop(df_w4[(df_w4[\"dummy\"] == 1)].index, inplace=True)\ndf_w4.drop(columns=[\"dummy\"], inplace=True)\ndf_w4.rename(columns={\"recieverMatch\": \"IR\"}, inplace=True)\n\ndel df_w4_bad\ndf_w4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w4.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleaning\ndf_data1 = pd.pivot_table(df_w4, values=[\"x\",\"y\",\"s\",\"o\",\"dir\",\"IR\"],\n#df_data1 = pd.pivot_table(df_w4, values=[\"x\",\"y\",\"IR\"], \\\n                          index=[\"gameId\",\"playId\",\"new_frameId\"], columns=[\"Side_playerId\"]).copy()\n\ndf_data1.drop(columns=[('IR','D1'),('IR','D2'),('IR','D3'),('IR','D4'),('IR','D5'), \\\n                       ('IR','D6'),('IR','D7'),('IR','D8'),('IR','D9'), \\\n                       ('IR','O0')], inplace=True)          # FIX\n\ndf_data1.columns = ['_'.join(col).strip() for col in df_data1.columns.values]\ndf_data1.reset_index(inplace=True)\nif (DELETE):\n    del df_w4\ndf_data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning\ndf_data2 = df_data1.join(df_p1, on=[\"gameId\",\"playId\"], how='left', rsuffix='_play').copy()\nif (DELETE):\n    del df_data1\n\ndf_data2.drop(columns=[\"gameClock\",\"intendedReciever\"], inplace=True)\n#df_data2.reset_index(inplace=True)\n#df_data2.fillna(-1, inplace=True)         # FIX\ndf_data2.fillna(0, inplace=True)         # FIX\ndf_data2.drop(df_data2[(df_data2[\"IR_O1\"] == 0) & (df_data2[\"IR_O2\"] == 0) & (df_data2[\"IR_O3\"] == 0) & \\\n                      (df_data2[\"IR_O4\"] == 0) & (df_data2[\"IR_O5\"] == 0)].index, inplace=True)      # FIX\n\nif (DELETE):\n    del df_p1\ndf_data2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving data to csv file\ndf_data2.to_csv(data_csv, index=False)\ndf_data2.info()\n\nif (DELETE):\n    del df_data2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dropout, Activation, Dense, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.optimizers import SGD, Adadelta\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the training set\n#path = \"/kaggle/working/\"\npath = \"/kaggle/input/dataandmodel/\"\ndf_xy_data = pd.read_csv(path + \"data_1_17_51.csv\")\n#df_xy_data = pd.read_csv(data_csv)\nxy_data = df_xy_data.iloc[:,3:].values\n\ndf_xy_data.head()\n#df_xy_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range=(0,1))\nxy_data = sc.fit_transform(xy_data)\n\ny_data = xy_data[:,0:5]\nx_data = xy_data[:,5:]\n\nx_data, y_data = np.array(x_data), np.array(y_data)\n#x_data = np.reshape(x_data, (x_data.shape[0], x_data.shape[1], x_data.shape[2]))\nprint (x_data.shape, y_data.shape)\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, shuffle=True, random_state=0)\nprint (x_train.shape, y_train.shape, x_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Augmentation\ncolumns = list(df_xy_data.columns)\na = columns.index(\"IR_O1\")-3\nb = columns.index(\"x_O1\")-8\nc = columns.index(\"y_O1\")-8\nd = columns.index(\"dir_O1\")-8\ne = columns.index(\"s_O1\")-8\nf = columns.index(\"o_O1\")-8\nprint (a,b,c,d,e,f)\ndata1_y = y_train.copy()\ndata1_x = x_train.copy()\n\nfor i in range(0,5):\n    for j in range(0,5):\n        if (j==i):\n            continue\n        for k in range(0,5):\n            if ((k==i) | (k==j)):\n                continue\n            for l in range(0,5):\n                if ((l==i) | (l==j) | (l==k)):\n                    continue\n                for m in range(0,5):\n                    if ((m==i) | (m==j) | (m==k) | (m==l)):\n                        continue\n                    elif ((i==0) & (j==1) & (k==2) & (l==3) & (m==4)):\n                        continue\n                    else:\n                        data2_y = y_train.copy()\n                        data2_x = x_train.copy()\n                        data2_y[:,[a+0,a+1,a+2,a+3,a+4]] = data2_y[:,[a+i,a+j,a+k,a+l,a+m]]\n                        data2_x[:,[b+0,b+1,b+2,b+3,b+4]] = data2_x[:,[b+i,b+j,b+k,b+l,b+m]]\n                        data2_x[:,[c+0,c+1,c+2,c+3,c+4]] = data2_x[:,[c+i,c+j,c+k,c+l,c+m]]\n                        data2_x[:,[d+0,d+1,d+2,d+3,d+4]] = data2_x[:,[d+i,d+j,d+k,d+l,d+m]]                        \n                        data2_x[:,[e+0,e+1,e+2,e+3,e+4]] = data2_x[:,[e+i,e+j,e+k,e+l,e+m]]                        \n                        data2_x[:,[f+0,f+1,f+2,f+3,f+4]] = data2_x[:,[f+i,f+j,f+k,f+l,f+m]]                        \n                        data1_y = np.vstack((data1_y,data2_y))\n                        data1_x = np.vstack((data1_x,data2_x))\n\nprint (y_train.shape, x_train.shape)                        \ny_train = data1_y.copy()\nx_train = data1_x.copy()\nprint (y_train.shape, x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model\nregressor = Sequential()\n        \nregressor.add(Dense(units=128, activation=\"relu\", kernel_initializer='he_uniform', input_shape=(x_train.shape[1],)))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=256, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=512, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=512, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=512, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=256, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=128, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=64, activation=\"relu\", kernel_initializer='he_uniform'))\nregressor.add(BatchNormalization())\nregressor.add(Dropout(0.4))\n\nregressor.add(Dense(units=5, activation=\"softmax\"))\n\nregressor.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n#regressor.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['accuracy'])\n#regressor.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['accuracy'])\nregressor.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading model weights for training\nregressor.load_weights(path + \"/model_epochs1-20_dropout40_categorical.h5\")\n# regressor.load_weights(\"/kaggle/working/model_1_20_dropout40.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=regressor.fit(x_train, y_train, epochs=1, batch_size=32, validation_split=0.1, shuffle=True, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\n#plt.ylim((0,0.16))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n# Prediction\ny_pred = regressor.predict(x_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_test = np.argmax(y_test, axis=1)\n\n# Confusion Matrix\ncm = metrics.confusion_matrix(y_test,y_pred)\nscore = accuracy_score(y_test,y_pred)*100\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt=\".0f\", linewidth=0.5, square=True, cmap=\"Blues_r\")\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Predicted\")\nall_sample_title = \"TEST Accuracy Score: {0}\".format(score)\nplt.title(all_sample_title, size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n# Prediction\ny_pred_train = regressor.predict(x_train)\ny_pred_train = np.argmax(y_pred_train, axis=1)\ny_train = np.argmax(y_train, axis=1)\n\n# Confusion Matrix\ncm = metrics.confusion_matrix(y_train,y_pred_train)\nscore = accuracy_score(y_train,y_pred_train)*100\n\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt=\".0f\", linewidth=0.5, square=True, cmap=\"Blues_r\")\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Predicted\")\nall_sample_title = \"TRAIN Accuracy Score: {0}\".format(score)\nplt.title(all_sample_title, size=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving model\n# regressor.save(\"/kaggle/working/model_epochs21-40_dropout40_categorical.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}