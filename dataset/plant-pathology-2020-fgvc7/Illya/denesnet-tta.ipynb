{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Leaf Disease Detection\n\nLearning goals:\n- Exploratory data analysis is important\n- Learn how to use [albumentations](https://github.com/albumentations-team/albumentations) augmentation library.\n- Learn how to combine albumentations with TensorFlow Dataset API\n- Learn how to write custom Grid Mask Augmentation class\n- Learn about Generalized Average Pooling\n- Learn about Label Smoothing\n- Learn how to use Test Time Augmentation (TTA)"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# paths\nimport os\nfrom os.path import join\n\n# RNG and math\nimport random\nimport math\nfrom math import ceil\n\n# progress bar\nfrom tqdm.notebook import tqdm # progress bar\n\n# data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# deep learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Input, Lambda\nfrom tensorflow.keras.applications import ResNet50, InceptionResNetV2\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\n!pip install -q efficientnet\nimport efficientnet.tfkeras as efn\n\n#plotting\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n# image manipulation\nimport cv2\n\n# Augmentations\nimport albumentations\nfrom albumentations import *\nfrom albumentations.core.transforms_interface import DualTransform\nfrom albumentations.augmentations import functional as F\n\n# datasets\nfrom kaggle_datasets import KaggleDatasets\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Main Modeling Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define size of the image to train on\nIMAGE_X = 512\nIMAGE_Y = 512\ninput_shape = (IMAGE_X, IMAGE_Y, 3)\n\nEPOCHS = 50\nLR = 0.00016\nBATCH_SIZE = 16\n\nLABEL_SMOOTHING_ALPHA=0.02\nUSE_GMP = True # Generalized Mean Pooling\n\nSEED = 42","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"IMAGE_PATH = \"../input/plant-pathology-2020-fgvc7/images/\"\nTEST_PATH = \"../input/plant-pathology-2020-fgvc7/test.csv\"\nTRAIN_PATH = \"../input/plant-pathology-2020-fgvc7/train.csv\"\nSUB_PATH = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\nMODEL_PATH = \"models/plant_pathology_model.h5\"\n\nsample_submission = pd.read_csv(SUB_PATH)\ndf_test = pd.read_csv(TEST_PATH)\ndf_train = pd.read_csv(TRAIN_PATH)\nlabels = list(df_train.columns[1:])\nAUTO = tf.data.experimental.AUTOTUNE\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Data"},{"metadata":{},"cell_type":"markdown","source":"### CSV files"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training set size:', len(df_train))\nfor label in labels:\n    print(f\"\\t{label}: {df_train[df_train[label]==1].shape[0]}\")\nprint('Test set size:', len(df_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(image_id):\n    file_path = \"{}{}\".format(image_id, \".jpg\")\n    image = cv2.imread(join(IMAGE_PATH, file_path))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\n\n# plot some images\ndef plot_sample_images(preprocess_fn=None, nrows=4, ncols=4):\n    fig, axs = plt.subplots(nrows, ncols,\n                            figsize=(12,10)\n                           )\n    axs = axs.ravel() # make 1D array for easy plotting in for loop\n\n    for i, image_id in enumerate(np.random.randint(len(df_train), size=nrows*ncols)):\n        # show an image\n        img = load_image(df_train['image_id'][image_id])\n        \n        if preprocess_fn:\n            img = preprocess_fn(img)\n        axs[i].imshow(img)\n        axs[i].axis(False)\n        label = df_train.loc[:, 'healthy':].iloc[image_id, :].idxmax()\n        axs[i].set_title('{} | {}'.format(df_train['image_id'][image_id], label))\n        plt.tight_layout()\nplot_sample_images(nrows=4, ncols=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the plot below you can check the exact values of each pixel. Notice that most of the pixels have high green and low blue values. However the spot on the leaf has high blue value."},{"metadata":{"trusted":true},"cell_type":"code","source":"image = load_image(df_train['image_id'][0])\nfig = px.imshow(image)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets have a closer look at the distribution of the channel values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code thanks to https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\n\nn_samples = 100\ntrain_images = df_train[\"image_id\"][:n_samples].apply(load_image)\n\nred_values = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\ngreen_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\nblue_values = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]\nvalues = [np.mean(train_images[idx]) for idx in range(len(train_images))]\n\nfig = ff.create_distplot([red_values, green_values, blue_values],\n                         group_labels=[\"R\", \"G\", \"B\"],\n                         colors=[\"red\", \"green\", \"blue\"])\nfig.update_layout(title_text=\"Distribution of channel values\")\nfig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plot above we can see that the most pronounced color is green, which has the highest values. All other channels are shifted to the left with blue being the least pronounced."},{"metadata":{},"cell_type":"markdown","source":"### Problems with the data\nSource: https://www.kaggle.com/c/plant-pathology-2020-fgvc7/discussion/154056\n\nSome of the images have incosistent labels.\nImages `Train_379` and `Train_1173` are the same images but have different labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_by_image_id(image_ids):\n\n    fig, axs = plt.subplots(1, len(image_ids),\n                                figsize=(12,10)\n                               )\n    axs = axs.ravel() # make 1D array for easy plotting in for loop\n\n    for i, image_id in enumerate(image_ids):\n        # show an image\n        img = load_image(image_id)\n        axs[i].imshow(img)\n        axs[i].axis(False)\n        img_ctgs = df_train[df_train['image_id'] == image_id][labels].values[0]\n        img_label = np.argmax(df_train[df_train['image_id'] == image_id].loc[:, 'healthy':'scab'].values[0])\n        axs[i].set_title('{} | {} | {}'.format(image_id, img_ctgs, labels[img_label]))\n        plt.tight_layout()\n        \nplot_by_image_id(['Train_379', 'Train_1173'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Images `Train_1` and `Train_171` are generated by the same image, but have different labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_by_image_id(['Train_1', 'Train_171'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How to solve the inconsistencies in the labeling?\nWe have a couple of options for solving these inconsistencies:\n1. Remove one duplicate images. However, now we are faced with questions:\n    - Which image to remove?\n    - Which label is the correct label?\n2. Label Smoothing (used in this notebook). Label smoothings relaxes the confidence we have in the provided labels. We can control the relaxation with the parameter $\\alpha$ and apply the following formula to our labels: $y_k^{LS}=y_K(1-\\alpha) + \\frac{\\alpha}{K}$, where $y_k$ is $k$'th element of the one-hot encoded 'true' vector $y$ and $K$ is the number of classes.\n    - Labels Smoothing makes the difference between prediction and 'true' label to be dependant on a constant $\\alpha$\n    - 'It encourages the activations of the penultimate layer to be close to the correct class template and equally distant to the templates of the incorrect classes.' [[source](https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326)]\n    - When does label smoothing help? [[paper](https://arxiv.org/pdf/1906.02629.pdf)], [[article](https://medium.com/@nainaakash012/when-does-label-smoothing-help-89654ec75326)]\n3. Knowledge distillation (discussed in the [thread](https://www.kaggle.com/c/plant-pathology-2020-fgvc7/discussion/154056)). In knowledge distillation, you train a small student network with labels generated by a large teacher network. Intuitively you can understand it as relying less on the probably mistaken data and learning the labels from the images themselves.\n    - Knowledge distillation in [Keras](https://arxiv.org/abs/2006.05525)\n    - Original paper: [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n    - [Knowledge Distillation: A Survey](https://arxiv.org/abs/2006.05525)"},{"metadata":{},"cell_type":"markdown","source":"## Data Generators and Augmentation\nFor loading the data, we are going to use the TensorFlow dataset API. For the data augmnetation, we are going to use Albumentations [[Docs](https://albumentations.ai/docs/getting_started/installation/)] and show how to use it with TensorFlow Dataset API."},{"metadata":{"trusted":true},"cell_type":"code","source":"class GridMask(DualTransform):\n    \n    \"\"\"GridMask augmentation for image classification and object detection.\n    \n    Author: Qishen Ha\n    Email: haqishen@gmail.com\n    2020/01/29\n\n    Args:\n        num_grid (int): number of grid in a row or column.\n        fill_value (int, float, lisf of int, list of float): value for dropped pixels.\n        rotate ((int, int) or int): range from which a random angle is picked. If rotate is a single int\n            an angle is picked from (-rotate, rotate). Default: (-90, 90)\n        mode (int):\n            0 - cropout a quarter of the square of each grid (left top)\n            1 - reserve a quarter of the square of each grid (left top)\n            2 - cropout 2 quarter of the square of each grid (left top & right bottom)\n\n    Targets:\n        image, mask\n\n    Image types:\n        uint8, float32\n\n    Reference:\n    |  https://arxiv.org/abs/2001.04086\n    |  https://github.com/akuxcw/GridMask\n    \"\"\"\n\n    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n        super(GridMask, self).__init__(always_apply, p)\n        if isinstance(num_grid, int):\n            num_grid = (num_grid, num_grid)\n        if isinstance(rotate, int):\n            rotate = (-rotate, rotate)\n        self.num_grid = num_grid\n        self.fill_value = fill_value\n        self.rotate = rotate\n        self.mode = mode\n        self.masks = None\n        self.rand_h_max = []\n        self.rand_w_max = []\n\n    def init_masks(self, height, width):\n        if self.masks is None:\n            self.masks = []\n            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n                grid_h = height / n_g\n                grid_w = width / n_g\n                this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n                for i in range(n_g + 1):\n                    for j in range(n_g + 1):\n                        this_mask[\n                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n                        ] = self.fill_value\n                        if self.mode == 2:\n                            this_mask[\n                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n                            ] = self.fill_value\n                \n                if self.mode == 1:\n                    this_mask = 1 - this_mask\n\n                self.masks.append(this_mask)\n                self.rand_h_max.append(grid_h)\n                self.rand_w_max.append(grid_w)\n\n    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n        h, w = image.shape[:2]\n        mask = F.rotate(mask, angle) if self.rotate[1] > 0 else mask\n        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n        image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n        return image\n\n    def get_params_dependent_on_targets(self, params):\n        img = params['image']\n        height, width = img.shape[:2]\n        self.init_masks(height, width)\n\n        mid = np.random.randint(len(self.masks))\n        mask = self.masks[mid]\n        rand_h = np.random.randint(self.rand_h_max[mid])\n        rand_w = np.random.randint(self.rand_w_max[mid])\n        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n\n        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n\n    @property\n    def targets_as_params(self):\n        return ['image']\n\n    def get_transform_init_args_names(self):\n        return ('num_grid', 'fill_value', 'rotate', 'mode')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we define a dictionary with different kinds of augmentations. The key is the name of the augmentation functions set, and the value is a single Albumentations class. You can use each Albumentations class one by one or use `Compose` to string several augmentations together. If you would like to choose only one augmentation at a time, you can use `OneOf`."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntransform = {\n    'train': Compose([\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n#         RandomRotate90(p=0.5),\n        GaussianBlur(blur_limit=(3, 7), always_apply=False, p=0.5),\n        ShiftScaleRotate(shift_limit=0.0625,\n                        scale_limit=0.1,\n                        rotate_limit=45,\n                        interpolation=cv2.INTER_LINEAR,\n                        border_mode=cv2.BORDER_REFLECT_101,\n                        always_apply=False,\n                        p=0.5,),\n        RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5, always_apply=False),\n        GridMask(num_grid=4, rotate=15, p=0.5)\n    ]),\n    \n    'vertical_flip': VerticalFlip(always_apply=True),\n    \n    'horizontal_flip': HorizontalFlip(always_apply=True),\n    \n    'vh_flip': Compose([\n        VerticalFlip(always_apply=True),\n        HorizontalFlip(always_apply=True)\n    ]),\n    \n    'brightness': RandomBrightnessContrast(\n        brightness_limit=0.2,\n        contrast_limit=0.2,\n        p=0.5, always_apply=True),\n    \n    'blur': GaussianBlur(blur_limit=(3, 7), always_apply=True),\n    \n    'shift_scale_rotate' : ShiftScaleRotate(\n                        shift_limit=0.0625,\n                        scale_limit=0.1,\n                        rotate_limit=45,\n                        interpolation=cv2.INTER_LINEAR,\n                        border_mode=cv2.BORDER_REFLECT_101,\n                        always_apply=True)\n}\n\ndef preprocess(df, test=False):\n    paths = df.image_id.apply(lambda x: IMAGE_PATH + x + '.jpg').values\n    labels = df.loc[:,'healthy':].values\n    if test==False:\n        return paths, labels\n    else:\n        return paths\n    \ndef decode_image(filename, label=None, image_size=(IMAGE_X, IMAGE_Y)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3) \n    image = tf.image.resize(image, image_size)\n    image = tf.cast(image, tf.float32)\n#     image = tf.image.per_image_standardization(image)\n    image = tf.divide(image, 255.)\n    if label is None:\n        return image\n    else:\n        return image, label\n    \ndef data_augment(image, label=None, seed=SEED):\n    image = tf.image.rot90(image,k=np.random.randint(4))\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n    if label is None:\n        return image\n    else:\n        return image, label\n\ndef get_augmentation_func(set_name='train'):\n    \"\"\"\n    Given augmentation class name returs a function to augment images\n    and the same function wrapped into tf.pyfunction, so it\n    can beused in the tensorflow computation graph\n    \"\"\"\n    def albu(image):\n        transforms = transform[set_name]\n        image = transforms(image=image.numpy())['image']\n        image = tf.cast(image, tf.float32)\n        return image\n\n    def albu_fn(image,label=None):\n        [image,] = tf.py_function(albu, [image], [tf.float32])\n        if label is None:\n            return image\n        else:\n            return image, label\n    return albu, albu_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_transform(num_images=7, set_name='train'):\n    fig, axs = plt.subplots(nrows=2, ncols=num_images, figsize=(30,10))\n    for i in range(0, num_images):\n        image_id = np.random.randint(len(df_train))\n        path, _ = preprocess(df_train.iloc[image_id: image_id+1])\n        image = decode_image(filename=path[0])\n        axs[0, i].imshow(image)\n        albu, albu_fn = get_augmentation_func(set_name)\n        image = albu(image)\n        axs[1, i].imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training augmenation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transform(5, 'train')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid = train_test_split(df_train, test_size=0.2, random_state=SEED)\n\nalbu, albu_fn = get_augmentation_func('train')\n\ntrain_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(train))\n    .map(decode_image, num_parallel_calls=AUTO)\n#     .map(data_augment, num_parallel_calls=AUTO)\n    .map(albu_fn, num_parallel_calls=AUTO)\n    .shuffle(SEED)\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .repeat()\n    .prefetch(AUTO))\n\nvalid_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(valid))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))\n\ntest_dataset = (tf.data.Dataset\n    .from_tensor_slices(preprocess(df_test,test=True))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import MobileNet, DenseNet121\n\n# Generalized Mean Pooling https://paperswithcode.com/method/generalized-mean-pooling \nclass GeneralizedMeanPool(tf.keras.layers.Layer):\n    def __init__(self):\n        super(GeneralizedMeanPool, self).__init__()\n        self.gm_exp = tf.Variable(3.0, dtype = tf.float32)\n\n    def call(self, inputs):\n        pool = (tf.reduce_mean(tf.abs(inputs ** (self.gm_exp)), \n            axis = [1, 2], \n            keepdims = False) + 1.e-7) ** (1. / self.gm_exp)\n        return pool\n\n\ndef create_model(input_shape, train_conv_layers=True, use_gmp=False):\n    input_ = Input(shape = input_shape)\n\n    #Create and complite model and show summary\n    \n    x_model = DenseNet121(weights='imagenet',\n                       include_top=False,\n                       input_tensor=input_,\n                       pooling=None,\n                       classes=None)\n    \n    for layer in x_model.layers:\n        layer.trainable = train_conv_layers\n    \n    # GMP\n    x = x_model.output\n    if use_gmp:\n        x = GeneralizedMeanPool()(x)\n    \n    #output \n    output = Dense(4, activation='softmax', name='plan_diseases')(x)\n   \n    \n    #model \n    model = Model(inputs = x_model.input, outputs=output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model(input_shape, train_conv_layers=True, use_gmp=USE_GMP)\n\n# label smoothing could be used by passing additional\n# parameter to the keras loss\nls_loss = tf.keras.losses.CategoricalCrossentropy(\n    label_smoothing=LABEL_SMOOTHING_ALPHA,\n    name='categorical_crossentropy'\n)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n              loss=ls_loss,\n              metrics = ['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_w = ModelCheckpoint('plant_best.h5',\n                                monitor='val_loss',\n                                verbose=0,\n                                save_best_only=True,\n                                save_weights_only=True,\n                                mode='auto',\n                                period=1)\n\nlast_w = ModelCheckpoint('plant_last.h5',\n                               monitor='val_loss',\n                                verbose=0,\n                                save_best_only=False,\n                              mode='auto',\n                                period=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.000001, verbose=1,cooldown=1)\ncallbacks = [best_w, last_w, reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"STEPS_PER_EPOCH = train.shape[0] // BATCH_SIZE\nhistory = model.fit(train_dataset,\n          steps_per_epoch=STEPS_PER_EPOCH,\n          epochs=EPOCHS, \n          verbose=1,\n          callbacks=callbacks,\n          validation_data=valid_dataset,\n          use_multiprocessing=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\ndef visualize_training_process(history):\n    \"\"\" \n    Visualize loss and accuracy from training history\n    \n    :param history: A Keras History object\n    \"\"\"\n    history_df = pd.DataFrame(history.history)\n    epochs = np.arange(1, len(history_df) + 1)\n    fig = make_subplots(2, 1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['categorical_accuracy'], mode='lines+markers', name='Accuracy Train'), row=1, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_categorical_accuracy'], mode='lines+markers', name='Accuracy Val'), row=1, col=1)\n    \n    fig.append_trace(go.Scatter(x=epochs, y=history_df['loss'], mode='lines+markers', name='Loss Train'), row=2, col=1)\n    fig.append_trace(go.Scatter(x=epochs, y=history_df['val_loss'], mode='lines+markers', name='Loss Val'), row=2, col=1)\n    \n    fig.update_layout( xaxis_title=\"Epochs\", template=\"plotly_white\")\n    \n    return fig\nvisualize_training_process(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the best model\nmodel.load_weights('plant_best.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions without Test Time Augmentation\ntest_pred = model.predict(test_dataset, verbose=1)\nsubmission_df = pd.read_csv(SUB_PATH)\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = test_pred\nsubmission_df.to_csv('submission.csv', index=False)\npd.Series(np.argmax(submission_df[labels].values,axis=1)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TTA all\ntest_pred_tta = np.zeros((len(df_test), 4))\n# Choose what augmentations to use during predictions\naugmentation_sets = [\n    'vertical_flip',\n    'horizontal_flip',\n    'vh_flip',\n    'brightness',\n    'blur',\n    'shift_scale_rotate'\n]\nfor i, set_name in enumerate(augmentation_sets):\n    _, albu_fn = get_augmentation_func(set_name)\n    test_dataset_tta = (\n        tf.data.Dataset.from_tensor_slices(preprocess(df_test, test=True))\n                       .map(decode_image, num_parallel_calls=AUTO)\n                       .map(albu_fn, num_parallel_calls=AUTO)    \n                       .batch(BATCH_SIZE))\n    \n    test_pred_tta += model.predict(test_dataset_tta, verbose=1)\n    \nsubmission_df = pd.read_csv(SUB_PATH)\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = test_pred_tta / len(augmentation_sets)\nsubmission_df.to_csv('submission_tta_all.csv', index=False)\npd.Series(np.argmax(submission_df[labels].values,axis=1)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TTA simple\ntest_pred_tta = np.zeros((len(df_test), 4))\n# Choose what augmentations to use during predictions\naugmentation_sets = [\n    'vertical_flip',\n    'horizontal_flip',\n    'vh_flip',\n#     'brightness',\n#     'blur',\n#     'shift_scale_rotate'\n]\nfor i, set_name in enumerate(augmentation_sets):\n    _, albu_fn = get_augmentation_func(set_name)\n    test_dataset_tta = (\n        tf.data.Dataset.from_tensor_slices(preprocess(df_test, test=True))\n                       .map(decode_image, num_parallel_calls=AUTO)\n                       .map(albu_fn, num_parallel_calls=AUTO)    \n                       .batch(BATCH_SIZE))\n    \n    test_pred_tta += model.predict(test_dataset_tta, verbose=1)\n    \nsubmission_df = pd.read_csv(SUB_PATH)\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = test_pred_tta / len(augmentation_sets)\nsubmission_df.to_csv('submission_tta_simple.csv', index=False)\npd.Series(np.argmax(submission_df[labels].values,axis=1)).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Evaluate performance of model by plotting confusion matrix\n# from sklearn.metrics import confusion_matrix\n\n# # see http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n# import itertools\n\n# def accuracy(y, y_pred):\n#     return np.sum(y == y_pred)/len(y)\n\n# def plot_confusion_matrix(cm, labels=None, title='Confusion Matrix'):\n#     import plotly.figure_factory as ff\n\n#     x = labels\n#     y = x\n\n#     # change each element of z to type string for annotations\n#     z_text = [[str(y) for y in x] for x in cm]\n\n#     # set up figure \n#     fig = ff.create_annotated_heatmap(cm, x=x, y=y, annotation_text=z_text, colorscale='YlGnBu', showscale=True)\n\n#     # add title\n#     fig.update_layout(title_text=title,\n#                       #xaxis = dict(title='x'),\n#                       #yaxis = dict(title='x')\n#                      )\n\n#     # add custom xaxis title\n#     fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n#                             x=0.5,\n#                             y=-0.15,\n#                             showarrow=False,\n#                             text=\"Predicted value\",\n#                             xref=\"paper\",\n#                             yref=\"paper\"))\n\n#     # add custom yaxis title\n#     fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n#                             x=-0.35,\n#                             y=0.5,\n#                             showarrow=False,\n#                             text=\"Real value\",\n#                             textangle=-90,\n#                             xref=\"paper\",\n#                             yref=\"paper\"))\n\n#     # adjust margins to make room for yaxis title\n#     fig.update_layout(margin=dict(t=100, l=200), width=700, height=600)\n#     fig.show()\n    \n# # predict labels from validation set\n# y_pred = model.predict(val_gen)\n# # convert data to label number\n\n \n# y_true = np.argmax(Y_val.values, axis=1) \n\n# # compute the confusion matrix\n# cm = confusion_matrix(y_true, y_pred) \n\n# plot_confusion_matrix(cm, labels, title='Confusion_matrix Validation Set (acc={:.3f})'.format(accuracy(y_true, y_pred)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}