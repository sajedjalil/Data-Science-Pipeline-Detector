{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import everything ##"},{"metadata":{"_uuid":"b0365289-413a-4431-bf11-5c0370806c2e","_cell_guid":"cf2df754-1b3d-46dd-9e7a-32fa21c60a02","trusted":true},"cell_type":"code","source":"import mxnet as mx\nimport numpy as np\nimport pandas as pd\nimport os, random, sys, time, cv2\nimport matplotlib.pyplot as plt\nfrom mxnet import gluon, nd, image, init, autograd\nfrom mxnet.gluon import data as gdata, model_zoo, nn, loss as gloss, utils as gutils\nimport gluoncv\n\nmx.random.seed(1024)\nsize = 320\nbatch_size = 16\nepoch = 25\nnum_workers = 0 if sys.platform.startswith('win32') else 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load data ##"},{"metadata":{"_uuid":"0ae6da4f-0e37-4e9b-a5d9-1c12f751cefa","_cell_guid":"8ed3e9d5-5e87-4f32-8e8a-d8205abd8210","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\n\nimages = list(train['image_id'].values)\nlabels = list(train[['healthy', 'multiple_diseases', 'rust', 'scab']].values)\n\ntest_images = list(test['image_id'].values)\ntrain_dir = '../input/plant-pathology-2020-fgvc7/images'\n\nassert len(labels)==len(images), 'train_label, train_ids lenth different'\nsample_num = len(images)\nids = range(0, sample_num)\n\nval_percent = 0.1\nval_num = int(len(images)*val_percent)\nval_ids = random.sample(range(0, sample_num), val_num)\ntrain_ids = [id for id in ids if id not in val_ids]\nassert len(train_ids)==sample_num-val_num, 'num wrong!'\ntrain_num = len(train_ids)\n\nval_images, val_labels = [None] * val_num, [None] * val_num\ntrain_images , train_labels = [None] * train_num, [None] * train_num\n\nfor idx, value in enumerate(val_ids):\n    val_images[idx] = images[value]\n    val_labels[idx] = labels[value]\n    \nfor idx, value in enumerate(train_ids):\n    train_images[idx] = images[value]\n    train_labels[idx] = labels[value]\n\nprint('Train data: {}, val data: {}, test data: {}.'.format(len(train_images), len(val_images), len(test_images)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create dataset class ##"},{"metadata":{"_uuid":"12d2eb76-0d17-4464-902f-2cfe8acad8a3","_cell_guid":"80b4e904-de9e-42d3-bc7d-32a5670234b6","trusted":true},"cell_type":"code","source":"def read_images(root=train_dir, is_train=True):\n    if is_train==True:\n        data_images = train_images\n        data_labels = train_labels\n        data_num = len(data_images)\n        data_type = 'Train dataset'\n    elif is_train==False:\n        data_images = val_images\n        data_labels = val_labels\n        data_num = len(data_images)\n        data_type = 'Val dataset'\n    elif is_train=='Test':\n        data_images = test_images\n        data_num = len(data_images)\n        data_labels = [None] * data_num\n        data_type = 'Test dataset'\n    features, labels = nd.zeros(shape=(data_num, size, size, 3)), nd.zeros(shape=(data_num, 4))\n    for i in range(data_num):\n        features[i] = image.imresize(image.imread(os.path.join(root, data_images[i] + '.jpg')), size, size)\n        labels[i] = data_labels[i]\n    print('{} read finished'.format(data_type))\n    return features, labels\n\nclass PlantDataset(gdata.Dataset):\n    def __init__(self, is_train, train_dir):\n#         self.rgb_mean = nd.array([0.485, 0.456, 0.406])\n#         self.rgb_std = nd.array([0.229, 0.224, 0.225])\n        features, labels = read_images(root=train_dir, is_train=is_train)\n        self.features = [self.normalize_image(feature) for feature in features]\n        self.labels = labels\n        print('read ' + str(len(self.features)) + ' examples')\n\n    def normalize_image(self, img):\n#         return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std\n        return img.astype('float32') / 255 \n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n    def __len__(self):\n        return len(self.features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Augmention and create data iterator ##"},{"metadata":{"_uuid":"c4a22f05-e679-4244-a84c-2334a93de41c","_cell_guid":"673c4fef-7c8d-4897-b1fe-eb258d66fe13","trusted":true},"cell_type":"code","source":"plant_train = PlantDataset(True, train_dir)\nplant_val = PlantDataset(False, train_dir)\nplant_test = PlantDataset('Test', train_dir)\n\njitter_param = 0.4\nlighting_param = 0.1\nflip_aug = gdata.vision.transforms.Compose([\n    gdata.vision.transforms.RandomFlipLeftRight(),\n    gdata.vision.transforms.RandomFlipTopBottom(),\n    gdata.vision.transforms.RandomColorJitter(brightness=jitter_param, contrast=jitter_param,\n                                 saturation=jitter_param),\n    gdata.vision.transforms.RandomLighting(lighting_param),\n    gdata.vision.transforms.ToTensor(),\n    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\nno_aug = gdata.vision.transforms.Compose([\n    gdata.vision.transforms.ToTensor(),\n    gdata.vision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\ntrain_iter = gdata.DataLoader(plant_train.transform_first(flip_aug), batch_size, shuffle=True,\n                             num_workers=num_workers, last_batch='keep')\nval_iter = gdata.DataLoader(plant_val.transform_first(no_aug), batch_size,\n                             num_workers=num_workers, last_batch='keep')\ntest_iter = gdata.DataLoader(plant_test.transform_first(no_aug), batch_size,\n                             num_workers=num_workers, last_batch='keep')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define more function ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_net(netname, ctx):\n    if netname == 'densenet':\n        net = model_zoo.vision.densenet161(pretrained=True).features\n    elif netname == 'mobilenet':\n        net = model_zoo.vision.mobilenet_v2_1_0(pretrained=True).features\n    elif netname == 'seresnext':\n        net = gluoncv.model_zoo.get_model('SE_ResNext101_64x4d', pretrained=True).features\n    elif netname == 'resnext':\n        net = gluoncv.model_zoo.get_model('ResNext101_64x4d', pretrained=True).features[:-1]\n        net.add(CBAM(2048, 64))\n        net[-1].initialize(init.Xavier())\n    net.add(nn.Dense(512), nn.Dropout(0.25), nn.Dense(4))\n    net[-3:].initialize(init.Xavier())\n    net.collect_params().reset_ctx(ctx)\n    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 0.00002, 'wd': 1e-3})\n    net.hybridize()\n    return net, trainer\n\ndef cross_entropy(y_hat, y):\n    y_hat = y_hat.log()\n    return -y * y_hat\n\ndef softmax_to_onehot(x, axis=1, ctx=mx.gpu(0)):\n    a = nd.argmax(x, axis=1)\n    b = nd.zeros(shape=(x.shape[0], x.shape[1]), ctx=ctx)\n    b[nd.arange(len(a)), a] = 1.\n    return b\n    \ndef evaluate_accuracy(data_iter, net, ctx):\n    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    acc_sum, n = nd.array([0]), 0\n    for batch in data_iter:\n        features, labels = batch\n        if labels.dtype != features.dtype:\n            labels = labels.astype(features.dtype)\n        Xs = gutils.split_and_load(features, ctx)\n        ys = gutils.split_and_load(labels, ctx)\n        for X, y in zip(Xs, ys):\n            y = y.astype('float32')\n            acc_sum += (softmax_to_onehot(net(X), axis=1, ctx=ctx[0]) == y).sum().copyto(mx.cpu())\n            n += y.size\n        acc_sum.wait_to_read()\n    return acc_sum.asscalar() / n\n\ndef train(train_iter, val_iter, net, loss, trainer, ctx, num_epochs, verbose=0):\n    \"\"\"Train and evaluate a model.\"\"\"\n    print('training {} on {}'.format(net.name, ctx))\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n        for i, batch in enumerate(train_iter):\n            features, labels = batch\n            batch_size = features.shape[0]\n            if labels.dtype != features.dtype:\n                labels = labels.astype(features.dtype)\n            Xs = gutils.split_and_load(features, ctx)\n            ys = gutils.split_and_load(labels, ctx)\n            ls = []\n            with autograd.record():\n                y_hats = [nd.softmax(net(X)) for X in Xs]\n                ls = [cross_entropy(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n            for l in ls:\n                l.backward()\n            trainer.step(batch_size)\n            train_l_sum += sum([l.sum().asscalar() for l in ls])\n            n += sum([l.size for l in ls])\n            train_acc_sum += sum([(softmax_to_onehot(y_hat, axis=1, ctx=ctx[0]) == y).sum().asscalar()\n                                 for y_hat, y in zip(y_hats, ys)])\n            m += sum([y.size for y in ys])\n        test_acc = evaluate_accuracy(val_iter, net, ctx[0])\n        \n        if verbose == 0:\n            print('epoch %d/%d, loss %.4f, train acc %.3f, val acc %.3f, '\n                  'time %.1f sec'\n                  % (epoch + 1, num_epochs, train_l_sum / n, train_acc_sum / m, test_acc,\n                     time.time() - start))\n        else:\n            if (epoch + 1) % verbose == 0 or epoch == 0:\n                print('epoch %d/%d, loss %.4f, train acc %.3f, val acc %.3f, '\n                      'time %.1f sec'\n                      % (epoch + 1, num_epochs, train_l_sum / n, train_acc_sum / m, test_acc,\n                         time.time() - start))\n    print('Train finished')\n    return net","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define models"},{"metadata":{"trusted":true},"cell_type":"code","source":"ctx = mx.gpu(0)\ntrain_loss = gloss.SoftmaxCrossEntropyLoss(axis=1, from_logits=True)\nsub_dir = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MobileNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def use_mobile_net(epoch=30):\n    preds_mobile = []\n    net_mobile, trainer_mobile = get_net('mobilenet', ctx=mx.gpu(0))\n    net_mobile = train(train_iter, val_iter, net_mobile, train_loss, trainer_mobile, ctx, epoch, verbose=10)\n    for X, _ in test_iter:\n        y_hat_mobile = nd.softmax(net_mobile(X.as_in_context(ctx))).asnumpy()\n        preds_mobile.extend(y_hat_mobile)\n    sub_mobile = pd.read_csv(sub_dir)\n    sub_mobile.loc[:, 'healthy':] = preds_mobile\n    sub_mobile.to_csv('submission_mobile.csv', index=False)\n    sub_mobile.head(5)\n    return y_hat_mobile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SE_ResNext"},{"metadata":{"trusted":true},"cell_type":"code","source":"def use_seresnext(epoch=30):\n    preds_seresnext = []\n    net_seresnext, trainer_seresnext = get_net('seresnext', ctx=mx.gpu(0))\n    net_seresnext = train(train_iter, val_iter, net_seresnext, train_loss, trainer_seresnext, ctx, epoch, verbose=5)\n    for X, _ in test_iter:\n        y_hat_seresnext = nd.softmax(net_seresnext(X.as_in_context(ctx))).asnumpy()\n        preds_seresnext.extend(y_hat_seresnext)\n    sub_seresnext = pd.read_csv(sub_dir)\n    sub_seresnext.loc[:, 'healthy':] = preds_seresnext\n    sub_seresnext.to_csv('submission_seresnext.csv', index=False)\n    print(sub_seresnext.head(5))\n    return preds_seresnext","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DenseNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def use_densenet(epoch=30):\n    preds_dense = []\n    net_dense, trainer_dense = get_net('densenet', ctx=mx.gpu(0))\n    net_dense = train(train_iter, val_iter, net_dense, train_loss, trainer_dense, ctx, epoch, verbose=10)\n    for X, _ in test_iter:\n        y_hat_dense = nd.softmax(net_dense(X.as_in_context(ctx))).asnumpy()\n        preds_dense.extend(y_hat_dense)\n    sub_dense = pd.read_csv(sub_dir)\n    sub_dense.loc[:, 'healthy':] = preds_dense\n    sub_dense.to_csv('submission_dense.csv', index=False)\n    print(sub_dense.head(5))\n    return preds_dense","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ResNext_CBAM"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CAM(nn.HybridBlock):\n  def __init__(self, num_channels, ratio, **kwargs):\n    super(CAM, self).__init__(**kwargs)\n    with self.name_scope():\n      self.avg_pool = nn.GlobalAvgPool2D()\n      self.max_pool = nn.GlobalMaxPool2D()\n      self.conv1 = nn.Conv2D(num_channels // ratio, 1, use_bias=False)\n      self.conv2 = nn.Conv2D(num_channels, 1, use_bias=False)\n\n  def hybrid_forward(self, F, X):\n    X_avg = self.avg_pool(X)\n    X_avg = self.conv1(X_avg)\n    X_avg = F.relu(X_avg)\n    X_avg = self.conv2(X_avg)\n\n    X_max = self.max_pool(X)\n    X_max = self.conv1(X_max)\n    X_max = F.relu(X_max)\n    X_max = self.conv2(X_max)\n\n    Y = X_avg + X_max\n    Y = F.sigmoid(Y)\n    return Y\n\n\nclass SAM(nn.HybridBlock):\n  def __init__(self, kernel_size=7, **kwargs):\n    super(SAM, self).__init__(**kwargs)\n    with self.name_scope():\n      self.kernel_size = kernel_size\n      assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n      self.padding = 3 if self.kernel_size == 7 else 1\n\n      self.conv = nn.Conv2D(1, kernel_size=self.kernel_size, padding=self.padding, use_bias=False)\n\n  def hybrid_forward(self, F, X):\n    X_avg = F.mean(X, axis=1, keepdims=True)\n    X_max = F.max(X, axis=1, keepdims=True)\n    Y = F.concat(X_avg, X_max, dim=1)\n    Y = self.conv(Y)\n    Y = F.sigmoid(Y)\n    return Y\n\n\nclass CBAM(nn.HybridBlock):\n  def __init__(self, num_channels, ratio, **kwargs):\n    super(CBAM, self).__init__(**kwargs)\n    with self.name_scope():\n      self.num_channels = num_channels\n      self.ratio = ratio\n      self.cam = CAM(self.num_channels, self.ratio)\n      self.sam = SAM()\n\n  def hybrid_forward(self, F, X):\n    residual = X\n    Y = F.broadcast_mul(self.cam(X), X)\n    Y = F.broadcast_mul(self.sam(Y), Y)\n    Y = F.relu(Y)\n    return Y + residual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def use_resnet(epoch=30):\n    preds_res = []\n    net_res, trainer_res = get_net('resnext', ctx=mx.gpu(0))\n    net_res = train(train_iter, val_iter, net_res, train_loss, trainer_res, ctx, epoch, verbose=5)\n    for X, _ in test_iter:\n        y_hat_res = nd.softmax(net_res(X.as_in_context(ctx))).asnumpy()\n        preds_res.extend(y_hat_res)\n    sub_res = pd.read_csv(sub_dir)\n    sub_res.loc[:, 'healthy':] = preds_res\n    sub_res.to_csv('submission_res.csv', index=False)\n    print(sub_res.head(5))\n    return preds_res","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds_mobile = use_mobile_net(epoch)\n# preds_res = use_resnet(epoch)\npreds_seresnext = use_seresnext(epoch)\npreds_dense = use_densenet(epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result ensemble ##"},{"metadata":{},"cell_type":"markdown","source":"### Method 1 ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def weighted_ensemble(preds_a, preds_b):\n    sub1, sub2, sub3 = pd.read_csv(sub_dir), pd.read_csv(sub_dir), pd.read_csv(sub_dir)\n    pred_1, pred_2, pred_3 = [], [], []\n    for a, b in zip(preds_a, preds_b):\n        pred_1.append(a * 0.25 + b * 0.75)\n        pred_2.append(a * 0.5 + b * 0.5)\n        pred_3.append(a * 0.75 + b * 0.25)\n\n    sub1.loc[:, 'healthy':] = pred_1\n    sub2.loc[:, 'healthy':] = pred_2\n    sub3.loc[:, 'healthy':] = pred_3\n    sub1.to_csv('submission1.csv', index=False)\n    sub2.to_csv('submission2.csv', index=False)\n    sub3.to_csv('submission3.csv', index=False)\n\nweighted_ensemble(preds_seresnext, preds_dense)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method 2 ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def average_ensemble_three(preds_a, preds_b, preds_c):\n    sub = pd.read_csv(sub_dir)\n    pred = []\n    for a, b, c in zip(preds_a, preds_b, preds_c):\n        pred.append((a + b + c) / 3.)\n    sub.loc[:, 'healthy':] = pred\n    sub.to_csv('submission.csv', index=False)\n    print(sub.head())\n\ndef average_ensemble_two(preds_a, preds_b):\n    sub = pd.read_csv(sub_dir)\n    pred = []\n    for a, b in zip(preds_a, preds_b):\n        pred.append((a + b) / 2.)\n    sub.loc[:, 'healthy':] = pred\n    sub.to_csv('submission.csv', index=False)\n    print(sub.head(5))\n    \n# average_ensemble_three(preds_res, preds_seresnext, preds_dense)\n# average_ensemble_two(preds_seresnext, preds_dense)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}