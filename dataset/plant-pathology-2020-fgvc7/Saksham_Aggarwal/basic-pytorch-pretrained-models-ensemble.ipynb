{"cells":[{"metadata":{},"cell_type":"markdown","source":"* **MODELS: RESNEXT, RESNET, EFFICIENTNET, DENSENET and variations**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**code for ENSEMBLE of different models' output is given at last **","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**PLEASE UPVOTE IF YOU FIND THIS KERNEL HELPFUL**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nimport time\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\n\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_csv_path = '/kaggle/input/plant-pathology-2020-fgvc7/train.csv'\ntest_csv_path = '/kaggle/input/plant-pathology-2020-fgvc7/test.csv'\nimages_dir = '/kaggle/input/plant-pathology-2020-fgvc7/images/'\nsubmission_df_path = '/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install efficientnet_pytorch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"model_name = 'densenet'\nnum_classes = 4\nbatch_size = 8\nnum_epochs = 30 #4\n# set val_split = 0 for no validation\nval_split = 0.2 #0.0\n# if dev_mode = True loads only a few samples\ndev_mode = False\nnum_dev_samples = 0\n# feature_extract = False   ==> fine-tune the whole model \n# feature_extract = True    ==> only update the reshaped layer parameters\nfeature_extract = False\npre_trained = True\nnum_cv_folds = 4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nDefine Transforms\nDefine Dataset Class\n'''\n\nclass ppDataset(Dataset):\n    def __init__(self, df, image_dir, return_labels=False, transforms=None):\n        self.df = df\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.return_labels = return_labels\n        self.label_map = {'healthy':0, 'multiple_diseases':1, 'rust':2, 'scab':3}\n        self.label_map_reverse = {v:k for k,v in self.label_map.items()}\n        \n    def __len__(self):\n        return self.df.__len__()\n    \n    def __getitem__(self, idx):\n        image_path = self.image_dir + self.df.loc[idx, 'image_id'] + '.jpg'\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transforms:\n            image = self.transforms(image)\n\n        if self.return_labels:\n            # label = torch.tensor(self.df.loc[idx, ['healthy', 'multiple_diseases', 'rust', 'scab']]).unsqueeze(1)\n            label = torch.tensor(self.df.loc[idx, ['healthy', 'multiple_diseases', 'rust', 'scab']]).unsqueeze(-1)\n            return image, label, self.label_map_reverse[label.squeeze(1).numpy().argmax()]\n        else:\n            return image\n\n\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((256, 256)),\n                \n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.5),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.05, hue=0.05),\n        transforms.RandomAffine(degrees=[0,45]),\n        transforms.CenterCrop(564),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]),\n    'val': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]),\n    'test': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For different versions of efficientnet simply replace 'efficientnet-b5' to 'efficientnet-bN' where N can be 0,1,2,3,4,5,6,7,8\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef train_model(model, datasets_dict, criterion, optimizer, batch_size, num_epochs = 25, lr_scheduler=None):\n    \n    since = time.time()\n    \n    device = get_device()\n    if device.type != 'cpu':\n        model.cuda()\n    model.to(device)\n    \n    do_validation = (datasets_dict.get('val') != None)\n    \n    train_acc_hist = []\n    train_loss_hist = []\n\n    if do_validation:\n        val_acc_history = []\n        val_loss_history = []\n        val_f1_history = []\n        val_dataloader = DataLoader(datasets_dict['val'], batch_size=batch_size, shuffle=True)\n        print('Validating on {} samples.'.format(datasets_dict['val'].__len__()))\n        best_model_wts = copy.deepcopy(model.state_dict())\n        best_acc = 0.0\n\n\n    for epoch in range(num_epochs):\n\n        tr_dataloader = DataLoader(datasets_dict['train'], batch_size=batch_size, shuffle=True, num_workers=4)\n\n        if do_validation:\n            val_preds = []\n            val_labels = []\n            phases = ['train', 'val']\n            dataloaders = {'train' : tr_dataloader, 'val' : val_dataloader}\n        else:\n            phases = ['train']\n            dataloaders = {'train' : tr_dataloader}\n\n\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n        print('-'*10)\n\n        for phase in phases:\n            if phase == 'train':    model.train()\n            elif phase == 'val':    model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels, _ in dataloaders[phase]:\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                labels = labels.squeeze(-1)\n                \n                optimizer.zero_grad()\n                model.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs, 1)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        if lr_scheduler:\n                            lr_scheduler.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(labels.argmax(dim=1) == outputs.argmax(dim=1))\n                if do_validation:\n                    val_preds.append(preds.cpu())\n                    val_labels.append(labels.argmax(dim=1).cpu())\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                val_loss_history.append(epoch_loss)\n            if phase == 'train':\n                train_acc_hist.append(epoch_acc)\n                train_loss_hist.append(epoch_loss)\n\n        if do_validation:\n            val_f1 = get_f1(val_preds, val_labels)\n            print(\"val F1 : \", val_f1)\n            val_f1_history.append(val_f1)\n\n        print()\n    \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    if do_validation:\n        print('Best val Acc: {:4f}'.format(best_acc))\n        model.load_state_dict(best_model_wts)\n        metrics = {'val_acc_hist':val_acc_history, 'val_f1_hist':val_f1_history, 'val_loss_hist':val_loss_history, 'train_acc_hist':train_acc_hist, 'train_loss_hist':train_loss_hist}\n    else:\n        metrics = {'train_acc_hist':train_acc_hist, 'train_loss_hist':train_loss_hist}\n    \n    return model, metrics\n\n\nclass DenseCrossEntropy(nn.Module):\n    def __init__(self):\n        super(DenseCrossEntropy, self).__init__()        \n    def forward(self, logits, labels):\n        logits = logits.float()\n        labels = labels.float()\n        logprobs = F.log_softmax(logits, dim=-1)\n        loss = -labels * logprobs\n        loss = loss.sum(-1)\n        return loss.mean()\n\n\ndef set_parameters_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\ndef get_device():\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n        print('We will use the GPU:', torch.cuda.get_device_name(0))\n    else:\n        print('No GPU available, using the CPU instead.')\n        device = torch.device(\"cpu\")\n    return device\n            \ndef initialize_model(model_name, num_classes, features_extract, use_pretrained):\n    model_ft = None\n    input_size = 0\n    valid_model_names = ['efficientnet','resnet101', 'resnet152', 'resnet50', 'resnext','densenet']\n    \n    if model_name not in valid_model_names:\n        print('Invalid model name, exiting. . .') \n        exit();\n    elif model_name=='densenet':\n        model_ft=models.densenet161(pretrained=use_pretrained)\n        set_parameters_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier =nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n    elif model_name=='efficientnet':\n        model_ft=EfficientNet.from_pretrained('efficientnet-b5')  \n        set_parameters_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft._fc.in_features\n        model_ft._fc =nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n    elif model_name == 'resnext':\n        model_ft = models.resnext101_32x8d(pretrained=use_pretrained)\n        set_parameters_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc =nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n    elif model_name == 'resnet152':\n        model_ft = models.resnet152(pretrained=use_pretrained)\n        set_parameters_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc =nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n    elif model_name == 'resnet50':\n        model_ft = models.resnet50(pretrained=use_pretrained)\n        set_parameters_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc =nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n    elif model_name == 'resnet101':\n        model_ft = models.resnet101(pretrained=use_pretrained)\n        set_parameters_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc =nn.Sequential(nn.Linear(num_ftrs,1000,bias=True),\n                          nn.ReLU(),\n                          nn.Dropout(p=0.5),\n                          nn.Linear(1000,4, bias = True))\n\n    \n    input_size = 224\n\n    return model_ft, input_size\n\n\ndef get_f1(val_preds, val_labels):\n    VP = []\n    VL = []\n    for l in [list(x.numpy()) for x in val_preds]: VP.extend(l)\n    for l in [list(x.numpy()) for x in val_labels]: VL.extend(l)\n    return f1_score(VL, VP, average='weighted')\n\n\ndef get_params_to_update(model, feature_extract):\n    if feature_extract:\n        params_to_update = []\n        for name,param in model.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n    else:\n        params_to_update = model.parameters()    \n    return params_to_update\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import KFold\n\n\ndef train_k_fold(num_cv_folds, dataframe_path, batch_size_kf = batch_size, num_epochs_kf = num_epochs):\n    \n    # read train dataframe\n    criterion_kf = DenseCrossEntropy()\n    dataframe = pd.read_csv(dataframe_path)\n    \n    if dev_mode:\n        dataframe = dataframe.sample(num_dev_samples)\n    \n    dataframe.reset_index(drop=True, inplace=True)\n    \n    kf = KFold(n_splits = num_cv_folds, shuffle=True)\n    hist_folds = []\n    for fold_idx,(tr_idx, val_idx) in enumerate(kf.split(dataframe)):\n        print('Fold {} of {}. . .'.format(fold_idx+1, num_cv_folds ))\n        \n        tr_df_kf = dataframe.loc[tr_idx]\n        val_df_kf = dataframe.loc[val_idx]        \n        \n        tr_df_kf.reset_index(drop=True, inplace=True)\n        val_df_kf.reset_index(drop=True, inplace=True)   \n        \n        tr_dataset_kf = ppDataset(tr_df_kf, images_dir, return_labels = True, transforms = data_transforms['train'])\n        val_dataset_kf = ppDataset(val_df_kf, images_dir, return_labels = True, transforms = data_transforms['val'])\n        datasets_dict_kf = {'train' : tr_dataset_kf, 'val' : val_dataset_kf }\n        \n        model_kf, _ = initialize_model(model_name, num_classes, feature_extract, use_pretrained=pre_trained)\n        optimizer_kf = torch.optim.AdamW(get_params_to_update(model_kf, feature_extract), lr = 2e-5, eps = 1e-8 )\n    \n        model_kf, hist_kf = train_model(model_kf, datasets_dict_kf, criterion_kf,\\\n                                       optimizer_kf, batch_size_kf, num_epochs=3, lr_scheduler=None)\n        hist_folds.append(hist_kf)\n        \n    return hist_folds\n        \n    \n\nif num_cv_folds > 0:\n    hist_folds = train_k_fold(num_cv_folds, train_csv_path)\n    print('{} fold validation accuracy: {}'.format(num_cv_folds, float(sum([x['val_acc_hist'][-1] for x in hist_folds])) / len(hist_folds)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=pre_trained)\n\n\nparams_to_update = get_params_to_update(model_ft, feature_extract)\n\n# optimizer_ft = torch.optim.SGD(params_to_update, lr=0.001, momentum=0.9)\noptimizer_ft = torch.optim.AdamW(params_to_update, lr = 0.0001, eps = 1e-8 )\n\n# lr_scheduler    =   torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size = 1, gamma = 0.6)\n\n#optimizer_ft = torch.optim.Adam(params_to_update, lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.000001, amsgrad=True)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', factor=0.5, patience=1, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n\n\ncriterion = DenseCrossEntropy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nRUN VALIDATION ON val_split amount of data\nIf val_split = 0, return a model traine\nd on all data\n'''\n\n# read dataFrames\nif dev_mode:\n    # tr_df = pd.read_csv(train_csv_path, nrows=num_dev_samples)\n    tr_df = pd.read_csv(train_csv_path).sample(num_dev_samples)\n    tr_df.reset_index(drop=True, inplace=True)\nelse:\n    tr_df = pd.read_csv(train_csv_path)\n\n\nif val_split > 0:\n    tr_df, val_df = train_test_split(tr_df, test_size = val_split)\n    val_df = val_df.reset_index(drop=True)\n    tr_df = tr_df.reset_index(drop=True)\n    \n\nte_df = pd.read_csv(test_csv_path)\n\n\n\n# create Dataset objects\ntr_dataset = ppDataset(tr_df, images_dir, return_labels = True, transforms = data_transforms['train'])\n\nte_dataset = ppDataset(te_df, images_dir, return_labels = False, transforms = data_transforms['test'])\n\n\nif val_split > 0:\n    val_dataset = ppDataset(val_df, images_dir, return_labels = True, transforms = data_transforms['val'])\n    datasets_dict = {'train' : tr_dataset, 'test' : te_dataset, 'val' : val_dataset }\nelse:\n    datasets_dict = {'train' : tr_dataset, 'test' : te_dataset,}\n\n\n# run the training loop\nmodel_ft, hist = train_model(model_ft, datasets_dict, criterion, optimizer_ft, batch_size, num_epochs=num_epochs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nPLOT train/val accuracy/loss\n'''\nif val_split > 0:\n    plt.grid()\n    plt.xlabel('epochs')\n    num_epochs = hist['train_acc_hist'].__len__()\n    plt.plot(range(1,num_epochs+1), hist['train_acc_hist'])\n    # plt.plot(range(1,num_epochs+1), hist['train_loss_hist'])\n    if hist.get('val_acc_hist'):\n        plt.plot(range(1,num_epochs+1), hist['val_acc_hist'])\n        plt.plot(range(1,num_epochs+1), hist['val_f1_hist'])\n        plt.legend(['train acc', 'val acc', 'val F1'])\n    else:\n        plt.legend(['train acc'])\n    plt.show()\n\n    plt.grid()\n    plt.xlabel('epochs')\n    num_epochs = hist['train_loss_hist'].__len__()\n    plt.plot(range(1,num_epochs+1), hist['train_loss_hist'])\n    # plt.plot(range(1,num_epochs+1), hist['train_loss_hist'])\n    if hist.get('val_loss_hist'):\n        plt.plot(range(1,num_epochs+1), hist['val_loss_hist'])\n        plt.legend(['train loss', 'val loss'])\n    else:\n        plt.legend(['train loss'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nGENERATE PREDICTIONS\n'''\n\nprint('Generating predictions for {} samples'.format(te_dataset.__len__()))\n\nte_dataloader = DataLoader(te_dataset, batch_size=batch_size, shuffle=False)\nsubmission_df = pd.read_csv(submission_df_path)\n\n\ndevice = get_device()\n\nmodel_ft.eval()\nmodel_ft.to(device)\ntest_preds = None\n\nfor inputs in tqdm(te_dataloader):\n\n    inputs = inputs.to(device)\n\n    with torch.no_grad():\n        outputs = model_ft(inputs)\n\n        if test_preds is None:\n            test_preds = outputs.data.cpu()\n        else:\n            test_preds = torch.cat((test_preds, outputs.data.cpu()), dim=0)\n    \ntest_preds = torch.softmax(test_preds, dim=1, dtype=float)\n\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = test_preds\n\nsubmission_df.to_csv('submission_5.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ENSEMBLE**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Save your input files in plantpathology1234 and run the following code by uncommenting it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#sub1=pd.read_csv('/kaggle/input/plantpathology1234/submission_1.csv')\n#sub2=pd.read_csv('/kaggle/input/plantpathology1234/submission_2.csv')\n#sub3=pd.read_csv('/kaggle/input/plantpathology1234/submission_3.csv')\n#sub4=pd.read_csv('/kaggle/input/plantpathology1234/submission_4.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sub=sub1\n#sub.healthy=(sub1.healthy+sub2.healthy+sub3.healthy+sub4.healthy)/4\n#sub.multiple_diseases=(sub1.multiple_diseases+sub2.multiple_diseases+sub3.multiple_diseases+sub4.multiple_diseases)/4\n#sub.rust=(sub1.rust+sub2.rust+sub3.rust+sub4.rust)/4\n#sub.scab=(sub1.scab+sub2.scab+sub3.scab+sub4.scab)/4\n#sub.to_csv('ensemble1234.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}