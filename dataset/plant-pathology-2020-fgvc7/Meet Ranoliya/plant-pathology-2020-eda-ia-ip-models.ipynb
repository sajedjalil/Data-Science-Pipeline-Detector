{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"![Tree](https://media.giphy.com/media/oNTQZNB67kMf5VHiCj/giphy.gif)"},{"metadata":{},"cell_type":"markdown","source":"Misdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection."},{"metadata":{},"cell_type":"markdown","source":"### Specific Objectives\n\nObjectives of this notebook is to train a model using images of training dataset to:\n\n1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf  \n2) Accurately distinguish between many diseases, sometimes more than one on a single leaf.  \n3) Deal with rare classes and novel symptoms.  \n4) Address depth perception—angle, light, shade, physiological age of the leaf.  \n5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning.  \n\n\n[Kaggle Competition Link](https://www.kaggle.com/c/plant-pathology-2020-fgvc7)"},{"metadata":{},"cell_type":"markdown","source":"Tools that I have used in this kernel:\n* EDA: **[Plotly](https://plot.ly/)** & **Matplotlib**\n* Image Augmentation: **Tensorflow-Keras**\n* Image Processing: **Open-CV** & **Scikit-image**\n* Model Training: **Tensorflow-Keras**\n\n<h3>\n<font color='red'>\n    <strong>Please!!! Upvote this kernel if you find it useful.</strong>\n</font>\n</h3>"},{"metadata":{},"cell_type":"markdown","source":"## Importing necessary libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nimport cv2 as cv\nfrom skimage import filters\nfrom skimage import morphology\n\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt\n\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nprint(f\"Tensorflow version: {tf.__version__}\")\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nSEED = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"INPUT_PATH = '/kaggle/input/plant-pathology-2020-fgvc7/'\nIMG_PATH = INPUT_PATH + 'images/'\nTRAIN_DATA = INPUT_PATH + 'train.csv'\nTEST_DATA = INPUT_PATH + 'test.csv'\nSAMPLE_SUB = INPUT_PATH + 'sample_submission.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA)\ntest_df = pd.read_csv(TEST_DATA)\nsampleSubmission_df = pd.read_csv(SAMPLE_SUB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA_IMG_SHAPE = (512,256)\n\ndef getImage(image_id,SHAPE=EDA_IMG_SHAPE):\n    img = cv.imread(IMG_PATH + image_id + '.jpg')\n    img = cv.resize(img,SHAPE)\n    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n    \n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nhealthy = [getImage(image_id) for image_id in train_df[train_df['healthy']==1].iloc[:,0]]\n\nmultiple_diseases = [getImage(image_id) for image_id in train_df[train_df['multiple_diseases']==1].iloc[:,0]]\n\nrust = [getImage(image_id) for image_id in train_df[train_df['rust']==1].iloc[:,0]]\n\nscab = [getImage(image_id) for image_id in train_df[train_df['scab']==1].iloc[:,0]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = {'healthy':healthy, 'multiple_diseases':multiple_diseases, 'rust':rust, 'scab': scab} ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotlyDataFrame(df,title):\n    \n    fig = go.Figure(data=[go.Table(\n    header = dict(values = df.columns),\n    cells = dict(values = [df[col] for col in df.columns]))])\n    \n    fig.update_layout(\n        title = title)\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotlyDataFrame(train_df.iloc[:15,:],'Train Data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotlyDataFrame(test_df.iloc[:15,:],'Test Data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sample Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotlyDataFrame(sampleSubmission_df.iloc[:15,:], 'Sample Submission')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Pie(labels=train_df.columns[1:],\n                             values=[np.sum(train_df[col]) for col in train_df.columns[1:]])])\n\nfig.update_traces(hoverinfo='label+percent',\n                  textinfo='value',\n                  textfont_size=20,\n                  marker=dict(line=dict(color='#000000', width=2)))\n\nfig.update_layout(title_text=\"Target Distribution of Training-Data \")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have relatively high number of images for single diseases than multiple diseases\n* We can increase the number of images for multiple diseases using various Image-Augmentation Techniques (ex. flipping, rotation etc)"},{"metadata":{},"cell_type":"markdown","source":"### Parallel Categorical Plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Parcats(dimensions=[dict(values=train_df[col],label=col) for col in train_df.columns[1:]],\n                          line={'color':train_df.healthy, 'colorscale':[[0,'red'],[1,'green']]}))\n\nfig.update_layout(title='Parallel Categorical Plot')\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There is no overlapping between two classes (ex. [1,1,0,0] etc)"},{"metadata":{},"cell_type":"markdown","source":"### Channel Distributions"},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing One Image and its Channel Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"img = cv.imread(IMG_PATH + random.choice(train_df.iloc[:,0]) + '.jpg')\nimg = cv.resize(img,(256,128))\nimg = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n\n\nfig = make_subplots(2,1)\n\nfig.add_trace(go.Image(z=img),1,1)\n\nfor channel, color in enumerate(['red','green','blue']):\n    fig.add_trace(go.Histogram(x=img[:,:,channel].ravel(),\n                               opacity=0.5,\n                               marker_color=color,\n                               name=f'{color} channel'),2,1)\n\nfig.update_layout(title='Image & its Channel Distribution')\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see channel distribution of the image. Green channel has higher values because we know this image is of a green leaf."},{"metadata":{},"cell_type":"markdown","source":"#### Healthy Leaves Image Visualization and Channel Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def displayImages(condition='healthy'):\n\n#     fig = make_subplots(3,3,horizontal_spacing=0.01,vertical_spacing=0.05)\n\n#     for i in range(9):\n#         image = random.choice(classes[condition])\n#         fig.add_trace(go.Image(z=image),i//3 + 1,i%3 + 1)\n\n#     fig.update_layout(title = f'{condition.capitalize()} Leaves Images',height=128*3 + 50,width=256*3 + 50)\n    \n#     fig.update_xaxes(showticklabels=False)\n#     fig.update_yaxes(showticklabels=False)\n    \n#     fig.show()\n\n    fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(20, 10))\n\n    for i in range(9):\n        \n        image = random.choice(classes[condition])\n        \n        ax[i//3,i%3].imshow(image)\n        \n    fig.suptitle(f'{condition.capitalize()} Class Leaves',fontsize=20)\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"displayImages('healthy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see all the leaves are totally green without any kind of spots on it\n* Maybe we could expect high values for the green channels in healthy leaves images & relatively low values for both green and red channels\n* We will later on compare it with the other classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['rgb(200, 0, 0)', 'rgb(0, 200, 0)', 'rgb(0,0,200)']\n\ndef plotChannelDistribution(condition):\n    \n    distributions = []\n        \n    for channel in range(3):\n        distributions.append([np.mean(img[:,:,channel]) for img in classes[condition]])\n    \n    fig = ff.create_distplot(distributions,\n                            group_labels=['red','green','blue'],\n                            colors=colors)\n    \n    fig.update_layout(title=f'{condition.capitalize()} leaves channel distribution')\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelDistribution('healthy')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Green channel distribution has high values for healthy leaves (Mean~132)\n* Red channel has Mean near ~90 & Blue has Mean near ~75\n* Blue channel distribution is more flattened and has less height "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### Rust Leaves Image Visualization and Channel Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"displayImages('rust')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see **Brownish-Yellow** spots on the leaves\n* Let's see the channel distribution to get the idea of how it is different from the healthy leaves"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelDistribution('rust')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scab Leaves Image Visualization and Channel Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"displayImages('scab')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see Dark Brown & Black spots on the leaves\n* These spots have high blue value than the green part"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelDistribution('scab')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the distributions are equal but are shifted by some amount\n* All the distributions are **Normal distribution** "},{"metadata":{},"cell_type":"markdown","source":"#### Multiple Diseases Leaves Image Visualization and Channel Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"displayImages('multiple_diseases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* These leaves contain more than one diseases as we can see different types of spots on a single leaf.\n* Some of the leaves are also eaten up\n* It could be hard to classify these leaves correctly as we can see some of the leaves have very few spots "},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelDistribution('multiple_diseases')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Channel wise comparision of different classes of leaves"},{"metadata":{},"cell_type":"markdown","source":"##### Red Channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"channelDict = {'red':0,'green':1,'blue':2}\n\ngroup_labels=[train_df.columns[i] for i in range(1,5)]\n\ncolors_cw = {'red':['rgb(250,0,0)','rgb(190,0,0)','rgb(130,0,0)','rgb(50,0,0)'],\n         'green':['rgb(0,250,0)','rgb(0,190,0)','rgb(0,130,0)','rgb(0,50,0)'],\n         'blue':['rgb(0,0,250)','rgb(0,0,190)','rgb(0,0,130)','rgb(0,0,50)']}\n\ndef plotChannelWiseDistribution(channel):\n    \n    distributions = []\n    \n    for c in [healthy, multiple_diseases, rust, scab]:\n        distributions.append([np.mean(img[:,:,channelDict[channel]]) for img in c])\n    \n    fig = ff.create_distplot(distributions,\n                            group_labels=group_labels,\n                            colors=colors_cw[channel],\n                            show_hist=False)\n    \n    fig.update_layout(title=f'{channel.capitalize()} channel distribution for all Classes')\n    \n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelWiseDistribution('red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Densities are different for different classes of the leaves but the mean value is nearly same (90-100) for all the classes. Scab & Rust classes have nearly identical Red-channel distribution. Hence we can say that red-channel might not be useful to classify between these two diseases."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"##### Green Channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelWiseDistribution('green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Green channel distributions are also nearly equal and this channel might not be useful for classification.\nHere we can see distributions of Scab & Rust diseases are some what different from each other and could be useful for classification."},{"metadata":{},"cell_type":"markdown","source":"##### Blue Channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotChannelWiseDistribution('blue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the distribution are very different from each others. This indicates that Blue-channel has some important information about the class of the leaves. This might be useful for classification."},{"metadata":{},"cell_type":"markdown","source":"## Image Augmentation\n\nImage data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n\nTraining deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nTransforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more."},{"metadata":{},"cell_type":"markdown","source":"### Sample Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getRandomImage():\n    return random.choice(classes[random.choice(train_df.columns[2:])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = getRandomImage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Image(z=img))\n\nfig.update_layout(title_text=\"Smaple Image\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Augmentation With ImageDataGenerator"},{"metadata":{},"cell_type":"markdown","source":"We can use this to generate augmented images using different transformations like:\n* Vertical Flipping\n* Horizontal Flipping\n* Shifted Images\n* Rotated Images\n* Zoomed Images\n* Images with different Brightness levels etc.\n\nPlus point of using this is that it will generate images in runtime (While training a model). It means we are not supposed to store this augmented images."},{"metadata":{"trusted":true},"cell_type":"code","source":"img = np.expand_dims(img,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ngenerator = tf.keras.preprocessing.image.ImageDataGenerator(vertical_flip=True,\n                                                            horizontal_flip=True,\n                                                            brightness_range=[0.5,1.5],\n                                                            zoom_range=[0.5,1.1])\n\niterator = generator.flow(img,batch_size=1)\n\n# fig = make_subplots(10,3,horizontal_spacing=0.01,vertical_spacing=0.01)\n\n# for i in range(30):\n#     image = iterator.next()[0].astype('uint8')\n    \n#     fig.add_trace(go.Image(z=image),i//3 + 1,i%3 + 1)\n\n# fig.update_layout(title_text=\"Augmented Images of the sample image\",\n#                  height=128*10 + 20,\n#                  width=256*3 + 20)\n\n# fig.update_xaxes(showticklabels=False)\n# fig.update_yaxes(showticklabels=False)\n\n# fig.show()\n\nfig, ax = plt.subplots(nrows=5, ncols=3, figsize=(15,10))\n\nax[0,0].imshow(img[0])\nax[0,0].set_title(\"Sample Image\",fontsize=10)\nax[0,0].set_xticks([])\nax[0,0].set_yticks([])\n\n\nfor i in range(1,15):\n    \n    image = iterator.next()[0].astype('uint8')\n    \n    ax[i//3,i%3].imshow(image)\n    ax[i//3,i%3].set_xticks([])\n    ax[i//3,i%3].set_yticks([])\n\nfig.suptitle(\"Augmented Images of the sample image\",fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generator** will keep on generating randomly augmented images (it is a never ending loop). That's why I have used it to generate only 15 Augmented images."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Image Processing"},{"metadata":{},"cell_type":"markdown","source":"I'll use **Open-CV** for **image processing**. Image processing can help us enhance our Classification-Model.  \nIn our case we just want the affected leaf hence I'll try to separate out that leaf from the unnecessary background. We can use **image-segmentation** techniques to do this."},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleImg = getRandomImage()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convertToHSV(img):\n    return cv.cvtColor(img,cv.COLOR_RGB2HSV_FULL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mask = np.zeros(sampleImg.shape[:2],np.uint8)\n\n# bgdModel = np.zeros((1,65),np.float64)\n# fgdModel = np.zeros((1,65),np.float64)\n\n# rect = (0,0,520,255)\n# cv.grabCut(sampleImg,mask,rect,bgdModel,fgdModel,20,cv.GC_INIT_WITH_RECT)\n\n# mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n# segImg = sampleImg*mask2[:,:,np.newaxis]\n\n# px.imshow(segImg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ROI (Region Of Interest) Selection\n\nTo select the **ROI**, I have used [Canny edge detection](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html#canny) to detect the edges of the leaves and to find the edges of the **rectangle ROI**, I have used this: **getROI** method."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getROI(img):\n    # convert the image to the gray-scale image\n    gray = cv.cvtColor(img,cv.COLOR_RGB2GRAY)\n    \n    # Detect the edges in the image using canny edge detection\n    edged = cv.Canny(gray,150,200)\n    \n    xm = img.shape[1]//2    # Middle coordinate of the x-axis (width of the image)\n    ym = img.shape[0]//2    # Middle coordinate of the y-axis (height of the image)\n    \n    # to find the bottom-y coordinate to the Rectangle-ROI\n    for i in range(img.shape[0]-1,-1,-1):\n        if np.sum(edged[i,xm-5:xm+5])!=0:\n            y_bottom = np.where(i+10<img.shape[0]-1,i+10,img.shape[0]-2)\n            break\n            \n    # to find the top-y coordinate to the Rectangle-ROI\n    for i in range(img.shape[0]):\n        if np.sum(edged[i,xm-5:xm+5])!=0:\n            y_top = np.where(i-10>1,i-10,2)\n            break\n    \n    # to find the top-x coordinate to the Rectangle-ROI\n    for i in range(img.shape[1]):\n        if np.sum(edged[ym-5:ym+5,i])!=0:\n            x_top = np.where(i-10>1,i-10,2)\n            break\n            \n    # to find the bottom-x coordinate to the Rectangle-ROI\n    for i in range(img.shape[1]-1,-1,-1):\n        if np.sum(edged[ym-5:ym+5,i])!=0:\n            x_bottom = np.where(i+10<img.shape[1]-1,i+10,img.shape[1]-2)\n            break\n\n    return edged,(x_top,y_top,x_bottom,y_bottom)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(20, 13))\n\nfor i in range(3):\n    orignal = getRandomImage()\n    edged, coordinates = getROI(orignal)\n    \n    roi = orignal.copy()\n    \n    (x_top,y_top,x_bottom,y_bottom) = coordinates\n    \n    roi[y_top-2:y_top,x_top:x_bottom+1] = [255,0,0]        # Top-edge\n    roi[y_bottom:y_bottom+2,x_top:x_bottom+1] = [255,0,0]  # Bottom-edge\n    roi[y_top:y_bottom+1,x_top-2:x_top] = [255,0,0]        # Left-edge\n    roi[y_top:y_bottom+1,x_bottom:x_bottom+2] = [255,0,0]  # Right-edge\n    \n    ax[i,0].imshow(orignal)\n    ax[i,0].set_title('Original Image', fontsize=15)\n    ax[i,1].imshow(edged, cmap='gray')\n    ax[i,1].set_title('Detected Edges', fontsize=15)\n    ax[i,2].imshow(roi)\n    ax[i,2].set_title('ROI', fontsize=15)\n    \nfig.suptitle(\"ROI selection using Canny Edge Detection\",fontsize=20)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This method will only work if the target leaf is in the *middle area* of the image & the images are of *good quality*. There could be many cases when it will not give the desired outputs.   \nAlso, we can see it is not giving accurate results for many of the images & we will not get a **good quality** of images every time in **real-world** scenarios. Hence, I'll try some other methods to accurately get the ROI, not as a rectangle but as the shape of the target leaf."},{"metadata":{},"cell_type":"markdown","source":"### ROI selection using Watershed Transformation\n\nAny grayscale image can be viewed as a topographic surface where high intensity denotes peaks and hills while low intensity denotes valleys. You start filling every isolated valleys (local minima) with different colored water (labels). As the water rises, depending on the peaks (gradients) nearby, water from different valleys, obviously with different colors will start to merge. To avoid that, you build barriers in the locations where water merges. You continue the work of filling water and building barriers until all the peaks are under water. Then the barriers you created gives you the segmentation result. This is the “philosophy” behind the watershed. \n\nYou can visit the [CMM](http://www.cmm.mines-paristech.fr/~beucher/wtshed.html) webpage on watershed to understand it with the help of some animations.\n\n[Useful Material](https://flothesof.github.io/removing-background-scikit-image.html) "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfig, ax = plt.subplots(nrows=6, ncols=3, figsize=(15,15))\n\nfor i in range(3):\n    orignal = getRandomImage()\n#     blur = cv.bilateralFilter(orignal,9,75,75)\n    \n    gray = cv.cvtColor(orignal,cv.COLOR_RGB2GRAY)\n    sobel = filters.sobel(gray)\n    \n#     sobel = cv.morphologyEx(sobel, cv.MORPH_OPEN, kernel)\n#     blurred = cv.bilateralFilter(sobel.astype('float32'),9,75,75)\n    blurred = filters.gaussian(sobel, sigma=2.0)\n    \n    ym = blurred.shape[0]//2\n    xm = blurred.shape[1]//2\n    \n    markers = np.zeros(blurred.shape,dtype=np.int)\n    # using corners of the image as background\n    markers[0,0:2*xm] = 1\n    markers[2*ym-1,0:2*xm] = 1\n    markers[0:2*ym,0] = 1\n    markers[0:2*ym,2*xm-1] = 1\n    \n    # using middle part of the image as foreground\n    markers[ym-50:ym+50,xm-20:xm+20] = 2\n    \n    mask = morphology.watershed(blurred, markers)\n    \n    ax[0,i].imshow(orignal)\n    ax[0,i].set_title('Original Image', fontsize=12)\n    \n    ax[1,i].imshow(gray, cmap='gray')\n    ax[1,i].set_title('Gray Image', fontsize=12)\n    \n    ax[2,i].imshow(sobel, cmap='gray')\n    ax[2,i].set_title('After Sobel Filter', fontsize=12)\n    \n    ax[3,i].imshow(blurred, cmap='gray')\n    ax[3,i].set_title('Blurred Image', fontsize=12)\n    \n    ax[4,i].imshow(mask, cmap='gray')\n    ax[4,i].set_title('Mask', fontsize=12)\n    \n    orignal[mask==1,:] = [0,0,0]\n    \n    ax[5,i].imshow(orignal)\n    ax[5,i].set_title('Segmented Image', fontsize=12)\n    \n\nfor i in range(6):\n    for j in range(3):\n        ax[i,j].set_xticks([])\n        ax[i,j].set_yticks([])\n    \nfig.suptitle(\"Image Segmentation (ROI selection) using Watershed Transformation\",fontsize=20)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see it is able to extract the foreground from the image but it is not accurate as there is so much noise (unnecessary edges) in the image. This method will not work if the target leaf is not in the middle of the image. Maybe we can assign *foreground* & *background* markers using some different technique to improve the performance of this method.  \nWe can use Canny edge detection before applying this method to reduce the area of focus. "},{"metadata":{},"cell_type":"markdown","source":"### HSV Conversion\n\n**HSV** is closer to how humans perceive color. It has three components: **Hue, Saturation, and Value**.  This color space describes colors (hue or tint) in terms of their shade (saturation or amount of gray) and their brightness value.  \n\nThe HSV color wheel sometimes appears as a cone or cylinder, but always with these three components:\n\n1) Hue  \nHue is the color portion of the model, expressed as a number from 0 to 360 degrees:  \n* Red falls between 0 and 60 degrees.\n* Yellow falls between 61 and 120 degrees.\n* Green falls between 121-180 degrees.\n* Cyan falls between 181-240 degrees.\n* Blue falls between 241-300 degrees.\n* Magenta falls between 301-360 degrees.\n\n2) Saturation  \nSaturation describes the amount of gray in a particular color, from 0 to 100 percent. Reducing this component toward zero introduces more gray and produces a faded effect. Sometimes, saturation appears as a range from just 0-1, where 0 is gray, and 1 is a primary color.\n  \n3) Value (or Brightness)  \nValue works in conjunction with saturation and describes the brightness or intensity of the color, from 0-100 percent, where 0 is completely black, and 100 is the brightest and reveals the most color.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))\n\nfor i in range(3):\n    orignal = getRandomImage()\n    hsv = convertToHSV(orignal)\n    \n    ax[i,0].imshow(orignal)\n    ax[i,0].set_title('Original Image', fontsize=15)\n    ax[i,1].imshow(hsv, cmap='gray')\n    ax[i,1].set_title('HSV Image', fontsize=15)\n    \nfig.suptitle(\"RGB to HSV Conversion\",fontsize=20)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll use this variation of the images to see if they could perform better than the orignals ones."},{"metadata":{},"cell_type":"markdown","source":"### Gray Scale Conversion"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(10, 10))\n\nfor i in range(3):\n    orignal = getRandomImage()\n    gray = cv.cvtColor(orignal,cv.COLOR_RGB2GRAY)\n    \n    ax[i,0].imshow(orignal)\n    ax[i,0].set_title('Original Image', fontsize=15)\n    ax[i,1].imshow(gray, cmap='gray')\n    ax[i,1].set_title('Gray Image', fontsize=15)\n    \nfig.suptitle(\"RGB to Gray Scale Conversion\",fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to the HSV images, I'll try this variation also to see if I could get some better results."},{"metadata":{},"cell_type":"markdown","source":"## Classification Model Building\n\nI will use **TPUs** for training this purpose.\n\n**What are Tensor Processing Units (TPUs) ?**\n\nTPUs are hardware **accelerators** specialized in deep learning tasks. Cloud TPUs are available in a base configuration with **8 cores** and also in larger configurations called \"TPU pods\" of up to **2048 cores**. The extra hardware can be used to accelerate training by increasing the training batch size.\n\n**Why TPUs ?**\n\nModern GPUs are organized around programmable \"cores\", a very flexible architecture that allows them to handle a variety of tasks such as 3D rendering, deep learning, physical simulations, etc.. TPUs on the other hand pair a classic vector processor with a dedicated **matrix multiply unit** and excel at any task where large matrix multiplications dominate, such as neural networks."},{"metadata":{},"cell_type":"markdown","source":"#### TPU Configuration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPU detection  \ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"TPU Detected\")\n    \nexcept ValueError:\n    print(\"TPU not Detected\")\n    tpu = None\n\n# TPUStrategy for distributed training\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nelse: # default strategy that works on CPU and single GPU\n    strategy = tf.distribute.get_strategy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Image Dataset loading"},{"metadata":{},"cell_type":"markdown","source":"#### Setting Hyperparameters "},{"metadata":{"trusted":true},"cell_type":"code","source":"image_count = train_df.shape[0]\n\nif tpu:\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nelse:\n    BATCH_SIZE = 64\n\nprint(\"Setting Batch size to: \",BATCH_SIZE)\n    \nIMG_HEIGHT = 512\nIMG_WIDTH = 512\nSTEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)\n\nGCS_PATH = KaggleDatasets().get_gcs_path()\nprint(\"GCS Path: \",GCS_PATH)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading Images & its Labels using tf.data.Dataset API"},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# read train & test filenames\ntrain_filenames = train_df['image_id'].apply(lambda x: GCS_PATH + '/images/' + x + '.jpg')\ntest_filenames = test_df['image_id'].apply(lambda x: GCS_PATH + '/images/' + x + '.jpg')\n\ntrain_labels = train_df.iloc[:,1:].values.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decodeImage(filename,label=None,image_size=(IMG_HEIGHT,IMG_WIDTH)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    \n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is not None:\n        return image, label\n    \n    return image\n    \ndef imageAugmentation(image,label=None):\n    \n    aug = [True,False]\n    \n    if random.choice(aug):\n        image = tf.image.random_brightness(image,max_delta=0.5,seed=SEED)\n    if random.choice(aug):\n        image = tf.image.random_flip_left_right(image,seed=SEED)\n    if random.choice(aug):\n        image = tf.image.random_flip_up_down(image,seed=SEED)\n    \n    if label is not None:\n        return image,label\n    \n    return image\n\n\ntrain_dataset = (\n    tf.data.Dataset.from_tensor_slices((train_filenames,train_labels))\n    .map(decodeImage, num_parallel_calls=AUTO)\n    .shuffle(500)\n    .cache()\n    .repeat()\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n    .map(imageAugmentation, num_parallel_calls=AUTO))\n\ntest_dataset = (\n    tf.data.Dataset.from_tensor_slices(test_filenames)\n    .map(decodeImage, num_parallel_calls=AUTO)\n    .cache()\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining different models & Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"def scratchModel():\n    model = tf.keras.models.Sequential()\n    \n    model.add(tf.keras.layers.Conv2D(64,kernel_size=5,padding='same',activation='relu',input_shape=[IMG_HEIGHT,IMG_WIDTH,3]))\n#     model.add(tf.keras.layers.Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n#     model.add(tf.keras.layers.MaxPool2D())\n    \n    model.add(tf.keras.layers.Conv2D(64,kernel_size=5,strides=2,activation='relu'))\n#     model.add(tf.keras.layers.Conv2D(64,kernel_size=5,padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n#     model.add(tf.keras.layers.MaxPool2D())\n    \n    model.add(tf.keras.layers.Conv2D(128,kernel_size=5,strides=2,activation='relu'))\n#     model.add(tf.keras.layers.Conv2D(128,kernel_size=5,padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n#     model.add(tf.keras.layers.MaxPool2D())\n    \n    model.add(tf.keras.layers.Conv2D(256,kernel_size=5,strides=2,activation='relu'))\n#     model.add(tf.keras.layers.Conv2D(256,kernel_size=5,padding='same'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n#     model.add(tf.keras.layers.MaxPool2D())\n    \n#     model.add(tf.keras.layers.Conv2D(256,kernel_size=3,strides=2,activation='relu'))\n# #     model.add(tf.keras.layers.Conv2D(512,kernel_size=3,padding='same'))\n#     model.add(tf.keras.layers.BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n# #     model.add(tf.keras.layers.MaxPool2D())\n    \n#     model.add(tf.keras.layers.Conv2D(256,kernel_size=3,strides=2,activation='relu'))\n# #     model.add(tf.keras.layers.Conv2D(512,kernel_size=3,padding='same'))\n#     model.add(tf.keras.layers.BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n# #     model.add(tf.keras.layers.MaxPool2D())\n\n#     model.add(tf.keras.layers.Conv2D(256,kernel_size=3,strides=2,activation='relu'))\n# #     model.add(tf.keras.layers.Conv2D(512,kernel_size=3,padding='same'))\n#     model.add(tf.keras.layers.BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n# #     model.add(tf.keras.layers.MaxPool2D())\n    \n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dense(521,activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(521))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(tf.keras.layers.Dense(4,activation='softmax'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getDenseNets(index):\n    \n    if index==0:\n        model = tf.keras.applications.DenseNet121(input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n                                                 include_top=False,\n                                                 weights='imagenet')\n    elif index==1:\n        model = tf.keras.applications.DenseNet169(input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n                                                 include_top=False,\n                                                 weights='imagenet')\n    else:\n        model = tf.keras.applications.DenseNet201(input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n                                                 include_top=False,\n                                                 weights='imagenet')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CyclicLR(tf.keras.callbacks.Callback):\n    \n    def __init__(self,base_lr=1e-5,max_lr=1e-3,stepsize=10):\n        super().__init__()\n        \n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.stepsize = stepsize\n        self.iterations = 0\n        self.history = {}\n        \n    def clr(self):\n        cycle = np.floor((1+self.iterations)/(2*self.stepsize))\n        x = np.abs(self.iterations/self.stepsize - 2*cycle + 1)\n        \n        return self.base_lr + (self.max_lr - self.base_lr)*(np.maximum(0,1-x))\n    \n    def on_train_begin(self,logs={}):\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n    \n    def on_batch_end(self,batch,logs=None):\n        logs = logs or {}\n        \n        self.iterations += 1\n        \n        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LRFinder(tf.keras.callbacks.Callback):\n    \n    def __init__(self,min_lr=1e-5,max_lr=1e-2,steps_per_epoch=None,epochs=None):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch*epochs\n        self.iteration = 0\n        self.history = {}\n        \n    def lr(self):\n        \n        x = self.iteration/self.total_iterations\n        \n        return self.min_lr + (self.max_lr - self.min_lr)*x\n    \n    def on_train_begin(self,logs={}):\n        \n        tf.keras.backend.set_value(self.model.optimizer.lr,self.min_lr)\n        \n    def on_batch_end(self,batch,logs=None):\n        logs = logs or {}\n        \n        self.iteration += 1\n        \n        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr())\n    \n    def plot_lr(self):\n        \n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('Learning rate')\n        plt.show()\n        \n    def plot_loss(self):\n        \n        plt.plot(self.history['lr'], self.history['loss'])\n        plt.xscale('log')\n        plt.xlabel('Learning rate')\n        plt.ylabel('Loss')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Find Learning rate before training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tempModel = tf.keras.applications.Xception(input_shape=[512,512,3],\n#                                                  include_top=False,\n#                                                  weights='imagenet')\n\n# tempModel.summary()\n\n# # Let's take a look to see how many layers are in the base model\n# print(\"Number of layers in the base model: \", len(tempModel.layers))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# (input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n#  include_top=False,\n#  weights='imagenet')\n\nwith strategy.scope():\n            \n    base_model = tf.keras.applications.Xception(input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n                                                 include_top=False,\n                                                 weights='imagenet')\n\n#     base_model.trainable = False\n    \n    # Let's take a look to see how many layers are in the base model\n    print(\"Number of layers in the base model: \", len(base_model.layers))\n\n    # Fine-tune from this layer onwards\n    fine_tune_at = np.floor(len(base_model.layers)*0.9)\n\n    # Freeze all the layers before the `fine_tune_at` layer\n#     for layer in base_model.layers[:int(fine_tune_at)]:\n#         layer.trainable =  False\n\n    \n    model = tf.keras.models.Sequential([\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(4,activation='softmax')\n    ])\n\n#     model = scratchModel()\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n             loss=tf.keras.losses.CategoricalCrossentropy(),\n             metrics=['categorical_accuracy'])\n    \n    model.summary()\n    \nLRF_EPOCHS = 4    \n\nlrfinder = LRFinder(steps_per_epoch=STEPS_PER_EPOCH,epochs=LRF_EPOCHS)\n    \nhistory = model.fit(train_dataset,epochs=LRF_EPOCHS,steps_per_epoch=STEPS_PER_EPOCH,callbacks=[lrfinder])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrfinder.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrfinder.plot_loss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will choose the learning rate range in which there is significant decrease in the loss"},{"metadata":{},"cell_type":"markdown","source":"### Validation Strategy\n\nI'll use **Stratified KFold** cross-validation for **training & validation** of the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"FOLDS = 5\n\nskf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n\n\ndef crossValidation(train_filenames, train_labels, fold=skf, callbacks=[], epochs=20,base_trainable=True,top_layers_trainable=False):\n        \n    val_scores = []\n    history = []\n\n    for i, (train_idx, test_idx) in enumerate(fold.split(train_filenames,[x.argmax() for x in train_labels])):\n\n        X_train, y_train = train_filenames[train_idx], train_labels[train_idx]\n        X_val, y_val = train_filenames[test_idx], train_labels[test_idx]\n        \n        \n        # Load Dataset \n        train_dataset = (\n        tf.data.Dataset.from_tensor_slices((X_train,y_train))\n        .map(decodeImage, num_parallel_calls=AUTO)\n        .shuffle(500)\n        .cache()\n        .repeat()\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n        .map(imageAugmentation, num_parallel_calls=AUTO))\n\n        validation_dataset = (\n        tf.data.Dataset.from_tensor_slices((X_val,y_val))\n        .map(decodeImage, num_parallel_calls=AUTO)\n        .cache()\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO))\n        \n        \n        # load model \n        with strategy.scope():\n            \n            base_model = tf.keras.applications.Xception(input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n                                                 include_top=False,\n                                                 weights='imagenet')\n            \n            if not base_trainable:\n                base_model.trainable = False\n            \n            if top_layers_trainable:\n                fine_tune_at = np.floor(len(base_model.layers)*0.9)\n\n                # Freeze all the layers before the `fine_tune_at` layer\n                for layer in base_model.layers[:int(fine_tune_at)]:\n                    layer.trainable =  False\n            \n            model = tf.keras.models.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(4,activation='softmax')\n            ])\n\n#             model = scratchModel()\n            \n            model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n                     loss=tf.keras.losses.CategoricalCrossentropy(),\n                     metrics=['categorical_accuracy'])\n            \n#         print(model.summary())\n        \n        \n        # Model training\n        print(f\"\\n\\nFold {i+1} Training: \")\n        print(\"---------------------------------------------------------------------------\\n\\n\")\n        history0 = model.fit(train_dataset,epochs=epochs,steps_per_epoch=len(X_train)//BATCH_SIZE,validation_data=validation_dataset,callbacks=callbacks)\n        print(\"\\n\\n---------------------------------------------------------------------------\\n\\n\")\n\n        history.append(history0)\n        \n        \n        # Valdiation Average F1-Score Calculation\n        val_predictions_prob = model.predict(validation_dataset)\n\n        val_predictions = val_predictions_prob.copy()\n        val_predictions[:,:] = 0\n        \n        for j, pred in enumerate(val_predictions_prob):\n            val_predictions[j,pred.argmax()] = 1\n        \n        val_score = 0\n        \n        for j in range(4):\n            val_score += f1_score(y_val[:,j],val_predictions[:,j])\n\n        val_score /= 4\n\n        print(f\"\\nFold {i+1} F1_Scores: \\nValidation: {val_score}\")\n        print(\"------------------------------------------------------------------------\\n\\n\")\n\n        val_scores.append(val_score)\n        \n        # Debugging\n#         break\n\n    return history, val_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validating the Model\n\nI'll use the learning rates that I found using the above mentioned part to cross-validate the model. I'll use **Cyclic-Learning** rate to train the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ncyclicLR = CyclicLR(base_lr=1e-4,max_lr=1e-3)\n# earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',baseline=0.9600,min_delta=0.001,patience=4)\n\nVAL_EPOCHS = 20\n\ncross_val_history, val_scores = crossValidation(train_filenames,\n                                                train_labels,\n                                                callbacks=[cyclicLR],\n                                                epochs=VAL_EPOCHS,\n                                                base_trainable=True,\n                                                top_layers_trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DenseNet169: Epochs = 50  CyclicLR (4e-4,1e-3) -> 0.8874265767038977, Epochs = 100 -> 0.900741222847377  \nDenseNet121: Epochs = 100 CyclicLR (2e-4,1e-3) -> 0.9049772549096043  \nDenseNet201: Epochs = 40  CyclicLR (2e-4,1e-3) -> 0.8866877092580883"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean Validation F1-Scores\n\nval_score = np.mean(val_scores)\n\nprint(\"Mean F1 Score: \",np.mean(val_scores))\nprint(\"Std F1 Score: \",np.std(val_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\ntrain_loss = np.zeros(VAL_EPOCHS)\nval_loss = np.zeros(VAL_EPOCHS)\n\nfor i, hist in enumerate(cross_val_history):\n    \n    train_loss = np.add(train_loss,hist.history['loss'])\n    val_loss = np.add(val_loss,hist.history['val_loss'])\n\n    \nfig.add_trace(go.Scatter(x=np.arange(1,VAL_EPOCHS+1),\n                        y=train_loss/FOLDS,\n                        mode='lines+markers',\n                        name='Train'))\n\nfig.add_trace(go.Scatter(x=np.arange(1,VAL_EPOCHS+1),\n                    y=val_loss/FOLDS,\n                        mode='lines+markers',\n                        name='Validation'))\n    \nfig.update_layout(title_text=\"Average Cross-Validation Losses\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\n\ntrain_accuracy = np.zeros(VAL_EPOCHS)\nval_accuracy = np.zeros(VAL_EPOCHS)\n\nfor i, hist in enumerate(cross_val_history):\n    \n    train_accuracy = np.add(train_accuracy,hist.history['categorical_accuracy'])\n    val_accuracy = np.add(val_accuracy,hist.history['val_categorical_accuracy'])\n    \n\nfig.add_trace(go.Scatter(x=np.arange(1,VAL_EPOCHS+1),\n                        y=train_accuracy/FOLDS,\n                        mode='lines+markers',\n                        name='Train'))\n\nfig.add_trace(go.Scatter(x=np.arange(1,VAL_EPOCHS+1),\n                    y=val_accuracy/FOLDS,\n                        mode='lines+markers',\n                        name='Validation'))\n    \nfig.update_layout(title_text=\"Average Cross-Validation Accuracies\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \nwith strategy.scope():\n    base_model = tf.keras.applications.Xception(input_shape=[IMG_HEIGHT,IMG_WIDTH,3],\n                                                     include_top=False,\n                                                     weights='imagenet')\n\n    model = tf.keras.models.Sequential([\n            base_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense(4,activation='softmax')\n        ])\n\n    #     model = scratchModel()\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n             loss=tf.keras.losses.CategoricalCrossentropy(),\n             metrics=['categorical_accuracy'])   \n\nEPOCHS = 20\n\nhistory = model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=EPOCHS,callbacks=[cyclicLR])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training History visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = make_subplots(1,2)\n\nfig.add_trace(go.Scatter(x=np.arange(1,EPOCHS+1),\n                        y=history.history['loss'],\n                        mode='lines+markers',\n                        name=f'Training Loss'),1,1)\n\nfig.add_trace(go.Scatter(x=np.arange(1,EPOCHS+1),\n                        y=history.history['categorical_accuracy'],\n                        mode='lines+markers',\n                        name=f'Training Accuracy'),1,2)\n\nfig.update_layout(title_text=\"Training Loss & Accuracy\")\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test-set Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = model.predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=2, figsize=(15, 18))\n\nfor i in range(5):\n    img_id = random.choice(np.arange(0,test_df.shape[0]))\n    test_image = getImage(test_df.image_id[img_id],(IMG_HEIGHT,IMG_WIDTH))\n    \n    ax[i,0].imshow(test_image)\n    ax[i,0].set_title(f'{test_df.image_id[img_id]}', fontsize=12)\n    ax[i,1].barh(y=train_df.columns[1:],width=test_predictions[img_id])\n    ax[i,1].set_title('Predictions', fontsize=12)\n    \nfig.suptitle(\"Test set Predictions\",fontsize=20)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Submission file "},{"metadata":{"trusted":true},"cell_type":"code","source":"sampleSubmission_df.iloc[:,1:] = test_predictions\n\nmodel_name = 'Xception'\n\nsampleSubmission_df.to_csv(f'/kaggle/working/{model_name}{val_score}.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|Model name|val_accuracy|Public LB Score|Notes|\n|----------|------------|---------------|-----|\n|Xception|0.8186432026214538|0.955|EPOCHS = 20 Val_acc = 0.8186 base_trainable = True top_layers_trainable = False|"},{"metadata":{},"cell_type":"markdown","source":"## Key-Takeaways\n\n* Various Image **Augmentation** & **Processing** techniques can be used to build more robust model.\n* Firstly, I was using **Plotly** for image data **Visualization** but I was taking too much memory and time to load because it is an **interactive** visualization library that is why I shifted to **Matplotlib** for image data **Visualization** which takes less time to load and uses less memory.\n* Training using TPUs makes training fast wich can help us try different models in less time"},{"metadata":{},"cell_type":"markdown","source":"<h3>\n<font color='red'>\n    <strong>Please!!! Upvote this kernel if you find it useful.</strong>\n</font>\n</h3>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}