{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nfrom sklearn.metrics import roc_auc_score\nimport os\nimport gc\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pydot\nfrom keras.datasets import mnist\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = 999\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\n\nnp.random.seed(42)\nimport tensorflow as tf\nfrom numpy import random\nimport keras as k\nfrom keras.layers import Dense, Flatten, Conv2D, Conv3D\n\nimport matplotlib.pylab as pylab\nparams = {'legend.fontsize': 'medium',\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large'}\n\nfrom keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D, TimeDistributed, GlobalAveragePooling2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom numpy import argmax, array_equal\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\n# from imgaug import augmenters\nfrom random import randint\npd.set_option('float_format', '{:.3f}'.format)\npylab.rcParams.update(params)\nplt.rcParams['figure.figsize'] = (15, 6)\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = 999\npd.options.display.max_columns = 99\n\nimport torch \nimport torchvision\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_labels = pd.read_csv(r'../input/plant-pathology-2020-fgvc7/train.csv')\ntest_labels = pd.read_csv(r'../input/plant-pathology-2020-fgvc7/test.csv')\nimage_path = Path(r'../input/plant-pathology-2020-fgvc7/images')\ntrain_images = train_labels['image_id'].tolist()\ntest_images = test_labels['image_id'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nfrom torchvision.transforms import transforms\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PlantsData(Dataset):\n    def __init__(self, directory, train_image_list,  size, labels, id_cols, target_cols, transform, img_aug):\n        self.directory = directory\n        self.labels = labels\n        self.id_cols = id_cols\n        self.target_cols = target_cols\n        self.size = size\n        self.train_image_list = train_image_list\n        self.transform = transform\n        self.aug = img_aug\n        \n    def __len__(self):\n        return len(self.train_image_list)\n    \n    def __getitem__(self,idx):\n        image_name = self.train_image_list[idx]\n        image_path = os.path.join(self.directory, image_name+\".jpg\")\n        img = cv2.imread(image_path)\n        img = cv2.resize(img, (self.size, self.size))\n        #Change background colurs for the image\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.aug(img)\n#         img = img/255.0\n        target = self.labels[self.labels[self.id_cols] == image_name][self.target_cols].values\n        \n        return img, target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ConvNet with pretrained models in Torch\n- Setting up the hyperparameters\n- Loading the Trained models into the layers\n- Also setup the training loops with mdoel checkpoint ans learning rate scheduler\n- Try to setup the locally connected 2D layers for  Pixel wise switichin in the network\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"in_channels = 3\nlearning_rate = 1e-3\nbatch_size = 32\nnum_epochs = 25\nnum_classes = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the layers in the mdoels\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init()\n        \n    def forward(self, x):\n        return x\n    \n    \n# loading the model\nmodel = torchvision.models.vgg16(pretrained=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PlantNet(nn.Module):\n    def __init__(self, n_channels, num_classes):\n        super(PlantNet, self).__init__()\n        self.n_channels = n_channels\n        self.num_classes = num_classes\n        self.VGG = torchvision.models.vgg16(pretrained=True)\n        self.conv1 = nn.Sequential(nn.Conv2d(512, 1024,  kernel_size = 3, stride = 1, padding = 1), nn.BatchNorm2d(1024), nn.ReLU())\n#         self.conv2 = nn.Sequential(nn.Conv2d(512, 1024,  kernel_size = 3, stride = 1, padding = 1), nn.BatchNorm2d(1024), nn.ReLU(inplace=True))\n        self.linear = nn.Sequential(nn.Linear(1024 * 8 * 8, 4096 ), nn.ReLU())\n        self.final_layer = nn.Sequential(nn.Linear(4096, 1024), nn.ReLU(), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, num_classes), nn.Sigmoid())\n        \n    def forward(self, x):\n        feature = self.VGG.features\n        vgg_features = feature(x)\n        out = self.conv1(vgg_features)\n#         out = self.conv2(out)\n        out = out.reshape(out.shape[0], -1)\n        out = self.linear(out)\n        output = self.final_layer(out)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_images)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_trans = transforms.Compose([transforms.ToPILImage(),\n                   transforms.RandomHorizontalFlip(p = 0.5), \n                   transforms.ColorJitter(brightness = 0.7),\n                   transforms.RandomRotation(degrees = 45),\n                   transforms.RandomVerticalFlip(p = 0.8),\n                   transforms.ToTensor()])\n\ntrain_loader = PlantsData(image_path, train_images[:1000], 256, train_labels, 'image_id', train_labels.columns[1:], transform = True, img_aug=my_trans)\ntrain_plant_loader = DataLoader(train_loader, batch_size = 32, shuffle = True)\n\nval_loader = PlantsData(image_path, train_images[1000: 1320], 256, train_labels, 'image_id', train_labels.columns[1:],  transform = True, img_aug=my_trans)\nval_plant_loader = DataLoader(val_loader, batch_size = 32, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm, tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = PlantNet(n_channels=3, num_classes = 4)\ndevice = 'cuda'\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\ncriterion = nn.CrossEntropyLoss()\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, verbose = True, factor = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Training loop for VGG modified network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"val_outputs = []\nval_targets = []\n\n\ndef save_checkpoint(checkpoint, filename = 'plant_models.path.tar'):\n    torch.save(checkpoint, filename)\nfilename = 'plant_models.path.tar'\n\nmean_loss = np.nan\nlosses = []\nf1_score_list = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n\n    checkpoint = {'state_dict':model.state_dict(), 'optimizer_state_dict':optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict()}\n    if epoch > 1:\n        if f1_score_list[epoch-1] > f1_score_list[epoch-2]:\n            print(f\"F1 score on validation set increased form {f1_score_list[epoch - 2]} to {f1_score_list[epoch-1]} saving the model as {filename}\")\n            save_checkpoint(checkpoint, filename = filename)\n\n    loop = tqdm(enumerate(train_plant_loader), position = 0, total = len(train_plant_loader), leave = True)\n\n    for i, (image, target) in loop:\n#         image = image.permute(0 ,3, 1 ,2)\n        image = image.to(device)\n        target = target.to(device)\n        \n        #Feeding the images to the model\n        output = model(image.float())\n        loss = criterion(output, torch.argmax(target.squeeze(), axis = 1))\n        \n        # Propogating the loss backwards\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        loop.set_description(f'Epoch {epoch+1}/{num_epochs}')\n        loop.set_postfix(loss = loss.item(), mean_loss = mean_loss, accuracy_score = accuracy_score(val_targets, val_outputs) , F1_score = f1_score(val_targets, val_outputs, average = 'weighted'))\n        losses.append(loss.item())\n  \n    model.eval()\n    with torch.no_grad():\n        val_outputs = []\n        val_targets = []\n        for i, (image, target) in enumerate(val_plant_loader):\n#             image = image.permute(0,3,1,2)\n            image = image.to(device)\n            target = target.to(device)\n\n            # Predictions from Model\n            outputs = model(image.float())\n            outputs = torch.argmax(outputs, axis = 1)\n            outputs = outputs.to('cpu')\n            val_outputs += outputs.numpy().tolist()\n\n            target = torch.argmax(target.squeeze(), axis = 1)\n            target = target.to('cpu')\n            val_targets += target.numpy().tolist() \n            \n    \n    f1_score_list.append(f1_score(val_targets, val_outputs, average = 'weighted'))\n    accuracy_list.append(accuracy_score(val_targets, val_outputs))\n      \n            \n    mean_loss = sum(losses)/len(losses)\n    scheduler.step(mean_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noutput = torch.randn(10, 120).float()\ntarget = torch.FloatTensor(10).uniform_(0, 120).long()\nloss = criterion(output, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.shape, target.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}