{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U efficientnet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\n\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\nfolder = '/kaggle/input/plant-pathology-2020-fgvc7'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.read_csv(os.path.join(folder,'train.csv'))\ntrain_id = Y['image_id']\nY = Y[Y.columns[1:]] # remove image_id column\nY = Y.values # convert to array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Resizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_resize(size, img_id):\n    '''\n    resize all images to same dimensions\n    amended from https://www.kaggle.com/shawon10/plant-pathology-classification-using-densenet121\n    '''\n    images=[]\n    for i, name in enumerate(img_id):\n        path=os.path.join(folder,'images',name+'.jpg')\n        img=cv2.imread(path)\n        image=cv2.resize(img,(size,size),interpolation=cv2.INTER_AREA)\n        images.append(image)\n        # print processing counter\n        if i%200==0:\n            print(i, 'images processed')\n    return images\n\n\nX = image_resize(100, train_id)\nX = np.array(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n                          width_shift_range=0.2, # Range for random horizontal shifts\n                          height_shift_range=0.2, # Range for random vertical shifts\n                          zoom_range=0.2, # Range for random zoom\n                          horizontal_flip=True, # Randomly flip inputs horizontally\n                          vertical_flip=True) # Randomly flip inputs vertically\n\ntrain_flow = aug.flow(X_train, Y_train, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(input_shape, classes):\n    '''\n    transfer learning from imagenet's weights, using Google's efficientnet7 architecture\n    top layer (include_top) is removed as the number of classes is changed\n    '''\n    base = efn.EfficientNetB7(input_shape=input_shape, weights='imagenet', include_top=False)\n\n    model = Sequential()\n    model.add(base)\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['accuracy'])\n    return model\n\n# each pic has been resized to 100x100, and with 3 channels (RGB)\ninput_shape = (100,100,3)\nclasses = 4\nmodel = model(input_shape, classes)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for every epoch, the total original images will be augmented randomly\nmodel.fit_generator(train_flow,\n                    steps_per_epoch=32,\n                    epochs=15,\n                    verbose=1,\n                    validation_data=(X_val, Y_val),\n                    use_multiprocessing=True,\n                    workers=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Training Curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_validate(model, loss_acc):\n    '''\n    Plot model accuracy or loss for both train and test validation per epoch\n    model : fitted model\n    loss_acc : input 'loss' or 'acc' to plot respective graph\n    '''\n    history = model.history.history\n\n    if loss_acc == 'loss':\n        axis_title = 'loss'\n        title = 'Loss'\n        epoch = len(history['loss'])\n    elif loss_acc == 'acc':\n        axis_title = 'accuracy'\n        title = 'Accuracy'\n        epoch = len(history['loss'])\n\n    plt.figure(figsize=(15,4))\n    plt.plot(history[axis_title])\n    plt.plot(history['val_' + axis_title])\n    plt.title('Model ' + title)\n    plt.ylabel(title)\n    plt.xlabel('Epoch')\n\n    plt.grid(b=True, which='major')\n    plt.minorticks_on()\n    plt.grid(b=True, which='minor', alpha=0.2)\n\n    plt.legend(['Train', 'Test'])\n    plt.show()\n\n\nplot_validate(model, 'acc')\nplot_validate(model, 'loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in test images & resize\nY_test = pd.read_csv(os.path.join(folder,'test.csv'))\ntest_id = Y_test['image_id']\nX_test = image_resize(100, test_id)\nX_test = np.array(X_test)\nprint('Test images done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get prediction probabilities for each class\npredict_prob = model.predict(X_test)\n\ndf_predict_prob = pd.DataFrame(predict_prob, columns=['healthy','multiple_diseases','rust','scab'])\n\n# join both image_id & df_predict_prob together for submission\nframe = [test_id, df_predict_prob]\ndf_submission = pd.concat(frame, axis=1)\ndf_submission.to_csv(r'submisson.csv', index=False)\n# df_submission.to_csv(r'/kaggle/working/submisson.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}