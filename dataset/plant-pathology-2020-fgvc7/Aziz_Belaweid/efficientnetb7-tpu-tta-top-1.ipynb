{"cells":[{"metadata":{},"cell_type":"markdown","source":"Special thanks to chris deotte's notebooks they have'been really helpful, Thanks you Grandmaster !","execution_count":null},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"!pip install efficientnet\n!pip install tensorflow-addons=='0.9.1'\nimport efficientnet.tfkeras as efn\n\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.layers import Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam , RMSprop \nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , EarlyStopping , ModelCheckpoint , LearningRateScheduler\n\nimport os\nimport random, re, math\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\n\nprint(tf.__version__)\nprint(tf.keras.__version__)\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications import MobileNet\n\nimport tensorflow_addons as tfa\n\n#for reproducible results\n#import random\n#seed_value = 13\n#random.seed(seed_value)\n#np.random.seed(seed_value)\n#tf.random.set_seed(seed_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed=13):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    os.environ['TF_KERAS'] = '1'\n    random.seed(seed)\n    \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/sample_submission.csv\")\ntest = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\ntrain = pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size = 800","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Train Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = []\nfor name in train['image_id'] :\n    path = '/kaggle/input/plant-pathology-2020-fgvc7/images/'+name+'.jpg'\n    image = cv2.imread(path)\n    image=cv2.resize(image,(img_size,img_size),interpolation=cv2.INTER_AREA)\n    train_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nfor i in range(25) :\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Test Images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = []\nfor name in test['image_id'] :\n    image = cv2.imread('/kaggle/input/plant-pathology-2020-fgvc7/images/'+name+'.jpg')\n    image=cv2.resize(image,(img_size,img_size),interpolation=cv2.INTER_AREA)\n    test_images.append(image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nfor i in range(25) :\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(test_images[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.drop('image_id',axis=1)\ny = np.array(y)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.ndarray(shape=(len(train_images),img_size,img_size,3),dtype=np.float32)\n\n\nfor i,image in enumerate(train_images) :\n    X[i] = image\n    \n    \nX = X/255.0    \nX.shape    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.ndarray(shape=(len(test_images),img_size,img_size,3),dtype=np.float32)\n\nfor i,image in enumerate(test_images) :\n    X_test[i]=image\n\nX_test = X_test / 255.0  \nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def construct_model() :\n    \n    model = Sequential()\n    \n    #Block 1\n    model.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu',input_shape=(img_size,img_size,3)))\n    model.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Dropout(0.3))\n    \n    #Block 2\n    \n    model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Dropout(0.3))\n\n    #Block 3\n    \n    model.add(Conv2D(256,kernel_size=(3,3),padding='valid',activation='relu'))\n    model.add(Conv2D(256,kernel_size=(3,3),padding='valid',activation='relu'))\n    model.add(Conv2D(256,kernel_size=(3,3),padding='valid',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Dropout(0.2))\n    \n    #Block 4 \n    \n    model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(Conv2D(512,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Dropout(0.2))    \n    \n    \n    \n    \n    \n    #Block 5\n    \n    model.add(Conv2D(1024,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(Conv2D(1024,kernel_size=(3,3),padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(Dropout(0.2))\n\n    \n    #Final Block\n    \n    model.add(Flatten())\n    model.add(Dense(512,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(256,activation='relu'))\n    model.add(Dense(4,activation='softmax'))\n        \n    #Compile\n    \n    batch_size = 32\n    epochs = 20 \n    \n    model.compile(optimizer='Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n    #model.fit(X,y,validation_split=0.1,batch_size=batch_size,epochs=epochs)\n    \n    return model  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_model()\nmodel.summary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_gen = ImageDataGenerator(rotation_range=45,\n                              horizontal_flip=True,\n                              vertical_flip=True,\n                              width_shift_range=0.2,\n                              height_shift_range=0.2,\n                              zoom_range = 0.1,\n                              shear_range = 0.1,\n                              #brightness_range = [0.5,1.5],\n                              fill_mode = 'nearest'\n                             )\ndata_gen.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Callbacks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. ReduceLR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr =  ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 10,\n  verbose = 0, mode = \"auto\", epsilon = 1e-04, cooldown = 0,\n  min_lr = 1e-5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Early Stopping","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"es = EarlyStopping(monitor = \"val_loss\" , verbose = 1 , mode = 'min' , patience = 50 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Model Checkpoint  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mc = ModelCheckpoint('best_model.h5', monitor = 'loss' , mode = 'min', verbose = 1 , save_best_only = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.Learning Rate Scheduler","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#lrs = LearningRateScheduler(lrfn,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.SWA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa\n\ncheckpoint_path = \"best_model.h5\"\n\nswa_mc = tfa.callbacks.AverageModelCheckpoint(filepath=checkpoint_path, save_weights_only=False,\n                                                   update_weights=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Other Losses :","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.Focal Loss :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K\nimport tensorflow as tf\n\ndef categorical_focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"\n    Implementation of Focal Loss from the paper in multiclass classification\n    Formula:\n        loss = -alpha*((1-p)^gamma)*log(p)\n    Parameters:\n        alpha -- the same as wighting factor in balanced cross entropy\n        gamma -- focusing parameter for modulating factor (1-p)\n    Default value:\n        gamma -- 2.0 as mentioned in the paper\n        alpha -- 0.25 as mentioned in the paper\n    \"\"\"\n    def focal_loss(y_true, y_pred):\n        # Define epsilon so that the backpropagation will not result in NaN\n        # for 0 divisor case\n        epsilon = K.epsilon()\n        # Add the epsilon to prediction value\n        #y_pred = y_pred + epsilon\n        # Clip the prediction value\n        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n        y_pred = tf.cast(y_pred,tf.float32)\n        y_true = tf.cast(y_true,tf.float32)\n        # Calculate cross entropy\n        cross_entropy = -y_true*K.log(y_pred)\n        # Calculate weight that consists of  modulating factor and weighting factor\n        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n        # Calculate focal loss\n        loss = weight * cross_entropy\n        # Sum the losses in mini_batch\n        loss = K.sum(loss, axis=1)\n        return loss\n    \n    return focal_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics : ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ndef roc_auc(preds, targs, labels=range(4)):\n    # One-hot encode targets\n    targs = np.eye(4)[targs]\n    return np.mean([roc_auc_score(targs[:,i], preds[:,i]) for i in labels])\ndef healthy_roc_auc(*args):\n    return roc_auc(*args, labels=[0])\ndef multiple_diseases_roc_auc(*args):\n    return roc_auc(*args, labels=[1])\ndef rust_roc_auc(*args):\n    return roc_auc(*args, labels=[2])\ndef scab_roc_auc(*args):\n    return roc_auc(*args, labels=[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def aucroc_healthy(y_true, y_pred):\n    return tf.py_func(healthy_roc_auc, (y_true, y_pred), tf.double)\n\ndef aucroc_multiple_diseases(y_true, y_pred):\n    return tf.py_func(multiple_diseases_roc_auc, (y_true, y_pred), tf.double)\n\ndef aucroc_rust(y_true, y_pred):\n    return tf.py_func(rust_roc_auc, (y_true, y_pred), tf.double)\ndef aucroc_scab(y_true, y_pred):\n    return tf.py_func(scab_roc_auc, (y_true, y_pred), tf.double)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 150,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.load_weights('best_model.h5')\nTTA_NUM = 5\nprobabilities = []\nfor i in range(TTA_NUM):\n    print(f'TTA Number: {i}\\n')\n    test_ds = create_test_data(ordered=False) \n    probabilities.append(model_effnet.predict(test_ds,verbose =1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntab = np.zeros((len(probabilities[1]),4))\nfor i in range(0,len(probabilities[1])) :\n    for j in range(0,TTA_NUM) :\n        tab[i] = tab[i] + probabilities[j][i]\ntab = tab / TTA_NUM              \nsub.loc[:, 'healthy':] = tab\nsub.to_csv('submissionEffnet+TTA.csv', index=False)\nsub.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_performance_transfer_learning(history) :\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    t = f.suptitle('TL Performance', fontsize=12)\n    f.subplots_adjust(top=0.85, wspace=0.3)\n\n    epoch_list = list(range(1,121))\n    ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n    ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n    ax1.set_xticks(np.arange(0, 121, 5))\n    ax1.set_ylabel('Accuracy Value')\n    ax1.set_xlabel('Epoch')\n    ax1.set_title('Accuracy')\n    l1 = ax1.legend(loc=\"best\")\n\n    ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n    ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n    ax2.set_xticks(np.arange(0, 121, 5))\n    ax2.set_ylabel('Loss Value')\n    ax2.set_xlabel('Epoch')\n    ax2.set_title('Loss')\n    l2 = ax2.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_performance(history) :\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    t = f.suptitle('Basic CNN Performance', fontsize=12)\n    f.subplots_adjust(top=0.85, wspace=0.3)\n\n    epoch_list = list(range(1,151))\n    ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n    ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n    ax1.set_xticks(np.arange(0, 151, 5))\n    ax1.set_ylabel('Accuracy Value')\n    ax1.set_xlabel('Epoch')\n    ax1.set_title('Accuracy')\n    l1 = ax1.legend(loc=\"best\")\n\n    ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n    ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n    ax2.set_xticks(np.arange(0, 151, 5))\n    ax2.set_ylabel('Loss Value')\n    ax2.set_xlabel('Epoch')\n    ax2.set_title('Loss')\n    l2 = ax2.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_performance(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_pred  = pd.DataFrame(predictions, columns = ['healthy','multiple_diseases','rust','scab'])\nresults = {'image_id' : test.image_id,\n            'healthy' : df_pred.healthy,\n            'multiple_diseases' : df_pred.multiple_diseases,\n            'rust' : df_pred.rust,\n            'scab' : df_pred.scab\n          }\ndf_results_cnn = pd.DataFrame(results)\ndf_results_cnn.to_csv('submissionCNN.csv',index=False) #0.926","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transfer Learning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\nNumber of layers for each model :\nMobileNet : 87, VGG : 19, DenseNet : 427, Inception : 311, ResNet : 175","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def construct_transfer_learning_model(model_name) :\n    if model_name == 'MobileNet' :\n        base_model=MobileNet(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))#imports the mobilenet model and discards the last 1000 neuron layer.\n        base_model.trainable = True\n        for layer in base_model.layers[:-3] :\n            layer.trainable = True\n    elif model_name == 'VGG16' : \n        base_model=vgg16.VGG16(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n        base_model.trainable = True\n        for layer in base_model.layers[:-3] :\n            layer.trainable = True\n    elif model_name == 'DenseNet' :\n        base_model = DenseNet121(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n        base_model.trainable = True\n        for layer in base_model.layers[:-3] :\n            layer.trainable = True\n    elif model_name == 'Inception' :\n        base_model=InceptionV3(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n        base_model.trainable = True\n        for layer in base_model.layers[:-3] :\n            layer.trainable = True\n    elif model_name == 'ResNet' :\n        base_model = ResNet50(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n        base_model.trainable = True\n        for layer in base_model.layers[:-3] :\n            layer.trainable = True\n    elif model_name == 'EfficientNet' :\n        base_model = efn.EfficientNetB3(weights = 'imagenet', include_top=False, input_shape = (img_size,img_size,3))\n        base_model.trainable = True\n        for layer in base_model.layers[:-3] :\n            layer.trainable = True\n    x=base_model.output\n    x=GlobalAveragePooling2D()(x)\n    x=Dense(512,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n    x=Dropout(0.3)(x)\n    x=Dense(256,activation='relu')(x) #dense layer 2\n    preds=Dense(4,activation='softmax')(x) #final layer with softmax activation\n    model=Model(inputs=base_model.input,outputs=preds)\n    model.compile(optimizer='Adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_transfer_learning_model('MobileNet')\nhistory = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 120,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_performance_transfer_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_transfer_learning_model('VGG16')\nhistory = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 120,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_performance_transfer_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_transfer_learning_model('DenseNet')\nhistory = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 140,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_performance_transfer_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)\ndf_pred  = pd.DataFrame(predictions, columns = ['healthy','multiple_diseases','rust','scab'])\nresults = {'image_id' : test.image_id,\n            'healthy' : df_pred.healthy,\n            'multiple_diseases' : df_pred.multiple_diseases,\n            'rust' : df_pred.rust,\n            'scab' : df_pred.scab\n          }\ndf_results_densenet = pd.DataFrame(results)\ndf_results_densenet.to_csv('submissionDenseNet.csv',index=False) #0.88","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_transfer_learning_model('Inception')\nhistory = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 120,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_performance_transfer_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)\ndf_pred  = pd.DataFrame(predictions, columns = ['healthy','multiple_diseases','rust','scab'])\nresults = {'image_id' : test.image_id,\n            'healthy' : df_pred.healthy,\n            'multiple_diseases' : df_pred.multiple_diseases,\n            'rust' : df_pred.rust,\n            'scab' : df_pred.scab\n          }\ndf_results = pd.DataFrame(results)\ndf_results.to_csv('submissionInception.csv',index=False) #0.878","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_transfer_learning_model('ResNet')\nhistory = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 120,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_performance_transfer_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)\ndf_pred  = pd.DataFrame(predictions, columns = ['healthy','multiple_diseases','rust','scab'])\nresults = {'image_id' : test.image_id,\n            'healthy' : df_pred.healthy,\n            'multiple_diseases' : df_pred.multiple_diseases,\n            'rust' : df_pred.rust,\n            'scab' : df_pred.scab\n          }\ndf_results = pd.DataFrame(results)\ndf_results.to_csv('submissionResNet.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = construct_transfer_learning_model('EfficientNet')\nhistory = model.fit_generator(data_gen.flow(X_train,Y_train,batch_size=32),\n                    steps_per_epoch = X_train.shape[0] // 32,\n                    epochs = 120,\n                    verbose = True,\n                    validation_data= (X_val,Y_val),\n                    callbacks = [reduce_lr,es,mc]\n                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_performance_transfer_learning(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(X_test)\ndf_pred  = pd.DataFrame(predictions, columns = ['healthy','multiple_diseases','rust','scab'])\nresults = {'image_id' : test.image_id,\n            'healthy' : df_pred.healthy,\n            'multiple_diseases' : df_pred.multiple_diseases,\n            'rust' : df_pred.rust,\n            'scab' : df_pred.scab\n          }\ndf_results_efficientnet = pd.DataFrame(results)\ndf_results_efficientnet.to_csv('submissionEfficientNet.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensembling Best Models :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_predictions = 0.25*df_results_densenet + 0.25*df_results_cnn + 0.25 * df_results_mobilenet + 0.25 * df_results_efficientnet\n\ndf_pred  = pd.DataFrame(df_predictions, columns = ['healthy','multiple_diseases','rust','scab'])\nresults = {'image_id' : test.image_id,\n            'healthy' : df_pred.healthy,\n            'multiple_diseases' : df_pred.multiple_diseases,\n            'rust' : df_pred.rust,\n            'scab' : df_pred.scab\n          }\ndf_results_densenet = pd.DataFrame(results)\ndf_results_densenet.to_csv('submissionStackNet.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# With TPU : ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n\ntry :\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU :',tpu.master())\nexcept ValueError : \n    tpu = None\n    \nif tpu :\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse :\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"Replicas :\", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('plant-pathology-2020-fgvc7')  \npath='../input/plant-pathology-2020-fgvc7/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsub = pd.read_csv(path + 'sample_submission.csv')\n\n\ntrain_paths = train.image_id.apply(lambda x : GCS_DS_PATH + '/images/' + x + '.jpg').values\ntest_paths = test.image_id.apply(lambda x : GCS_DS_PATH + '/images/' + x + '.jpg').values\n\n\ntrain_labels = train.loc[:,'healthy':].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Smoothing function :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def LabelSmoothing(encodings , alpha):\n    K = encodings.shape[1]\n    y_ls = (1 - alpha) * encodings + alpha / K\n    return y_ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_classes = 4\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nimg_size = 800\nEPOCHS = 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bool_random_brightness = False\nbool_random_contrast = False\nbool_random_hue = False\nbool_random_saturation = False\n\ngridmask_rate =0\ncutmix_rate = 0.4\nmixup_rate = 0\nrotation = False\nrandom_blackout = False\ncrop_size = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(filename,label=None, image_size=(img_size,img_size)) :\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits,channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image,image_size)\n    if label == None :\n        return image\n    else :\n        return image, label\n    \ndef data_augment(image, label=None, seed=2020) :\n    image = tf.image.random_flip_left_right(image,seed=seed)\n    image = tf.image.random_flip_up_down(image,seed=seed)\n    #image = tf.keras.preprocessing.image.random_rotation(image,45)\n    if crop_size :   \n        image = tf.image.random_crop(image, size=[crop_size, crop_size, 3], seed=seed)\n    if bool_random_brightness:\n        image = tf.image.random_brightness(image,0.2)\n    if bool_random_contrast:\n        image = tf.image.random_contrast(image,0.6,1.4)\n    if bool_random_hue:\n        image = tf.image.random_hue(image,0.07)\n    if bool_random_saturation:\n        image = tf.image.random_saturation(image,0.5,1.5)\n    if random_blackout :\n        image= transform_random_blackout(image)\n        \n        \n    \n    if label == None :\n        return image\n    else :\n        return image,label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CutMix :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# batch\ndef cutmix(image, label, PROBABILITY = cutmix_rate):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    \n    DIM = img_size    \n    imgs = []; labs = []\n    \n    for j in range(BATCH_SIZE):\n        \n        #random_uniform( shape, minval=0, maxval=None)        \n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\n        \n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast(tf.random.uniform([], 0, BATCH_SIZE), tf.int32)\n        \n        # CHOOSE RANDOM LOCATION\n        x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n        y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n        \n        # Beta(1, 1)\n        b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\n        \n\n        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        \n        # MAKE CUTMIX IMAGE\n        one = image[j,ya:yb,0:xa,:]\n        two = image[k,ya:yb,xa:xb,:]\n        three = image[j,ya:yb,xb:DIM,:]        \n        #ya:yb\n        middle = tf.concat([one,two,three],axis=1)\n\n        img = tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        \n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        lab1 = label[j,]\n        lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n\n    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE, nb_classes))\n    return image2,label2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MixUp","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def mixup(image, label, PROBABILITY = mixup_rate):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with mixup applied\n    DIM = img_size\n    \n    imgs = []; labs = []\n    for j in range(BATCH_SIZE):\n        \n        # CHOOSE RANDOM\n        k = tf.cast( tf.random.uniform([],0,BATCH_SIZE),tf.int32)\n        a = tf.random.uniform([],0,1) # this is beta dist with alpha=1.0\n\n        #mixup\n        P = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\n        if P==1:\n            a=0.\n        \n        # MAKE MIXUP IMAGE\n        img1 = image[j,]\n        img2 = image[k,]\n        imgs.append((1-a)*img1 + a*img2)\n        \n        # MAKE CUTMIX LABEL\n        lab1 = label[j,]\n        lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n            \n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE,nb_classes))\n    return image2,label2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_cutmix_mix_up(image,label):\n    # THIS FUNCTION APPLIES BOTH CUTMIX AND MIXUP\n    DIM = img_size\n    SWITCH = 0.5\n    CUTMIX_PROB = 0.666\n    MIXUP_PROB = 0.666\n    # FOR SWITCH PERCENT OF TIME WE DO CUTMIX AND (1-SWITCH) WE DO MIXUP\n    image2, label2 = cutmix(image, label, CUTMIX_PROB)\n    image3, label3 = mixup(image, label, MIXUP_PROB)\n    imgs = []; labs = []\n    for j in range(AUG_BATCH):\n        P = tf.cast( tf.random.uniform([],0,1)<=SWITCH, tf.float32)\n        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n        labs.append(P*label2[j,]+(1-P)*label3[j,])\n    # RESHAPE HACK SO TPU COMPILER KNOWS SHAPE OF OUTPUT TENSOR (maybe use Python typing instead?)\n    image4 = tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n    label4 = tf.reshape(tf.stack(labs),(AUG_BATCH,nb_classes))\n    return image4,label4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Blackout : (has issue needs to be fixed)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_random_blackout(img, sl=0.1, sh=0.2, rl=0.4):\n\n    h, w, c = img_size, img_size, 3\n    origin_area = tf.cast(h*w, tf.float32)\n\n    e_size_l = tf.cast(tf.round(tf.sqrt(origin_area * sl * rl)), tf.int32)\n    e_size_h = tf.cast(tf.round(tf.sqrt(origin_area * sh / rl)), tf.int32)\n\n    e_height_h = tf.minimum(e_size_h, h)\n    e_width_h = tf.minimum(e_size_h, w)\n\n    erase_height = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_height_h, dtype=tf.int32)\n    erase_width = tf.random.uniform(shape=[], minval=e_size_l, maxval=e_width_h, dtype=tf.int32)\n\n    erase_area = tf.zeros(shape=[erase_height, erase_width, c])\n    erase_area = tf.cast(erase_area, tf.uint8)\n\n    pad_h = h - erase_height\n    pad_top = tf.random.uniform(shape=[], minval=0, maxval=pad_h, dtype=tf.int32)\n    pad_bottom = pad_h - pad_top\n\n    pad_w = w - erase_width\n    pad_left = tf.random.uniform(shape=[], minval=0, maxval=pad_w, dtype=tf.int32)\n    pad_right = pad_w - pad_left\n\n    erase_mask = tf.pad([erase_area], [[0,0],[pad_top, pad_bottom], [pad_left, pad_right], [0,0]], constant_values=1)\n    erase_mask = tf.squeeze(erase_mask, axis=0)\n    erased_img = tf.multiply(tf.cast(img,tf.float32), tf.cast(erase_mask, tf.float32))\n\n    return tf.cast(erased_img, img.dtype)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Rotations : (Has issue needs to be fixed)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_rotation(image,label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = img_size\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image,tf.transpose(idx3))\n    print(d.shape)    \n    return tf.reshape(d,[DIM,DIM,3]),label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GridMask :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform(image, inv_mat, image_shape):\n    h, w, c = image_shape\n    cx, cy = w//2, h//2\n    new_xs = tf.repeat( tf.range(-cx, cx, 1), h)\n    new_ys = tf.tile( tf.range(-cy, cy, 1), [w])\n    new_zs = tf.ones([h*w], dtype=tf.int32)\n    old_coords = tf.matmul(inv_mat, tf.cast(tf.stack([new_xs, new_ys, new_zs]), tf.float32))\n    old_coords_x, old_coords_y = tf.round(old_coords[0, :] + w//2), tf.round(old_coords[1, :] + h//2)\n    clip_mask_x = tf.logical_or(old_coords_x<0, old_coords_x>w-1)\n    clip_mask_y = tf.logical_or(old_coords_y<0, old_coords_y>h-1)\n    clip_mask = tf.logical_or(clip_mask_x, clip_mask_y)\n    old_coords_x = tf.boolean_mask(old_coords_x, tf.logical_not(clip_mask))\n    old_coords_y = tf.boolean_mask(old_coords_y, tf.logical_not(clip_mask))\n    new_coords_x = tf.boolean_mask(new_xs+cx, tf.logical_not(clip_mask))\n    new_coords_y = tf.boolean_mask(new_ys+cy, tf.logical_not(clip_mask))\n    old_coords = tf.cast(tf.stack([old_coords_y, old_coords_x]), tf.int32)\n    new_coords = tf.cast(tf.stack([new_coords_y, new_coords_x]), tf.int64)\n    rotated_image_values = tf.gather_nd(image, tf.transpose(old_coords))\n    rotated_image_channel = list()\n    for i in range(c):\n        vals = rotated_image_values[:,i]\n        sparse_channel = tf.SparseTensor(tf.transpose(new_coords), vals, [h, w])\n        rotated_image_channel.append(tf.sparse.to_dense(sparse_channel, default_value=0, validate_indices=False))\n    return tf.transpose(tf.stack(rotated_image_channel), [1,2,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_rotate(image, angle, image_shape):\n    def get_rotation_mat_inv(angle):\n        # transform to radian\n        angle = math.pi * angle / 180\n        cos_val = tf.math.cos(angle)\n        sin_val = tf.math.sin(angle)\n        one = tf.constant([1], tf.float32)\n        zero = tf.constant([0], tf.float32)\n        rot_mat_inv = tf.concat([cos_val, sin_val, zero, -sin_val, cos_val, zero, zero, zero, one], axis=0)\n        rot_mat_inv = tf.reshape(rot_mat_inv, [3,3])\n        return rot_mat_inv\n    angle = float(angle) * tf.random.normal([1],dtype='float32')\n    rot_mat_inv = get_rotation_mat_inv(angle)\n    return transform(image, rot_mat_inv, image_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def GridMask(image_height, image_width, d1, d2, rotate_angle=1, ratio=0.5):\n    h, w = image_height, image_width\n    hh = int(np.ceil(np.sqrt(h*h+w*w)))\n    hh = hh+1 if hh%2==1 else hh\n    d = tf.random.uniform(shape=[], minval=d1, maxval=d2, dtype=tf.int32)\n    l = tf.cast(tf.cast(d,tf.float32)*ratio+0.5, tf.int32)\n\n    st_h = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n    st_w = tf.random.uniform(shape=[], minval=0, maxval=d, dtype=tf.int32)\n\n    y_ranges = tf.range(-1 * d + st_h, -1 * d + st_h + l)\n    x_ranges = tf.range(-1 * d + st_w, -1 * d + st_w + l)\n\n    for i in range(0, hh//d+1):\n        s1 = i * d + st_h\n        s2 = i * d + st_w\n        y_ranges = tf.concat([y_ranges, tf.range(s1,s1+l)], axis=0)\n        x_ranges = tf.concat([x_ranges, tf.range(s2,s2+l)], axis=0)\n\n    x_clip_mask = tf.logical_or(x_ranges < 0 , x_ranges > hh-1)\n    y_clip_mask = tf.logical_or(y_ranges < 0 , y_ranges > hh-1)\n    clip_mask = tf.logical_or(x_clip_mask, y_clip_mask)\n\n    x_ranges = tf.boolean_mask(x_ranges, tf.logical_not(clip_mask))\n    y_ranges = tf.boolean_mask(y_ranges, tf.logical_not(clip_mask))\n\n    hh_ranges = tf.tile(tf.range(0,hh), [tf.cast(tf.reduce_sum(tf.ones_like(x_ranges)), tf.int32)])\n    x_ranges = tf.repeat(x_ranges, hh)\n    y_ranges = tf.repeat(y_ranges, hh)\n\n    y_hh_indices = tf.transpose(tf.stack([y_ranges, hh_ranges]))\n    x_hh_indices = tf.transpose(tf.stack([hh_ranges, x_ranges]))\n\n    y_mask_sparse = tf.SparseTensor(tf.cast(y_hh_indices, tf.int64),  tf.zeros_like(y_ranges), [hh, hh])\n    y_mask = tf.sparse.to_dense(y_mask_sparse, 1, False)\n\n    x_mask_sparse = tf.SparseTensor(tf.cast(x_hh_indices, tf.int64), tf.zeros_like(x_ranges), [hh, hh])\n    x_mask = tf.sparse.to_dense(x_mask_sparse, 1, False)\n\n    mask = tf.expand_dims( tf.clip_by_value(x_mask + y_mask, 0, 1), axis=-1)\n\n    mask = random_rotate(mask, rotate_angle, [hh, hh, 1])\n    mask = tf.image.crop_to_bounding_box(mask, (hh-h)//2, (hh-w)//2, image_height, image_width)\n\n    return mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_grid_mask(image, image_shape, PROBABILITY = gridmask_rate):\n    AugParams = {\n        'd1' : 100,\n        'd2': 160,\n        'rotate' : 45,\n        'ratio' : 0.3\n    }\n    \n        \n    mask = GridMask(image_shape[0], image_shape[1], AugParams['d1'], AugParams['d2'], AugParams['rotate'], AugParams['ratio'])\n    if image_shape[-1] == 3:\n        mask = tf.concat([mask, mask, mask], axis=-1)\n        mask = tf.cast(mask,tf.float32)\n        P = tf.cast(tf.random.uniform([], 0, 1) <= PROBABILITY, tf.int32)\n    if P==1:\n        return image*mask\n    else:\n        return image\n\ndef gridmask(img_batch, label_batch):\n    return apply_grid_mask(img_batch, (img_size,img_size, 3)), label_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset=(\n    tf.data.Dataset\n    .from_tensor_slices((train_paths,train_labels.astype(np.float32)))\n    .map(decode_image , num_parallel_calls=AUTO)\n    .map(data_augment , num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n   )\n#train_dataset = train_dataset.map(cutmix,num_parallel_calls = AUTO)\n#train_dataset = train_dataset.map(transform_rotation, num_parallel_calls = AUTO)\n#train_dataset = train_dataset.map(gridmask, num_parallel_calls = AUTO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_train_data(train_paths,train_labels) :\n    \n    \n    train_dataset=(tf.data.Dataset\n    .from_tensor_slices((train_paths,train_labels.astype(np.float32)))\n    .map(decode_image,num_parallel_calls = AUTO)\n    .map(data_augment,num_parallel_calls = AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO))\n    \n    if cutmix_rate :\n        train_dataset = train_dataset.map(cutmix,num_parallel_calls = AUTO) \n    if mixup_rate : \n        train_dataset = train_dataset.map(mixup, num_parallel_calls = AUTO)\n    if rotation :\n        train_dataset = train_dataset.map(transform_rotation, num_parallel_calls = AUTO)\n    if blackout :\n        train_dataset = train_dataset.map(transform_random_blackout, num_parallel_calls = AUTO)\n    if gridmask_rate:\n        train_dataset =train_dataset.map(gridmask, num_parallel_calls=AUTO)    \n     \n    return train_dataset    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_validation_data(valid_paths,valid_labels) :\n    valid_data = (\n        tf.data.Dataset\n        .from_tensor_slices((valid_paths,valid_labels))\n        .map(decode_image, num_parallel_calls = AUTO)\n        .map(data_augment, num_parallel_calls= AUTO)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n        \n    ) \n    return valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset=(\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image ,num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_test_data(ordered=False) :\n    test_dataset=(\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image ,num_parallel_calls=AUTO)\n    .map(data_augment ,num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n     )\n    return test_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_start = 0.00001\n\nlr_max = 0.0001 * strategy.num_replicas_in_sync\nlr_min = 0.00001 \nlr_rampup_epochs = 15\nlr_sustain_epochs = 4\nlr_exp_decay = .8\n\n\ndef lrfn(epoch) :\n    if epoch < lr_rampup_epochs :\n        lr = lr_start + (lr_max-lr_min) / lr_rampup_epochs * epoch\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs :\n        lr = lr_max\n    else :\n        lr = lr_min + (lr_max - lr_min) * lr_exp_decay**(epoch - lr_sustain_epochs - lr_rampup_epochs)\n    return lr\n\nlr_callback = LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(rng,y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_start = 0.00001\n\nlr_max = 0.0001 * strategy.num_replicas_in_sync\nlr_min = 0.00001 \nlr_rampup_epochs = 13\nlr_sustain_epochs = 2\nlr_exp_decay = .8\n\n\ndef lrfn2(epoch) :\n    if epoch < lr_rampup_epochs :\n        lr = lr_start + (lr_max-lr_min) / lr_rampup_epochs * epoch\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs :\n        lr = lr_max\n    else :\n        lr = lr_min + (lr_max - lr_min) * lr_exp_decay**(epoch - lr_sustain_epochs - lr_rampup_epochs)\n    return lr\n\nlr_callback = LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\n\nfrom matplotlib import pyplot as plt\n\nplt.plot(rng,y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    base_model = efn.EfficientNetB7(weights='imagenet',\n                                    include_top = False,\n                                    pooling='avg',\n                                    input_shape=(img_size,img_size,3)\n                                   )\n    x = base_model.output\n    predictions = Dense(nb_classes,activation='softmax')(x)\n    return Model(inputs = base_model.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = efn.EfficientNetB7(weights='noisy-student',\n                                        include_top = False,\n                                        #pooling='avg',\n                                        input_shape=(img_size,img_size,3)\n                                       )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"focal_loss = True\nlabel_smoothing = 0\nSWA = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications import DenseNet121, DenseNet201\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications import MobileNet , MobileNetV2\nfrom tensorflow.keras.applications import InceptionResNetV2\nimport tensorflow.keras.layers as L\n\n\ndef get_model_generalized(name):\n    if name == 'EfficientNet7' :\n        base_model = efn.EfficientNetB7(weights='noisy-student',\n                                        include_top = False,\n                                        #pooling='avg',\n                                        input_shape=(img_size,img_size,3)\n                                       )\n        base_model.trainable = True\n        for layer in base_model.layers[:-20] :\n            layer.trainable = True\n    if name == 'EfficientNet3' :\n        base_model = efn.EfficientNetB3(weights='noisy-student',\n                                        include_top = False,\n                                        #pooling='avg',\n                                        input_shape=(img_size,img_size,3)\n                                       )\n        base_model.trainable = True\n        for layer in base_model.layers[:-25] :\n            layer.trainable = True        \n            \n    elif name == 'DenseNet' :\n        base_model = DenseNet201(weights='imagenet',include_top=False,input_shape=(img_size,img_size,3))\n        base_model.trainable = True\n        for layer in base_model.layers[:-25] :\n            layer.trainable = True\n    elif name == 'MobileNet' :\n        base_model = MobileNet(weights = 'imagenet', include_top=False,pooling='avg',input_shape=(img_size,img_size,3))\n    elif name == 'Inception' :\n        base_model = InceptionV3(weights = 'imagenet',include_top=False,pooling='avg',input_shape=(img_size,img_size,3))\n    elif name == 'ResNet' :\n        base_model = ResNet50(weights = 'imagenet',include_top=False,pooling='avg',input_shape=(img_size,img_size,3))\n    elif name == 'Incepresnet' :\n        base_model = InceptionResNetV2(weights = 'imagenet',include_top=False,input_shape=(img_size,img_size,3)) \n        base_model.trainable = True\n        for layer in base_model.layers[:-25] :\n            layer.trainable = True\n    x = base_model.output\n    x = L.GlobalAveragePooling2D()(x)\n    x = Dense(1024,activation='relu')(x)\n    x = Dropout(0.3)(x,training=True)\n    predictions = Dense(nb_classes,activation='softmax')(x)\n    model = Model(inputs = base_model.input, outputs=predictions) \n    \n    \n    if focal_loss : \n        loss= tfa.losses.SigmoidFocalCrossEntropy(reduction=tf.keras.losses.Reduction.AUTO)\n    elif label_smoothing :\n        loss=CategoricalCrossentropy(label_smoothing=label_smoothing)\n    else :\n        loss = 'categorical_cross_entropy'\n    if SWA :\n        opt = tf.keras.optimizers.Adam(lr=1e-5) # roll back\n        opt = tfa.optimizers.SWA(opt)\n    else :\n        opt = 'adam'\n        \n    model.compile(optimizer=opt,loss=loss,metrics=['accuracy',tf.keras.metrics.AUC()]) #roc_auc(),healthy_roc_auc(),multiple_diseases_roc_auc(),rust_roc_auc(),scab_roc_auc()\n    #model.compile(optimizer='adam',loss=categorical_focal_loss(gamma=2.0, alpha=0.25),metrics=['accuracy',tf.keras.metrics.AUC()])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DenseNet TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_dense = get_model_generalized(\"DenseNet\")\nmodel_dense.fit(\n    train2_dataset,\n    steps_per_epoch = train2_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dense.load_weights('best_model.h5')\npreds = model_dense.predict(test_dataset)\nsub.loc[:, 'healthy':] = preds\nsub.to_csv('submissionDenseNet.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TTA_NUM = 5\nprobabilities = []\nfor i in range(TTA_NUM):\n    print(f'TTA Number: {i}\\n')\n    test_ds = create_test_data(ordered=False) \n    probabilities.append(model_dense.predict(test_ds,verbose =1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntab_dense = np.zeros((len(probabilities[1]),4))\nfor i in range(0,len(probabilities[1])) :\n    for j in range(0,TTA_NUM) :\n        tab_dense[i] = tab_dense[i] + probabilities[j][i]\ntab_dense = tab_dense / TTA_NUM               \nsub.loc[:, 'healthy':] = tab_dense\nsub.to_csv('densenet+5TTA+pseudo+fl.csv', index=False)\nsub.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Inception TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_Inception = get_model_generalized(\"Inception\")\n'''model_Inception.fit(\n    train_dataset,\n    steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS \n)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_Inception.load_weights('best_model.h5')\npreds = model_Inception.predict(test_dataset)\nsub.loc[:, 'healthy':] = preds\nsub.to_csv('submissionInception.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MobileNet TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_mobilenet = get_model_generalized(\"MobileNet\")\n'''model_mobilenet.fit(\n    train_dataset,\n    steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS \n)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_mobilenet.load_weights('best_model.h5')\npreds = model_mobilenet.predict(test_dataset)\nsub.loc[:, 'healthy':] = preds\nsub.to_csv('submissionMobileNet.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNet TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_resnet = get_model_generalized(\"ResNet\")\n''' \nmodel_resnet.fit(\n    train_dataset,\n    steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS \n)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_resnet.load_weights('best_model.h5')\npreds = model_resnet.predict(test_dataset)\nsub.loc[:, 'healthy':] = preds\nsub.to_csv('submissionResnet.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# InceptionResNetV2 :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_incepresnet = get_model_generalized(\"Incepresnet\")\nmodel_incepresnet.fit(\n    train2_dataset,\n    steps_per_epoch = train2_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS \n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_incepresnet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_incepresnet.load_weights('best_model.h5')\npreds_incep = model_incepresnet.predict(test_dataset)\nsub.loc[:, 'healthy':] = preds_incep\nsub.to_csv('submissionIncepresnet.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_incepresnet.load_weights('best_model.h5')\nTTA_NUM = 5\nprobabilities = []\nfor i in range(TTA_NUM):\n    print(f'TTA Number: {i}\\n')\n    test_ds = create_test_data(ordered=False) \n    probabilities.append(model_incepresnet.predict(test_ds,verbose =1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntab_incep = np.zeros((len(probabilities[1]),4))\nfor i in range(0,len(probabilities[1])) :\n    for j in range(0,TTA_NUM) :\n        tab_incep[i] = tab_incep[i] + probabilities[j][i]\ntab_incep = tab_incep / TTA_NUM               \nsub.loc[:, 'healthy':] = tab_incep\nsub.to_csv('Incepresnet+5TTA+pseudolabeling+focalloss+0.4cutmix.csv', index=False)\nsub.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EfficienetB3 TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_effnet = get_model_generalized(\"EfficientNet3\")\n\nmodel_effnet.fit(\n    train_dataset,\n    steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.load_weights('best_model.h5')\npreds_eff = model_effnet.predict(test_dataset)\nsub.loc[:, 'healthy':] = preds_eff\nsub.to_csv('submissionEffnet3.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.load_weights('best_model.h5')\nTTA_NUM = 8\nprobabilities = []\nfor i in range(TTA_NUM):\n    print(f'TTA Number: {i}\\n')\n    test_ds = create_test_data(ordered=False) \n    probabilities.append(model_effnet.predict(test_ds,verbose =1))\nimport numpy as np\ntab = np.zeros((len(probabilities[1]),4))\nfor i in range(0,len(probabilities[1])) :\n    for j in range(0,TTA_NUM) :\n        tab[i] = tab[i] + probabilities[j][i]\ntab = tab / TTA_NUM              \nsub.loc[:, 'healthy':] = tab\nsub.to_csv('submissionEffnet3+TTA.csv', index=False)\nsub.head()        \ndel model_effnet\nimport gc\ngc.collect()   \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EfficientB7 TPU ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# First Training phase :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    model_effnet = get_model_generalized(\"EfficientNet7\")\nmodel_effnet.fit(\n    train_dataset,\n    steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Second Training Phase (pseudo labeling) :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pseudo = pd.read_csv('../input/heroseo-pseudo-labeling/train_pseudo_label_99999_v2.csv')\n\ntrain2_paths = train_pseudo.image_id.apply(lambda x : GCS_DS_PATH + '/images/' + x + '.jpg').values\ntrain2_labels = train_pseudo.loc[:,'healthy':].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2_dataset=(\n    tf.data.Dataset\n    .from_tensor_slices((train2_paths,train2_labels.astype(np.float32)))\n    .map(decode_image , num_parallel_calls=AUTO)\n    .map(data_augment , num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n   )\n#train2_dataset = train2_dataset.map(cutmix,num_parallel_calls = AUTO)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model_effnet.load_weights('best_model.h5')\n\nwith strategy.scope() :\n    model_effnet = get_model_generalized(\"EfficientNet7\")\nmodel_effnet.fit(\n    train2_dataset,\n    steps_per_epoch = train2_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"predictions with / without label smoothing :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ls = 0\nmodel_effnet.load_weights('best_model.h5')\npreds_eff = model_effnet.predict(test_dataset)\nif ls :\n    new_pred = LabelSmoothing(preds_eff, alpha=0.01)\n    sub.loc[:, 'healthy':] = preds_eff\n    sub.to_csv('EffnetB7+0.01LS+FL+Pseudo+swa.csv', index=False)\nelse :\n    sub.loc[:, 'healthy':] = preds_eff\n    sub.to_csv('EffnetB7+FL+Pseudo+swa.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.load_weights('best_model.h5')\nTTA_NUM = 5\nprobabilities = []\nfor i in range(TTA_NUM):\n    print(f'TTA Number: {i}\\n')\n    test_ds = create_test_data(ordered=False) \n    probabilities.append(model_effnet.predict(test_ds,verbose =1))\ntab1 = np.zeros((len(probabilities[1]),4))\nfor i in range(0,len(probabilities[1])) :\n    for j in range(0,TTA_NUM) :\n        tab1[i] = tab1[i] + probabilities[j][i]\ntab1 = tab1 / TTA_NUM    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if ls : #0.1,0.08 ,0.01\n    tab1 = LabelSmoothing(tab1, alpha=ls)\n    sub.loc[:, 'healthy':] = tab1           #.round().astype(int) rounding results\n    sub.to_csv('EffnetB7+5TTA+FL+LS+Pseudo+swa.csv', index=False)\nelse :\n    sub.loc[:, 'healthy':] = tab1           #.round().astype(int) rounding results\n    sub.to_csv('EffnetB7+5TTA+FL+Pseudo+swa.csv', index=False)\nsub.head()        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Validate :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_data = pd.read_csv('../input/validationforplantpathology/plants_val120_train1974.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def splitter(df):\n    train = df.index[~df.is_valid].tolist()\n    valid = df.index[df.is_valid].tolist()\n    return train, valid\n\ntrain,valid = splitter(valid_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_paths = valid_data.iloc[valid].image_id.apply(lambda x : GCS_DS_PATH + '/images/' + x + '.jpg').values\nvalid_labels = valid_data.iloc[valid].loc[:,'healthy':].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataset=(\n    tf.data.Dataset\n    .from_tensor_slices(valid_paths)\n    .map(decode_image ,num_parallel_calls=AUTO)\n    .map(data_augment ,num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n     )\n\npredictions = model_effnet.predict(valid_dataset,verbose =1)\npredictions = predictions.round().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_non_category = [ np.argmax(t) for t in valid_labels ]\ny_predict_non_category = [ np.argmax(t) for t in predictions ]\n\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test_non_category, y_predict_non_category)\n\n\nprint(conf_mat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model_effnet\nimport gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = (tab + tab1) / 2\nsub.to_csv('efficientnet3+7.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = (tab + tab_incep) / 2\nsub.to_csv('submissionEnsemblenet1.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = preds_eff*0.25  + preds_incep*0.75\nsub.to_csv('submissionEnsemblenet2.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.loc[:, 'healthy':] = tab*0.75  + tab_incep*0.25\nsub.to_csv('submissionEnsemblenet3.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nSEED = 13 \nprobs = []\nhistories = []\nFOLDS = 5\n\nkfolds = KFold(FOLDS , shuffle=True,random_state= SEED)\nfor i,(train_indices,valid_indices) in enumerate(kfolds.split(train_paths,train_labels)) :\n    print() ; print('#'*25)\n    print('Fold' , i+1)\n    print('#'*25)\n    \n    \n    trn_paths = train_paths[train_indices]\n    trn_labels = train_labels[train_indices]\n    \n    vld_paths = train_paths[valid_indices]\n    vld_labels = train_labels[valid_indices]\n    with strategy.scope() :\n        model_effnet = get_model_generalized(\"EfficientNet\")\n        history = model_effnet.fit(\n            create_train_data(trn_paths,trn_labels),\n            epochs = EPOCHS,\n            steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n            callbacks = [lr_callback,mc],\n            validation_data = create_validation_data(vld_paths,vld_labels),\n            verbose = 1\n        )\n                \n        model_effnet.load_weights('best_model.h5')\n        prob = model_effnet.predict(\n            create_test_data(ordered=False),\n            verbose = 1\n        )\n        \n        probs.append(prob)\n        histories.append(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob_sum = 0\nfor prob in probs :\n    prob_sum = prob_sum + prob\nprob_avg = prob_sum / FOLDS\n\nsub.loc[:, 'healthy':] = prob_avg\nsub.to_csv('submissionEffnet+5Kfolds.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pretraining using Plant Village Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os \n\npath = '../input/plantvillageapplecolor/Apple___'\nlabels = ['Apple_scab','Black_rot','Cedar_apple_rust','healthy']\nall_image_ids = []\nall_labels = []\n\nfor label in labels :\n    image_ids = os.listdir(path+label)\n    for i,image_id in enumerate(image_ids) :\n        image_ids[i] =  'Apple___'+label+'/'+image_id\n    labels = [label] * len(image_ids)\n    \n    all_image_ids = all_image_ids + image_ids\n    all_labels = all_labels + labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.DataFrame(\n    {\n    'id' : all_image_ids,\n    'label' : all_labels    \n    }\n) \ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train_df, pd.get_dummies(train_df.label)], axis=1)\ntrain_df.drop('label',axis=1,inplace=True)\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_classes = 4\nimg_size = 800\nEPOCHS = 30\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('plantvillageapplecolor')\n\n\ntrain_labels = train_df.loc[:,'Apple_scab':].values.astype(np.int64) #if we don't convert to int64 we will get tpu error because int8\n\n#train_labels = train_df[labels].values.astype(np.int64)\ntrain_paths = train_df.id.apply(lambda x: GCS_DS_PATH + '/' + x).values\npretraining_data = create_train_data(train_paths,train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_paths","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope() :\n    #model = get_model()\n    model_effnet = get_model_generalized(\"EfficientNet\")\n\nmodel_effnet.fit(\n    pretraining_data,\n    steps_per_epoch = train_labels.shape[0] // BATCH_SIZE,\n    callbacks = [lr_callback,mc],\n    epochs = EPOCHS )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.save('effnet_pretrained.h5')\n#clear_session()\n\n#del model_effnet\n#del train_dataset\n#del train_labels\n#del strategy\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n'''def get_pretrained_model():\n    base_model = load_model('./effnet_pretrained.h5', compile=False)\n    base_model.trainable = True\n    for layer in base_model.layers[:-20] :\n        layer.trainable = True\n    x = base_model.layers[-1].output    \n    predictions = Dense(nb_classes, activation=\"softmax\")(x)\n    return Model(inputs=base_model.input, outputs=predictions)'''\n\n\nwith strategy.scope():\n    model = get_pretrained_model()\n    \nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_effnet.fit(\n    train_dataset, \n    steps_per_epoch=train_labels.shape[0] // BATCH_SIZE,\n    callbacks=[lr_callback,mc],\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = model.predict(test_dataset)\nsub.loc[:, 'healthy':] = probs\nsub.to_csv('submissionPretrained.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TTA_NUM = 5\nprobabilities = []\nfor i in range(TTA_NUM):\n    #print(f'TTA Number: {i}\\n')\n    test_ds = create_test_data(ordered=False) \n    probabilities.append(model.predict(test_ds,verbose =1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ntab = np.zeros((len(probabilities[1]),4))\nfor i in range(0,len(probabilities[1])) :\n    for j in range(0,TTA_NUM) :\n        tab[i] = tab[i] + probabilities[j][i]\ntab = tab / TTA_NUM              \nsub.loc[:, 'healthy':] = tab\nsub.to_csv('submissionEffnet+TTA.csv', index=False)\nsub.head()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heroseo's Data :","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n\ntry :\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU :',tpu.master())\nexcept ValueError : \n    tpu = None\n    \nif tpu :\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse :\n    strategy = tf.distribute.get_strategy()\n    \nprint(\"Replicas :\", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('plantpathology-apple-dataset')  \npath1='../input/plant-pathology-2020-fgvc7/'\npath='../input/plantpathology-apple-dataset/'\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path1 + 'test.csv')\nsub = pd.read_csv(path1 + 'sample_submission.csv')\n\n\ntrain_paths = train.image_id.apply(lambda x : GCS_DS_PATH + '/images/' + x + '.jpg').values\ntest_paths = test.image_id.apply(lambda x : GCS_DS_PATH + '/images/' + x + '.jpg').values\n\n\ntrain_labels = train.loc[:,'healthy':].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}