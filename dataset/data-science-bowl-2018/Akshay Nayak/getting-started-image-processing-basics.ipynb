{"cells":[{"metadata":{"_uuid":"ac6dc7f127e670de9a7fcd0b6ea85b0a8f650773","_cell_guid":"56f4b2c4-85ea-4b7d-8aaf-815a57cec5a5"},"cell_type":"markdown","source":"In this notebook, I've tried to break down Stephen Bailey's fantastic notebook ( https://www.kaggle.com/stkbailey/teaching-notebook-for-total-imaging-newbies) so as to make the concepts simpler to understand.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d53a5b302633391627998ae196d937394eda80c1","_cell_guid":"ba2df6a1-00c4-4116-9678-af554db08837","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7a098aababfaf5745cf424f5e909118cb12b5d47","_cell_guid":"ec72492d-2e2d-4182-ae73-71764766b8c6","trusted":true},"cell_type":"code","source":"#Importing the other necessary libraries\nimport pathlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"866b93139279ee05b3e779c3fcb643a3f9435e47","_cell_guid":"14e23e72-e1e6-45e1-b670-876c10aa5320"},"cell_type":"markdown","source":"**Reading the image**","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"9a790c40bfa0a6d1031f5f9440ee00a2ca5c1318","_cell_guid":"f35830c2-58d5-4f44-a68d-688f6a3864bf","trusted":true},"cell_type":"code","source":"#Importing OpenCV - the computer vision library\nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e5098831accd95783c4b4349fbfec6fdbcf3664","_cell_guid":"bdb2e93f-3dc3-486e-9463-a6e471505800"},"cell_type":"markdown","source":"We will be using the same example. We can try out something different (i.e a different picture) in future work.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"7f441ecf3f5dddcf9c9fd6774f51889e9b965a02","_cell_guid":"b2a79966-b7cd-49b8-b7fd-1fcd795361e5","trusted":true},"cell_type":"code","source":"# Glob the training data and load a single image path\ntraining_paths = pathlib.Path('../input/stage1_train').glob('*/images/*.png')\ntraining_sorted = sorted([x for x in training_paths])\nim_path = training_sorted[45]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf81402e9ec3c1ec1169545e32fdcfd06a8c9874","_cell_guid":"faf9532f-eb8d-471c-86a8-5aad87925874","trusted":true},"cell_type":"code","source":"#To read the image \nbgrimg = cv2.imread(str(im_path))\nplt.imshow(bgrimg)\nplt.xticks([]) #To get rid of the x-ticks and y-ticks on the image axis\nplt.yticks([])\nprint('Original Image Shape',bgrimg.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e18f5a0fb0e741ce736e3b437bdb3c79141b8e6","_cell_guid":"305e16a1-0e36-4cb0-bdf5-ef6123ae315a","trusted":true},"cell_type":"code","source":"#To see the structure of the image let's display one row of the image matrix\nprint('The first row of the image matrix contains',len(bgrimg[1]),'pixels')\nprint(bgrimg[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0712c09106a4b8b5dfa820d091bdc19619ebfe06","_cell_guid":"88b34e10-5c12-4549-975c-8e7a7c461503"},"cell_type":"markdown","source":"The image has been read in the BGR colorspace. We have a third dimension as every pixel is represented by it's B, G and R components. This is the default colorpsace in which images are read in OpenCV. A particular BGR/RGB color space is defined by the three chromaticities of the red, green, and blue additive primaries, and can produce any chromaticity that is the triangle defined by those primary colors. In simpler terms - An RGB color can be understood by thinking of it as all possible colors that can be made from three colored lights for red, green, and blue. For more information : https://en.wikipedia.org/wiki/RGB_color_space","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d0dc0df53bfc0d5a54541abe38d53b726ed719f7"},"cell_type":"markdown","source":"## Basic Solution","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"dd8194b893e87d106edc3e8c4873447e1d18bbfd","_cell_guid":"5c703e75-c026-4595-9ad8-de31501ff817","trusted":true},"cell_type":"code","source":"#To transfrom the colorspace from BGR to grayscale so as to make things simpler\ngrayimg = cv2.cvtColor(bgrimg,cv2.COLOR_BGR2GRAY)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66057751d2f802290fd4cd541d9d1600362bd3cd","_cell_guid":"2931d307-8d6b-496f-87e5-f275c8af4247","trusted":true},"cell_type":"code","source":"#To plot the image\nplt.imshow(grayimg,cmap='gray') #cmap has been used as matplotlib uses some default colormap to plot grayscale images\nplt.xticks([]) #To get rid of the x-ticks and y-ticks on the image axis\nplt.yticks([])\nprint('New Image Shape',grayimg.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a7f578d464093474ade21b3dd58f02954640c65","_cell_guid":"8781f2ca-4599-4be5-97ba-113b5aa0d3fe"},"cell_type":"markdown","source":"It is important to understand the structure of the image here. We reduced a dimension when we transformed from the BGR colorspace to grayscale. Why did this happen? This is because grayscale is a range of monochromatic shades from black to white. Therefore, a grayscale image contains only shades of gray and no color (i.e it primarily contains only black and white). Transforming the colorspace removes all color information, leaving only the luminance of each pixel. Since digital images are displayed using a combination of red, green, and blue (RGB) colors, each pixel has three separate luminance values. Therefore, these three values must be combined into a single value when removing color from an image. Luminance can also be described as brightness or intensity, which can be measured on a scale from black (zero intensity) to white (full intensity)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"10db9741dff4415f0985c0c4d0546c59487379c9","_cell_guid":"265ec972-afbf-48b6-92de-2170725bc35e","trusted":true},"cell_type":"code","source":"#To understand this further, let's display one entire row of the image matrix\nprint('The first row of the image matrix contains',len(grayimg[1]),'pixels')\nprint(grayimg[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a8a9550e649df3c78468c9378dc47f1eba3980d","_cell_guid":"75156eeb-4ed6-412a-be55-b47fa413abed"},"cell_type":"markdown","source":"Thus this displays one entire row of the image matrix with the corresponding luminance or intensities of every pixel","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d8ee013744f82953ac1a11bfc90ff406c47c0ce8","_cell_guid":"48c826de-f6f3-499c-8794-9268cfdc3e65"},"cell_type":"markdown","source":"**Removing the background**","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b75cd7a4a1c9bc10ae7482fb1bd0e12047f32cc2","_cell_guid":"8d487379-ea6d-42d2-ae03-d52c135188fc","trusted":true},"cell_type":"code","source":"#Okay let's look at the distribution of the intensity values of all the pixels\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nsns.distplot(grayimg.flatten(),kde=False)#This is to flatten the matrix and put the intensity values of all the pixels in one single row vector\nplt.title('Distribution of intensity values')\n\n#To zoom in on the distribution and see if there is more than one prominent peak \nplt.subplot(1,2,2)\nsns.distplot(grayimg.flatten(),kde=False) \nplt.ylim(0,30000) \nplt.title('Distribution of intensity values (Zoomed In)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f04d335a4111284e0e83aeada76e54cb12989f5","_cell_guid":"b8ebb992-1d69-4b23-8210-3de46ba99ac4"},"cell_type":"markdown","source":"We can see that there are 2 prominent peaks. The count of pixels with intensity values around 0 is extrememly high (250000). We would expect this to occur as the nuclei cover a smaller portion of the picture as compared to the background which is primarily black. Our job here is to seperate the two, that is, seperate the nuclei from the background. The optimal seperation value is somewhere around 20 but rather than relying on such descriptive statistics, we should take a more formal approach such as using Otsu's method.\nOtsu's method, named after Nobuyuki Otsu is used to automatically perform clustering-based image thresholding, or, the reduction of a graylevel image to a binary image. The algorithm assumes that the image contains two classes of pixels following bi-modal histogram (foreground pixels and background pixels), it then calculates the optimum threshold separating the two classes so that their combined spread (intra-class variance) is minimal, or equivalently, so that their inter-class variance is maximal. Otsuâ€™s method exhibits relatively good performance if the histogram can be assumed to have bimodal distribution and assumed to possess a deep and sharp valley between two peaks (source : https://en.wikipedia.org/wiki/Otsu%27s_method) ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8de96e891bccdf041636785699450acdbfe62589","_cell_guid":"f0818f3d-09e3-42f4-a957-e71113eac096","trusted":true},"cell_type":"code","source":"from skimage.filters import threshold_otsu\nthresh_val = threshold_otsu(grayimg)\nprint('The optimal seperation value is',thresh_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ba94cc2e0254cd275603f0b1d8902567e9fe11d","_cell_guid":"3e69d253-0073-48a4-b9f2-b2a1afe9ee9c"},"cell_type":"markdown","source":"Now we'll use the np.where function to encode all pixels with an intensity value > the threshold value as 1 and all other pixels as 0.  The result of this function will be stored in a variable called mask","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4bf0da02a6151ab1665172a7237792ebc32daf48","_cell_guid":"9184b1f2-6813-482b-8610-80950e725591","trusted":true},"cell_type":"code","source":"mask=np.where(grayimg>thresh_val,1,0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b76c8ad7fd1bad64d5b467a7902149cd193c5067","_cell_guid":"efb88dac-e47f-407f-b148-69d799cc6c24","trusted":true},"cell_type":"code","source":"#To plot the original image and mask side by side\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.imshow(grayimg,cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(1,2,2)\nmaskimg = mask.copy()\nplt.imshow(maskimg, cmap='viridis')\nplt.title('Mask')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8265549971d112680dd79672046a4c19ffd530e2","_cell_guid":"6f4c74fc-97a2-4c0a-b0ee-64ad72a5c96a"},"cell_type":"markdown","source":"We see that the mask has done a decent job. If these images were to appear in a newspaper column titled 'Spot the difference between' (except the obvious colour difference), it would have had people scratch their heads in frustration. However a more careful look suggests that the mask hasn't found out all the nuclei, especially the two in the top right corner. Around the (500,400) mark, the three nuclei have been all combined together to form one cluster. The darker coloured nuclei are causing a problem as the pixels that represent these nuclei have intensity values lesser than Otsu's threshold value.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"75629c529dde9b68bc99679428a8909222baf7fc","_cell_guid":"fcd0e6d4-526c-4faf-ba8e-7965c9482905","trusted":true},"cell_type":"code","source":"#Let's see if K-Means does a good job on this data \nfrom sklearn.cluster import KMeans\nkmeans=KMeans(n_clusters=2) #2 as we're still trying to seperate the lighter coloured nuclei from the darker coloured background \nkmeans.fit(grayimg.reshape(grayimg.shape[0]*grayimg.shape[1],1))\n\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nplt.imshow(kmeans.labels_.reshape(520,696),cmap='magma')\nplt.title('K-Means')\n\nplt.subplot(1,2,2)\nplt.imshow(maskimg, cmap='viridis')\nplt.title('Mask with Otsu Seperation')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9ecf81bfb3933e60b722da553f92c1a071c4437","_cell_guid":"2af4d70e-b5c9-4279-8c58-6a835e6d3fd9"},"cell_type":"markdown","source":"It's extrememly hard to tell if there's a difference. Let's see if there is any difference by comparing the labels of Otsu and K-Means at a pixel level, summing over the booleans and dividing them by the total number of pixels in the image. If the result is 1, it means there is no difference at all","outputs":[],"execution_count":null},{"metadata":{"_uuid":"fd8b06ba6f9d338268d10890b1e25c7d16b4772f","_cell_guid":"6f7bbd23-9252-4df8-b083-11a008ba1d27","trusted":true},"cell_type":"code","source":"#To check if there's any difference\nsum((kmeans.labels_.reshape(520,696)==mask).flatten())/(mask.shape[0]*mask.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c736c738012429044f8c874b323508c9f33b49ed","_cell_guid":"d341e110-ed5b-4d9e-9406-e94a7e554063"},"cell_type":"markdown","source":"There is no difference at all. For a deeper explanantion as to why this could have happened, one may read D Liu's paper (http://ieeexplore.ieee.org/document/5254345/?reload=true) where he has compared K-Means with Otsu's method ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"46318c8debbdae699a5e18b2faf40c2289e64b4b","_cell_guid":"9d7a7185-3199-4d39-94e4-d3bef71d30f4"},"cell_type":"markdown","source":"**Object identification**","outputs":[],"execution_count":null},{"metadata":{"_uuid":"3b18bf8faca0a2cc2ee5d168db7fd270c30119d1","_cell_guid":"2e1c6030-f2f3-4ba1-a294-69edfd380c02"},"cell_type":"markdown","source":"To get a count of the total number of nuclei, we can use the ndimage.label function which labels features (pixels) in an array based on their interconnectedness. So for example if [1 1 1 0 0 1 1] was our row vector, using ndimage.label on this would give us [1 1 1 0 0 2 2] signifying the fact that there are 2 distinct objects in the row vector. The function returns the labeled array and the number of distinct objects it found in the array.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"4598f6919fdfdffbe6dfaa6076298327da956156","_cell_guid":"6f1bd8ad-111c-4979-b11f-eacf5c5405f3","trusted":true},"cell_type":"code","source":"from scipy import ndimage\nlabels,nlabels=ndimage.label(mask)\nprint('There are',nlabels,'distinct nuclei in the mask.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bba8540ae572b47313acfa27dccd8268dd5eaab1","_cell_guid":"c99ccc4c-dd14-4d3f-8cd4-fd074b2c5f8d"},"cell_type":"markdown","source":"Now, there could be more nuclei than that as some nuclei have been combined into one and our mask hasn't been able to identify all the nuclei, especially the ones in the top right corner. However the 2 seperate spots in the top right corner get labelled as 2 different objects.\n\nAll in all the two major problems in this image are:\n- Insignificant spots/dots being labelled as nuclei. These spots should have their labels (KMeans, Otsu) set to 0 if their sizes are too small. This problem has been caused by some nuclei that have pixels where the intensity values are lesser than Otsu's threshold value, thus causing only some pixels to have their label encoded as 1.\n- The nuclei that are closer to one another get clustered to form one nuclei. So we need to seperate them using some edge detection algorithm (like convolution with a sobel filter or canny edge detector as suggested by Ramsu)\n\nNow for this competition we need to have a seperate mask for every nucleus. In the file named 'stage1_train_labels.csv.zip', we have the image IDs in one column and the Run Length Encoded (RLE) vector for one such mask (i.e for one nucleus) in the other column.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"67e7111134f58789ab574cf4dfe22102fb865edd","_cell_guid":"9504ab83-eca1-4c45-9e71-2c9ae9da9b3d","trusted":true},"cell_type":"code","source":"#Since we need to create a seperate mask for every nucelus, let's store the masks in an iterable like a list \nlabel_array=[]\n#We need to iterate from 1 as ndimage.label encodes every object starting from number 1\nfor i in range(1,nlabels+1):\n    label_mask = np.where(labels==i,1,0)\n    label_array.append(label_mask)\n#To see one such mask\nlabel_array[68]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ce13bd0e86e50985cc087d3d14a1c36bc97f356","_cell_guid":"4f217479-ec77-4173-9e24-45223a05cb9f"},"cell_type":"markdown","source":"The 1s represent 1 such object (nucleus) in the entire picture.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2fb9d567c5857f654105f17bcc70746fc6434aeb","_cell_guid":"18cc602b-d815-4667-b9dc-aa8e142adef1"},"cell_type":"markdown","source":"**Run Length Encoding**","outputs":[],"execution_count":null},{"metadata":{"_uuid":"be9c57f91bc79ba366de0d3bc56795df2877c88a","_cell_guid":"2a136957-b22a-4486-a9db-be863edbb201"},"cell_type":"markdown","source":" Every mask for every nucleus requires an RLE vector. This is the format required by the competition. \n\nWhat is RLE?\n\nRLE or Run Length Encoding converts a matrix into a vector and returns the position/starting point of the first pixel from where we observe an object (identified by a 1) and gives us a count of how many pixels from that pixel we see the series of 1s. In the ndimage.label function example of [1 1 1 0 0 1 1], running RLE would give us 0 3 5 2, which means 3 pixels from the zeroth pixel (inclusive) and 2 pixels from the 5th pixel we see a series of 1s","outputs":[],"execution_count":null},{"metadata":{"_uuid":"365620aa033f4637ef1b80177c73a9c06996f2da","_cell_guid":"e31bff7f-f58d-4795-9e46-ca2ee4b02b07","trusted":true,"collapsed":true},"cell_type":"code","source":"#Function for rle encoding\ndef rle(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return \" \".join([str(i) for i in run_lengths])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f95f3ec367c7326316609b4642ceb076fcb878c","_cell_guid":"4c53cc55-d5fe-4089-ad36-49d7e0096634"},"cell_type":"markdown","source":"Credit to Kaggle user rahlkin https://www.kaggle.com/rakhlin/fast-run-length-encoding-python for developing this function that has been used by many Kagglers for the purpose of this competition.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5f60eb923ec64626269ec40f373a6b3c52d6d7b4","_cell_guid":"9876cdda-4f07-4e16-9a73-74118a95bd53","trusted":true},"cell_type":"code","source":"#Running RLE on the last label_mask in label_array gives us \nrle(label_mask)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65b36ed87f4458f1171f6844c0c132c8b53aeb2b"},"cell_type":"markdown","source":"**Putting everything together**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"91c62bdfd3a8253b606eb0516fce7cd0269957f8"},"cell_type":"code","source":"im_path.parts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ddd31fa3f5bf2fc4ec9aa10a23cccb9e66898fdd"},"cell_type":"code","source":"#Now defining a function that does is applicable to all images\ndef basic(im_path):\n    #Reading the image\n    im_id=im_path.parts[-3] #To extract the image ID\n    bgr = cv2.imread(str(im_path)) #Reading it in OpenCV\n    gray = cv2.cvtColor(bgr,cv2.COLOR_BGR2GRAY) #Converting everything to grayscale from BGR\n\n    #To remove the background\n    thresh_val = threshold_otsu(gray) #Using Otsu's method to seperate the foreground objects from the background\n    mask = np.where(gray > thresh_val, 1, 0) #Coding objects with intensity values higher than background as 1\n    \n    #Extracting connected objects\n    test_rle=pd.DataFrame()\n    labels, nlabels = ndimage.label(mask) #labels gives us the label of the different objects in every image starting from 1 and nlabels gives us the total number of objects in every image\n    for i in range(1,nlabels+1): #Iterating through every object/label\n        label_mask = np.where(labels==i,1,0) #Individual masks for every nucleus\n        RLE = rle(label_mask) #RLE for every mask\n        solution = pd.Series({'ImageId': im_id, 'EncodedPixels': RLE})\n        test_rle = test_rle.append(solution, ignore_index=True)\n    \n    #Return the dataframe\n    return(test_rle)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3711f2ebd7922078f393c91daa17e05a08f5b2b2"},"cell_type":"code","source":"#Defining a function that takes a list of image paths (pathlib.Path objects), analyzes each and returns a submission ready DataFrame\ndef list_of_images(im_path_list):\n    all_df = pd.DataFrame()\n    for im_path in im_path_list: #We'll use this for the test images\n        im_df = basic(im_path) #Creating one dataframe for every image \n        all_df = all_df.append(im_df, ignore_index=True) #Appending all these dataframes\n    \n    #Returing the submission ready dataframe\n    return (all_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5d1b1bb6803bb0047f9f3463aae1c2c74143a53b"},"cell_type":"code","source":"#Final submission\ntest_images = pathlib.Path('../input/stage1_test/').glob('*/images/*.png')\nbasic_solution = list_of_images(list(test_images))\nbasic_solution.to_csv('basic_solution.csv', index=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03ebb84cd739c922354abbc3881ef8a739b12951","_cell_guid":"65eceb60-5e1f-4554-83f6-249d33b8d9a8"},"cell_type":"markdown","source":"The submission scored 0.201 which gives us our baseline accuracy. Any layer of complexity that we add onto this should better this score, failing which it is absolutely useless.\n\nSome important questions to ask are:\n\n- Will we achieve a satisfactory performance by converting all pictures to grayscale? What are the different types of pictures in the dataset?\n- What are the numerous ways to seperate the background from objects of interest? Otsu's method requires computing a graylevel histogram for us to find the optimum seperation value. In that respect KMeans may work better on images that aren't particularly grayscale or on images where there is no sharp contrast in the intensity values between objects of interest and the background\n- What are some useful edge detection algorithms to create boundaries between nuclei that are extremely close to one another?\n- How do we as humans identify objects in an image? We indeed take it for granted but if we think of objects as anything that has a fixed shape and size and is prominent with respect to the background, what is the technical (or computer) definition of these terms?","outputs":[],"execution_count":null}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}