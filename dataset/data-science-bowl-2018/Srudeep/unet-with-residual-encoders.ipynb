{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unziping Dataset","metadata":{}},{"cell_type":"code","source":"from zipfile import ZipFile  \nfile_name = \"../input/data-science-bowl-2018/stage1_train.zip\" \nwith ZipFile(file_name, 'r') as zip: \n    print('Extracting the files') \n    zip.extractall(\"stage1_train\") \n    print('Done!')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:09.891997Z","iopub.execute_input":"2022-02-04T05:09:09.89253Z","iopub.status.idle":"2022-02-04T05:09:16.333182Z","shell.execute_reply.started":"2022-02-04T05:09:09.892438Z","shell.execute_reply":"2022-02-04T05:09:16.332475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading and Importing all necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install albumentations==0.4.6","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:16.334737Z","iopub.execute_input":"2022-02-04T05:09:16.334983Z","iopub.status.idle":"2022-02-04T05:09:27.991546Z","shell.execute_reply.started":"2022-02-04T05:09:16.33495Z","shell.execute_reply":"2022-02-04T05:09:27.990603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To check whether the installation is successfull or not\nimport albumentations\nalbumentations.__version__","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:27.993145Z","iopub.execute_input":"2022-02-04T05:09:27.993409Z","iopub.status.idle":"2022-02-04T05:09:29.757964Z","shell.execute_reply.started":"2022-02-04T05:09:27.993372Z","shell.execute_reply":"2022-02-04T05:09:29.757228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport copy\nfrom collections import defaultdict\nimport torch\nimport shutil\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, utils\nfrom torch import nn\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\n#import tqdm as tqdm\nfrom tqdm import tqdm as tqdm\n\nfrom albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\nimport cv2\n\nfrom torch.autograd import Variable\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nfrom torch.optim import Adam, SGD\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torch import nn\nimport zipfile\n\nimport random\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:29.759992Z","iopub.execute_input":"2022-02-04T05:09:29.760315Z","iopub.status.idle":"2022-02-04T05:09:31.511533Z","shell.execute_reply.started":"2022-02-04T05:09:29.760278Z","shell.execute_reply":"2022-02-04T05:09:31.510684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = 'stage1_train/'","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.51483Z","iopub.execute_input":"2022-02-04T05:09:31.515164Z","iopub.status.idle":"2022-02-04T05:09:31.520807Z","shell.execute_reply.started":"2022-02-04T05:09:31.515135Z","shell.execute_reply":"2022-02-04T05:09:31.520136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lenData = len(os.listdir(TRAIN_PATH))\nprint(\"Total Number of Images: {}\".format(lenData))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.522578Z","iopub.execute_input":"2022-02-04T05:09:31.523278Z","iopub.status.idle":"2022-02-04T05:09:31.531166Z","shell.execute_reply.started":"2022-02-04T05:09:31.523239Z","shell.execute_reply":"2022-02-04T05:09:31.53013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"code","source":"#Albumentation\ndef get_train_transform():\n   return A.Compose(\n       [\n        A.Resize(256, 256),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        A.HorizontalFlip(p=0.25),\n        A.VerticalFlip(p=0.25),\n        ToTensor()\n        ])\n   \n#Dataset Loader\nclass LoadDataSet(Dataset):\n        def __init__(self,path, transform=None):\n            self.path = path\n            self.folders = os.listdir(path)\n            self.transforms = get_train_transform()\n        \n        def __len__(self):\n            return len(self.folders)\n              \n        \n        def __getitem__(self,idx):\n            image_folder = os.path.join(self.path,self.folders[idx],'images/')\n            mask_folder = os.path.join(self.path,self.folders[idx],'masks/')\n            image_path = os.path.join(image_folder,os.listdir(image_folder)[0])\n            \n            img = io.imread(image_path)[:,:,:3].astype('float32')\n            img = transform.resize(img,(128,128))\n            \n            mask = self.get_mask(mask_folder, 128, 128 ).astype('float32')\n\n            augmented = self.transforms(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n            mask = mask[0].permute(2, 0, 1)\n            return (img,mask) \n\n\n        def get_mask(self,mask_folder,IMG_HEIGHT, IMG_WIDTH):\n            mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n            for mask_ in os.listdir(mask_folder):\n                    mask_ = io.imread(os.path.join(mask_folder,mask_))\n                    mask_ = transform.resize(mask_, (IMG_HEIGHT, IMG_WIDTH))\n                    mask_ = np.expand_dims(mask_,axis=-1)\n                    mask = np.maximum(mask, mask_)\n              \n            return mask","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.532894Z","iopub.execute_input":"2022-02-04T05:09:31.533149Z","iopub.status.idle":"2022-02-04T05:09:31.549205Z","shell.execute_reply.started":"2022-02-04T05:09:31.533117Z","shell.execute_reply":"2022-02-04T05:09:31.548521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = LoadDataSet(TRAIN_PATH, transform=get_train_transform())","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.550798Z","iopub.execute_input":"2022-02-04T05:09:31.551567Z","iopub.status.idle":"2022-02-04T05:09:31.558775Z","shell.execute_reply.started":"2022-02-04T05:09:31.551523Z","shell.execute_reply":"2022-02-04T05:09:31.55806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the shape of image and mask\nimage, mask = train_dataset.__getitem__(0)\nprint(image.shape)\nprint(mask.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.559706Z","iopub.execute_input":"2022-02-04T05:09:31.561664Z","iopub.status.idle":"2022-02-04T05:09:31.658152Z","shell.execute_reply.started":"2022-02-04T05:09:31.56162Z","shell.execute_reply":"2022-02-04T05:09:31.657412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.__len__()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.661088Z","iopub.execute_input":"2022-02-04T05:09:31.66186Z","iopub.status.idle":"2022-02-04T05:09:31.667323Z","shell.execute_reply.started":"2022-02-04T05:09:31.661815Z","shell.execute_reply":"2022-02-04T05:09:31.666517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.668889Z","iopub.execute_input":"2022-02-04T05:09:31.6696Z","iopub.status.idle":"2022-02-04T05:09:31.728785Z","shell.execute_reply.started":"2022-02-04T05:09:31.66956Z","shell.execute_reply":"2022-02-04T05:09:31.728013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.730099Z","iopub.execute_input":"2022-02-04T05:09:31.730378Z","iopub.status.idle":"2022-02-04T05:09:31.738768Z","shell.execute_reply.started":"2022-02-04T05:09:31.730331Z","shell.execute_reply":"2022-02-04T05:09:31.737374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_image(img):\n    \"\"\"\n    Function to de-standardize the image\n    arg: Input Image\n    returns: De-standardized image\n    \"\"\"\n    img = np.array(np.transpose(img, (1,2,0)))\n    mean = np.array((0.485, 0.456, 0.406))\n    std = np.array((0.229, 0.224, 0.225))\n    img  = std * img + mean\n    img = img*255\n    img = img.astype(np.uint8)\n    return img\n\ndef format_mask(mask):\n    \"\"\"\n    Function to reshape the mask\n    \"\"\"\n    mask = np.squeeze(np.transpose(mask, (1,2,0)))\n    return mask","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.74018Z","iopub.execute_input":"2022-02-04T05:09:31.742489Z","iopub.status.idle":"2022-02-04T05:09:31.748834Z","shell.execute_reply.started":"2022-02-04T05:09:31.742452Z","shell.execute_reply":"2022-02-04T05:09:31.748059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_dataset(n_images, predict=None):\n  \"\"\"\n  Function to visualize images and masks\n  \"\"\"\n  maxRange = train_dataset.__len__()\n  images = random.sample(range(0, maxRange), n_images)\n  figure, ax = plt.subplots(nrows=len(images), ncols=2, figsize=(5, 8))\n  print(images)\n  for i in range(0, len(images)):\n    img_no = images[i]\n    image, mask = train_dataset.__getitem__(img_no)\n    image = format_image(image)\n    mask = format_mask(mask)\n    ax[i, 0].imshow(image)\n    ax[i, 1].imshow(mask, interpolation=\"nearest\", cmap=\"gray\")\n    ax[i, 0].set_title(\"Ground Truth Image\")\n    ax[i, 1].set_title(\"Mask\")\n    ax[i, 0].set_axis_off()\n    ax[i, 1].set_axis_off()\n  plt.tight_layout()\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.750267Z","iopub.execute_input":"2022-02-04T05:09:31.750835Z","iopub.status.idle":"2022-02-04T05:09:31.760823Z","shell.execute_reply.started":"2022-02-04T05:09:31.750796Z","shell.execute_reply":"2022-02-04T05:09:31.760021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:31.762411Z","iopub.execute_input":"2022-02-04T05:09:31.76297Z","iopub.status.idle":"2022-02-04T05:09:32.45684Z","shell.execute_reply.started":"2022-02-04T05:09:31.762933Z","shell.execute_reply":"2022-02-04T05:09:32.456182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Split train and validation set.\ntrain_data, valid_data = random_split(train_dataset, [469, 201])\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nval_loader = DataLoader(dataset=valid_data, batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:32.457874Z","iopub.execute_input":"2022-02-04T05:09:32.458224Z","iopub.status.idle":"2022-02-04T05:09:32.467359Z","shell.execute_reply.started":"2022-02-04T05:09:32.458191Z","shell.execute_reply":"2022-02-04T05:09:32.466637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Model - Unet with residual encoders","metadata":{}},{"cell_type":"code","source":"class ResidualConv(nn.Module):\n    def __init__(self, input_dim, output_dim, stride, padding):\n        super(ResidualConv, self).__init__()\n\n        self.conv_block = nn.Sequential(\n            nn.BatchNorm2d(input_dim),\n            nn.ReLU(),\n            nn.Conv2d(\n                input_dim, output_dim, kernel_size=3, stride=stride, padding=padding\n            ),\n            nn.BatchNorm2d(output_dim),\n            nn.ReLU(),\n            nn.Conv2d(output_dim, output_dim, kernel_size=3, padding=1),\n        )\n        self.conv_skip = nn.Sequential(\n            nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(output_dim),\n        )\n\n    def forward(self, x):\n\n        return self.conv_block(x) + self.conv_skip(x)\n\n\nclass Upsample(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel, stride):\n        super(Upsample, self).__init__()\n\n        self.upsample = nn.ConvTranspose2d(\n            input_dim, output_dim, kernel_size=kernel, stride=stride\n        )\n\n    def forward(self, x):\n        return self.upsample(x)\n\n\nclass ResUnet(nn.Module):\n    def __init__(self, channel, filters=[16, 32, 64, 128, 256, 512, 1024]):\n        super(ResUnet, self).__init__()\n\n        self.input_layer = nn.Sequential(\n            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1),\n            nn.BatchNorm2d(filters[0]),\n            nn.ReLU(),\n            nn.Conv2d(filters[0], filters[0], kernel_size=3, padding=1),\n        )\n        self.input_skip = nn.Sequential(\n            nn.Conv2d(channel, filters[0], kernel_size=3, padding=1)\n        )\n\n        self.residual_conv_1 = ResidualConv(filters[0], filters[1], 2, 1)\n        self.residual_conv_2 = ResidualConv(filters[1], filters[2], 2, 1)\n        self.residual_conv_3 = ResidualConv(filters[2], filters[3], 2, 1)\n        self.residual_conv_4 = ResidualConv(filters[3], filters[4], 2, 1)\n        self.residual_conv_5 = ResidualConv(filters[4], filters[5], 2, 1)\n\n        self.bridge = ResidualConv(filters[5], filters[6], 2, 1)\n\n        self.upsample_1 = Upsample(filters[6], filters[6], 2, 2)\n        self.up_residual_conv1 = ResidualConv(filters[6] + filters[5], filters[5], 1, 1)\n\n        self.upsample_2 = Upsample(filters[5], filters[5], 2, 2)\n        self.up_residual_conv2 = ResidualConv(filters[5] + filters[4], filters[4], 1, 1)\n\n        self.upsample_3 = Upsample(filters[4], filters[4], 2, 2)\n        self.up_residual_conv3 = ResidualConv(filters[4] + filters[3], filters[3], 1, 1)\n\n        self.upsample_4 = Upsample(filters[3], filters[3], 2, 2)\n        self.up_residual_conv4 = ResidualConv(filters[3] + filters[2], filters[2], 1, 1)\n\n        self.upsample_5 = Upsample(filters[2], filters[2], 2, 2)\n        self.up_residual_conv5 = ResidualConv(filters[2] + filters[1], filters[1], 1, 1)\n\n        self.upsample_6 = Upsample(filters[1], filters[1], 2, 2)\n        self.up_residual_conv6 = ResidualConv(filters[1] + filters[0], filters[0], 1, 1)\n\n        self.output_layer = nn.Sequential(\n            nn.Conv2d(filters[0], 1, 1, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        # Encode\n        x1 = self.input_layer(x) + self.input_skip(x)\n        x2 = self.residual_conv_1(x1)\n        x3 = self.residual_conv_2(x2)\n        x4 = self.residual_conv_3(x3)\n        x5 = self.residual_conv_4(x4)\n        x6 = self.residual_conv_5(x5)\n        # Bridge\n        x7 = self.bridge(x6)\n        # Decode\n        x7 = self.upsample_1(x7)\n        x8 = torch.cat([x7, x6], dim=1)\n\n        x9 = self.up_residual_conv1(x8)\n        \n        x9 = self.upsample_2(x9)\n        x10 = torch.cat([x9, x5], dim=1)\n\n        x11 = self.up_residual_conv2(x10)\n        \n        x11 = self.upsample_3(x11)\n        x12 = torch.cat([x11, x4], dim=1)\n\n        x13 = self.up_residual_conv3(x12)\n\n        x13 = self.upsample_4(x13)\n        x14 = torch.cat([x13, x3], dim=1)\n\n        x15 = self.up_residual_conv4(x14)\n\n        x15 = self.upsample_5(x15)\n        x16 = torch.cat([x15, x2], dim=1)\n\n        x17 = self.up_residual_conv5(x16)\n\n        x17 = self.upsample_6(x17)\n        x18 = torch.cat([x17, x1], dim=1)\n\n        x19 = self.up_residual_conv6(x18)\n\n        output = self.output_layer(x19)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:32.469022Z","iopub.execute_input":"2022-02-04T05:09:32.469363Z","iopub.status.idle":"2022-02-04T05:09:32.49768Z","shell.execute_reply.started":"2022-02-04T05:09:32.469308Z","shell.execute_reply":"2022-02-04T05:09:32.496757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Loss function","metadata":{}},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice\n\n\nclass IoU(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoU, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)/(union + smooth)\n                \n        return IoU","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:32.50049Z","iopub.execute_input":"2022-02-04T05:09:32.500676Z","iopub.status.idle":"2022-02-04T05:09:32.511617Z","shell.execute_reply.started":"2022-02-04T05:09:32.500654Z","shell.execute_reply":"2022-02-04T05:09:32.510663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResUnet(3).cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr = 1e-3)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:32.512793Z","iopub.execute_input":"2022-02-04T05:09:32.513698Z","iopub.status.idle":"2022-02-04T05:09:36.147612Z","shell.execute_reply.started":"2022-02-04T05:09:32.513658Z","shell.execute_reply":"2022-02-04T05:09:36.146872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_ckp(state, is_best, checkpoint_path, best_model_path):\n    \"\"\"\n    state: checkpoint we want to save\n    is_best: is this the best checkpoint; min validation loss\n    checkpoint_path: path to save checkpoint\n    best_model_path: path to save best model\n    \"\"\"\n    f_path = checkpoint_path\n    # save checkpoint data to the path given, checkpoint_path\n    torch.save(state, f_path)\n    # if it is a best model, min validation loss\n    if is_best:\n        best_fpath = best_model_path\n        # copy that checkpoint file to best path given, best_model_path\n        shutil.copyfile(f_path, best_fpath)\n\ndef load_ckp(checkpoint_fpath, model, optimizer):\n    \"\"\"\n    checkpoint_path: path to save checkpoint\n    model: model that we want to load checkpoint parameters into       \n    optimizer: optimizer we defined in previous training\n    \"\"\"\n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['state_dict'])\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    valid_loss_min = checkpoint['valid_loss_min']\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:36.14877Z","iopub.execute_input":"2022-02-04T05:09:36.149013Z","iopub.status.idle":"2022-02-04T05:09:36.158375Z","shell.execute_reply.started":"2022-02-04T05:09:36.14898Z","shell.execute_reply":"2022-02-04T05:09:36.157668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating model weight directory\nweight_path = \"model_weight\"\nif not os.path.exists(weight_path):\n    os.makedirs(weight_path)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:36.159574Z","iopub.execute_input":"2022-02-04T05:09:36.159912Z","iopub.status.idle":"2022-02-04T05:09:36.168266Z","shell.execute_reply.started":"2022-02-04T05:09:36.159866Z","shell.execute_reply":"2022-02-04T05:09:36.167604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"#from engine import evaluate\ncriterion = DiceLoss()\naccuracy_metric = IoU()\nnum_epochs=20\nvalid_loss_min = np.Inf\n\ncheckpoint_path = 'model_weight/chkpoint_'\nbest_model_path = 'model_weight/bestmodel.pt'\n\ntotal_train_loss = []\ntotal_train_score = []\ntotal_valid_loss = []\ntotal_valid_score = []\n\nlosses_value = 0\nfor epoch in range(num_epochs):\n  \n    train_loss = []\n    train_score = []\n    valid_loss = []\n    valid_score = []\n    #<-----------Training Loop---------------------------->\n    pbar = tqdm(train_loader, desc = 'description')\n    for x_train, y_train in pbar:\n      x_train = torch.autograd.Variable(x_train).cuda()\n      y_train = torch.autograd.Variable(y_train).cuda()\n      optimizer.zero_grad()\n      output = model(x_train)\n      #Loss\n      loss = criterion(output, y_train)\n      losses_value = loss.item()\n      #Score\n      score = accuracy_metric(output,y_train)\n      loss.backward()\n      optimizer.step()\n      train_loss.append(losses_value)\n      train_score.append(score.item())\n      #train_score.append(score)\n      pbar.set_description(f\"Epoch: {epoch+1}, loss: {losses_value}, IoU: {score}\")\n\n    #<---------------Validation Loop---------------------->\n    with torch.no_grad():\n      for image,mask in val_loader:\n        image = torch.autograd.Variable(image).cuda()\n        mask = torch.autograd.Variable(mask).cuda()\n        output = model(image)\n        ## Compute Loss Value.\n        loss = criterion(output, mask)\n        losses_value = loss.item()\n        ## Compute Accuracy Score\n        score = accuracy_metric(output,mask)\n        valid_loss.append(losses_value)\n        valid_score.append(score.item())\n\n    total_train_loss.append(np.mean(train_loss))\n    total_train_score.append(np.mean(train_score))\n    total_valid_loss.append(np.mean(valid_loss))\n    total_valid_score.append(np.mean(valid_score))\n    print(f\"\\n###############Train Loss: {total_train_loss[-1]}, Train IOU: {total_train_score[-1]}###############\")\n    print(f\"###############Valid Loss: {total_valid_loss[-1]}, Valid IOU: {total_valid_score[-1]}###############\")\n\n    #Save best model Checkpoint\n    # create checkpoint variable and add important data\n    checkpoint = {\n        'epoch': epoch + 1,\n        'valid_loss_min': total_valid_loss[-1],\n        'state_dict': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n    }\n    \n    # save checkpoint\n    save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n    \n    ## TODO: save the model if validation loss has decreased\n    if total_valid_loss[-1] <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,total_valid_loss[-1]))\n        # save checkpoint as best model\n        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n        valid_loss_min = total_valid_loss[-1]","metadata":{"execution":{"iopub.status.busy":"2022-02-04T05:09:36.171638Z","iopub.execute_input":"2022-02-04T05:09:36.171897Z","iopub.status.idle":"2022-02-04T06:38:03.589247Z","shell.execute_reply.started":"2022-02-04T05:09:36.171859Z","shell.execute_reply":"2022-02-04T06:38:03.588403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nplt.figure(1)\nplt.figure(figsize=(15,5))\nsns.set_style(style=\"darkgrid\")\nplt.subplot(1, 2, 1)\nsns.lineplot(x=range(1,21), y=total_train_loss, label=\"Train Loss\")\nsns.lineplot(x=range(1,21), y=total_valid_loss, label=\"Valid Loss\")\nplt.title(\"Loss\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"DiceLoss\")\n\nplt.subplot(1, 2, 2)\nsns.lineplot(x=range(1,21), y=total_train_score, label=\"Train Score\")\nsns.lineplot(x=range(1,21), y=total_valid_score, label=\"Valid Score\")\nplt.title(\"Score (IoU)\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"IoU\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T07:25:42.312612Z","iopub.execute_input":"2022-02-04T07:25:42.313202Z","iopub.status.idle":"2022-02-04T07:25:43.281926Z","shell.execute_reply.started":"2022-02-04T07:25:42.313105Z","shell.execute_reply":"2022-02-04T07:25:43.280123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validating the model","metadata":{}},{"cell_type":"code","source":"#loading the saved model\nmodel, optimizer, start_epoch, valid_loss_min = load_ckp(checkpoint_path, model, optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T06:38:04.119213Z","iopub.status.idle":"2022-02-04T06:38:04.119739Z","shell.execute_reply.started":"2022-02-04T06:38:04.119537Z","shell.execute_reply":"2022-02-04T06:38:04.119557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_predict(model, n_images):\n  #model = model.eval()\n  figure, ax = plt.subplots(nrows=n_images, ncols=3, figsize=(15, 18))\n  with torch.no_grad():\n    for data,mask in val_loader:\n        data = torch.autograd.Variable(data, volatile=True).cuda()\n        mask = torch.autograd.Variable(mask, volatile=True).cuda()\n        o = model(data)\n        break\n  for img_no in range(0, n_images):\n    tm=o[img_no][0].data.cpu().numpy()\n    img = data[img_no].data.cpu()\n    msk = mask[img_no].data.cpu()\n    img = format_image(img)\n    msk = format_mask(msk)\n    ax[img_no, 0].imshow(img)\n    ax[img_no, 1].imshow(msk, interpolation=\"nearest\", cmap=\"gray\")\n    ax[img_no, 2].imshow(tm, interpolation=\"nearest\", cmap=\"gray\")\n    ax[img_no, 0].set_title(\"Ground Truth Image\")\n    ax[img_no, 1].set_title(\"Ground Truth Mask\")\n    ax[img_no, 2].set_title(\"Predicted Mask\")\n    ax[img_no, 0].set_axis_off()\n    ax[img_no, 1].set_axis_off()\n    ax[img_no, 2].set_axis_off()\n  plt.tight_layout()\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T06:38:04.120637Z","iopub.status.idle":"2022-02-04T06:38:04.121946Z","shell.execute_reply.started":"2022-02-04T06:38:04.121674Z","shell.execute_reply":"2022-02-04T06:38:04.121717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_predict(model, 6)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T06:38:04.122982Z","iopub.status.idle":"2022-02-04T06:38:04.12374Z","shell.execute_reply.started":"2022-02-04T06:38:04.123502Z","shell.execute_reply":"2022-02-04T06:38:04.123528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}