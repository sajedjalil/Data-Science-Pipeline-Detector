{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","name":"python","nbconvert_exporter":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3"}},"cells":[{"metadata":{"_cell_guid":"a5ceca49-680d-4032-a770-9defe2130804","_uuid":"c78c893920ad453af293aedfa83e34c56dbb4e19"},"source":"This dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations.\n\nEach image is represented by an associated ImageId. Files belonging to an image are contained in a folder with this ImageId. Within this folder are two subfolders:\n\nimages contains the image file.\nmasks contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).","cell_type":"markdown"},{"metadata":{"_cell_guid":"29646d45-5c66-4962-b3cc-f440bba45309","_uuid":"58903f26e600e7101960fbbf51ce1652af5d1f2b"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom glob import glob\nimport os\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# I used code from https://www.kaggle.com/kmader/nuclei-overview-to-submission to load the data\ntrain_labels = pd.read_csv('../input/stage1_train_labels.csv')\ntrain_labels['EncodedPixels'] = train_labels['EncodedPixels'].map(lambda ep: [int(x) for x in ep.split(' ')])\nall_images = glob(os.path.join('../input/', 'stage1_*', '*', '*', '*'))\nimg_df = pd.DataFrame({'path': all_images})\nimg_id = lambda in_path: in_path.split('/')[-3]\nimg_type = lambda in_path: in_path.split('/')[-2]\nimg_group = lambda in_path: in_path.split('/')[-4].split('_')[1]\nimg_stage = lambda in_path: in_path.split('/')[-4].split('_')[0]\nimg_df['ImageId'] = img_df['path'].map(img_id)\nimg_df['ImageType'] = img_df['path'].map(img_type)\nimg_df['TrainingSplit'] = img_df['path'].map(img_group)\nimg_df['Stage'] = img_df['path'].map(img_stage)\ntrain_df = img_df.query('TrainingSplit==\"train\"')\ntrain_rows = []\ngroup_cols = ['Stage', 'ImageId']\nfor n_group, n_rows in train_df.groupby(group_cols):\n    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\n    c_row['masks'] = n_rows.query('ImageType == \"masks\"')['path'].values.tolist()\n    c_row['images'] = n_rows.query('ImageType == \"images\"')['path'].values.tolist()\n    train_rows += [c_row]\ntrain_img_df = pd.DataFrame(train_rows)    \nIMG_CHANNELS = 3\ndef read_and_stack(in_img_list):\n    return np.sum(np.stack([imread(c_img) for c_img in in_img_list], 0), 0)/255.0\ntrain_img_df['images'] = train_img_df['images'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])\ntrain_img_df['masks'] = train_img_df['masks'].map(read_and_stack).map(lambda x: x.astype(int))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e82d6f3c-7f59-4959-8a5d-6e11e81a4aab","collapsed":true,"_uuid":"3981a8cd2858addb2363d95a77fc85a83eacb399"},"source":"test_df = img_df.query('TrainingSplit==\"test\"')\ntest_rows = []\ngroup_cols = ['Stage', 'ImageId']\nfor n_group, n_rows in test_df.groupby(group_cols):\n    c_row = {col_name: col_value for col_name, col_value in zip(group_cols, n_group)}\n    c_row['images'] = n_rows.query('ImageType == \"images\"')['path'].values.tolist()\n    test_rows += [c_row]\ntest_img_df = pd.DataFrame(test_rows)    \n\ntest_img_df['images'] = test_img_df['images'].map(read_and_stack).map(lambda x: x[:,:,:IMG_CHANNELS])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"68dcf2a5-3ccd-4893-b4b6-e886e493ae07","_uuid":"d3b72b0d78fd7cfbbf006d85406ae6e694954f87"},"source":"Let' check out these nuclei","cell_type":"markdown"},{"metadata":{"_cell_guid":"945d534d-eed4-4553-8812-a4018c499f9c","_uuid":"b98f9876c8b819ab691cbd3ce4b24c017d4b97d5"},"source":"f,axa = plt.subplots(1,2,figsize = (12,5))\naxa[0].imshow(train_img_df['images'][2])\naxa[1].imshow(train_img_df['images'][5])\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"17035b23-5ca0-498d-8d3e-edcbe6c256c4","_uuid":"8bd438c970d0c4c4d814d7e87c423162eaea945e"},"source":"We are dealing with images with different size som RGB some in Grey Scale. I","cell_type":"markdown"},{"metadata":{"_cell_guid":"447fb724-bbdf-40f8-b1e7-976a56e814f4","_uuid":"c1b0a6bc0724c8324feba91da2f85577d114b6ee"},"source":"train_img_df['images'].map(lambda x: x.shape).value_counts()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"438b3da5-5942-4a9e-8bac-1c76a9375c6b","_uuid":"0963bd443c80a5bc5650d7c457e3861d1b9782e7"},"source":"I will split the dataset 70% for training and 30% for Test/Validation.","cell_type":"markdown"},{"metadata":{"_cell_guid":"0cde627f-e636-4029-a3ed-66b41cbd14ff","collapsed":true,"_uuid":"e04e6eb2049e8310127a14422fca9249348d0c41"},"source":"import math\ndf = train_img_df.sample(frac=1, random_state= 42)\nTrain = df[0:math.floor(len(df)*0.7)]\nValidation = df[len(Train):]\nTest = test_img_df","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e8d1f51a-b034-4c78-8e62-b0bf12be9dec","_uuid":"4b7da1add8d7d5361a98eef8c48047ad7c6bb5ee"},"source":"Then I define a function to resize all imges and turn them to 1 channel.","cell_type":"markdown"},{"metadata":{"_cell_guid":"edff8bcd-5eb6-4ddc-a043-2a1ae5a6253b","collapsed":true,"_uuid":"336d8845a66a7744be485dffc4c6add6c9391e6c"},"source":"from skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nWIDTH = 128\nHEIGHT = 128\n\ndef process(data,notest=True):\n    X = []\n    Y = []\n    print(\"Resizing all...\")\n    for i in range(len(data.images)):\n        img = resize(data.images.iloc[i], (HEIGHT, WIDTH,IMG_CHANNELS), mode='constant', preserve_range=True)\n        X.append(img)\n        if(notest):\n            img = resize(data.masks.iloc[i], (HEIGHT, WIDTH,1), mode='constant', preserve_range=True)\n            Y.append(img)\n    print(\"Done\")    \n    return X, Y\n\n    print(\"Turning all to 1 Channel\")\n    images= []\n    for i in range(len(data.images)):\n        img = np.zeros([WIDTH,HEIGHT])\n        for r in range(len(X[i])):\n            for c in range(len(X[i][r])):\n                img[r][c] = X[i][r][c].mean()\n        img = np.asarray(img).reshape(WIDTH,HEIGHT,1)\n        images.append(img)  \n    X = images\n    if(notest):\n        for i in range(len(data.images)):\n            Y[i] = np.asarray(Y[i]).reshape(WIDTH,HEIGHT,1)\n    print('Done')\n    return X, Y\n        ","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f78f3505-7248-4845-bd4d-22a57f28f6bc","_uuid":"234f7b71384a7cbe2564ca5693274d2cc3b0b13a"},"source":"print(\"Processing Train\")\nX_train,Y_train = process(Train)\nprint(\"Processing Test\")\nX_val,Y_val = process(Validation)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"00ff5cc3-198c-43cf-8cac-716a5bd6df5a","_uuid":"3a64b441737853e3e4e4e15e1ccf49f3b32b11e1"},"source":"X_test,Y_test = process(Test,notest= False)\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"78be11c6-855c-4ea2-9faf-54a209a23eea","_uuid":"8464d61fb6371cd7d9dd4a808058a0a7d0dc5fcb"},"source":"np.asarray(X_test[0]).shape","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"16188e4a-7240-4483-bf00-38acbf329f1e","_uuid":"cb91b05328b2fbcf61ddd3b2187a7a392c7f38e6"},"source":"n_img = 6\nfig, axa = plt.subplots(2, 3, figsize = (15, 6))\naxa[0][0].imshow(X_train[0].reshape(WIDTH,HEIGHT,3))\naxa[0][1].imshow(X_train[1].reshape(WIDTH,HEIGHT,3))\naxa[0][2].imshow(X_train[2].reshape(WIDTH,HEIGHT,3))\naxa[1][0].imshow(Y_train[0].reshape(WIDTH,HEIGHT))\naxa[1][1].imshow(Y_train[1].reshape(WIDTH,HEIGHT))\naxa[1][2].imshow(Y_train[2].reshape(WIDTH,HEIGHT))\n\nplt.show()\n\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a59952cf-8a61-4795-a3b2-e4656e31d8f1","collapsed":true,"_uuid":"3c694cd2afea4c1b04a98594ba22ac6400dded60"},"source":"This is the first time I design a U-net, the Idea is well depicted in the following picture (which is taken from the original U Net paper). The idea is the network is composed of two paths: a contracting path (left side of the figure) and an expansive path (on the right side of the figure). The former consists of the repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation for downsampling.  Every step in the expansive path consists of an upsampling of the curren feature map followed by a convolution, a concatenation with the correspondingly \nfeature map from the contracting path, and convolutions again with RELU activation.\n\n![](http://tuatini.me/content/images/2017/09/u-net-architecture-1.png)\n\n\nhere follows some other kernels bout U-nets\nhttps://www.kaggle.com/drn01z3/end-to-end-baseline-with-u-net-keras\nhttps://www.kaggle.com/toregil/a-lung-u-net-in-keras\n","cell_type":"markdown"},{"metadata":{"_cell_guid":"0e8946b1-fab5-48f1-8e9c-46471247032a","collapsed":true,"_uuid":"92bb988eb4fbb38953fe389c2592de6c3c1c462a"},"source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())\n","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"5e0816ba-4e27-4748-ac89-34df8a3e2a8b","_uuid":"7ca558934f66c6a01a61eeb610ed0fd58d8a9fc1"},"source":"from keras.models import Model\nfrom keras.layers import *\nfrom keras.layers import UpSampling2D\nfrom keras.callbacks import * \nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.preprocessing.image import ImageDataGenerator\nimport keras.backend as K\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"fdaa453d-c95e-428b-a2de-f7b2bcbe0cdb","collapsed":true,"_uuid":"ab75cfa3ddcaec85f770e13cab213aa9da0d9cf1"},"source":"input_layer = Input(shape=np.asarray(X_train).shape[1:])\nc1 = Conv2D(filters=8,\n            input_shape=[WIDTH,HEIGHT,IMG_CHANNELS],\n            kernel_size=(3,3), activation='relu', padding='same')(input_layer)\nl = MaxPool2D(strides=(2,2))(c1)\n\nc2 = Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(l)\nl = MaxPool2D(strides=(2,2))(c2)\n\nc3 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\nl = MaxPool2D(strides=(2,2))(c3)\n\nc4 = Conv2D(filters=32, kernel_size=(1,1), activation='relu', padding='same')(l)\n\n#we concatenate the c3 output with the upsample of c4 and apply a further convolution\nl = concatenate([UpSampling2D(size=(2,2))(c4), c3], axis=-1)\nl = Conv2D(filters=32, kernel_size=(2,2), activation='relu', padding='same')(l)\n\n#the same up to the first layer\nl = concatenate([UpSampling2D(size=(2,2))(l), c2], axis=-1)\nl = Conv2D(filters=24, kernel_size=(2,2), activation='relu', padding='same')(l)\n\nl = concatenate([UpSampling2D(size=(2,2))(l), c1], axis=-1)\nl = Conv2D(filters=16, kernel_size=(2,2), activation='relu', padding='same')(l)\n\nl = Conv2D(filters=64, kernel_size=(1,1), activation='relu')(l)\n\nl = Dropout(0.5)(l)\noutput_layer = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid')(l)\n                                                         \nmodel = Model(input_layer, output_layer)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"70a9811d-338f-466a-aacc-491d7f436245","_uuid":"8fb14ffa9835d4c5ba49e5cd67ab516425a15fde"},"source":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])\nmodel.summary()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"1c969948-5acd-414d-af03-d59768095483","_uuid":"ed41a564ab45987809cc4a8e696856f9a1af6396"},"source":"earlystopper = EarlyStopping(patience=5, verbose=1)\ncheckpointer = ModelCheckpoint('model-dsbowl2018-1.h5', verbose=1, save_best_only=True)\nresults = model.fit(np.asarray(X_train), np.asarray(Y_train), validation_split=0.1, batch_size=16, epochs=10, \n                    callbacks=[earlystopper, checkpointer])","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"90093af4-95bf-4c2d-bce1-36eee1c4db71","_uuid":"b914f55c143a2136feb4dd4f539fda8bc981baf2"},"source":"## Validation","cell_type":"markdown"},{"metadata":{"_cell_guid":"10e77429-7c4c-478b-9ed8-1edf1199614e","_uuid":"2db763150c6f8f4c655780e75449d062dcab0fdb"},"source":"Let's see how the trained U-Net performes against the Validation dataset","cell_type":"markdown"},{"metadata":{"_cell_guid":"e27e4e0f-1971-4a50-b32d-f7a1ddd546d3","collapsed":true,"_uuid":"d6f3344472a5bc5ab2ae69152bb2a7f1e2d14ef6"},"source":"preds = model.predict(np.asarray(X_val))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d97e93bd-0969-4978-9fc2-ac2e15f04f82","_uuid":"bf213aef8a747789380c928fea312d10fd49d319"},"source":"n_img = 6\nfig, axa = plt.subplots(2, 3, figsize = (15, 6))\naxa[0][0].imshow(X_val[0].reshape(WIDTH,HEIGHT,3))\naxa[0][1].imshow(X_val[1].reshape(WIDTH,HEIGHT,3))\naxa[0][2].imshow(X_val[2].reshape(WIDTH,HEIGHT,3))\naxa[1][0].imshow(preds[0].reshape(WIDTH,HEIGHT))\naxa[1][1].imshow(preds[1].reshape(WIDTH,HEIGHT))\naxa[1][2].imshow(preds[2].reshape(WIDTH,HEIGHT))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"from skimage.filters import threshold_otsu\nfpreds =[]\nfor p in preds:\n    #thresh = threshold_otsu(p)\n    thresh = 0.5\n    binary = p > thresh\n    fpreds.append(binary)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"def iou(A,B):\n    intersect = (A*B)\n    union = (A+B)>0\n    return intersect.sum()/union.sum()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"ious = []\nfor i in range(len(Y_val)):\n    ious.append(iou(Y_val[i],fpreds[i]))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{},"source":"pd.Series(ious).mean()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9e74cada-17a6-49b9-b2a0-fbcc7039008b","_uuid":"d75c55c835432adbdf3af173c55222f03ef0a867"},"source":"## Test","cell_type":"markdown"},{"metadata":{"_cell_guid":"a3a44ffd-354c-45c8-9b38-cbf3b80c24da","collapsed":true,"_uuid":"d54e1a1280a6b5580c56d36d9dda17a9a351d683"},"source":"tpreds = model.predict(np.asarray(X_test))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"fpreds = []\nfor p in tpreds:\n    #thresh = threshold_otsu(p)\n    thresh = 0.5\n    binary = p > thresh\n    fpreds.append(binary)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":true},"source":"fpreds = tpreds","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"970c85c3-1a3d-4654-b338-979b28d880cf","collapsed":true,"_uuid":"bd86271a60ae324373bcf5b35fcb6f402c30fdd9"},"source":"fig, axa = plt.subplots(2, 3, figsize = (15, 6))\naxa[0][0].imshow(X_test[0].reshape(WIDTH,HEIGHT,3))\naxa[0][1].imshow(X_test[10].reshape(WIDTH,HEIGHT,3))\naxa[0][2].imshow(X_test[2].reshape(WIDTH,HEIGHT,3))\naxa[1][0].imshow(tpreds[0].reshape(WIDTH,HEIGHT))\naxa[1][1].imshow(tpreds[10].reshape(WIDTH,HEIGHT))\naxa[1][2].imshow(tpreds[2].reshape(WIDTH,HEIGHT))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"97f74d02-3841-4f7a-b32c-c07caa4235d6","collapsed":true,"_uuid":"276d72bf3bbc50151d325cb1698f4f070ab05f59"},"source":"test_sizes = Test.images.map(lambda x: x.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"9e950d0c-1ef1-4818-8382-86f885b71a74","collapsed":true,"_uuid":"eae5a47527d679e802658f08cf05fa6f3247d0e4"},"source":"preds_test_upsampled = []\nfor i in range(len(tpreds)):\n    preds_test_upsampled.append(resize(np.squeeze(tpreds[i]), \n                                       (test_sizes[i][0], test_sizes[i][1]), \n                                       mode='constant', preserve_range=True))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"02e1ffb4-7d7f-490c-9d92-8eaf37daa09d","collapsed":true,"scrolled":true,"_uuid":"8e3b33b01e830a9d2903b871d2d3aa2990315986"},"source":"fig, axa = plt.subplots(1, 3, figsize = (15, 6))\naxa[0].imshow(preds_test_upsampled[0].reshape(test_sizes[0][0], test_sizes[0][1]))\naxa[1].imshow(preds_test_upsampled[1].reshape(test_sizes[1][0], test_sizes[1][1]))\naxa[2].imshow(preds_test_upsampled[2].reshape(test_sizes[2][0], test_sizes[2][1]))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"5483c231-ddfc-40d5-bc67-e71f52860d6d","_uuid":"4ab002351213eb6a4f61c860d5ef79baf55bdba6"},"source":"Thanks to https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277","cell_type":"markdown"},{"metadata":{"_cell_guid":"585cff23-4ccb-45ad-bd95-794b3b759020","collapsed":true,"_uuid":"f13eec719c223b2b8f29cc6f0f191e68b40abb5c"},"source":"# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a50e8d0d-3fc7-4a1f-9f35-908c721be300","collapsed":true,"_uuid":"f72369d0c72436da32830f16a97530912a5ff911"},"source":"new_test_ids = []\nrles = []\nfor n, id_ in enumerate(Test.ImageId):\n    rle = list(prob_to_rles(preds_test_upsampled[n]))\n    rles.extend(rle)\n    new_test_ids.extend([id_] * len(rle))","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"146275c1-f139-47ce-af06-5d7ecc0f5479","collapsed":true,"_uuid":"c4136e776f67f9fcab48fbae7473f71e4699e56e"},"source":"sub = pd.DataFrame()\nsub['ImageId'] = new_test_ids\nsub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsub.to_csv('sub-dsbowl2018-1.csv', index=False)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"b392a576-ebd2-4121-85d8-8997ee593dac","collapsed":true,"_uuid":"5019e69ce54056eb67b4fb92075c64bf9f9765c3"},"source":"","execution_count":null,"cell_type":"code","outputs":[]}],"nbformat":4,"nbformat_minor":1}