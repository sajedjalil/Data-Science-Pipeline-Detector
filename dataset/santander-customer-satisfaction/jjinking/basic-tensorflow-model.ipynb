{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import xgboost as xgb\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_auc_score"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Read data\ndf_train = pd.read_csv(\"../input/train.csv\", index_col='ID')\nfeature_cols = list(df_train.columns)\nfeature_cols.remove(\"TARGET\")\ndf_test = pd.read_csv(\"../input/test.csv\", index_col='ID')\n\n# Split up the data\nX_all = df_train[feature_cols]\ny_all = df_train[\"TARGET\"]\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.3, random_state=5, stratify=y_all)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Get top features from xgb model\nmodel = xgb.XGBRegressor(\n    learning_rate=0.1,\n    n_estimators=1000,\n    max_depth=5,\n    min_child_weight=9,\n    gamma=0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'binary:logistic',\n    nthread=4,\n    scale_pos_weight=1,\n    seed=5\n)\n\n# Train cv\nxgb_param = model.get_xgb_params()\ndtrain = xgb.DMatrix(X_train.values, label=y_train.values, missing=np.nan)\ncv_result = xgb.cv(\n    xgb_param,\n    dtrain,\n    num_boost_round=model.get_params()['n_estimators'],\n    nfold=5,\n    metrics=['auc'],\n    early_stopping_rounds=50)\nbest_n_estimators = cv_result.shape[0]\nmodel.set_params(n_estimators=best_n_estimators)\n\n# Train model\nmodel.fit(X_train, y_train, eval_metric='auc')\n\n# Predict training data\ny_hat_train = model.predict(X_train)\n\n# Predict test data\ny_hat_test = model.predict(X_test)\n\n# Print model report:\nprint(\"\\nModel Report\")\nprint(\"best n_estimators: {}\".format(best_n_estimators))\nprint(\"AUC Score (Train): %f\" % roc_auc_score(y_train, y_hat_train))\nprint(\"AUC Score (Test) : %f\" % roc_auc_score(y_test,  y_hat_test))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Get important features\nfeat_imp = list(pd.Series(model.booster().get_fscore()).sort_values(ascending=False).index)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Even out the targets\ndf_train_1 = df_train[df_train[\"TARGET\"] == 1]\ndf_train_0 = df_train[df_train[\"TARGET\"] == 0].head(df_train_1.shape[0])\ndf_train = df_train_1.append(df_train_0)\n\n# Scale data\nX_all = df_train[feat_imp].copy(deep=True)\ny_all = df_train[\"TARGET\"]\nX_all[feat_imp] = sklearn.preprocessing.scale(X_all, axis=0, with_mean=True, with_std=True, copy=True)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.3, random_state=5, stratify=y_all)\n\n# Create second complementary column at position 0\ny_train_2cols = np.array(list(zip((1 - y_train).values, y_train.values)))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Tensorflow Model\nimport tensorflow as tf\n\n# Hyperparameters\nn_steps = 3001\nbatch_size = 200\nlearning_rate0 = 0.05\ndecay_steps = 500\ndecay_rate = 0.8\n\n# Network parameters\nn_h1 = 20\nn_h2 = 20\nn_features = X_train.shape[1]\nn_labels = 2\n\n# L2 regularization\nbeta = 1e-5\n\n# Dropout\nkeep_prob = 0.5\n\ngraph = tf.Graph()\nwith graph.as_default():\n    # Allocate variables\n    tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, n_features))\n    tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, n_labels))\n    tf_X_test = tf.constant(X_test.values, dtype=tf.float32)\n    \n    tf_keep_prob = tf.placeholder(tf.float32)\n    \n    # Hidden layer\n    with tf.name_scope('h1') as scope:\n        weights_h1 = tf.Variable(\n            tf.truncated_normal(\n                [n_features, n_h1],\n                stddev=1.0 / np.sqrt(n_features)\n        ), name='weights_h1')\n        biases_h1 = tf.Variable(tf.zeros([n_h1]), name='biases_h1')        \n        h1 = tf.nn.relu(tf.matmul(tf_X_train, weights_h1) + biases_h1)\n\n        # Dropout\n        h1 = tf.nn.dropout(h1, tf_keep_prob)\n        \n    # Hidden layer 2\n    with tf.name_scope('h2') as scope:\n        weights_h2 = tf.Variable(\n            tf.truncated_normal(\n                [n_h1, n_h2],\n                stddev=1.0 / np.sqrt(n_h1)\n        ), name='weights_h2')\n        biases_h2 = tf.Variable(tf.zeros([n_h2]), name='biases_h2')\n        h2 = tf.nn.relu(tf.matmul(h1, weights_h2) + biases_h2)\n\n        # Dropout\n        h2 = tf.nn.dropout(h2, tf_keep_prob)\n        \n    # Output layer\n    with tf.name_scope('softmax_linear'):\n        weights_out = tf.Variable(\n            tf.truncated_normal(\n                [n_h2, n_labels],\n                stddev=1.0 / np.sqrt(n_h2)\n            ), name='weights_out')\n        biases_out = tf.Variable(tf.zeros([n_labels]), name='biases_out')\n        logits = tf.matmul(h2, weights_out) + biases_out\n\n    # Training computation.\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_train))\n    \n    # L2 Regularization\n    #loss += beta * (tf.nn.l2_loss(weights_h1) + tf.nn.l2_loss(weights_h2) + tf.nn.l2_loss(weights_out))\n\n    # Optimizer\n    global_step = tf.Variable(0, trainable=False)\n    learning_rate = tf.train.exponential_decay(\n        learning_rate0, global_step, decay_steps, decay_rate, staircase=False)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n    \n    # Predictions for the training and test datasets.\n    yhat_train = tf.nn.softmax(logits)\n    yhat_test = tf.nn.relu(tf.matmul(tf_X_test, weights_h1) + biases_h1)\n    yhat_test = tf.nn.relu(tf.matmul(yhat_test, weights_h2) + biases_h2)\n    yhat_test = tf.nn.softmax(tf.matmul(yhat_test, weights_out) + biases_out)\n    \nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    for step in range(n_steps):    \n        _batch_idx = np.random.choice(X_train.shape[0], size=batch_size, replace=False)\n        batch_X = X_train.values[_batch_idx, :]\n        batch_y = y_train_2cols[_batch_idx, :]\n\n        # Prepare a dictionary telling the session where to feed the minibatch.\n        feed_dict = {\n            tf_X_train: batch_X,\n            tf_y_train: batch_y,\n            tf_keep_prob: keep_prob\n        }\n        _, l, pred = session.run([optimizer, loss, yhat_train], feed_dict=feed_dict)\n        if (step % 500 == 0):\n            print(\"Batch loss at step {0:d}: {1:.6f}\".format(step, l))\n            print(\"Batch score: {0:.6f}\".format(roc_auc_score(batch_y[:, 1], pred[:, 1])))\n            print(\"Test score: {0:.6f}\".format(roc_auc_score(y_test.values, yhat_test.eval()[:, 1])))\n    print(\"Final Test score: {0:.6f}\".format(roc_auc_score(y_test.values, yhat_test.eval()[:, 1])))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}