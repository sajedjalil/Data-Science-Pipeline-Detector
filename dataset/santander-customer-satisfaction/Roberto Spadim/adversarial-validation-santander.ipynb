{"metadata":{"_change_revision":0,"_is_fork":false,"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py","version":"3.6.3","mimetype":"text/x-python","pygments_lexer":"ipython3","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"b69ebf0d696a8185c2cfcd69c3909ca3e599c90e","_cell_guid":"788f05ff-541a-4ebb-98e9-d09422109955"},"cell_type":"markdown","source":"FROM https://www.kaggle.com/rspadim/adversarial-validation-porto-seguro"},{"metadata":{"_uuid":"4863a6e30358fbb65fe3ef2d6a4045dc1ae034a6","_cell_guid":"bcbf5e1a-6442-47ba-5435-7053db612903"},"execution_count":null,"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n","outputs":[]},{"metadata":{"_uuid":"45aad642ad15eebf87376343bd6421aa5bbe5156","collapsed":true,"_cell_guid":"3f4ffc92-a776-6715-b7f3-925f1547d4fc"},"execution_count":null,"cell_type":"code","source":"# We start by loading the training / test data and combining them with minimal preprocessing necessary\nxtrain = pd.read_csv('../input/train.csv')\nxtrain.drop(['ID', 'TARGET'], axis = 1, inplace = True)\nxtest = pd.read_csv('../input/test.csv')\nxtest.drop(['ID'], axis = 1, inplace = True)\n\n# add identifier and combine\nxtrain['istrain'] = 1\nxtest['istrain'] = 0\nxdat = pd.concat([xtrain, xtest], axis = 0)\ny = xdat['istrain']; xdat.drop('istrain', axis = 1, inplace = True)","outputs":[]},{"metadata":{"_uuid":"e7bdea3e061b85af7eba75d5a5e0e407ba2d8285","collapsed":true,"_cell_guid":"891a4cdc-d085-b67f-45e7-dcdc3ca0f8b2"},"execution_count":null,"cell_type":"code","source":"skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 44) # why stratified k fold?\nxgb_params = { # is this parameters ok?\n        'learning_rate': 0.05, 'max_depth': 4,'subsample': 0.9,\n        'colsample_bytree': 0.9,'objective': 'binary:logistic',\n        'silent': 1, 'n_estimators':100, 'gamma':1,\n        'min_child_weight':4, 'n_jobs':-1\n        }   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)     ","outputs":[]},{"metadata":{"_uuid":"6ffde62c36939f3f7c3c6db97ba8257fede6b19c","_cell_guid":"e103b1ab-479c-777d-ee19-a9f3a572a7c4"},"cell_type":"markdown","source":"[FROM OTHER CONTEST]\n\nCalculate the AUC for each fold"},{"metadata":{"_uuid":"c460000353cffc78554d772be84707b5e3e48be2","_cell_guid":"223a095a-712f-fa90-707a-ea34fa091fc0"},"execution_count":null,"cell_type":"code","source":"for train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric='logloss', verbose=False,early_stopping_rounds=10) # it takes ~ 80 rounds to fit\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))\n        \n#final dataset:\nclf.fit(xdat, y, eval_set=[(x1, y1)],\neval_metric='logloss', verbose=False,early_stopping_rounds=10) # it takes ~ 80 rounds to fit\n\nprval = clf.predict_proba(xdat)[:,1]\nprint(roc_auc_score(y,prval))","outputs":[]},{"metadata":{"_uuid":"9bfef4d19fc2a2592488f800ae0e9473059b719c","collapsed":true,"_cell_guid":"9e501264-042f-47fb-b2a9-24f099f0880a"},"cell_type":"markdown","source":"---\nwhat about KNN?"},{"metadata":{"_uuid":"c94e626e2ba8fec3474141db5626e37c16e3d3b9","collapsed":true,"_cell_guid":"0e64fb58-681e-4525-8b40-f0f28697e9d9"},"execution_count":null,"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_params={\n    'n_neighbors':5, # first try value\n    'weights':'distance',\n    'metric':'manhattan' #i like this name =)\n    \n    #distances to test: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n    #float:            \n    #   euclidean, manhattan, chebyshev, minkowski, wminkowski, seuclidean, mahalanobis\n    #integers: \n    #   hamming, canberra, braycurtis\n}\n\nclf = KNeighborsClassifier(**knn_params)      #good bye xgboost","outputs":[]},{"metadata":{"_uuid":"f275eadc856cf70ddd87c1df2cf8f09a0827efbe","collapsed":true,"_cell_guid":"1398c3c9-9d08-4dbf-941f-3e6a5b08473f"},"execution_count":null,"cell_type":"code","source":"for train_index, test_index in skf.split(xdat, y):\n        x0, x1 = xdat.iloc[train_index], xdat.iloc[test_index]\n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        print(x0.shape)\n        clf.fit(x0, y0) # very easy parameters :)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        print(roc_auc_score(y1,prval))\n        \n#final dataset:\nclf.fit(xdat, y)\n\nprval = clf.predict_proba(xdat)[:,1]\nprint(roc_auc_score(y,prval))","outputs":[]}],"nbformat":4}