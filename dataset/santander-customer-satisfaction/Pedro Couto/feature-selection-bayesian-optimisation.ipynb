{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Santander Customer Satisfaction\n\nThe problem is realitvely simple, we need to get the highest AUC of the ROC curve as we can.\n\n## In order to solve this problem, we are going to do the following steps:\n- 1 Loading Data and Packeges;\n- 2 Basic Exploratory Analysis;\n- 3 Dataset Split (train - test);\n- 4 Features Selection;  \n    - 4.1 Removing low variance features;  \n    - 4.2 Removing repeated features;\n    - 4.3 Using SelectKBest to compare \"f_classif\" & \"mutual_info_classif\" approaches;\n- 5 Bayesian Opitimization to the XGBClassifier model;\n- 6 Model scoring;\n- 7 Results Analysis;\n- 8 Next steps;\n- 9 References;\n\nLet us begin!"},{"metadata":{},"cell_type":"markdown","source":"### 1 Loading Data and Packeges"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the Train and Test datasets\ndf_train = pd.read_csv(\"../input/santander-customer-satisfaction/train.csv\")\ndf_test = pd.read_csv(\"../input/santander-customer-satisfaction/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2 Basic Exploratory Analysis\nFor this step, let us address the following points:\n- Are the data in the columns numeric or do they need to be encoded?\n- Can the test dataset really be used or is it useful only for a Kaggle competition?\n- Are there any missing data?\n- What is the proportion of dissatisfied customers (1) in the dataset df_train?\n- Does it make sense to apply a features selection method on the data?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the genearl infos of df_train\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the genearl infos of df_test\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the outputs of the cells above, we can say that:\n- All columns are already in a numeric format. This means that we don't need to do any encoding to convert any type of variable into a numeric variable.  \n- Since this is an anonym dataset, we have any cue if there are categorical variables. So, there is no need to make any encode to address this problem.\n- Lastly, df_train has 371 columns and df_test has 370 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the first 5 rows of df_train\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the first 5 rows of df_test\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By comparing the two cells above, it is clear that df_test doesn't have the TARGET variable.\n\nAs expected, since these datasets come originally from a Kaggle competition, the test dataset should not have the TARGET column.\n\nIn order to make the test dataset useful, I will split the df_train in train and test datasets and keep df_test as the last check.\nAfter we test the model with test split data, we can double-check the performance by making predictions on df_test and uploading the results on Kaggle competition as a late submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if is there any missing value in both train and test datasets\ndf_train.isnull().sum().sum(), df_test.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we can conclude that both datasets are free from any missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigating the proportion of unsatisfied customers on df_train\nrate_insatisfied = df_train.TARGET.value_counts()[1] / df_train.TARGET.value_counts()[0]\nrate_insatisfied * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have an **extremely unbalanced dataset**, approximately 4.12% positive. This must be taken into account in two situations:\n- To split the data in train and test.\n- To choose hyperparameters such as \"class_weight\" by Random Forest.\n\nSince both train and test dataset are relatively large (around 76k rows and 370 columns), and we don't know the what each feature represents and how they can impact the model, it demands a features selection for three reasons:\n1. To know which features bring most relevant prediction power to the model;\n2. Avoid using features that could degrade the model performance;\n3. Minimize the computational coast by using the minimal amount of features that provide the best model performance."},{"metadata":{},"cell_type":"markdown","source":"### 3 Dataset Split\nHere we are going to split the df_train in train and test dataset. \n\nAs the train_test_split method does the segmentation at random, even with an extremely unbalanced dataset, the split should occur so that both training and testing have the same proportion of unsatisfied customers.  \n**However, as it is difficult to guarantee randomness in fact, we can make a stratified split based on the TARGET variable and thus ensure that the proportion is exact in both datasets.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Spliting the dataset on a proportion of 80% for train and 20% for test.\nX_train, X_test, y_train, y_test = train_test_split(df_train.drop('TARGET', axis = 1), df_train.TARGET, \n                                                    train_size = 0.8, stratify = df_train.TARGET,\n                                                    random_state = 42)\n\n#Checando o resultado do splot\nX_train.shape, y_train.shape[0], X_test.shape, y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4 Feature Selection\n\nHere we want to investigate the following questions:\n- Are there constant and/or semi-constates features that can be removed?\n- Are there duplicate features?\n- Does it make sense to perform some more filtering to reach a smaller group of features?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making copys of X_train and X_test to work with in this section\nX_train_clean = X_train.copy()\nX_test_clean = X_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Investigating if there are constant or semi-constat feature in X_train\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Removing all features that have variance under 0.01\nselector = VarianceThreshold(threshold = 0.01)\nselector.fit(X_train_clean)\nmask_clean = selector.get_support()\nX_train_clean = X_train_clean[X_train_clean.columns[mask_clean]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cheking if we realy removed something\n(len(df_train.columns) - 1) - X_train_clean.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total of remaning features\nX_train_clean.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this filtering, 104 features were removed. Thus, the dataset has become leaner without losing predictive power, as these features do not add information to the ML model that impact its ability to classify an instance.  \n**We have now 266 features.**"},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Removing repeated features;"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there is any duplicated column\nremove = []\ncols = X_train_clean.columns\nfor i in range(len(cols)-1):\n    column = X_train_clean[cols[i]].values\n    for j in range(i+1,len(cols)):\n        if np.array_equal(column, X_train_clean[cols[j]].values):\n            remove.append(cols[j])\n\n\n# If yes, than they will be dropped here\nX_train_clean.drop(remove, axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if any column was dropped\nX_train_clean.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There were 266 columns before checking for duplicate features and now there are 251. Soon there were 15 repeated features."},{"metadata":{},"cell_type":"markdown","source":"#### 4.3 Using SelectKBest to compare \"f_classif\" & \"mutual_info_classif\" approaches\n\nThere are two types of methods for evaluating features in conjunction with SelectKBest: f_classif (fc) and mutual_info_classif (mic). The first works best when the features and Target have a more linear relationship. The second is more appropriate when there are non-linear relationships.  \nSINCE KAGGLE LIMITS THE SESSION IN 9 HOURS, THERE IS NOT ENOUGH TIME TO RUN THE ANALYSIS FOR MUTUAL_INFO_CLASSIF EVEN WHEN WE USE A WHOLE SESSION FOR MUTUAL_INFO_CLASSIF. THEREFORE, IN THIS NOTEBOOK WILL BE PRESENTED ONLY THE ANALYSIS FOR F_CLASSIF METHOD, BUT I WILL LEAVE THE CODE FOR MUTUAL_INFO_CLASSIF FOR ANYONE WHO MIGHT WANT TO USE IT.  \nThe analysis for both methods are availeble on my github: https://github.com/PedroHCouto/Santander-Case/blob/master/Part%20A%20-%20Classification.ipynb\n\nAs the dataset is anonymized and the quantity of features is too large to make a quality study on the feature-target relationship, both methods will be tested and the one that produces a stable region with the highest AUC value will be chosen.\n\nFor this, different K values will be tested with the SelectKBest class, which will be used to train an XGBClassifier model and evaluated using the AUC metric. Having a collection of values, a graph for fc and another for mic will be created.\n\nThus, through a visual analysis, it is possible to choose the best K value as well as the best method for scoring features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.model_selection import cross_val_score\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's first analyse the method f_classif (fc)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create an automated routine to test different K values for f_classif method\n\nK_vs_score_fc = [] #List to store AUC of each K with f_classif\n\nstart = time.time()\n\nfor k in range(2, 247, 2):\n    start = time.time()\n    \n    # Instantiating a KBest object for each of the metrics in order to obtain the K features with the highest value\n    selector_fc = SelectKBest(score_func = f_classif, k = k)\n\n    \n    # Selecting K-features and modifying the dataset\n    X_train_selected_fc = selector_fc.fit_transform(X_train_clean, y_train)\n\n    \n    # Instantiating an XGBClassifier object\n    clf = xgb.XGBClassifier(seed=42)\n    \n    # Using 10-CV to calculate AUC for each K value avoinding overfitting\n    auc_fc = cross_val_score(clf, X_train_selected_fc, y_train, cv = 10, scoring = 'roc_auc')\n\n    \n    # Adding the average values obtained in the CV for further analysis.\n    K_vs_score_fc.append(auc_fc.mean())\n\n    \n    end = time.time()\n    # Returning the metrics related to the tested K and the time spent on this iteration of the loop\n    print(\"k = {} - auc_fc = {} - Time = {}s\".format(k, auc_fc.mean(), end-start))\n    \nend = time.time()\nprint(end - start)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's analyse the mutual_info_classif (mic) method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just for purpose of sharing this piece of code\n# Create an automated routine to test different K values for mutual_info_classif\n\nK_vs_score_mic = [] #List to store AUC of each K with mutual_info_classif\n\n\nfor k in range(2, 247, 2):\n    start = time.time()\n    \n    # Instantiating a KBest object for each of the metrics in order to obtain the K features with the highest value\n    selector_mic = SelectKBest(score_func = mutual_info_classif, k = k)\n    \n    # Selecting K-features and modifying the dataset\n    X_train_selected_mic = selector_mic.fit_transform(X_train_clean, y_train) \n    \n    # Instantiating an XGBClassifier object\n    clf = xgb.XGBClassifier(seed=42)\n    \n    # Using 10-CV to calculate AUC for each K value avoinding overfitting\n    auc_mic = cross_val_score(clf, X_train_selected_mic, y_train, cv = 10, scoring = 'roc_auc')\n    \n    # Adding the average values obtained in the CV for further analysis.\n    K_vs_score_mic.append(auc_mic.mean())\n    \n    end = time.time()\n    # Returning the metrics related to the tested K and the time spent on this iteration of the loop\n    print(\"k = {} - auc_mic = {} - Time = {}s\".format(k, auc_mic.mean(), end-start))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Checking if both list have 123 elements each\nlen(K_vs_score_fc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ploting K_vs_score_fc (# of K-Best features vs AUC)\n\n# Figure setup\nfig, ax = plt.subplots(figsize = (20, 8))\nplt.title('Score valeus for each K with f_classif method', fontsize=18)\nplt.ylabel('Score', fontsize = 16)\nplt.xlabel('Value of K', fontsize = 16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\n\n# Create the lines\nplt.plot(np.arange(2, 247, 2), K_vs_score_fc, color='blue', linewidth=2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Through the graphs above, it is noted that the best values are between 0.80 and 0.82 AUC. However, as the graphs have a range from 0.70 to 0.82 due to small K values.  \n  \nThus, the visualization only of the range between 0.8 and 0.82 must be done, in order to ensure a better evaluation of the K value and which method will be maintained for the next steps."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ploting K_vs_score_fc (# of K-Best features vs AUC) \nimport matplotlib.patches as patches\n\n# Figure setup\nfig, ax = plt.subplots(1, figsize = (20, 8))\nplt.title('Score valeus for each K with f_classif method', fontsize=18)\nplt.ylabel('Score', fontsize = 16)\nplt.xlabel('Value of K', fontsize = 16)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\n\n# Create the lines\nplt.plot(np.arange(2, 247, 2), K_vs_score_fc, color='blue', linewidth=2)\nax.set_ylim(0.80, 0.825);\n\n# Create a Rectangle patch\nrect = patches.Rectangle((82, 0.817), 20, (0.823 - 0.817), linewidth=2, edgecolor='r', facecolor='none')\n\n# Add the patch to the Axes\nax.add_patch(rect)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzing which method generates a better set of features to be used, we look for two main points:\n\n- The smallest number of features that generate the highest AUC value;\n- That the K-features be in a more stable region of the curve because if K is just a peak, this can bring some instability to the model.\n\nApplying these conditions to the above graphs, we observed that the region around K = 96, red rectangle, gives us a behaviour that satisfies the both conditions.\n\nTherefore, ___we selected for K the value of 96___, which would be an intermediate point in this region.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selection the 96 best features aconrdingly to f_classif\nselector_fc = SelectKBest(score_func = f_classif, k = 96)\nselector_fc.fit(X_train_clean, y_train)\nmask_selected = selector_fc.get_support()\n\n# Saving the selected columns in a list\nselected_col = X_train_clean.columns[mask_selected]\nselected_col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can then visualize the importance of each feature according to the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotando o feature score das 96 melhores features\nfeature_score = pd.Series(selector_fc.scores_, index=X_train_clean.columns).sort_values(ascending=False)\n\nfig, ax = plt.subplots(figsize=(20, 12))\nax.barh(feature_score.index[0:30], feature_score[0:30])\nplt.gca().invert_yaxis()\n\n\nax.set_xlabel('K-Score', fontsize=18);\nax.set_ylabel('Features', fontsize=18);\nax.set_title('30 best features by its K-Score', fontsize = 20)\nplt.yticks(fontsize = 14)\nplt.xticks(fontsize = 14)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that a small group of 10 features has a great impact on the classification and the rest generates only a much smaller impactor.  \n\nThe ten features with the greatest impact are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[print(i) for i in feature_score.index[0:10]];","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have the list of features that we will use for this task, we can create dataset where only the desired features should be present."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating datasets where only with the selected 96 features are included\nX_train_selected = X_train[selected_col]\nX_test_selected = X_test[selected_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the first 5 rows of X_train_selected and its shape\nX_train_selected.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the first 5 rows of X_train_selected and its shape\nX_test_selected.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a good understanding of the features and a good selection of which have the greatest impact on the model, we can move on to the next steps."},{"metadata":{},"cell_type":"markdown","source":"### 5 Bayesian Opitimization to the XGBClassifier model\n\nFor this classification task, we are going to use a XGBoost Classifier algorithm. This algorithm is known for its great performance, robustness and simplicity to understand the learning process.\n\nSo, by using an algorithm an interesting approach is to optimize its hyperparameters in a way we can have the best performance it can offer to us.  \n\nFor this task, we will use Bayesian Optimization approach. Some articles prove its greater efficiency when compared to grid search and a performance that is similar or even better than random search. Another advantage is that Bayesian Optimization allows us to optimize multiples hyperparameters at the same time.\n\nThe scikit-optimize package provides us with a great structure to perform Bayesian optimization of hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using a random forest to optimize\nfrom skopt import forest_minimize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for hyperparamters tunning\n# Implementation learned on a lesson of Mario Filho (Kagle Grandmaster) for parametes optmization.\n# Link to the video: https://www.youtube.com/watch?v=WhnkeasZNHI\ndef tune_xgbc(params):\n    \"\"\"Function to be passed as scikit-optimize minimizer/maximizer input\n    \n    Parameters:\n    Tuples with information about the range that the optimizer should use for that parameter, \n    as well as the behaviour that it should follow in that range.\n    \n    Returns:\n    float: the metric that should be minimized. If the objective is maximization, then the negative \n    of the desired metric must be returned. In this case, the negative AUC average generated by CV is returned.\n    \"\"\"\n    \n    \n    #Hyperparameters to be optimized\n    print(params)\n    learning_rate = params[0] \n    n_estimators = params[1] \n    max_depth = params[2]\n    min_child_weight = params[3]\n    gamma = params[4]\n    subsample = params[5]\n    colsample_bytree = params[6]\n        \n    \n    #Model to be optimized\n    mdl = xgb.XGBClassifier(learning_rate = learning_rate, n_estimators = n_estimators, max_depth = max_depth, \n                            min_child_weight = min_child_weight, gamma = gamma, subsample = subsample, \n                            colsample_bytree = colsample_bytree, seed = 42)\n    \n\n    #Cross-Validation in order to avoid overfitting\n    auc = cross_val_score(mdl, X_train_selected, y_train, cv = 10, scoring = 'roc_auc')\n    \n    print(auc.mean())\n    # as the function is minimization (forest_minimize), we need to use the negative of the desired metric (AUC)\n    return -auc.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a sample space in which the initial randomic search should be performed\nspace = [(1e-3, 1e-1, 'log-uniform'), # learning rate\n          (100, 2000), # n_estimators\n          (1, 10), # max_depth \n          (1, 6.), # min_child_weight \n          (0, 0.5), # gamma \n          (0.5, 1.), # subsample \n          (0.5, 1.)] # colsample_bytree \n\n# Minimization using a random forest with 20 random samples and 50 iterations for Bayesian optimization.\nresult = forest_minimize(tune_xgbc, space, random_state = 42, n_random_starts = 20, n_calls  = 25, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters optimized values\nhyperparameters = ['learning rate', 'n_estimators', 'max_depth', 'min_child_weight', 'gamma', 'subsample',\n                   'colsample_bytree']\n\nfor i in range(0, len(result.x)): \n    print('{}: {}'.format(hyperparameters[i], result.x[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the convergence of the optimization for the parameters that lead to the highest AUC."},{"metadata":{"trusted":true},"cell_type":"code","source":"from skopt.plots import plot_convergence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting up the figure\nfig, ax = plt.subplots(figsize = (20,8))\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.grid(b = None)\n\n# Ploting\nplot_convergence(result)\n\n# Setting up axes and title\nax.set_title('Convergence Plot', fontsize = 18)\nax.set_xlabel('Number of calls (n)', fontsize = 16)\nax.set_ylabel('min(x) after n calls', fontsize = 16);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In practice, the negative of the AUC was minimized, that is, the lower the value, the better the chosen parameters performed.  \n\nThus, it is possible to notice some very relevant leaps along the iterations of the Bayesian optimization. Near the sixth iteration, the negative of the AUC already signals that the hyperparameters found had reached a stable region and values optimized for the AUC.  \n\nTherefore, we proceed to the next steps with the optimal values found for the parameters."},{"metadata":{},"cell_type":"markdown","source":"### 6 Model scoring  \n\nNow that the most important features and the best hyperparameters for the model are known, this model can be applied to the test data for the final evaluation of the model. Again, the AUC metric will be used here."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the model with the optimized hyperparameters\nclf_optimized = xgb.XGBClassifier(learning_rate = result.x[0], n_estimators = result.x[1], max_depth = result.x[2], \n                            min_child_weight = result.x[3], gamma = result.x[4], subsample = result.x[5], \n                            colsample_bytree = result.x[6], seed = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting the model to the X_train_selected dataset\nclf_optimized.fit(X_train_selected, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the performance of the model in the test data (which have not been used so far).\ny_predicted = clf_optimized.predict_proba(X_test_selected)[:,1]\nauc(y_test, y_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, our model AUC Score was **0.8477!** Pretty good so far!\n\nHow dataset comes from a competition of Kaggle, we can test the site of the training data (df_test) on the platform and see what was the AUC value obtained in a dataset of 75818 instances never seen by the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# making predctions on the test dataset (df_test), from Kaggle, with the selected features and optimized parameters\ny_predicted_df_test = clf_optimized.predict_proba(df_test[selected_col])[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving the result into a csv file to be uploaded into Kaggle late subimission \n# https://www.kaggle.com/c/santander-customer-satisfaction/submit\nsub = pd.Series(y_predicted_df_test, index = df_test['ID'], name = 'TARGET')\nsub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now upload the file and see which result we achieve!"},{"metadata":{},"cell_type":"markdown","source":"Seems that our model is also a very good job at data that it has never seen before. That is great!"},{"metadata":{},"cell_type":"markdown","source":"### 7 Results Analysis  \nThroughout the model development process, steps were taken to select features and optimize to generate a robust model, capable of generalizing well to new data and maximizing profits by correctly classifying customers by minimizing the amount of FP and maximizing the amount of TP.  \n\nSo, using the AUC metric, we arrived at a model that:\n\n- On test data, split in step 3, **scored 0.8477 for AUC;**\n- On Kaggle data, in 75818 new instances, **scored 0,8305 for AUC.**\n\nIt can therefore be concluded that the objective of creating a model that maximizes profits has been achieved satisfactorily.\n\nIn addition, we can analyze the ROC curve and better understand the AUC generated by the model.  \nThis brings a better understanding of how profit maximization can be done and we can see the optimum point for the classification decision threshold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code base on this post: https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\nimport sklearn.metrics as metrics\n\n# Calculate FPR and TPR for all thresholds\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_predicted)\nroc_auc = metrics.auc(fpr, tpr)\n\n# Plotting the ROC curve\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize = (20, 8))\nplt.title('Receiver Operating Characteristic', fontsize=18)\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.4f' % roc_auc)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.legend(loc = 'upper left', fontsize = 16)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate', fontsize = 16)\nplt.xlabel('False Positive Rate', fontsize = 16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, analyzing the ROC curve, we can choose the cut-off that maximizes profits. In this case, the point to be chosen is where the AUC curve approaches (shortest distance) from the top of the y scale. Thus, it can be concluded that the cut-off to be chosen is the one that generates an FPR of 0.12 and a TPR of approximately 0.61."},{"metadata":{},"cell_type":"markdown","source":"### 8 Next steps\n\nFor further iterations on this project in order to improve the analysis and the results, I would suggest 3 main points:\n\n- Work on feature engineering creating new features if possible;\n- Try out different ML algorithms and compare them to the XGBClassifier\n- As Caio Martins (https://github.com/CaioMar/) did and suggested me, a nice improvement would be to create a function that calculates the total profit. It is possible once we have values for TP and FP."},{"metadata":{},"cell_type":"markdown","source":"### 9 References\n[1] Banerjee. Prashant, Comprehensive Guide on Feature Selection., https://www.kaggle.com/prashant111/comprehensive-guide-on-feature-selection  \n[2] D. Beniaguev., Advanced Feature Exploration. https://www.kaggle.com/selfishgene/advanced-feature-exploration  \n[3] M. Filho., A forma mais simples de selecionar as melhores variáveis usando Scikit-learn. https://www.youtube.com/watch?v=Bcn5e7LYMhg&t=2027s  \n[4] M. Filho., Como Remover Variáveis Irrelevantes de um Modelo de Machine Learning, https://www.youtube.com/watch?v=6-mKATDSQmk&t=1454s  \n[5] M. Filho., Como Tunar Hiperparâmetros de Machine Learning Sem Perder Tempo, https://www.youtube.com/watch?v=WhnkeasZNHI  \n[6] G. Caponetto., Random Search vs Grid Search for hyperparameter optimization, https://towardsdatascience.com/random-search-vs-grid-search-for-hyperparameter-optimization-345e1422899d  \n[7] A. JAIN., Complete Guide to Parameter Tuning in XGBoost with codes in Python, https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ \n[8] How to plot ROC curve in Python, https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python  \n[9] F. Santana., Algoritmo K-means: Aprenda essa Técnica Essêncial através de Exemplos Passo a Passo com Python, https://minerandodados.com.br/algoritmo-k-means-python-passo-passo/  \n[10] A. Géron., Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems, Alta Books, Rio de Janeiro, 2019, 516 p.  \n[11] W. McKinney., Python for data analysis, Novatec Editora Ltda, São Paulo, 2019, 613 p.  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}