{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A brief description of XGBoost\n<strong><h4>Please Upovote This kernel, if it was useful or helpful!</h4></strong>\n<img src = https://www.h2o.ai/wp-content/uploads/2018/07/xgboost-narrow.png>","metadata":{}},{"cell_type":"markdown","source":"# Before We discussing XGBoost, let's briefly talk about Boosting\n- (XGBoost에 관하여 설명드리기 전에 Boosting에 관하여 간략하게 짚고 넘어가겠습니다.)\n\n### What's Boosting?\n- One of the ensemble techniques is to combine several Week learner's to improve performance.\n    - (앙상블 기법중 하나로 약한 학습기 여러개를 합하여 성능을 높이는 방법입니다.)\n\n<img src = https://itwiki.kr/images/4/45/%EB%B6%80%EC%8A%A4%ED%8C%85%28Boosting%29.png>\n\n","metadata":{}},{"cell_type":"markdown","source":"# What's XGBoost?\n\n- XGBoost stands for Extreme Gradient Boosting.\n    - (XGBoost는 Extend Graident Boosting의 약자입니다.)\n- The algorithm implemented using the boosting technique is representative of Gradient Boost. XGBoost is the implementation of this algorithm in parllel operation.\n    - (부스팅 기법을 이용하여 구현된 알고리즘은 Gradient Boost입니다. 이 알고리즘에 병렬 학습을 이용 가능하도록 한 것이 XGBoost 입니다.)\n- It's the leading machine learning library for regression, classification.\n    - (머신러닝 라이브러리에서 분류, 회귀에 관한 모델을 제공합니다.)\n\n<img src = https://diya-blogpost.s3.us-east-1.amazonaws.com/imgs_2020NLP/xgboost/XGBoost-feature.png>\n\n## Advantages of XGBoost(XGBoost의 장점)\n- A large and growing list of data scientist globally that are actively contributing to XGBoost open source development.\n    - (많은 데이터 과학자들이 XGBoost의 오픈 소스 개발에 적극적으로 기여하고 있습니다.)\n- Usage on a wide range of applications, including solving problems in regression, classification and user-defined prediction challenges.\n    - (XGBoost는 광범위하게 applications에서 사용되고 있으며, 회귀, 분류를 포함하여 다양한 문제를 해결하는데 사용됩니다.)\n- A library that was built from the ground up to be efficient, felxtible and portable\n    - (효율적이며, 유연하게 변경 가능한 라이브러리입니다.)\n- By using Greedy-Algorithm, Automatic pruning is possible. Therefore, Overfitting rarely occurs.\n    - (그리디 알고리즘(탐욕법)을 사용함으로 써, 자동 가지치기가 가능합니다. 그렇게 됨으로써 과적합이 드물게 발생합니다.)\n     \n\n## Disadvantages of XGBoost(XGBoost의 단점)\n- Although the speed problem, which is a disadvantage of GBM, has been solved to extent, but it is still slow.\n    - (GBM의 단점인 속도문제를 해결하였지만, 그래도 느립니다.)\n- If Hyperparameter modify use GridSearchCV, Speed is very slow.\n    - (만약 GridSearchCV를 사용하여, 하이퍼파라미터를 변경할 경우, 속도는 현저히 느립니다.)    ","metadata":{}},{"cell_type":"markdown","source":"# XGBoost \n\n- <h1>Hyperparameter</h1>\n\n    - eta,learning_rate : Learning Rate (0~1)\n        - (학습률 (0~1))\n    - n_estimators : Number of Weak Learner (ex. Decision Tree), (default: 10)\n        - (약한 학습기의 개수)\n    - min_child_weight : Minimum sum of weights for all observations needed in child. (default: 1)\n        - (child 에 필요한 모든 관측지에 대한 관측치에 대한 가중치의 최소 합)\n    - max_depth : Tree max_depth (default: 6)\n        - (트리의 깊이 낮으면 낮을수록 과적합을 방지합니다.)\n    - subsample : Data Sampling rate for tree\n        - (각 트리별 데이터 샘플링 비율)\n    - colsample_bytree :  feature sampling rate for tree\n        - (각 트리별 feature 샘플링 비율)\n    - gamma : It is the minmum loss reduction value that will determine the further division of leaf node\n        - (리프노드의 추가분할을 결정할 최소손실 값이다.)\n    - reg_lambda : L2 Regulation\n        - (L2 가중치)\n    - reg_alpha : L1 Regulation\n        - (L1 가중치)\n    - scale_pos_weight : Balancing unbalanced datasets\n        - (불균형 데이터셋의 균형 유지)\n\n\n<h3>If you need to use GridSearchCV to find best_params_</h3>\n\n- I'm recommend that you start with the largest range and work your way down to finding best_params_.\n    - 큼 범위부터 시작해서 범위를 좁혀가는 방식으로 best_params_을 찾는 방식을 추천드립니다.\n\n<h3>If you make visualization about xgboost</h3>\n\n- XGB_model.plot_importance : indicate the importance of a characteristic\n    - 각 feature의 중요도를 시각화 하여 나타낼 수 있습니다.\n- XGB_model.plot_tree : indicate the Decision Tree\n    - 의사 결정트리를 나타냅니다.\n\n\n* Reference : https://xgboost.readthedocs.io/en/latest/python/python_api.html    ","metadata":{}},{"cell_type":"code","source":"# This is an example. \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n\ncust_df = pd.read_csv('../input/santander-customer-satisfaction/train.csv')\n\ncust_df['var3'].value_counts()\ncust_df['var3'].replace(-999999, 2, inplace = True)\ncust_df.drop('ID', axis = 1, inplace = True)\n\nX_feature = cust_df.iloc[: , :-1]\ny_label = cust_df.iloc[:,-1]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_feature, y_label, test_size = 0.2, random_state = 56)\nxgb_clf = XGBClassifier(n_estimators = 100, random_state = 156)\nxgb_clf.fit(X_train, Y_train, early_stopping_rounds = 100,eval_metric ='auc', eval_set = [(X_test, Y_test)])  ","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:42:59.125705Z","iopub.execute_input":"2021-12-21T05:42:59.126111Z","iopub.status.idle":"2021-12-21T05:43:46.537054Z","shell.execute_reply.started":"2021-12-21T05:42:59.126062Z","shell.execute_reply":"2021-12-21T05:43:46.535992Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Plot_imortance\n- you can visualize the importance of features.\n    - (plot_importance를 활용하여 특성의 중요도를 시각화할 수 있습니다.)","metadata":{}},{"cell_type":"code","source":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfig , ax = plt.subplots(1, 1, figsize = (10, 10))\nplot_importance(xgb_clf, max_num_features= 20, ax = ax, height = 0.4)","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:43:46.538819Z","iopub.execute_input":"2021-12-21T05:43:46.53906Z","iopub.status.idle":"2021-12-21T05:43:47.076436Z","shell.execute_reply.started":"2021-12-21T05:43:46.53903Z","shell.execute_reply":"2021-12-21T05:43:47.075543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost plot_tree\n- you can visualize Decision tree\n    - (의사결정트리를 활용하여 분류를 어떻게 하였는지 시각화할 수 있습니다.)","metadata":{}},{"cell_type":"code","source":"from xgboost import plot_tree\nfig, ax = plt.subplots(figsize=(80, 80))\nxgboost.plot_tree(xgb_clf, num_trees=4, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-21T05:43:47.078041Z","iopub.execute_input":"2021-12-21T05:43:47.078319Z","iopub.status.idle":"2021-12-21T05:43:50.781557Z","shell.execute_reply.started":"2021-12-21T05:43:47.078286Z","shell.execute_reply":"2021-12-21T05:43:50.780778Z"},"trusted":true},"execution_count":null,"outputs":[]}]}