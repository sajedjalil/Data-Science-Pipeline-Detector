{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.6.0"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"_change_revision":0,"_is_fork":false},"cells":[{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"This Notebook is the registry of a three guys work in a discipline of introduction to Machine Learning at USP (Brazil).\n------------------------------------------------------------------------","metadata":{"_cell_guid":"41b05790-1a2e-c7cc-c089-2fa3f9874b72","_uuid":"f44d748b7237666f569576b0dc022af1dfa6cefa"}},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"----------\n\n1. Loading and cleaning up data\n-------------------\n\n**1.1. Removing non-informative, constant and duplicate columns**","metadata":{"_cell_guid":"23a6dfff-6071-1855-af21-ba38e5f30cbe","_uuid":"2d59aaff3ebcafed3b06151dee21287430e6bc33"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"b3ce0c4a-489a-a104-8a8f-6f87f09ecebe","_uuid":"83ae5397b6a865bff41308193db2c653a05bd0f7","_execution_state":"busy"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# load data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\n# remove \"ID\" column\ntrain_df.drop(\"ID\", axis=1, inplace=True)\ntest_df.drop(\"ID\", axis=1, inplace=True)\n\n# remove constant columns\nfor col in train_df.columns:\n    if (train_df[col].values.std()==0):\n        train_df.drop(col, axis=1, inplace=True)\n        test_df.drop(col, axis=1, inplace=True)\n\n# remove duplicate columns\ncols_to_drop = []\nfor i in range(len(train_df.columns)):\n    col1 = train_df.columns[i]\n    for j in range(i+1, len(train_df.columns)):\n        col2 = train_df.columns[j]\n        if (np.array_equal(train_df[col2].values, train_df[col1].values)):\n            cols_to_drop.append(col2)\nfor col in cols_to_drop:\n    train_df.drop(col, axis=1, inplace=True)\n    test_df.drop(col, axis=1, inplace=True)\n\n# split original train data frame into 80:20 train and test subsets for internal tests\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(train_df, test_size = 0.2)\n\n# split features and target variables\ntrain_features_df = train_df.drop('TARGET', axis=1)\ntrain_target_df = train_df['TARGET']\ntest_features_df = test_df"},{"outputs":[],"cell_type":"code","execution_count":null,"source":"[len(train_df), len(train_df[0]), len(test_df)]","metadata":{"collapsed":false,"_uuid":"49b863e4e5282641f3780a6ce48e158edeb726df","_execution_state":"busy"}},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"**1.2. Classifying features by type: continuous, discrete or categoric**","metadata":{"_cell_guid":"d66fbff6-5265-2d33-ac74-76d687b7340b","_uuid":"4939ef6abb243c82ddfd0cabe84abfb627fe12b9"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"00ebf557-7e49-14e5-4641-247ac630c25f","_uuid":"00e5fbb5712e3a1d2713b4ae2c45a2ba295e5b96","_execution_state":"busy"},"source":"categoric_ref = 20\nfeature_types = []\nfor i in range(len(train_features_df.columns)):\n    col = train_features_df.columns[i]\n    var_set = set(train_features_df[col].values)\n    var_nr = len(var_set)\n    var_sum = sum(var_set)\n    if (var_sum-int(var_sum)>0.01):\n        var_type = \"Continuous\"\n    else:\n        if (var_nr>categoric_ref):\n            var_type = \"Discrete\"\n        else:\n            var_type = \"Categoric\"\n    feature_types.append(var_type)\nfeature_types[0:20]"},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"**1.3. Replacing extreme values by representative values according to each feature type**","metadata":{"_cell_guid":"ba45b6f0-4d85-5f75-0a2e-5a89689990cb","_uuid":"630a927ca6578d823c25dd1c043ea52000584944"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"99594cd7-e5f9-57c4-c9f1-296f8c4f558e","_uuid":"674763cf461e6f54c8675ad513f8c398bdc3ee3d","_execution_state":"busy"},"source":"# replace extreme values by mean values of each feature\nimport statistics as stc\nvars_max = max([max(train_features_df[train_features_df.columns[i]].values) for i in range(len(train_features_df.columns))])\nvars_min = min([min(train_features_df[train_features_df.columns[i]].values) for i in range(len(train_features_df.columns))])\nprint([vars_max, vars_min])\nfor i in range(len(train_features_df.columns)):\n    col = train_features_df.columns[i]\n    if (feature_types[i] != \"Categoric\"):\n        var_replace = train_features_df[col].values.mean()\n    else:\n        feat_vals = set(train_features_df[col].values)\n        try:\n            feat_vals = feat_vals.discard(vars_min)\n            feat_vals = feat_vals.discard(vars_max)\n            var_replace=stc.mode(feat_vals)\n        except:\n            var_replace=0\n    train_features_df[col].replace(to_replace=vars_max, value=var_replace, inplace=True)\n    train_features_df[col].replace(to_replace=vars_min, value=var_replace, inplace=True)\nvars_max = max([max(train_features_df[train_features_df.columns[i]].values) for i in range(len(train_features_df.columns))])\nvars_min = min([min(train_features_df[train_features_df.columns[i]].values) for i in range(len(train_features_df.columns))])\nprint([vars_max, vars_min])"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"0c68a4db-aad0-aa4b-bfa0-782cba415799","_uuid":"1ce4ee563310dc84b09c9b8ba83a7d87e43ac4b1","_execution_state":"busy"},"source":"train_features_df.head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"44fc63a0-9ec0-33be-1394-4050ac8df1b2","_uuid":"3598eaa0eaa3e6dab7ebec29b22f17c3ca5e6c91","_execution_state":"busy"},"source":"train_target_df.head()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"357bff54-45a5-9160-8540-2a60a8a119b0","_uuid":"79229ce924a90e893e046fbf9223d2c1a314a279","_execution_state":"busy"},"source":"test_features_df.head()"},{"outputs":[],"cell_type":"code","execution_count":null,"source":"# show 20 feature distributions randomly chosen\nfrom random import randint\ncols_nr = 5\nrows_nr = 20 #int(vars_nr/5) +1\nplt.figure(figsize=(15, 50))\nfor i in range(20):\n    ind = randint(0,len(train_features_df.columns))\n    col = train_features_df.columns[ind]\n    plt.subplot(rows_nr, cols_nr, i + 1)\n    plt.hist(train_features_df[col])\n    if (feature_types[ind]==\"Continuous\"):\n        plt.gca().set_xscale(\"log\")\n    plt.gca().set_yscale(\"log\")\n    plt.xlabel(\"${}$\".format(col), fontsize=14)\n    if i == 0:\n        plt.ylabel(\"$TARGET$\", fontsize=14)\n    plt.title(\"{}\".format(feature_types[ind]+\" - id=\"+str(ind)), fontsize=16)\n    plt.tight_layout()\nplt.show()","metadata":{"collapsed":false,"_uuid":"5bf68a658985f336879ba34c19eb5e26253786a5","_cell_guid":"c8846eab-7a32-4d63-93e7-a6b1f847c849","_execution_state":"busy"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"set(train_features_df[train_features_df.columns[176]].values)","metadata":{"collapsed":false,"_uuid":"64d596f0b78b7f9b4132cdec74dd1e53d0d3f19f","_cell_guid":"53b61dd0-e127-4e24-958e-f3a29ceaefe0","_execution_state":"busy"}},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"----------\n\n2. Selecting the most representative variables\n----------------------------------------------\n\n**2.1. Using the Normalized Mutual Information between each variable and TARGET:**","metadata":{"_cell_guid":"874ad4b4-ba71-5cb0-b51c-45e175edc30d","_uuid":"dd0681f62f1780e2e59666664a6bea2faac8e873"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"3788c863-cc15-95bc-5661-166c5c50a7cc","_uuid":"452ba2b3826907df288d1fcc263f8a919985cfd7","_execution_state":"busy"},"source":"# ordering all variables according to its MI regarding to TARGET\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\nimport bisect\nmi = []\nvar_ids_mitop = []\nfor i in range(len(train_features_df.columns)):\n    scoring_var = train_features_df[train_features_df.columns[i]].values\n    mi.append(round(normalized_mutual_info_score(train_target_df, scoring_var), 4))\n    inserted=False\n    for j in range(len(var_ids_mitop)):\n        if (mi[i]>mi[var_ids_mitop[j]]):\n            var_ids_mitop.insert(j, i)\n            inserted=True\n            break\n    if (not inserted):\n        var_ids_mitop.append(i)\n\n# selecting the best variables with low MI (<0.5) between them\nmi_ref = 0.5\nvar_ids_mitop_relev = []\nfor i in range(len(var_ids_mitop)):\n    var_id = var_ids_mitop[i]\n    var_ref = train_features_df[train_features_df.columns[var_id]].values\n    low_mi = True\n    for j in range(len(var_ids_mitop_relev)):\n        var = train_features_df[train_features_df.columns[var_ids_mitop_relev[j]]].values\n        if (normalized_mutual_info_score(var, var_ref)>mi_ref):\n            low_mi = False\n            break\n    if (low_mi):\n        var_ids_mitop_relev.append(var_id)\n\nmis_top20 = [[train_features_df.columns[i], feature_types[i], mi[i]] for i in var_ids_mitop_relev[0:20]]\nmis_top20"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"0c8710de-acd1-080b-b7cf-b0993e50c96d","_uuid":"7701f4c636418d53eb5f102b453b7cc8b034a866","_execution_state":"busy"},"source":"len(var_ids_mitop_relev)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c82df779-1c49-a924-ac9c-c963a3737509","_uuid":"c209ffb9d9d505e70cde7be643d36e507cdbdee6","_execution_state":"busy"},"source":"cols_nr = 5\nrows_nr = 4 #int(vars_nr/5) +1\nplt.figure(figsize=(15, 10))\nfor i in range(20):\n    ind = randint(0, len(var_ids_mitop_relev))\n    col = train_features.columns[var_ids_mitop_relev[ind]]\n    plt.subplot(rows_nr, cols_nr, i + 1)\n    plt.scatter(train_features[col], train_target.values)\n    plt.xlabel(\"$var_{}$\".format(str(var_ids_mitop_relev[ind] + 1)), fontsize=14)\n    if i == 0:\n        plt.ylabel(\"$TARGET$\", fontsize=14)\n    plt.title(\"MI={:.2f}\".format(mi[var_ids_mitop_relev[ind]]), fontsize=16)\n    plt.tight_layout()\nplt.show()"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"87b7c6b1-47a8-f4c5-3a32-80232ff96966","_uuid":"cd180171478214685e53321b0d2ea9cc396b68e2","_execution_state":"busy"},"source":"cols_nr = 5\nrows_nr = 4 #int(vars_nr/5) +1\nplt.figure(figsize=(15, 10))\nfor i in range(20):\n    var_id = var_ids_mitop_relev[i]\n    col = train_features.columns[var_id]\n    plt.subplot(rows_nr, cols_nr, i + 1)\n    plt.hist(train_features[col])\n    if (feature_types[var_id]==\"Continuous\"):\n        plt.gca().set_xscale(\"log\")\n    plt.gca().set_yscale(\"log\")\n    plt.xlabel(\"${}$\".format(col), fontsize=14)\n    if i == 0:\n        plt.ylabel(\"$TARGET$\", fontsize=14)\n    plt.title(\"MI={:.2f}\".format(mi[var_id]), fontsize=16)\n    plt.tight_layout()\nplt.show()"},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"**2.2. Another selection criterion based on sklearn.ExtraTreesClassifier:**","metadata":{"_cell_guid":"087b828b-cb94-dd6b-05bc-a2debd9d6639","_uuid":"263a0b47843187029632c807ac7c8ef1c241ee7c"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"d278a874-c783-3536-19c0-409cd795a0b4","_uuid":"8119ea57bb6c787b4ce7a393c9fb3849d9fab17e","_execution_state":"busy"},"source":"# split data into train and test\nfrom sklearn.cross_validation import train_test_split\ntest = test_features\nX = train_features\ny = train_target.values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1729)\n\n## # Feature selection\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(random_state=1729)\nselector = clf.fit(train_features, train_target)\n\n# plot most important features\nfeat_imp = pd.Series(clf.feature_importances_, index = train_features.columns).sort_values(ascending=False)\nfeat_imp[:40].plot(kind='bar', title='Feature Importances according to ExtraTreesClassifier', figsize=(12, 8))\nplt.ylabel('Feature Importance Score')\nplt.subplots_adjust(bottom=0.3)\nplt.savefig('1.png')\nplt.show()\n\n# clf.feature_importances_ \nfs = SelectFromModel(selector, prefit=True)\n\nX_train = fs.transform(train_features)\nX_test = fs.transform(test_features)\n\nprint(X_train.shape, X_test.shape)\n\n# calculate the auc score\n#print(\"Roc AUC: \", roc_auc_score(y_test, m2_xgb.predict_proba(X_test)[:,1], average='macro'))\n              \n## # Submission\n#probs = m2_xgb.predict_proba(test)\n\n#submission = pd.DataFrame({\"ID\":test_id, \"TARGET\": probs[:,1]})\n#submission.to_csv(\"submission.csv\", index=False)"},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"3. Making a balanced sample for some ML technics\n------------------------------------------------","metadata":{"collapsed":false,"_uuid":"4e3fe3204f67a46ed756cd0f0d86cbbccd58d57a","_cell_guid":"2703f6ea-d378-46b5-b492-26038f8f7927","_execution_state":"idle"}},{"outputs":[],"cell_type":"code","execution_count":null,"source":"from numpy import unique\nfrom numpy import random \n\ndef balanced_sample_maker(X, y, random_seed=None):\n    \"\"\" return a balanced data set by oversampling minority class \n        current version is developed on assumption that the positive\n        class is the minority.\n    Parameters:\n    ===========\n    X: {numpy.ndarrray}\n    y: {numpy.ndarray}\n    \"\"\"\n    uniq_levels = unique(y)\n    uniq_counts = {level: sum(y == level) for level in uniq_levels}\n    if not random_seed is None:\n        random.seed(random_seed)\n\n    # find observation index of each class levels\n    groupby_levels = {}\n    for ii, level in enumerate(uniq_levels):\n        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n        groupby_levels[level] = obs_idx\n\n    # oversampling on observations of positive label\n    sample_size = uniq_counts[0]\n    over_sample_idx = random.choice(groupby_levels[1], size=sample_size, replace=True).tolist()\n    balanced_copy_idx = groupby_levels[0] + over_sample_idx\n    random.shuffle(balanced_copy_idx)\n\n    return X[balanced_copy_idx, :], y[balanced_copy_idx]\n\ntrain_features_balanced, train_target_balanced = balanced_sample_maker(train_features, train_target.ravel())\n[len(train_features), len(train_features_balanced)]","metadata":{"collapsed":false,"_uuid":"57e0183ab8920a6142d581629408ffdcd7d0f30f","_cell_guid":"9132d841-be3f-4153-aef2-1c04437ee764","_execution_state":"busy"}},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"----------\n4. Na√Øve Bayes implementation for the 20 most relevant variables\n-----------------------------------------------------------------\n**4.1. Relevance by Mutual Information**","metadata":{"_cell_guid":"bf3bce9c-1430-bc02-975d-829aeb3a8e8c","_uuid":"dfb692eeddca16a918932c5d45557f8b10ce1ddc"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"dba2f496-0765-d790-47bc-d7e14d8db256","_uuid":"b2699e9de64f73a6462f80d379d22db482e5685f","_execution_state":"busy"},"source":"import sklearn.naive_bayes as nb\nfeature_idxs = var_ids_mitop_relev[0:1]\ntrain_features_array = np.array(train_features[feature_idxs])\ntrain_target_array = np.array(train_target.ravel())\ntest_features_array = np.array(test_features[feature_idxs])\n#gnb = nb.GaussianNB().fit(train_features_array, train_target_array)\ntrain_features_array = np.array([[0,1] for i in range(50)]+[[1,0] for i in range(50)])\ntrain_target_array = np.array([0 for i in range(50)]+[1 for i in range(50)])\nmnb = nb.MultinomialNB().fit(train_features_array, train_target_array)\ntrain_prediction_array = mnb.predict(train_features_array)\n#test_prediction_array = mnb.predict(test_features_array)\n# calculate the auc score\nprint(\"Roc AUC: \", roc_auc_score(train_target_array, train_prediction_array, average='macro'))\nprint(\"Roc AUC: \", roc_auc_score(train_target_array, [randint(0,1) for i in range(len(train_target_array))], average='macro'))\n\n#out = [sum(train_prediction_array), sum(train_target_array), sum(test_prediction_array), len(test_prediction_array)]\n#out"},{"outputs":[],"cell_type":"markdown","execution_count":null,"source":"**4.2. Relevance by ExtraTreesClassifier**","metadata":{"_cell_guid":"fd77e410-d83f-57ea-e441-4a1e5fd9769a","_uuid":"740f97c282982e57a47e6a19a809b40803b485d7"}},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"c1ecda49-f05e-76fe-2cd9-f29039bd2493","_uuid":"943e154f5c109f1efe63a156e2c10a6b8864bec3","_execution_state":"busy"},"source":"selected_features = feat_imp.keys().values[0:20]\ntrain_features_array = np.array(train_features[selected_features])\ntrain_target_array = np.array(train_target).ravel()\ntest_features_array = np.array(test_features[selected_features])\ngnb = nb.GaussianNB().fit(train_features_array, train_target_array)\n#mnb = nb.MultinomialNB().fit(train_features, train_target)\ntrain_prediction_array = gnb.predict(train_features_array)\ntest_prediction_array = gnb.predict(test_features_array)\nout = [sum(train_prediction_array), sum(train_target_array), sum(test_prediction_array), len(test_prediction_array)]\nout"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"24768a5a-3169-37d7-ecd5-40aee320b61d","_uuid":"8beac6d7c1a4095f17379fea4ead9c7f93f3bc23","_execution_state":"busy"},"source":"print(gnb.theta_)\nprint(gnb.sigma_)"},{"cell_type":"code","execution_count":null,"outputs":[],"metadata":{"_cell_guid":"cf30a17e-b9c9-b40e-90ce-0fa997c9b826","_uuid":"6c209cf91de4bc1441b6a44e6965d8557a2bf72a","_execution_state":"busy"},"source":"df = train_features\nvar_maxs = [max(df[df.columns[i]].values) for i in range(len(df.columns))]\nvar_maxs"}]}