{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Genetic Algorithms: Feature Selection & Hyperparameters\n\n![](https://g-scop.grenoble-inp.fr/medias/photo/gscop-rub-oc_1425923414759-png)\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nThis notebook aims to combine my love of genetic algorithms with the model building steps of feature selection and hyperparameter optimisation. I have previously used genetic algorithms to choose which features to include in a model and then as a secondary process conducted hyperparameter optimisation. This time I want to see if the two steps can be integrated.\n\nThis notebook has 2 primary objectives:\n1. Serve as practice using Python as I am typically an R user\n1. Apply Genetic Algorithms to combined feature selection and hyperparameter optimisation\n\nI believe there are two main ways to achieve the optimisation of feature selection and hyperparameters:\n\n1. Nested: For a candidate solution, firstly choose which features to include and then optimise hyperparameters\n1. Simualtaneous: For a candidate solution choose both feature subsets and hyperparameters at the same time\n\nThis notebook explores the first case, the reason for this is it is thought that nesting will provide a greater range of hyperparameter values for each model. As a result this likely means that this approach will take longer than the simualtaneous approach.\n\nThe 'Santander Customer Satisfaction' dataset will be used within this notebook as it high dimensional (300+ predictors), providing a good opportunity to test the optimisation process (especially the third objective). \n\nThe notebook has been split into the following sections:\n\n* [Introducing Genetic Algorithms](#section-one)\n* [Prepare Environment](#section-two)\n* [Basic Data Insight & Health Check](#section-three)\n* [Validation & Cross Validation](#section-four)\n* [Preprocessing](#section-five)\n* [Optimisation Model Development](#section-six)\n    - [Generate Population](#section-six-subsection-one)\n    - [Solution Evaluation](#section-six-subsection-two)   \n* [Model Optimisation](#section-seven)\n    - [ElasticNet](#section-seven-subsection-one)\n    - [XGBOOST](#section-seven-subsection-two)\n    - [Analysis Findings](#section-seven-subsection-three)      \n* [Model Evaluation](#section-eight)\n* [Conclusion](#section-nine)  \n* [Submission](#section-ten)  "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n## Introducing Genetic Algorithms\n\nGenetic Algorithms (GA) are a stochastic optimisation tool which draws inspiration from biological evolution. GA aims to minimise (or maximise in this case) a fitness function by changing decision variables. The core concepts of genetic algorithms are:\n\n* Fitness function - This is the function or measure which we want to maximise or minimise\n* Gene - An individual part of a solution\n* Chromosome - The combined gene's to create a single solution\n* Population - This is a predifined number of different Chromosomes to have a family of solutions\n* Generation - The model will run for a predifined number of iterations/ generations. Within each generation a new 'population' will be created and evaluated\n* Selection - The core concept of GA is survival of the fittest, the selection process allows us to probabilistically choose the best solutions in the current generation to be progressed for the next generation\n* Crossover - Once the fittest solutions are selected, crossover is the process of merging two solutions with the aim of producing offspring which has a better fitness score \n* Mutation - This is the process of introducing randomness into the population by probabilistically changing elements of a solution\n* Elitism - This is ensuring that the best solution in the current generation enters the next generation without being altered at all\n\nThe image below provides a visual guide to how the optimisation process works.\n\n![](https://www.neuraldesigner.com/images/genetic_algorithm.png)\n\n\n### Relating GA to Feature Selection and Hyperparameters\n\nIn the context of feature selection and hyperparameter optimisation, some elements of GA are described below:\n\n* Fitness function - An evaluation metric such AUC, with higher values being considered as more fit solutions\n* Gene - An individual predictor encoded as a binary flag (1 = included, 0 = excluded). It can also be an individual hyperparameter taking any real value\n* Chromosome - All of the individual features which were flagged as included, plus the hyperparameter values choosen\n* Population - A predefined number of models which have different features (included/ excluded) and hyperparameters of varying values"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## Prepare Environment\n\nNow that we have a basic understanding of GA, we prepare the environment to build the optimisation model and search for a good predictive model (features and hyperparameters) for this dataset. In this section we prepare the environment for analysis which includes loading libraries for functionality and data to work with. \n\nThe following packages are used in this analysis:\n\n1. Numpy: Numerical computing\n1. Pandas: Dataframes\n1. Scipy: Similarity measures\n1. Sklearn: Supervised Learning framework"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# -------------------------------------------------- Libraries --------------------------------------------------\n#################################################################################################################\n\n# Numerical computing & dataframes\nimport numpy as np # numerical computing\nimport pandas as pd # dataframes\nimport numpy.random as rnd # Pseudo random numbers\nfrom scipy import spatial\n\n# Modelling with sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn import metrics\n\n# Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n# warnings\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# Visuals\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\n\n#################################################################################################################\n# -------------------------------------------------- Load Data --------------------------------------------------\n#################################################################################################################\n\n# Load Data\ndf_train = pd.read_csv(\"../input/santander-customer-satisfaction/train.csv\", index_col=\"ID\")\ndf_test = pd.read_csv(\"../input/santander-customer-satisfaction/test.csv\", index_col=\"ID\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Basic Data Insight & Health Check\n\nThe output below shows that we have 76K* rows in this dataset and 370 columns (quite high dimensional). The number of columns was one of the reasons why this dataset was choosen. Our target variable is binary and we can see that it is highly unbalanced with 96% of observations having the 0 trait (dissatisfied) and only 4% having the 1 trait (satisfied)."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# ------------------------------------------------- Data Insight ------------------------------------------------\n#################################################################################################################\n\n# Dataset shape\nprint(\"{} rows and {} columns\".format(*df_train.shape))\n\n# Target variable\ndf_train.TARGET.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The outputs below provide some intuition around the dataset, we can see that all predictors are numeric fields, with 259 being integer and the remaining 111 being of type float.\n\nRunning a quick check for missingness, we see that there are no cells with 'nan' but when looking at the summary stats of 'var3' we can see that some values are coded as -999999 which indicates a missing value."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# ------------------------------------------------- Data Health -------------------------------------------------\n#################################################################################################################\n\n# Data types\ndisplay(df_train.dtypes.value_counts())\n\n# No missing observations in the dataset.\nprint(\"Number of Missing entries: \" + str(df_train.isnull().sum().sum()))\n\n# Describe date\ndf_train.var3.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output below shows the percentage of rows which the missing value represents in 'var3'. As the missing entries relate to a very small proportion we can replace it with the median value which was shown to be two above, which leaves us with a nice health dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# View proprtion of missingness\ndisplay(df_train['var3'].value_counts(normalize=True)[0:3])\n\n# Replace missing value\ndf_train['var3'] = df_train['var3'].replace(-999999, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## Validation & Cross Validation\n\nAs this dataset was part of a kaggle competition there is a specific test dataset. To support in the model building process a validation set will be created, which will account for 80% of the training data (reducing the actual training data to 80%). This will enable us to evaluate potential solutions quickly.\n\n5 fold cross validation will be applied to the new training data within the optimisation process to help get a stable estimate of which features and hyperparameters to include as our final models. \n\nOnce we are happy with our model choice we will build the model on the full training data (train + validation) and use it to predict the test data provided. "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# -------------------------------------------------- Split Data -------------------------------------------------\n#################################################################################################################\n\n# Create a validation and training dataset\ndf_train, df_validation = train_test_split(df_train,\n                                           test_size=0.2,\n                                           random_state=1989,\n                                           stratify=df_train.TARGET,\n                                           shuffle=True)\n\n#################################################################################################################\n# ----------------------------------------------- Cross Validation ----------------------------------------------\n#################################################################################################################\n\n# Store the Kfold object\nkfold = KFold(n_splits=5, random_state=1989, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## Preprocessing\n\nIn this section we will apply a simple standardisation and range scaler to all of our numeric variables. In general it is vital to conduct an in-depth exploratory analysis as well as feature engineering, however as this is not the purpose of this notebook, it will be omitted."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# ----------------------------------------------- Preprocess Data -----------------------------------------------\n#################################################################################################################\n\n# Identify columns\nfts_num = df_train.drop(axis=1,columns=['TARGET']).select_dtypes(np.number).columns\n\n# Numerical Transformer StandardScaler\ntrans_num = Pipeline(steps = [('Standarise', StandardScaler()), ('MinMax', MinMaxScaler())])\n\n# Create a single Preprocessing step for predictors\npreprocessor_preds = ColumnTransformer(\n    transformers=[\n        ('num', trans_num, fts_num) # Centre and scale and constrain range\n    ])\n\n# Apply the transformations to train\ndf_train2 = pd.DataFrame(preprocessor_preds.fit_transform(df_train))\ndf_train2.columns = fts_num\n\n# Apply the transformations to validation\ndf_validation2 = pd.DataFrame(preprocessor_preds.fit_transform(df_validation))\ndf_validation2.columns = fts_num\n\n# Apply the transformations to test\ndf_test2 = pd.DataFrame(preprocessor_preds.fit_transform(df_test))\ndf_test2.columns = fts_num\n\n# Create preprocessed training data\ndf_train = pd.concat([df_train2,\n                      df_train.drop(axis=1,columns=fts_num).reset_index().drop(axis=1,columns=['ID'])],\n                     axis=1)\n\n# Create preprocessed validation data\ndf_validation = pd.concat([df_validation2,\n                           df_validation.drop(axis=1,columns=fts_num).reset_index().drop(axis=1,columns=['ID'])],\n                          axis=1)\n\n# Create preprocessed test data\ndf_test = pd.concat([df_test2,\n                     df_test.drop(axis=1,columns=fts_num).reset_index().drop(axis=1,columns=['ID'])],\n                    axis=1)\n\n# Clear objects\ndel df_train2, df_validation2, df_test2, fts_num, trans_num, preprocessor_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## Optimisation Model Development\n\nIn this section we walk through the functions which will make up our GA optimisation model."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-subsection-one\"></a>\n### Generate Population\n\nThis section focusses on generating a population of solutions adhering to the principles with GA for each generation. \n\nThe first function below generates the initial solutions for both features and hyperparameters.\n\nChoosing Initial Features:\n1. For each solution in the population randomly choose a percentage between 10-91% to be applied to the total number of features\n1. For each solution randomly choose features until it accounts for the percentage of columns of the total columns\n\nChoosing Initial Hyperparameters:\n1. For each solution generate a random value for each hyperparameter between a user stated min and max"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Randomly generate candidates\ndef f_random_candidates(features_name, population, hyperparams, output_type, df_pop=False):\n    '''create an initial population'''\n   \n    # Create solution for features\n    if output_type == 'feature':\n        \n        # Initial population will have between 10-91% of features\n        feature_size = rnd.choice(a=range(10,91),size=population, replace=True)\n        feature_size = [np.round(pct / 100 * len(features_name)) for pct in feature_size]\n        \n        # Create a list of feature positions for each candidate\n        selection = [rnd.choice(a=range(0,len(features_name)-1), replace=False, size=cols.astype('int')) \\\n                     for cols in feature_size]\n        \n        selection = [list(selection[i]) for i in range(len(selection))]\n        \n        # Return\n        return selection\n    \n    # Create solution for hyperparameters\n    elif (output_type == 'hyperparams') & (hyperparams != False):\n        \n        # Generate random numbers in range for each hyperparameter\n        random_hyperparams = []\n        for j in range(len(hyperparams['names'])):\n            temp = (np.random.uniform(hyperparams['min_value'][j],\n                                      hyperparams['max_value'][j],\n                                      population))\n            random_hyperparams.append(temp)\n        \n        # Get length of features\n        n_features = df_pop['features'].apply(len).tolist()\n\n        # Store hyperparameters in diction\n        hyperparam_vals = []\n        for i in range(population):\n            val = {'name':[],'value':[]}\n            for j in range(len(hyperparams['names'])):\n                val['name'].append(hyperparams['names'][j])\n                temp = random_hyperparams[j][i]\n                if hyperparams['type'][j] == 'int':\n                    temp = np.int64(round(temp))\n                if hyperparams['names'][j] == 'max_features':\n                    temp = min(temp, n_features[i])\n                val['value'].append(temp)           \n            \n            hyperparam_vals.append(val)\n            del val\n\n        # Return\n        return hyperparam_vals","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the second generation onwards, we can use the crossover function to generate new solutions for the next generation. This function allows us to merge good solutions with the aim of creating a better solution, this is done for both hyperparameters and feature selection.\n\nCrossover Features:\n1. Randomly generate an integer which represents a cross point between the first and last feature (ultimately a column index)\n1. Weighted sampling of the previous generation's solutions and select two parents\n1. Create a child solution which have all the features before the cross point (column index) from first parent and all the features from the second parent after the cross point\n\nCrossover Hyperparameters:\n1. Weighted sample of previous generation solutions of the size of the number of hyperparameters\n1. Randomly choose which hyperparameter to take from which parent solution\n1. Create the child hyperparameters from the choosen parent solution"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Crossover function\ndef f_gen_child_crossover(df, features_name, hyperparams, output_type):\n    '''Mutate 2 parents to create a child'''\n    \n    # Crossover features\n    if output_type == 'feature':\n        \n        # Create an integer list of features\n        l_features = list(range(0,len(features_name)))\n           \n        # Identify a random cross over point\n        cross_point = np.int(rnd.randint(low=0, high=len(features_name), size=1))\n        \n        # Extract Two Parents\n        selection = np.random.choice(df.features, \n                                     size=2, \n                                     replace=False, \n                                     p=df.probability)     \n        par1 = list(selection[0])\n        par2 = list(selection[1])\n            \n        # Convert to Boolean\n        par1 = [item in par1 for item in l_features]\n        par2 = [item in par2 for item in l_features]\n        \n        # Single point cross over and convert to indices\n        child = par1[0:cross_point] + par2[cross_point:]\n        child = [i for i,x in enumerate(child) if x == True]    \n    \n        # Return \n        return child\n    \n    # Crossover hyperparameters\n    elif (output_type == 'hyperparams') & (hyperparams != False):\n        \n        # Identify the number of parameters\n        n_hyperparameters = len(hyperparams['min_value'])\n        \n        # Extract n Parents\n        selection = np.random.choice(df.hyperparameters, \n                                     size=n_hyperparameters, \n                                     replace=False, \n                                     p=df.probability)  \n        \n        # Randomly choose which parent to select each parameter from\n        parent_choice = list(np.random.choice(range(n_hyperparameters),\n                                              size = n_hyperparameters,\n                                              replace=False))\n        \n        # Copy the parent as the child\n        child = selection[0]\n\n        # Update child vector with choosen parent\n        for i in range(n_hyperparameters):\n            child['value'][i] = selection[parent_choice[i]]['value'][i]\n                    \n        # Return \n        return child","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the second generation, we can also use the mutatue function to alter new solutions for the next generation. This function allows us to introduce randomness into the population by randomly altering an aspect of the solution.\n\nMutate Features:\n1. For each feature in the dataframe, generate a random number between 0 and 1\n1. If the generated probability is below the user stated mutation rate, then reverse the switch for that column (i.e. if a feature is included then remove it and vice versa).\n\nMutate Hyperparameters:\n1. For each hyperparameter in the choosen model, generate a random number between 0 and 1\n1. If the random number is below the user stated mutation rate, the generate a random number between a stated range\n1. Finally check if the hyperparameter is outside of the min-max range and reduce it to that range if necessary"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Mutate function\ndef f_gen_child_mutate(candidate, features_name, p_mutate,\n                       hyperparams, output_type, \n                       hyperparams_increment):\n    '''Mutate 2 parents to create a child'''\n    \n    # Mutate Features\n    if output_type == 'feature':\n        \n        # Create an integer list of features\n        l_features = list(range(0,len(features_name)))\n        \n        # Convert feature into boolean vector\n        candidate = [item in candidate for item in l_features]\n        \n        # Conditionally mutate features in chromosome (reverse binary flag)          \n        candidate_new = []\n        for item in candidate:\n            if rnd.rand() <= p_mutate:\n                candidate_new.append(not item)\n            else:\n                candidate_new.append(item)\n                \n        # Convert to indicies\n        candidate_new = [i for i,x in enumerate(candidate_new) if x == True]    \n    \n        # Return \n        return candidate_new\n    \n    # Mutate hyperparameters\n    elif (output_type == 'hyperparams') & (hyperparams != False):\n        \n        # Identify size of mutation\n        v_mutate = (np.random.uniform((1-hyperparams_increment), \n                                      (1+hyperparams_increment), 1)).item()\n        \n        # Identify Min and Max for parameters\n        l_min =  hyperparams['min_value']\n        l_max =  hyperparams['max_value']\n        \n        # Identify the number of parameters\n        n_hyperparameters = len(l_min)\n        \n        # Probabilistically mutate certain parameters\n        candidate_new = []       \n        for i in range(n_hyperparameters):\n            if rnd.rand() <= p_mutate:   \n                temp = candidate['value'][i] * v_mutate\n                if hyperparams['type'][i] == 'int':\n                    temp = np.int64(round(temp))                \n                candidate_new.append(temp)\n            else:\n                candidate_new.append(candidate['value'][i])\n        \n        # Ensure that value is between ranges\n        for i in range(n_hyperparameters):\n            if (candidate_new[i] < l_min[i]):\n                candidate_new[i] = l_min[i]\n            elif (candidate_new[i] > l_max[i]):    \n                candidate_new[i] = l_max[i]\n\n        # Update values                \n        candidate['value'] = candidate_new\n\n        # return\n        return candidate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function houses the previous three and calls them in succession when generating a new population. The function 'f_random_candidates' is called in the first generation to create an initial population. The subsequent generations make use of the functions 'f_gen_child_crossover' and 'f_gen_child_mutate' to create a new population of candidate solutions (models).\n\nThis function takes care of the nesting, ultimately the user can specify how many hyperparameters are generated for each feature selection (candidate model). Larger values of nesting will explore a wider range of hyperpameters for each feature selection. The crossover and mutation applies to each version of the candidate with the best hyperparameter found the candidate feature selection maintained."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Function to generate a population of candidates\ndef f_generate_population(inital_flag, population, features_name, \n                          p_crossover, p_mutate,\n                          hyperparams, hyperparams_increment,\n                          hyperparams_multiple,\n                          df=False, generation=0, initalise=False):\n    '''Generates all candidates in population'''\n    \n    # Create initial population\n    if inital_flag == True:\n        \n        # Check if there is an initial solution & reduce\n        # population by one if there is\n        if initalise != False:\n            population = population - 1\n        \n        # generate random features\n        df_pop = pd.DataFrame({'generation':generation,\n                               'candidate':range(0,population),\n                               'features':f_random_candidates(features_name,\n                                                              population,\n                                                              hyperparams,\n                                                              output_type = 'feature')})\n        \n        # Duplicate rows for population range\n        df_pop = df_pop.loc[df_pop.index.repeat(hyperparams_multiple)]\n        \n        # Generate population\n        df_pop['hyperparameters'] = \\\n            f_random_candidates(features_name=features_name,\n                                population = population * hyperparams_multiple,\n                                hyperparams=hyperparams,\n                                output_type = 'hyperparams',\n                                df_pop=df_pop)\n    \n        # If Initial solution then add in\n        if initalise != False:\n            df = pd.DataFrame({'generation':generation,\n                               'candidate':range(population, population + 1),\n                               'features':[initalise['features']],\n                               'hyperparameters':[initalise['hyperparameters']]},\n                              index=[population])\n            \n            df_pop = df_pop.append(df)\n        \n        \n        # Reset Index\n        df_pop.index = range(0, population * hyperparams_multiple)\n        \n        # Return\n        return df_pop\n    else:\n        # Distribute the population\n        population_crossover = round(population * p_crossover)\n        population_remainder = population-population_crossover\n        \n        # ----- Create crossover candidates -----\n        \n        # Create crossover populate for feature selection\n        df_pop = pd.DataFrame({'generation':generation,\n                               'candidate':range(0,population_crossover)})\n        df_pop['features'] = [f_gen_child_crossover(df=df,\n                                                    features_name=features_name,\n                                                    hyperparams=hyperparams,\n                                                    output_type = 'feature') \\\n                              for _ in range(population_crossover)]\n            \n        # Duplicate rows for population range\n        df_pop = df_pop.loc[df_pop.index.repeat(hyperparams_multiple)]\n            \n        # Create crossover population for hyperparameters\n        df_pop['hyperparameters'] = \\\n            [f_gen_child_crossover(df=df,\n                                   features_name=features_name,\n                                   hyperparams=hyperparams,\n                                   output_type = 'hyperparams') \\\n             for _ in range(population_crossover * hyperparams_multiple)]\n           \n        # Reset Index\n        df_pop.index = range(0, population_crossover * hyperparams_multiple)        \n                \n        # ----- Create Randomly Selected candidates -----\n        \n        # Initialise population\n        df_temp = pd.DataFrame({'generation':generation,\n                                'candidate':range(population_crossover, \n                                                  population)})   \n        # Randomly select candidates\n        selected_index = \\\n            df.sample(n=population_remainder,\n                      replace = False, \n                      weights=df.probability).candidate.tolist()        \n        \n        # Extract hyperparameters\n        selected_features = df.iloc[selected_index,:].features.tolist()\n        selected_params = df.iloc[selected_index,:].hyperparameters.tolist()\n        \n        # Update temp dataframe\n        df_temp['features'] = [selected_features[i] \n                               for i in range(len(selected_features))]\n        df_temp['hyperparameters'] = [selected_params[i] \n                               for i in range(len(selected_params))]        \n        \n        # Duplicate rows for population range\n        df_temp = df_temp.loc[df_temp.index.repeat(population_remainder)]\n        \n        # Append to population dataframe\n        df_pop = df_pop.append(df_temp,ignore_index=True)\n        \n        # Clear up\n        del selected_features, selected_params, df_temp\n        \n        # ----- Mutate Population -----\n        \n        # Mutate existing candidate features\n        df_pop['features'] = \\\n            df_pop.features.apply(f_gen_child_mutate, \n                                  features_name=features_name,\n                                  p_mutate=p_mutate,\n                                  hyperparams=hyperparams,\n                                  output_type = 'feature',\n                                  hyperparams_increment=hyperparams_increment)\n        \n        # Mutate existing candidate hyperparameters\n        df_pop['hyperparameters'] = \\\n            df_pop.hyperparameters.apply(f_gen_child_mutate,\n                                         features_name=features_name,\n                                         p_mutate=p_mutate,\n                                         hyperparams=hyperparams,\n                                         output_type = 'hyperparams',\n                                         hyperparams_increment=\n                                             hyperparams_increment)      \n        \n        # ----- Hyperparameter fix -----\n        if hyperparams != False:\n        \n            # Get length of features\n            n_features = df_pop['features'].apply(len).tolist()\n            \n            # Hyperparameter fix            \n            for i in range(population):\n                for j in range(len(hyperparams['names'])):\n                    if hyperparams['names'][j] == 'max_features':\n                        if df_pop.hyperparameters[i]['value'][j] > n_features[i] :\n                            df_pop.hyperparameters[i]['value'][j] = \\\n                                n_features[i]\n\n        # Return\n        return df_pop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six-subsection-two\"></a>\n### Solution Evaluation\n\nThis section is focussed on evaluating the solutions (models) created in each generation. This includes conducting cross validation and calculating the average AUC across folds.\n\nThe function below is used to conduct cross validation and get an estimate of the model performance for a single model."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Evaluate solution fitness\n@ignore_warnings(category=ConvergenceWarning)\ndef f_fitness(model, eval_metric, features, target, \n              feature_idx, kfold, hyperparams):\n    '''Evaluates fitness of proposed solution'''\n        \n    # Extract the hyperparameters\n    n_hyperparams = len(hyperparams['name'])\n    hyperparameters = {hyperparams['name'][0]:hyperparams['value'][0]}\n    if n_hyperparams > 1:\n        for i in range(n_hyperparams):\n            tempparameters = {hyperparams['name'][i]:hyperparams['value'][i]}\n            hyperparameters = {**hyperparameters, **tempparameters}\n    \n    # Determine CV strategy\n    if kfold == False:\n        kfold = 5\n    else:\n        kfold = kfold\n    \n    # Apply cross validation to the modells\n    results = cross_val_score(model.set_params(**hyperparameters), \n                              features.iloc[:,feature_idx], \n                              target,\n                              cv=kfold,\n                              scoring=eval_metric)\n    \n    # Replace NA's with 0\n    results[np.isnan(results)] = 0\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below calls the above function to get the performance value for each model. The evaulation for each fold is then averaged to have a final score for that candidate solution (model)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Apply evaluation score to current population\ndef f_evaluation_score(df, features, target, eval_metric, model,\n                       kfold, hyperparams):\n    '''Apply f_fitness to each candidate'''\n    \n    # Calculate the evaluation metric\n    evaluation_score = []\n    for val in range(0, len(df)):\n        eval_score = f_fitness(model=model,\n                               eval_metric=eval_metric,\n                               features = features,\n                               target=target,\n                               feature_idx=df['features'][val],\n                               kfold=kfold,\n                               hyperparams=df['hyperparameters'][val])\n        \n        # Average evaluation metric across folds\n        evaluation_score.append(eval_score.mean())\n        \n        # Clear object\n        del eval_score\n    \n    # Clear object\n    del val\n    \n    # return evaluation score\n    return evaluation_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following three functions are used to calculate the similarity between each solution and the best solution based on the performance metric in a generation. This enables us to track how similar the generated solutions are becoming over time. Jaccard similarity is used for the feature selection element of the algorithm and cosine similarity is used for the hyperparameter similarity.\n\nThe third function calls the jaccard and cosine functions for each solution to get an estimate of their similarity. It also creates a probability field which is very important for the population generation element of the algorithm. This probability is based on the fitness function and is used as the weighting function when selecting parent solutions."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Calculate jaccard similarity\ndef f_j_sim(list1, list2):\n    s1 = set(list1)\n    s2 = set(list2)\n    return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n\n# Calculate cosine similarity\ndef f_c_sim(l_other, l_best_score):\n    \n    # Extract hyperparameter values\n    l_other = l_other['value']\n    \n    # calculate similarity        \n    sim = 1 - spatial.distance.cosine(l_best_score, l_other)\n    \n    # return\n    return sim\n\n# Calculate similarity between candidates and probability for next gen selection\ndef f_sim_n_prob(df):\n        \n    # Calculate similarity of solutions with best solutions - Features\n    l_best_score = df.features[df['fitness_score'].idxmax()]\n    df['similarity_features'] = df['features'].apply(f_j_sim, list2=l_best_score)\n    del l_best_score\n\n    # Calculate similarity of solutions with best solutions\n    l_best_score = df.hyperparameters[df['fitness_score'].idxmax()]['value']\n    df['similarity_hyperparameters'] = df.hyperparameters.apply(f_c_sim, l_best_score=l_best_score)\n    del l_best_score \n        \n    # Calculate cumulative probability for future stages\n    df['probability'] = (df['fitness_score'] / sum(df['fitness_score']))\n    \n    # return\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function adds the model performance data to storage as well as calculates the number of features included in the model. The function calls the previous solution evaluation functions in turn. One vital bit of functionality is that it allows the user to alter the fitness function from being purely focussed on the evaluation metric (AUC) to including the number of features as part of the modelling process. This ultimately allows the user to determine how important finding a simple model is compared to optimising purely for AUC."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Function to populate attributes of candidates\ndef f_population_features(df, features, target, desiriability,\n                          eval_metric, model, kfold, hyperparams):\n    '''Get features of all candidates in population'''\n    \n    # Calculate feature size for candidates\n    df['feature_size'] = df['features'].apply(len)\n    \n    # Calculate evaluation score for candidates\n    df['evaluation_score'] = f_evaluation_score(df, \n                                                features, \n                                                target, \n                                                eval_metric, \n                                                model,\n                                                kfold,\n                                                hyperparams)\n    \n    # Conditionally create desirability fitness score\n    if desiriability != False:\n        \n        # Create scalars - Features\n        v_lb_features = desiriability['lb'][1]\n        v_ub_features = desiriability['ub'][1]\n        v_s_features = desiriability['s'][1]\n        \n         # Create scalars - Evaluation Metric\n        v_lb_eval = desiriability['lb'][0]\n        v_ub_eval = desiriability['ub'][0]\n        v_s_eval = desiriability['s'][0]        \n        \n        # Calculate desirability for features\n        df['desire_features']  = [0 if x > v_ub_features else 1 \n                                  if x < v_lb_features else \n                                      ((x-v_ub_features)/\n                                       (v_lb_features-v_ub_features))**\n                                      v_s_features \n                                  for x in df['feature_size']]\n    \n        # Calculate desirability for evaluation metric\n        df['desire_eval']  = [0 if x < v_lb_eval else 1\n                              if x > v_ub_eval else \n                                      ((x-v_lb_eval)/\n                                       (v_ub_eval-v_lb_eval))**\n                                      v_s_eval \n                              for x in df['evaluation_score']]\n        \n        # calculate fitness score\n        df['fitness_score'] = (df['desire_features'] * df['desire_eval'])**0.5\n        \n        # Drop fields\n        df = df.drop(columns=['desire_features', 'desire_eval'])\n\n    else:        \n        # calculate fitness score\n        df['fitness_score'] = df['evaluation_score']\n        \n    # Return \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final function below is the wrapper function which controls the optimisation process in it's entiriety, it allows the user to state the parameters of the search including:\n\n1. Evaluation metrics (any metric accepted by sklearn)\n1. Which model to use e.g. Elasticnet\n1. Which hyperparameters are associated with the model (set to false if model doesn't require hyperparameters)\n1. Desirability (Do we want to optimise purely for the performance metric or do we want to induce model simplicity)\n1. Cross over rate\n1. Mutation rate\n1. Elitism\n1. Maximum generations without improvement"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Main Optimisation Function\ndef f_model_optimisation(df,\n                         target_var,\n                         generations, \n                         population,                   \n                         eval_metric,\n                         model,\n                         kfold=False,\n                         hyperparams_multiple = 3,\n                         hyperparams = False,\n                         desiriability=False,\n                         p_crossover=0.8,\n                         p_mutate=0.01,\n                         hyperparams_increment = 0.1,\n                         elitism=False,\n                         gens_no_improve = False,\n                         initalise = False):\n    '''Function uses GA's to choose features and tune hyperparameters'''\n    \n    # Print Model Stats\n    print('Model Initialisation')\n    \n    # --------- Split features and target ---------\n    features = df.drop(target_var,axis=1)\n    features_name = features.columns\n    target = df[target_var]\n    \n    # --------- First Generation ---------\n    \n    # Generate inital candidate features solutions\n    df_pop_cur = f_generate_population(inital_flag=True, \n                                       population=population, \n                                       features_name=features_name,\n                                       p_crossover=p_crossover,\n                                       p_mutate=p_mutate,\n                                       hyperparams=hyperparams,\n                                       hyperparams_increment=hyperparams_increment,\n                                       hyperparams_multiple=hyperparams_multiple,\n                                       initalise=initalise)\n    \n    # Enrich candidate solutions with features\n    df_pop_cur = f_population_features(df=df_pop_cur, \n                                       features=features, \n                                       target=target,\n                                       desiriability=desiriability,\n                                       eval_metric=eval_metric,\n                                       model=model,\n                                       kfold=kfold,\n                                       hyperparams=hyperparams\n                                       )\n    \n    # Extract best score for each candidate\n    df_pop_cur = df_pop_cur.loc[df_pop_cur.reset_index().\\\n                                groupby(['candidate'])['fitness_score'].\\\n                                    idxmax()]\n    \n    # Enrich candidate solutions with similarity & probability\n    df_pop_cur = f_sim_n_prob(df_pop_cur)\n    \n    # --------- Create search storage ---------\n    df_output = df_pop_cur.copy()\n\n    # Print Model Stats\n    print('Gen: 00' +\n          ' - Generation Mean:' + str(round(df_output.fitness_score.mean(), 4)).zfill(4) +\n          ' - Generation Best:' + str(round(df_output.fitness_score.max(), 4)).zfill(4) +\n          ' - Global Best:' + str(round(df_output.fitness_score.max(), 4)).zfill(4)\n          )\n    \n    # Track best solution\n    if gens_no_improve != False:\n        count = 0\n        v_best = df_output.fitness_score.max()\n    \n    # --------- Run additional generations ---------\n\n    # Loop for additional generations\n    for gen in range(1, generations):\n                \n        # --------- Elitism ---------\n        if elitism > 0:\n            \n            # Create a dataframe with elite candidates\n            df_elite = df_output.nlargest(columns='fitness_score', n=elitism)\n            df_elite['candidate'] = population - 1\n            df_elite['generation'] = gen \n            df_elite = df_elite.drop(columns=['similarity_features', \n                                              'similarity_hyperparameters', 'probability'])\n        # --------- New Population ---------\n        \n        # Generate next candidate solutions   \n        df_pop_cur = f_generate_population(inital_flag=False, \n                                           generation = gen,\n                                           population=(population-elitism),\n                                           features_name=features_name,\n                                           df=df_pop_cur,\n                                           p_crossover=p_crossover,\n                                           p_mutate=p_mutate,\n                                           hyperparams=hyperparams,\n                                           hyperparams_increment=hyperparams_increment,\n                                           hyperparams_multiple=hyperparams_multiple\n                                           )\n        \n        # Enrich candidate solutions with features\n        df_pop_cur = f_population_features(df=df_pop_cur, \n                                           features=features, \n                                           target=target,\n                                           desiriability=desiriability,\n                                           eval_metric=eval_metric,\n                                           model=model,\n                                           kfold=kfold,\n                                           hyperparams=hyperparams)\n        \n        # Add elite\n        if elitism > 0:\n            df_pop_cur = pd.concat([df_pop_cur, df_elite]).reset_index().drop(columns=['index'])\n            del df_elite\n               \n        # Extract best score for each candidate\n        df_pop_cur = df_pop_cur.loc[df_pop_cur.reset_index().\\\n                                    groupby(['candidate'])['fitness_score'].\\\n                                    idxmax()]        \n        \n        # Enrich candidate solutions with similarity & probability\n        df_pop_cur = f_sim_n_prob(df=df_pop_cur)\n        \n        # Update Output\n        df_output = df_output.append(df_pop_cur,ignore_index=True)\n                \n        # Print Model Stats\n        print('Gen: ' + str(gen).zfill(2) +\n              ' - Generation Mean:' + str(round(df_output[df_output.generation == gen].fitness_score.mean(), 4)).zfill(4) +\n              ' - Generation Best:' + str(round(df_output[df_output.generation == gen].fitness_score.max(), 4)).zfill(4) +\n              ' - Global Best:' + str(round(df_output.fitness_score.max(), 4)).zfill(4)\n              )\n        \n        # Track number of generations with no improvement\n        if gens_no_improve != False:\n            if df_output.fitness_score.max() > v_best:\n                count = 0\n                v_best = df_output.fitness_score.max()\n            else:\n                count += 1\n\n            # Conditionally break loop\n            if count == gens_no_improve:\n                break\n    \n    # Return df\n    return df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n## Model Optimisation\n\nNow that our function is set up we can apply it to our data to see how successful this search will be. In this section we will apply the following models to our optimisaton process.\n\nModels:\n1. ElasticNet\n1. XGBOOST\n\nWithin each model there will be a look at the two objectives of maximising AUC as well as inducing sparcity with models.\n\nThe research surrounding GA suggests that a good setting for the search is:\n\n1. Crossover rate = 80%\n1. Mutation rate = 1 or 2% (2% has been choosen)\n\nIn the book [Feature Enginerring and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) the authors suggest that we should not employ elitism in our optimisation strategy. This ultimately means that the best solution isn't perfectly reserved for the next generation. The ethos here is to avoid getting stuck in a local maxima.\n\nBeyond these default values there are a number of other settings which will be tweaked for the individual models based on time."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-subsection-one\"></a>\n### ElasticNet\n\nElasticNet allows us to apply regulisation to a regression model and has two hyperparameters which we can tune:\n\n* Alpha\n* L1 ratio\n\nIn the following subsections the optimisation will be run to optimise purely AUC and then to build a balanced model (good predictive power but as simple as possible).\n\n#### Optimise AUC\n\nThe output below shows the AUC found by the GA, it was run for 5 generations with a population of 20 candidate models with 5 hyperparameters being produced for each cadidate solution. The hyperparameters are banded between a range of 0 & 0.01 for alpha and 0 and 1 for l1_ratio.\n\nThe output below shows the performance of the model of the generations, we can see that it generally achieves AUC scores in the high 70's. \n\n*Note as I have not built a seed into the optimisation model it will produce slightly different results each time when run"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Run Optimisation - Optimise for AUC\ndf_ENet_AUC = f_model_optimisation(df=df_train,\n                                   target_var='TARGET',\n                                   generations=7, \n                                   population=20,\n                                   p_crossover=0.8,\n                                   p_mutate=0.02,\n                                   hyperparams_increment=0.01,\n                                   hyperparams_multiple = 5,\n                                   eval_metric='roc_auc',\n                                   kfold=False,\n                                   model=ElasticNet(),\n                                   hyperparams = {'names':['alpha', 'l1_ratio'],\n                                                  'min_value': [0, 0],\n                                                  'max_value': [0.01, 1],\n                                                  'type':['float', 'float']}\n                                  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-subsection-two\"></a>\n### XGBOOST\n\nThe final model which we will run through our optimisation is XGBOOST, again this model has multiple hyperparameters which can be tuned, the one's which will be focussed on are:\n\n1. learning_rate = Control the weighting of new trees added to the model\n1. max_depth = The maximum depth of a tree\n1. min_child_weight = The minimum sum of weights of all observations required in a child\n1. gamma = minimum loss reduction required to make a split\n1. colsample_bytree = The fraction of columns to be randomly samples for each tree\n\n#### Optimise AUC\n\nThe GA search was run for 5 generations with a population of 20 candidate models with 5 hyperparameter variants being produced for each cadidate solution. The hyperparameter ranges are:\n\n* learning_rate = [0.03, 0.3]\n* max_depth = [2, 15]\n* min_child_weight = [1, 7]\n* gamma = [0, 0.5]\n* colsample_bytree = [0.3, 0.7]\n\nThe output below shows the performance of the model across the generations, we can see that it generally achieves AUC scores in the low-mid 80's. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Run Optimisation - Optimise for AUC\ndf_xgb_AUC = f_model_optimisation(df=df_train,\n                                  target_var='TARGET',\n                                  generations=5, \n                                  population=20,\n                                  p_crossover=0.8,\n                                  p_mutate=0.02,\n                                  hyperparams_increment=0.01,\n                                  hyperparams_multiple = 5,\n                                  eval_metric='roc_auc',\n                                  kfold=False,\n                                  model=XGBClassifier(objective=\"binary:logistic\", scale_pos_weight = 25),\n                                  hyperparams = {'names':['learning_rate', 'max_depth', \n                                                          'min_child_weight', 'gamma', 'colsample_bytree'],\n                                                 'min_value': [0.03, 2,  1, 0,   0.3],\n                                                 'max_value': [0.3,  15, 7, 0.5, 0.7],\n                                                 'type':['float', 'int', 'int', 'float', 'float']}\n                                 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-seven-subsection-three\"></a>\n### Analysis Findings\n\nThe results for all candidate models assessed by the GA were stored and now can be investigated. The graphic below shows the mean and max AUC achieved by each model across the generations. We can see that the mean scores across the generations are generally increasing indicating that the GA is driving towards more similar solutions across generations. The max score doesn't necessarily increase between generations as it reflects accepting worse solutions to enter new parts of the search space."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Summarise EN scores\ndf_temp_EN = df_ENet_AUC.groupby('generation')['evaluation_score'].agg(['mean', 'max']).reset_index()\ndf_temp_EN['model'] = 'ElasticNet'\n\n# Summarise XG scores\ndf_temp_XG = df_xgb_AUC.groupby('generation')['evaluation_score'].agg(['mean', 'max']).reset_index()\ndf_temp_XG['model'] = 'XGBOOST'\n\n# Combine Dataframes\ndf_vis = pd.concat([df_temp_EN, df_temp_XG])\n\n# Plot evaluation score\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfig.suptitle(\"Average and Best Evaluation Scores Per Generation\", fontsize=16)    \nax[0].set_title(\"ELasticNet\")\nax[1].set_title(\"XGBOOST\")\nsns.lineplot(data = df_temp_EN, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[0])\nsns.lineplot(data = df_temp_EN, x= 'generation', y='max', color=\"red\", label='Max', ax = ax[0])\nsns.lineplot(data = df_temp_XG, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[1])\nsns.lineplot(data = df_temp_XG, x= 'generation', y='max', color=\"red\", label='Max', ax = ax[1])\nax[0].set(ylim=(0.7, 0.85))\nax[1].set(ylim=(0.7, 0.85))\nax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\nax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.setp(ax[:], xlabel='Generation')\nplt.setp(ax[:], ylabel='AUC')\nplt.show()\n\n# Clear objects\ndel df_temp_EN, df_temp_XG, fig, df_vis, ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we can see that the mean scores are increasing, the distribution of features are becoming much tighter as the model converges towards popular predictors."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# EN scores\ndf_temp_EN = df_ENet_AUC\n\n# XG scores\ndf_temp_XG = df_xgb_AUC\n\n# Plot evaluation score\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfig.suptitle(\"Distribution of Feature Size Per Generation\", fontsize=16)    \nax[0].set_title(\"ELasticNet\")\nax[1].set_title(\"XGBOOST\")\nsns.boxplot(data = df_temp_EN, x= 'generation', y='feature_size', color=\"blue\", ax = ax[0])\nsns.boxplot(data = df_temp_XG, x= 'generation', y='feature_size', color=\"red\", ax = ax[1])\nax[0].set(ylim=(0, 369))\nax[1].set(ylim=(0, 369))\nplt.setp(ax[:], xlabel='Generation')\nplt.setp(ax[:], ylabel='Number of Features')\nplt.show()\n\n# Clear objects\ndel df_temp_EN, df_temp_XG, fig, ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart below shows the average similarity within the population against the best solution found in a generation. Similarity is in reference to the features included in the model using JACCARD similarity."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Summarise EN scores\ndf_temp_EN = df_ENet_AUC.groupby('generation')['similarity_features'].agg(['mean']).reset_index()\ndf_temp_EN['model'] = 'ElasticNet'\n\n# Summarise XG scores\ndf_temp_XG = df_xgb_AUC.groupby('generation')['similarity_features'].agg(['mean', 'max']).reset_index()\ndf_temp_XG['model'] = 'XGBOOST'\n\n# Combine Dataframes\ndf_vis = pd.concat([df_temp_EN, df_temp_XG])\n\n# Plot evaluation score\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfig.suptitle(\"Average Similarity Between Candidate Features and Best Solution's Per Generation\", fontsize=16)    \nax[0].set_title(\"ELasticNet\")\nax[1].set_title(\"XGBOOST\")\nsns.lineplot(data = df_temp_EN, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[0])\nsns.lineplot(data = df_temp_XG, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[1])\nax[0].set(ylim=(0, 1))\nax[1].set(ylim=(0, 1))\nax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\nax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.setp(ax[:], xlabel='Generation')\nplt.setp(ax[:], ylabel='Similarity')\nplt.show()\n\n# Clear objects\ndel df_temp_EN, df_temp_XG, fig, df_vis, ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the hyperparameters conver much faster, likely due to the smaller range of values to start with."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Summarise EN scores\ndf_temp_EN = df_ENet_AUC.groupby('generation')['similarity_hyperparameters'].agg(['mean']).reset_index()\ndf_temp_EN['model'] = 'ElasticNet'\n\n# Summarise XG scores\ndf_temp_XG = df_xgb_AUC.groupby('generation')['similarity_hyperparameters'].agg(['mean', 'max']).reset_index()\ndf_temp_XG['model'] = 'XGBOOST'\n\n# Combine Dataframes\ndf_vis = pd.concat([df_temp_EN, df_temp_XG])\n\n# Plot evaluation score\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfig.suptitle(\"Average Similarity Between Candidate Hyperparameters and Best Candidate Per Generation\", fontsize=16)    \nax[0].set_title(\"ELasticNet\")\nax[1].set_title(\"XGBOOST\")\nsns.lineplot(data = df_temp_EN, x= 'generation', y='mean', color=\"blue\", ax = ax[0])\nsns.lineplot(data = df_temp_XG, x= 'generation', y='mean', color=\"blue\", ax = ax[1])\nax[0].set(ylim=(0, 1))\nax[1].set(ylim=(0, 1))\nax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\nax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.setp(ax[:], xlabel='Generation')\nplt.setp(ax[:], ylabel='Similarity')\nplt.show()\n\n# Clear objects\ndel df_temp_EN, df_temp_XG, fig, df_vis, ax","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n## Model Evaluation\n\nWe can now use the best model found by the GA and use this to predict against our validation dataset, the output below shows how the model performs on the training set in CV and the validation set."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# -------------------------------------------------- ElasticNet -------------------------------------------------\n#################################################################################################################\n\n# Extract best solution\ndf_best_en = df_ENet_AUC[df_ENet_AUC.fitness_score == max(df_ENet_AUC.fitness_score)]\n\n# Extract features\nl_best_features_en = (df_best_en.features.tolist())[0]\n\n# Extract the hyperparameters\nl_best_hyperparms_en = (df_best_en.hyperparameters.tolist())[0]\nn_hyperparams_en = len(l_best_hyperparms_en['name'])\nhyperparameters_en = {l_best_hyperparms_en['name'][0]:l_best_hyperparms_en['value'][0]}\nif n_hyperparams_en > 1:\n    for i in range(n_hyperparams_en):\n        tempparameters = {l_best_hyperparms_en['name'][i]:l_best_hyperparms_en['value'][i]}\n        hyperparameters_en = {**hyperparameters_en, **tempparameters}\n\n# Create model object\nmodel_en = ElasticNet()\n\n# Set Hyperparameters\nmodel_en.set_params(**hyperparameters_en)\n\n# Fit model\nmodel_en.fit(df_train.iloc[:,l_best_features_en], df_train.TARGET)\n\n# Predict on validation\ny_pred_en = model_en.predict(df_validation.iloc[:,l_best_features_en])\n\n\n#################################################################################################################\n# --------------------------------------------------- XGBOOST ---------------------------------------------------\n#################################################################################################################\n\n# Extract best solution\ndf_best_xg = df_xgb_AUC[df_xgb_AUC.fitness_score == max(df_xgb_AUC.fitness_score)]\n\n# Extract features\nl_best_features_xg = (df_best_xg.features.tolist())[0]\n\n# Extract the hyperparameters\nl_best_hyperparms_xg = (df_best_xg.hyperparameters.tolist())[0]\nn_hyperparams_xg = len(l_best_hyperparms_xg['name'])\nhyperparameters_xg = {l_best_hyperparms_xg['name'][0]:l_best_hyperparms_xg['value'][0]}\nif n_hyperparams_xg > 1:\n    for i in range(n_hyperparams_xg):\n        tempparameters = {l_best_hyperparms_xg['name'][i]:l_best_hyperparms_xg['value'][i]}\n        hyperparameters_xg = {**hyperparameters_xg, **tempparameters}\n\n# Create model object\nmodel_xg = XGBClassifier(scale_pos_weight = 25)\n\n# Set Hyperparameters\nmodel_xg.set_params(**hyperparameters_xg)\n\n# Fit model\nmodel_xg.fit(df_train.iloc[:,l_best_features_xg], df_train.TARGET)\n\n# Predict on validation\ny_pred_temp = ((model_xg.predict_proba(df_validation.iloc[:,l_best_features_xg])).tolist())\ny_pred_xg = []\nfor i in range(len(df_validation)):\n    y_pred_xg.append(y_pred_temp[i][1])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#################################################################################################################\n# ------------------------------------------------ Output Scores ------------------------------------------------\n#################################################################################################################\n\n# Evaluation scores\nprint('ElasticNET')\nprint('Train Set AUC:', round((df_best_en.evaluation_score.tolist())[0], 3))\nprint('Validation Set AUC:', round(metrics.roc_auc_score(df_validation.TARGET, y_pred_en), 3))\nprint(' ')\n\n# Evaluation scores\nprint('XGBOOST')\nprint('Train Set AUC:', round((df_best_xg.evaluation_score.tolist())[0], 3))\nprint('Validation Set AUC:', round(metrics.roc_auc_score(df_validation.TARGET, y_pred_xg), 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-nine\"></a>\n## Conclusion\n\nTo conclude, GA appears to be a useful tool to apply both feature selection and hyperparameter tuning. The main drawback found during this process is the computational commitment required, while my code can 100% be optimised it is a relatively significant time committment. Taking the execution point out of consideration for the moment the GA does typically drive the modelling solution in the right areas of the search space.\n\nIn practice I might be more tempted to use GA to generate sparse models with reasonable predictive power rather than to maximise a performance metric. I might explore this in another notebook.\n\nIt has been good practice playing with functions and creating a workflow for both machine learning models and for a hack implementation of a GA for hyperparameter tuning and feature selection."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-ten\"></a>\n## Submission"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Read and submit score\ny_pred_temp = ((model_xg.predict_proba(df_test.iloc[:,l_best_features_xg])).tolist())\ny_pred = []\nfor i in range(len(df_test)):\n    y_pred.append(y_pred_temp[i][1])\nsubmission = pd.read_csv('../input/santander-customer-satisfaction/sample_submission.csv', encoding='latin-1')\nsubmission.loc[:, 'TARGET'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}