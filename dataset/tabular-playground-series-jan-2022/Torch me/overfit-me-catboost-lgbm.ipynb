{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS Jan 2022 XGB + Cat + LGBM\n__First notebook__\nhttps://www.kaggle.com/kartushovdanil/tps-jan-22-eda-atboost-prophet\n* In the first notebook, I noticed that it is better to train on the entire sample than on a k-fold.\n* New idea: make out of year cross-validation, that will be better then the entire sample, and we can also find problem areas.\n* Focus on the best solution to the find outliers and minimize residual\n* Make residual analysis of 5 years\n* Meta seed model? Done\n_________________________________________________________\n__soon__\n* Not sure, but try minimize residual of april\n* Covid?","metadata":{}},{"cell_type":"code","source":"#================== TPS Jan 2022 =======================#\n#----------------  import packages  -------------------#\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom cycler import cycler\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport math\nimport dateutil.easter as easter\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nimport lightgbm\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport pickle\n\nfrom math import ceil, floor, sqrt\n\n#------------------  load data   ---------------------#\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\n\n \nholiday_data = pd.read_csv('../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv')\n\ngdp_per_capita = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')\n\npopulation = pd.read_csv('../input/population-20152019-finland-norway-sweden/population_2015-2019_Finland_Norway_Sweden.csv',index_col = 'year')\n\n#!tree ../input/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-18T23:10:15.721334Z","iopub.execute_input":"2022-01-18T23:10:15.721642Z","iopub.status.idle":"2022-01-18T23:10:18.266657Z","shell.execute_reply.started":"2022-01-18T23:10:15.72161Z","shell.execute_reply":"2022-01-18T23:10:18.265767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PSEUDO_STATUS = True\nPSEUDO_DIR = \"../input/pseudolabelsjan22/pseudo_labels_v0.csv\"\nif PSEUDO_STATUS == True:\n    pseudo = pd.read_csv(PSEUDO_DIR)\n    pseudo_test = test.copy()\n    pseudo_test['num_sold'] = pseudo['num_sold']\n    #pseudo_df = engineer(pseudo_test)\n\n    \n    \ndef preparate_df(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n    #if 'num_sold' in df.columns:\n        #df['log_num_sold'] = np.log(df['num_sold'])\n    return df\n\ntrain = preparate_df(train)\ntest = preparate_df(test)\npseudo_df = preparate_df(pseudo_test)\n#pseudo_test = preparate_df(pseudo_test)\n\n\n\npopulation.columns = gdp_per_capita.columns\npopulation_dictionary = population.unstack().to_dict()\ntrain[\"population\"] = train.set_index(['country','year']).index.map(population_dictionary.get)\ntest[\"population\"] = test.set_index(['country','year']).index.map(population_dictionary.get)\npseudo_df[\"population\"] = pseudo_df.set_index(['country','year']).index.map(population_dictionary.get)\n\n\n\ndef holiday_features(holiday_df, df):\n    \n    fin_holiday = holiday_df.loc[holiday_df.country == 'Finland']\n    swe_holiday = holiday_df.loc[holiday_df.country == 'Sweden']\n    nor_holiday = holiday_df.loc[holiday_df.country == 'Norway']\n    \n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    \n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    \n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df\n\n\n\nholiday_features(holiday_data, train)\nholiday_features(holiday_data, test)\nholiday_features(holiday_data, pseudo_df)\nholiday_features(holiday_data, pseudo_test)\n\ngdp_dict = gdp_per_capita.unstack().to_dict()\n\n# Create new 'gdp_per_capita' column\ntrain['gdp_per_capita'] = train.set_index(['country', 'year']).index.map(gdp_dict.get)\ntest['gdp_per_capita']  = test.set_index(['country', 'year']).index.map(gdp_dict.get)\npseudo_df['gdp_per_capita']  = pseudo_df.set_index(['country', 'year']).index.map(gdp_dict.get)\npseudo_test['gdp_per_capita']  = pseudo_test.set_index(['country', 'year']).index.map(gdp_dict.get)\n\ndef fourier_features(df):\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n        \n    df['KaggleRama'] = df.store == 'KaggleRama'\n    \n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    \n    for k in range(1, 3):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        \n    return df\n\nfourier_features(train)\nfourier_features(test)\nfourier_features(pseudo_df)\n\nle_dict = {feature: LabelEncoder().fit(train[feature]) for feature in ['country', 'product', 'store']}\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = df\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}), #  + list(range(17, 25))\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in list(range(19, 26))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                        #pd.DataFrame({f\"june{d}\":\n                        #              (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(22, 31))}),\n                        #pd.DataFrame({f\"july{d}\":\n                        #              (df.date.dt.month == 7) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(1, 3))})],\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    for feature in ['country', 'product', 'store']:\n        new_df[feature] = le_dict[feature].transform(df[feature])\n    \n    return new_df #.astype(np.float32)\n\ntrain = engineer_more(train)\ntrain['date'] = train.date\ntrain['num_sold'] = train.num_sold.astype(np.float32)\ntest = engineer_more(test)\npseudo_df = engineer_more(pseudo_df)\n\ndef better_than_median(inputs, axis):\n    \"\"\"Compute the mean of the predictions if there are no outliers,\n    or the median if there are outliers.\n\n    Parameter: inputs = ndarray of shape (n_samples, n_folds)\"\"\"\n    spread = inputs.max(axis=axis) - inputs.min(axis=axis) \n    spread_lim = 0.45\n    print(f\"Inliers:  {(spread < spread_lim).sum():7} -> compute mean\")\n    print(f\"Outliers: {(spread >= spread_lim).sum():7} -> compute median\")\n    print(f\"Total:    {len(inputs):7}\")\n    return np.where(spread < spread_lim,\n                    np.mean(inputs, axis=axis),\n                    np.median(inputs, axis=axis))\n\n#preproc = StandardScaler()\n#train = preproc.fit_transform(train)\n#def smape(y_true, y_pred):\n#    \"\"\"SMAPE Loss\"\"\"\n#    return np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n#\n\ndef smape(y_true, y_pred):\n    \"\"\"\n    SMAPE Loss\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Estimated target values.\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n        SMAPE output is non-negative floating point. The best value is 0.0.\n\n    \"\"\"\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\nreduce_mem_usage(train)\nreduce_mem_usage(test)\nreduce_mem_usage(pseudo_df)\n\nprint(train.shape, test.shape, pseudo_df.shape)\n\nprint(train.columns)\nprint(test.columns)\nprint(pseudo_df.columns)\n\ntrain['target'] = train['num_sold'] / train['gdp_per_capita']\npseudo_df['target'] = pseudo_df['num_sold'] / pseudo_df['gdp_per_capita']\n\nFEATURES = [col for col in test.columns if col not in['row_id', 'date', 'num_sold', 'target']]\nTARGET = ['target']\n#\n#    pseudo_df['date'] = pseudo_test.date \n#    pseudo_df['num_sold'] = pseudo_test.num_sold.astype(np.float32)\n#    pseudo_df['target'] = np.log(pseudo_df['num_sold'] / pseudo_df['gdp'])\n#\n#    train_df = pd.concat([train_df,pseudo_df],axis=0)\n#    train_df = train_df.reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:18.268488Z","iopub.execute_input":"2022-01-18T23:10:18.268727Z","iopub.status.idle":"2022-01-18T23:10:21.042986Z","shell.execute_reply.started":"2022-01-18T23:10:18.268696Z","shell.execute_reply":"2022-01-18T23:10:21.041916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_val = train.isnull().sum()\nprint(missing_val[missing_val > 0])\nmissing_val = test.isnull().sum()\nprint(missing_val[missing_val > 0])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:21.044454Z","iopub.execute_input":"2022-01-18T23:10:21.044728Z","iopub.status.idle":"2022-01-18T23:10:21.081764Z","shell.execute_reply.started":"2022-01-18T23:10:21.044694Z","shell.execute_reply":"2022-01-18T23:10:21.080712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pseudo_df[FEATURES].head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:24.991915Z","iopub.execute_input":"2022-01-18T23:10:24.994289Z","iopub.status.idle":"2022-01-18T23:10:25.030059Z","shell.execute_reply.started":"2022-01-18T23:10:24.994228Z","shell.execute_reply":"2022-01-18T23:10:25.029354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params = {\n        'objective': 'regression',\n        'force_row_wise': True,\n        'verbosity': -1,\n        'seed': 1,\n        'learning_rate': 0.03,\n        'lambda_l1': 5e-05,\n        'lambda_l2': 1e-06,\n        'num_leaves': 20,\n        'feature_fraction': 0.6,\n        'bagging_fraction': 0.43,\n        'bagging_freq': 5,\n        'min_child_samples': 17,\n        }                        \n\ncat_params = {\n        'eval_metric': 'SMAPE', \n        'use_best_model': True,\n        'learning_rate': 0.04421730001498909,\n        'depth': 6,\n        'random_seed': 1,\n        'l2_leaf_reg': 0.24960109471113703,\n        'random_strength': 2.1314060037536735,\n        'grow_policy': 'SymmetricTree',\n        'max_bin': 406,\n        'min_data_in_leaf': 77,\n        'bootstrap_type': 'Bayesian',\n        'bagging_temperature': 0.7392707417524894}\n\nxgb_params = {\n        'tree_method': 'hist',\n        'grow_policy' : 'lossguide',\n        'learning_rate': 0.03399878704233446,\n        'max_depth': 5,\n        'reg_alpha': 0.7814373604498039,\n        'reg_lambda': 0.00018093104956619317,\n        'max_delta_step': 2,\n        'min_child_weight': 14,\n        'colsample_bytree': 0.6489299778623602,\n        'subsample': 0.6033298718112065,\n        'max_leaves': 187,  \n        }\n\nxgb_params2 ={'lambda': 0.0012338191278124635, 'alpha': 3.284395992431614, 'eta': 0.09886834650237164, 'colsample_bytree': 0.9, 'subsample': 0.6, 'learning_rate': 0.018, 'n_estimators': 2000, 'max_depth': 5, 'min_child_weight': 3}\n#def training_predictions():\n#    all_score_list = []\n#    lgb_score_list = []\n#    cat_score_list = []\n#    xgb_score_list = []\n#    xgb_test = xgb.DMatrix(test[FEATURES])\n#    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:25.261431Z","iopub.execute_input":"2022-01-18T23:10:25.261933Z","iopub.status.idle":"2022-01-18T23:10:25.273123Z","shell.execute_reply.started":"2022-01-18T23:10:25.261897Z","shell.execute_reply":"2022-01-18T23:10:25.272255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:25.661418Z","iopub.execute_input":"2022-01-18T23:10:25.661996Z","iopub.status.idle":"2022-01-18T23:10:25.688704Z","shell.execute_reply.started":"2022-01-18T23:10:25.661952Z","shell.execute_reply":"2022-01-18T23:10:25.688063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_importance(model):\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': FEATURES})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(10, 7), legend=None)\n    plt.title(f'CatBoost - Feature Importance {year}')\n    plt.ylabel('Features')\n    plt.xlabel('Importance');\n    \ndef plot_residual(xgb_pred, lgbm_pred, cat_pred, y_true, date):\n    plt.rcParams['figure.dpi'] = 600\n    colormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\n    plt.rc('axes', prop_cycle=(cycler('color', colormap)))\n    background_color = '#f6f5f5'\n    \n    #all_pred = (cat_pred + xgb_pred + lgbm_pred).mean()\n\n\n    fig, axs = plt.subplots(2, 1, figsize=(20, 10), facecolor='#f6f5f5')\n    sns.kdeplot(y_true - xgb_pred,fill=True, alpha=0.7, label='xgb', ax=axs[0])\n    sns.kdeplot(y_true - lgbm_pred,fill=True, alpha=0.7, label='lgbm', ax=axs[0])\n    sns.kdeplot(y_true - cat_pred,fill=True, alpha=0.7, label='cat', ax=axs[0])\n    #sns.kdeplot(y_true - all_pred,fill=True, alpha=0.7, label='all', ax=axs[0])\n    axs[0].set_title('Distribution Residual', fontdict={'fontsize': 12, 'fontweight': 'bold'})\n    axs[0].legend()\n\n    sns.lineplot(date['date'], y_true - xgb_pred, alpha=0.7, label='xgb',ax=axs[1])\n    sns.lineplot(date['date'], y_true - lgbm_pred, alpha=0.7, label='lgbm',ax=axs[1])\n    sns.lineplot(date['date'], y_true - cat_pred, alpha=0.7, label='cat',ax=axs[1])\n    #sns.lineplot(pseudo_df['date'], y_true - all_pred, alpha=0.7, label='all',ax=axs[1])\n    axs[1].set_title('Plot Residual', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n    axs[1].legend()\n\n    for i in range(2):\n        for s in [\"top\",\"right\"]:\n            axs[i].spines[s].set_visible(False)\n            axs[i].set_facecolor(background_color)\n            axs[i].grid(which='major', axis='x', zorder=1, color='#EEEEEE', linewidth=0.4)\n            axs[i].xaxis.offsetText.set_fontsize(4)\n            axs[i].yaxis.offsetText.set_fontsize(4)\n            axs[i].set_ylabel('')\n            axs[i].set_xlabel('')\n            axs[i].tick_params(labelsize=8, width=1)\n            \n    plt.plot()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:26.155877Z","iopub.execute_input":"2022-01-18T23:10:26.156185Z","iopub.status.idle":"2022-01-18T23:10:26.174231Z","shell.execute_reply.started":"2022-01-18T23:10:26.156152Z","shell.execute_reply":"2022-01-18T23:10:26.173499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_models(train, pseudo_df, test, log=False, year_split=False):\n    cat_pred_df = pd.DataFrame()\n    lgbm_pred_df = pd.DataFrame()\n    cat_pred = []\n    xgb_pred = []\n    lgbm_pred = []      \n    if year_split == True:\n        year_date = [2015,2016,2017,2018]\n    else:\n        year_date = [1]\n    for year in year_date:\n        #----------------------- df ----------------------------\n        if year_split==True:\n            X_train = train[train['year'] != year][FEATURES]\n            y_train = train[train['year'] != year][TARGET]\n            X_valid = train[train['year'] == year][FEATURES]\n            y_valid = train[train['year'] == year][TARGET]\n            test = test[FEATURES]\n            y_test = pseudo_df[TARGET]\n        else:\n            X_train = train[FEATURES]\n            y_train = train[TARGET]\n            X_valid = train[FEATURES]\n            y_valid = train[TARGET]\n            test = test[FEATURES]\n            y_test = pseudo_df[TARGET]\n        \n        if log==True:\n            y_train = np.log1p(y_train)\n            y_valid = np.log1p(y_valid)\n            y_test = np.log1p(y_test)\n        \n        \n        lgb_data_train = lightgbm.Dataset(\n                        X_train,\n                        label = y_train)\n        \n        lgb_data_valid = lightgbm.Dataset(\n                    X_valid, \n                    label = y_valid)\n        \n        lgb_data_test = lightgbm.Dataset(\n                    test)\n\n        #xgb_data_valid = xgb.DMatrix(\n        #            X_valid,\n        #            label = y_valid)\n        #\n        #xgb_data_train = xgb.DMatrix(\n        #                X_train,\n        #                label=y_train)\n        #\n        #xgb_data_test = xgb.DMatrix(\n        #                test)\n        \n        #--------------------- models --------------------------\n        cat_model = CatBoostRegressor(**cat_params) \n        cat_model.fit(X_train,y_train,eval_set =[(X_valid,y_valid)], verbose = 0, early_stopping_rounds = 200)\n        \n        #xgb_model = xgb.XGBRegressor(**xgb_params2)\n        #xgb_model.fit(X_train, y_train,\n        #                eval_set=[(X_train,y_train),(X_valid, y_valid)],\n        #                verbose = False,\n        #                early_stopping_rounds = 60)\n        \n        lgb_model = lightgbm.train(\n                        lgb_params,\n                        lgb_data_train,\n                        num_boost_round=2000)\n        \n        #---------------------- plot ---------------------------\n        #plot_importance(cat_model)\n        #---------------------- predict ---------------------------\n        cat_pred_train = cat_model.predict(X_train)\n        cat_pred_valid = cat_model.predict(X_valid)\n        cat_pred_test = cat_model.predict(test)\n        cat_pred.append(cat_pred_test)\n        \n        #xgb_pred_train = xgb_model.predict(X_train)\n        #xgb_pred_valid = xgb_model.predict(X_valid)\n        #xgb_pred_test = xgb_model.predict(test)\n        #xgb_pred.append(xgb_pred_test)\n        \n        lgbm_pred_train = lgb_model.predict(X_train)\n        lgbm_pred_valid = lgb_model.predict(X_valid)\n        lgbm_pred_test = lgb_model.predict(test)\n        lgbm_pred.append(lgbm_pred_test)\n        \n        if log==True:\n            cat_pred_train = np.expm1(cat_pred_train)\n            cat_pred_valid = np.expm1(cat_pred_valid)\n            cat_pred_test = np.expm1(cat_pred_test)\n            #xgb_pred_train = np.expm1(xgb_pred_train)\n            #xgb_pred_valid = np.expm1(xgb_pred_valid)\n            #xgb_pred_test = np.expm1(xgb_pred_test)\n            lgbm_pred_train = np.expm1(lgbm_pred_train)\n            lgbm_pred_valid = np.expm1(lgbm_pred_valid)\n            lgbm_pred_test = np.expm1(lgbm_pred_test)\n            \n            y_train = np.expm1(y_train)\n            y_valid = np.expm1(y_valid)\n            y_test = np.expm1(y_test)\n        \n        \n        if year_split==True:\n            cat_pred_df[f'y_{year}'] = cat_pred_test\n            lgbm_pred_df[f'y_{year}'] = lgbm_pred_test\n        else:\n            cat_pred_df['y'] = cat_pred_test\n            lgbm_pred_df['y'] = lgbm_pred_test\n        \n        #---------------------- score ---------------------------\n        #train_score = smape(y_train, pred_train)\n        cat_train_score = np.round(np.mean(smape(y_train.target, cat_pred_train)),4)\n        #valid_score = smape(y_valid, pred_valid)\n        cat_valid_score = np.round(np.mean(smape(y_valid.target, cat_pred_valid)),4)\n        cat_test_score = np.round(np.mean(smape(y_test.target, cat_pred_test)),4)\n        \n        ##train_score = smape(y_train, pred_train)\n        #xgb_train_score = np.round(np.mean(smape(y_train.target, xgb_pred_train)),4)\n        ##valid_score = smape(y_valid, pred_valid)\n        #xgb_valid_score = np.round(np.mean(smape(y_valid.target, xgb_pred_valid)),4)\n        #xgb_test_score = np.round(np.mean(smape(y_test.target, xgb_pred_test)),4)\n        \n        #train_score = smape(y_train, pred_train)\n        lgbm_train_score = np.round(np.mean(smape(y_train.target, lgbm_pred_train)),4)\n        #valid_score = smape(y_valid, pred_valid)\n        lgbm_valid_score = np.round(np.mean(smape(y_valid.target, lgbm_pred_valid)),4)\n        lgbm_test_score = np.round(np.mean(smape(y_test.target, lgbm_pred_test)),4)\n        \n        #---------------------- output ---------------------------\n        print('=====================================================')\n        print(f'                    YEAR: != {year}')\n        print(f'--------------------- CatBoost ---------------------')\n        print(f' --------  train smape out of {year}: {cat_train_score}  --------  ')\n        print(f' --------  valid smape {year}: {cat_valid_score}  --------  ')\n        print(f' --------  test smape 2019: {cat_test_score}  --------  ')\n        #print(f'-------------------- XGBBoost ----------------------')\n        #print(f' --------  train smape out of {year}: {xgb_train_score}  --------  ')\n        #print(f' --------  valid smape {year}: {xgb_valid_score}  --------  ')\n        #print(f' --------  test smape 2019: {xgb_test_score}  --------  ')\n        print(f'------------------ LGBM -----------------------')\n        print(f' --------  train smape out of {year}: {lgbm_train_score}  --------  ')\n        print(f' --------  valid smape {year}: {lgbm_valid_score}  --------  ')\n        print(f' --------  test smape 2019: {lgbm_test_score}  --------  ')\n        \n        #plot_residual(xgb_pred_train, lgbm_pred_train, cat_pred_train, y_train.target, train[train['year'] != year])\n        #print(xgb_pred_train.shape, lgbm_pred_train.shape, cat_pred_train.shape, y_train.shape, train[train['year'] != year])\n        \n    return cat_pred, xgb_pred, lgbm_pred, cat_pred_df, lgbm_pred_df\n        \ncat_pred, xgb_pred, lgbm_pred, cat_pred_df, lgbm_pred_df = fit_models(train, pseudo_df, test)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:27.23642Z","iopub.execute_input":"2022-01-18T23:10:27.237136Z","iopub.status.idle":"2022-01-18T23:10:57.722328Z","shell.execute_reply.started":"2022-01-18T23:10:27.237082Z","shell.execute_reply":"2022-01-18T23:10:57.721614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ssb = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\n\n#ssb['num_sold'] = pd.DataFrame(cat_pred).mean()\n#ssb.to_csv('submit_cat_mean_oof_nonlog_population_ysplitfalse.csv', index = False)\n#ssb['num_sold'] = pd.DataFrame(xgb_pred).mean()\n#ssb.to_csv('submit_xgb_mean_oof.csv', index = False)\n#ssb['num_sold'] = pd.DataFrame(lgbm_pred).mean()\n#ssb.to_csv('submit_lgbm_mean_oof.csv', index = False)\n#ssb['num_sold'] = cat_pred[3] * 0.98\n#ssb.to_csv('submit_cat_2015_2018_magic_num.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:57.723664Z","iopub.execute_input":"2022-01-18T23:10:57.72399Z","iopub.status.idle":"2022-01-18T23:10:57.741002Z","shell.execute_reply.started":"2022-01-18T23:10:57.723961Z","shell.execute_reply":"2022-01-18T23:10:57.740368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat_pred = \nall_pred = cat_pred + lgbm_pred \n#ssb['num_sold'] = pd.DataFrame(all_pred).mean()\n#ssb.to_csv('submit_all_mean_oof.csv', index = False)\n\n#ssb['num_sold'] = pd.DataFrame(all_pred).mean() * 0.99\n#ssb.to_csv('submit_all_mean_oof_0dot99.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:57.742139Z","iopub.execute_input":"2022-01-18T23:10:57.742471Z","iopub.status.idle":"2022-01-18T23:10:57.746557Z","shell.execute_reply.started":"2022-01-18T23:10:57.742443Z","shell.execute_reply":"2022-01-18T23:10:57.745491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_pred = cat_pred[:2] + xgb_pred[:2] + lgbm_pred[:2]\n#\n#ssb['num_sold'] = pd.DataFrame(all_pred).mean()\n#ssb.to_csv('submit_all_mean_oof_2m.csv', index = False)\n#\n#ssb['num_sold'] = pd.DataFrame(all_pred).mean() * 1.01\n#ssb.to_csv('submit_all_mean_oof_1dot01.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:57.748533Z","iopub.execute_input":"2022-01-18T23:10:57.748778Z","iopub.status.idle":"2022-01-18T23:10:57.760261Z","shell.execute_reply.started":"2022-01-18T23:10:57.74875Z","shell.execute_reply":"2022-01-18T23:10:57.759307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.rcParams['figure.dpi'] = 600\ncolormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\nplt.rc('axes', prop_cycle=(cycler('color', colormap)))\nbackground_color = '#f6f5f5'\n\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 10), facecolor='#f6f5f5')\n#sns.kdeplot(pseudo_df.target - pd.DataFrame(xgb_pred).mean(),fill=True, alpha=0.7, label='xgb', ax=axs[0])\nsns.kdeplot(pseudo_df.target - pd.DataFrame(lgbm_pred).mean(),fill=True, alpha=0.7, label='lgbm', ax=axs[0])\nsns.kdeplot(pseudo_df.target - pd.DataFrame(cat_pred).mean(),fill=True, alpha=0.7, label='cat', ax=axs[0])\nsns.kdeplot(pseudo_df.target - pd.DataFrame(all_pred).mean(),fill=True, alpha=0.7, label='all', ax=axs[0])\naxs[0].set_title('Distribution Residual', fontdict={'fontsize': 14, 'fontweight': 'bold'})\naxs[0].legend()\n\n#sns.lineplot(pseudo_df['date'], pseudo_df.target - pd.DataFrame(xgb_pred).mean(), alpha=0.7, label='xgb',ax=axs[1])\nsns.lineplot(pseudo_df['date'], pseudo_df.target - pd.DataFrame(lgbm_pred).mean(), alpha=0.7, label='lgbm',ax=axs[1])\nsns.lineplot(pseudo_df['date'], pseudo_df.target - pd.DataFrame(cat_pred).mean(), alpha=0.7, label='cat',ax=axs[1])\nsns.lineplot(pseudo_df['date'], pseudo_df.target - pd.DataFrame(all_pred).mean(), alpha=0.7, label='all',ax=axs[1])\naxs[1].set_title('Plot Residual', fontdict={'fontsize': 14, 'fontweight': 'bold'})\naxs[1].legend()\n\nfor i in range(2):\n    for s in [\"top\",\"right\"]:\n        axs[i].spines[s].set_visible(False)\n        axs[i].set_facecolor(background_color)\n        axs[i].grid(which='major', axis='x', zorder=1, color='#EEEEEE', linewidth=0.4)\n        axs[i].xaxis.offsetText.set_fontsize(4)\n        axs[i].yaxis.offsetText.set_fontsize(4)\n        axs[i].set_ylabel('')\n        axs[i].set_xlabel('')\n        axs[i].tick_params(labelsize=8, width=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:10:57.761699Z","iopub.execute_input":"2022-01-18T23:10:57.762098Z","iopub.status.idle":"2022-01-18T23:11:34.852036Z","shell.execute_reply.started":"2022-01-18T23:10:57.762059Z","shell.execute_reply":"2022-01-18T23:11:34.85113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pseudo_df['residual'] = pseudo_df.target - pd.DataFrame(all_pred).mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:11:34.8537Z","iopub.execute_input":"2022-01-18T23:11:34.854204Z","iopub.status.idle":"2022-01-18T23:11:35.159462Z","shell.execute_reply.started":"2022-01-18T23:11:34.854161Z","shell.execute_reply":"2022-01-18T23:11:35.158545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pseudo_df['residual'].sort_values(ascending=False)\nprint(pseudo_df['residual'].mean())\nprint(pseudo_df['residual'].std())","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:11:35.160561Z","iopub.execute_input":"2022-01-18T23:11:35.160782Z","iopub.status.idle":"2022-01-18T23:11:35.169421Z","shell.execute_reply.started":"2022-01-18T23:11:35.160753Z","shell.execute_reply":"2022-01-18T23:11:35.168318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#subm = test.gdp_per_capita * pd.DataFrame(all_pred).mean()\n#ssb = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\n#ssb['num_sold'] = subm \n#ssb.to_csv('submit_cat_lgbm_gpd.csv', index = False)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:11:35.1708Z","iopub.execute_input":"2022-01-18T23:11:35.171031Z","iopub.status.idle":"2022-01-18T23:11:35.178973Z","shell.execute_reply.started":"2022-01-18T23:11:35.171005Z","shell.execute_reply":"2022-01-18T23:11:35.178235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mate_seed_model(num_seed):\n    cat_meta_model_pred = pd.DataFrame()\n    lgb_meta_model_pred = pd.DataFrame()\n    for i in range(num_seed):\n        lgb_params['seed'] = i\n        cat_params['random_seed'] = i\n        print('')\n        print('')\n        print('========================================================')\n        print(f'===================== seed {i} =========================')\n        print('========================================================')\n        print('')\n        print('')\n        cat_pred, xgb_pred, lgbm_pred, cat_pred_df, lgbm_pred_df = fit_models(train, pseudo_df, test)\n        cat_pred_df.columns = [col + f'_seed_{i}' for col in cat_pred_df.columns]\n        lgbm_pred_df.columns = [col + f'_seed_{i}' for col in lgbm_pred_df.columns]\n        cat_meta_model_pred = pd.concat([cat_meta_model_pred, cat_pred_df], axis=1)\n        lgb_meta_model_pred = pd.concat([lgb_meta_model_pred, lgbm_pred_df], axis=1)\n    return cat_meta_model_pred, lgb_meta_model_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:52:30.816155Z","iopub.execute_input":"2022-01-18T23:52:30.816429Z","iopub.status.idle":"2022-01-18T23:52:30.825026Z","shell.execute_reply.started":"2022-01-18T23:52:30.816401Z","shell.execute_reply":"2022-01-18T23:52:30.824372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_meta_model_pred, lgb_meta_model_pred = mate_seed_model(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T23:54:10.97947Z","iopub.execute_input":"2022-01-18T23:54:10.979841Z","iopub.status.idle":"2022-01-18T23:59:18.113198Z","shell.execute_reply.started":"2022-01-18T23:54:10.9798Z","shell.execute_reply":"2022-01-18T23:59:18.111896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from functools import reduce\n#from operator import add\ncat_meta_model_pred = cat_meta_model_pred.mean(axis=1)\nlgb_meta_model_pred = lgb_meta_model_pred.mean(axis=1)\nall_meta_model_pred = (cat_meta_model_pred.mean(axis=1) + lgb_meta_model_pred.mean(axis=1))/2","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:02:17.801423Z","iopub.execute_input":"2022-01-19T00:02:17.801705Z","iopub.status.idle":"2022-01-19T00:02:17.841669Z","shell.execute_reply.started":"2022-01-19T00:02:17.801675Z","shell.execute_reply":"2022-01-19T00:02:17.840144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.rcParams['figure.dpi'] = 600\ncolormap = ['#1DBA94','#1C5ED2', '#FFC300', '#C70039']\nplt.rc('axes', prop_cycle=(cycler('color', colormap)))\nbackground_color = '#f6f5f5'\n\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 10), facecolor='#f6f5f5')\n#sns.kdeplot(pseudo_df.target - pd.DataFrame(xgb_pred).mean(),fill=True, alpha=0.7, label='xgb', ax=axs[0])\nsns.kdeplot(pseudo_df.target - lgb_meta_model_pred,fill=True, alpha=0.7, label='lgbm', ax=axs[0])\nsns.kdeplot(pseudo_df.target - cat_meta_model_pred,fill=True, alpha=0.7, label='cat', ax=axs[0])\nsns.kdeplot(pseudo_df.target - all_meta_model_pred,fill=True, alpha=0.7, label='all', ax=axs[0])\naxs[0].set_title('Distribution Residual', fontdict={'fontsize': 14, 'fontweight': 'bold'})\naxs[0].legend()\n\n#sns.lineplot(pseudo_df['date'], pseudo_df.target - pd.DataFrame(xgb_pred).mean(), alpha=0.7, label='xgb',ax=axs[1])\nsns.lineplot(pseudo_df['date'], pseudo_df.target - lgb_meta_model_pred, alpha=0.7, label='lgbm',ax=axs[1])\nsns.lineplot(pseudo_df['date'], pseudo_df.target - cat_meta_model_pred, alpha=0.7, label='cat',ax=axs[1])\nsns.lineplot(pseudo_df['date'], pseudo_df.target - all_meta_model_pred, alpha=0.7, label='all',ax=axs[1])\naxs[1].set_title('Plot Residual', fontdict={'fontsize': 14, 'fontweight': 'bold'})\naxs[1].legend()\n\nfor i in range(2):\n    for s in [\"top\",\"right\"]:\n        axs[i].spines[s].set_visible(False)\n        axs[i].set_facecolor(background_color)\n        axs[i].grid(which='major', axis='x', zorder=1, color='#EEEEEE', linewidth=0.4)\n        axs[i].xaxis.offsetText.set_fontsize(4)\n        axs[i].yaxis.offsetText.set_fontsize(4)\n        axs[i].set_ylabel('')\n        axs[i].set_xlabel('')\n        axs[i].tick_params(labelsize=8, width=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:03:37.306768Z","iopub.execute_input":"2022-01-19T00:03:37.307149Z","iopub.status.idle":"2022-01-19T00:04:12.623148Z","shell.execute_reply.started":"2022-01-19T00:03:37.307111Z","shell.execute_reply":"2022-01-19T00:04:12.622257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = test.gdp_per_capita * all_meta_model_pred\nssb = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\nssb['num_sold'] = subm \n","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:04:12.624829Z","iopub.execute_input":"2022-01-19T00:04:12.625094Z","iopub.status.idle":"2022-01-19T00:04:12.638025Z","shell.execute_reply.started":"2022-01-19T00:04:12.625063Z","shell.execute_reply":"2022-01-19T00:04:12.637404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\nssb.num_sold = ssb.num_sold.apply(lambda x: geometric_round(x))\n\nssb.to_csv('submit_cat_lgbm_gpd_meta_geomround_nolog_nottsplit.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:04:12.639372Z","iopub.execute_input":"2022-01-19T00:04:12.639782Z","iopub.status.idle":"2022-01-19T00:04:12.84551Z","shell.execute_reply.started":"2022-01-19T00:04:12.639751Z","shell.execute_reply":"2022-01-19T00:04:12.844497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ssb","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:05:11.781973Z","iopub.execute_input":"2022-01-19T00:05:11.782287Z","iopub.status.idle":"2022-01-19T00:05:11.797977Z","shell.execute_reply.started":"2022-01-19T00:05:11.782256Z","shell.execute_reply":"2022-01-19T00:05:11.79701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lgb_meta_model_pred\n\nsubm = test.gdp_per_capita * lgb_meta_model_pred\nssb = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\nssb['num_sold'] = subm \n\n\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\nssb.num_sold = ssb.num_sold.apply(lambda x: geometric_round(x))\n\nssb.to_csv('submit_lgbm_gpd_meta_geomround_nolog_nottsplit.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:09:16.097939Z","iopub.execute_input":"2022-01-19T00:09:16.098731Z","iopub.status.idle":"2022-01-19T00:09:16.314539Z","shell.execute_reply.started":"2022-01-19T00:09:16.098684Z","shell.execute_reply":"2022-01-19T00:09:16.313509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cat_meta_model_pred\n\nsubm = test.gdp_per_capita * cat_meta_model_pred\nssb = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')\nssb['num_sold'] = subm \n\n\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\n\nssb.num_sold = ssb.num_sold.apply(lambda x: geometric_round(x))\n\nssb.to_csv('submit_cat_gpd_meta_geomround_nolog_nottsplit.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T00:09:30.702495Z","iopub.execute_input":"2022-01-19T00:09:30.702825Z","iopub.status.idle":"2022-01-19T00:09:30.913183Z","shell.execute_reply.started":"2022-01-19T00:09:30.702791Z","shell.execute_reply":"2022-01-19T00:09:30.912175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}