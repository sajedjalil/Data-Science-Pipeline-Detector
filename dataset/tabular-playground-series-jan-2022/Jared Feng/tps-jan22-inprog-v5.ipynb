{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TODO:\n\n0. Read more notebooks and posts.\n1. **Reimplement the CV step**\n2. Try NNs\n3. Try Time2Vec time encoding with NNs. Time2Vec transforms absolute time to sin() activations (variable size) with learnable frequencies and phase shifts. This could be a more efficient way to encode time than the hand-crafted sin() and cos() transformations. Paper: https://arxiv.org/abs/1907.05321\n4. Try manually coded loss function on PyTorch. (haven't done that before, not sure if possible)","metadata":{}},{"cell_type":"markdown","source":"# Version History:\n\nV1: [baseline notebook] date processing + catboost regressor + ts split cv. CV: 27.1\n\nV2: \n\nadded fourier transforms for the one-hot encoded categorical features; \n\nadded more date features including week (of year) and quarter; \n\ntidied up some codes of loading data & feature engineering. CV: 24.9\n\nV3: \n\nadded grouping and averaging by groups model; \n\nplotted target & prediction discrepancies. Train score: 9.7, LB: 7.1.\n\nV4: \n\nadded holiday encoding; \n\nreduced selected features for all models. Train score: 9.5, LB: 7.1.\n\nV5:\n\nfixed `DateProcessor` bug of incorrectly extrapolating year column;\n\nupgraded `DateParser` to a class;\n\nadded helper functions for selecting features and splitting data by year. LB: 7.0.\n\n**Note: Reason for poor performance of V1 and V2, see Lesson #2**","metadata":{}},{"cell_type":"markdown","source":"# Lessons:\n\n1. Boosting models like LightGBM are constrained to predict within the range of values of the target variable in the training data and don't extrapolate when there is strong trend. From: https://www.kaggle.com/rohanrao/a-modern-time-series-tutorial#Auto-ARIMAX\n2. Redundant date features: people claim that they are useful sometimes, but these date features will dominate the other three categorical ones (if they outnumber the other ones) in my case. Then the model cannot see how important the `store`, `country`, and `product` features are. Even within the date features themselves, many of them turn out to be completely irrelevant, only four (`date_month_sin`, `date_month_cos`, `date_dayofweek_sin`, `date_dayofweek_cos`) turn out to be essential, other two (`holiday` and `date_year`) could be good too given a good model.\n3. Transform targets using log() before fitting a linear model. Reasons: https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model/comments\n4. SMAPE penalizes underestimations more than overestimations. https://www.kaggle.com/carlmcbrideellis/tps-jan-2022-a-simple-average-model-no-ml/notebook\n","metadata":{}},{"cell_type":"markdown","source":"## Other Models and Reading Materials\n1. https://www.kaggle.com/andreshg/timeseries-analysis-a-complete-guide. \n2. Prophet: https://www.kaggle.com/robikscube/time-series-forecasting-with-prophet\n3. CNN-LSTM hybrid: https://www.kaggle.com/dimitreoliveira/deep-learning-for-time-series-forecasting/notebook\n4. LSTM as Autoencoder and then MLP: https://www.kaggle.com/dimitreoliveira/time-series-forecasting-with-lstm-autoencoders/notebook#Regular-LSTM-model.\n5. https://builtin.com/data-science/time-series-forecasting-python (TS classic models)\n6. https://www.kaggle.com/jagangupta/time-series-basics-exploring-traditional-ts (TS classic models)\n7. https://towardsdatascience.com/an-overview-of-time-series-forecasting-models-a2fa7a358fcb (more TS)\n8. Temporal CNN for ts: https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298344","metadata":{}},{"cell_type":"markdown","source":" # Packages and helpers","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport os\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:18:19.237421Z","iopub.execute_input":"2022-01-04T04:18:19.237802Z","iopub.status.idle":"2022-01-04T04:18:20.292215Z","shell.execute_reply.started":"2022-01-04T04:18:19.237706Z","shell.execute_reply":"2022-01-04T04:18:20.291286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Helper functions. Thanks to: https://www.kaggle.com/vad13irt/tps-2022-baseline/","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed=1):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \ndef make_submission(ids, predictions, path=\"submission.csv\"):\n    assert len(ids) == len(predictions), f\"Lengths of `ids` ({len(ids)}) and `predictions` ({len(predictions)}) aren't the same.\"\n    assert not predictions.isnull().any(), 'Some predictions are blank!'\n    ids = ids.astype(int)\n    predictions = predictions.astype(float)\n    df = pd.DataFrame({\n        \"row_id\": ids,\n        \"num_sold\": predictions,\n    })\n    \n    df.to_csv(path, index=False)\n    print('submission csv outputted!')\n    return \n\nSEED = 24\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:18:30.156212Z","iopub.execute_input":"2022-01-04T04:18:30.156502Z","iopub.status.idle":"2022-01-04T04:18:30.16428Z","shell.execute_reply.started":"2022-01-04T04:18:30.156469Z","shell.execute_reply":"2022-01-04T04:18:30.16328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loader","metadata":{}},{"cell_type":"code","source":"def DataLoader(path, train=True):\n    df = pd.read_csv(path, parse_dates=['date'])\n    ids = df.pop(\"row_id\")\n    if train:\n        targets = df.pop('num_sold')\n        return df, ids, targets\n    return df, ids\n\ntrain_path = '../input/tabular-playground-series-jan-2022/train.csv'\ntest_path = \"../input/tabular-playground-series-jan-2022/test.csv\"\nholiday_path = '../input/holidays-finland-norway-sweden-20152019/Holidays_Finland_Norway_Sweden_2015-2019.csv'","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:18:31.407665Z","iopub.execute_input":"2022-01-04T04:18:31.408194Z","iopub.status.idle":"2022-01-04T04:18:31.414832Z","shell.execute_reply.started":"2022-01-04T04:18:31.408146Z","shell.execute_reply":"2022-01-04T04:18:31.413903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are no missing values in the original data.","metadata":{}},{"cell_type":"code","source":"train_df , _, targets = DataLoader(train_path, train=True)\nprint('Missing values in data: \\n',train_df.isnull().any())\nprint('\\nMissing values in targets: ',targets.isnull().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df , _= DataLoader(test_path, train=False)\nprint('Missing values in data: \\n',test_df.isnull().any())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering (experimental)\n\n**Note: not all features from this section are useful.**","metadata":{}},{"cell_type":"markdown","source":"## (1) Transform year, month, and day \n","metadata":{}},{"cell_type":"markdown","source":"### Method 1: Trigonometric transforms\nThanks to: https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298202\n- To get info on the year, just min-max normalize the year number.\n- To extract any cyclical feature we desire, multiply the feature by `2*np.pi` and divide by the maximum of that feature, then apply `np.sin()` and `np.cos()`. Thus, one cyclical feature ends up having two columns.\n- In total, we are creating seven new columns, and effectively excluding the original `'date'` column from the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DateProcessor(BaseEstimator, TransformerMixin):\n    def __init__(self, date_format='%d/%m/%Y', hours_secs=False):\n        self.format = date_format\n        self.earliest_yr = None\n        self.latest_yr = None\n        self.columns = None\n        self.time_transformations = [\n            ('day_sin', lambda x: np.sin(2*np.pi*x.dt.day/31)),\n            ('day_cos', lambda x: np.cos(2*np.pi*x.dt.day/31)),\n            ('dayofweek_sin', \n                lambda x: np.sin(2*np.pi*x.dt.dayofweek/6)),\n            ('dayofweek_cos', \n                lambda x: np.cos(2*np.pi*x.dt.dayofweek/6)),\n            ('dayofyear_sin', \n                lambda x: np.sin(2*np.pi*x.dt.dayofyear/366)),\n            ('dayofyear_cos', \n                lambda x: np.cos(2*np.pi*x.dt.dayofyear/366)),\n            ('dayofyear', lambda x: x.dt.dayofyear),\n            ('week_sin', \n                lambda x: np.sin(2*np.pi*x.dt.isocalendar().week.astype(int)/52)),\n            ('week_cos', \n                lambda x: np.cos(2*np.pi*x.dt.isocalendar().week.astype(int)/52)),\n            ('month_sin', \n                lambda x: np.sin(2*np.pi*x.dt.month/12)),\n            ('month_cos', \n                lambda x: np.cos(2*np.pi*x.dt.month/12)),\n            ('quarter_sin', \n                lambda x: np.sin(2*np.pi*x.dt.quarter/4)),\n            ('quarter_cos',\n                lambda x: np.cos(2*np.pi*x.dt.quarter/4)),\n            #('year', \n            #    lambda x: (x.dt.year - x.dt.year.min()\n            #              ) / (x.dt.year.max() - x.dt.year.min()))\n            ('year', \n                lambda x: (x.dt.year - self.earliest_yr)\n                  / (self.latest_yr - self.earliest_yr))\n        ]\n        if hours_secs:\n            self.time_transformations = [\n                ('hour_sin', \n                lambda x: np.sin(2*np.pi*x.dt.hour/23)),\n                ('hour_cos', \n                lambda x: np.cos(2*np.pi*x.dt.hour/23)),\n                ('minute_sin', \n                lambda x: np.sin(2*np.pi*x.dt.minute/59)),\n                ('minute_cos', \n                lambda x: np.cos(2*np.pi*x.dt.minute/59))\n            ] + self.time_transformations\n\n    def fit(self, X, y=None, **fit_params):\n        self.columns = self.transform(X.iloc[0:1,:]).columns\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        transformed = list()\n        for col in X.columns:\n            if col == 'date':\n                time_column = pd.to_datetime(X[col],\n                                  format=self.format)\n                for label, func in self.time_transformations:\n                    transformed.append(func(time_column))\n                    transformed[-1].name += '_' + label\n        transformed = pd.concat(transformed, axis=1)\n        return transformed\n\n    def fit_transform(self, X, y=None, **fit_params):\n        time_column = pd.to_datetime(X['date'], format=self.format)\n        self.earliest_yr = time_column.dt.year.min()\n        self.latest_yr = time_column.dt.year.max()\n        self.fit(X, y, **fit_params)\n        return self.transform(X)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-04T04:18:34.492945Z","iopub.execute_input":"2022-01-04T04:18:34.493223Z","iopub.status.idle":"2022-01-04T04:18:34.61307Z","shell.execute_reply.started":"2022-01-04T04:18:34.493191Z","shell.execute_reply":"2022-01-04T04:18:34.612247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Method 2: Merely extract the year, month, and date numbers, & normalize the years","metadata":{}},{"cell_type":"code","source":"#if DATE_TFM_METHOD != 'trigonometric':\n\nclass DateParser():\n    def __init__(self):\n        self.earliest_yr = None\n        self.latest_yr = None\n    \n    def parse(self, df, train_set=True):\n        df[\"date_day\"] = df[\"date\"].dt.day\n        df[\"date_week\"] = df['date'].dt.isocalendar().week\n        df[\"date_month\"] = df[\"date\"].dt.month\n        df[\"date_quarter\"] = df[\"date\"].dt.quarter\n        df[\"date_year\"] = df[\"date\"].dt.year\n        df['date_dayofweek'] = df['date'].dt.dayofweek\n        df['date_dayofyear'] = df['date'].dt.dayofyear\n        if train_set:\n            self.earliest_yr = df.date_year.min()\n            self.latest_yr = df.date_year.max()\n        else: \n            assert self.earliest_yr != None, 'Please provide earliest year!'\n            assert self.latest_yr != None, 'Please provide latest year!'\n        df.date_year = (df.date_year - self.earliest_yr)/(self.latest_yr - self.earliest_yr)\n        return df","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:18:56.370719Z","iopub.execute_input":"2022-01-04T04:18:56.371027Z","iopub.status.idle":"2022-01-04T04:18:56.380908Z","shell.execute_reply.started":"2022-01-04T04:18:56.370997Z","shell.execute_reply":"2022-01-04T04:18:56.379901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (2) One-hot encode the categorical features\nOne-hot encoding cat features is optional, as the boosting model often is capable of dealing with the cat features itself. But it could be required for other models.\n\n**This step is prerequisite to the next step, i.e. fourier transform**\n\nNote: sometimes it does improve performance of boosting models (even without Fourier transforming the one-hot encoded categorical features).","metadata":{}},{"cell_type":"code","source":"# To one-hot encode the categorical features, i.e. countries, products, and stores\n# no need to encode the last categories\ndef OneHot(df):\n    new_df = pd.DataFrame({})\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n        \n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    \n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n    \n    return new_df","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:18:58.73854Z","iopub.execute_input":"2022-01-04T04:18:58.738835Z","iopub.status.idle":"2022-01-04T04:18:58.744939Z","shell.execute_reply.started":"2022-01-04T04:18:58.738791Z","shell.execute_reply":"2022-01-04T04:18:58.744028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (3) Fourier transform for the categorical features\nThanks to: https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model/notebook","metadata":{}},{"cell_type":"code","source":"# Requires day of year column\n# Prerequisite: OneHot() the training dataframe\n\n# The three products have different seasonal patterns\ndef FourierTfm(df, mult=50):\n    for k in range(1, mult):\n        df[f'sin{k}'] = np.sin(df.date_dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(df.date_dayofyear / 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:00.143324Z","iopub.execute_input":"2022-01-04T04:19:00.143628Z","iopub.status.idle":"2022-01-04T04:19:00.150887Z","shell.execute_reply.started":"2022-01-04T04:19:00.143593Z","shell.execute_reply":"2022-01-04T04:19:00.150043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (4) Encode holiday info\n\nSource dataset: https://www.kaggle.com/drcapa/holidays-finland-norway-sweden-20152019\n\nThe goal here is to add one column to `train_df` so that it indicates whether the date of the current entry is an holiday in the corresponding country.","metadata":{}},{"cell_type":"code","source":"def GetHoliday(holiday_path, df):\n    \"\"\"\n    Get a boolean feature of whether the current row is a holiday sale\n    \"\"\"\n    \n    holiday = pd.read_csv(holiday_path, parse_dates=['Date'])\n    fin_holiday = holiday.loc[holiday.Country == 'Finland']\n    swe_holiday = holiday.loc[holiday.Country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.Country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.Date).astype(float)\n    df['swe holiday'] = df.date.isin(swe_holiday.Date).astype(float)\n    df['nor holiday'] = df.date.isin(nor_holiday.Date).astype(float)\n    \n    df['holiday'] = np.zeros(df.shape[0]).astype(float)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:01.428433Z","iopub.execute_input":"2022-01-04T04:19:01.429045Z","iopub.status.idle":"2022-01-04T04:19:01.438235Z","shell.execute_reply.started":"2022-01-04T04:19:01.429008Z","shell.execute_reply":"2022-01-04T04:19:01.437363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (5) Feature engineering master function","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:05.738918Z","iopub.execute_input":"2022-01-04T04:19:05.739213Z","iopub.status.idle":"2022-01-04T04:19:05.744617Z","shell.execute_reply.started":"2022-01-04T04:19:05.739178Z","shell.execute_reply":"2022-01-04T04:19:05.743747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Engineer(df, train_set=True, fourier_mult=0, trigonometric_dates=True, date_tfm=None):\n    \"\"\"\n    Return a new dataframe with the engineered features\n    \"\"\"\n    \n    new_df = pd.DataFrame({})\n    \n    # Keep original columns in df\n    kept_cols = df[['country', 'store', 'product', 'date']]\n    new_df[kept_cols.columns] = kept_cols\n    new_df[\"date\"] = pd.to_datetime(new_df[\"date\"])\n    \n    # Encode holiday info\n    df = GetHoliday(holiday_path, df)\n    new_df['holiday'] = df['holiday']\n    \n    # Transforming the dates (Method 1 or 2)\n    if trigonometric_dates:\n        if train_set:\n            date_tfm = DateProcessor(date_format='%Y-%m-%d', hours_secs=False)\n            new_dates = date_tfm.fit_transform(pd.DataFrame(df['date']))\n        else: \n            assert date_tfm != None, 'Please provide the DateProcessor object!'\n            new_dates = date_tfm.transform(pd.DataFrame(df['date']))\n        new_df[new_dates.columns] = new_dates\n    # need to fix dateparser (upgrade to class, stop it from overwriting new_df)\n    else: \n        if train_set: \n            date_tfm = DateParser()\n        assert date_tfm != None, 'Please provide the DateParser object!'\n        new_df = date_tfm.parse(new_df, train_set=train_set)\n\n    # One-hot encoding of cat features (no need to encode the last categories)\n    tmp_df = OneHot(df)\n    new_df[tmp_df.columns] = tmp_df\n        \n    # Seasonal variations (Fourier series)\n    if fourier_mult:\n        new_df = FourierTfm(new_df, mult=fourier_mult)\n    \n    # Return (if this is for training set, return both the engineered dataset and the date transformer)\n    return new_df, date_tfm\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:07.818448Z","iopub.execute_input":"2022-01-04T04:19:07.819317Z","iopub.status.idle":"2022-01-04T04:19:07.828597Z","shell.execute_reply.started":"2022-01-04T04:19:07.819273Z","shell.execute_reply":"2022-01-04T04:19:07.828002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I add an option for `load_transform_data` to apply `np.log()` to the target values.","metadata":{}},{"cell_type":"code","source":"def load_transform_data(path=None, train=True, date_tfm = None, hyperparams=None, log_targets=False):\n    \"\"\"\n    Load data and apply feature engineering\n    \"\"\"\n    \n    if train:\n        orig_df, _, targets = DataLoader(path, train=True)\n    else:\n        orig_df, ids = DataLoader(path, train=False)\n        \n    FOURIER_MULT = hyperparams['FOURIER_MULT']\n    TRIGON = hyperparams['TRIGON']\n    \n    df, date_tfm = Engineer(orig_df,\n                           train_set=train,\n                           fourier_mult=FOURIER_MULT,\n                           trigonometric_dates=TRIGON,\n                           date_tfm = date_tfm)\n    if train:\n        targets = targets.astype(np.float32)\n        if log_targets:\n            df[\"num_sold\"] = np.log(targets)\n        else:\n            df[\"num_sold\"] = targets\n    else: \n        df['row_id'] = ids\n        \n    df['day_of_the_week'] = df['date'].dt.day_name()\n    df['month'] = df['date'].dt.month_name()\n    \n        \n    for col in df.columns:\n        if df[col].dtype != 'O' and df[col].dtype != '<M8[ns]' and col != 'row_id':\n            df[col] = df[col].astype(np.float32)\n    \n    return df, date_tfm","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:28.876633Z","iopub.execute_input":"2022-01-04T04:19:28.87692Z","iopub.status.idle":"2022-01-04T04:19:28.887612Z","shell.execute_reply.started":"2022-01-04T04:19:28.876882Z","shell.execute_reply":"2022-01-04T04:19:28.886823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering hyperparams\nengn_params = {'FOURIER_MULT': 0, 'TRIGON': True}\n\n# Load train data & apply feature engineering\ntrain_df, date_tfm = load_transform_data(path=train_path, train=True, hyperparams=engn_params)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:38.170682Z","iopub.execute_input":"2022-01-04T04:19:38.170993Z","iopub.status.idle":"2022-01-04T04:19:38.467344Z","shell.execute_reply.started":"2022-01-04T04:19:38.170964Z","shell.execute_reply":"2022-01-04T04:19:38.46635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df, _ = load_transform_data(path=test_path, train=False, date_tfm = date_tfm, hyperparams=engn_params)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:40.052865Z","iopub.execute_input":"2022-01-04T04:19:40.053135Z","iopub.status.idle":"2022-01-04T04:19:40.294624Z","shell.execute_reply.started":"2022-01-04T04:19:40.053106Z","shell.execute_reply":"2022-01-04T04:19:40.29356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling & training (experimental)\n\n(2nd day of competition) I discover that keeping all the produced features is fallible. Especially when I let the number of date features explode, all models wouldn't behave nicely. Only some of the features I produce turn out to be useful, which is shown in the models below. \n\n## (1) Grouping and averaging by features\nThanks to: https://www.kaggle.com/carlmcbrideellis/tps-jan-2022-a-simple-average-model-no-ml/notebook.\n\nThe selected features to group are: country, store, products, day-of-week, holiday, and month. For December entries, there is an additional `'day'` column, because daily variation is bigger in December, and we'd like to capture that.\n","metadata":{}},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    \"\"\"\n    Loss function for the competition\n    \"\"\"\n    \n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SplitByYear(df, train_start_yr, train_end_yr, test_df=pd.DataFrame()):\n    \"\"\"\n    Split the original dataset into two subsets, one for training, one for test. \n    Make sure the date column of df is parsed when doing pd.read_csv(df)\n    \"\"\"\n    \n    # test year is one year after the end training year\n    test_yr = train_end_yr + 1\n    if test_yr < 2019:\n        test_df = df[df.date.between(f'{test_yr}-01-01', f'{test_yr}-12-31')].copy()\n    train_df = df[df.date.between(f'{train_start_yr}-01-01', f'{train_end_yr}-12-31')].copy()\n    return train_df, test_df\n\ndef TrainAndValid_no_model(train_path, engn_params, train_start_yr, train_end_yr, test_path=None, log_targets=False):\n    \"\"\"\n    Use averaging method (i.e. Model 1) to get predictions.\n    \"\"\"\n    \n    orig_train_df, date_tfm = load_transform_data(path=train_path, train=True, hyperparams=engn_params, log_targets=log_targets)\n    train_df, valid_df = SplitByYear(orig_train_df, train_start_yr, train_end_yr)\n    train_means = train_df.groupby(['country','store','product','month','day_of_the_week'])['num_sold'].mean().to_dict()\n    train_df['pred'] = train_df.set_index(['country','store','product','month','day_of_the_week']).index.map(train_means.get)\n    \n    train_df_dec = train_df.query(\"month == 'December'\").copy()\n    train_means_dec = train_df_dec.groupby(['country','store','product','date_dayofyear'])['num_sold'].mean().to_dict()\n    train_df_dec['pred'] = train_df_dec.set_index(['country','store','product','date_dayofyear']).index.map(train_means_dec.get)\n    train_df.update(train_df_dec)\n    train_df['pred'] = np.ceil(train_df['pred'])\n    print('train score: ', SMAPE(train_df[\"num_sold\"], train_df[\"pred\"]))\n    \n    if len(valid_df) > 0:\n        valid_df['pred'] = valid_df.set_index(['country','store','product', 'month', 'day_of_the_week']).index.map(train_means.get)\n        valid_df_dec = valid_df.query(\"month == 'December'\").copy()\n        valid_df_dec['pred'] = valid_df_dec.set_index(['country','store','product','date_dayofyear']).index.map(train_means_dec.get)\n        valid_df.update(valid_df_dec)\n        valid_df['pred'] = np.ceil(valid_df['pred'])\n        print('valid score: ', SMAPE(valid_df['num_sold'], valid_df['pred']))\n    if test_path:\n        test_df, date_tfm = load_transform_data(path=test_path, train=False, hyperparams=engn_params, date_tfm=date_tfm, log_targets=log_targets)\n        test_df['pred'] = test_df.set_index(['country','store','product', 'month', 'day_of_the_week']).index.map(train_means.get)\n        test_df_dec = test_df.query(\"month == 'December'\").copy()\n        test_df_dec['pred'] = test_df_dec.set_index(['country','store','product','date_dayofyear']).index.map(train_means_dec.get)\n        test_df.update(test_df_dec)\n        test_df['pred'] = np.ceil(test_df['pred'])\n    else:\n        test_df = None\n    return train_df, valid_df, test_df\n\ndef PlotPredVsTarget(df, holiday=0, start_year = 2018, end_year = 2018, country='Finland', store='KaggleRama'):\n    \"\"\"\n    Produce a line plot of target values and predicted values, in a given country at a given store. \n    X-axis is time, y-axis is sale volumne, colors represent product types.\n    \"\"\"\n    \n    plt.style.use('fivethirtyeight')\n    plt.rcParams.update({'font.size': 16})\n    one_country_and_store = df.query(f\"country == '{country}' & store == '{store}'\").copy()\n    one_country_and_store = one_country_and_store[one_country_and_store.date.between(f'{start_year}-01-01', f'{end_year}-12-31')].copy()\n\n    if holiday:\n        fig, axs = plt.subplots(1+holiday, figsize=(30*(end_year - start_year + 1), 26))\n        sns.lineplot(ax=axs[0], data=one_country_and_store, x=\"date\", y=\"num_sold\", hue=\"product\", linewidth = 2, linestyle='--')\n        sns.lineplot(ax=axs[0], data=one_country_and_store, x=\"date\", y=\"pred\", hue=\"product\", linewidth = 3.5)\n        sns.scatterplot(ax=axs[1], data=one_country_and_store, x='date', y='holiday', s=100)\n    else:\n        fig, axs = plt.subplots(1+holiday, figsize=(30*(end_year - start_year + 1), 13))\n        sns.lineplot(data=one_country_and_store, x=\"date\", y=\"num_sold\", hue=\"product\", linewidth = 2, linestyle='--')\n        sns.lineplot(data=one_country_and_store, x=\"date\", y=\"pred\", hue=\"product\", linewidth = 3.5)\n    plt.legend([],[], frameon=False)       \n    return ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:44.267438Z","iopub.execute_input":"2022-01-04T04:19:44.267711Z","iopub.status.idle":"2022-01-04T04:19:44.295793Z","shell.execute_reply.started":"2022-01-04T04:19:44.267683Z","shell.execute_reply":"2022-01-04T04:19:44.29495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"engn_params = {'FOURIER_MULT': 0, 'TRIGON': True}\ntrain_df, valid_df, test_df = TrainAndValid_no_model(train_path, engn_params, 2015, 2017, test_path, log_targets=False)\nif len(valid_df) > 0:\n    PlotPredVsTarget(valid_df, 1, 2018, 2018, 'Finland', 'KaggleRama')\n\nmake_submission(test_df['row_id'], test_df['pred'], path=\"sub_averaging.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:19:46.112508Z","iopub.execute_input":"2022-01-04T04:19:46.112822Z","iopub.status.idle":"2022-01-04T04:19:47.790717Z","shell.execute_reply.started":"2022-01-04T04:19:46.112767Z","shell.execute_reply":"2022-01-04T04:19:47.790033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, this simple averaging method captures the majority of the targets, except for days in December, and perhaps on holidays too. Next step is to get the holidays of the three countries from 2015 to 2019.","metadata":{}},{"cell_type":"markdown","source":"## (2) CatBoost Regressor\n\nAccording to experiments using PyCaret by https://www.kaggle.com/akmalmir/pycaret-for-starters-tps-jan-2022, I am using catboost regressor as the baseline model.\n\nThe trigonometric transformations of selected date features (month & day of week) do little to catboost performance.\n\nFindings: Finland customers have higher enthusiasm all week. ","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.linear_model import HuberRegressor\nfrom dateutil.easter import easter\nfrom datetime import timedelta\n\ndef GetEasterDates(years, forward=6):\n    Easter_dates = []\n    for year in years:\n        Easter_dates.append(easter(year))\n        # also calculate the dates of the thirteen days following Easter\n        for day in range(1, forward+1):\n            Easter_dates.append(easter(year)+timedelta(days=day))        \n    return Easter_dates\n\ndef GetCatBoost(params):\n    model = CatBoostRegressor(**params)\n    return model\n\ndef FeatureSelect(df, engn_params, december=False):\n    \"\"\"\n    Selects particular features that are useful before model training\n    \"\"\"\n    \n    if engn_params['TRIGON']:\n        feat_list = ['Finland', 'Norway', 'KaggleRama', 'Kaggle Mug', 'Kaggle Sticker', 'date_month_sin', 'date_month_cos', 'date_dayofweek_sin', 'date_dayofweek_cos', 'holiday', 'date_year']\n        if december:\n            feat_list.extend(['date_day_sin', 'date_day_cos'])\n    else:\n        feat_list = ['Finland', 'Norway', 'KaggleRama', 'Kaggle Mug', 'Kaggle Sticker', 'holiday', 'date_year', 'date_dayofweek', 'date_month']\n        if december:\n            feat_list.append('date_day')\n\n    return df[feat_list]\n\ndef TrainAndValid(train_path, model_name, model_params, engn_params, train_start_yr, train_end_yr, log_targets=False):\n    \"\"\"\n    Load data, apply feature engineering, do train and validation split (by year) if possible, select good features, train given model, print scores, and return trained model and dataframes\n    \"\"\"\n    print(f'Train dates range from year {train_start_yr} to year {train_end_yr}.')\n    \n    CORRECT = 1.05\n    \n    if model_name == 'catboost':\n        model1 = GetCatBoost(model_params)\n        model2 = GetCatBoost(model_params)\n        model3 = GetCatBoost(model_params)\n    elif model_name == 'huber':\n        model1 = HuberRegressor(**model_params)\n        model2 = HuberRegressor(**model_params)\n        model3 = HuberRegressor(**model_params)\n\n    orig_train_df, date_tfm = load_transform_data(path=train_path, train=True, hyperparams=engn_params, log_targets=log_targets)\n    train_df, valid_df = SplitByYear(orig_train_df, train_start_yr, train_end_yr)\n    train_df_dec = train_df.query(\"month == 'December'\")\n    easter_dates_tr = GetEasterDates(range(train_start_yr, train_end_yr+1))\n    train_df_easter = train_df.query(\"date == @easter_dates_tr\")\n    \n    if len(valid_df) > 0:\n        valid_df_dec = valid_df.query(\"month == 'December'\")\n        easter_dates_va = GetEasterDates(range(train_end_yr+1,train_end_yr+2))\n        valid_df_easter = valid_df.query(\"date == @easter_dates_va\")\n        valid_data = FeatureSelect(valid_df, engn_params)\n        valid_targets = valid_df['num_sold']\n        \n    train_data = FeatureSelect(train_df, engn_params)\n    train_targets = train_df['num_sold']\n\n    model1.fit(train_data, train_targets)\n    train_df['pred'] = np.ceil(model1.predict(train_data)*CORRECT)\n    print('train score (w/o december feature): ', SMAPE(train_targets, train_df['pred']))\n    \n    if len(valid_df) > 0:\n        valid_df['pred'] = np.ceil(model1.predict(valid_data)*CORRECT)\n        print('valid score (w/o december feature): ', SMAPE(valid_targets, valid_df['pred']))\n    \n    train_data = FeatureSelect(train_df_dec, engn_params, december=True)\n    train_targets = train_df_dec['num_sold']\n\n    model2.fit(train_data, train_targets)\n    train_df_dec['pred'] = np.ceil(model2.predict(train_data)*CORRECT)\n    train_df.update(train_df_dec)\n    print('train score (with december feature): ', SMAPE(train_targets, train_df['pred']))\n    \n    if len(valid_df) > 0:\n        valid_data = FeatureSelect(valid_df_dec, engn_params, december=True)\n        valid_targets = valid_df_dec['num_sold']\n        valid_df_dec['pred'] = np.ceil(model2.predict(valid_data)*CORRECT)\n        valid_df.update(valid_df_dec)\n        print('valid score (with december feature): ', SMAPE(valid_targets, valid_df['pred']))\n        \n    train_data = FeatureSelect(train_df_easter, engn_params, december=True)\n    train_targets = train_df_easter['num_sold']\n\n    model3.fit(train_data, train_targets)\n    train_df_easter['pred'] = np.ceil(model3.predict(train_data)*CORRECT)\n    train_df.update(train_df_easter)\n    print('train score (with december + easter feature): ', SMAPE(train_targets, train_df['pred']))\n    \n    if len(valid_df) > 0:\n        valid_data = FeatureSelect(valid_df_easter, engn_params, december=True)\n        valid_targets = valid_df_easter['num_sold']\n        valid_df_easter['pred'] = np.ceil(model3.predict(valid_data)*CORRECT)\n        valid_df.update(valid_df_easter)\n        print('valid score (with december + easter feature): ', SMAPE(valid_targets, valid_df['pred']))\n        \n    if log_targets:\n        train_df.num_sold = np.exp(train_df.num_sold)\n        valid_df.num_sold = np.exp(valid_df.num_sold)\n        train_df.pred = np.exp(train_df.pred)\n        valid_df.pred = np.exp(valid_df.pred)\n        print('transformed back to original pred values!')\n    \n    return model1, model2, model3, train_df, valid_df, date_tfm\n\ndef Predict(test_path, model1, model2, model3, engn_params, date_tfm, log_targets=False):\n    \"\"\"\n    Get prediction of test data from a trained model\n    \"\"\"\n    \n    CORRECT = 1.05\n    \n    test_df, date_tfm = load_transform_data(path=test_path, train=False, hyperparams=engn_params, date_tfm=date_tfm, log_targets=log_targets)\n    test_data = FeatureSelect(test_df, engn_params)\n    test_df['pred'] = np.ceil(model1.predict(test_data)*CORRECT)\n    \n    test_df_dec = test_df.query(\"month == 'December'\")\n    test_data = FeatureSelect(test_df_dec, engn_params, december=True)\n    test_df_dec['pred'] = np.ceil(model2.predict(test_data)*CORRECT)\n    test_df.update(test_df_dec)\n    \n    easter_dates = GetEasterDates(range(2019, 2020))\n    test_df_easter = test_df.query(\"date == @easter_dates\")\n    test_data = FeatureSelect(test_df_easter, engn_params, december=True)\n    test_df_easter['pred'] = np.ceil(model3.predict(test_data)*CORRECT)\n    test_df.update(test_df_easter)\n    return test_df\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T05:02:12.518059Z","iopub.execute_input":"2022-01-04T05:02:12.51834Z","iopub.status.idle":"2022-01-04T05:02:12.54992Z","shell.execute_reply.started":"2022-01-04T05:02:12.518311Z","shell.execute_reply":"2022-01-04T05:02:12.549219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_params= {'verbose': 0, \n               'one_hot_max_size': 2,\n               #'boosting_type': 'Ordered',\n               'random_seed': 24,\n               'learning_rate': 0.3, \n               'iterations': 4000,\n               'depth': 7}\n               #'l2_leaf_reg': 7, \n               #'border_count': 64}\n#model_params= {'verbose': 0}\nengn_params = {'FOURIER_MULT': 0, 'TRIGON': True}\n\nmodel1, model2, model3, train_df, valid_df, date_tfm = TrainAndValid(train_path, \n                                         'catboost',\n                                         model_params, \n                                         engn_params, \n                                         train_start_yr = 2015, \n                                         train_end_yr = 2018,\n                                         log_targets=False)\n\nif len(valid_df) > 0:\n    PlotPredVsTarget(valid_df, 1, 2018, 2018, 'Finland', 'KaggleRama')\ntest_df = Predict(test_path, model1, model2, model3, engn_params, date_tfm, log_targets=False)\nmake_submission(test_df['row_id'], test_df['pred'], 'submission_catboost.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T05:02:17.025541Z","iopub.execute_input":"2022-01-04T05:02:17.025848Z","iopub.status.idle":"2022-01-04T05:02:40.561661Z","shell.execute_reply.started":"2022-01-04T05:02:17.025793Z","shell.execute_reply":"2022-01-04T05:02:40.560844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adding `'date_year'` columns apart from those features used in Model (1) has a marginal improvement on the performance. \n\nAdding `'holiday'` can make the model fit better on the holiday dates!\n\nThe cat boost model also fails to capture the huge daily variation in the 2nd half of December. So it inflates the prediction of the 1st half of December keeping the shapes of the curve unchanged. \n\nSales tend to be volatile around Easter until the end of June too.","metadata":{}},{"cell_type":"markdown","source":"# Inference with CV\n\nNot as the section title suggests, the implementation of cross validation is still in progress.","metadata":{}},{"cell_type":"markdown","source":"**With the extrapolation bug fixed in the `DataProcessor` class,** now we conduct the same feature engineering to the test data. See bug demo in the same section as here in versions before #19.","metadata":{}}]}