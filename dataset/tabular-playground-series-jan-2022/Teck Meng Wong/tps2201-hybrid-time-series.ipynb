{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome and have fun learning\n\n\n\n#### Linear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. We'll learn how to create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other. \n\n- Feature engineering and Linear model based on excellent: https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model @ambrosm\n- Hybrid model from Time series course: https://www.kaggle.com/learn/time-series\n- Holidays dataset: https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298990\n\nObjective of this notebook used to be a ~simple~ and robust time series regression for future use.\n\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!</strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.\n</blockquote>\n\n**Notes:**\n\n## Imports and Configuration ##","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn -U\n# IntelÂ® Extension for Scikit-learn installation:\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:25:07.682469Z","iopub.execute_input":"2022-02-01T10:25:07.683087Z","iopub.status.idle":"2022-02-01T10:27:21.17329Z","shell.execute_reply.started":"2022-02-01T10:25:07.682984Z","shell.execute_reply":"2022-02-01T10:27:21.172131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/scikit-learn-contrib/py-earth@v0.2dev","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:21.175414Z","iopub.execute_input":"2022-02-01T10:27:21.175744Z","iopub.status.idle":"2022-02-01T10:27:21.182021Z","shell.execute_reply.started":"2022-02-01T10:27:21.1757Z","shell.execute_reply":"2022-02-01T10:27:21.18018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\n\nimport ipywidgets as widgets\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nfrom datetime import date\nimport holidays\nimport calendar\nimport dateutil.easter as easter\n\nfrom collections import defaultdict\nle = defaultdict(LabelEncoder)\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(12, 8))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n    legend=False,\n)\n%config InlineBackend.figure_format = 'retina'\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\nimport os\nimport math\nimport random\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:21.183443Z","iopub.execute_input":"2022-02-01T10:27:21.1837Z","iopub.status.idle":"2022-02-01T10:27:23.231024Z","shell.execute_reply.started":"2022-02-01T10:27:21.183668Z","shell.execute_reply":"2022-02-01T10:27:23.230058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning\nMost of the global variables are not used.","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------------------------\n# Some parameters to config \nPRODUCTION = True # True: For submission run. False: Fast trial run\n\n# Hyperparameters\nFOLDS = 15 if PRODUCTION else 5   # Only 5 or 10.\nREPEAT = 5 if PRODUCTION else 1\nSEED_START = 0\n\n# NN hyperparameters\nEPOCHS = 500        # Does not matter with Early stopping. Deep network should not take too much epochs to learn\nHIDDEN_LAYERS = (200, 100)\n\nRANDOM_STATE = 42\nVERBOSE = 0\n\n# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"../input/tabular-playground-series-jan-2022\"\nGPU = False          # True: use GPU.\nFEATURE_ENGINEERING = True\n\nPSEUDO_LABEL = False # PSEUDO are not ground true and will not help long term, only used for final push\nBLEND = False        # Blend previous run\nPSEUDO_DIR = \"../input/tpsjan22-10-advanced-linear-model-with-cci/submission_linear_model_rounded.csv\"\nPSEUDO_DIR2 = \"../input/tpsjan22-10-advanced-linear-model-with-cci/submission_linear_model_rounded.csv\"\n\nN_ESTIMATORS = 700 if PSEUDO_LABEL else 240\nLOSS_CORRECTION = 1\n\n# time series data common new feature  \nDATE = \"date\"\nYEAR = \"year\"\nQUARTER = \"quarter\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\nDAYOFYEAR = \"dayofyear\"\nWEEKOFYEAR = \"weekofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:23.234075Z","iopub.execute_input":"2022-02-01T10:27:23.234862Z","iopub.status.idle":"2022-02-01T10:27:23.257173Z","shell.execute_reply.started":"2022-02-01T10:27:23.234808Z","shell.execute_reply":"2022-02-01T10:27:23.255626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(RANDOM_STATE)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:23.258544Z","iopub.execute_input":"2022-02-01T10:27:23.258982Z","iopub.status.idle":"2022-02-01T10:27:26.752692Z","shell.execute_reply.started":"2022-02-01T10:27:23.258937Z","shell.execute_reply":"2022-02-01T10:27:26.751975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function SMAPE\nâ€‹â€‹i=1â€‹âˆ‘â€‹Nâ€‹â€‹wâ€‹iâ€‹â€‹â€‹â€‹100â€‹i=1â€‹âˆ‘â€‹Nâ€‹â€‹â€‹(âˆ£tâ€‹iâ€‹â€‹âˆ£+âˆ£aâ€‹iâ€‹â€‹âˆ£)/2â€‹â€‹wâ€‹iâ€‹â€‹âˆ£aâ€‹iâ€‹â€‹âˆ’tâ€‹iâ€‹â€‹âˆ£â€‹â€‹â€‹â€‹","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\ndef smape_loss(y_true, y_pred):\n    \"\"\"\n    SMAPE Loss\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Ground truth (correct) target values.\n    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        Estimated target values.\n    Returns\n    -------\n    loss : float or ndarray of floats\n        If multioutput is 'raw_values', then mean absolute error is returned\n        for each output separately.\n        If multioutput is 'uniform_average' or an ndarray of weights, then the\n        weighted average of all output errors is returned.\n        SMAPE output is non-negative floating point. The best value is 0.0.\n\n    \"\"\"\n    assert(y_true.shape == y_pred.shape)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:26.753945Z","iopub.execute_input":"2022-02-01T10:27:26.754398Z","iopub.status.idle":"2022-02-01T10:27:26.856826Z","shell.execute_reply.started":"2022-02-01T10:27:26.754346Z","shell.execute_reply":"2022-02-01T10:27:26.855692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/282735\ndef better_than_median(inputs, axis):\n    \"\"\"Compute the mean of the predictions if there are no outliers,\n    or the median if there are outliers.\n\n    Parameter: inputs = ndarray of shape (n_samples, n_folds)\"\"\"\n    spread = inputs.max(axis=axis) - inputs.min(axis=axis) \n    spread_lim = 0.45\n    print(f\"Inliers:  {(spread < spread_lim).sum():7} -> compute mean\")\n    print(f\"Outliers: {(spread >= spread_lim).sum():7} -> compute median\")\n    print(f\"Total:    {len(inputs):7}\")\n    return np.where(spread < spread_lim,\n                    np.mean(inputs, axis=axis),\n                    np.median(inputs, axis=axis))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:26.859017Z","iopub.execute_input":"2022-02-01T10:27:26.85938Z","iopub.status.idle":"2022-02-01T10:27:26.954736Z","shell.execute_reply.started":"2022-02-01T10:27:26.859334Z","shell.execute_reply":"2022-02-01T10:27:26.95403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from math import ceil, floor, sqrt\n# from https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n\n    return result_array","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:26.956262Z","iopub.execute_input":"2022-02-01T10:27:26.957203Z","iopub.status.idle":"2022-02-01T10:27:27.045035Z","shell.execute_reply.started":"2022-02-01T10:27:26.957144Z","shell.execute_reply":"2022-02-01T10:27:27.044201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_periodogram(ts, detrend='linear', ax=None):\n    from scipy.signal import periodogram\n    fs = pd.Timedelta(\"1Y\") / pd.Timedelta(\"1D\")\n    freqencies, spectrum = periodogram(\n        ts,\n        fs=fs,\n        detrend=detrend,\n        window=\"boxcar\",\n        scaling='spectrum',\n    )\n    if ax is None:\n        _, ax = plt.subplots()\n    ax.step(freqencies, spectrum, color=\"purple\")\n    ax.set_xscale(\"log\")\n    ax.set_xticks([1, 2, 4, 6, 12, 26, 52, 104])\n    ax.set_xticklabels(\n        [\n            \"Annual (1)\",\n            \"Semiannual (2)\",\n            \"Quarterly (4)\",\n            \"Bimonthly (6)\",\n            \"Monthly (12)\",\n            \"Biweekly (26)\",\n            \"Weekly (52)\",\n            \"Semiweekly (104)\",\n        ],\n        rotation=30,\n    )\n    ax.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0, 0))\n    ax.set_ylabel(\"Variance\")\n    ax.set_title(\"Periodogram\")\n    return ax","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:27.047502Z","iopub.execute_input":"2022-02-01T10:27:27.04785Z","iopub.status.idle":"2022-02-01T10:27:27.156476Z","shell.execute_reply.started":"2022-02-01T10:27:27.047799Z","shell.execute_reply":"2022-02-01T10:27:27.155648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. We'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"### Handle Missing Values ###\n\nHandling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise.","metadata":{}},{"cell_type":"code","source":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:27.158881Z","iopub.execute_input":"2022-02-01T10:27:27.159496Z","iopub.status.idle":"2022-02-01T10:27:27.17652Z","shell.execute_reply.started":"2022-02-01T10:27:27.159457Z","shell.execute_reply":"2022-02-01T10:27:27.17566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data/Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Periodic spline features\nWe can try an alternative encoding of the periodic time-related features using spline transformations with a large enough number of splines, and as a result a larger number of expanded features compared to the sine/cosine transformation:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import SplineTransformer\n\n\ndef periodic_spline_transformer(period, n_splines=None, degree=3):\n    if n_splines is None:\n        n_splines = period\n    n_knots = n_splines + 1  # periodic and include_bias is True\n    return SplineTransformer(\n        degree=degree,\n        n_knots=n_knots,\n        knots=np.linspace(0, period, n_knots).reshape(n_knots, 1),\n        extrapolation=\"periodic\",\n        include_bias=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:27.177848Z","iopub.execute_input":"2022-02-01T10:27:27.17857Z","iopub.status.idle":"2022-02-01T10:27:27.192394Z","shell.execute_reply.started":"2022-02-01T10:27:27.178506Z","shell.execute_reply":"2022-02-01T10:27:27.191192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"year_df = pd.DataFrame(\n    np.linspace(0, 365, 1000).reshape(-1, 1),\n    columns=[DAYOFYEAR],\n)\nsplines = periodic_spline_transformer(365, n_splines=12, degree=2).fit_transform(year_df)\nsplines_df = pd.DataFrame(\n    splines,\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n)\npd.concat([year_df, splines_df], axis=\"columns\").plot(x=DAYOFYEAR, cmap=plt.cm.tab20b)\n_ = plt.title(f\"Periodic spline-based encoding for the {DAYOFYEAR} feature\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:27.19381Z","iopub.execute_input":"2022-02-01T10:27:27.19472Z","iopub.status.idle":"2022-02-01T10:27:28.176213Z","shell.execute_reply.started":"2022-02-01T10:27:27.194667Z","shell.execute_reply":"2022-02-01T10:27:28.175441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/samuelcortinhas/tps-jan-22-quick-eda-hybrid-model/notebook\ndef unofficial_holiday(df):\n    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n    \n    # load holiday info.\n    hol_path = '../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv'\n    holiday = pd.read_csv(hol_path)\n    \n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.177555Z","iopub.execute_input":"2022-02-01T10:27:28.17796Z","iopub.status.idle":"2022-02-01T10:27:28.188624Z","shell.execute_reply.started":"2022-02-01T10:27:28.177929Z","shell.execute_reply":"2022-02-01T10:27:28.187546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BUID calendar columns\nMONTH_COLUMNS = []\nWEEKOFYEAR_COLUMNS = []\nDAYOFYEAR_COLUMNS = []\nWEEKDAY_COLUMNS = []\n\nfor x in [MONTH,WEEKOFYEAR,DAYOFYEAR,WEEKDAY]:\n    for y in [f'mug_{x}', f'hat_{x}', f'stick_{x}']:\n        if x == MONTH:\n            MONTH_COLUMNS.append(y)\n        if x == WEEKOFYEAR:\n            WEEKOFYEAR_COLUMNS.append(y)\n        if x == DAYOFYEAR:\n            DAYOFYEAR_COLUMNS.append(y)\n        if x == WEEKDAY:\n            WEEKDAY_COLUMNS.append(y)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.189961Z","iopub.execute_input":"2022-02-01T10:27:28.190496Z","iopub.status.idle":"2022-02-01T10:27:28.203637Z","shell.execute_reply.started":"2022-02-01T10:27:28.190463Z","shell.execute_reply":"2022-02-01T10:27:28.202894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fourier_features(index, freq, order):\n    time = np.arange(len(index), dtype=np.float32)\n    k = 2 * np.pi * (1 / freq) * time\n    features = {}\n    for i in range(1, order + 1):\n        features.update({\n            f\"sin_{freq}_{i}\": np.sin(i * k),\n            f\"cos_{freq}_{i}\": np.cos(i * k),\n        })\n    return pd.DataFrame(features, index=index)\n\ndef get_basic_ts_features(df):\n    \n    gdp_df = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\n    gdp_df.set_index('year', inplace=True)\n#     gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation\n    def get_gdp(row):\n        country = row.country\n        return gdp_df.loc[row.date.year, country] #**gdp_exponent\n\n    # Apply GDP log\n    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n    \n#     # Split GDP by country (for linear model)\n#     df['fin_gdp']=np.where(df['country'] == 'Finland', df['gdp'], 0)\n#     df['nor_gdp']=np.where(df['country'] == 'Norway', df['gdp'], 0)\n#     df['swe_gdp']=np.where(df['country'] == 'Sweden', df['gdp'], 0)\n    \n#     # Drop column\n#     df=df.drop(['gdp'],axis=1)\n    \n    # one-hot encoding should be used. linear model should not learn this as numeric value\n#     df[YEAR] = df[DATE].dt.year\n#     df[MONTH] = df[DATE].dt.month\n#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n#     df[WEEKDAY] = df[DATE].dt.weekday\n#     df[DAY] = df[DATE].dt.day # day in month\n#     df[DAYOFMONTH] = df[DATE].dt.days_in_month\n#     df[DAYOFWEEK] = df[DATE].dt.dayofweek\n#     df[MONTH] = df[DATE].dt.month # Min SMAPE: 4.005319478790032\n#     df[QUARTER] = df.date.dt.quarter\n\n#     df['wd0'] = df[DATE].dt.weekday == 0 # + Monday\n#     df['wd1'] = df[DATE].dt.weekday == 1 # Tuesday\n#     df['wd2'] = df[DATE].dt.weekday == 2\n#     df['wd3'] = df[DATE].dt.weekday == 3\n    df['wd4'] = df[DATE].dt.weekday == 4 # + Friday\n    df['wd56'] = df[DATE].dt.weekday >= 5 # + Weekend\n\n#     df[f'mug_wd4'] = np.where(df['product'] == 'Kaggle Mug', df[f'wd4'], False)\n#     df[f'mug_wd56'] = np.where(df['product'] == 'Kaggle Mug', df[f'wd56'], False)\n#     df[f'hat_wd4'] = np.where(df['product'] == 'Kaggle Hat', df[f'wd4'], False)\n#     df[f'hat_wd56'] = np.where(df['product'] == 'Kaggle Hat', df[f'wd56'], False)\n#     df[f'stick_wd4'] = np.where(df['product'] == 'Kaggle Sticker', df[f'wd4'], False)\n#     df[f'stick_wd56'] = np.where(df['product'] == 'Kaggle Sticker', df[f'wd56'], False)\n#     df = df.drop(columns=[f'wd4', f'wd56'])\n    # 4 seasons\n#     df['season'] = ((df[DATE].dt.month % 12 + 3) // 3).map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n\n    return df\n\ndef feature_splines(df):\n    # one-hot encoding should be used. linear model should not learn this as numeric value\n#     df[MONTH] = df[DATE].dt.month\n#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n#     df[WEEKDAY] = df[DATE].dt.weekday\n#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n    \n    dayofyear_splines = periodic_spline_transformer(365, n_splines=9, degree=2).fit_transform(df[DATE].dt.dayofyear.values.reshape(-1, 1))\n    splines_df = pd.DataFrame(\n        dayofyear_splines,\n        columns=[f\"spline_{i}\" for i in range(dayofyear_splines.shape[1])],\n    )\n    for i in range(dayofyear_splines.shape[1]):\n        df[f'mug_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Mug', splines_df[f\"spline_{i}\"], 0.)\n        df[f'hat_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Hat', splines_df[f\"spline_{i}\"], 0.)\n#         df[f'stick_{DAYOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Sticker', splines_df[f\"spline_{i}\"], 0.)\n#         df[f'fin_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Finland', splines_df[f\"spline_{i}\"], 0.)\n#         df[f'nor_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Norway', splines_df[f\"spline_{i}\"], 0.)\n#         df[f'swe_{DAYOFYEAR}{i}'] = np.where(df['country'] == 'Sweden', splines_df[f\"spline_{i}\"], 0.)\n\n#     weekofyear_splines = periodic_spline_transformer(52, n_splines=2, degree=2).fit_transform(df[DATE].dt.isocalendar().week.values.astype(np.float64).reshape(-1,1))\n#     splines_df = pd.DataFrame(\n#         weekofyear_splines,\n#         columns=[f\"spline_{i}\" for i in range(weekofyear_splines.shape[1])],\n#     )\n#     for i in range(weekofyear_splines.shape[1]):\n#         df[f'weekofyear_{WEEKOFYEAR}{i}'] = splines_df[f\"spline_{i}\"]\n#         df[f'hat_{WEEKOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Hat', splines_df[f\"spline_{i}\"], 0)\n#         df[f'stick_{WEEKOFYEAR}{i}'] = np.where(df['product'] == 'Kaggle Sticker', splines_df[f\"spline_{i}\"], 0)\n#     df[f'mug_{MONTH}'] = np.where(df['product'] == 'Kaggle Mug', df[MONTH], 0)\n#     df[f'mug_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Mug', df[WEEKOFYEAR], 0)\n#     df[f'mug_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Mug', df[DAYOFYEAR], 0)\n#     df[f'mug_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Mug', df[WEEKDAY], 0)\n#     df[f'hat_{MONTH}'] = np.where(df['product'] == 'Kaggle Hat', df[MONTH], 0)\n#     df[f'hat_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Hat', df[WEEKOFYEAR], 0)\n#     df[f'hat_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Hat', df[DAYOFYEAR], 0)\n#     df[f'hat_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Hat', df[WEEKDAY], 0)\n#     df[f'stick_{MONTH}'] = np.where(df['product'] == 'Kaggle Sticker', df[MONTH], 0)\n#     df[f'stick_{WEEKOFYEAR}'] = np.where(df['product'] == 'Kaggle Sticker', df[WEEKOFYEAR], 0)\n#     df[f'stick_{DAYOFYEAR}'] = np.where(df['product'] == 'Kaggle Sticker', df[DAYOFYEAR], 0)\n#     df[f'stick_{WEEKDAY}'] = np.where(df['product'] == 'Kaggle Sticker', df[WEEKDAY], 0)\n\n#     df = df.drop(columns=[DAYOFYEAR]) #MONTH, WEEKOFYEAR, WEEKDAY\n\n    return df\n\ndef feature_periodic(df):\n    # 21 days cyclic for lunar\n    # 21 4.244872419046287 31 4.23870 37 4.2359085545955875 47 4.24590382934362 39 4.236812122257115 \n    # 35 4.2358561209794665 33 4.237682217183017 36 4.230652791910613 3 4.241000488616227 4.23833321067532\n    #[7, 14, 21, 28, 30, 31, 91] range(1, 32, 4) range(1,3,1)[1,2,4]\n    # Long term periodic\n    dayofyear = df.date.dt.dayofyear\n    j=-36\n    for k in [2]:\n        df = pd.concat([df,\n                        pd.DataFrame({\n                            f\"sin{k}\": np.sin((dayofyear+j) / 365 * 1 * math.pi * k),\n                            f\"cos{k}\": np.cos((dayofyear+j) / 365 * 1 * math.pi * k),\n                                     })], axis=1)\n        # Products\n        df[f'mug_sin{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'sin{k}'], 0)\n        df[f'mug_cos{k}'] = np.where(df['product'] == 'Kaggle Mug', df[f'cos{k}'], 0)\n        df[f'hat_sin{k}'] = np.where(df['product'] == 'Kaggle Hat', df[f'sin{k}'], 0)\n        df[f'hat_cos{k}'] = np.where(df['product'] == 'Kaggle Hat', df[f'cos{k}'], 0)\n#         df[f'stick_sin{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'sin{k}'], 0)\n#         df[f'stick_cos{k}'] = np.where(df['product'] == 'Kaggle Sticker', df[f'cos{k}'], 0)\n        df = df.drop(columns=[f'sin{k}', f'cos{k}'])\n\n    # Short term Periodic\n    weekday = df.date.dt.weekday\n    df[f'weekly_sin'] = np.sin((1 / 7) * 2 * math.pi*(weekday+1)) #+\n    df[f'weekly_cos'] = np.cos((1 / 7) * 2 * math.pi*(weekday+1)) #+\n    df[f'semiweekly_sin'] = np.sin((1 / 7) * 4 * math.pi*(dayofyear-1.5)) #+ â…sin(1/7 ðœ‹â‹…4(ð‘¥âˆ’2))â†\n    df[f'semiweekly_cos'] = np.cos((1 / 7) * 4 * math.pi*(dayofyear-1.5)) #+ â…cos(1/7 ðœ‹â‹…4ð‘¥)â†\n    \n    df[f'fin_weekly_sin'] = np.where(df['country'] == 'Finland', df[f'weekly_sin'], 0)\n    df[f'fin_weekly_cos'] = np.where(df['country'] == 'Finland', df[f'weekly_cos'], 0)\n    df[f'nor_weekly_sin'] = np.where(df['country'] == 'Norway', df[f'weekly_sin'], 0)\n    df[f'nor_weekly_cos'] = np.where(df['country'] == 'Norway', df[f'weekly_cos'], 0)\n    df[f'swe_weekly_sin'] = np.where(df['country'] == 'Sweden', df[f'weekly_sin'], 0)\n    df[f'swe_weekly_cos'] = np.where(df['country'] == 'Sweden', df[f'weekly_cos'], 0)\n    \n    df[f'mug_weekly_sin'] = np.where(df['product'] == 'Kaggle Mug', df[f'weekly_sin'], 0)\n    df[f'mug_weekly_cos'] = np.where(df['product'] == 'Kaggle Mug', df[f'weekly_cos'], 0)\n    df[f'hat_weekly_sin'] = np.where(df['product'] == 'Kaggle Hat', df[f'weekly_sin'], 0)\n    df[f'hat_weekly_cos'] = np.where(df['product'] == 'Kaggle Hat', df[f'weekly_cos'], 0)\n    df[f'stick_weekly_sin'] = np.where(df['product'] == 'Kaggle Sticker', df[f'weekly_sin'], 0)\n    df[f'stick_weekly_cos'] = np.where(df['product'] == 'Kaggle Sticker', df[f'weekly_cos'], 0)\n    \n    df[f'mug_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Mug', df[f'semiweekly_sin'], 0)\n    df[f'mug_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Mug', df[f'semiweekly_cos'], 0)\n    df[f'hat_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Hat', df[f'semiweekly_sin'], 0)\n    df[f'hat_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Hat', df[f'semiweekly_cos'], 0)\n#     df[f'stick_semiweekly_sin'] = np.where(df['product'] == 'Kaggle Sticker', df[f'semiweekly_sin'], 0)\n#     df[f'stick_semiweekly_cos'] = np.where(df['product'] == 'Kaggle Sticker', df[f'semiweekly_cos'], 0)\n    \n    df = df.drop(columns=['weekly_sin', 'weekly_cos', 'semiweekly_sin', 'semiweekly_cos'])\n    \n#     df[f'semiannual_sin'] = np.sin(dayofyear / 182.5 * 2 * math.pi)\n#     df[f'semiannual_cos'] = np.cos(dayofyear / 182.5 * 2 * math.pi)\n    \n    return df\n\ndef feature_holiday(df):\n# Dec Jan\n    # End of year\n    df = pd.concat([df,\n                        pd.DataFrame({f\"f-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"s-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"n-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})\n                       ], axis=1)\n        \n    # May\n    df = pd.concat([df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}),\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & \n                                      (df.country == 'Norway')\n                                      for d in list(range(18, 28))})\n                        ], axis=1)\n    \n    # June and July 8, 14\n    df = pd.concat([df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                       ], axis=1)\n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & \n                                      (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})], axis=1)\n\n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n                                      for d in list(range(0, 9))})], axis=1)\n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})], axis=1)\n    # Easter April\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.205494Z","iopub.execute_input":"2022-02-01T10:27:28.206008Z","iopub.status.idle":"2022-02-01T10:27:28.269852Z","shell.execute_reply.started":"2022-02-01T10:27:28.205965Z","shell.execute_reply":"2022-02-01T10:27:28.269074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import ephem\n\n    # one-hot encoding should be used. linear model should not learn this as numeric value\n#     df[YEAR] = df[DATE].dt.year\n#     df[MONTH] = df[DATE].dt.month\n#     df[WEEK] = df[DATE].dt.week\n#     df[DAY] = df[DATE].dt.day\n#     df[DAYOFYEAR] = df[DATE].dt.dayofyear\n#     df[WEEKOFYEAR] = df[DATE].dt.isocalendar().week\n#     df[DAYOFMONTH] = df[DATE].dt.days_in_month\n#     df[DAYOFWEEK] = df[DATE].dt.dayofweek\n#     df[WEEKDAY] = df[DATE].dt.weekday\n#     df['wd1'] = df[DATE].dt.weekday == 1\n#     df['wd2'] = df[DATE].dt.weekday == 2\n#     df['wd3'] = df[DATE].dt.weekday == 3\n#     df['wd4'] = df[DATE].dt.weekday == 4\n#     df.loc[(df.date.dt.year != 2016) & (df.date.dt.month >=3), DAYOFYEAR] += 1 # fix for leap years\n    # 4 seasons\n#     df['season'] = ((df[DATE].dt.month % 12 + 3) // 3).map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n#     df[MONTH] = df[MONTH].apply(lambda x: calendar.month_abbr[x])\n        # Countries\n#         df[f'finland_sin{k}'] = np.where(df['country'] == 'Finland', df[f'sin{k}'], 0) # new: 4.015424129340626 old: 4.030858784243854\n#         df[f'finland_cos{k}'] = np.where(df['country'] == 'Finland', df[f'cos{k}'], 0)\n#         df[f'norway_sin{k}'] = np.where(df['country'] == 'Norway', df[f'sin{k}'], 0)\n#         df[f'norway_cos{k}'] = np.where(df['country'] == 'Norway', df[f'cos{k}'], 0)\n#         df[f'sweden_sin{k}'] = np.where(df['country'] == 'Sweden', df[f'sin{k}'], 0)\n#         df[f'sweden_cos{k}'] = np.where(df['country'] == 'Sweden', df[f'cos{k}'], 0)\n#         df[f'mart_sin{k}'] = np.where(df['store'] == 'KaggleMart', df[f'sin{k}'], 0)\n#         df[f'mart_cos{k}'] = np.where(df['store'] == 'KaggleMart', df[f'cos{k}'], 0)\n#         df[f'rama_sin{k}'] = np.where(df['store'] == 'KaggleRama', df[f'sin{k}'], 0)\n#         df[f'rama_cos{k}'] = np.where(df['store'] == 'KaggleRama', df[f'cos{k}'], 0)\n#         df[f'finland_sin{k}'] = np.where(df['country'] == 'Finland', df[f'sin{k}'], 0)\n#         df[f'finland_cos{k}'] = np.where(df['country'] == 'Finland', df[f'cos{k}'], 0)\n#         df[f'norway_sin{k}'] = np.where(df['country'] == 'Norway', df[f'sin{k}'], 0)\n#         df[f'norway_cos{k}'] = np.where(df['country'] == 'Norway', df[f'cos{k}'], 0)\n#         df[f'store_sin{k}'] = np.where(df['store'] == 'KaggleMart', df[f'sin{k}'], 0)\n#         df[f'store_cos{k}'] = np.where(df['store'] == 'KaggleMart', df[f'cos{k}'], 0)\n#     df[f'semiweekly_sin'] = np.sin((1 / 7) * 4 * math.pi*(dayofyear-2)) #+ â…sin(1/7 ðœ‹â‹…4(ð‘¥âˆ’2))â†\n#     df[f'semiweekly_cos'] = np.cos((1 / 2) * 2 * math.pi*dayofyear) #+ â…cos(1/2 ðœ‹â‹…2ð‘¥)â†\n#     df[f'lunar_sin'] = np.sin((1 / 21) * 2 * math.pi*dayofyear)\n#     df[f'lunar_cos'] = np.cos((1 / 21) * 2 * math.pi*dayofyear)\n#     df[f'season_sin'] = np.sin(dayofyear / 91.5 * 2 * math.pi)\n#     df[f'season_cos'] = np.cos(dayofyear / 91.5 * 2 * math.pi)\n#     df[f'lunar_phase'] = df.date.apply(lambda x: ephem.Moon(str(x)).moon_phase)\n#     df[f'semiannual_sin'] = np.sin(dayofyear / 182.5 * 2 * math.pi)\n#     df[f'semiannual_cos'] = np.cos(dayofyear / 182.5 * 2 * math.pi)\n#     df[f'sin31'] = np.sin((dayofyear) / 365 * 2 * math.pi*31) #+\n#     df[f'cos31'] = np.cos(dayofyear / 365 * 2 * math.pi*31)\n\n    # Individual holidays\n#     df = pd.concat([df, pd.DataFrame({f'fin{ptr[1]}':\n#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Finland')\n#                                       for ptr in holidays.Finland(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n#     df = pd.concat([df, pd.DataFrame({f'nor{ptr[1]}':\n#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Norway')\n#                                       for ptr in holidays.Norway(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n#     df = pd.concat([df, pd.DataFrame({f'swe{ptr[1]}':\n#                                       (df.date == pd.Timestamp(ptr[0])) & (df.country == 'Sweden')\n#                                       for ptr in holidays.Sweden(years = [2015,2016,2017,2018,2019]).items()})], axis=1)\n    \n    #Swedish Rock Concert\n    #Jun 3, 2015 â€“ Jun 6, 2015\n    #Jun 8, 2016 â€“ Jun 11, 2016\n    #Jun 7, 2017 â€“ Jun 10, 2017\n    #Jun 6, 2018 â€“ Jun 10, 2018\n    #Jun 5, 2019 â€“ Jun 8, 2019\n#     swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),\n#                                          2016: pd.Timestamp(('2016-06-11')),\n#                                          2017: pd.Timestamp(('2017-06-10')),\n#                                          2018: pd.Timestamp(('2018-06-10')),\n#                                          2019: pd.Timestamp(('2019-06-8'))})\n#     df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n#                                       (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n#                                       for d in list(range(-3, 3))})], axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:28.27153Z","iopub.execute_input":"2022-02-01T10:27:28.271941Z","iopub.status.idle":"2022-02-01T10:27:28.288141Z","shell.execute_reply.started":"2022-02-01T10:27:28.271908Z","shell.execute_reply":"2022-02-01T10:27:28.286824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineer(df):\n    df = get_basic_ts_features(df)\n#     df = feature_splines(df)\n    df = feature_periodic(df)\n    df = feature_holiday(df)\n    df = unofficial_holiday(df)\n    return df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.289276Z","iopub.execute_input":"2022-02-01T10:27:28.290038Z","iopub.status.idle":"2022-02-01T10:27:28.306534Z","shell.execute_reply.started":"2022-02-01T10:27:28.289983Z","shell.execute_reply":"2022-02-01T10:27:28.305344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir / \"train.csv\", parse_dates=[DATE],\n                    usecols=['date', 'country', 'store', 'product', 'num_sold'],\n                    dtype={\n                        'country': 'category',\n                        'store': 'category',\n                        'product': 'category',\n                        'num_sold': 'float64',\n                    },\n                    infer_datetime_format=True,)\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID, parse_dates=[DATE])\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    df_train[DATE] = pd.to_datetime(df_train[DATE])\n    df_test[DATE] = pd.to_datetime(df_test[DATE])\n    return df_train, df_test, column_y\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.307582Z","iopub.execute_input":"2022-02-01T10:27:28.3082Z","iopub.status.idle":"2022-02-01T10:27:28.321932Z","shell.execute_reply.started":"2022-02-01T10:27:28.308164Z","shell.execute_reply":"2022-02-01T10:27:28.320595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(df_train, df_test):\n    # Preprocessing\n    if FEATURE_ENGINEERING:\n        df_train = feature_engineer(df_train)\n        df_test = feature_engineer(df_test)\n\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.323583Z","iopub.execute_input":"2022-02-01T10:27:28.324519Z","iopub.status.idle":"2022-02-01T10:27:28.334591Z","shell.execute_reply.started":"2022-02-01T10:27:28.324471Z","shell.execute_reply":"2022-02-01T10:27:28.333996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data #\n\nAnd now we can call the data loader and get the processed data splits:","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_df, test_df, column_y = load_data()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.336236Z","iopub.execute_input":"2022-02-01T10:27:28.336812Z","iopub.status.idle":"2022-02-01T10:27:28.466073Z","shell.execute_reply.started":"2022-02-01T10:27:28.336767Z","shell.execute_reply":"2022-02-01T10:27:28.465082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pseudolabeling","metadata":{}},{"cell_type":"code","source":"df_pseudolabels = pd.read_csv(PSEUDO_DIR, index_col=ID)\ndf_pseudolabels[DATE] = pd.to_datetime(test_df[DATE])\ndf_pseudolabels.to_csv(\"pseudo_labels_v0.csv\", index=True)\n# if PSEUDO_LABEL:\n    # df_pseudolabels = df_pseudolabels.set_index([DATE]).sort_index()\ntest_df[column_y] = df_pseudolabels[column_y].astype(np.float64)\ntrain_df = pd.concat([train_df, test_df], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.467163Z","iopub.execute_input":"2022-02-01T10:27:28.467371Z","iopub.status.idle":"2022-02-01T10:27:28.578949Z","shell.execute_reply.started":"2022-02-01T10:27:28.467345Z","shell.execute_reply":"2022-02-01T10:27:28.577981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df, test_df = process_data(train_df, test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:28.580163Z","iopub.execute_input":"2022-02-01T10:27:28.580401Z","iopub.status.idle":"2022-02-01T10:27:33.172784Z","shell.execute_reply.started":"2022-02-01T10:27:28.580371Z","shell.execute_reply":"2022-02-01T10:27:33.171865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_df.copy()\ntest_data = test_df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:33.174379Z","iopub.execute_input":"2022-02-01T10:27:33.174896Z","iopub.status.idle":"2022-02-01T10:27:33.184414Z","shell.execute_reply.started":"2022-02-01T10:27:33.174852Z","shell.execute_reply":"2022-02-01T10:27:33.183541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.set_index([DATE]).sort_index()\nX_test = test_data.set_index([DATE]).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:33.186295Z","iopub.execute_input":"2022-02-01T10:27:33.186612Z","iopub.status.idle":"2022-02-01T10:27:33.228201Z","shell.execute_reply.started":"2022-02-01T10:27:33.186561Z","shell.execute_reply":"2022-02-01T10:27:33.227517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])\nmissing_val = X_test.isnull().sum()\nprint(missing_val[missing_val > 0])","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:33.229404Z","iopub.execute_input":"2022-02-01T10:27:33.229814Z","iopub.status.idle":"2022-02-01T10:27:33.264997Z","shell.execute_reply.started":"2022-02-01T10:27:33.229783Z","shell.execute_reply":"2022-02-01T10:27:33.264163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.set_index(['date', 'country', 'store', 'product']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:33.266183Z","iopub.execute_input":"2022-02-01T10:27:33.266697Z","iopub.status.idle":"2022-02-01T10:27:33.311018Z","shell.execute_reply.started":"2022-02-01T10:27:33.266658Z","shell.execute_reply":"2022-02-01T10:27:33.310347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing fourier features\nCan it replicate the chaos below?","metadata":{}},{"cell_type":"code","source":"fig_dims = (30,6)\ntrain_subset = train_data.loc['2015-01-1':'2015-12-27']\n# ax = train_subset.stick_semiweekly_cos.plot(title='Period', figsize=fig_dims)\n# ax = train_subset.hat_sin2.plot(title='Period', figsize=fig_dims) #lunar_cos weekly_cos sin2 lunar_sin season_sin semiweekly_sin semiannual_sin weekly_sin\n# ax = train_subset.mug_cos2.plot(title='Period', figsize=fig_dims)\n# ax = train_subset.sin2.plot(title='Period', figsize=fig_dims)\n# _ = ax.set(ylabel=\"Wave\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:33.312491Z","iopub.execute_input":"2022-02-01T10:27:33.312972Z","iopub.status.idle":"2022-02-01T10:27:33.323203Z","shell.execute_reply.started":"2022-02-01T10:27:33.312927Z","shell.execute_reply":"2022-02-01T10:27:33.322553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PRODUCTION:\n    kaggle_sales_2015 = (\n        train_data\n        .groupby(['country', 'store', 'product', 'date'])\n        .mean()\n        .unstack(['country', 'store', 'product'])\n        .loc['2015']\n    )\n    kaggle_sales_2016 = (\n        train_data\n        .groupby(['country', 'store', 'product', 'date'])\n        .mean()\n        .unstack(['country', 'store', 'product'])\n        .loc['2016']\n    )\n    kaggle_sales_2017 = (\n        train_data\n        .groupby(['country', 'store', 'product', 'date'])\n        .mean()\n        .unstack(['country', 'store', 'product'])\n        .loc['2017']\n    )\n    kaggle_sales_2018 = (\n        train_data\n        .groupby(['country', 'store', 'product', 'date'])\n        .mean()\n        .unstack(['country', 'store', 'product'])\n        .loc['2018']\n    )\n    frames = [kaggle_sales_2015, kaggle_sales_2016, kaggle_sales_2017, kaggle_sales_2018]\n    kaggle_sales = pd.concat(frames)\n\n    fig_dims = (20,12)\n    ax = kaggle_sales.num_sold.plot(title='Sales Trends', figsize=fig_dims)\n    _ = ax.set(ylabel=\"Numbers sold\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:33.327687Z","iopub.execute_input":"2022-02-01T10:27:33.328061Z","iopub.status.idle":"2022-02-01T10:27:36.941821Z","shell.execute_reply.started":"2022-02-01T10:27:33.328024Z","shell.execute_reply":"2022-02-01T10:27:36.940676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some sample of the dataset.","metadata":{}},{"cell_type":"code","source":"# X.plot(y='weekofyear_weekofyear0', cmap=plt.cm.tab20b)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:36.943438Z","iopub.execute_input":"2022-02-01T10:27:36.943805Z","iopub.status.idle":"2022-02-01T10:27:36.94676Z","shell.execute_reply.started":"2022-02-01T10:27:36.943769Z","shell.execute_reply":"2022-02-01T10:27:36.946056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if PRODUCTION: \ntrain_data.groupby(column_y).apply(lambda s: s.sample(min(len(s), 5)))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:36.947922Z","iopub.execute_input":"2022-02-01T10:27:36.948261Z","iopub.status.idle":"2022-02-01T10:27:39.065409Z","shell.execute_reply.started":"2022-02-01T10:27:36.948233Z","shell.execute_reply":"2022-02-01T10:27:39.064203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean up","metadata":{}},{"cell_type":"code","source":"del test_df\ndel train_data\ndel test_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:39.067203Z","iopub.execute_input":"2022-02-01T10:27:39.067531Z","iopub.status.idle":"2022-02-01T10:27:39.217975Z","shell.execute_reply.started":"2022-02-01T10:27:39.067489Z","shell.execute_reply":"2022-02-01T10:27:39.217015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What is Seasonality? #\n\nWe say that a time series exhibits **seasonality** whenever there is a regular, periodic change in the mean of the series. Seasonal changes generally follow the clock and calendar -- repetitions over a day, a week, or a year are common. Seasonality is often driven by the cycles of the natural world over days and years or by conventions of social behavior surrounding dates and times.\n### Choosing Fourier features with the Periodogram\n\nHow many Fourier pairs should we actually include in our feature set? We can answer this question with the periodogram. The **periodogram** tells you the strength of the frequencies in a time series. Specifically, the value on the y-axis of the graph is `(a ** 2 + b ** 2) / 2`, where `a` and `b` are the coefficients of the sine and cosine at that frequency (as in the *Fourier Components* plot above).\n\n<figure style=\"padding: 1em;\">\n<img src=\"https://i.imgur.com/PK6WEe3.png\" width=600, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>Periodogram for the <em>Wiki Trigonometry</em> series.</center></figcaption>\n</figure>\n\nFrom left to right, the periodogram drops off after *Quarterly*, four times a year. That was why we chose four Fourier pairs to model the annual season. The *Weekly* frequency we ignore since it's better modeled with indicators.\n\n### Computing Fourier features (optional)\n\nKnowing how Fourier features are computed isn't essential to using them, but if seeing the details would clarify things, the cell hidden cell below illustrates how a set of Fourier features could be derived from the index of a time series. (We'll use a library function from `statsmodels` for our applications, however.)","metadata":{}},{"cell_type":"markdown","source":"Now let's look at the periodogram:","metadata":{}},{"cell_type":"code","source":"if PRODUCTION:\n    plot_periodogram(X[column_y]);","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:39.219451Z","iopub.execute_input":"2022-02-01T10:27:39.220081Z","iopub.status.idle":"2022-02-01T10:27:40.129282Z","shell.execute_reply.started":"2022-02-01T10:27:39.220043Z","shell.execute_reply":"2022-02-01T10:27:40.128592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The periodogram agrees with the seasonal plots above: a strong semiweekly season and a weaker annual season. The weekly season we'll model with indicators and the annual season with Fourier features. From right to left, the periodogram falls off between Bimonthly (6) and Monthly (12), so let's use 10 Fourier pairs.\n\nWe'll create our seasonal features using DeterministicProcess, the same utility we used in Lesson 2 to create trend features. To use two seasonal periods (weekly and annual), we'll need to instantiate one of them as an \"additional term\":","metadata":{}},{"cell_type":"markdown","source":"# Components and Residuals #\n\nSo that we can design effective hybrids, we need a better understanding of how time series are constructed. We've studied up to now three patterns of dependence: trend, seasons, and cycles. Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random *error*:\n\n```\nseries = trend + seasons + cycles + error\n```\n\nEach of the terms in this model we would then call a **component** of the time series.\n\nThe **residuals** of a model are the difference between the target the model was trained on and the predictions the model makes -- the difference between the actual curve and the fitted curve, in other words. Plot the residuals against a feature, and you get the \"left over\" part of the target, or what the model failed to learn about the target from that feature.","metadata":{}},{"cell_type":"code","source":"# annotations: https://stackoverflow.com/a/49238256/5769929\ndef seasonal_plot(X, y, period, freq, ax=None):\n    if ax is None:\n        _, ax = plt.subplots()\n    palette = sns.color_palette(\"husl\", n_colors=X[period].nunique(),)\n    ax = sns.lineplot(\n        x=freq,\n        y=y,\n        hue=period,\n        data=X,\n        ci=False,\n        ax=ax,\n        palette=palette,\n        legend=False,\n    )\n    ax.set_title(f\"Seasonal Plot ({period}/{freq})\")\n    for line, name in zip(ax.lines, X[period].unique()):\n        y_ = line.get_ydata()[-1]\n        ax.annotate(\n            name,\n            xy=(1, y_),\n            xytext=(6, 0),\n            color=line.get_color(),\n            xycoords=ax.get_yaxis_transform(),\n            textcoords=\"offset points\",\n            size=14,\n            va=\"center\",\n        )\n    return ax","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:40.130356Z","iopub.execute_input":"2022-02-01T10:27:40.13066Z","iopub.status.idle":"2022-02-01T10:27:40.138559Z","shell.execute_reply.started":"2022-02-01T10:27:40.130633Z","shell.execute_reply":"2022-02-01T10:27:40.137695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions","metadata":{}},{"cell_type":"code","source":"import matplotlib.dates as mdates\nfrom matplotlib.dates import MONTHLY, WEEKLY, DAILY\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat', period_start='2015-01-01', period_end='2019-12-31'):\n    locator = mdates.AutoDateLocator(minticks=12)\n    locator.maxticks[WEEKLY] = 24\n    locator.maxticks[DAILY] = 24\n    dtFmt = mdates.ConciseDateFormatter(locator)\n    \n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range(period_start, period_end, freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df[column_y] = model.predict(demo_df[features])\n    if PSEUDO_LABEL:\n        demo_df[column_y] *= LOSS_CORRECTION\n    train_subset = X[(X.country == country) & (X.store == store) & (X['product'] == product)].copy()\n    train_subset = train_subset.loc[period_start:period_end]\n    fig, ax = plt.subplots(figsize=(32, 8))\n    plt.plot(demo_df[DATE], demo_df.num_sold, label='prediction', alpha=0.5, color='blue')\n    plt.plot(train_subset.index, train_subset.num_sold, label='true', alpha=0.3, color='red', linestyle='--')\n    plt.scatter(train_subset.index, train_subset.num_sold, label='true', alpha=0.3, color='red', s=2)\n    plt.grid(True)\n    plt.grid(which='major',axis ='y', linestyle=':', linewidth='0.5', color='black')\n    plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n    ax.xaxis.set_major_formatter(dtFmt) # apply the format to the desired axis\n    ax.xaxis.set_major_locator(locator)\n    \n    plt.legend()\n    plt.title(f'{country} {store} {product} Predictions and true from {period_start} to {period_end}')\n    plt.tight_layout()\n    plt.show()\n    return demo_df['num_sold']","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.140336Z","iopub.execute_input":"2022-02-01T10:27:40.14066Z","iopub.status.idle":"2022-02-01T10:27:40.160092Z","shell.execute_reply.started":"2022-02-01T10:27:40.140619Z","shell.execute_reply":"2022-02-01T10:27:40.15922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_true_vs_prediction(df_true, df_hat):\n    plt.figure(figsize=(20, 13))\n    plt.scatter(np.arange(len(df_hat)), np.log1p(df_hat), label='prediction', alpha=0.5, color='blue', s=3) #np.arange(len(df_hat))\n    plt.scatter(np.arange(len(df_true)), np.log1p(df_true), label='Pseudo/true', alpha=0.5, color='red', s=7) #np.arange(len(df_true))\n    plt.legend()\n    plt.title(f'Predictions VS Pseudo-label {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.161652Z","iopub.execute_input":"2022-02-01T10:27:40.162536Z","iopub.status.idle":"2022-02-01T10:27:40.178851Z","shell.execute_reply.started":"2022-02-01T10:27:40.162486Z","shell.execute_reply":"2022-02-01T10:27:40.178173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_residuals(y_residuals):\n    plt.figure(figsize=(13, 3))\n    plt.scatter(np.arange(len(y_residuals)), y_residuals, label='residuals', alpha=0.1, color='blue', s=5)\n    plt.legend()\n    plt.title(f'Linear Model residuals {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.180442Z","iopub.execute_input":"2022-02-01T10:27:40.180959Z","iopub.status.idle":"2022-02-01T10:27:40.198037Z","shell.execute_reply.started":"2022-02-01T10:27:40.180913Z","shell.execute_reply":"2022-02-01T10:27:40.19727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_oof(y_true, y_predict):\n    # Plot y_true vs. y_pred\n    plt.figure(figsize=(5, 5))\n    plt.scatter(y_true, y_predict, s=3, color='r', alpha=0.5)\n#     plt.scatter(np.log1p(y_true), np.log1p(y_predict), s=1, color='g', alpha=0.3)\n    plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n    plt.gca().set_aspect('equal')\n    plt.xlabel('y_true')\n    plt.ylabel('y_pred')\n    plt.title('OOF Predictions')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.199604Z","iopub.execute_input":"2022-02-01T10:27:40.200098Z","iopub.status.idle":"2022-02-01T10:27:40.213759Z","shell.execute_reply.started":"2022-02-01T10:27:40.200052Z","shell.execute_reply":"2022-02-01T10:27:40.212625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_min_SMAPE(y_true, y_predict):\n    loss_correction = 1\n    scores = []\n    # float step\n    for WEIGHT in np.arange(0.988, 1.02, 0.0001):\n        y_hat = y_predict.copy()\n        y_hat *= WEIGHT\n        scores.append(np.array([WEIGHT, np.mean(smape_loss(y_true, y_hat))]))\n        \n    scores = np.vstack(scores)\n    min_SMAPE = np.min(scores[:,1])\n    print(f'min SMAPE {min_SMAPE:.5f}')\n    for x in scores:\n        if x[1] == min_SMAPE:\n            loss_correction = x[0]\n            print(f'loss_correction: {x[0]:.5f}')\n            \n    plt.figure(figsize=(5, 3))\n    plt.plot(scores[:,0],scores[:,1])\n    plt.scatter([loss_correction], [min_SMAPE], color='g')\n    plt.ylabel(f'SMAPE')\n    plt.xlabel(f'loss_correction: {loss_correction:.5f}')\n    plt.legend()\n    plt.title(f'min SMAPE:{min_SMAPE:.5f} scaling')\n    plt.show()\n    \n    return loss_correction","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.215935Z","iopub.execute_input":"2022-02-01T10:27:40.216197Z","iopub.status.idle":"2022-02-01T10:27:40.229806Z","shell.execute_reply.started":"2022-02-01T10:27:40.216168Z","shell.execute_reply":"2022-02-01T10:27:40.22863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_SMAPE(y_va, y_va_pred):\n    loss_correction = 1\n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n    smape = np.mean(smape_loss(y_va, y_va_pred))\n    loss_correction = find_min_SMAPE(y_va, y_va_pred)\n    y_va_pred *= loss_correction\n    print(f\"SMAPE (before correction: {smape_before_correction:.5f})\")\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred))}')\n    return loss_correction","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.23137Z","iopub.execute_input":"2022-02-01T10:27:40.2317Z","iopub.status.idle":"2022-02-01T10:27:40.248922Z","shell.execute_reply.started":"2022-02-01T10:27:40.231656Z","shell.execute_reply":"2022-02-01T10:27:40.247957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, X, y, cv):\n    cv_results = cross_validate(\n        model,\n        X,\n        y,\n        cv=cv,\n        scoring=[\"neg_mean_absolute_error\", \"neg_root_mean_squared_error\"],\n    )\n    mae = -cv_results[\"test_neg_mean_absolute_error\"]\n    rmse = -cv_results[\"test_neg_root_mean_squared_error\"]\n    print(\n        f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n        f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.251176Z","iopub.execute_input":"2022-02-01T10:27:40.252106Z","iopub.status.idle":"2022-02-01T10:27:40.264436Z","shell.execute_reply.started":"2022-02-01T10:27:40.252047Z","shell.execute_reply":"2022-02-01T10:27:40.263622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_selector, ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.model_selection import cross_validate, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn import set_config\nset_config(display='diagram') \n\n# Model 1 (trend)\nfrom pyearth import Earth # https://contrib.scikit-learn.org/py-earth/content.html#api\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge, HuberRegressor, RidgeCV, TheilSenRegressor, SGDRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn.kernel_approximation import Nystroem\n\n# Model 2\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, StackingRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.265798Z","iopub.execute_input":"2022-02-01T10:27:40.266648Z","iopub.status.idle":"2022-02-01T10:27:40.321584Z","shell.execute_reply.started":"2022-02-01T10:27:40.266601Z","shell.execute_reply":"2022-02-01T10:27:40.320686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid Models\nLinear regression excels at extrapolating trends, but can't learn interactions. XGBoost excels at learning interactions, but can't extrapolate trends. We'll learn how to create \"hybrid\" forecasters that combine complementary learning algorithms and let the strengths of one make up for the weakness of the other.","metadata":{}},{"cell_type":"code","source":"# You'll add fit and predict methods to this minimal class\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n\nclass BoostedHybrid(BaseEstimator, RegressorMixin):\n    def __init__(self, model_1, model_2):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.y_columns = None  # store column names from fit method\n    def fit(self, X, y):\n        \"\"\"A reference implementation of a fitting function.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n            The target values (class labels in classification, real numbers in\n            regression).\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        X, y = check_X_y(X, y, accept_sparse=True)\n        # Train model_1\n        self.model_1.fit(X, y)\n\n        # Make predictions\n        y_fit = self.model_1.predict(X)\n        # Compute residuals\n        y_resid = y - y_fit\n\n        # Train model_2 on residuals , eval_set=[(X_1_valid, y_valid_resid)]\n        self.model_2.fit(X, y_resid)\n        # Model2 prediction\n        y_fit2 = self.model_2.predict(X)\n        # Compute noise\n        y_resid2 = y_resid - y_fit2\n        \n        # Save data for question checking\n        self.y = y\n        self.y_fit = y_fit\n        self.y_resid = y_resid\n        self.y_fit2 = y_fit2\n        self.y_resid2 = y_resid2\n\n        self.is_fitted_ = True\n        return self\n    \n    def predict(self, X):\n        \"\"\" A reference implementation of a predicting function.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            The training input samples.\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            Returns an array of ones.\n        \"\"\"\n        X = check_array(X, accept_sparse=True)\n        check_is_fitted(self, 'is_fitted_')\n        # Predict with model_1\n        y_predict = self.model_1.predict(X)\n        # Add model_2 predictions to model_1 predictions\n        y_predict += self.model_2.predict(X)\n\n        return y_predict","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.322969Z","iopub.execute_input":"2022-02-01T10:27:40.323279Z","iopub.status.idle":"2022-02-01T10:27:40.335985Z","shell.execute_reply.started":"2022-02-01T10:27:40.32324Z","shell.execute_reply":"2022-02-01T10:27:40.335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.graphics.gofplots import qqplot\ndef model_fit_eval(hybrid_model, X_train, y_train, X_valid, y_valid, X_test, df_test, loss_correction):\n#     test_pred_list = []\n    # Boosted Hybrid\n    hybrid_model.fit(X_train, y_train) #, X_valid, y_valid\n    \n    loss_correction = 1\n    ###### Preprocess the validation data\n    y_va = y_valid.copy()\n    # Inference for validation\n    y_va_pred = hybrid_model.predict(X_valid)\n    print(f'***********Validation Data*****************')\n    loss_correction = evaluate_SMAPE(y_va, y_va_pred)\n    \n    ###### Validate against 2019 PSEU #######\n    loss_correction = 1\n    ###### Preprocess the test data\n    y_test = df_test[column_y].values.reshape(-1, 1)\n    # Inference test 2019 for validation\n    y_test_prediction = hybrid_model.predict(X_test[features])\n    # Evaluation: SMAPE\n    print(f'***********Test Data*****************')\n    loss_correction = evaluate_SMAPE(y_test, y_test_prediction.reshape(-1, 1))\n    ### Mean test prediction ###\n#     test_pred_list.append(y_test_prediction)\n    \n    ###### Validation dataset\n    ###### Visualize and evaluate\n    print(f'***********Validation Data*****************')\n    plot_oof(y_va, y_va_pred)\n    plot_true_vs_prediction(y_va, y_va_pred)\n    print(f'***********Test Data*****************')\n    plot_oof(y_test, y_test_prediction)\n    plot_true_vs_prediction(y_test, y_test_prediction)\n    # Model_1 residual\n    plot_residuals(hybrid_model.regressor_[1].estimators_[0].y_resid)\n    qqplot(np.expm1(hybrid_model.regressor_[1].estimators_[0].y_resid))\n    # Model_2 residual. Best case is normal gaussian noise\n    plot_residuals(hybrid_model.regressor_[1].estimators_[0].y_resid2)\n    qqplot(np.expm1(hybrid_model.regressor_[1].estimators_[0].y_resid2))\n    \n    return hybrid_model, y_test_prediction, loss_correction","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.337293Z","iopub.execute_input":"2022-02-01T10:27:40.337501Z","iopub.status.idle":"2022-02-01T10:27:40.431744Z","shell.execute_reply.started":"2022-02-01T10:27:40.337476Z","shell.execute_reply":"2022-02-01T10:27:40.430722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ts_cv = TimeSeriesSplit(n_splits=FOLDS)\n# alphas = np.logspace(-6, 6, 25)\n# naive_linear_pipeline = make_pipeline(\n#     ColumnTransformer(\n#         transformers=[\n#             (\"categorical\", one_hot_encoder, make_column_selector(dtype_include=object)),\n#             (\"numeric\", StandardScaler(), make_column_selector(dtype_include=np.number)),\n#         ],\n#         remainder='passthrough',\n#     ),\n#     RidgeCV(),\n# )\n# evaluate(naive_linear_pipeline, X_2, np.log1p(y), cv=ts_cv)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-01T10:27:40.433005Z","iopub.execute_input":"2022-02-01T10:27:40.433294Z","iopub.status.idle":"2022-02-01T10:27:40.438078Z","shell.execute_reply.started":"2022-02-01T10:27:40.433261Z","shell.execute_reply":"2022-02-01T10:27:40.437202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacker\n- models_1(List): Any linear model\n- models_2(List): Tree based model. Fitting model_1 residual.","metadata":{}},{"cell_type":"code","source":"%%time\ngc.collect()\n# pseudohubererror squarederror\n\ndef build_estimator_stack(estimator_stack, seed=RANDOM_STATE):\n    if GPU:\n        param_xgb = {\n                    'objective' : 'reg:squarederror',\n                    'tree_method' : 'gpu_hist',\n                    'learning_rate': 0.12,\n                    'max_depth': 4,\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed\n                 }\n        param_cat = {\n                    'loss_function' : 'RMSE', # SMAPE RMSE Huber\n                    'eval_metric': 'RMSE',\n                    'task_type' : 'GPU',            \n                    'learning_rate': 0.13,\n                    'max_depth': 4,\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed,\n                    'verbose': VERBOSE\n                 }\n        param_lgb = {\n                    'objective' : 'regression',\n                    'max_depth': 4,\n                    'n_estimators': N_ESTIMATORS,\n                    'device' : 'gpu',\n                    'random_state': seed\n                 }\n    else: #CPU\n        param_xgb = {\n                    'objective' : 'reg:squarederror',\n                    'tree_method' : 'hist',\n                    'learning_rate': 0.12,\n                    'max_depth': 4,\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed\n                 }\n        param_cat = {\n                    'loss_function' : 'RMSE',\n                    'eval_metric': 'RMSE',\n                    'learning_rate': 0.13,\n                    'max_depth': 4,\n                    'n_estimators': N_ESTIMATORS,\n#                     'iterations': 700,\n#                     'od_type' : 'Iter',\n#                     'od_wait' : 20,\n                    'random_state': seed,\n                    'verbose': VERBOSE\n                 }\n        param_lgb = {\n                    'objective' : 'regression',\n                    'max_depth': 4,\n                    'n_estimators': N_ESTIMATORS,\n                    'random_state': seed\n                 }\n    if PRODUCTION:\n        # Linear estimator. Try different combinations of the algorithms above KNeighborsRegressor fit_intercept=False\n        models_1 = [\n                    Ridge(fit_intercept=False, random_state=seed),\n                    ElasticNet(fit_intercept=False, random_state=seed),\n                    HuberRegressor(fit_intercept=False, epsilon=1.20, max_iter=1300),\n                   ]\n        # Residue estimator\n        models_2 = [\n                    XGBRegressor(**param_xgb),\n                    lgb.LGBMRegressor(**param_lgb),\n                    CatBoostRegressor(**param_cat),\n                   ]\n    else: # Trial run\n        # Linear estimator. Try different combinations of the algorithms above KNeighborsRegressor. Remove bias fit_intercept=False\n        models_1 = [\n#                     Ridge(fit_intercept=False, random_state=seed),\n#                     ElasticNet(fit_intercept=False, random_state=seed),\n#                     LinearRegression(fit_intercept=True),\n#                     SGDRegressor(fit_intercept=False, random_state=seed),\n#                     Lasso(fit_intercept=False, random_state=seed),\n#                     TheilSenRegressor(fit_intercept=False, random_state=seed),\n                    HuberRegressor(fit_intercept=False, epsilon=1.20, max_iter=1300),\n#                     Earth(verbose=VERBOSE),\n#                     MLPRegressor(   hidden_layer_sizes=HIDDEN_LAYERS,\n#                                     learning_rate_init=0.01,\n#                                     learning_rate='adaptive',\n#                                     early_stopping=True,\n#                                     max_iter=EPOCHS,\n#                                     random_state=seed,\n#                                     ),\n\n                   ]\n        # Residue estimator\n        models_2 = [\n                    CatBoostRegressor(**param_cat),\n#                     lgb.LGBMRegressor(**param_lgb),\n#                     XGBRegressor(**param_xgb),\n                   ]\n\n    for model_1 in models_1:\n        for model_2 in models_2:\n            model1_name = type(model_1).__name__\n            model2_name = type(model_2).__name__\n            hybrid_model = BoostedHybrid(\n                    model_1 = model_1,\n                    model_2 = model_2\n                            )\n            print(f'******************Stacking {model1_name:>16} with {model2_name:<18}*************************')\n            estimator_stack.append((f'model_{model1_name}_{model2_name}', hybrid_model))\n    return estimator_stack","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.439272Z","iopub.execute_input":"2022-02-01T10:27:40.439695Z","iopub.status.idle":"2022-02-01T10:27:40.605473Z","shell.execute_reply.started":"2022-02-01T10:27:40.439664Z","shell.execute_reply":"2022-02-01T10:27:40.604444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pipeline MasterClass","metadata":{}},{"cell_type":"code","source":"# tscv = TimeSeriesSplit(n_splits=FOLDS) # cv=tscv , n_jobs=-1\n\ndef build_stacking_regressor(estimator_stack, seed=RANDOM_STATE):\n    # X pipeline\n    stacking_regressor = make_pipeline(\n        ColumnTransformer(\n            transformers=[\n                (\"categorical\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), make_column_selector(dtype_include=object)),\n#                 (\"cyclic_year\", periodic_spline_transformer(365, n_splines=9, degree=2), [DAYOFYEAR]),\n#                 (\"cyclic_weekofyear\", periodic_spline_transformer(52, n_splines=7, degree=2), [WEEKOFYEAR]),\n#                 (\"cyclic_month\", periodic_spline_transformer(12, n_splines=6, degree=3), [MONTH]),\n#                 (\"cyclic_weekday\", periodic_spline_transformer(7, n_splines=3, degree=2), [WEEKDAY]),\n#                 (\"numeric\", MinMaxScaler(), make_column_selector(dtype_include=np.number)),\n            ],\n            remainder=MinMaxScaler(), #'passthrough', #\n        ),\n#         Nystroem(kernel=\"poly\", degree=2, n_components=300, random_state=seed),\n        StackingRegressor(estimators=estimator_stack, final_estimator=RidgeCV(), cv=FOLDS, n_jobs=-1, verbose=VERBOSE),\n    )\n    # X y pipeline with y log transform\n    model = TransformedTargetRegressor(\n        regressor=stacking_regressor, func=np.log1p, inverse_func=np.expm1, check_inverse=False\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.607039Z","iopub.execute_input":"2022-02-01T10:27:40.607359Z","iopub.status.idle":"2022-02-01T10:27:40.620928Z","shell.execute_reply.started":"2022-02-01T10:27:40.607327Z","shell.execute_reply":"2022-02-01T10:27:40.620184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data splitting X_2 X_test y","metadata":{}},{"cell_type":"markdown","source":"2018 as test dataset if not production","metadata":{}},{"cell_type":"code","source":"def get_Xy(X):\n    # Target series\n    y = X.loc[:, column_y]\n    X_2 = X.drop(column_y, axis=1)\n\n    features = X_2.columns\n\n    if PSEUDO_LABEL:\n        TRAIN_START_DATE = \"2015-01-01\"\n        TRAIN_END_DATE = \"2019-12-31\"\n        VALID_START_DATE = \"2015-01-01\"\n        VALID_END_DATE = \"2018-12-31\"\n    else:\n        if PRODUCTION:\n            TRAIN_START_DATE = \"2015-01-01\"\n            TRAIN_END_DATE = \"2018-12-31\"\n            VALID_START_DATE = \"2015-01-01\"\n            VALID_END_DATE = \"2018-12-31\"\n        else: # 2018 Validation\n            TRAIN_START_DATE = \"2015-01-01\"\n            TRAIN_END_DATE = \"2017-12-31\"\n            VALID_START_DATE = \"2018-01-01\"\n            VALID_END_DATE = \"2018-12-31\"\n\n    y_train, y_valid = y[TRAIN_START_DATE:TRAIN_END_DATE], y[VALID_START_DATE:VALID_END_DATE]\n    X2_train, X2_valid = X_2.loc[TRAIN_START_DATE:TRAIN_END_DATE], X_2.loc[VALID_START_DATE:VALID_END_DATE]\n    return y, y_train, y_valid, X2_train, X2_valid, features","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:27:40.622077Z","iopub.execute_input":"2022-02-01T10:27:40.622898Z","iopub.status.idle":"2022-02-01T10:27:40.637853Z","shell.execute_reply.started":"2022-02-01T10:27:40.622858Z","shell.execute_reply":"2022-02-01T10:27:40.637178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"### Product channel test\n\n- Min SMAPE: 3.9165618565465845 \n- month week day- linear 4.0052295549526225 \n- month week day+ linear 3.9153803500063686 \n- WEEKDAY+ 3.91795141135476\n- GDP exp: Min SMAPE: 3.9835975216489867","metadata":{}},{"cell_type":"code","source":"%%time\nPRODUCT=False\n# PRODUCT='Kaggle Sticker'\n# X = X.loc[X['product'] == PRODUCT]\n# X_test = X_test.loc[X_test['product'] == PRODUCT]\n# train_df.loc[train_df['product'] == 'Kaggle Mug']\n\n# y_test to use\nif PRODUCT:\n    df_2019 = train_df.loc[(train_df['product'] == PRODUCT) & (train_df[DATE] >= pd.to_datetime(date(2019, 1, 1)))]\nelse:\n    df_2019 = df_pseudolabels\n\ntest_prediction_list=[]\nfor seed in range(SEED_START, (SEED_START+REPEAT), 1):\n    estimator_stack = []\n    y, y_train, y_valid, X2_train, X2_valid, features = get_Xy(X)\n    estimator_stack = build_estimator_stack(estimator_stack=estimator_stack, seed=seed)\n    stacking_regressor = build_stacking_regressor(estimator_stack, seed=seed)\n    print(f'****************** Run using seed: {seed} ******************')\n    model, y_test_prediction, LOSS_CORRECTION = model_fit_eval(stacking_regressor, X2_train, y_train, X2_valid, y_valid, X_test, df_2019, LOSS_CORRECTION)\n    test_prediction_list.append(y_test_prediction)\nmodel","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-02-01T10:27:40.638882Z","iopub.execute_input":"2022-02-01T10:27:40.639512Z","iopub.status.idle":"2022-02-01T10:37:19.751295Z","shell.execute_reply.started":"2022-02-01T10:27:40.63948Z","shell.execute_reply":"2022-02-01T10:37:19.750342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Debug","metadata":{}},{"cell_type":"code","source":"for ptr in holidays.Norway(years = [2018], observed=True).items():\n    print(ptr)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:19.752952Z","iopub.execute_input":"2022-02-01T10:37:19.753222Z","iopub.status.idle":"2022-02-01T10:37:19.761571Z","shell.execute_reply.started":"2022-02-01T10:37:19.75318Z","shell.execute_reply":"2022-02-01T10:37:19.760646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Debug\nfor country in np.unique(train_df['country']):\n    for product in np.unique(train_df['product']):\n        for store in np.unique(train_df['store']):\n            y_fit = plot_five_years_combination(feature_engineer, country=country, product=product, store=store,period_start='2018-2-01', period_end='2018-2-28')\n            break\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:19.762921Z","iopub.execute_input":"2022-02-01T10:37:19.763186Z","iopub.status.idle":"2022-02-01T10:37:23.326747Z","shell.execute_reply.started":"2022-02-01T10:37:19.763155Z","shell.execute_reply":"2022-02-01T10:37:23.323368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference year 2019 test data","metadata":{}},{"cell_type":"markdown","source":"# Inference validation","metadata":{}},{"cell_type":"code","source":"for country in np.unique(train_df['country']):\n    for product in np.unique(train_df['product']):\n        for store in np.unique(train_df['store']):\n            y_fit = plot_five_years_combination(feature_engineer, country=country, product=product, store=store)\n            break","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:23.327957Z","iopub.execute_input":"2022-02-01T10:37:23.328203Z","iopub.status.idle":"2022-02-01T10:37:43.086074Z","shell.execute_reply.started":"2022-02-01T10:37:23.328176Z","shell.execute_reply":"2022-02-01T10:37:43.085187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n","metadata":{}},{"cell_type":"code","source":"len(test_prediction_list)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:43.087435Z","iopub.execute_input":"2022-02-01T10:37:43.087716Z","iopub.status.idle":"2022-02-01T10:37:43.094286Z","shell.execute_reply.started":"2022-02-01T10:37:43.08768Z","shell.execute_reply":"2022-02-01T10:37:43.093424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:43.095757Z","iopub.execute_input":"2022-02-01T10:37:43.096036Z","iopub.status.idle":"2022-02-01T10:37:43.118052Z","shell.execute_reply.started":"2022-02-01T10:37:43.095998Z","shell.execute_reply":"2022-02-01T10:37:43.117164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mean vs Median","metadata":{}},{"cell_type":"code","source":"if BLEND:\n    test_prediction_list.append(df_pseudolabels[column_y].values) #blender 1\n    df_pseudolabels1 = pd.read_csv(PSEUDO_DIR2, index_col=ID)\n    test_prediction_list.append(df_pseudolabels1[column_y].values) #blender 2\ntest_prediction_list_median = np.median(test_prediction_list, axis=0) # median is better https://www.kaggle.com/saraswatitiwari/tabular-playground-series-22\ntest_prediction_list_mean = np.mean(test_prediction_list, axis=0) # median is better https://www.kaggle.com/saraswatitiwari/tabular-playground-series-22\n###### Validate against 2019 PSEU #######\nloss_correction = 1\n###### Preprocess the test data\ny_test = df_2019[column_y].values.reshape(-1, 1)\nprint(f'*********** Median *****************')\nmedian_correction = evaluate_SMAPE(y_test, test_prediction_list_median.reshape(-1, 1))\nprint(f'*********** Mean *****************')\nmean_correction = evaluate_SMAPE(y_test, test_prediction_list_mean.reshape(-1, 1))\n\ntest_prediction = (test_prediction_list_mean*mean_correction) if (np.abs(1.-mean_correction) <= np.abs(1.-median_correction)) else (test_prediction_list_median*median_correction)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:43.119871Z","iopub.execute_input":"2022-02-01T10:37:43.120216Z","iopub.status.idle":"2022-02-01T10:37:43.859767Z","shell.execute_reply.started":"2022-02-01T10:37:43.120174Z","shell.execute_reply":"2022-02-01T10:37:43.858949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif len(test_prediction) > 0:\n    # Create the submission file\n    submission = pd.DataFrame(data=np.zeros((sub.shape[0],2)),index = sub.index.tolist(),columns=[ID,column_y])\n    submission[ID] = sub[ID]\n    submission[column_y] = test_prediction\n    submission.to_csv('pseudo_labels_v1.csv', index=False)\n    # round\n    submission[column_y] = geometric_round(submission[column_y]).astype(int) #https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/299162\n    submission.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df[column_y], bins=np.linspace(0, 3000, 201),\n             density=True, label='Training')\n    plt.hist(submission[column_y], bins=np.linspace(0, 3000, 201),\n             density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel(column_y)\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:43.860828Z","iopub.execute_input":"2022-02-01T10:37:43.861027Z","iopub.status.idle":"2022-02-01T10:37:45.077104Z","shell.execute_reply.started":"2022-02-01T10:37:43.861002Z","shell.execute_reply":"2022-02-01T10:37:45.076187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(submission.head(30))\ndisplay(submission.tail(30))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:45.078295Z","iopub.execute_input":"2022-02-01T10:37:45.078534Z","iopub.status.idle":"2022-02-01T10:37:45.095978Z","shell.execute_reply.started":"2022-02-01T10:37:45.078501Z","shell.execute_reply":"2022-02-01T10:37:45.095199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[column_y].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:45.097221Z","iopub.execute_input":"2022-02-01T10:37:45.099311Z","iopub.status.idle":"2022-02-01T10:37:45.112737Z","shell.execute_reply.started":"2022-02-01T10:37:45.099256Z","shell.execute_reply":"2022-02-01T10:37:45.112058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pseudolabels[column_y].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:37:45.113689Z","iopub.execute_input":"2022-02-01T10:37:45.114269Z","iopub.status.idle":"2022-02-01T10:37:45.125231Z","shell.execute_reply.started":"2022-02-01T10:37:45.114225Z","shell.execute_reply":"2022-02-01T10:37:45.124427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Variance\nVariance = Ïƒ2=Î£(xiâˆ’Î¼)**2/n","metadata":{}},{"cell_type":"markdown","source":"To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https://www.kaggle.com/c/tabular-playground-series-jan-2022/code) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","metadata":{}},{"cell_type":"code","source":"for x in X.columns:\n    print(x)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-01T10:37:45.126388Z","iopub.execute_input":"2022-02-01T10:37:45.127016Z","iopub.status.idle":"2022-02-01T10:37:45.157944Z","shell.execute_reply.started":"2022-02-01T10:37:45.126981Z","shell.execute_reply":"2022-02-01T10:37:45.156913Z"},"trusted":true},"execution_count":null,"outputs":[]}]}