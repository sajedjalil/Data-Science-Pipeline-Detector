{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import os\nimport math as m\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read train / test data.\ntrain = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:27.948196Z","iopub.execute_input":"2022-01-26T05:39:27.948559Z","iopub.status.idle":"2022-01-26T05:39:27.99496Z","shell.execute_reply.started":"2022-01-26T05:39:27.948526Z","shell.execute_reply":"2022-01-26T05:39:27.993975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport dateutil.easter as easter\nimport datetime\nimport itertools\nimport operator\n\n# A - Add year/month/day column\ntrain['date'] = pd.to_datetime(train['date'], format='%Y-%m-%d')\ntest['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n\ntrain['year'] = train['date'].apply(lambda x: x.year)\ntrain['month'] = train['date'].apply(lambda x: x.month)\ntrain['day'] = train['date'].apply(lambda x: x.day)\n\ntest['year'] = test['date'].apply(lambda x: x.year)\ntest['month'] = test['date'].apply(lambda x: x.month)\ntest['day'] = test['date'].apply(lambda x: x.day)\n\n# B - Add weekday column\ntrain['weekday'] = train['date'].apply(lambda x: x.weekday())\ntest['weekday'] = test['date'].apply(lambda x: x.weekday())\nweekday_label = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n# C - Add GDP per capita column\nGDP_PC = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv', index_col='year')\nGDP_PC_dict = GDP_PC.unstack().to_dict()\ntrain['GDP_PC'] = train.set_index(['country', 'year']).index.map(GDP_PC_dict)\ntest['GDP_PC'] = test.set_index(['country', 'year']).index.map(GDP_PC_dict)\n\n# D - Add snow depth column\nsnow = pd.read_csv('../input/finland-norway-and-sweden-weather-data-20152019/nordics_weather.csv')\nsnow['date'] = pd.to_datetime(snow['date'], format='%m/%d/%Y')\nsnow['year'] = snow['date'].apply(lambda x: x.year)\nsnow['month'] = snow['date'].apply(lambda x: x.month)\nsnow['day'] = snow['date'].apply(lambda x: x.day)\nsnow = snow[['country', 'year', 'month', 'day', 'snow_depth']]\n\nsnow_map = snow.set_index(['country', 'year', 'month', 'day']).to_dict()['snow_depth']\n\ntrain['snow_depth'] = train.set_index(['country', 'year', 'month', 'day']).index.map(snow_map)\ntest['snow_depth'] = test.set_index(['country', 'year', 'month', 'day']).index.map(snow_map)\n\n# E - Holiday (Samuel Cortinhas)\ntrain['holiday'] = 0\nholiday = pd.read_csv('../input/public-and-unofficial-holidays-nor-fin-swe-201519/holidays.csv')\nholiday['date'] = pd.to_datetime(holiday['date'], format='%Y-%m-%d')\n\n## Add Divine Mercy Sunday\nholiday_dms = holiday[holiday['event']=='Easter Sunday']\nholiday_dms['event'] = 'Divine Mercy Sunday'\nholiday_dms['date'] = holiday_dms['date'] + datetime.timedelta(days=7)\nholiday = pd.concat([holiday, holiday_dms], axis=0)\n\n## add year, month, day\nholiday['year'] = holiday['date'].apply(lambda x: x.year)\nholiday['month'] = holiday['date'].apply(lambda x: x.month)\nholiday['day'] = holiday['date'].apply(lambda x: x.day)\n\nfor df in [train, test]:\n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n# F - Fourier (Samuel Cortinhas)\nfor df in [train, test]:\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 2):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * m.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * m.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n# drop temporary one hot encoding\ntrain = train.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)\ntest = test.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:27.997485Z","iopub.execute_input":"2022-01-26T05:39:27.997995Z","iopub.status.idle":"2022-01-26T05:39:29.710537Z","shell.execute_reply.started":"2022-01-26T05:39:27.997949Z","shell.execute_reply":"2022-01-26T05:39:29.709563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization\n## Total sales by date","metadata":{}},{"cell_type":"code","source":"series = train.groupby(['year', 'date']).num_sold.sum()\n\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2016, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year].index, series[year].values)\n    fig.show()\n    ax.set_title(f'Sales trend in {year}', fontsize=8)\n    ax.set_xlabel('Date', fontsize=8)\n    ax.set_ylabel('Sales', fontsize=8)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:29.712156Z","iopub.execute_input":"2022-01-26T05:39:29.712721Z","iopub.status.idle":"2022-01-26T05:39:30.606842Z","shell.execute_reply.started":"2022-01-26T05:39:29.712674Z","shell.execute_reply":"2022-01-26T05:39:30.605596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Cyclical movements are influenced by the day of the week.\n- Sales peak at the end of December.\n- There is also a peak in April.\n\n**need to add weekday column**\n\n## Total sales by products.","metadata":{}},{"cell_type":"code","source":"series = train.groupby(['year', 'product', 'month']).num_sold.sum()\n\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2016, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year]['Kaggle Hat'].index,\n            series[year]['Kaggle Hat'].values,\n            label='Kaggle Hat')\n    ax.plot(series[year]['Kaggle Mug'].index,\n            series[year]['Kaggle Mug'].values,\n            label='Kaggle Mug')\n    ax.plot(series[year]['Kaggle Sticker'].index,\n            series[year]['Kaggle Sticker'].values,\n            label='Kaggle Sticker')\n    ax.set_title(f'Sales trend in {year}', fontsize=8)\n    ax.set_xlabel('Month', fontsize=8)\n    ax.set_ylabel('Sales', fontsize=8)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:30.609593Z","iopub.execute_input":"2022-01-26T05:39:30.612439Z","iopub.status.idle":"2022-01-26T05:39:31.728031Z","shell.execute_reply.started":"2022-01-26T05:39:30.612389Z","shell.execute_reply":"2022-01-26T05:39:31.727126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Only Hat has peak in April.\n\n**Might need to add some kind of feature, but I'm not sure...**\n\n-> is it effected Divine Mercy Sunday?\n\n## GDP per capita","metadata":{}},{"cell_type":"code","source":"plt.plot(GDP_PC.index, GDP_PC['Finland'].values, label='Finland')\nplt.plot(GDP_PC.index, GDP_PC['Norway'].values, label='Norway')\nplt.plot(GDP_PC.index, GDP_PC['Sweden'].values, label='Sweden')\nplt.xticks([2015, 2016, 2017, 2018, 2019])\nplt.xlabel('year')\nplt.ylabel('GDP per capita')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:31.729919Z","iopub.execute_input":"2022-01-26T05:39:31.730536Z","iopub.status.idle":"2022-01-26T05:39:31.977255Z","shell.execute_reply.started":"2022-01-26T05:39:31.730492Z","shell.execute_reply":"2022-01-26T05:39:31.976278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- GDP must be related to sales, and data in 2019 is important to predict.\n- I think the impact of Covid-19 will also show up in GDP, but I don't know with annual data through 2019.\n\n## Snow depth","metadata":{}},{"cell_type":"code","source":"series = snow.groupby(['year', 'country', 'month']).snow_depth.sum()\n\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2016, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year]['Norway'].index, series[year]['Norway'].values, label='Norway')\n    ax.plot(series[year]['Sweden'].index, series[year]['Sweden'].values, label='Sweden')\n    ax.plot(series[year]['Finland'].index, series[year]['Finland'].values, label='Finland')\n    # ax.set_title(f'Sales trend in {year}', fontsize=8, loc='right')\n    ax.set_xlabel('Day', fontsize=8)\n    ax.set_ylabel('Snow depth', fontsize=8)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:31.979006Z","iopub.execute_input":"2022-01-26T05:39:31.979551Z","iopub.status.idle":"2022-01-26T05:39:33.066009Z","shell.execute_reply.started":"2022-01-26T05:39:31.9795Z","shell.execute_reply":"2022-01-26T05:39:33.06514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- This feature might be related to Hat sales, but, really?\n\n## Holiday in April","metadata":{}},{"cell_type":"code","source":"for year in [2015, 2016, 2017, 2018]:\n    month = 4\n    print(fin_holiday[(fin_holiday['year']==year) & (fin_holiday['month']==month)][['date', 'country', 'event', 'type']])\n    print('----')\n    print(swe_holiday[(swe_holiday['year']==year) & (swe_holiday['month']==month)][['date', 'country', 'event', 'type']])\n    print('----')\n    print(nor_holiday[(nor_holiday['year']==year) & (nor_holiday['month']==month)][['date', 'country', 'event', 'type']])\n    \n    series = train.groupby(['year', 'country', 'month', 'date']).num_sold.sum()\n    \n    fig = plt.figure(figsize=(20, 5))\n    for i, country in enumerate(['Finland', 'Sweden', 'Norway']):\n        ax = fig.add_subplot(1, 3, i+1)\n        ax.plot(series[year][country][month].index, series[year][country][month].values)\n        fig.show()\n        ax.set_title(f'Sales trend in {country}', fontsize=12)\n        ax.set_xlabel('Date', fontsize=8)\n        ax.set_ylabel('Sales', fontsize=8)\n        plt.xticks(fontsize=6)\n        plt.yticks(fontsize=8)\n        plt.legend(fontsize=8)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:33.06783Z","iopub.execute_input":"2022-01-26T05:39:33.068458Z","iopub.status.idle":"2022-01-26T05:39:35.749157Z","shell.execute_reply.started":"2022-01-26T05:39:33.068413Z","shell.execute_reply":"2022-01-26T05:39:35.748161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Divine Mercy Sunday** is as effective as Easter sunday.\n\n**We need to add Divine Mercy Sunday column**","metadata":{}},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"train_test_concat = pd.concat([train, test])\n\nfor c in ['country', 'store', 'product']:\n    # Determine how to transform the data based on the training data.\n    le = LabelEncoder()\n\n    # Transform\n    train_test_concat[c] = le.fit_transform(train_test_concat[c].fillna('NA'))\n\ntrain = train_test_concat[train_test_concat['row_id'].isin(train['row_id'])]\ntest = train_test_concat[train_test_concat['row_id'].isin(test['row_id'])]\n\n# Delete unuse columns.\n# Separate training data and target variables.\nresponse_label = 'num_sold'\ntrain_y = train[response_label]\ndrop_label = ['row_id', 'date', response_label]\ntrain_x = train.drop([response_label, *drop_label], axis=1)\ntest_x = test.drop([*drop_label], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:39:35.750797Z","iopub.execute_input":"2022-01-26T05:39:35.751671Z","iopub.status.idle":"2022-01-26T05:39:35.816984Z","shell.execute_reply.started":"2022-01-26T05:39:35.75162Z","shell.execute_reply":"2022-01-26T05:39:35.816023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n# import optuna.integration.lightgbm as lgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nimport seaborn as sns\nfrom rgf.sklearn import RGFRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n\n# Define sMAPE func.\ndef smape(true, preds):\n    return 1/len(true) * np.sum(2 * np.abs(preds-true) / (np.abs(true) + np.abs(preds)) * 100)\n\n\nlgbm_params = {\n            'task': 'train',\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'force_col_wise': True,\n            'metric': 'mape'\n}\n\nkeras_params = {'epochs': 400,\n                'batch_size': 512}\n\net_params = {'n_estimators': 100,\n             'max_features': 0.5,\n             'max_depth': 18,\n             'min_samples_leaf': 4,\n             'n_jobs': -1}\n\nrf_params = {'n_estimators': 125,\n             'max_features': 0.2,\n             'max_depth': 25,\n             'min_samples_leaf': 4,\n             'n_jobs': -1}\n\nrgf_params = {'algorithm': 'RGF_Sib',\n              'loss': 'Log'}\n\n\ndef build_fn():\n    model_nn = Sequential()\n    model_nn.add(Dense(64, input_dim=train_x.shape[1]))\n    model_nn.add(BatchNormalization())\n    model_nn.add(Activation('selu'))\n    model_nn.add(Dense(128))\n    model_nn.add(BatchNormalization())\n    model_nn.add(Dropout(0.2))\n    model_nn.add(Activation('selu'))\n    model_nn.add(Dense(64, activation='selu'))\n    model_nn.add(Dense(1))\n\n    model_nn.compile(loss='mean_absolute_percentage_error',\n                     optimizer='adam',\n                     metrics=['MeanAbsolutePercentageError'])\n\n    return model_nn\n\n\nkeras_reg = KerasRegressor(build_fn=build_fn, verbose=0, **keras_params)\nkeras_reg._estimator_type = 'regressor'\n\nestimators = [\n    ('lgb', lgb.LGBMRegressor(**lgbm_params)),\n    ('nn', keras_reg),\n    ('rgf', RGFRegressor(**rgf_params)),\n    ('et', ExtraTreesRegressor(**et_params)),\n    ('rf', RandomForestRegressor(**rf_params)),\n    # ('lr', LinearRegression()),\n    ('knn', KNeighborsRegressor())\n]\n\nmodel_stack = StackingRegressor(estimators=estimators,\n                                final_estimator=LinearRegression())\nmodel_stack.fit(train_x, train_y)\npred = model_stack.predict(test_x)\n\n# Cross validation.\n# Split train data into 4 folds.\n#score_folds = []\n#kf = KFold(n_splits=4, shuffle=True, random_state=71)\n#for tr_idx, va_idx in kf.split(train_x):\n#    # Separate training data and validation data.\n#    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n#    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n#\n#    # Convert feature values and objective variables to lightgbm data structure.\n#    lgb_train = lgb.Dataset(tr_x, tr_y)\n#    lgb_eval = lgb.Dataset(va_x, va_y)\n#\n#    #  Hyper param setting.\n#    params = {\n#        'task': 'train',\n#        'boosting_type': 'gbdt',\n#        'objective': 'regression',\n#        'force_col_wise': True,\n#        'metric': 'mape'\n#    }\n#    \n#    num_round = 8000\n#\n#    # Train\n#    model = lgb.train(params,\n#                      train_set=lgb_train,\n#                      num_boost_round=num_round,\n#                      early_stopping_rounds=1000,\n#                      verbose_eval=100,\n#                      valid_sets=lgb_eval)\n#\n#    # Score confirmation with validation data.\n#    va_pred = model.predict(va_x)\n#    mape = smape(va_y, va_pred)\n#    score_folds.append(mape)\n#    best_params_folds.append(model.params)\n#\n#print(score_folds)\n#pred = model.predict(test_x)\ntest['num_sold'] = pred\n\nplt.plot(train['date'], train_y)\nplt.plot(test['date'], pred)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- I think prediction data has enough feature of previous year.\n- But, I feel like it's missing the peak of April.\n- Covid-19 spread from 2019, but we may need to dig a little deeper into the impact of this.\n- For example, the number of infected people and monthly GDP.","metadata":{}},{"cell_type":"markdown","source":"# Output","metadata":{}},{"cell_type":"code","source":"# Make submission file\nsubmission = pd.DataFrame({'row_id': test['row_id'],\n                           'num_sold': pred.astype('int')})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:47:42.86222Z","iopub.status.idle":"2022-01-26T05:47:42.863068Z","shell.execute_reply.started":"2022-01-26T05:47:42.862747Z","shell.execute_reply":"2022-01-26T05:47:42.862778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Re-Visualization\n## Comparison of each year sales and predicted sales","metadata":{}},{"cell_type":"code","source":"series = train.groupby(['year', 'date']).num_sold.sum()\nseries_test = test.groupby(['date']).num_sold.sum()\nfig = plt.figure(figsize=(30, 13))\nfor i, year in enumerate([2015, 2017, 2018]):\n    ax = fig.add_subplot(2, 2, i+1)\n    ax.plot(series[year].index, series[year].values, label='train')\n    ax.plot(series[year].index, series_test.values, label='test')\n    fig.show()\n    ax.set_title(f'Sales trend in {year}', fontsize=12)\n    ax.set_xlabel('Date', fontsize=8)\n    ax.set_ylabel('Sales', fontsize=8)\n    plt.xticks(fontsize=4)\n    plt.yticks(fontsize=8)\n    plt.legend(fontsize=8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:47:42.8654Z","iopub.status.idle":"2022-01-26T05:47:42.86596Z","shell.execute_reply.started":"2022-01-26T05:47:42.86563Z","shell.execute_reply":"2022-01-26T05:47:42.865659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Prediction data has enough feature of previous years.\n- But, end-of-year sales has some amount of fluctuation each year.\n\n**Can we add some feature that related to end-of-year sales?**","metadata":{}},{"cell_type":"code","source":"series = train.groupby(['year', 'country', 'product', 'date']).num_sold.sum()\nseries_test = test.groupby(['country', 'product', 'date']).num_sold.sum()\nfig = plt.figure(figsize=(30, 45))\ncountry_label = {0: 'Finland', 1: 'Norway', 2: 'Sweden'}\nproduct_label = {0: 'Kaggle Mug', 1: 'Kaggle Hat', 2: 'Kaggle Speaker'}\nfor k, year in enumerate([2015, 2017, 2018]):\n    for j, country in enumerate([0, 1, 2]):\n        for i, product in enumerate([0, 1, 2]):\n            ax = fig.add_subplot(9, 3, i+1+j*3+k*9)\n            ax.plot(series[year][country][product].index, series[year][country][product].values, label='train')\n            ax.plot(series[year][country][product].index, series_test[country][product].values, label='test')\n            ax.set_title(f'Sales trend in {product_label[product]}, {country_label[country]}', fontsize=12)\n            ax.set_xlabel('Date', fontsize=8)\n            ax.set_ylabel('Sales', fontsize=8)\n            plt.xticks(fontsize=10)\n            plt.yticks(fontsize=8)\n            plt.legend(fontsize=8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-26T05:47:42.867923Z","iopub.status.idle":"2022-01-26T05:47:42.868505Z","shell.execute_reply.started":"2022-01-26T05:47:42.868179Z","shell.execute_reply":"2022-01-26T05:47:42.868239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Weekly trend looks good. They have enough feature.\n- Annual offset trend might be related in GDP.\n\n**I don't have any idea to be added external dataset..**","metadata":{}}]}