{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Although I've made many improvements to this notebook and improved the score, much of the credit should go to:  \n\n#### Credit to [@teckmengwong](https://www.kaggle.com/teckmengwong/tps2201-hybrid-time-series) for a fun notebook to work with.  and:\n\n#### Feature engineering and Linear model based on excellent: https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model @ambrosm  and:\n\n#### Hybrid model from Time series course: https://www.kaggle.com/learn/time-series","metadata":{}},{"cell_type":"markdown","source":"\n\n\n## Imports and Configuration ##","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\nfrom catboost import CatBoostRegressor\n\nimport ipywidgets as widgets\nfrom learntools.time_series.style import *  # plot style settings\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom pyearth import Earth\n\nfrom datetime import date\nimport holidays\nimport calendar\nimport dateutil.easter as easter\n\nfrom collections import defaultdict\nle = defaultdict(LabelEncoder)\n\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, StackingRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, HuberRegressor, RidgeCV\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(12, 8))\nplt.rc(\"axes\",labelweight=\"bold\",labelsize=\"large\",titleweight=\"bold\",titlesize=16,titlepad=10,)\nplot_params = dict(color=\"0.75\",style=\".-\",markeredgecolor=\"0.25\",markerfacecolor=\"0.25\",legend=False,)\n%config InlineBackend.figure_format = 'retina'\n\nimport gc\nimport os\nimport math\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T00:07:05.49086Z","iopub.execute_input":"2022-01-23T00:07:05.491403Z","iopub.status.idle":"2022-01-23T00:07:05.543437Z","shell.execute_reply.started":"2022-01-23T00:07:05.491359Z","shell.execute_reply":"2022-01-23T00:07:05.542778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRODUCTION = True # True: For submission run. False: Fast trial run\nRANDOM_STATE = 42\nVERBOSE = 0\n\n# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"../input/tabular-playground-series-jan-2022\"\nFEATURE_ENGINEERING = True\nPSEUDO_LABEL = True # PSEUDO are not ground true and will not help long term, only used for final push\nBLEND = True\n\nPSEUDO_DIR = \"../input/pseuodo-labels/pseudo_labels_v0.csv\"  \n\n# time series data common new feature  \nDATE = \"date\"\nYEAR = \"year\"\nMONTH = \"month\"\nWEEK = \"week\"\nDAY = \"day\"\nDAYOFYEAR = \"dayofyear\"\nDAYOFMONTH = \"dayofMonth\"\nDAYOFWEEK = \"dayofweek\"\nWEEKDAY = \"weekday\"\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T00:07:05.544855Z","iopub.execute_input":"2022-01-23T00:07:05.545239Z","iopub.status.idle":"2022-01-23T00:07:05.550703Z","shell.execute_reply.started":"2022-01-23T00:07:05.54521Z","shell.execute_reply":"2022-01-23T00:07:05.550161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function SMAPE\n​​i=1​∑​N​​w​i​​​​100​i=1​∑​N​​​(∣t​i​​∣+∣a​i​​∣)/2​​w​i​​∣a​i​​−t​i​​∣​​​​","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/36414\ndef smape_loss(y_true, y_pred):\n\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.551684Z","iopub.execute_input":"2022-01-23T00:07:05.551996Z","iopub.status.idle":"2022-01-23T00:07:05.568653Z","shell.execute_reply.started":"2022-01-23T00:07:05.551951Z","shell.execute_reply":"2022-01-23T00:07:05.567646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data/Feature Engineering","metadata":{}},{"cell_type":"code","source":"def get_basic_ts_features(df):\n    \n    gdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n    gdp_df.set_index('year', inplace=True)\n    gdp_exponent = 1.2121103201489674 # see https://www.kaggle.com/ambrosm/tpsjan22-03-linear-model for an explanation\n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n    \n    df['gdp'] = np.log1p(df.apply(get_gdp, axis=1))\n    \n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n    for store in ['KaggleMart']:\n        df[store] = df['store'] == store\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n    df[MONTH] = df[DATE].dt.month\n    # 4 seasons\n    df['season'] = ((df[DATE].dt.month % 12 + 3) // 3).map({1:'DJF', 2: 'MAM', 3:'JJA', 4:'SON'})\n    df['wd4'] = df[DATE].dt.weekday == 4\n    df['wd56'] = df[DATE].dt.weekday >= 5\n    \n    # 21 days cyclic for lunar\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 32, 4):\n        df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        df[f'Finland_sin{k}'] = df[f'sin{k}'] * df['Finland']\n        df[f'Finland_cos{k}'] = df[f'cos{k}'] * df['Finland']\n        df[f'Norway_sin{k}'] = df[f'sin{k}'] * df['Norway']\n        df[f'Norway_cos{k}'] = df[f'cos{k}'] * df['Norway']\n        df[f'store_sin{k}'] = df[f'sin{k}'] * df['KaggleMart']\n        df[f'store_cos{k}'] = df[f'cos{k}'] * df['KaggleMart']\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n    \n    # End of year\n    # Dec\n    for d in range(24, 32):\n        df[f\"dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d)\n    for d in range(24, 32):\n        df[f\"n-dec{d}\"] = (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n    # Jan\n    for d in range(1, 14):\n        df[f\"f-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n    for d in range(1, 10):\n        df[f\"n-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n    for d in range(1, 15):\n        df[f\"s-jan{d}\"] = (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n    # May\n    for d in list(range(1, 10)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d)\n    for d in list(range(19, 26)):\n        df[f\"may{d}\"] = (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n    # June\n    for d in list(range(8, 14)):\n        df[f\"june{d}\"] = (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n    \n    swed_rock_fest  = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-6')),2016: pd.Timestamp(('2016-06-11')),\n                                           2017: pd.Timestamp(('2017-06-10')),2018: pd.Timestamp(('2018-06-10')),\n                                           2019: pd.Timestamp(('2019-06-8'))})\n\n    df = pd.concat([df, pd.DataFrame({f\"swed_rock_fest{d}\":\n                                      (df.date - swed_rock_fest == np.timedelta64(d, \"D\")) & (df.country == 'Sweden')\n                                      for d in list(range(-3, 3))})], axis=1)\n\n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    \n    for d in list(range(-4, 6)):\n        df[f\"wed_june{d}\"] = (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n        \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),2016: pd.Timestamp(('2016-11-6')),\n                                        2017: pd.Timestamp(('2017-11-5')),2018: pd.Timestamp(('2018-11-4')),\n                                        2019: pd.Timestamp(('2019-11-3'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\":\n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country == 'Norway')\n                                      for d in list(range(0, 9))})], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})], axis=1)\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":(df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df  \n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.570994Z","iopub.execute_input":"2022-01-23T00:07:05.571225Z","iopub.status.idle":"2022-01-23T00:07:05.610193Z","shell.execute_reply.started":"2022-01-23T00:07:05.571197Z","shell.execute_reply":"2022-01-23T00:07:05.609413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Holiday generator","metadata":{}},{"cell_type":"code","source":"for ptr in holidays.Norway(years = [2019], observed=True).items():\n    print(ptr)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.611161Z","iopub.execute_input":"2022-01-23T00:07:05.611631Z","iopub.status.idle":"2022-01-23T00:07:05.631361Z","shell.execute_reply.started":"2022-01-23T00:07:05.611591Z","shell.execute_reply":"2022-01-23T00:07:05.630259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineer(df):\n    df = get_basic_ts_features(df)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.633129Z","iopub.execute_input":"2022-01-23T00:07:05.633451Z","iopub.status.idle":"2022-01-23T00:07:05.646555Z","shell.execute_reply.started":"2022-01-23T00:07:05.633408Z","shell.execute_reply":"2022-01-23T00:07:05.645859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=ID)\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID)\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    return df_train, df_test, column_y","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    df_train = pd.read_csv(data_dir / \"train.csv\", parse_dates=[DATE],\n                    usecols=['date', 'country', 'store', 'product', 'num_sold'],\n                    dtype={'country': 'category','store': 'category','product': 'category', 'num_sold': 'float32',},\n                    infer_datetime_format=True,)\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID, parse_dates=[DATE])\n    column_y = df_train.columns.difference(df_test.columns)[0]  # column_y target_col label_col\n    df_train[DATE] = pd.to_datetime(df_train[DATE])\n    df_test[DATE] = pd.to_datetime(df_test[DATE])\n    \n    return df_train, df_test, column_y\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.647613Z","iopub.execute_input":"2022-01-23T00:07:05.647947Z","iopub.status.idle":"2022-01-23T00:07:05.661344Z","shell.execute_reply.started":"2022-01-23T00:07:05.647912Z","shell.execute_reply":"2022-01-23T00:07:05.660563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(df_train, df_test):\n    \n    if FEATURE_ENGINEERING:\n        df_train = feature_engineer(df_train)\n        df_test = feature_engineer(df_test)\n\n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.66238Z","iopub.execute_input":"2022-01-23T00:07:05.66293Z","iopub.status.idle":"2022-01-23T00:07:05.673886Z","shell.execute_reply.started":"2022-01-23T00:07:05.662898Z","shell.execute_reply":"2022-01-23T00:07:05.672755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data #\n\nAnd now we can call the data loader and get the processed data splits:","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_df, test_df, column_y = load_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.675123Z","iopub.execute_input":"2022-01-23T00:07:05.675345Z","iopub.status.idle":"2022-01-23T00:07:05.765742Z","shell.execute_reply.started":"2022-01-23T00:07:05.675318Z","shell.execute_reply":"2022-01-23T00:07:05.76476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df, test_df = process_data(train_df, test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:05.768475Z","iopub.execute_input":"2022-01-23T00:07:05.768733Z","iopub.status.idle":"2022-01-23T00:07:09.063129Z","shell.execute_reply.started":"2022-01-23T00:07:05.768703Z","shell.execute_reply":"2022-01-23T00:07:09.062277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"code","source":"train_data = train_df.copy()\ntrain_data[DATE] = train_df.date.dt.to_period('D')\ntest_data = test_df.copy()\ntest_data[DATE] = test_df.date.dt.to_period('D')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:09.064436Z","iopub.execute_input":"2022-01-23T00:07:09.064672Z","iopub.status.idle":"2022-01-23T00:07:09.125176Z","shell.execute_reply.started":"2022-01-23T00:07:09.064643Z","shell.execute_reply":"2022-01-23T00:07:09.124258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pseudolabeling","metadata":{}},{"cell_type":"code","source":"df_pseudolabels = pd.read_csv(PSEUDO_DIR, index_col=ID)\ndf_pseudolabels[DATE] = pd.to_datetime(test_df[DATE])\ndf_pseudolabels.to_csv(\"pseudo_labels_v0.csv\", index=True)\ntest_data[column_y] = df_pseudolabels[column_y].astype(np.float32)\ntrain_data = pd.concat([train_data, test_data], axis=0)\ntrain_df = pd.concat([train_df, test_data], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:09.126345Z","iopub.execute_input":"2022-01-23T00:07:09.126564Z","iopub.status.idle":"2022-01-23T00:07:09.429636Z","shell.execute_reply.started":"2022-01-23T00:07:09.126538Z","shell.execute_reply":"2022-01-23T00:07:09.42869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.set_index([DATE]).sort_index()\nX_test = test_data.set_index([DATE]).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:09.431Z","iopub.execute_input":"2022-01-23T00:07:09.431262Z","iopub.status.idle":"2022-01-23T00:07:09.50426Z","shell.execute_reply.started":"2022-01-23T00:07:09.431235Z","shell.execute_reply":"2022-01-23T00:07:09.503208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Removal of 2016 test","metadata":{}},{"cell_type":"code","source":"train_data = train_data.set_index(['date', 'country', 'store', 'product']).sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:09.505658Z","iopub.execute_input":"2022-01-23T00:07:09.505996Z","iopub.status.idle":"2022-01-23T00:07:09.569592Z","shell.execute_reply.started":"2022-01-23T00:07:09.505941Z","shell.execute_reply":"2022-01-23T00:07:09.568613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2015 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2015'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T00:07:09.57108Z","iopub.execute_input":"2022-01-23T00:07:09.571499Z","iopub.status.idle":"2022-01-23T00:07:10.299876Z","shell.execute_reply.started":"2022-01-23T00:07:09.571468Z","shell.execute_reply":"2022-01-23T00:07:10.299199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2016 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2016'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T00:07:10.300844Z","iopub.execute_input":"2022-01-23T00:07:10.301444Z","iopub.status.idle":"2022-01-23T00:07:10.883287Z","shell.execute_reply.started":"2022-01-23T00:07:10.301407Z","shell.execute_reply":"2022-01-23T00:07:10.882229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2017 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2017'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T00:07:10.884572Z","iopub.execute_input":"2022-01-23T00:07:10.884807Z","iopub.status.idle":"2022-01-23T00:07:11.427346Z","shell.execute_reply.started":"2022-01-23T00:07:10.884777Z","shell.execute_reply":"2022-01-23T00:07:11.426144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales_2018 = (train_data.groupby(['country', 'store', 'product', 'date']).mean()\n                     .unstack(['country', 'store', 'product']).loc['2018'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-23T00:07:11.429025Z","iopub.execute_input":"2022-01-23T00:07:11.42927Z","iopub.status.idle":"2022-01-23T00:07:11.949353Z","shell.execute_reply.started":"2022-01-23T00:07:11.429242Z","shell.execute_reply":"2022-01-23T00:07:11.948446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = [kaggle_sales_2015, kaggle_sales_2016, kaggle_sales_2017, kaggle_sales_2018]\nkaggle_sales = pd.concat(frames)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:11.950741Z","iopub.execute_input":"2022-01-23T00:07:11.951028Z","iopub.status.idle":"2022-01-23T00:07:12.003163Z","shell.execute_reply.started":"2022-01-23T00:07:11.950993Z","shell.execute_reply":"2022-01-23T00:07:12.002321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle_sales","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:12.004506Z","iopub.execute_input":"2022-01-23T00:07:12.004737Z","iopub.status.idle":"2022-01-23T00:07:12.071647Z","shell.execute_reply.started":"2022-01-23T00:07:12.00471Z","shell.execute_reply":"2022-01-23T00:07:12.07102Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:12.072792Z","iopub.execute_input":"2022-01-23T00:07:12.073171Z","iopub.status.idle":"2022-01-23T00:07:12.2571Z","shell.execute_reply.started":"2022-01-23T00:07:12.07314Z","shell.execute_reply":"2022-01-23T00:07:12.25612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:12.258395Z","iopub.execute_input":"2022-01-23T00:07:12.258647Z","iopub.status.idle":"2022-01-23T00:07:12.3006Z","shell.execute_reply.started":"2022-01-23T00:07:12.258617Z","shell.execute_reply":"2022-01-23T00:07:12.299582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby(column_y).apply(lambda s: s.sample(min(len(s), 5)))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:12.302127Z","iopub.execute_input":"2022-01-23T00:07:12.302525Z","iopub.status.idle":"2022-01-23T00:07:17.3952Z","shell.execute_reply.started":"2022-01-23T00:07:12.302479Z","shell.execute_reply":"2022-01-23T00:07:17.394348Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['month']","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:17.396681Z","iopub.execute_input":"2022-01-23T00:07:17.396904Z","iopub.status.idle":"2022-01-23T00:07:17.414729Z","shell.execute_reply.started":"2022-01-23T00:07:17.396877Z","shell.execute_reply":"2022-01-23T00:07:17.41406Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig_dims = (50,30)\nax = kaggle_sales.num_sold.plot(title='Sales Trends', figsize=fig_dims)\n_ = ax.set(ylabel=\"Numbers sold\")","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:17.415935Z","iopub.execute_input":"2022-01-23T00:07:17.416747Z","iopub.status.idle":"2022-01-23T00:07:22.942636Z","shell.execute_reply.started":"2022-01-23T00:07:17.416699Z","shell.execute_reply":"2022-01-23T00:07:22.941985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,'store': store,'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df[GROUP_INDEX] = demo_df[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))\n    demo_df['num_sold'] = np.expm1(model.predict(preproc.transform(demo_df[features])))\n    train_subset = train_df[(train_df.country == country) & (train_df.store == store) & (train_df['product'] == product)]\n    plt.figure(figsize=(24, 8))\n    plt.plot(demo_df[DATE], demo_df.num_sold, label='prediction', alpha=0.5)\n    plt.scatter(train_subset[DATE], train_subset.num_sold, label='true', alpha=0.5, color='red', s=2)\n    plt.grid(True)\n    plt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n    plt.legend()\n    plt.title(f'{country} {store} {product} Predictions and true for five years')\n    plt.show()\n    \n    return demo_df['num_sold']","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:22.943654Z","iopub.execute_input":"2022-01-23T00:07:22.944253Z","iopub.status.idle":"2022-01-23T00:07:22.952948Z","shell.execute_reply.started":"2022-01-23T00:07:22.944219Z","shell.execute_reply":"2022-01-23T00:07:22.951956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_min_SMAPE(y_true, y_predict):\n    loss_correction = 1\n    scores = []\n    # float step\n    for WEIGHT in np.arange(0.97, 1.02, 0.0001):\n        y_hat = y_predict.copy()\n        y_hat *= WEIGHT\n        scores.append(np.array([WEIGHT, np.mean(smape_loss(y_true, y_hat))]))\n        \n    scores = np.vstack(scores)\n    min_SMAPE = np.min(scores[:,1])\n    print(f'min SMAPE {min_SMAPE:.5f}')\n    for x in scores:\n        if x[1] == min_SMAPE:\n            loss_correction = x[0]\n            print(f'loss_correction: {x[0]:.5f}')\n            \n    plt.figure(figsize=(5, 3))\n    plt.plot(scores[:,0],scores[:,1])\n    plt.scatter([loss_correction], [min_SMAPE], color='g')\n    plt.ylabel(f'SMAPE')\n    plt.xlabel(f'loss_correction: {loss_correction:.5f}')\n    plt.legend()\n    plt.title(f'min SMAPE:{min_SMAPE:.5f} scaling')\n    plt.show()\n    \n    return loss_correction","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:22.954655Z","iopub.execute_input":"2022-01-23T00:07:22.955483Z","iopub.status.idle":"2022-01-23T00:07:22.984173Z","shell.execute_reply.started":"2022-01-23T00:07:22.955432Z","shell.execute_reply":"2022-01-23T00:07:22.983379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_true_vs_prediction(df_true, df_hat):\n    plt.figure(figsize=(20, 13))\n    plt.scatter(np.arange(len(df_hat)), np.log1p(df_hat), label='prediction', alpha=0.5, color='blue', s=3) \n    plt.scatter(np.arange(len(df_true)), np.log1p(df_true), label='Pseudo/true', alpha=0.5, color='red', s=7) \n    plt.legend()\n    plt.title(f'Predictions VS Pseudo-label {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:22.988862Z","iopub.execute_input":"2022-01-23T00:07:22.989178Z","iopub.status.idle":"2022-01-23T00:07:23.000919Z","shell.execute_reply.started":"2022-01-23T00:07:22.989143Z","shell.execute_reply":"2022-01-23T00:07:23.000027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_residuals(y_residuals):\n    plt.figure(figsize=(13, 3))\n    plt.scatter(np.arange(len(y_residuals)), np.log1p(y_residuals), label='residuals', alpha=0.1, color='blue', s=5)\n    plt.legend()\n    plt.title(f'Linear Model residuals {column_y} (LOG)') #{df_true.index[0]} - {df_true.index[-1]}\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.002364Z","iopub.execute_input":"2022-01-23T00:07:23.002997Z","iopub.status.idle":"2022-01-23T00:07:23.014685Z","shell.execute_reply.started":"2022-01-23T00:07:23.002937Z","shell.execute_reply":"2022-01-23T00:07:23.014019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_oof(y_true, y_predict):\n    plt.figure(figsize=(5, 5))\n    plt.scatter(y_true, y_predict, s=1, color='r', alpha=0.5)\n    plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n    plt.gca().set_aspect('equal')\n    plt.xlabel('y_true')\n    plt.ylabel('y_pred')\n    plt.title('OOF Predictions')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.016073Z","iopub.execute_input":"2022-01-23T00:07:23.016416Z","iopub.status.idle":"2022-01-23T00:07:23.03088Z","shell.execute_reply.started":"2022-01-23T00:07:23.016384Z","shell.execute_reply":"2022-01-23T00:07:23.02948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_SMAPE(y_va, y_va_pred):\n    loss_correction = 1\n    \n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n    smape = np.mean(smape_loss(y_va, y_va_pred))\n    loss_correction = find_min_SMAPE(y_va, y_va_pred)\n    y_va_pred *= loss_correction\n    print(f\"SMAPE (before correction: {smape_before_correction:.5f})\")\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred))}')\n    \n    return loss_correction","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.032747Z","iopub.execute_input":"2022-01-23T00:07:23.033275Z","iopub.status.idle":"2022-01-23T00:07:23.046727Z","shell.execute_reply.started":"2022-01-23T00:07:23.033238Z","shell.execute_reply":"2022-01-23T00:07:23.045786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing X_2 X_test y","metadata":{}},{"cell_type":"code","source":"GROUP_INDEX = ['country', 'store', 'product', 'month', 'season']\n\n# Target series\ny = X.loc[:, column_y]\n\n# X_1: Features for Linear Regression\nfourier = CalendarFourier(freq=\"A\", order=10)  # 10 sin/cos pairs for \"A\"nnual seasonality\n\ndp = DeterministicProcess(index=X.index,constant=True,order=1,seasonal=True,additional_terms=[fourier],drop=True,)\n\nX_1 = dp.in_sample()  # create features for dates in tunnel.index\n\n# X_2: Features for XGBoo\nX_2 = X.drop(column_y, axis=1)\n\n# Encoding the variable\nX_2[GROUP_INDEX] = X_2[GROUP_INDEX].apply(lambda x: le[x.name].fit_transform(x))\n\n# Using the dictionary to label future data\nX_test[GROUP_INDEX] = X_test[GROUP_INDEX].apply(lambda x: le[x.name].transform(x))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.048369Z","iopub.execute_input":"2022-01-23T00:07:23.048688Z","iopub.status.idle":"2022-01-23T00:07:23.290502Z","shell.execute_reply.started":"2022-01-23T00:07:23.048653Z","shell.execute_reply":"2022-01-23T00:07:23.28967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = X_2.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.291927Z","iopub.execute_input":"2022-01-23T00:07:23.292411Z","iopub.status.idle":"2022-01-23T00:07:23.296429Z","shell.execute_reply.started":"2022-01-23T00:07:23.292365Z","shell.execute_reply":"2022-01-23T00:07:23.295707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PSEUDO_LABEL:\n    TRAIN_END_DATE = \"2019-12-31\"\n    VALID_START_DATE = \"2015-01-01\"\n    VALID_END_DATE = \"2018-12-31\"\nelse:\n    if PRODUCTION:\n        TRAIN_END_DATE = \"2018-12-31\"\n    else:\n        TRAIN_END_DATE = \"2017-12-31\"\n    VALID_START_DATE = \"2018-01-01\"\n    VALID_END_DATE = \"2018-12-31\"\n\ny_train, y_valid = y[:TRAIN_END_DATE], y[VALID_START_DATE:VALID_END_DATE]\nX1_train, X1_valid = X_1[:TRAIN_END_DATE], X_1[VALID_START_DATE:VALID_END_DATE]\nX2_train, X2_valid = X_2.loc[:TRAIN_END_DATE], X_2.loc[VALID_START_DATE:VALID_END_DATE]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.29782Z","iopub.execute_input":"2022-01-23T00:07:23.298271Z","iopub.status.idle":"2022-01-23T00:07:23.315941Z","shell.execute_reply.started":"2022-01-23T00:07:23.298225Z","shell.execute_reply":"2022-01-23T00:07:23.314924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# You'll add fit and predict methods to this minimal class\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n\nclass BoostedHybrid(BaseEstimator, RegressorMixin):\n    def __init__(self, model_1, model_2, scaler):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.scaler = scaler\n        self.y_columns = None  # store column names from fit method\n    \n    def fit(self, X, y): #, X_1_valid, y_valid\n        \n        X, y = check_X_y(X, y, accept_sparse=True)\n        # Train model_1\n        self.model_1.fit(X, y)\n\n        # Make predictions\n        y_fit = self.model_1.predict(X)\n        # Compute residuals\n        y_resid = y - y_fit\n\n        # Train model_2 on residuals , eval_set=[(X_1_valid, y_valid_resid)]\n        self.model_2.fit(X, y_resid)\n        \n        # Model2 prediction\n        y_fit2 = self.model_2.predict(X)\n        # Compute noise\n        y_resid2 = y_resid - y_fit2\n        \n        # Save data for question checking\n        self.y = y\n        self.y_fit = y_fit\n        self.y_resid = y_resid\n        self.y_fit2 = y_fit2\n        self.y_resid2 = y_resid2\n\n        self.is_fitted_ = True\n        \n        return self\n\n\n    def predict(self, X):\n        \n        X = check_array(X, accept_sparse=True)\n        check_is_fitted(self, 'is_fitted_')\n        # Predict with model_1\n        y_predict = self.model_1.predict(X)\n        \n        # Add model_2 predictions to model_1 predictions\n        y_predict += self.model_2.predict(X)\n\n        return y_predict\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.317792Z","iopub.execute_input":"2022-01-23T00:07:23.318438Z","iopub.status.idle":"2022-01-23T00:07:23.332413Z","shell.execute_reply.started":"2022-01-23T00:07:23.318376Z","shell.execute_reply":"2022-01-23T00:07:23.331615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Using StandardScaler","metadata":{}},{"cell_type":"code","source":"preproc = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.333792Z","iopub.execute_input":"2022-01-23T00:07:23.334257Z","iopub.status.idle":"2022-01-23T00:07:23.347734Z","shell.execute_reply.started":"2022-01-23T00:07:23.334207Z","shell.execute_reply":"2022-01-23T00:07:23.347148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_fit_eval(hybrid_model, X_train, y_train, X_valid, y_valid, scaler, loss_correction):\n    test_pred_list = []\n    \n    # Boosted Hybrid\n    hybrid_model.fit(X_train, y_train) #, X_valid, y_valid\n    y_va_pred = hybrid_model.predict(X_valid)\n        \n    ###### Preprocess the validation data\n    y_va = np.expm1(y_valid.copy())\n    \n    # Inference for validation\n    y_va_pred = np.expm1(hybrid_model.predict(X_valid))\n    loss_correction = evaluate_SMAPE(y_va, y_va_pred)\n    \n    ###### Visualize and evual\n    plot_oof(y_va, y_va_pred)\n    plot_true_vs_prediction(y_va, y_va_pred)\n    \n    ###### Validate against 2019 PSEU #######\n    loss_correction = 1\n    \n    ###### Preprocess the validation data\n    y_va = df_pseudolabels[column_y].values.reshape(-1, 1)\n    \n    # Inference test 2019 for validation\n    y_va_pred = np.expm1(hybrid_model.predict(scaler.transform(X_test[features])))\n    \n    # Evaluation: Execution time and SMAPE\n    smape_before_correction = np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)))\n    smape = np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)))\n    print(f'***********Test Data*****************')\n    loss_correction = find_min_SMAPE(y_va, y_va_pred.reshape(-1, 1))\n    \n    ### Mean test prediction ###\n    test_pred_list.append(y_va_pred)\n\n    print(f'SMAPE (before correction: {smape_before_correction:.5f})')\n    print(f'Min SMAPE: {np.mean(smape_loss(y_va, y_va_pred.reshape(-1, 1)*loss_correction))}')\n    \n    return hybrid_model, test_pred_list, loss_correction","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.348861Z","iopub.execute_input":"2022-01-23T00:07:23.349788Z","iopub.status.idle":"2022-01-23T00:07:23.359728Z","shell.execute_reply.started":"2022-01-23T00:07:23.349747Z","shell.execute_reply":"2022-01-23T00:07:23.359171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = dict(booster= 'gbtree',\n                  tree_method = 'exact',\n                  objective='reg:pseudohubererror',\n                  n_jobs = -1,\n                  max_depth=5,\n                  learning_rate=0.5406089095129346,\n                  n_estimators=5918,\n                  min_child_weight=3,\n                  colsample_bytree=0.2595406744619732,\n                  # subsample=trial.suggest_float(\"subsample\", 0.3, .8),\n                  reg_alpha=97.39174536138904,\n                  reg_lambda=0.5348869112742457,\n                  # colsample_bylevel=trial.suggest_float(\"colsample_bylevel\",.3,.5),\n                  gamma=0.1024158842570319,\n                  # max_delta_step=trial.suggest_float(\"max_delta_step\",0,1),\n                  num_parallel_tree=1,\n                  random_state=35,\n                 )\n\nlgbm_params = {\n              'learning_rate':0.025292895772398984,\n              \"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              'boosting_type': \"gbdt\",\n              'verbosity': -1,\n              'n_jobs': -1, \n              'seed': 21,\n              'reg_alpha': 0.0029751624135773416,\n              'reg_lambda': 0.650014120724397,\n              'lambda_l1': 1.1096023419303558, \n              'lambda_l2': 1.996527963987735, \n              'num_leaves': 109, \n               # 'feature_fraction': 0.6259927292757151, \n               # 'bagging_fraction': 0.9782210574588895, \n               # 'bagging_freq': 1, \n              'n_estimators': 2606, \n              'max_depth': 1, \n              'max_bin': 244, \n              'min_data_in_leaf': 366,\n              'random_state' : RANDOM_STATE,\n              }\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.361079Z","iopub.execute_input":"2022-01-23T00:07:23.361683Z","iopub.status.idle":"2022-01-23T00:07:23.376997Z","shell.execute_reply.started":"2022-01-23T00:07:23.361652Z","shell.execute_reply":"2022-01-23T00:07:23.376046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xgb_params = dict(booster= 'gbtree',\n#                   tree_method = 'exact',\n#                   objective='reg:pseudohubererror',\n#                   n_jobs = -1,\n#                   max_depth=trial.suggest_int(\"max_depth\",2,5),\n#                   learning_rate=trial.suggest_float('learning_rate',.001,1),\n#                   n_estimators=trial.suggest_int('n_estimators',1000,10000),\n#                   min_child_weight=trial.suggest_int('min_child_weight',1,3),\n#                   colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#                   # subsample=trial.suggest_float(\"subsample\", 0.3, .8),\n#                   reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#                   reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#                   # colsample_bylevel=trial.suggest_float(\"colsample_bylevel\",.3,.5),\n#                   gamma=trial.suggest_float(\"gamma\", 0,1),\n#                   # max_delta_step=trial.suggest_float(\"max_delta_step\",0,1),\n#                   num_parallel_tree=1,\n#                   random_state=35,\n#                   )","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.378219Z","iopub.execute_input":"2022-01-23T00:07:23.378447Z","iopub.status.idle":"2022-01-23T00:07:23.392062Z","shell.execute_reply.started":"2022-01-23T00:07:23.378418Z","shell.execute_reply":"2022-01-23T00:07:23.391434Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# import lightgbm as lgb\n# import sklearn.datasets\n# import sklearn.metrics\n\n# LOSS_CORRECTION = 1\n# estimator_stack = []\n\n# def objective(trial):\n#     LOSS_CORRECTION = 1\n#     xgb_params = dict(booster= 'gbtree',\n#                       tree_method = 'exact',\n#                       objective='reg:pseudohubererror',\n#                       n_jobs = -1,\n#                       max_depth=trial.suggest_int(\"max_depth\",2,5),\n#                       learning_rate=trial.suggest_float('learning_rate',.001,1),\n#                       n_estimators=trial.suggest_int('n_estimators',1000,10000),\n#                       min_child_weight=trial.suggest_int('min_child_weight',1,3),\n#                       colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#                       # subsample=trial.suggest_float(\"subsample\", 0.3, .8),\n#                       reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#                       reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#                       # colsample_bylevel=trial.suggest_float(\"colsample_bylevel\",.3,.5),\n#                       gamma=trial.suggest_float(\"gamma\", 0,1),\n#                       # max_delta_step=trial.suggest_float(\"max_delta_step\",0,1),\n#                       num_parallel_tree=1,\n#                       random_state=35,\n#                      )\n    \n#     X2 = preproc.fit_transform(X2_train[features])\n#     model = BoostedHybrid(model_1 = Ridge(alpha=.85),model_2 = XGBRegressor(**xgb_params),scaler = preproc)\n#     model, test_pred_list, LOSS_CORRECTION = model_fit_eval(model, X2, np.log1p(y_train),preproc.transform(X2_valid[features]),\n#                                                             np.log1p(y_valid),preproc, LOSS_CORRECTION)\n    \n#     return LOSS_CORRECTION  \n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=500)\n# xgb_params = study.best_params\n# print('best params',xgb_params)\n\n\n# def objective(trial):\n#     LOSS_CORRECTION=1\n#     lgbm_params = {\n#                   'learning_rate': trial.suggest_float('learning_rate', .001, 1.0),\n#                   \"objective\": \"regression\",\n#                   \"metric\": \"rmse\",\n#                   'boosting_type': \"gbdt\",\n#                   'verbosity': -1,\n#                   'n_jobs': -1, \n#                   'seed': 21,\n#                   'lambda_l1': trial.suggest_float('lambda_l1',.1,2.0),\n#                   'lambda_l2': trial.suggest_float('lambda_l2', .1,2.0),\n#                   'num_leaves': trial.suggest_int('num_leaves', 100, 500),\n#                    #'feature_fraction': trial.suggest_float('feature_fraction',.01,1.0),\n#                    # 'bagging_fraction': trial.suggest_float('bagging_fraction',.01,1.0),\n#                    # 'bagging_freq': trial.suggest_int('bagging_freq',1,2), \n#                   'n_estimators': trial.suggest_int('n_estimators',100,5000), \n#                   'max_depth': trial.suggest_int('max_depth', 1,7), \n#                   'max_bin': trial.suggest_int('max_bin', 10,500),\n#                   # 'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',300,5000),\n#                   }\n    \n    \n#     X2 = preproc.fit_transform(X2_train[features])\n#     model = BoostedHybrid(model_1 = Ridge(alpha=.85),model_2 = lgb.LGBMRegressor(**lgbm_params),scaler = preproc)\n#     model, test_pred_list, LOSS_CORRECTION = model_fit_eval(model, X2, np.log1p(y_train),preproc.transform(X2_valid[features]),\n#                                                             np.log1p(y_valid),preproc, LOSS_CORRECTION)\n    \n#     return LOSS_CORRECTION  \n    \n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=500)\n# lgbm_params = study.best_params\n# print('best params',lgbm_params)\n\n\n###########################################################################################\n# ***********Test Data*****************\n# min SMAPE 0.57772\n# loss_correction: 0.99910\n# SMAPE (before correction: 0.58343)\n# Min SMAPE: 0.5777192140317523\n# [LightGBM] [Warning] lambda_l2 is set=1.996527963987735, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.996527963987735\n# [LightGBM] [Warning] lambda_l1 is set=1.1096023419303558, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1096023419303558\n# min SMAPE 4.19017\n# loss_correction: 1.00080\n# SMAPE (before correction: 4.19074)\n# Min SMAPE: 4.190172283130202\n# [I 2022-01-22 22:00:03,571] Trial 73 finished with value: 0.9984999999999968 and parameters: {'learning_rate': 0.025292895772398984, 'lambda_l1': 1.1096023419303558, 'lambda_l2': 1.996527963987735, 'num_leaves': 109, 'n_estimators': 2606, 'max_depth': 1, 'max_bin': 244}. Best is trial 73 with value: 0.9984999999999968.\n# ***********Test Data*****************\n# min SMAPE 0.63428\n# loss_correction: 0.99850\n# SMAPE (before correction: 0.64643)\n# Min SMAPE: 0.6342847324685896\n# [LightGBM] [Warning] lambda_l2 is set=1.9365125171332647, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.9365125171332647\n# [LightGBM] [Warning] lambda_l1 is set=1.1349580383858455, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1349580383858455\n# min SMAPE 4.15496\n# loss_correction: 1.00090\n# SMAPE (before correction: 4.15572)\n# Min SMAPE: 4.154962358508988\n#################################################\n\n# param1 = {'loss_function': 'MultiRMSE','eval_metric': 'MultiRMSE','n_estimators': 1000,'od_type' : 'Iter',\n#             'od_wait' : 20,'random_state': RANDOM_STATE,'verbose': VERBOSE}\n\n# # Try different combinations of the algorithms above KNeighborsRegressor\n# models_1 = [Earth(verbose=VERBOSE), Ridge(alpha=.85), HuberRegressor(epsilon=1.20, max_iter=500),\n#             MLPRegressor(  hidden_layer_sizes=(256, 128),learning_rate_init=0.01,early_stopping=True,\n#                             random_state=RANDOM_STATE ),]\n\n# models_2 = [XGBRegressor(objective='reg:pseudohubererror', tree_method='hist', n_estimators=1000),\n#             lgb.LGBMRegressor(objective='regression', n_estimators=1000, random_state=RANDOM_STATE),\n#             CatBoostRegressor(**param1),]\n\n# for model_1 in models_1:\n#     for model_2 in models_2:\n#         model1_name = type(model_1).__name__\n#         model2_name = type(model_2).__name__\n#         hybrid_model = BoostedHybrid(model_1 = model_1,model_2 = model_2,scaler = preproc)\n#         print(f'******************Stacking {model1_name:>15} with {model2_name:<18}*************************')\n#         estimator_stack.append((f'model_{model1_name}_{model2_name}', hybrid_model))\n        \n# \n# model = StackingRegressor(estimators=estimator_stack, final_estimator=RidgeCV(alphas=[.7,.8,.85,.9,1,1.1,1.2]),\n#                           n_jobs=-1, verbose=VERBOSE)\n\n# model, test_pred_list, LOSS_CORRECTION = model_fit_eval(model, X2, np.log1p(y_train),preproc.transform(X2_valid[features]),\n#                                                         np.log1p(y_valid),preproc, LOSS_CORRECTION)\n\n# model = XGBRegressor(**xgb_params)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.393407Z","iopub.execute_input":"2022-01-23T00:07:23.393833Z","iopub.status.idle":"2022-01-23T00:07:23.410697Z","shell.execute_reply.started":"2022-01-23T00:07:23.393803Z","shell.execute_reply":"2022-01-23T00:07:23.409803Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nLOSS_CORRECTION = 1\nestimator_stack = []\n\nparam1 = {'loss_function': 'MultiRMSE','eval_metric': 'MultiRMSE','n_estimators': 1000,'od_type' : 'Iter',\n          'od_wait' : 20,'random_state': RANDOM_STATE,'verbose': VERBOSE}\n\n# Try different combinations of the algorithms above KNeighborsRegressor\nmodels_1 = [Earth(verbose=VERBOSE), Ridge(alpha=.85), HuberRegressor(epsilon=1.20, max_iter=500),\n            MLPRegressor(hidden_layer_sizes=(256, 128),learning_rate_init=0.01,early_stopping=True,\n                         random_state=RANDOM_STATE ),]\n\nmodels_2 = [XGBRegressor(**xgb_params),lgb.LGBMRegressor(**lgbm_params),CatBoostRegressor(**param1),]\n\nfor model_1 in models_1:\n    for model_2 in models_2:\n        model1_name = type(model_1).__name__\n        model2_name = type(model_2).__name__\n        hybrid_model = BoostedHybrid(model_1 = model_1,model_2 = model_2,scaler = preproc)\n        print(f'******************Stacking {model1_name:>15} with {model2_name:<18}*************************')\n        estimator_stack.append((f'model_{model1_name}_{model2_name}', hybrid_model))\n        \nX2 = preproc.fit_transform(X2_train[features])\n\nmodel = StackingRegressor(estimators=estimator_stack,final_estimator=RidgeCV(), n_jobs=-1, verbose=VERBOSE)\n\nmodel,test_pred_list,LOSS_CORRECTION=model_fit_eval(model,X2,np.log1p(y_train),preproc.transform(X2_valid[features]),\n                                                    np.log1p(y_valid),preproc,LOSS_CORRECTION)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:07:23.411802Z","iopub.execute_input":"2022-01-23T00:07:23.412056Z","iopub.status.idle":"2022-01-23T00:46:01.527796Z","shell.execute_reply.started":"2022-01-23T00:07:23.412026Z","shell.execute_reply":"2022-01-23T00:46:01.527115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference validation","metadata":{}},{"cell_type":"code","source":"for country in np.unique(train_df['country']):\n    for product in np.unique(train_df['product']):\n        for store in np.unique(train_df['store']):\n            y_fit = plot_five_years_combination(feature_engineer, country=country, product=product, store=store)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:01.529594Z","iopub.execute_input":"2022-01-23T00:46:01.52984Z","iopub.status.idle":"2022-01-23T00:46:02.225745Z","shell.execute_reply.started":"2022-01-23T00:46:01.52981Z","shell.execute_reply":"2022-01-23T00:46:02.224561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference year 2019 test data","metadata":{}},{"cell_type":"code","source":"y_pred = sum(test_pred_list) / len(test_pred_list) #model.predict(X_test[features])","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.227172Z","iopub.status.idle":"2022-01-23T00:46:02.22754Z","shell.execute_reply.started":"2022-01-23T00:46:02.227351Z","shell.execute_reply":"2022-01-23T00:46:02.227369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nLOSS_CORRECTION = 1\n\n###### Preprocess the validation data\ny_va = df_pseudolabels[column_y].values.reshape(-1, 1)\n\n# Inference for validation\ny_va_pred = y_pred.copy().reshape(-1, 1) #model.predict(X_test[features])\n\n# Evaluation: Execution time and SMAPE\nsmape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\nsmape = np.mean(smape_loss(y_va, y_va_pred))\nLOSS_CORRECTION = find_min_SMAPE(y_va, y_va_pred)\ny_va_pred *= LOSS_CORRECTION\n\nprint(f\" SMAPE: {smape:.5f} (before correction: {smape_before_correction:.5f})\")\nprint(np.mean(smape_loss(y_va, y_va_pred)))\n\n# [I 2022-01-21 19:08:22,256] Trial 21 finished with value: 0.996799999999997 and parameters: {'max_depth': 5, \n# 'learning_rate': 0.5406089095129346, 'n_estimators': 5918, 'min_child_weight': 3, 'colsample_bytree': \n# 0.2595406744619732, 'reg_alpha': 97.39174536138904, 'reg_lambda': 0.5348869112742457, 'gamma': \n# 0.1024158842570319}. Best is trial 12 with value: 0.996799999999997.\n# ***********Test Data*****************\n# min SMAPE 0.78355\n# loss_correction: 0.99680\n# SMAPE (before correction: 0.82951)\n# Min SMAPE: 0.7835522368396695\n# min SMAPE 4.13588\n# loss_correction: 1.00080\n# SMAPE (before correction: 4.13639)\n# Min SMAPE: 4.1358802070879515","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.228768Z","iopub.status.idle":"2022-01-23T00:46:02.229106Z","shell.execute_reply.started":"2022-01-23T00:46:02.228921Z","shell.execute_reply":"2022-01-23T00:46:02.228937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_oof(y_va, y_va_pred)\nplot_true_vs_prediction(y_va, y_va_pred)\nplot_residuals(model.estimators_[0].y_resid)\nplot_residuals(model.estimators_[0].y_resid2)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.232007Z","iopub.status.idle":"2022-01-23T00:46:02.232317Z","shell.execute_reply.started":"2022-01-23T00:46:02.232155Z","shell.execute_reply":"2022-01-23T00:46:02.232172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n","metadata":{}},{"cell_type":"code","source":"from math import ceil, floor, sqrt\n# from https://www.kaggle.com/fergusfindley/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n\n    return result_array","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.233716Z","iopub.status.idle":"2022-01-23T00:46:02.234041Z","shell.execute_reply.started":"2022-01-23T00:46:02.233862Z","shell.execute_reply":"2022-01-23T00:46:02.233879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jan-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.235199Z","iopub.status.idle":"2022-01-23T00:46:02.235501Z","shell.execute_reply.started":"2022-01-23T00:46:02.235342Z","shell.execute_reply":"2022-01-23T00:46:02.235359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inference for test\ntest_prediction_list = []\ntest_prediction_list.append(y_pred) # * LOSS_CORRECTION)\ndf_pseudolabels1 = pd.DataFrame()\ndf_pseudolabels2 = pd.DataFrame()\ndf_pseudolabels3 = pd.DataFrame()\n\nif BLEND:\n    \n    test_prediction_list.append(df_pseudolabels[column_y].values) #blender 1\n    \n    df_pseudolabels1 = pd.read_csv('../input/tell-me-the-magic-number/submission.csv', index_col=ID)    \n    \n    test_prediction_list.append(df_pseudolabels1[column_y].values) #blender 2\n    \n    df_pseudolabels2 = pd.read_csv('../input/tps-01-2022/submission.csv', index_col=ID)\n    \n    test_prediction_list.append(df_pseudolabels2[column_y].values)\n    \n    df_pseudolabels3 = pd.read_csv('../input/tpsjan22-03-linear-model/submission_linear_model.csv', index_col=ID)\n    \n    test_prediction_list.append(df_pseudolabels3[column_y].values)\n\ntest_prediction_list = np.median(test_prediction_list, axis=0) \n\n\nif len(test_prediction_list) > 0:\n    # Create the submission file\n    submission = pd.DataFrame(data=np.zeros((sub.shape[0],2)),index = sub.index.tolist(),columns=[ID,column_y])\n    submission[ID] = sub[ID]\n    submission[column_y] = test_prediction_list\n    \n    #https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/299162\n    submission[column_y] = geometric_round(submission[column_y]).astype(int) \n    \n    submission.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df[column_y], bins=np.linspace(0, 3000, 201),density=True, label='Training')\n    plt.hist(submission[column_y], bins=np.linspace(0, 3000, 201),density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel(column_y)\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.2367Z","iopub.status.idle":"2022-01-23T00:46:02.237021Z","shell.execute_reply.started":"2022-01-23T00:46:02.236845Z","shell.execute_reply":"2022-01-23T00:46:02.236862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(submission.head(30))\ndisplay(submission.tail(30))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:46:02.238433Z","iopub.status.idle":"2022-01-23T00:46:02.238763Z","shell.execute_reply.started":"2022-01-23T00:46:02.238593Z","shell.execute_reply":"2022-01-23T00:46:02.23861Z"},"trusted":true},"execution_count":null,"outputs":[]}]}