{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What are you trying to do in this notebook?\nThis notebook contains the dataset with dates of festivities in Finland, Norway and Sweden in the years used by the competitions.\nWe will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow us to try numerous different modeling approaches.\n\n#### Why are you trying it?\n- To check the data structure. \n- We have a combination of time series based on countries, stores and products. \n- To check all the combinations appear in train and test.\n- To visualize the timing split between train and test.\n- To verify that no date is missing from train and test.\n\n#### What we learned while making this notebook?\n- To explore the data.\n- Creating panels of products, countries, shops.\n- To explore seasonality based on months.\n- To examine seasonality at a week level.\n- To obesere recurrences at a monthly level.\n- To enrich the data (using festivities and GDP data), etc.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:00.236412Z","iopub.execute_input":"2022-01-07T15:09:00.23726Z","iopub.status.idle":"2022-01-07T15:09:00.279474Z","shell.execute_reply.started":"2022-01-07T15:09:00.237135Z","shell.execute_reply":"2022-01-07T15:09:00.278755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:00.281106Z","iopub.execute_input":"2022-01-07T15:09:00.281416Z","iopub.status.idle":"2022-01-07T15:09:01.07659Z","shell.execute_reply.started":"2022-01-07T15:09:00.28138Z","shell.execute_reply":"2022-01-07T15:09:01.075864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train and test data\ntrain = pd.read_csv(\"../input/tabular-playground-series-jan-2022/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2022/test.csv\", parse_dates=['date'])","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.079052Z","iopub.execute_input":"2022-01-07T15:09:01.079527Z","iopub.status.idle":"2022-01-07T15:09:01.163093Z","shell.execute_reply.started":"2022-01-07T15:09:01.07949Z","shell.execute_reply":"2022-01-07T15:09:01.162364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.165139Z","iopub.execute_input":"2022-01-07T15:09:01.165405Z","iopub.status.idle":"2022-01-07T15:09:01.173846Z","shell.execute_reply.started":"2022-01-07T15:09:01.16537Z","shell.execute_reply":"2022-01-07T15:09:01.17295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# figuring out the theoretically possible level combination\ntime_series = ['country', 'store', 'product']\ncombinations = 1\nfor feat in time_series:\n    combinations *= train[feat].nunique()\n    \nprint(f\"There are {combinations} possible combinations\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.1753Z","iopub.execute_input":"2022-01-07T15:09:01.175835Z","iopub.status.idle":"2022-01-07T15:09:01.193963Z","shell.execute_reply.started":"2022-01-07T15:09:01.175795Z","shell.execute_reply":"2022-01-07T15:09:01.193009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_series = ['country', 'store', 'product']\ncountry_store_product_train = train[time_series].drop_duplicates().sort_values(time_series)\ncountry_store_product_test =test[time_series].drop_duplicates().sort_values(time_series)\n\ncond_1 = len(country_store_product_train) == combinations\nprint(f\"Are all theoretical combinations present in train: {cond_1}\")\ncond_2 = (country_store_product_train == country_store_product_test).all().all()\nprint(f\"Are combinations the same in train and test: {cond_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.195246Z","iopub.execute_input":"2022-01-07T15:09:01.19592Z","iopub.status.idle":"2022-01-07T15:09:01.223927Z","shell.execute_reply.started":"2022-01-07T15:09:01.195879Z","shell.execute_reply":"2022-01-07T15:09:01.223101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dates = train.date.drop_duplicates().sort_values()\ntest_dates = test.date.drop_duplicates().sort_values()\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n\ncolor_index = np.array([1] * len(train_dates) + [0] * len(test_dates))\n\nax.scatter(range(len(train_dates)), [.5] * len(train_dates),\n           c=color_index[:len(train_dates)], marker='_', lw=15, cmap=cmap_cv,\n           label='train', vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train_dates), len(train_dates) + len(test_dates)), [.55] * len(test_dates),\n           c=color_index[len(train_dates):], marker='_', lw=15, cmap=cmap_cv,\n           label='test', vmin=-.2, vmax=1.2)\n\ntick_locations = np.cumsum([0, 365, 366, 365, 365, 365])\nfor i in (tick_locations):\n    ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    \nax.set_xticks(tick_locations)\nax.set_xticklabels([2015, 2016, 2017, 2018, 2019, 2020], rotation = 0)\nax.set_yticklabels(labels=[])\nplt.ylim([0.45, 0.60])\nax.legend(loc=\"upper left\", title=\"data\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.225225Z","iopub.execute_input":"2022-01-07T15:09:01.225602Z","iopub.status.idle":"2022-01-07T15:09:01.473219Z","shell.execute_reply.started":"2022-01-07T15:09:01.225564Z","shell.execute_reply":"2022-01-07T15:09:01.472542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_train = pd.date_range(start=train_dates.min(), end=train_dates.max()).difference(train_dates)\nmissing_test = pd.date_range(start=test_dates.min(), end=test_dates.max()).difference(test_dates)\nprint(f\"missing dates in train: {len(missing_train)} and in test: {len(missing_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.474367Z","iopub.execute_input":"2022-01-07T15:09:01.474611Z","iopub.status.idle":"2022-01-07T15:09:01.483094Z","shell.execute_reply.started":"2022-01-07T15:09:01.474577Z","shell.execute_reply":"2022-01-07T15:09:01.482377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create different time granularity\n\ndef process_time(df):\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week\n    df['week'][df['week']>52] = 52\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    return df\n\ntrain = process_time(train)\ntest = process_time(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.484199Z","iopub.execute_input":"2022-01-07T15:09:01.484654Z","iopub.status.idle":"2022-01-07T15:09:01.540145Z","shell.execute_reply.started":"2022-01-07T15:09:01.484616Z","shell.execute_reply":"2022-01-07T15:09:01.539427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            selected.set_index('date').groupby('year')['num_sold'].mean().plot(ax=ax)\n            ax.set_title(f\"{country}:{store}\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:01.542975Z","iopub.execute_input":"2022-01-07T15:09:01.543335Z","iopub.status.idle":"2022-01-07T15:09:04.710354Z","shell.execute_reply.started":"2022-01-07T15:09:01.543295Z","shell.execute_reply":"2022-01-07T15:09:04.708766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('month')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{product} | {country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:04.711882Z","iopub.execute_input":"2022-01-07T15:09:04.712367Z","iopub.status.idle":"2022-01-07T15:09:08.735418Z","shell.execute_reply.started":"2022-01-07T15:09:04.712298Z","shell.execute_reply":"2022-01-07T15:09:08.733902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('week')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:08.736854Z","iopub.execute_input":"2022-01-07T15:09:08.737375Z","iopub.status.idle":"2022-01-07T15:09:12.364381Z","shell.execute_reply.started":"2022-01-07T15:09:08.737335Z","shell.execute_reply":"2022-01-07T15:09:12.363318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('day')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:12.365881Z","iopub.execute_input":"2022-01-07T15:09:12.366343Z","iopub.status.idle":"2022-01-07T15:09:16.378414Z","shell.execute_reply.started":"2022-01-07T15:09:12.366307Z","shell.execute_reply":"2022-01-07T15:09:16.376935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('dayofweek')['num_sold'].sum().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:16.379672Z","iopub.execute_input":"2022-01-07T15:09:16.380394Z","iopub.status.idle":"2022-01-07T15:09:19.886499Z","shell.execute_reply.started":"2022-01-07T15:09:16.380341Z","shell.execute_reply":"2022-01-07T15:09:19.88583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"festivities = pd.read_csv(\"../input/festivities-in-finland-norway-sweden-tsp-0122/nordic_holidays.csv\",\n                          parse_dates=['date'],\n                          usecols=['date', 'country', 'holiday'])","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:19.887798Z","iopub.execute_input":"2022-01-07T15:09:19.88852Z","iopub.status.idle":"2022-01-07T15:09:19.906591Z","shell.execute_reply.started":"2022-01-07T15:09:19.888481Z","shell.execute_reply":"2022-01-07T15:09:19.905876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdp = pd.read_csv(\"../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\ngdp = np.concatenate([gdp[['year', 'GDP_Finland']].values, \n                      gdp[['year', 'GDP_Norway']].values, \n                      gdp[['year', 'GDP_Sweden']].values])\ngdp = pd.DataFrame(gdp, columns=['year', 'gdp'])\ngdp['country'] = ['Finland']*5 + ['Norway']*5 +['Sweden']*5","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:19.908049Z","iopub.execute_input":"2022-01-07T15:09:19.908325Z","iopub.status.idle":"2022-01-07T15:09:19.923114Z","shell.execute_reply.started":"2022-01-07T15:09:19.90829Z","shell.execute_reply":"2022-01-07T15:09:19.922379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\ndef process_data(df):\n    \n    processed = dict()\n    processed['row_id'] = df['row_id']\n    \n    print(\"creating dummies for main effects of time, country, store and product\")\n    to_dummies = ['country', 'store', 'product', 'month', 'week', 'day', 'dayofweek']\n    for feat in to_dummies:\n        tmp = pd.get_dummies(df[feat])\n        for col in tmp.columns:\n            processed[feat+'_'+str(col)] = tmp[col]\n    \n    print(\"creating dummies with 7 gg halo effect for Nordic holidays\")\n    tmp = pd.get_dummies(\n        df.merge(festivities, on=['date', 'country'], how='left').sort_values('row_id')['holiday'])\n    for col in tmp.columns:\n            peak = tmp[col].values + tmp[col].rolling(7).mean().fillna(0).values\n            processed['holiday_'+str(col)] =  peak\n    \n    print(\"creating interactions\")\n    high_lvl_interactions = [\n        ['country', 'product', 'month'],\n        ['country', 'product', 'week'],\n        ['country', 'store', 'week'],\n        ['country', 'product', 'month', 'day'],\n        ['country', 'product', 'month', 'dayofweek'],\n    ]\n    for sel in high_lvl_interactions:\n        tmp = pd.get_dummies(df[sel].apply(lambda row: '_'.join(row.values.astype(str)), axis=1))\n        for col in tmp.columns:\n            processed[col] = tmp[col]\n            \n    print(\"modelling time as continuous per each country\")\n    for country in ['Finland', 'Norway', 'Sweden']:\n        processed[country + '_prog'] = ((df.row_id // 18) + 1) * (df['country']==country).astype(int)\n        processed[country + '_prog^2'] = (processed[country + '_prog']**2)\n        processed[country + '_prog^3'] = (processed[country + '_prog']**3)\n        \n    print(\"adding gdp\")\n    gdp_countries = df.merge(gdp, on=['country', 'year'], how='left')['gdp'].values\n    for country in ['Finland', 'Norway', 'Sweden']:\n        processed['gdp_'+ country] = gdp_countries * (df['country']==country).astype(int)\n            \n    print(f\"completed processing {len(processed)-1} features\")\n    \n    values = list()\n    columns = list()\n    for key, value in processed.items():\n        values.append(np.array(value))\n        columns.append(key)\n        \n    values = np.array(values).T        \n    processed = pd.DataFrame(values, columns=columns)\n    \n    print(\"resorting row ids\")\n    processed = processed.sort_values('row_id').set_index('row_id')\n    return processed\n\ndef process_target(df):\n    target = pd.DataFrame({'row_id':df['row_id'], 'num_sold':df['num_sold']})\n    target = target.sort_values('row_id').set_index('row_id')\n    return target\n\ntrain_test = process_data(train.append(test))\n\nprocessed_train = train_test.iloc[:len(train)].copy()\nprocessed_test = train_test.iloc[len(train):].copy()\n\ntarget = np.ravel(process_target(train))    ","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:19.926116Z","iopub.execute_input":"2022-01-07T15:09:19.92632Z","iopub.status.idle":"2022-01-07T15:09:26.145744Z","shell.execute_reply.started":"2022-01-07T15:09:19.926295Z","shell.execute_reply":"2022-01-07T15:09:26.144913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighting(df, weights):\n    return df.year.replace(weights).values\n    \nweights = weighting(train, {2015:0.125, 2016:0.25, 2017:0.5, 2018:1})","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:26.146936Z","iopub.execute_input":"2022-01-07T15:09:26.147324Z","iopub.status.idle":"2022-01-07T15:09:26.169033Z","shell.execute_reply.started":"2022-01-07T15:09:26.147289Z","shell.execute_reply":"2022-01-07T15:09:26.16807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SMAPE(y_true, y_pred):\n    # From https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_exp(y_true, y_pred):\n    y_true = np.exp(y_true)\n    y_pred = np.exp(y_pred)\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_err(y_true, y_pred):\n    # From https://www.kaggle.com/cpmpml/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return diff","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:26.170568Z","iopub.execute_input":"2022-01-07T15:09:26.171101Z","iopub.status.idle":"2022-01-07T15:09:26.184243Z","shell.execute_reply.started":"2022-01-07T15:09:26.171061Z","shell.execute_reply":"2022-01-07T15:09:26.183367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# due to float calculations the computation is approximated\na = 5\nb = 7\nprint(a * b) # pure multiplicative\nprint(np.exp(np.log(a) + np.log(b))) # multiplicative made additive by log","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:26.188312Z","iopub.execute_input":"2022-01-07T15:09:26.188531Z","iopub.status.idle":"2022-01-07T15:09:26.203529Z","shell.execute_reply.started":"2022-01-07T15:09:26.188504Z","shell.execute_reply":"2022-01-07T15:09:26.201595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/tabular-playground-series-jan-2022/sample_submission.csv\")\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T15:09:46.569762Z","iopub.execute_input":"2022-01-07T15:09:46.57001Z","iopub.status.idle":"2022-01-07T15:09:46.593747Z","shell.execute_reply.started":"2022-01-07T15:09:46.569982Z","shell.execute_reply":"2022-01-07T15:09:46.593073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Did it work?\nThe notebook goes together with the EDA notebook, which visualizes the various seasonal effects and the differences in growth rate. Scikit-learn doesn't offer SMAPE as a loss function. As a workaround, I'm training for Huber loss with a transformed target, apply a correction factor, and we'll see how far we'll get.\n\nThe transformed target for the regression is the log of the sales numbers.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach? \nLook at a notebook which presents feature engineering (based on the insights of this EDA) and a linear model which makes use of the features.","metadata":{}},{"cell_type":"markdown","source":"Credit goes to :\n- Festivities in Finland, Norway, Sweden (TSP 01-22)\n- GDP 2015-2019: Finland, Norway, and Sweden\n\n**Thanks for this datasets!**","metadata":{}}]}