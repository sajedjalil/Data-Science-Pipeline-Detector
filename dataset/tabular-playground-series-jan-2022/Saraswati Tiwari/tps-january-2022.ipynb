{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What are you trying to do in this notebook?\nFor this challenge, we will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow us to try numerous different modeling approaches.\n\n#### Why are you trying it?\nThere are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. we want to figure out which of the store chains(KaggleMart or KaggleRama) would have the best sales going forward.\n\nFiles\n\n- train.csv - the training set, which includes the sales data for each date-country-store-item combination.\n- test.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\n- sample_submission.csv - a sample submission file in the correct format.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:50.597313Z","iopub.execute_input":"2022-01-06T09:16:50.598172Z","iopub.status.idle":"2022-01-06T09:16:50.631448Z","shell.execute_reply.started":"2022-01-06T09:16:50.598061Z","shell.execute_reply":"2022-01-06T09:16:50.63062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date, timedelta\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, HuberRegressor","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:50.633244Z","iopub.execute_input":"2022-01-06T09:16:50.633506Z","iopub.status.idle":"2022-01-06T09:16:51.545286Z","shell.execute_reply.started":"2022-01-06T09:16:50.63347Z","shell.execute_reply":"2022-01-06T09:16:51.544492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_train_df = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\noriginal_test_df = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\ngdp_df = pd.read_csv('../input/gdp-20152019-finland-norway-and-sweden/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n\ngdp_df.set_index('year', inplace=True)\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)\noriginal_train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:51.548383Z","iopub.execute_input":"2022-01-06T09:16:51.549021Z","iopub.status.idle":"2022-01-06T09:16:51.656778Z","shell.execute_reply.started":"2022-01-06T09:16:51.548987Z","shell.execute_reply":"2022-01-06T09:16:51.65606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n\n#print(smape_loss(tf.constant([1, 2]), tf.constant([3, 4]))) # should print [100, 66.6667]","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:51.65791Z","iopub.execute_input":"2022-01-06T09:16:51.658164Z","iopub.status.idle":"2022-01-06T09:16:51.662525Z","shell.execute_reply.started":"2022-01-06T09:16:51.658128Z","shell.execute_reply":"2022-01-06T09:16:51.661813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    new_df = pd.DataFrame({#'year': df.date.dt.year, # This feature makes it possible to fit an annual growth rate\n                           'dayofyear': df.date.dt.dayofyear,\n                           'gdp': df.apply(get_gdp, axis=1),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n        #new_df[f\"{country}_year\"] = (df.country == country) * (df.date.dt.year - 2016)\n        #new_df[f\"{country}_peak_year\"] = (df.country == country) * (new_df.dec29 | new_df.dec30 | new_df.easter_week) * (df.date.dt.year - 2016)\n        \n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 20):\n        new_df[f'sin{k}'] = np.sin(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear / 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n#test_df.year = 2018 # no growth patch, see https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298318\n\nfeatures = test_df.columns\n\nfor df in [train_df, test_df]:\n    df[features] = df[features].astype(np.float32)\nprint(list(features))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:51.664735Z","iopub.execute_input":"2022-01-06T09:16:51.665241Z","iopub.status.idle":"2022-01-06T09:16:53.64615Z","shell.execute_reply.started":"2022-01-06T09:16:51.665184Z","shell.execute_reply":"2022-01-06T09:16:53.644447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(X_tr, X_va=None, outliers=False):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data\n    X_tr_f = X_tr[features]\n    preproc = StandardScaler()\n    X_tr_f = preproc.fit_transform(X_tr_f)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n    \n    # Train the model\n    #model = LinearRegression() # 5.80558\n    model = HuberRegressor(epsilon=1.20, max_iter=500) # 5.80143 (epsilon=1.20) *****************\n    model.fit(X_tr_f, np.log(y_tr).ravel())\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = X_va[features]\n        X_va_f = preproc.transform(X_va_f)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n\n        # Inference for validation\n        y_va_pred = np.exp(model.predict(X_va_f)).reshape(-1, 1)\n        \n        # Evaluation: Execution time and SMAPE\n        smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n        #y_va_pred *= LOSS_CORRECTION\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}   (before correction: {smape_before_correction:.5f})\")\n        print(np.mean(smape_loss(y_va, y_va_pred)))\n        \n        # Plot y_true vs. y_pred\n        plt.figure(figsize=(10, 10))\n        plt.scatter(y_va, y_va_pred, s=1, color='r')\n        #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n        plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n        plt.gca().set_aspect('equal')\n        plt.xlabel('y_true')\n        plt.ylabel('y_pred')\n        plt.title('OOF Predictions')\n        plt.show()\n        \n    return preproc, model\n\npreproc, model = fit_model(train_df)\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df['num_sold'] = np.exp(model.predict(preproc.transform(demo_df[features])))\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n    plt.legend()\n    plt.title('Predictions and true num_sold for five years')\n    plt.show()\n\nplot_five_years_combination(engineer)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:53.647616Z","iopub.execute_input":"2022-01-06T09:16:53.648066Z","iopub.status.idle":"2022-01-06T09:16:57.933514Z","shell.execute_reply.started":"2022-01-06T09:16:53.648016Z","shell.execute_reply":"2022-01-06T09:16:57.932741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['pred'] = np.exp(model.predict(preproc.transform(train_df[features])))\nby_date = train_df.groupby(level='date')\nresiduals = (by_date.pred.sum() - by_date.num_sold.sum()) / (by_date.pred.sum() + by_date.num_sold.sum()) * 200\n\n# Plot all residuals (four-year range, sum of all products)\ndef plot_all_residuals(residuals):\n    plt.figure(figsize=(20,6))\n    plt.scatter(residuals.index,\n                residuals,\n                s=1, color='k')\n    plt.vlines(pd.date_range('2015-01-01', '2019-01-01', freq='M'),\n               plt.ylim()[0], plt.ylim()[1], alpha=0.5)\n    plt.vlines(pd.date_range('2015-01-01', '2019-01-01', freq='Y'),\n               plt.ylim()[0], plt.ylim()[1], alpha=0.5)\n    plt.title('Residuals for four years')\n    plt.show()\n    \nplot_all_residuals(residuals)\n\n# Plot residuals for interesting intervals\ndef plot_around(residuals, m, d, w):\n    \"\"\"Plot residuals in an interval of with 2*w around month=m and day=d\"\"\"\n    plt.figure()\n    plt.title(f\"Residuals around m={m} d={d}\")\n    for y in np.arange(2015, 2020):\n        d0 = pd.Timestamp(date(y, m, d))\n        residual_range = residuals[(residuals.index > d0 - timedelta(w)) & \n                                   (residuals.index < d0 + timedelta(w))]\n        plt.plot([(r - d0).days for r in residual_range.index], residual_range, label=str(y))\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\n    plt.legend()\n    plt.show()\n\nplot_around(residuals, 1, 1, 20) # end of year peak\nplot_around(residuals, 5, 1, 50) # three moveable peaks depending on Easter\n#plot_around(residuals, 5, 21, 10) # zoom-in\n#plot_around(residuals, 5, 31, 15) # zoom-in\nplot_around(residuals, 6, 10, 10) # first half of June (with overlay of Pentecost in 2017)\nplot_around(residuals, 6, 30, 10) # moveable peak end of June\nplot_around(residuals, 11, 5, 10) # moveable peak beginning of November","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:57.934564Z","iopub.execute_input":"2022-01-06T09:16:57.934801Z","iopub.status.idle":"2022-01-06T09:16:59.395845Z","shell.execute_reply.started":"2022-01-06T09:16:57.934767Z","shell.execute_reply":"2022-01-06T09:16:59.3951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature engineering\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)})],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) \n                                      for d in range(1, 13)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10)) + list(range(17, 25))})],\n                       axis=1)\n    \n    # June\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) \n                                      for d in list(range(6, 14))})],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-5, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(0, 10))})],\n                       axis=1)\n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(0, 9)) + list(range(50, 60)) + list(range(40, 46))})],\n                       axis=1)\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * (df.date.dt.year - 2016)\n    #    new_df[f\"{country}_peak_year\"] = (df.country == country) * (new_df.dec29 | new_df.dec30 | new_df.easter_week) * (df.date.dt.year - 2016)\n        \n    return new_df.astype(np.float32)\n\ntrain_df = engineer_more(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer_more(original_test_df)\ntest_df.year = 2018 # no growth patch, see https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/298318\n\nfeatures = test_df.columns\nprint(list(features))","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:16:59.397465Z","iopub.execute_input":"2022-01-06T09:16:59.397717Z","iopub.status.idle":"2022-01-06T09:17:02.222929Z","shell.execute_reply.started":"2022-01-06T09:16:59.397683Z","shell.execute_reply":"2022-01-06T09:17:02.221728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preproc, model = fit_model(train_df)\ntrain_df['pred'] = np.exp(model.predict(preproc.transform(train_df[features])))\nby_date = train_df.groupby(level='date')\nresiduals = (by_date.pred.sum() - by_date.num_sold.sum()) / (by_date.pred.sum() + by_date.num_sold.sum()) * 200\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\nplot_five_years_combination(engineer_more)\n\n# Plot all residuals (four-year range, sum of all products)\nplot_all_residuals(residuals)\n\n# Plot residuals for interesting intervals\nplot_around(residuals, 1, 1, 20) # end of year peak\nplot_around(residuals, 5, 1, 50) # three moveable peaks depending on Easter\n#plot_around(residuals, 5, 21, 10) # zoom-in\n#plot_around(residuals, 5, 31, 15) # zoom-in\nplot_around(residuals, 6, 10, 10) # first half of June (with overlay of Pentecost in 2017)\nplot_around(residuals, 6, 30, 10) # moveable peak end of June\nplot_around(residuals, 11, 5, 10) # moveable peak beginning of November","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:17:02.224346Z","iopub.execute_input":"2022-01-06T09:17:02.2246Z","iopub.status.idle":"2022-01-06T09:17:15.20776Z","shell.execute_reply.started":"2022-01-06T09:17:02.224565Z","shell.execute_reply":"2022-01-06T09:17:15.207081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nOUTLIERS = True\nTRAIN_VAL_CUT = datetime(2018, 1, 1)\nLOSS_CORRECTION = 1 # correction factor between Huber loss and SMAPE: 1.035 ( for linear regression with MSE use 1.038)\n\n# Make the results reproducible\nnp.random.seed(202100)\n\ntotal_start_time = datetime.now()\nfor run in range(RUNS):\n    fold = 0\n    train_idx = np.arange(len(train_df))[train_df.date < TRAIN_VAL_CUT]\n    val_idx = np.arange(len(train_df))[train_df.date > TRAIN_VAL_CUT]\n    print(f\"Fold {run}.{fold}\")\n    X_tr = train_df.iloc[train_idx]\n    X_va = train_df.iloc[val_idx]\n    \n    preproc, model = fit_model(X_tr, X_va)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:17:15.209084Z","iopub.execute_input":"2022-01-06T09:17:15.209345Z","iopub.status.idle":"2022-01-06T09:17:21.631953Z","shell.execute_reply.started":"2022-01-06T09:17:15.20931Z","shell.execute_reply":"2022-01-06T09:17:21.631276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model on the complete training data\ntrain_idx = np.arange(len(train_df))\nX_tr = train_df.iloc[train_idx]\npreproc, model = fit_model(X_tr, None)\n\nplot_five_years_combination(engineer_more) # Quick check for debugging\n\n# Inference for test\ntest_pred_list = []\ntest_pred_list.append(np.exp(model.predict(preproc.transform(test_df[features]))) * LOSS_CORRECTION)\n\nif len(test_pred_list) > 0:\n    # Create the submission file\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) / len(test_pred_list)\n    sub.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n             density=True, label='Training')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201),\n             density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel('num_sold')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:17:21.634254Z","iopub.execute_input":"2022-01-06T09:17:21.634658Z","iopub.status.idle":"2022-01-06T09:17:34.348823Z","shell.execute_reply.started":"2022-01-06T09:17:21.634619Z","shell.execute_reply":"2022-01-06T09:17:34.347928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-01-06T09:17:34.350347Z","iopub.execute_input":"2022-01-06T09:17:34.350716Z","iopub.status.idle":"2022-01-06T09:17:34.365674Z","shell.execute_reply.started":"2022-01-06T09:17:34.350667Z","shell.execute_reply":"2022-01-06T09:17:34.363606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Did it work?\nThe notebook goes together with the EDA notebook, which visualizes the various seasonal effects and the differences in growth rate. Scikit-learn doesn't offer SMAPE as a loss function. As a workaround, I'm training for Huber loss with a transformed target, apply a correction factor, and we'll see how far we'll get.\n\nThe transformed target for the regression is the log of the sales numbers.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach? \nLook at a notebook which presents feature engineering (based on the insights of this EDA) and a linear model which makes use of the features.","metadata":{}}]}