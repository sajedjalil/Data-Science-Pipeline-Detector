{"cells":[{"metadata":{"_uuid":"1b8c18e1d49e580b84b194084a08d59e2a497b54"},"cell_type":"markdown","source":"## Overview"},{"metadata":{"_uuid":"3f0a329467823b3dc6d5f86b79f55500cc44acc8"},"cell_type":"markdown","source":"There are two most obvious network architectures to approach this competition: U-net and SSD. Each of them has pros and cons. In particular, U-net provides a relatively simple way to solve the competition challenge using image segmentation. However, this competition requires prediction of an individual mask for each ship rather than one mask for entire image. Therefore, some creative postprocessing may be needed, especially to separate ships with overlapping masks, if it is even possible. Another drawback is that the data is labeled with using pixelized bounding boxes rather than real ship masks, therefore the score of U-net based models is lowered. Meanwhile, implementation of SSD requires usage of rotating bounding boxes (https://arxiv.org/pdf/1711.09405.pdf) that is not common and, therefore, would take additional efforts for development of the model and the corresponding loss function. In addition, bounding boxes are not provided in this competition and must be generated based on the pixel masks. Nevertheless, this approach is expected to provide higher score than U-net, especially since the data is labeled based on pixelized bounding boxes (I expect organizers used SSD with rotating bounding boxes to label train and test data).\nSince the first approach is more straightforward, I'll begin with presenting a kernel about U-net. In this post I will describe how to use pretrained ResNet34 to build a high accuracy image segmentation model. In particular, after training only a decoder for 1 epoch (15 min) on 256x256 rescaled images, the dice coefficient reaches ~0.8 (IoU ~0.67) that significantly outperforms all publicly available models posted so far in this competition. After training the entire model for 6 additional epochs with learning rate annealing, the dice coefficient reaches ~0.86 (IoU ~0.75). Due to the kernel run time limit, the model is further trained only for two epochs on 384x384 (dice ~0.87) followed by one epoch on 768x768 images. In an independent run I trained a model on 384x384 images for 12 epochs that boosted dice to 0.89 followed by training on full resolution images that increased dice further to 0.905."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from fastai.conv_learner import *\nfrom fastai.dataset import *\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd71a95c05902b29ce4e3768a8127c2a0b7098d"},"cell_type":"markdown","source":"### Data"},{"metadata":{"trusted":true,"_uuid":"b93d5422dfd31f43df9cdd3e301547cc95f25980"},"cell_type":"code","source":"PATH = './'\nTRAIN = '../input/airbus-ship-detection/train_v2/'\nTEST = '../input/airbus-ship-detection/test_v2/'\nSEGMENTATION = '../input/airbus-ship-detection/train_ship_segmentations_v2.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a72e076dc04cf1786edae26a1d2f15ec3de234a"},"cell_type":"code","source":"nw = 2   #number of workers for data loader\narch = resnet34 #specify target architecture","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49199ea17e9e9ba9893c10d06c1fb419e22aeb1b"},"cell_type":"code","source":"train_names = [f for f in os.listdir(TRAIN)]\ntest_names = [f for f in os.listdir(TEST)]\n#5% of data in the validation set is sufficient for model evaluation\ntr_n, val_n = train_test_split(train_names, test_size=0.05, random_state=42)\nsegmentation_df = pd.read_csv(os.path.join(PATH, SEGMENTATION)).set_index('ImageId')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b95102ddd563191f9b15eb115f736e93c837d080"},"cell_type":"markdown","source":"One of the challenges of this competition is strong data unbalance. Even if only images with ships are considered, the ratio of mask pixels to the total number of pixels is ~1:1000. If images with no ships are included, this ratio goes to ~1:10000, which is quite tough to handle. Therefore, I drop all images without ships, that makes the training set more balanced and also reduces the time per each epoch almost by 4 times. In an independent run, when the dice of my model reached 0.895, I ran it on images without ships and identified ~3600 false positive predictions out ~70k images. The incorrectly predicted images were incorporated to the training set as negative examples, and training was continued. The problem of false positive predictions can be further mitigated by stacking U-net model with a classification model predicting if ships are present in a particular image (https://www.kaggle.com/iafoss/fine-tuning-resnet34-on-ship-detection - ~98% accuracy). \nI also noticed that in some kernels the dataset is tried to be balanced by keeping approximately the same number of images with 0, 1, 2, etc. ships. However, this strategy would be effective for such task as ship counting rather than training U-net or SSD.  One possible way to balance the dataset is creative cropping the images that keeps approximately the same number of pixels corresponding to a ship or something else. However, I doubt that such approach will effective in this competition. Therefore, a special loss function must be used to mitigate the data unbalance."},{"metadata":{"trusted":true,"_uuid":"6a3d8c70e03964738322ca99084f088adc1c5f3e"},"cell_type":"code","source":"def cut_empty(names):\n    return [name for name in names \n            if(type(segmentation_df.loc[name]['EncodedPixels']) != float)]\n\ntr_n = cut_empty(tr_n)\nval_n = cut_empty(val_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"505f803555b8d9d0378a73d227da4b8174f2086d"},"cell_type":"code","source":"def get_mask(img_id, df):\n    shape = (768,768)\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    masks = df.loc[img_id]['EncodedPixels']\n    if(type(masks) == float): return img.reshape(shape)\n    if(type(masks) == str): masks = [masks]\n    for mask in masks:\n        s = mask.split()\n        for i in range(len(s)//2):\n            start = int(s[2*i]) - 1\n            length = int(s[2*i+1])\n            img[start:start+length] = 1\n    return img.reshape(shape).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f67a326e1269bffca392fcf4ac10bf4a532ca56b"},"cell_type":"code","source":"class pdFilesDataset(FilesDataset):\n    def __init__(self, fnames, path, transform):\n        self.segmentation_df = pd.read_csv(SEGMENTATION).set_index('ImageId')\n        super().__init__(fnames, transform, path)\n    \n    def get_x(self, i):\n        img = open_image(os.path.join(self.path, self.fnames[i]))\n        if self.sz == 768: return img \n        else: return cv2.resize(img, (self.sz, self.sz),cv2.INTER_AREA)\n    \n    def get_y(self, i):\n        mask = np.zeros((768,768), dtype=np.uint8) if (self.path == TEST) \\\n            else get_mask(self.fnames[i], self.segmentation_df)\n        img = Image.fromarray(mask).resize((self.sz, self.sz),cv2.INTER_AREA).convert('RGB')\n        return np.array(img).astype(np.float32)\n    \n    def get_c(self): return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"835b75b3508f418bac53ee6725694af208e1f28c"},"cell_type":"code","source":"def get_data(sz,bs):\n    #data augmentation\n    aug_tfms = [RandomRotate(10, tfm_y=TfmType.CLASS),\n                RandomDihedral(tfm_y=TfmType.CLASS),\n                RandomLighting(0.05, 0.05, tfm_y=TfmType.CLASS)]\n    tfms = tfms_from_model(arch, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, \n                aug_tfms=aug_tfms)\n    tr_names = tr_n if (len(tr_n)%bs == 0) else tr_n[:-(len(tr_n)%bs)] #cut incomplete batch\n    ds = ImageData.get_ds(pdFilesDataset, (tr_names,TRAIN), \n                (val_n,TRAIN), tfms, test=(test_names,TEST))\n    md = ImageData(PATH, ds, bs, num_workers=nw, classes=None)\n    return md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b77a79e1e1e3f4c8a9e7120feb9a843b41492f8"},"cell_type":"markdown","source":"### Model"},{"metadata":{"_uuid":"cc66537bd5382d9e874cfa8c0616965e8d9f5f7d"},"cell_type":"markdown","source":"The model used in this kernel is inspired by a Carvana example from FastAI course (http://course.fast.ai/index.html). It is composed of a ResNet34 based encoder and a simple upsampling decoder. Similar to the original U-net, skip connections are added between encoder and decoder to facilitate the information flow at different detalization levels. Meanwhile, using a pretrained ResNet34 model allows us to have a powerful encoder capable of handling elaborated feature, in comparison with the original U-net, without a risk of overfitting and necessity of training a big model from scratch. The total capacity of the model is ~21M parameters. Before using, the original ResNet34 model was further fine-tuned on ship/no-ship classification task (https://www.kaggle.com/iafoss/fine-tuning-resnet34-on-ship-detection)."},{"metadata":{"trusted":true,"_uuid":"29bc49d15a3b92692463c5b1ec17b96f353a95c2"},"cell_type":"code","source":"cut,lr_cut = model_meta[arch]\ndef get_base(pre=True):                   #load ResNet34 model\n    layers = cut_model(arch(pre), cut)\n    return nn.Sequential(*layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77ca73006aec0c9dfe4be6deb2d6cf524840cd62"},"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, x_in, filters=64):\n        super().__init__()\n        self.conv1 = nn.Conv2d(x_in,filters,1)\n        self.bn1 = nn.BatchNorm2d(filters)\n        self.conv2 = nn.Conv2d(filters,filters,(3,3),padding=1)\n        self.bn2 = nn.BatchNorm2d(filters)\n        self.conv3 = nn.Conv2d(filters,x_in,1)\n        \n    def forward(self, x):\n        r = self.conv1(x)\n        r = F.relu(r)\n        r = self.bn1(r)\n        r = self.conv2(r)\n        r = F.relu(r)\n        r = self.bn2(r)\n        r = self.conv3(r)\n        return x + r\n    \nclass SEBlock(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                nn.Sigmoid())\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\nclass UnetBlock(nn.Module):\n    def __init__(self, up_in, x_in, n_out, dropout=0.0):\n        super().__init__()\n        up_out = x_out = n_out//2\n        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n        self.cat_conv = nn.Conv2d(n_out, n_out, (3,3), padding=(1,1))\n        self.bn = nn.BatchNorm2d(n_out)\n        self.se = SEBlock(n_out)\n        self.dropout = nn.Dropout2d(dropout)\n        self.r = ResBlock(n_out,n_out//2)\n        #self.conv1 = nn.Conv2d(2*n_out,n_out,1)\n        #self.conv2 = nn.Conv2d(n_out,n_out,1)\n        #self.bn_g = nn.BatchNorm2d(n_out)\n        #self.GLU = nn.GLU(dim=1)\n        #self.conv1 = nn.Conv2d(n_out,n_out,(3,3), padding=(1,1))\n        #self.conv2 = nn.Conv2d(n_out,n_out,(3,3), padding=(1,1))\n        #self.bn_in = nn.BatchNorm2d(n_out)\n        \n    def forward(self, up_p, x_p):\n        up_p = self.tr_conv(up_p)\n        x_p = self.x_conv(x_p)\n        cat_p = self.dropout(torch.cat([up_p,x_p], dim=1))\n        x = self.bn(F.relu(self.cat_conv(cat_p)))\n        x = self.r(x)\n        #g = self.bn_g(self.conv2(F.relu(self.conv1(cat_p))))\n        #x = self.GLU(torch.cat([x_p,g], dim=1))\n        x = self.se(x)\n        return x\n\nclass SaveFeatures():\n    features=None\n    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output): self.features = output\n    def remove(self): self.hook.remove()\n    \nclass Unet34SE_hc(nn.Module):\n    def __init__(self, rn, dropout = 0.0):\n        super().__init__()\n        self.rn = rn\n        self.hc_sz = 16\n        self.filters = [64,64,128,256,512]\n        self.u_out = [self.hc_sz,128,128,256,256]\n        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n        self.up1 = UnetBlock(self.filters[4],self.filters[3],self.u_out[4],dropout)\n        self.up2 = UnetBlock(self.u_out[4],self.filters[2],self.u_out[3],dropout)\n        self.up3 = UnetBlock(self.u_out[3],self.filters[1],self.u_out[2],dropout/2)\n        self.up4 = UnetBlock(self.u_out[2],self.filters[0],self.u_out[1])\n        self.up5 = nn.ConvTranspose2d(self.u_out[1], self.u_out[0], 2, stride=2)\n        self.hc_neck1 = nn.Conv2d(self.u_out[4], self.hc_sz, 1)\n        self.hc_neck2 = nn.Conv2d(self.u_out[3], self.hc_sz, 1)\n        self.hc_neck3 = nn.Conv2d(self.u_out[2], self.hc_sz, 1)\n        self.hc_neck4 = nn.Conv2d(self.u_out[1], self.hc_sz, 1)\n        self.head = nn.Sequential(nn.Conv2d(5*self.hc_sz,2*self.hc_sz,3,padding=1),\n                                 nn.ReLU(inplace=True),\n                                 nn.Conv2d(2*self.hc_sz,1,1))\n        \n    def forward(self,x):\n        x = F.relu(self.rn(x))\n        x = self.up1(x, self.sfs[3].features)\n        hc1 = self.hc_neck1(x)\n        x = self.up2(x, self.sfs[2].features)\n        hc2 = self.hc_neck2(x)\n        x = self.up3(x, self.sfs[1].features)\n        hc3 = self.hc_neck3(x)\n        x = self.up4(x, self.sfs[0].features)\n        hc4 = self.hc_neck4(x)\n        x = self.up5(x)\n        x = torch.cat((x,\n            F.interpolate(hc4,scale_factor=2,mode='bilinear'),\n            F.interpolate(hc3,scale_factor=4,mode='bilinear'),\n            F.interpolate(hc2,scale_factor=8,mode='bilinear'),\n            F.interpolate(hc1,scale_factor=16,mode='bilinear'),\n            ),dim=1)\n        x = self.head(x)\n        return x[:,0]\n    \n    def close(self):\n        for sf in self.sfs: sf.remove()\n            \nclass UnetModel():\n    def __init__(self,model,name='Unet'):\n        self.model,self.name = model,name\n\n    def get_layer_groups(self, precompute):\n        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n        return lgs + [children(self.model)[1:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c71a3af2a83b1122902a3f39e6fda1af5afbb99"},"cell_type":"markdown","source":"### Loss function"},{"metadata":{"_uuid":"19d0fe1a3e00cea135163b5540338e2744615c3d"},"cell_type":"markdown","source":"Loss function is one of the most crucial parts of the completion. Due to strong data unbalance, simple loss functions, such as Binary Cross-Entropy loss, do not really work. Soft dice loss can be helpful since it boosts prediction of correct masks, but it leads to unstable training. Winners of image segmentation challenges typically combine BCE loss with dice (http://blog.kaggle.com/2017/12/22/carvana-image-masking-first-place-interview/). Similar loss function is used in publicly available models in this completion. I would agree that this combined loss function works perfectly for Carvana completion, where the number of pixels in the mask is about half of the total number of pixels. However, 1:1000 pixel unbalance deteriorates training with BCE. \nIf one tries to recall what is the loss function that should be used for strongly unbalanced data set, it is focal loss (https://arxiv.org/pdf/1708.02002.pdf), which revolutionized one stage object localization method in 2017. This loss function demonstrates amazing results on datasets with unbalance level 1:10-1000. In addition to focal loss, I include -log(soft dice loss). Log is important in the convex of the current competition since it boosts the loss for the cases when objects are not detected correctly and dice is close to zero. It allows to avoid false negative predictions or completely incorrect masks for images with one ship (the major part of the training set). Also, since the loss for such objects is very high, the model more effectively incorporates the knowledge about such objects and handles them even in images with multiple ships. To bring two losses to similar scale, focal loss is multiplied by 10. The implementation of focal loss is borrowed from https://becominghuman.ai/investigating-focal-and-dice-loss-for-the-kaggle-2018-data-science-bowl-65fb9af4f36c ."},{"metadata":{"trusted":true,"_uuid":"8de4222fd28596dbae5d55f8172ac28271f678f2"},"cell_type":"code","source":"def dice_loss(input, target):\n    input = torch.sigmoid(input)\n    smooth = 1.0\n\n    iflat = input.view(-1)\n    tflat = target.view(-1)\n    intersection = (iflat * tflat).sum()\n    \n    return ((2.0 * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"456acf8ffe7b80cd94eb73bfaeb6b1061bbb44c7"},"cell_type":"code","source":"class FocalLoss(nn.Module):\n    def __init__(self, gamma):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, input, target):\n        if not (target.size() == input.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + \\\n            ((-max_val).exp() + (-input - max_val).exp()).log()\n\n        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        \n        return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cfb00a50821861c69534ee7398bd220c02900d8"},"cell_type":"code","source":"class MixedLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n        \n    def forward(self, input, target):\n        loss = self.alpha*self.focal(input, target) - torch.log(dice_loss(input, target))\n        return loss.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37baddf38bb569cbf2d2f186a5171767402b94fa"},"cell_type":"code","source":"def dice(pred, targs):\n    pred = (pred>0).float()\n    return 2.0 * (pred*targs).sum() / ((pred+targs).sum() + 1.0)\n\ndef IoU(pred, targs):\n    pred = (pred>0).float()\n    intersection = (pred*targs).sum()\n    return intersection / ((pred+targs).sum() - intersection + 1.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b3a20e5e2fdedf1439737f40c14c78f9a69e29a"},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true,"_uuid":"1a3b8f6b454a8d8b48505fbc5964e900a59ef09e","scrolled":false},"cell_type":"code","source":"m = to_gpu(Unet34SE_hc(get_base(True),0.15))\nmodels = UnetModel(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d52f2e1e45ec597d901a814f592ac153168ebff"},"cell_type":"code","source":"#models.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab92ee6aaa8d31587d1bc6d2cad9e0407444c5de"},"cell_type":"code","source":"sz = 256 #image size\nbs = 24#64  #batch size\n\nmd = get_data(sz,bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb1016214b0de75f56d2036241a27684b4c4bd1b"},"cell_type":"code","source":"learn = ConvLearner(md, models)\nlearn.opt_fn=optim.Adamax\nlearn.clip = 1.0\nlearn.crit = MixedLoss(10.0, 2.0)\nlearn.metrics=[accuracy_thresh(0.5),dice,IoU]\nwd=1e-7\nlr = 2.5e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"556805d33fafba8284a911c91dd347c926aef55b"},"cell_type":"code","source":"learn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4d4aa02ec617bdf974fcdd77497af1ce5d9993aa"},"cell_type":"code","source":"#learn.freeze_to(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d2d28f3482827318f673aee6fe3e60e2cf0ac44"},"cell_type":"markdown","source":"Training only the decoder part for 1 epoch (15 min) leads to ~0.8 dice that outperforms all publicly available models in this competition."},{"metadata":{"trusted":true,"_uuid":"fd9510104082f97119adebc00bb16750710266a2","scrolled":false},"cell_type":"code","source":"learn.fit(lr,1,wds=wd,cycle_len=1,use_clr=(5,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a06ef9bc205d434ef5be901ea489a368e24d1435"},"cell_type":"code","source":"learn.save('Unet34r_256_0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1288146ed507f7a6876d756e5472f8dda15cc58"},"cell_type":"markdown","source":"Unfreeze the model and train it with differential learning rate. The lr of the head part is still 1e-3, while the middle layers of the model are trained with 1e-4 lr, and the base is trained with even smaller lr, 1e-5, since low level detectors do not vary much from one image data set to another."},{"metadata":{"trusted":true,"_uuid":"eef154a2aa260f370c9665ec804b1abecdef3ccf"},"cell_type":"code","source":"lrs = np.array([lr/100,lr/10,lr])\nlearn.unfreeze() #unfreeze the encoder\nlearn.bn_freeze(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8498665da79f8a147b0599142bb4034e399c38aa"},"cell_type":"code","source":"learn.fit(lrs,2,wds=wd,cycle_len=1,use_clr=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8d9a6f6ecb66054d32e8c41d316672dbd9bb03b","collapsed":true},"cell_type":"code","source":"learn.fit(lrs/3,2,wds=wd,cycle_len=2,use_clr=(20,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c2b287d62042002e17cf7797ae48ccd05dd36cd"},"cell_type":"markdown","source":"The training has been run with learning rate annealing. Periodic lr increase followed by slow decrease drives the system out of steep minima (when lr is high) towards broader ones (which are explored when lr decreases) that enhances the ability of the model to generalize and reduces overfitting."},{"metadata":{"trusted":true,"_uuid":"1fd829270a788812cb9f3f7f5fe7c6e43d934ab7","collapsed":true},"cell_type":"code","source":"learn.sched.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16ea8e6c7a593be13aad3cc6fe88e4cb657db35d"},"cell_type":"markdown","source":"Saved model can be ued for further training or for making predictions."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df25dc961326f3e554b9c4d38525705664ec5075"},"cell_type":"code","source":"learn.save('Unet34r_256_1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5f92ae7e3e348f95129a3742ad76e4a502bf6d1"},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fb4a8386560ebde1f2277d94ecb5ee4a1bbc50d2"},"cell_type":"code","source":"def Show_images(x,yp,yt):\n    columns = 3\n    rows = min(bs,8)\n    fig=plt.figure(figsize=(columns*4, rows*4))\n    for i in range(rows):\n        fig.add_subplot(rows, columns, 3*i+1)\n        plt.axis('off')\n        plt.imshow(x[i])\n        fig.add_subplot(rows, columns, 3*i+2)\n        plt.axis('off')\n        plt.imshow(yp[i])\n        fig.add_subplot(rows, columns, 3*i+3)\n        plt.axis('off')\n        plt.imshow(yt[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0206fb587e6bb9ddfc7366f6d04e47101b275514"},"cell_type":"code","source":"learn.model.eval();\nx,y = next(iter(md.val_dl))\nyp = to_np(F.sigmoid(learn.model(V(x))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cda6a85fcbe5263406777a19729d0aec890ab99","collapsed":true},"cell_type":"code","source":"Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9850c2014ba1980bc0cca9b9a1177af5caa24da0"},"cell_type":"markdown","source":"The results are not ideal, but almost all ships are captured correctly even if the model is making the prediction on a very low resolution (256x256) images."},{"metadata":{"_uuid":"8f0b3a89ccb06468283d8e2611c3981895aae24a"},"cell_type":"markdown","source":"### Training (384x384)"},{"metadata":{"_uuid":"43deeffeee35bde4b11ae3a1aae13ecfabeebf4c"},"cell_type":"markdown","source":"Fortunately, modern convolutional nets support input images of arbitrary resolution. To decrease the training time, one can start training the model on low resolution images first and continue training on higher resolution images for fewer epochs. In addition, a model pretrained on low resolution images first generalizes better since a pixel information is less available and high order features are tended to be used."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"99f0972e31576ba82335a54825cc71853e7a5afd"},"cell_type":"code","source":"sz = 384 #image size\nbs = 16  #batch size\n\nmd = get_data(sz,bs)\nlearn.set_data(md)\nlearn.unfreeze()\nlearn.bn_freeze(True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cc8a6dd9fbf4d03893088d3be3e259942089d30"},"cell_type":"markdown","source":"Due to the kernel run time limit, the model was further trained only for two epochs on 384x384 (dice ~0.87) followed by one epoch on 768x768 images. In an independent run I trained a model on 384x384 images for 12 epochs that boosted dice to 0.89 followed by training on full resolution images that increased dice further to 0.905."},{"metadata":{"trusted":true,"_uuid":"5d40a6e24844002d52bb844d2a4873c164d06e39","collapsed":true},"cell_type":"code","source":"#learn.fit(lrs/5,1,wds=wd,cycle_len=2,use_clr=(10,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fce1cccfb4fd63853d31ae3af9f2e2897ae1e802"},"cell_type":"code","source":"#learn.save('Unet34r_384_1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e129b20579e8dff1523f9da36e4b54bcbde9b58"},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":true,"_uuid":"c92425b7455925580e6d03b373babaebc261047e","collapsed":true},"cell_type":"code","source":"learn.model.eval();\nx,y = next(iter(md.val_dl))\nyp = to_np(F.sigmoid(learn.model(V(x))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"507ef05ee2301e0ebcc69a679a4eb57a99a88e4f"},"cell_type":"code","source":"Show_images(np.asarray(md.val_ds.denorm(x)), yp, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a2b5a8a6d4826f8ef2ccb1b1e26c749edae3aa9"},"cell_type":"markdown","source":"### Training (768x768)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82744b9640856bc71a861240263f8d314544ebe6"},"cell_type":"code","source":"#sz = 768 #image size\n#bs = 6  #batch size\n\n#md = get_data(sz,bs)\n#learn.set_data(md)\n#learn.unfreeze()\n#learn.bn_freeze(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfbb04dc51276c62a97b5a02ff92dbb20b206a3f","collapsed":true},"cell_type":"code","source":"#learn.fit(lrs/10,1,wds=wd,cycle_len=1,use_clr=(10,8))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"addc63f66aa829048a2e285964771967d409aa93"},"cell_type":"markdown","source":"Training for just one epoch is insufficient to achieve high dice. However, if the training is continued, the dice can reach 0.90+."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6bc3ceacb7e616594fd2c8ea960e2e733ba81de"},"cell_type":"code","source":"#learn.save('Unet34_768_1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7584a0ba0ad6667987757dbe5cf3a3dcfea6614"},"cell_type":"markdown","source":"And finally, I put a picture (original image, prediction, ground truth) obtained by the model with dice 0.895 trained further on full resolution images in an independent run. Apart from one tiny ship in the last image, everything is captured. When I zoomed it in, it is really not clear if it is a ship of just a small island: I see only several white pixels, and there are several small islands under the water. Another interesting thing is that the model is able to capture details that it was not trained for. In particular, in 4-th image the model captures antennas (upper right ship) and the shape of ships, even if training set is composed of pixelized bounding boxes."},{"metadata":{"_uuid":"4e23a4bf43cf33af8e36b93035642b30e3853bea"},"cell_type":"markdown","source":"![1](https://image.ibb.co/mrqdze/Ship_Detection.png)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}