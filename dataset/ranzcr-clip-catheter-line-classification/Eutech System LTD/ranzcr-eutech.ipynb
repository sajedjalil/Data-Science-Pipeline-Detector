{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip freeze","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install imutils\n#!pip freeze\n\n#!pip install -r '../input/requirement/requirements.txt'\n#!pip install h5py==2.10.0\n#!pip install tensorflow==2.1.0\n#!pip install keras==2.3.1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train.py**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from PIL import Image\n\nimport pandas as pd\nimport numpy as np\nimport os\n#import keras\nimport random\nimport cv2\nfrom tqdm import tqdm\nimport math\n#import seaborn as sns\n\n#from keras.preprocessing import image\n#from keras.models import load_model\n#from imutils import paths\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.utils import class_weight\n\n#import matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Convolution2D,BatchNormalization\nfrom tensorflow.keras.layers import Flatten,MaxPooling2D,Dropout\n\nfrom tensorflow.keras.applications import DenseNet121, DenseNet201,ResNet50,ResNet152V2\n\nfrom tensorflow.keras.applications.densenet import preprocess_input\n\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator,img_to_array\n\nfrom tensorflow.keras.models import Model, load_model\n\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard, LearningRateScheduler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nconfig = tf.compat.v1.ConfigProto(gpu_options = \n                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n# device_count = {'GPU': 1}\n)\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\ntf.compat.v1.keras.backend.set_session(session)\n\nimport os\n\nBASE_DIR = '../input/ranzcr-clip-catheter-line-classification'\n\n\n\n\ntrain= pd.read_csv(os.path.join(BASE_DIR,'train.csv'))\n\n\ntrain['StudyInstanceUID'] = train['StudyInstanceUID'].apply(lambda x: x +\".jpg\")\n\n\n\nprint(int(len(train)*0.9))\ntrain_split = int(len(train)*0.9)\nval_split=int(len(train)*0.1)\n#test_split= int(len(train)*0.2)\n\ntrain_data = train[:train_split]\n\nval_data = train[train_split:train_split+val_split]\n\n#test_data = train[train_split+val_split:]\n\n\n\ntarget_cols = ['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal', 'NGT - Abnormal', \n               'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', 'CVC - Abnormal',\n               'CVC - Borderline', 'CVC - Normal', 'Swan Ganz Catheter Present']\n\n'''\nfor c in target_cols:\n    plt.hist(train_data[c].values)\n    plt.title(f'target: {c}')\n    plt.show()\n'''  \n  \n# random labels_dict\nlabels_dict = train[target_cols].value_counts().to_dict()\n\n\n\ncls_weight = {}\nfor i in range(len(target_cols)):\n    single_cls_weight={}\n    for j in range(len(np.unique(train_data[target_cols[i]]))):\n        single_cls_weight[np.unique(train_data[target_cols[i]])[j]]=class_weight.compute_class_weight('balanced',\n                                 np.unique(train_data[target_cols[i]]),\n                                 train_data[target_cols[i]])[j]\n    cls_weight[i] = single_cls_weight\n    \n    \n    \n#print(cls_weight)\n\n\ntrainAug = ImageDataGenerator(\n  rescale=1/255.0,\n  rotation_range=30,\n  zoom_range=0.2,\n  width_shift_range=0.1,\n  height_shift_range=0.1,\n  shear_range=0.05,\n  horizontal_flip=True,\n  vertical_flip=True,\n  fill_mode=\"nearest\")\n\nvalidationAug = ImageDataGenerator(rescale=1./255)\n\n\ntrainGen = trainAug.flow_from_dataframe(\n    train_data, directory=os.path.join(BASE_DIR,'train'), x_col='StudyInstanceUID', y_col=target_cols,\n    class_mode='raw',\n    weight_col=None, target_size=(256, 256), color_mode='rgb',\n    classes=None, batch_size=64, shuffle=True,\n    seed=None, save_to_dir=None, save_prefix='',\n    save_format='jpg', subset=None, interpolation='nearest',\n    validate_filenames=True\n)\n\nvalGen = validationAug.flow_from_dataframe(\n    val_data, directory=os.path.join(BASE_DIR,'train'), x_col='StudyInstanceUID', y_col=target_cols,\n    class_mode='raw',\n    weight_col=None, target_size=(256, 256), color_mode='rgb',\n    classes=None, batch_size=64, shuffle=True,\n    seed=None, save_to_dir=None, save_prefix='',\n    save_format='jpg', subset=None, interpolation='nearest',\n    validate_filenames=True\n)\n\n\n\nbase_model=ResNet152V2(weights=None,include_top=False, input_shape = (256,256,3)) \n#base_model.summary()\n\n\nx= base_model.output\n\n\n\n#preds=Dense(14,activation='sigmoid')(x)\n\n#model=Model(inputs=base_model.input,outputs=preds)\n\n#model.summary()\n\n#model.load_weights('brucechou1983_CheXNet_Keras_0.3.0_weights.h5')\n\n#model.summary()\n\n#x = model.output\n\nfor layer in base_model.layers:\n    layer.trainable=False\n\nx=Flatten()(x)\n#new_x  = model.output\nx=Dense(512,activation='relu')(x)\nx=BatchNormalization()(x)\npreds=Dense(11,activation='softmax')(x)\n\nmodel=Model(inputs=base_model.input,outputs=preds)\n    \n#model.summary()\n\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\n\ndef scheduler(epochs):\n    learning_rate = 0.001\n    tf.summary.scalar('learning rate', data=learning_rate, step=epochs)\n    return learning_rate\n\nlrscheduler = LearningRateScheduler(scheduler)\nanne = ReduceLROnPlateau(monitor='lr', factor=0.1, patience=3, verbose=1, min_lr=1e-5)\ncheckpoint = ModelCheckpoint('model_6.h5', verbose=1, save_best_only=True, monitor='val_auc')\n#ts = TensorBoard(log_dir=os.path.join(BASE_DIR, \"log_resnet\"))\n\n#datagen = ImageDataGenerator(zoom_range = 0.2, horizontal_flip=True, shear_range=0.2)\n\n\n#datagen.fit(X_train)\n# Fits-the-model\nhistory = model.fit(\n               trainGen,\n               steps_per_epoch=trainGen.n//trainGen.batch_size,\n               epochs=5,\n               verbose=1,\n               callbacks=[checkpoint,anne,lrscheduler],\n               validation_data=valGen)\n\n\n\n#model = load_model('model_3.h5')\n\n#model.summary()\n\n\n\n#predict the class\n#result = model.predict(test_image)\n#values = prediction_to_tags(result)\n\n#f1_score = f1(test_data,values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\nsubmission_data = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/sample_submission.csv')\ntest_data['StudyInstanceUID'] = test_data['StudyInstanceUID'].apply(lambda x: x +\".jpg\")\n\n\ntestGen = validationAug.flow_from_dataframe(\n    test_data, directory=os.path.join(BASE_DIR,'test'), x_col='StudyInstanceUID', y_col=target_cols,\n    class_mode='raw',\n    weight_col=None, target_size=(224, 224), color_mode='rgb',\n    classes=None, batch_size=64, shuffle=False\n    )\n\n\nevaluation=model.evaluate(testGen,steps=(testGen.n//testGen.batch_size)+1) \n\npred_indices = model.predict(testGen,steps=(testGen.n//testGen.batch_size)+1)\nprediction = pred_indices.round()\n\nsubmission_data[target_cols]=prediction\n\nsubmission_data.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}