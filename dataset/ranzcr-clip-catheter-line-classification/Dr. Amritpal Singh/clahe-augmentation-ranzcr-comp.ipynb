{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CLAHE augmentations\n\nYou can look into my other notebooks to get more detailed augmentation insights.\nlist of my other notebooks - \n\n1. Understanding Image Augmentation - [link](https://www.kaggle.com/amritpal333/understanding-image-augmentation-ranzcr-comp)\n2. CLAHE augmentation  - [link](https://www.kaggle.com/amritpal333/clahe-augmentation-ranzcr-comp)\n3. Findings your own mean,standerd deviation for the dataset - [link](https://www.kaggle.com/amritpal333/using-custom-mean-std-ranzcr-comp)\n\n\n## Upvote the notebooks if you find them insightful."},{"metadata":{},"cell_type":"markdown","source":"# What is CLAHE?\n\n**Contrast Limited Adaptive Histogram Equalization (CLAHE)** to equalize images. \n\nCLAHE is a variant of Adaptive histogram equalization (AHE) which takes care of over-amplification of the contrast. \nOperates on small regions in the image, called tiles, rather than the entire image. \nThe neighboring tiles are then combined using bilinear interpolation to remove the artificial boundaries.\n\n### This algorithm can be applied to improve the contrast of images.\n\n"},{"metadata":{},"cell_type":"markdown","source":"![CLAHE](https://docs.opencv.org/master/histogram_equalization.png)"},{"metadata":{"_uuid":"af89a040-95d5-49ba-bb59-eed78dedff0a","_cell_guid":"7faae6ad-2c75-4b64-aa3f-fae0807a1aee","trusted":true},"cell_type":"code","source":"#!pip install albumentations\nimport albumentations\n\nimport torch\nfrom albumentations import ( Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, IAAAdditiveGaussianNoise, Transpose, ToGray )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport matplotlib.pyplot as plt\n\nseed = 42\n\nimport pandas as pd\nimport os\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Albumentation module\nhttps://albumentations.ai/docs/api_reference/augmentations/transforms"},{"metadata":{"_uuid":"8a98a696-c072-4ebb-ab18-89f167062a99","_cell_guid":"0a640f66-daf1-4038-b127-4d37c172a369","trusted":true},"cell_type":"code","source":"train_path = '../input/ranzcr-clip-catheter-line-classification/train'\ntrain_files = os.listdir(train_path)\n\ntrain_df = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv')\n\ntrain = train_df.reset_index(drop=True) # reset index on both dataframes\n\nprint(train.shape)\n\nnum_channel = 3\n\nimg_size = 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\t#image = image*(1/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 9 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(3,3, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_list = [[ 0.4914168 , 0.4914168 , 0.4914168] , [0.485,0.456,0.406] , [0.9,0.9,0.9] , 0.496 ]\nstd_list = [[0.407278, 0.407278 , 0.407278] , [0.229,0.224,0.225] , [0.9,0.9,0.9] , 0.407278]\n\nmean = mean_list[0]\nstd=  std_list[0]\n\nprint(mean , std)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CLAHE function\n\n> clip_limit - upper threshold value for contrast limiting. If clip_limit is a single float value, the range will be (1, clip_limit). Default: (1, 4).\n\n> tile_grid_size  - size of grid for histogram equalization. Default: (8, 8).\n \n> p  \t- probability of applying the transform. Default: 0.5.\n \nTargets: image"},{"metadata":{},"cell_type":"markdown","source":"# Reference images"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     #albumentations.Normalize(mean= mean ,std= std ,),\n                                    ])\n\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.CLAHE(clip_limit=(1,4), p= 1),\n                                        #albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                                                            ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How far can we go?"},{"metadata":{},"cell_type":"markdown","source":"setting clim limit to 1,40\nand tile_grid_size to 15,15"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.CLAHE(clip_limit=(1,30), tile_grid_size = (10,10) ,p= 1),\n                                   ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These look good to use."},{"metadata":{},"cell_type":"markdown","source":"# Trying differnt combinations of image augmentation\n\n(I hope to find one that magically works!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                #albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.3), lightness=(0.5, 0.7), p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = [0.485,0.456,0.406]\nstd = [0.229,0.224,0.225]\n\ntransform = albumentations.Compose([  albumentations.RandomCrop(width=256, height=256),\n    albumentations.Normalize(mean= mean ,  std= std ,) \n])\n\n# Read an image with OpenCV and convert it to the RGB colorspace\nimage = cv2.imread(\"../input/ranzcr-clip-catheter-line-classification/test/1.2.826.0.1.3680043.8.498.10003659706701445041816900371598078663.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Augment an image\ntransformed = transform(image=image)\ntransformed_image = transformed[\"image\"]\n\n\nfig = plt.figure()\nfig.set_size_inches(15, 15)\n\nax = plt.subplot(1,1, 1)\nplt.tight_layout()\nax.axis('off')\nplt.imshow(image)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}