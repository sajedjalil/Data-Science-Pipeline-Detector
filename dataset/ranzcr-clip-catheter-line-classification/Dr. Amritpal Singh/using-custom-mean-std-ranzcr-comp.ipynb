{"cells":[{"metadata":{"_uuid":"1f69e6b8-5f99-4cac-95e3-38389fbf237a","_cell_guid":"1445c960-a7e6-4740-940e-bafcde383d11","trusted":true},"cell_type":"code","source":"#!pip install albumentations\nimport albumentations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af89a040-95d5-49ba-bb59-eed78dedff0a","_cell_guid":"7faae6ad-2c75-4b64-aa3f-fae0807a1aee","trusted":true},"cell_type":"code","source":"import torch\nfrom albumentations import ( Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, IAAAdditiveGaussianNoise, Transpose, ToGray )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport matplotlib.pyplot as plt\n\nseed = 42\n\nimport pandas as pd\nimport os\nimport cv2\nfrom torch.utils.data import Dataset,DataLoader\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\t\n    def __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\tif self.num_channels == 1:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ), 0)\n        \n\t\telse:\n\t\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = image #*(1/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1/255)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a98a696-c072-4ebb-ab18-89f167062a99","_cell_guid":"0a640f66-daf1-4038-b127-4d37c172a369","trusted":true},"cell_type":"code","source":"train_path = '../input/ranzcr-clip-catheter-line-classification/train'\ntrain_files = os.listdir(train_path)\n\ntrain_df = pd.read_csv('../input/ranzcr-clip-catheter-line-classification/train.csv')\n\ntrain = train_df.reset_index(drop=True) # reset index on both dataframes\n\nprint(train.shape)\n\nnum_channel = 3\n\nimg_size = 255","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Try different Mean and standard deviations"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_list = [[ 0.4914168 , 0.4914168 , 0.4914168] , [0.485,0.456,0.406] , [0.9,0.9,0.9] , 0.496 ]\nstd_list = [[0.407278, 0.407278 , 0.407278] , [0.229,0.224,0.225] , [0.9,0.9,0.9] , 0.407278]\n\nmean = mean_list[0]\nstd=  std_list[0]\n\nnorm_255 = False #False\n\n\n#Not needed. Instead pass value of max_pixel value.\n\nif norm_255 == True:\n\tfor x in range(0,3):\n\t\tmean[x] = mean[x]*255\n\t\tstd[x] = std[x]*255\n\n\nprint(mean , std)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Image = Image/255"},{"metadata":{"trusted":true},"cell_type":"code","source":"recale_image_by = 1\nmax_pixel_value = 255 \n\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels\n    \n    \ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(8, 8)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            print(data[i])\n\nfor i in range(0,4):\n    mean = mean_list[i]\n    std = std_list[i]\n    print(mean , std)\n    train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                         albumentations.Normalize(mean= mean ,std= std , max_pixel_value=max_pixel_value),])\n\n    Show_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reducing image pixels by a factor of 255 first\n\nNot needed\n\nmean and std are already scaled in albumentations , so we dont need to multiply mean/std by 255. [code_here](https://github.com/albumentations-team/albumentations/blob/ff83de8a51184bca97be3a9fc6905c56b44c494a/albumentations/augmentations/functional.py#L129)"},{"metadata":{"_uuid":"65e1ad07-1a45-47ef-a60e-9f0d98ee57b1","_cell_guid":"ca8a9a5c-2b25-4280-897b-ab1c227b06bd","trusted":true},"cell_type":"code","source":"recale_image_by = 255\nmax_pixel_value = 1 \n\n\nclass Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\timage = image*(1/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels\n    \n    \ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(8, 8)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            print(data[i])\n\nfor i in range(0,4):\n    mean = mean_list[i]\n    std = std_list[i]\n    print(mean , std)\n    train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                         albumentations.Normalize(mean= mean ,std= std , max_pixel_value=max_pixel_value),])\n\n    Show_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Ranzcr_jpg_train_dataset(Dataset):    \n\tdef __init__(self, files_folder_path, df, num_channels , transfroms = None ):\n\t\tself.files_folder_path = files_folder_path\n\t\tself.df = df\n\t\tself.transforms = transfroms\n\t\tself.num_channels = num_channels\n\n\tdef __len__(self):\n\t\treturn len(self.df)\n\n\tdef __getitem__(self, idx):\n\n\t\timage_id = self.df.StudyInstanceUID.values[idx]\n\t\timage = cv2.imread(os.path.join(self.files_folder_path, image_id + \".jpg\" ))\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t\t#image = image*(1/recale_image_by)\n        \n\t\tif self.transforms:\n\t\t\timage = self.transforms(image=image)['image']\n\t\tlabels = self.df[self.df.StudyInstanceUID == image_id].values.tolist()[0][1:-1]\n\t\tlabels = torch.tensor(labels,dtype= torch.float32) #.view(1,-1)\n\n\t\treturn image, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef Show_Xrays(augmentation):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 9 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(3,3, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     #albumentations.Normalize(mean= mean ,std= std ,),\n                                    ])\n\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([ albumentations.Resize(height=img_size, width=img_size, p=1.0), \n                                     albumentations.Normalize(mean= mean ,std= std ,p = 1),\n                                    ])\n\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbd85435-4cdd-4ed8-a954-dc87854ffcab","_cell_guid":"a928b971-e98f-48fe-8cb1-c1fce088b4e6","trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),                            \n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                        #albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        albumentations.CLAHE(clip_limit=(1,10), p= 1),\n                                        albumentations.Normalize(mean= mean ,std= std ,)\n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalizing data first"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([   albumentations.Normalize(mean= mean ,std= std ,) , \n                                     albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1),\n                                        albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7), \n                                        #albumentations.CLAHE(clip_limit=(1,10), p= 1)\n                                        \n                                    ])\nShow_Xrays(train_augs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ShiftScaleRotate(always_apply=False, p=1, shift_limit_x=(-0.0625, 0.0625),\n                 shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.09999999999999998, 0.10000000000000009),\n                 rotate_limit=(-45, 45), interpolation=1, border_mode=4, value=None, mask_value=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying differnt combinations of image augmentation\n\n(I hope to find one that magically works!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                #albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.3), lightness=(0.5, 0.7), p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t#albumentations.ShiftScaleRotate(shift_limit_x=(-0.0125, 0.0125),shift_limit_y=(-0.0125, 0.0125) ,rotate_limit=(-15, 15) , p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                ########albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t#####albumentations.ElasticTransform(alpha=3),\n                #albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                #albumentations.MotionBlur(p=1),\n                #albumentations.MedianBlur(p=1),\n                #albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                ###albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=5, p= 0.5),\n\t\t\t\t#albumentations.Normalize(mean= mean ,  std= std ,) \n               ])\n\nShow_Xrays(train_augs)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = [0.485,0.456,0.406]\nstd = [0.229,0.224,0.225]\n\ntransform = albumentations.Compose([  albumentations.RandomCrop(width=256, height=256),\n    albumentations.Normalize(mean= mean ,  std= std ,) \n])\n\n# Read an image with OpenCV and convert it to the RGB colorspace\nimage = cv2.imread(\"../input/ranzcr-clip-catheter-line-classification/test/1.2.826.0.1.3680043.8.498.10003659706701445041816900371598078663.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Augment an image\ntransformed = transform(image=image)\ntransformed_image = transformed[\"image\"]\n\n\nfig = plt.figure()\nfig.set_size_inches(15, 15)\n\nax = plt.subplot(1,1, 1)\nplt.tight_layout()\nax.axis('off')\nplt.imshow(image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving all varints of augmentation to folder"},{"metadata":{},"cell_type":"markdown","source":"https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomRain"},{"metadata":{"trusted":true},"cell_type":"code","source":"reference_images = ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augment_list = [albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\talbumentations.ShiftScaleRotate(p=1),\n\t\t\t\talbumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\talbumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\talbumentations.CLAHE(clip_limit=(1,4), p=1),\n                albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\talbumentations.ElasticTransform(alpha=3),\n                albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                albumentations.Cutout(max_h_size=int(img_size * 0.05), max_w_size=int(img_size * 0.05), num_holes=10, p= 0.5),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augment_list = [  albumentations.Resize(img_size, img_size),\n                albumentations.RandomResizedCrop(img_size, img_size, scale=(0.9, 1), p=1), \n\t\t\t\t  ########albumentations.HorizontalFlip(p=1),\n\t\t\t\t  albumentations.ShiftScaleRotate(p=1),\n\t\t\t\t   albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=1),\n\t\t\t\t   albumentations.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=1),\n\t\t\t\t   albumentations.CLAHE(clip_limit=(1,4), p=1),\n                   albumentations.OpticalDistortion(distort_limit=1.0),\n\t\t\t\t   #albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n\t\t\t\t   albumentations.ElasticTransform(alpha=3),\n                 albumentations.GaussNoise(var_limit=[10, 50], p=1),\n                 #albumentations.GaussianBlur(p=1),\n                 albumentations.MotionBlur(p=1),\n                albumentations.MedianBlur(p=1),\n                albumentations.augmentations.transforms.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), p=1),\n                #albumentations.imgaug.transforms.IAASuperpixels(p_replace=0.1, n_segments=100, p=1),\n                albumentations.imgaug.transforms.IAASharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1),\n                albumentations.imgaug.transforms.IAAEmboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=1),\n                albumentations.imgaug.transforms.IAAPerspective (scale=(0.05, 0.1), keep_size=True, p=1),\n                ######albumentations.augmentations.domain_adaptation.HistogramMatching (reference_images , blend_ratio=(0.5, 1.0), p=0.5),\n                #albumentations.augmentations.transforms.ToSepia(p=1),\n                albumentations.augmentations.transforms.ToGray(p=1),\n                #albumentations.imgaug.transforms.IAAAdditiveGaussianNoise(loc=0, scale=(2.5500000000000003, 12.75), per_channel=False, p=1),\n                \n                albumentations.augmentations.transforms.RandomGamma(gamma_limit=(80, 120), eps=None, p=1),\n                #albumentations.augmentations.transforms.Solarize(threshold=128, p=1),\n                #albumentations.augmentations.transforms.RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, alpha_coef=0.08, p=1),\n                #albumentations.augmentations.transforms.RandomRain(slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.7, rain_type=None, p=1),\n\t\t\t\t#albumentations.Cutout(max_h_size=int(img_size * 0.1), max_w_size=int(img_size * 0.1), num_holes=5, p=1),\n\t\t\t\talbumentations.Normalize(mean= mean ,  std= std ,) \n               ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Show_save_Xrays(augmentation , name):\n    num_channels = 3\n    trainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , augmentation )\n    trainloader = DataLoader(trainset, batch_size = 1 , num_workers = 3 , shuffle = False)\n\n    fig = plt.figure()\n    fig.set_size_inches(25, 25)\n    #fig.savefig('test2png.png', dpi=100)\n\n    for batch_i, (data, target) in tqdm(enumerate(trainloader)):\n        #print(data.shape)\n        if batch_i == 1:\n            break\n        for i in range(data.shape[0]):\n            ax = plt.subplot(1 ,1, i+1)\n            plt.tight_layout()\n            ax.axis('off')\n            plt.imshow(data[i])\n            fig.savefig( str(name) + '.png', dpi=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Save_to_folder = False\n\n\nif Save_to_folder == True:\n    for i in range(len(augment_list)):\n        train_augs = albumentations.Compose([ albumentations.Resize(img_size, img_size) , augment_list[i]])\n        Show_save_Xrays(train_augs , augment_list[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!zip -r -q 'Albumentation_1*1.zip'  ./","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm ./*.zip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm ./*.png","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding your our mean and std for image dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mean_std(loader):\n    # var[X] = E[X**2] - E[X]**2\n    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n\n    for data, _ in tqdm(loader):\n        #channels_sum += torch.mean(data, dim=[0, 2, 3])\n        channels_sum += torch.Tensor.float(data ).mean(2, dim=[0, 2, 3])\n        \n        channels_sqrd_sum += torch.Tensor.float(data ** 2, dim=[0, 2, 3])\n        #channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n        num_batches += 1\n\n    mean = channels_sum / num_batches\n    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n\n    return mean, std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_augs = albumentations.Compose([ albumentations.Resize(img_size, img_size),    ToTensorV2()     ])\n\ntrainset = Ranzcr_jpg_train_dataset(train_path,  train, num_channels , train_augs )\ntrainloader = DataLoader(trainset, batch_size = 9 , num_workers = 3 , shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = [125.3113, 125.3113, 125.3113]\nstd = [103.8559, 103.8559, 103.8559]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,3):\n    a = (((std[i] - mean[i] )**2)/417)**0.5\n    print(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,3):\n    a = (std[i]*(1/255))\n    print(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std = [0.407278, 0.407278 , 0.407278]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0,3):\n    a = (mean[i]*(1/255))\n    print(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = [ 0.4914168 , 0.4914168 , 0.4914168]\nstd = [0.407278, 0.407278 , 0.407278]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_channel = 0\nG_channel = 0\nB_channel = 0\n\npathDir = '../input/ranzcr-clip-catheter-line-classification/test/*.jpg'\n\nfrom glob import glob\nimg_list = glob(pathDir)\nprint(len(img_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"R_total , G_total ,B_total = 0, 0 , 0\n\ntotal_pixel = 0\nfor idx in range(len(img_list)):\n    filename = img_list[idx]\n    img = plt.imread(filename)\n\n    total_pixel = total_pixel + img.shape[0] * img.shape[1]\n\n    R_total += np.sum((img[:, :, 0] - R_mean) ** 2)\n    G_total = G_total + np.sum((img[:, :, 1] - G_mean) ** 2)\n    B_total = B_total + np.sum((img[:, :, 2] - B_mean) ** 2)\n\n    \n    \nR_std = sqrt(R_total / total_count)\nG_std = sqrt(G_total / total_count)\nB_std = sqrt(B_total / total_count)\n\n\ntraindata = datasets.ImageFolder(data_dir + '/train', transforms.ToTensor())\nimage_means = torch.stack([t.mean(1).mean(1) for t, c in traindata])\nimage_means.mean(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}