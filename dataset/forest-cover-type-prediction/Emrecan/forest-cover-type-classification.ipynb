{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d29d10a71e77013eb5c881785975dc0ed9bf63c"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#data imports\ndata_train = pd.read_csv(\"../input/train.csv\")\ndata_test = pd.read_csv(\"../input/test.csv\")\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70c6888e45d688f1fe8a7867edcaf64d6e0dab07"},"cell_type":"code","source":"data_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\ndata_train[\"Soil_Count\"] = data_train[features_soil].apply(sum, axis=1)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.Soil_Count.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test[features_soil].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no soil_type7 or soil_type15 in train data this i'm not sure other soil types usefull or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Soil_Type\"] = data_train[features_soil].apply(np.argmax, axis=1)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Soil_Type\"] = data_train[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1])\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\ndata_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(sum, axis=1)\ndata_train.Wilderness_Area.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(np.argmax, axis=1)\ndata_train[\"Wilderness_Area\"] = data_train[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1])\ndata_train.Wilderness_Area.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2de82c2a2e4feefc2eec40de6feb42ffa30d48a6"},"cell_type":"code","source":"sns.countplot(data_train.Cover_Type)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', \"Cover_Type\"]\nsns.heatmap(data=data_train[features].corr(), annot=True, linecolor=\"w\", fmt=\".1\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nfor ind, each in enumerate([\"Elevation\", \"Aspect\" , \"Slope\", \"Hillshade_3pm\", \"Hillshade_Noon\", \"Hillshade_9am\"]):\n    plt.subplot(2, 3, ind + 1)\n    sns.distplot(data_train[each])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_train[\"Hillshade_Noon\"].apply(lambda x: x**4))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_train[\"Hillshade_9am\"].apply(lambda x: x**4))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data_train[\"Slope\"].apply(np.sqrt))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Aspect_Slope\"] = data_train.Aspect * data_train.Slope\nsns.distplot(data_train[\"Aspect_Slope\"].apply(np.cbrt))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Elevation_Slope\"] = np.sqrt(data_train.Elevation * data_train.Slope)\nsns.distplot(data_train[\"Elevation_Slope\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Elevation_Aspect\"] = np.sqrt(data_train.Elevation * data_train.Aspect)\nsns.distplot(data_train[\"Elevation_Aspect\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"Hillshade_1\"] = (data_train.Hillshade_3pm * data_train.Hillshade_Noon * data_train.Hillshade_9am)\nsns.distplot(data_train[\"Hillshade_1\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nfor ind, each in enumerate(['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways',  'Horizontal_Distance_To_Fire_Points']):\n    plt.subplot(2, 2, ind + 1)\n    sns.distplot(data_train[each])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.sqrt(data_train[\"Horizontal_Distance_To_Roadways\"]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.sqrt(data_train[\"Horizontal_Distance_To_Fire_Points\"]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clear_dataset(dataset):\n    features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n    features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n    dataset[\"Soil_Type\"] = dataset[features_soil].apply(np.argmax, axis=1)\n    dataset[\"Soil_Type\"] = dataset[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1]).astype(int)\n    dataset = dataset.drop([\"Soil_Type15\", \"Soil_Type7\"], axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[features_wilderness].apply(np.argmax, axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1]).astype(int)\n    #dataset = dataset.drop(features_wilderness, axis=1)\n    dataset[\"Hillshade_1\"] = (dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_1_sqrt\"] = np.sqrt(dataset[\"Hillshade_1\"])\n    dataset[\"Hillshade_2\"] = (dataset.Hillshade_3pm * dataset.Hillshade_9am)\n    dataset[\"Hillshade_2_sqrt\"] = np.sqrt(dataset[\"Hillshade_2\"])\n    dataset[\"Hillshade_3\"] = (dataset.Hillshade_3pm * dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_3_sqrt\"] = np.sqrt(dataset[\"Hillshade_3\"])\n    dataset.Hillshade_1 = dataset.Hillshade_1.astype(float)\n    dataset[\"DistanceToHydrology\"] = np.sqrt(dataset.Horizontal_Distance_To_Hydrology ** 2 + dataset.Vertical_Distance_To_Hydrology ** 2)\n    dataset[\"Horizontal_Distance_To_Roadways_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Roadways\"])\n    dataset[\"Horizontal_Distance_To_Fire_Points_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Fire_Points\"])\n    dataset[\"Slope_sqrt\"] = np.sqrt(dataset[\"Slope\"])\n    dataset[\"Hillshade_9am_cube\"] = dataset[\"Hillshade_9am\"].apply(lambda x: x**3)\n    dataset[\"Hillshade_Noon_cube\"] = dataset[\"Hillshade_Noon\"].apply(lambda x: x**3)\n    dataset[\"Aspect_Slope_cbrt\"] = np.cbrt(dataset.Aspect * dataset.Slope)\n    dataset[\"Elevation_Slope_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Slope)\n    dataset[\"Elevation_Aspect_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Aspect)\n    dataset[\"Elevation_sqrt\"] = np.sqrt(dataset.Elevation)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"../input/train.csv\")\nfinal_train = clear_dataset(data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA, TruncatedSVD\npca = PCA(n_components = 2 )  # whitten = normalize\nx_pca = pca.fit_transform(x_data)\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(np.stack([x_pca[:,0], x_pca[:,1], y_data], axis=1), columns=[\"p1\", \"p2\", \"Cover_Type\"])\ncolor = [\"blue\", \"green\", \"purple\", \"yellow\", \"red\", \"orange\", \"cyan\"]\nplt.figure(1,figsize=(9,6))\nfor each in y_data.unique():\n    plt.scatter(df.p1[df.Cover_Type == each],df.p2[df.Cover_Type == each],color = color[each - 1],label = each, alpha=0.5)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives / (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives / (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=19, max_features=11,n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\n\ny_predicted = clf.predict(x_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"2122ba5b7a25a806f2679fc438d99c0ceaeacc36"},"cell_type":"code","source":"cm = confusion_matrix(y_val, y_predicted)\nplt.figure(1, figsize=(9,6))\nplot_confusion_matrix(cm, classes = set(y_data.unique()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility/extendibility\n              2. complicated models/datasets\n          But for many situations Scikit-plot is the way to go\n          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Ä°mportances"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true,"_uuid":"29cee83f4ed7a451091f2dce21e4b0c064931353"},"cell_type":"code","source":"from xgboost import XGBClassifier\nclf = XGBClassifier(n_estimators=200, learning_rate=0.3, max_depth=3,n_jobs=-1, seed=42, objective=\"multi:softmax\")\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_val, y_predicted)\nplt.figure(1, figsize=(9,6))\nplot_confusion_matrix(cm, classes = set(y_data.unique()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LighGBM"},{"metadata":{"trusted":true,"_uuid":"ec8f44cc8301d9ba96e3bda946b8111e8d574ced"},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nclf = LGBMClassifier(n_estimators=200, learning_rate=0.3, max_depth=3,n_jobs=-1, seed=42, objective=\"multi:softmax\")\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"bc664c7d2f15ff69060b875e854f8360761df255"},"cell_type":"code","source":"cm = confusion_matrix(y_val, y_predicted)\nplt.figure(1, figsize=(9,6))\nplot_confusion_matrix(cm, classes = set(y_data.unique()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importances"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction And Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = pd.read_csv(\"../input/test.csv\")\nfinal_test = clear_dataset(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted = clf.predict(x_train)\naccuracy, precision, recall, f1 = get_metrics(y_train, y_predicted)\nprint(\"train accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_data, y_data)\ntest_preds = clf.predict(final_test.drop([\"Id\"], axis=1))\noutput = pd.DataFrame({'Id': data_test.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('rf_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}