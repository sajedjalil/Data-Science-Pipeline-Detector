{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport time\nfrom babel.dates import format_date, format_datetime, format_time\n\nimport tensorflow as tf\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n\nfrom transformers import XLMRobertaTokenizer, XLMRobertaModel, AdamW, XLMRobertaForSequenceClassification\nfrom transformers import get_linear_schedule_with_warmup\nfrom tensorflow.keras.layers import Dropout\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check device \n# Get the GPU device name if available.\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n\n# If we dont have GPU but a CPU, training will take place on CPU instead\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \ntorch.cuda.empty_cache()\n    \n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train_path = '../input/contradictory-my-dear-watson/train.csv'\ndata_test_path = '../input/contradictory-my-dear-watson/test.csv'\n\n# Load and shuffle train dataset\ndata_train = pd.read_csv(data_train_path)\ndata_train.sample(frac=1) # Shuffle train dataframe\ndata_train['hypothesis'].astype(str)\ndata_train['premise'].astype(str)\n\n# Load test dataset\ndata_test = pd.read_csv(data_test_path)\ndata_test['hypothesis'].astype(str)\ndata_test['premise'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get nÂº of texts in the different languages available in the dataset\ndata_train.groupby('language')['id'].count().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get length of all the premises in the train set\nseq_len_premise = [len(i.split()) for i in data_train['premise']]\n\npd.Series(seq_len_premise).hist(bins = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get length of all the hypothesis in the train set\nseq_len_hypothesis = [len(i.split()) for i in data_train['hypothesis']]\n\npd.Series(seq_len_hypothesis).hist(bins = 25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_seed = 42\n\nSEQ_LEN = 90 # Lower than max_position_embeddings field in config file (GPU exhausted)\n#model_name = 'roberta-base'\nmodel_name = 'xlm-roberta-base'\nbatch_size = 16\nepochs = 15 # number of training epochs\nlearning_rate = 1e-5 # Controls how large a step is taken when updating model weights during training.\nsteps_per_epoch = 50\nnum_workers = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(data_train[['premise', 'hypothesis']].values.tolist(), \n                                                      data_train['label'], test_size=0.20, random_state=random_seed)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the RoBERTa tokenizer and tokenize the data\nprint('Loading BERT tokenizer...')\ntokenizer = XLMRobertaTokenizer.from_pretrained(model_name, do_lower_case=True)\n\ntokens = tokenizer(x_train, truncation=True)\n\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(f' Tokens: {tokens}')\n#print(f' Tokens IDs: {token_ids}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.special_tokens_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trencoding = tokenizer.batch_encode_plus(\n  x_train,\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\nvalencoding = tokenizer.batch_encode_plus(\n  x_valid,\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\ntestencoding = tokenizer.batch_encode_plus(\n  data_test[['premise', 'hypothesis']].values.tolist(),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trencoding.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find Class Weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight('balanced', np.unique(data_train['label'].values.tolist()), \n                                 data_train['label'])\n\n#print(class_wts)\n\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\n#cross_entropy  = nn.NLLLoss(weight=weights) \ncross_entropy  = nn.CrossEntropyLoss(weight=weights) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadData(prep_df, batch_size, num_workers, sampler):\n    \n    return  DataLoader(\n            prep_df,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            sampler=sampler,\n            pin_memory=True\n        )\n\n## convert lists to tensors\ntrain_seq = torch.tensor(trencoding['input_ids'])\ntrain_mask = torch.tensor(trencoding['attention_mask'])\ntrain_y = torch.tensor(y_train.tolist())\n\nval_seq = torch.tensor(valencoding['input_ids'])\nval_mask = torch.tensor(valencoding['attention_mask'])\nval_y = torch.tensor(y_valid.tolist())\n\ntest_seq = torch.tensor(testencoding['input_ids'])\ntest_mask = torch.tensor(testencoding['attention_mask'])\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n# Train Data Loader\ntraindata = loadData(train_data, batch_size, num_workers, train_sampler)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n# Val Data Loader\nvaldata = loadData(val_data, batch_size, num_workers, val_sampler)\n\n\n# wrap tensors\ntest_data = TensorDataset(test_seq, test_mask)\n# sampler for sampling the data during training\ntest_sampler = SequentialSampler(test_data)\n# Val Data Loader\ntestdata = DataLoader(test_data)\n\n\nprint('Number of data in the train set', len(traindata))\nprint('Number of data in the validation set', len(valdata))\nprint('Number of data in the test set', len(testdata))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for ids, mask, l in traindata:\n#    print(mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"#class XML_RoBERTa_Arch(nn.Module):\n#    \n#    def __init__(self, n_classes):\n#        \n#        super(XML_RoBERTa_Arch, self).__init__()\n#        \n#        self.bert = RobertaModel.from_pretrained(model_name, return_dict=False)\n#        self.d1 = nn.Dropout(0.2)\n#        self.l1 = torch.nn.Linear(768, 64)\n#        self.bn1 = torch.nn.LayerNorm(64)\n#        self.d2 = torch.nn.Dropout(0.2)\n#        self.l2 = torch.nn.Linear(64, n_classes)\n#\n#    def forward(self, input_ids, attention_mask):\n#        _, x = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n#        x = self.d1(x)\n#        x = self.l1(x)\n#        x = self.bn1(x)\n#        x = torch.nn.Tanh()(x)\n#        x = self.d2(x)\n#        x = self.l2(x)\n#        \n#        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class XML_RoBERTa_Arch(nn.Module):\n    \n    def __init__(self, n_classes, freeze_bert=False):\n        \n        super(XML_RoBERTa_Arch,self).__init__()\n        # Instantiating BERT model object\n        self.bert = XLMRobertaModel.from_pretrained(model_name, return_dict=False)\n        \n        # Freeze bert layers\n        if freeze_bert:\n            for p in self.bert.parameters():\n                p.requires_grad = False\n                \n        self.bert_drop_1 = nn.Dropout(0.3)\n        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n        self.bn = nn.BatchNorm1d(768) # (768)\n        self.bert_drop_2 = nn.Dropout(0.25)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,3)\n\n\n    def forward(self, input_ids, attention_mask):\n        _, output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n        )\n        output = self.bert_drop_1(output)\n        output = self.fc(output)\n        output = self.bn(output)\n        output = self.bert_drop_2(output)\n        output = self.out(output)        \n        return output\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = np.unique(data_train['label'])\nprint('Downloading the XML RoBERTa custom model...')\nmodel = XML_RoBERTa_Arch(len(class_names))\nmodel.to(device) # Model to GPU\n\n#optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [{'params': [p for n, p in param_optimizer \n                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n                        {'params': [p for n, p in param_optimizer \n                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n\nprint('Preparing the optimizer...')\n#optimizer \noptimizer = AdamW(optimizer_parameters, lr=learning_rate)\nsteps = steps_per_epoch\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = steps\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to train the bert model\ndef trainBERT():\n  \n    print('Training...')\n    model.train()\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save model predictions\n    total_preds=[]\n\n    # iterate over batches\n    for step, batch in enumerate(traindata):\n    \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [r.to(device) for r in batch]\n\n        sent_id, mask, labels = batch\n        # clear previously calculated gradients \n        model.zero_grad()        \n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n        # backward pass to calculate the gradients\n        loss.backward()\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # update parameters\n        optimizer.step()\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n        # append the model predictions\n        total_preds.append(preds)\n        \n        torch.cuda.empty_cache()\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(traindata)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n    print(\"\\nEvaluating...\")\n    t0 = time.time()\n    # deactivate dropout layers\n    model.eval()\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step, batch in enumerate(valdata):\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n\n            # model predictions\n            preds = model(sent_id, mask)\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds, labels)\n            total_loss = total_loss + loss.item()\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n\n        torch.cuda.empty_cache()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(valdata) \n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# Empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n# for each epoch perform training and evaluation\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = trainBERT()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'xmlrob_weights.pt') # Save model weight's (you can also save it in .bin format)\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nTest Set...')\n\ntest_preds = []\n\nprint('Total batches:', len(testdata))\n\nfor fold_index in range(0, 3):\n    \n    print('\\nFold Model', fold_index)\n\n    # Load the fold model\n    path_model = 'xmlrob_weights.pt'\n    model.load_state_dict(torch.load(path_model))\n\n    # Send the model to the GPU\n    model.to(device)\n\n    stacked_val_labels = []\n    \n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n\n\n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n\n    for j, test_batch in enumerate(testdata):\n\n        inference_status = 'Batch ' + str(j + 1)\n\n        print(inference_status, end='\\r')\n\n        b_input_ids = test_batch[0].to(device)\n        b_input_mask = test_batch[1].to(device)   \n\n\n        outputs = model(b_input_ids, \n                attention_mask=b_input_mask)\n\n        # Get the preds\n        preds = outputs[0]\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        \n        # Stack the predictions.\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n\n        \n    test_preds.append(stacked_val_preds)\n    \n            \nprint('\\nPrediction complete.')     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sum the predictions of all fold models\nfor i, item in enumerate(test_preds):\n    if i == 0:\n        preds = item\n        \n    else:\n        # Sum the matrices\n        preds = item + preds\n\n        \n# Average the predictions\navg_preds = preds/(len(test_preds))\n\n# Take the argmax. \n# This returns the column index of the max value in each row.\ntest_predictions = np.argmax(avg_preds, axis=1)\n\n# Take a look of the output\nprint(type(test_predictions))\nprint(len(test_predictions))\nprint()\nprint(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data_test['id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"path_sample = '../input/contradictory-my-dear-watson/sample_submission.csv'\nsubmission = pd.read_csv(path_sample)    \nsubmission['prediction'] = test_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}