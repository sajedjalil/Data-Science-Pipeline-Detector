{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is based on https://www.kaggle.com/anasofiauzsoy/tutorial-notebook from the Kaggle's competition https://www.kaggle.com/c/contradictory-my-dear-watson","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\nIn this tutorial we'll look at the Contradictory, My Dear Watson competition dataset, build a preliminary model using Tensorflow 2, Keras, and BERT, and prepare a submission file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nos.environ['WANDB_API_KEY'] = '0' # to silence warning\n\nfrom transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 50\nINPUT_DIR = '/kaggle/input/contradictory-my-dear-watson/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk(INPUT_DIR):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's set up our TPU:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(INPUT_DIR + '/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The training set contains:\n- A premise\n- An hypothesis\n- A label (0 = entailment, 1 = neutral, 2 = contradiction)\n- The language of the text.\n\nFor more information about what these mean and how the data is structured, check out the data page: https://www.kaggle.com/c/contradictory-my-dear-watson/data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at one of the pair of sentences:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.premise.values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.hypothesis.values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.label.values[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These statements are contradictory, and the label shows that.\n\nLet's look at the distribution of languages in the training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To start out, we can use a pretrained model. Here, we'll use a multilingual BERT model from huggingface. For more information about BERT, see: https://github.com/google-research/bert/blob/master/multilingual.md\n\nFirst, we download the tokenizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encode_sentence('I love machine learning')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BERT uses three kind of input data: input word IDs, input masks, and input type IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences, and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: https://huggingface.co/transformers/model_doc/bert.html#tfbertmodel\n\nNow, we're going to encode all of our premise/hypothesis pairs for input into BERT.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(hypotheses, premises, tokenizer):\n    \n    num_examples = len(hypotheses)\n\n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n    inputs = {\n        'input_word_ids': input_word_ids.to_tensor(),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids}\n\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create and train model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As a result of my experiments, here is my model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n\n    x = embedding\n    \n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Conv1D(32, 3)(x)\n    \n    x = x[:,0,:]\n    \n    output = tf.keras.layers.Dense(3, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = build_model()\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 4\n\nhistory = model.fit(train_input,\n                    train.label.values,\n                    epochs=EPOCHS,\n                    verbose=1,\n                    batch_size=64,\n                    validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating & Submitting Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(INPUT_DIR + '/test.csv')\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The submission file will consist of the ID column and a prediction column. We can just copy the ID column from the test file, make it a dataframe, and then add our prediction column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}