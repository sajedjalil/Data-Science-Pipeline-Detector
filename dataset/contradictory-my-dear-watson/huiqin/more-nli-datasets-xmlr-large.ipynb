{"cells":[{"metadata":{},"cell_type":"markdown","source":"Folded from https://www.kaggle.com/yihdarshieh/more-nli-datasets\nChanges:\n1. Using xmlr large model instead of base model;\n2. Set the random seeds\n3. Using more mnli datas(60000)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<center><img src=\"https://raw.githubusercontent.com/chiapas/kaggle/master/competitions/contradictory-my-dear-watson/header.png\" width=\"1000\"></center>\n<br>\n<center><h1>Detecting contradiction and entailment in multilingual text using TPUs</h1></center>\n<br>\n\n#### Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\n#### In this notebook, we will use more NLI datasets, including\n\n* [The Stanford Natural Language Inference Corpus (SNLI)](https://nlp.stanford.edu/projects/snli/)\n* [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https://cims.nyu.edu/~sbowman/multinli/)\n* [Cross-lingual NLI Corpus (XNLI)](https://cims.nyu.edu/~sbowman/xnli/)\n\n#### We will also use Hugging Face recent library [nlp](https://huggingface.co/nlp/) to work with these datasets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Import","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\nimport numpy as np\nimport random\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport plotly.express as px\n\n!pip uninstall -y transformers\n!pip install transformers\n\nimport transformers\nimport tokenizers\n\n# Hugging Face new library for datasets (https://huggingface.co/nlp/)\n!pip install nlp\nimport nlp\n\nimport datetime\n\nstrategy = None\n\ndef seed_all(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nseed_all(2020)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3a20929-8e1d-48d2-869c-cc57f8c63cc9","_cell_guid":"20666a1f-e31b-4134-94f8-fea9a50998d3","trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Competition dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\n\noriginal_train = shuffle(original_train)\noriginal_valid = original_train[:len(original_train) // 5]\noriginal_train = original_train[len(original_train) // 5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"original - training: {len(original_train)} examples\")\noriginal_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"original - validation: {len(original_valid)} examples\")\noriginal_valid.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_test = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\nprint(f\"original - test: {len(original_test)} examples\")\noriginal_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's use Hugging Face new library [nlp](https://huggingface.co/nlp/), to get more NLI datasets.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Load a dataset - The Multi-Genre NLI Corpus (MNLI)\nFirst, let's load the [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https://cims.nyu.edu/~sbowman/multinli/). It contains $433000$ sentence pairs annotated with textual entailment information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mnli = nlp.load_dataset(path='glue', name='mnli')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### check the loaded dataset\n\nLet's look some information about the MNLI dataset. The (default) return value of [nlp.load_dataset](https://huggingface.co/nlp/package_reference/loading_methods.html#nlp.load_dataset) is a dictionary with split names as keys, usually they are `train`, `validation` and `test`, but not always. The values are [nlp.arrow_dataset.Dataset](https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset).\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(mnli, '\\n')\n\nprint('The split names in MNLI dataset:')\nfor k in mnli:\n    print('   ', k)\n    \n# Get the datasets\nprint(\"\\nmnli['train'] is \", type(mnli['train']))\n\nmnli['train']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### look inside 'nlp.arrow_dataset.Dataset'\n\nIn order to get the number of examples in a dataset, for example, `mnli['train']`, you can do\n```\n    mnli['train'].num_rows\n```\n\n\nYou can iterate a [nlp.arrow_dataset.Dataset](https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset) object like:\n```\n    for elt in mnli['train']:\n        ...\n```\nEach step, you get an example (which is a dictionary containing features - in a general sense).\n\nYou can also access the content of a [nlp.arrow_dataset.Dataset](https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset) object by specifying a feature name . For example, the training dataset in `mnli` has `premise`, `hypothesis`, `label` and `idx` as features.\n\nYou can either specify a feature name first (you get a list) followed by a slice, like\n```\n    # You get a `list` first, then slice it\n    mnli['train']['premise'][:3]\n```\nor use slice notation first to get a dictionary (which represents a sliced dataset) followed by a feature name.\n```\n    # You get a `dictionary` (of lists) first, then a list\n    mnli['train'][:3]['premise']\n```\n\nThe results will be the same.\n\nIn order to get the name of the classes, you can do\n\n```\nmnli['train'].features['label'].names\n```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's use what we learned to check some training examples","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The number of training examples in mnli dataset:', mnli['train'].num_rows)\nprint('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)\nprint('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\\n')\n\nprint('The class names in mnli dataset:', mnli['train'].features['label'].names)\nprint('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\\n')\n\nfor elt in mnli['train']:\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', mnli['train'].features['label'].names[elt['label']])\n    print('idx', elt['idx'])\n    print('-' * 80)\n    \n    if elt['idx'] >= 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset, described in [this competition data page](https://www.kaggle.com/c/contradictory-my-dear-watson/data):\n\n> label: the classification of the relationship between the premise and hypothesis (0 for entailment, 1 for neutral, 2 for contradiction)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Load more extra datasets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The Stanford Natural Language Inference Corpus (SNLI)\n\nFirst, let's load the [The Stanford Natural Language Inference Corpus (SNLI)](https://nlp.stanford.edu/projects/snli/). It contains $570000$ sentence pairs annotated with textual entailment information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"snli = nlp.load_dataset(path='snli')\n\nprint('The number of training examples in snli dataset:', snli['train'].num_rows)\nprint('The number of validation examples in snli dataset:', snli['validation'].num_rows, '\\n')\n\nprint('The class names in snli dataset:', snli['train'].features['label'].names)\nprint('The feature names in snli dataset:', list(snli['train'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(snli['train']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', snli['train'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 10:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset.\n\nIn [SNLI](https://nlp.stanford.edu/projects/snli/), we have the same premise with different hypotheses/labels. With a first try, I got `nan` as the training loss value. So I won't use this dataset in the current notebook.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### The Cross-Lingual NLI Corpus (XNLI)\n\nThe [MNLI](https://cims.nyu.edu/~sbowman/multinli/) and [SNLI](https://nlp.stanford.edu/projects/snli/) contain only english sentences. Let's load the [Cross-lingual NLI Corpus (XNLI)](https://cims.nyu.edu/~sbowman/xnli/) dataset. It contains only validation and test dataset, not training examples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xnli = nlp.load_dataset(path='xnli')\n\nprint('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\\n')\n\nprint('The class names in xnli dataset:', xnli['validation'].features['label'].names)\nprint('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(xnli['validation']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', xnli['validation'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 3:\n        break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The class names are still\n```\n    ['entailment', 'neutral', 'contradiction'],\n```\nhowever, the features `premise` and `hypothesis` are no longer `string` but `dictionary` which contain sentences in different language! ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Unified dataset format\n\nSince the 4 datasets have different formats, we are going to create an unified interface, which uses [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), to make working with them easier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Make a unified format of raw datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_features(elt):\n    '''\n    Args:\n        elt: elements of a `nlp.arrow_dataset.Dataset` that we have seen above\n    \n    Yields: tuples of 3 elements: (premise, hypothesis, language)\n    '''\n\n    if type(elt) == pd.core.series.Series:\n        yield (elt['premise'], elt['hypothesis'], elt['lang_abv'])    \n    \n    elif type(elt['premise']) == str:  \n        yield (elt['premise'], elt['hypothesis'], 'en')\n    \n    elif type(elt) == dict:\n        \n        # dict of strings\n        premises = elt['premise']\n        \n        # dict of lists\n        hypotheses_dict = elt['hypothesis']\n        \n        # lists\n        langs = hypotheses_dict['language']\n        translations = hypotheses_dict['translation']\n        \n        hypotheses = {k: v for k, v in zip(langs, translations)}\n                \n        for lang in elt['premise']:\n            if lang in hypotheses:\n                yield (elt['premise'][lang], hypotheses[lang], lang)\n        \ndef _get_raw_datasets_from_nlp(ds: nlp.arrow_dataset.Dataset):\n    \"\"\" From a `nlp.arrow_dataset.Dataset` that we have seen above to a generator of dictionaries with unified format.\n    \n    Yield a dictionary with keys: 'premise', 'hypothesis', 'label', 'lang'\n    \"\"\"\n    \n    for _, elt in enumerate(ds):\n        label = elt['label']\n        for features in _get_features(elt):\n            \n            label = -1\n            if 'label' in elt:\n                label= elt['label']\n            \n            yield {'premise': features[0], 'hypothesis': features[1], 'label': label, 'lang': features[2]}\n            \ndef _get_raw_datasets_from_dataframe(ds: pd.core.frame.DataFrame):\n    \n    result = []\n    \n    for idx, elt in ds.iterrows():\n        for features in _get_features(elt):\n            \n            label = -1\n            if 'label' in elt:\n                label= elt['label']\n            \n            yield {'premise': features[0], 'hypothesis': features[1], 'label': label, 'lang': features[2]}\n\nraw_ds_mapping = {\n    'original train': (_get_raw_datasets_from_dataframe, original_train, len(original_train)),\n    'original valid': (_get_raw_datasets_from_dataframe, original_valid, len(original_valid)),\n    'snli train': (_get_raw_datasets_from_nlp, snli['train'], snli['train'].num_rows),\n    'snli valid': (_get_raw_datasets_from_nlp, snli['validation'], snli['validation'].num_rows),\n    'mnli train': (_get_raw_datasets_from_nlp, mnli['train'], mnli['train'].num_rows),\n    'mnli valid 1': (_get_raw_datasets_from_nlp, mnli['validation_matched'], mnli['validation_matched'].num_rows),\n    'mnli valid 2': (_get_raw_datasets_from_nlp, mnli['validation_mismatched'], mnli['validation_mismatched'].num_rows),\n    'xnli valid': (_get_raw_datasets_from_nlp, xnli['validation'], xnli['validation'].num_rows * 15), # 15 languages\n    'original test': (_get_raw_datasets_from_dataframe, original_test, len(original_test)),\n}\ndef get_raw_dataset(ds_name):\n    \n    fn, ds, nb_examples = raw_ds_mapping[ds_name]\n    \n    for x in fn(ds):\n        yield x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### sanity check","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in raw_ds_mapping:\n    for idx, x in enumerate(get_raw_dataset(k)):\n        print(x)\n        if idx >= 3:\n            break","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Working with tf.data.Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_unbatched_dataset(ds_names, model_name, max_len=64):\n    \"\"\"\n    Args:\n        ds_names: list[str] or dict[str:int], the names of dataset to use, and optionally, how many examples to use from each of them.\n        model_name: list[str], a list of valid Hugging Face transformers' model names.\n            For example: 'distilbert-base-uncased', 'bert-base-uncased', etc.\n    \n    Returns:\n        A `tf.data.Dataset`.\n    \"\"\"\n\n    if type(ds_names) == list:\n        ds_names = {k: None for k in ds_names}\n    ds_names = {k: v for k, v in ds_names.items() if k in raw_ds_mapping}    \n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    \n    # This is a list of generators\n    raw_datasets = [get_raw_dataset(x) for x in ds_names]\n    \n    nb_examples = 0\n\n    sentence_pairs = []\n    labels = []    \n    for name in ds_names:\n        \n        raw_ds = get_raw_dataset(name)\n        nb_examples_to_use = raw_ds_mapping[name][2]\n        if ds_names[name]:\n            nb_examples_to_use = min(ds_names[name], nb_examples_to_use)\n        nb_examples += nb_examples_to_use\n        \n        n = 0\n        for x in raw_ds:\n            sentence_pairs.append((x['premise'], x['hypothesis']))\n            labels.append(x['label'])\n            n += 1\n            if n >= nb_examples_to_use:\n                break\n\n    # `transformers.tokenization_utils_base.BatchEncoding` object -> `dict`\n    r = dict(tokenizer.batch_encode_plus(batch_text_or_text_pairs=sentence_pairs, max_length=max_len, padding='max_length', truncation=True))\n\n    # This is very slow\n    dataset = tf.data.Dataset.from_tensor_slices((r, labels))\n\n    return dataset, nb_examples\n\ndef get_batched_training_dataset(dataset, nb_examples, batch_size=16, shuffle_buffer_size=1, repeat=False):\n    \n    if repeat:\n        dataset = dataset.repeat()\n    \n    if not shuffle_buffer_size:\n        shuffle_buffer_size = nb_examples\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    \n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    \n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset\n\n\ndef get_prediction_dataset(dataset, batch_size=16):\n    \n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### sanity check","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in raw_ds_mapping.keys():\n\n    ds, nb_examples = get_unbatched_dataset({k: 100}, model_name='distilbert-base-uncased')\n    ds_batched = get_batched_training_dataset(ds, nb_examples, batch_size=16, shuffle_buffer_size=1, repeat=False)\n    print('{} - select {} examples'.format(k, nb_examples))\n    \n    for x in ds_batched:\n        # print(x)\n        break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68cee874-838e-4f0e-9f80-bfebc4a295d0","_cell_guid":"a7f5d429-083e-4d81-883f-e032dfb0e236","trusted":true},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def keep_head_tail(lst, len_wanted, head_ratio=0.5):#0.5\n    len_head = int(len_wanted*head_ratio)\n    len_tail = len_wanted-len_head\n    if len_tail==0: return lst[:len_head]\n    return lst[:len_head]+lst[-len_tail:] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tranier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(tf.keras.Model):\n    \n    def __init__(self, model_name):\n        \n        super(Classifier, self).__init__()\n        \n        self.transformer = transformers.TFAutoModel.from_pretrained(model_name)\n        self.dropout = tf.keras.layers.Dropout(rate=0.05)\n        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n        self.classifier = tf.keras.layers.Dense(3)\n\n    def call(self, inputs, training=False):\n        \n        # Sequence outputs\n        x = self.transformer(inputs, training=training)[0]        \n        x = self.dropout(x, training=training)\n        x = self.global_pool(x)\n        \n        return self.classifier(x)\n\nclass Trainer:\n    \n    def __init__(\n        self, ds_names, model_name, max_len=64,\n        batch_size_per_replica=16, prediction_batch_size_per_replica=64,\n        shuffle_buffer_size=1\n    ):\n\n        global strategy\n        \n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        except ValueError:\n            strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\n        print('Number of replicas:', strategy.num_replicas_in_sync)             \n        \n        self.ds_names = ds_names\n        self.model_name = model_name\n        self.max_len = max_len\n    \n        self.batch_size_per_replica = batch_size_per_replica\n        self.prediction_batch_size_per_replica = prediction_batch_size_per_replica\n        \n        self.batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n        self.prediction_batch_size = prediction_batch_size_per_replica * strategy.num_replicas_in_sync\n\n        self.shuffle_buffer_size = shuffle_buffer_size\n\n        train_ds, self.nb_examples = get_unbatched_dataset(\n            ds_names=ds_names, model_name=model_name, max_len=max_len\n        )\n        self.train_ds = get_batched_training_dataset(\n            train_ds, self.nb_examples, batch_size=self.batch_size,\n            shuffle_buffer_size=self.shuffle_buffer_size, repeat=True\n        )\n        \n        valid_ds, self.nb_valid_examples = get_unbatched_dataset(\n            ds_names=['original valid'], model_name=model_name, max_len=max_len\n        )\n        self.valid_ds = get_prediction_dataset(valid_ds, self.prediction_batch_size)\n        self.valid_labels = next(iter(self.valid_ds.map(lambda inputs, label: label).unbatch().batch(len(original_valid))))\n        \n        test_ds, self.nb_test_examples = get_unbatched_dataset(\n            ds_names=['original test'], model_name=model_name, max_len=max_len\n        )\n        self.test_ds = get_prediction_dataset(test_ds, self.prediction_batch_size)\n        \n        self.steps_per_epoch = self.nb_examples // self.batch_size\n                   \n    def get_model(self, model_name, lr, verbose=False):\n\n        with strategy.scope():\n\n            model = Classifier(model_name)\n\n            # False = transfer learning, True = fine-tuning\n            model.trainable = True \n\n            # Just run a dummy batch, not necessary\n            dummy = model(tf.constant(1, shape=[1, 64]))\n\n            if verbose:\n                model.summary()\n\n            # Instiate an optimizer with a learning rate schedule\n            optimizer = tf.keras.optimizers.Adam(lr=lr)\n\n            # Only `NONE` and `SUM` are allowed, and it has to be explicitly specified.\n            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n            \n            # Instantiate metrics\n            metrics = {\n                'train loss': tf.keras.metrics.Sum(),\n                'train acc': tf.keras.metrics.SparseCategoricalAccuracy()\n            }\n\n            return model, loss_fn, optimizer, metrics\n        \n    def get_routines(self, model, loss_fn, optimizer, metrics):\n\n        def train_1_step(batch):\n            \n            inputs, labels = batch\n    \n            with tf.GradientTape() as tape:\n\n                logits = model(inputs, training=True)\n                # Remember that we use the `SUM` reduction when we define the loss object.\n                loss = loss_fn(labels, logits) / self.batch_size\n\n            grads = tape.gradient(loss, model.trainable_variables)\n\n            # Update the model's parameters.\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))            \n            \n            # update metrics\n            metrics['train loss'].update_state(loss)\n            metrics['train acc'].update_state(labels, logits)\n\n        @tf.function\n        def dist_train_1_epoch(data_iter):\n            \"\"\"\n            Iterating inside `tf.function` to optimized training time.\n            \"\"\"\n            for _ in tf.range(self.steps_per_epoch):\n                strategy.run(train_1_step, args=(next(data_iter),))        \n\n        @tf.function                \n        def predict_step(batch):\n\n            inputs, _ = batch\n            \n            logits = model(inputs, training=False)\n            return logits\n\n        def predict_fn(dist_test_ds):\n\n            all_logits = []\n            for batch in dist_test_ds:\n\n                # PerReplica object\n                logits = strategy.run(predict_step, args=(batch,))\n\n                # Tuple of tensors\n                logits = strategy.experimental_local_results(logits)\n\n                # tf.Tensor\n                logits = tf.concat(logits, axis=0)\n\n                all_logits.append(logits)\n\n            # tf.Tensor\n            logits = tf.concat(all_logits, axis=0)\n\n            return logits         \n                \n        return dist_train_1_epoch, predict_fn\n        \n    def train(self, train_name, model_name, epochs, verbose=False):\n\n        global strategy\n        \n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        except ValueError:\n            strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\n        print('Number of replicas:', strategy.num_replicas_in_sync)        \n        \n        model, loss_fn, optimizer, metrics = self.get_model(model_name, 1e-5, verbose=verbose)\n        dist_train_1_epoch, predict_fn = self.get_routines(model, loss_fn, optimizer, metrics)\n        \n        train_dist_ds = strategy.experimental_distribute_dataset(self.train_ds)\n        train_dist_iter = iter(train_dist_ds)\n        \n        dist_valid_ds = strategy.experimental_distribute_dataset(self.valid_ds)\n        dist_test_ds = strategy.experimental_distribute_dataset(self.test_ds)\n\n        history = {}\n        best_acc=0.5\n        for epoch in range(epochs):\n            \n            s = datetime.datetime.now()\n\n            dist_train_1_epoch(train_dist_iter)\n\n            # get metrics\n            train_loss = metrics['train loss'].result() / self.steps_per_epoch\n            train_acc = metrics['train acc'].result()\n\n            # reset metrics\n            metrics['train loss'].reset_states()\n            metrics['train acc'].reset_states()\n                   \n            print('epoch: {}\\n'.format(epoch + 1))\n            print('train loss: {}'.format(train_loss))\n            print('train acc: {}\\n'.format(train_acc)) \n                \n            e = datetime.datetime.now()\n            elapsed = (e - s).total_seconds()            \n            \n            logits = predict_fn(dist_valid_ds)\n\n            valid_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(self.valid_labels, logits, from_logits=True, axis=-1))\n            valid_acc = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(self.valid_labels, logits))\n            if valid_acc>best_acc:\n                best_acc=valid_acc\n                #save the model\n                model.save_weights('best.h5')\n                \n            print('valid loss: {}'.format(valid_loss))\n            print('valid acc: {}\\n'.format(valid_acc))\n            \n            print('train timing: {}\\n'.format(elapsed))\n            \n            history[epoch] = {\n                'train loss': float(train_loss),\n                'train acc': float(train_acc),\n                'valid loss': float(valid_loss),\n                'valid acc': float(valid_acc),                \n                'train timing': elapsed\n            }\n\n            print('-' * 40)\n        \n        print('best acc:{}'.format(best_acc))\n        model.load_weights('best.h5')\n        logits = predict_fn(dist_test_ds)\n        preds = tf.math.argmax(logits, axis=-1)\n        \n        submission = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/sample_submission.csv')\n        submission['prediction'] = preds.numpy()\n        submission.to_csv(f'submission-{train_name}.csv', index=False)\n        \n        return history, submission,logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_config(trainer):\n\n    print('nb. of training examples used: {}'.format(trainer.nb_examples))\n    print('nb. of valid examples used: {}'.format(trainer.nb_valid_examples))\n    print('nb. of test examples used: {}'.format(trainer.nb_test_examples))\n    \n    print('per replica batch size for training: {}'.format(trainer.batch_size_per_replica))\n    print('batch size for training: {}'.format(trainer.batch_size))\n\n    print('per replica batch size for prediction: {}'.format(trainer.prediction_batch_size_per_replica))\n    print('batch size for prediction: {}'.format(trainer.prediction_batch_size))\n    \n    print('steps per epoch: {}'.format(trainer.steps_per_epoch))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 15#15\n# model_name = 'jplu/tf-xlm-roberta-base'\nmodel_name = 'jplu/tf-xlm-roberta-large'\n# model_name = 'distilbert-base-uncased'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train on the original dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainer = Trainer(\n#     ds_names={'original train': None}, model_name=model_name,\n#     max_len=64, batch_size_per_replica=16, prediction_batch_size_per_replica=64,\n#     shuffle_buffer_size=None\n# )\n\n# print_config(trainer)\n\n# train_name = f'{model_name} + original-dataset'.replace('/', '-')\n# history_1, submission_1 = trainer.train(train_name=train_name, model_name=model_name, epochs=epochs, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Plot history","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot(history, metric):\n    \"\"\"\n    metric: 'loss' or 'acc'\n    \"\"\"\n    \n    h = {\n        f'train {metric}': [history[epoch][f'train {metric}'] for epoch in history],\n        f'valid {metric}': [history[epoch][f'valid {metric}'] for epoch in history]\n    }\n        \n    fig = px.line(\n        h, x=range(1, len(history) + 1), y=[f'train {metric}', f'valid {metric}'], \n        title=f'model {metric}', labels={'x': 'Epoch', 'value': metric}\n    )\n    fig.show()\n    \ndef plot_2(history1, history2, metric, desc1, desc2):\n    \n    h = {\n        f'train {metric} - {desc1}': [history1[epoch][f'train {metric}'] for epoch in history1],\n        f'valid {metric} - {desc1}': [history1[epoch][f'valid {metric}'] for epoch in history1],\n        f'train {metric} - {desc2}': [history2[epoch][f'train {metric}'] for epoch in history2],\n        f'valid {metric} - {desc2}': [history2[epoch][f'valid {metric}'] for epoch in history2]        \n    }\n        \n    fig = px.line(\n        h, x=range(1, len(history1) + 1), y=[f'train {metric} - {desc1}', f'valid {metric} - {desc1}', f'train {metric} - {desc2}', f'valid {metric} - {desc2}'], \n        title=f'model {metric}', labels={'x': 'Epoch', 'value': metric}\n    )\n    fig.show()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot(history_1, 'loss')\n# plot(history_1, 'acc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.read_csv(f'submission-{train_name}.csv').head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### original dataset + xnli","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# trainer = Trainer(\n#     ds_names={'original train': None, 'xnli valid': None}, model_name=model_name,\n#     max_len=64, batch_size_per_replica=16, prediction_batch_size_per_replica=64,\n#     shuffle_buffer_size=None\n# )\n\n# print_config(trainer)\n\n# train_name = f'{model_name} + extra-xnli'.replace('/', '-')\n# history_2, submission_2 = trainer.train(train_name=train_name, model_name=model_name, epochs=epochs, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot(history_2, 'loss')\n# plot(history_2, 'acc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compare [only original dataset] vs. [+ xnli]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_2(history_1, history_2, 'loss', desc1='only original dataset', desc2='+ xnli')\n# plot_2(history_1, history_2, 'acc', desc1='only original dataset', desc2='+ xnli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.read_csv(f'submission-{train_name}.csv').head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### original dataset + xnli + mnli","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = Trainer(\n    ds_names={'original train': None, 'xnli valid': None, 'mnli train': 60000, 'mnli valid 1': None, 'mnli valid 2': None}, model_name=model_name,\n    max_len=208, batch_size_per_replica=16, prediction_batch_size_per_replica=64,#16\n    shuffle_buffer_size=None\n)\n\nprint_config(trainer)\n\ntrain_name = f'{model_name} + extra-xnli-mnli'.replace('/', '-')\nhistory_3, submission_3,preds = trainer.train(train_name=train_name, model_name=model_name, epochs=epochs, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.savez_compressed('preds',a=preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(history_3, 'loss')\nplot(history_3, 'acc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compare [only original dataset] vs. [+ xnli/mnli]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_2(history_1, history_3, 'loss', desc1='only original dataset', desc2='+ mnli/xnli')\n# plot_2(history_1, history_3, 'acc', desc1='only original dataset', desc2='+ mnli+xnli')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Compare [+ xnli] vs. [+ xnli/mnli]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_2(history_2, history_3, 'loss', desc1='+ xnli', desc2='+ mnli/xnli')\n# plot_2(history_2, history_3, 'acc', desc1='+ xnli', desc2='+ mnli/xnli')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = pd.read_csv(f'submission-{train_name}.csv')\ns.to_csv(f'submission.csv', index=False)\n\ns.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}