{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nimport math\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport torch\nimport torch.nn as nn\n\n#Initialise the random seeds\ndef random_init(**kwargs):\n    random.seed(kwargs['seed'])\n    torch.manual_seed(kwargs['seed'])\n    torch.cuda.manual_seed(kwargs['seed'])\n    torch.backends.cudnn.deterministic = True\n\ndef normalise(text):\n    chars = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n    text = text.upper()\n    words=[]\n    for w in text.strip().split():\n        if w.startswith('HTTP'):\n            continue\n        while len(w)>0 and w[0] not in chars:\n            w = w[1:]\n        while len(w)>0 and w[-1] not in chars:\n            w = w[:-1]\n        if len(w) == 0:\n            continue\n        words.append(w)\n    text=' '.join(words)\n    return text\n\ndef read_vocabulary(train_text, **kwargs):\n    vocab = dict()\n    counts = dict()\n    num_words = 0\n    for line in train_text:\n        line = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n        for char in line:\n            if char not in vocab:\n                vocab[char] = num_words\n                counts[char] = 0\n                num_words+=1\n            counts[char] += 1\n    num_words = 0\n    vocab2 = dict()\n    if not kwargs['characters']:\n        for w in vocab:\n            if counts[w] >= args['min_count']:\n                vocab2[w] = num_words\n                num_words += 1\n    vocab = vocab2\n    for word in [kwargs['start_token'],kwargs['end_token'],kwargs['unk_token']]:\n        if word not in vocab:\n            vocab[word] = num_words\n            num_words += 1\n    return vocab\n\ndef load_data(premise, hypothesis, targets=None, cv=False, **kwargs):\n    assert len(premise) == len(hypothesis)\n    num_seq = len(premise)\n    max_words = max([len(t) for t in premise+hypothesis])+2\n    dataset = len(kwargs['vocab'])*torch.ones((2,max_words,num_seq),dtype=torch.long)\n    labels = torch.zeros((num_seq),dtype=torch.uint8)\n    idx = 0\n    utoken_value = kwargs['vocab'][kwargs['unk_token']]\n    for i,line in tqdm(enumerate(premise),desc='Allocating data memory',disable=(kwargs['verbose']<2)):\n        words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n        if len(words)==0 or words[0] != kwargs['start_token']:\n            words.insert(0,kwargs['start_token'])\n        if words[-1] != kwargs['end_token']:\n            words.append(kwargs['end_token'])\n        for jdx,word in enumerate(words):\n            dataset[0,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n        line=hypothesis[i]\n        words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n        if len(words)==0 or words[0] != kwargs['start_token']:\n            words.insert(0,kwargs['start_token'])\n        if words[-1] != kwargs['end_token']:\n            words.append(kwargs['end_token'])\n        for jdx,word in enumerate(words):\n            dataset[1,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n        if targets is not None:\n            labels[idx] = targets[i]\n        idx += 1\n\n    if cv == False:\n        return dataset, labels\n\n    idx = [i for i in range(num_seq)]\n    random.shuffle(idx)\n    trainset = dataset[:,:,idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n    trainlabels = labels[idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n    validset = dataset[:,:,idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n    validlabels = labels[idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n    return trainset, validset, trainlabels, validlabels\n\nclass LSTMEncoder(nn.Module):\n    def __init__(self, **kwargs):\n        \n        super(LSTMEncoder, self).__init__()\n        #Base variables\n        self.vocab = kwargs['vocab']\n        self.in_dim = len(self.vocab)\n        self.start_token = kwargs['start_token']\n        self.end_token = kwargs['end_token']\n        self.unk_token = kwargs['unk_token']\n        self.characters = kwargs['characters']\n        self.embed_dim = kwargs['embedding_size']\n        self.hid_dim = kwargs['hidden_size']\n        self.n_layers = kwargs['num_layers']\n        \n        #Define the embedding layer\n        self.embed = nn.Embedding(self.in_dim+1,self.embed_dim,padding_idx=self.in_dim)\n        #Define the lstm layer\n        self.lstm = nn.LSTM(input_size=self.embed_dim,hidden_size=self.hid_dim,num_layers=self.n_layers)\n    \n    def forward(self, inputs, lengths):\n        #Inputs are size (LxBx1)\n        #Forward embedding layer\n        emb = self.embed(inputs)\n        #Embeddings are size (LxBxself.embed_dim)\n\n        #Pack the sequences for GRU\n        packed = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths)\n        #Forward the GRU\n        packed_rec, self.hidden = self.lstm(packed,self.hidden)\n        #Unpack the sequences\n        rec, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_rec)\n        #Hidden outputs are size (LxBxself.hidden_size)\n        \n        #Get last embeddings\n        out = rec[lengths-1,list(range(rec.shape[1])),:]\n        #Outputs are size (Bxself.hid_dim)\n        \n        return out\n    \n    def init_hidden(self, bsz):\n        #Initialise the hidden state\n        weight = next(self.parameters())\n        self.hidden = (weight.new_zeros(self.n_layers, bsz, self.hid_dim),weight.new_zeros(self.n_layers, bsz, self.hid_dim))\n\n    def detach_hidden(self):\n        #Detach the hidden state\n        self.hidden=(self.hidden[0].detach(),self.hidden[1].detach())\n\n    def cpu_hidden(self):\n        #Set the hidden state to CPU\n        self.hidden=(self.hidden[0].detach().cpu(),self.hidden[1].detach().cpu())\n        \nclass Predictor(nn.Module):\n    def __init__(self, **kwargs):\n        \n        super(Predictor, self).__init__()\n        self.hid_dim = kwargs['hidden_size']*2\n        self.out_dim = 3\n        #Define the output layer and softmax\n        self.linear = nn.Linear(self.hid_dim,self.out_dim)\n        self.softmax = nn.LogSoftmax(dim=1)\n        \n    def forward(self,input1,input2):\n        #Outputs are size (Bxself.hid_dim)\n        inputs = torch.cat((input1,input2),dim=1)\n        out = self.softmax(self.linear(inputs))\n        return out\n\ndef train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**kwargs):\n    trainlen = trainset.shape[2]\n    nbatches = math.ceil(trainlen/kwargs['batch_size'])\n    total_loss = 0\n    total_backs = 0\n    with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n        encoder = encoder.train()\n        for b in range(nbatches):\n            #Data batch\n            X1 = trainset[0,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n            seq_length1 = torch.sum(mask1,dim=0)\n            ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n            max_seq_length1 = torch.max(seq_length1)\n            X1 = X1[:,dec_index1]\n            X1 = X1[0:max_seq_length1]\n            rev_dec_index1 = list(range(seq_length1.shape[0]))\n            for i,j in enumerate(dec_index1):\n                rev_dec_index1[j] = i\n            X2 = trainset[1,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n            seq_length2 = torch.sum(mask2,dim=0)\n            ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n            max_seq_length2 = torch.max(seq_length2)\n            X2 = X2[:,dec_index2]\n            X2 = X2[0:max_seq_length2]\n            rev_dec_index2 = list(range(seq_length2.shape[0]))\n            for i,j in enumerate(dec_index2):\n                rev_dec_index2[j] = i\n            Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            #Forward pass\n            encoder.init_hidden(X1.size(1))\n            embeddings1 = encoder(X1,ordered_seq_length1)\n            encoder.detach_hidden()\n            encoder.init_hidden(X2.size(1))\n            embeddings2 = encoder(X2,ordered_seq_length2)\n            embeddings1 = embeddings1[rev_dec_index1]\n            embeddings2 = embeddings2[rev_dec_index2]\n            posteriors = predictor(embeddings1,embeddings2)\n            loss = criterion(posteriors,Y)\n            #Backpropagate\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #Estimate the latest loss\n            if total_backs == 100:\n                total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n            else:\n                total_loss += loss.detach().cpu().numpy()\n                total_backs += 1\n            encoder.detach_hidden()\n            pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n            pbar.update()\n    return total_loss/(total_backs+1)\n\ndef evaluate_model(testset,encoder,predictor,**kwargs):\n    testlen = testset.shape[2]\n    nbatches = math.ceil(testlen/kwargs['batch_size'])\n    predictions = np.zeros((testlen,))\n    with torch.no_grad():\n        encoder = encoder.eval()\n        for b in range(nbatches):\n            #Data batch\n            X1 = testset[0,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n            seq_length1 = torch.sum(mask1,dim=0)\n            ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n            max_seq_length1 = torch.max(seq_length1)\n            X1 = X1[:,dec_index1]\n            X1 = X1[0:max_seq_length1]\n            rev_dec_index1 = list(range(seq_length1.shape[0]))\n            for i,j in enumerate(dec_index1):\n                rev_dec_index1[j] = i\n            X2 = testset[1,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n            mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n            seq_length2 = torch.sum(mask2,dim=0)\n            ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n            max_seq_length2 = torch.max(seq_length2)\n            X2 = X2[:,dec_index2]\n            X2 = X2[0:max_seq_length2]\n            rev_dec_index2 = list(range(seq_length2.shape[0]))\n            for i,j in enumerate(dec_index2):\n                rev_dec_index2[j] = i\n            #Forward pass\n            encoder.init_hidden(X1.size(1))\n            embeddings1 = encoder(X1,ordered_seq_length1)\n            encoder.init_hidden(X2.size(1))\n            embeddings2 = encoder(X2,ordered_seq_length2)\n            embeddings1 = embeddings1[rev_dec_index1]\n            embeddings2 = embeddings2[rev_dec_index2]\n            posteriors = predictor(embeddings1,embeddings2)\n            #posteriors = model(X,ordered_seq_length)\n            estimated = torch.argmax(posteriors,dim=1)\n            predictions[b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])] = estimated.detach().cpu().numpy()\n    return predictions\n        \n#Arguments\nargs = {\n    'cv_percentage': 0.1,\n    'epochs': 20,\n    'batch_size': 128,\n    'embedding_size': 16,\n    'hidden_size': 64,\n    'num_layers': 1,\n    'learning_rate': 0.01,\n    'seed': 0,\n    'start_token': '<s>',\n    'end_token': '<\\s>',\n    'unk_token': '<UNK>',\n    'verbose': 1,\n    'characters': False,\n    'min_count': 15,\n    'device': torch.device(('cuda:0' if torch.cuda.is_available() else 'cpu'))\n    }\n\n#Read data\ntrain_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\n#Extract only English language cases\ntrain_data = train_data.loc[train_data['language']=='English']\ntest_data = test_data.loc[test_data['language']=='English']\n#Extract premises and hypothesis\ntrain_premise = [normalise(v) for v in train_data.premise.values]\ntrain_hypothesis = [normalise(v) for v in train_data.hypothesis.values]\ntest_premise = [normalise(v) for v in test_data.premise.values]\ntest_hypothesis = [normalise(v) for v in test_data.hypothesis.values]\ntrain_targets = train_data.label.values\nprint('Training: {0:d} pairs in English. Evaluation: {1:d} pairs in English'.format(len(train_premise),len(test_premise)))\nprint('Label distribution in training set: {0:s}'.format(str({i:'{0:.2f}%'.format(100*len(np.where(train_targets==i)[0])/len(train_targets)) for i in [0,1,2]})))\n\nbatch_sizes = [64,128,256]\nmin_counts = [5,15,25]\n\nit_idx = 0\nvalid_predictions = dict()\ntest_predictions = dict()\nvalid_accuracies = dict()\n\nfor batch_size in batch_sizes:\n    for min_count in min_counts:\n        args['batch_size'] = batch_size\n        args['min_count'] = min_count\n    \n        random_init(**args)\n\n        #Make vocabulary and load data\n        args['vocab'] = read_vocabulary(train_premise+train_hypothesis, **args)\n        #print('Vocabulary size: {0:d} tokens'.format(len(args['vocab'])))\n        trainset, validset, trainlabels, validlabels = load_data(train_premise, train_hypothesis, train_targets, cv=True, **args)\n        testset, _ = load_data(test_premise, test_hypothesis, None, cv=False, **args)\n\n        #Create model, optimiser and criterion\n        encoder = LSTMEncoder(**args).to(args['device'])\n        predictor = Predictor(**args).to(args['device'])\n        optimizer = torch.optim.Adam(list(encoder.parameters())+list(predictor.parameters()),lr=args['learning_rate'])\n        criterion = nn.NLLLoss(reduction='mean').to(args['device'])\n\n        #Train epochs\n        best_acc = 0.0\n        for ep in range(1,args['epochs']+1):\n            loss = train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**args)\n            val_pred = evaluate_model(validset,encoder,predictor,**args)\n            test_pred = evaluate_model(testset,encoder,predictor,**args)\n            acc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\n            if acc >= best_acc:\n                best_acc = acc\n                best_epoch = ep\n                best_loss = loss\n                valid_predictions[it_idx] = val_pred\n                valid_accuracies[it_idx] = acc\n                test_predictions[it_idx] = test_pred\n        print('Run {0:d}. Best epoch: {1:d} of {2:d}. Training loss: {3:.2f}, validation accuracy: {4:.2f}%, test label distribution: {5:s}'.format(it_idx+1,best_epoch,args['epochs'],best_loss,best_acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n        it_idx += 1\n\n#Do the score combination\nbest_epochs = np.argsort([valid_accuracies[ep] for ep in range(it_idx)])[::-1]\nval_pred = np.array([valid_predictions[ep] for ep in best_epochs[0:5]])\nval_pred = np.argmax(np.array([np.sum((val_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\ntest_pred = np.array([test_predictions[ep] for ep in best_epochs[0:5]])\ntest_pred = np.argmax(np.array([np.sum((test_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\nacc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\nprint('Ensemble. Cross-validation accuracy: {0:.2f}%, test label distribution: {1:s}'.format(acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n#Set all predictions to the majority category\ndf_out = pd.DataFrame({'id': pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')['id'], 'prediction': np.argmax([len(np.where(train_targets==i)[0]) for i in [0,1,2]])})\n#Set only English language cases to the predicted labels\ndf_out.loc[df_out['id'].isin(test_data['id']),'prediction']=test_pred\ndf_out.to_csv('/kaggle/working/submission.csv'.format(it_idx,acc),index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T11:18:25.674382Z","iopub.execute_input":"2021-08-18T11:18:25.674891Z","iopub.status.idle":"2021-08-18T11:39:38.936499Z","shell.execute_reply.started":"2021-08-18T11:18:25.674792Z","shell.execute_reply":"2021-08-18T11:39:38.935567Z"},"trusted":true},"execution_count":null,"outputs":[]}]}