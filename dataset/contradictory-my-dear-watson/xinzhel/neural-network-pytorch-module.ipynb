{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Prerequisite Knowledge\n### 1. What is a neural network? High-level Concept vs Math\n<img src=\"https://i.loli.net/2021/08/12/7yaGWTIKlHNz6mD.png\" width='500'>\n\n* High-level neural net (Left of the above image):  \n    * **neural diagram** which is analogous to the biological network of neuron; multiple layers of **vertices/nodes/neurons** connected by **links**\n    * Implementation Pro: helpful when you use `pytorch.nn` module or `keras` to build NN in high level (or layer level).\n    * The activation of neurons in the previous layer determines the activation of ones in the current layer.\n\n\n* Math  (Right of the above image): \n    * a series of transformation via **matrix-vector product** where transformation matrix is all the links between two layers in the neural diagram or \n    * **a function made up of a composition of multivirate, linear or non-linear activation functions** where weights are links and input/output values are neurons\n    * Implementation Pro: helpful when you build neural network in a low level (or neuron level), e.g., `pytorch.functional` which contains the basic functions for high-level object-oriented modules with `torch.Tensor` (matrices) as parameters. it is also helpful when you \n        + want to trace the parameters and inputs/outputs of each layers\n        + output flexible statistical estimates, \n        + design loss function without much numerical problem\n        + understand and modify optimization process via calculus knowledge\n \n    * From the high-school linear function to **multivirate functions**. Solving \n        $$2 w_{1}- w_{2}=0$$\n        $$-w_{1} + 2w_{2}=3$$\n        This could be concisely represented as matrix multipication:\n        $$\n        \\left[\\begin{array}{rr}\n        2 & -1 \\\\\n        -1 & 2\n        \\end{array}\\right]\\left[\\begin{array}{l}\n        x \\\\\n        y\n        \\end{array}\\right]=\\left[\\begin{array}{l}\n        0 \\\\\n        3\n        \\end{array}\\right]\n        $$\n    Notice that: each equation represents a line in two dimensional space (or a plane) <-> For neural network, each unit represents the affine transformation (linear transformation with bias) of the whole plane since it contains a function rather than equation, e.g., $\\text{hidden_neuron_1} = 2 w_{1}- w_{2}$\n\n### 2. Common Transformations\n* The common transformations could be divided into two categories: functions (no parameters during learning process) and architectures (containing learnable parameters). As a Pythonic framework for Deep Learning, PyTorch implements these two types explicitly as its standard libraries:\n    * `torch.nn`: a standard library for PyTorch Out-of-the-box `Module`, e.g., `Linear`, `Conv1d`, `Dropout`\n    * `torch.nn.functional`: a standard library for functions without associated parameters\n\n* Linear Functioin \n    + Use case: basic transformation\n    + Parameters: matrix $W$\n    + Categories: Architecture\n    + Notation: Let  $m$ = the number of examples, $n_i$ = the input dimension, $n_o$ = the output dimension\n    + The linear function:  map input $X$ to the output $y$ by finding the parameters $W$ (determined by loss function): \n        +  $\\hat{y} = W \\cdot X$  where $\\mathbf{X} \\in \\mathbb{R}^{n_i \\times m}$ and $\\mathbf{W} \\in \\mathbb{R}^{n_o \\times n_i}$ and $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{n_o \\times m}$ which is equivalent to \n        +  $\\hat{y} = X \\cdot W$  where $\\mathbf{X} \\in \\mathbb{R}^{m \\times n_i}$ and $\\mathbf{W} \\in \\mathbb{R}^{n_i \\times n_o}$ and $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{m \\times n_o}$ (This form conforms to the implementation in Tensorflow).\n        + If a bias term $b\\in \\mathbb{R}^{n_o \\times 1}$ is added: $\\hat{y} = W \\cdot X + b$. But no need to do that since (1) $W$ do the job of transforming input or $b$ could be considered as  $w_{n+1} \\dot x$ where $x=1$.\n        + Although the first notation makes more sense for me to imagine it as the transformation in the high-dimension space, the second notation is commonly used for implementation. Here I change (use $\\theta$ and transpose) the second notation into $\\theta^{T} X$. Because $\\theta$ which can avoid confusion for NLP notations and unify weights and bias.\n        + Anyway, the linear node and bias nodes would not be shown in neural network diagrams since every neuron is assumed to have a linear node along with its corresponding bias. But we just use its output denoted as $z$ where $z = \\theta^{T} X$\n    \n* Sigmoid, softmax, tanh\n    $$p = h_\\theta=\\frac{1}{1+e^{-z}} $$\n    + Use Case: output probability distribution\n    + Parameters: outputs from the last layer\n    + Categories: Functions\n    \n* Cross entropy for loss\n    + Use case: Although this notebook is mostly relevant to forward pass of NN, another essence of neural network is learning parameters like $W$ vias optimization algorithm (normally gradient descend) which need well-defined [**objective/loss function**](https://www.kaggle.com/sergioli212/cross-entropy-loss-details).\n    + Parameters: model outputs and ground truth\n    + Categories: Functions\n\n* More complication architecturesï¼š convolution (CNN), recurrent (RNN), Transformer\n    + This is designed with utilizations of basic linear function and other functions.\n    + World is built on space and time which introduce awesome properties into data(e.g. Pictures, Text, Audio,Vedio and sensor readings and so on). The most important property is that positions of one example matter to explain something. For example, the trend of temperature can only be reflected along sequential steps in order or images of human face could only be recognized when the nose is put between eyes and the mouth(otherwise, it would be recognized as the monster, actually nothing.) Even though these positional relations may be extracted by human, it just may not be well-thought-out and not intellgent when it comes to a bulk of data. Therefore, some NN architecture could be constructed to consider the nature of positional relations, e.g. Convolution NN and Recurrent NN. In essence, they are just another type of transformation or pieces of Lego for constructing more beautiful toys. Here I just take RNN for example. More details see: [RNN/LSTM](https://www.kaggle.com/sergioli212/rnn-lstm-from-scratch/), [CNN](https://www.kaggle.com/sergioli212/convolution/).\n\n\n* (Optional) The way I think of why we need different transformations in neural net: Just like playing Lego we need more than regular piecesðŸ”º to build real-world things ðŸš‰, ðŸ“±... , neural network consist of more than linear transformation to model the real-world problem. This depends on prior knowledge of what problem you want to solve. For example,  if you build â°, you need irregularly shaped pieces like wheelðŸ½âš™ï¸. If you predict the increase of human population in China next year, you may use exponential function since you know  it will increase exponentially by time. If you predict image class, you will use convolution transformation since yoou know image has to be understood by its combination of pixels rather than each pixel separately.\n\n\n### 3. Last words to make the story complete\n* An Error Function is often used with a neural network for its appication, e.g.,  classification.\n* With error function to quantify how wrong the current neural network, we can use gradient descent to improve the neural network by optimizing the weights/parameters/links. That's why we need a good error function.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable \nimport tensorflow as tf\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-10-13T06:21:08.025415Z","iopub.execute_input":"2021-10-13T06:21:08.025802Z","iopub.status.idle":"2021-10-13T06:21:08.031754Z","shell.execute_reply.started":"2021-10-13T06:21:08.02577Z","shell.execute_reply":"2021-10-13T06:21:08.030361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Basic data structure: `torch.Tensor`","metadata":{}},{"cell_type":"code","source":"# matrix-vector product\nW = torch.randn((4,3))\nb = torch.randn(4)\nx = torch.randn(3)\ntorch.matmul(W, x) + b","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use `Tensor` and primitive operations to build a linear layer from scratch","metadata":{}},{"cell_type":"code","source":"# input data x with shape (10, 5)\nX= torch.randn(10, 5)\n# in numpy\n# X = np.random.randn(10,5)\n\n# Linear Function\nW = torch.randn(2,10) # initialize parameters: W\nb = torch.randn(2,1) # initialize parameters:  b\nZ = torch.matmul(W, X) + b  \n# equal to\n# torch.mm(W, X) + b\n# z_0 = torch.sum(W[0]* X[0].T) +b  # element-wise multiplication & sum for vector => matrix multiplication \n# in numpy\n# W = np.random.randn(2,10) # initialize parameters: W\n# b = np.random.randn(2,1) # initialize parameters:  b\n# Z = np.dot(W, X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use `Tensor` and primitive operations to build a RNN layer from scratch","metadata":{}},{"cell_type":"code","source":"# input data with middle dimention 10 represents sequence length\nX = torch.Tensor(np.random.randn(10, 3, 4))\nn_x, m, T_x = X.shape # 3, 10, 4\nn_y, n_a = 2, 5 \n\n# pytorch linear API + tanh activation function\nlinear_xa = torch.nn.Linear(n_x, n_a)\nlinear_aa = torch.nn.Linear(n_a, n_a)\na_prev = torch.randn(m, n_a)\nfor t in range(T_x):\n    input1 = linear_xa(X[:,:,t])\n    input2 = linear_aa(a_prev)\n    a_prev = torch.tanh(input1+input2)\n# print(a_prev.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how neural network transforms images. I use\n* a typical mnist dataset which aims to identify digits\n*  two layers of Dense/fully connected layer.","metadata":{}},{"cell_type":"code","source":"# Softmax Funtion\n# def softmax(x, axis=0):\n#     e_x = torch.exp(x - torch.max(x))\n#     return e_x / e_x.sum(axis=axis)\ndef softmax(x, axis=-1):\n    return torch.exp(x) / torch.sum(torch.exp(x), dim=axis).view(-1, 1)\n\n# Sigmoid Function\n# def sigmoid(x):\n#     return 1/(1 + np.exp(-x)) \ndef sigmoid(x):\n    return 1 / (1 + torch.exp(-x))\n\n\n# To make generalized function for one layer with linear + activation function\ndef Layer(X, n_output, activation):\n    n_input = X.shape[1]\n    W = torch.randn(n_input, n_output)\n    b = torch.randn((1, n_output))\n    return activation(torch.mm(X, W) + b)\n    \n\nmnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\nmnist_train = torch.utils.data.TensorDataset(torch.Tensor(mnist_train[0]),torch.Tensor(mnist_train[1]))\nmnist_test = torch.utils.data.TensorDataset(torch.Tensor(mnist_test[0]),torch.Tensor(mnist_test[1]))\n# shuffle the dataset every time we staet going through the data loader again, i.e. every iteration\ntrain_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True) \ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\n# print(images.shape)\n# print(labels.shape)\n# plt.imshow(images[1].numpy(), cmap='Greys_r')\nh = Layer(images.view(images.shape[0], -1), 256, sigmoid)\nout = Layer(h, 10, softmax)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## High-level neural network using `nn.Module`\n\nAs said above, a neural network is just a stack of operations on data input tensors and model parameter tensors. `nn.Module` has the basic implementation to record the model parameters and operations in high level.\n\nIn a nutshell, \n\n**All the neural networks in Pytorch are built upon the parent class `nn.Module`**\n\nThe following code cell demonstrates how model parameters are used by Linear module `class Linear(Module)`.","metadata":{}},{"cell_type":"code","source":"# Use Pytorch Linear Module\nnn_module = nn.Linear(5, 2)\nfor p in nn_module.parameters():\n    print('W or b: ', p.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T06:40:36.757776Z","iopub.execute_input":"2021-10-13T06:40:36.758152Z","iopub.status.idle":"2021-10-13T06:40:36.767176Z","shell.execute_reply.started":"2021-10-13T06:40:36.758121Z","shell.execute_reply":"2021-10-13T06:40:36.765828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Module` is used **in a nested way**.","metadata":{}},{"cell_type":"code","source":"# build customized pytorch nn modules\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__() # pytorch will register layers and operations we put into the network\n        \n        self.hidden = nn.Linear(784, 256)\n        self.output = nn.Linear(256, 10)\n        \n        self.sigmoid = nn.Sigmoid()\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = self.hidden(x)\n        x = self.sigmoid(x)\n        x = self.output(x)\n        x = self.softmax(x)\n        \n        return x\n        \nmodel = Network()\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-05-22T07:57:52.806615Z","iopub.execute_input":"2021-05-22T07:57:52.806999Z","iopub.status.idle":"2021-05-22T07:57:52.819659Z","shell.execute_reply.started":"2021-05-22T07:57:52.806968Z","shell.execute_reply":"2021-05-22T07:57:52.818732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Closing Words \n* A neural network is an universal function approximator. For example, if you use softmax or sigmoid, a neural network represents conditional probability distribution. For understanding this, see [this notebook](https://www.kaggle.com/sergioli212/loss-function-and-optimization).\n* the above models/functions are not trained so they are not a model fitting into any data. For understanding this, see [this notebook](https://www.kaggle.com/sergioli212/loss-function-and-optimization).\n* Bullshit: The reason I would like to decompose NN architecture and modelling process is because, I think, I always see a lot of implementation packages and tutorials tend to mix up concepts or algorithms as a black box which hurts the flexibility to identify  and adjust finer pieces for solving my problem. For example, to generate adversarial text of NLP, I had a trouble to understand why we could update input instead of model parameters and what is problem of discrete text space and how to add constraints for ensuring fluency fo generating text via loss function (or adversarial objective functions here). All in all, each functions (linear, activation, stacking function, loss functions) during forward pass and their differentiation properties during backward pass should be well understood for deep learning practitioners.","metadata":{}},{"cell_type":"markdown","source":"## Practical Refereces\n* [How to initialize weights?](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)","metadata":{}}]}