{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Augmentation (Offline)\n\n**In computer vision problems, there is a virtual infinitude of techniques you can use to augment your images ranging from simple techniques like randomly flipping images to blending images together with CutMix or MixUp. In natural language processing, it is not as easy to come up with similar augmentation strategies; we must be a little more creative**\n\n**The first idea I had was to randomly replace words with their synonyms or to randomly add word synonyms to the sequence, but then I saw [this kernel](https://www.kaggle.com/jpmiller/augmenting-data-with-translations) which is based on [this discussion thread](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038) and realized we can do better: we can use translation to augment our data and do several things:**\n\n1. We can experiment and see if training our model on one language is better/worse than training on multiple languages\n2. We can change the distribution of languages in our dataset, perhaps translating sentences to low-resource languages like Swahili and Urdu\n3. We can randomly translate sentences to another language and then translate them back to the original like so:\n\n![](https://amitness.com/images/backtranslation-en-fr.png)\n\n*Image from [@amitness](https://www.kaggle.com/amitness)*\n\n**We can also apply this augmentation in two ways: offline augmentation or online augmentation. In the first, we augment before we feed to the model, adding to our dataset size. This is preferable for smaller datasets where we are not worried about training taking too long. When you can't afford an increase in size, you resort to online augmentation where augment the data every epoch. We will use offline augmentation in this commit (for more [see](https://www.kaggle.com/c/datasciencebowl/discussion/12597))**\n\n**For now, we will only translate to languages currently present in our dataset, but translating to languages outside of our dataset might give us better performance. Please note that some of these language codes are slightly different within the `googletrans` Python API. See [here](https://py-googletrans.readthedocs.io/en/latest/) for more**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"GEN_BACK_TR = True\n\nGEN_UPSAMPLE = False\n\nGEN_EN_ONLY = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#python basics\nfrom matplotlib import pyplot as plt\nimport math, os, re, time\nimport numpy as np, pandas as pd, seaborn as sns\n\n#nlp augmentation\n!pip install --quiet googletrans\nfrom googletrans import Translator\n\n#model evaluation\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n#for fast parallel processing\nfrom dask import bag, diagnostics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def back_translate(sequence, PROB = 1):\n    languages = ['en', 'fr', 'th', 'tr', 'ur', 'ru', 'bg', 'de', 'ar', 'zh-cn', 'hi',\n                 'sw', 'vi', 'es', 'el']\n    \n    #instantiate translator\n    translator = Translator()\n    \n    #store original language so we can convert back\n    org_lang = translator.detect(sequence).lang\n    \n    #randomly choose language to translate sequence to  \n    random_lang = np.random.choice([lang for lang in languages if lang is not org_lang])\n    \n    if org_lang in languages:\n        #translate to new language and back to original\n        translated = translator.translate(sequence, dest = random_lang).text\n        #translate back to original language\n        translated_back = translator.translate(translated, dest = org_lang).text\n    \n        #apply with certain probability\n        if np.random.uniform(0, 1) <= PROB:\n            output_sequence = translated_back\n        else:\n            output_sequence = sequence\n            \n    #if detected language not in our list of languages, do nothing\n    else:\n        output_sequence = sequence\n    \n    return output_sequence\n\n#check performance\nfor i in range(5):\n    output = back_translate('I genuinely have no idea what the output of this sequence of words will be')\n    print(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applies above define function with Dask\ndef back_translate_parallel(dataset):\n    prem_bag = bag.from_sequence(dataset['premise'].tolist()).map(back_translate)\n    hyp_bag =  bag.from_sequence(dataset['hypothesis'].tolist()).map(back_translate)\n    \n    with diagnostics.ProgressBar():\n        prems = prem_bag.compute()\n        hyps = hyp_bag.compute()\n\n    #pair premises and hypothesis\n    dataset[['premise', 'hypothesis']] = list(zip(prems, hyps))\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"twice_train_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_train.csv')\ntwice_test_aug = pd.read_csv('../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if GEN_BACK_TR:\n#now we apply translation augmentation\n    train_thrice_aug = twice_train_aug.pipe(back_translate_parallel)\n    test_thrice_aug = twice_test_aug.pipe(back_translate_parallel)\n    \n    train_thrice_aug.to_csv('thrice_translation_aug_train.csv')\n    test_thrice_aug.to_csv('thrice_translation_aug_test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# For TTA\n\n**I have already created datasets where each premise/hypothesis is separately mapped to a random language and back: these datasets can be found [here](https://www.kaggle.com/tuckerarrants/contradictorywatsontranslationaug) and [here](https://www.kaggle.com/tuckerarrants/contradictorywatsontwicetranslatedaug). One is translated a single time and the other is twice translated:**\n\n**Note that we can augment the test set as well and use test time augmentation (TTA) where we make separate predictions on the original sequences and the augmented sequences, and then use the average of the predictions for our final predictions:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#offline loading\ntrain = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\n\ntrain_aug = pd.read_csv(\"../input/contradictorywatsontwicetranslatedaug/translation_aug_train.csv\")\ntest_aug = pd.read_csv(\"../input/contradictorywatsontwicetranslatedaug/translation_aug_test.csv\")\n\ntrain_twice_aug = pd.read_csv(\"../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_train.csv\")\ntest_twice_aug = pd.read_csv(\"../input/contradictorywatsontwicetranslatedaug/twice_translated_aug_test.csv\")\n\n#view original\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view first aug\nprint(train_aug.shape)\ntrain_aug.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view second aug\nprint(train_twice_aug.shape)\ntrain_twice_aug.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view third aug\nprint(train_thrice_aug.shape)\ntrain_thrice_aug.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Wonderful! We can see that this translation procedure consistently alters our sentences without much information loss. Feel free to use these datasets for your own experiments and see how they improve your model's performance**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# For Oversampling\n\n**Now we will use the same idea to create a training dataset of only undersampled languages, like Urdu and Swahili. The only difference here is that we are not translating to random languages or translating back to the original sentence:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#check most undersampled languages in training dataset\ntrain['language'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check most undersampled languages in test dataset\ntest['language'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have a choose to make here: do we translate based on language prevalence in the training, testing dataset, or based on the languages that XLM-R was trained on? I will base my translation on the test set languages, so I will create a Vietnamese, Hindi, and Bulgarian training datasets, for now:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def translation(sequence, lang):\n    \n    #instantiate translator\n    translator = Translator()\n    \n    org_lang = translator.detect(sequence).lang\n    \n    if lang is not org_lang:\n        #translate to new language and back to original\n        translated = translator.translate(sequence, dest = lang).text\n        \n    else:\n        translated = sequence\n    \n    return translated\n\ndef translation_parallel(dataset, lang):\n    prem_bag = bag.from_sequence(dataset['premise'].tolist()).map(lambda x: translation(x, lang = lang))\n    hyp_bag =  bag.from_sequence(dataset['hypothesis'].tolist()).map(lambda x: translation(x, lang = lang))\n    \n    with diagnostics.ProgressBar():\n        prems = prem_bag.compute()\n        hyps = hyp_bag.compute()\n\n    #pair premises and hypothesis\n    dataset[['premise', 'hypothesis']] = list(zip(prems, hyps))\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#translate to Vietnamese\nprem_bag_vi = bag.from_sequence(train['premise'].tolist()).map(lambda x: translation(x, lang = 'vi'))\nhyp_bag_vi =  bag.from_sequence(train['hypothesis'].tolist()).map(lambda x: translation(x, lang = 'vi'))\n\n#translate to Hindi\nprem_bag_hi = bag.from_sequence(train['premise'].tolist()).map(lambda x: translation(x, lang = 'hi'))\nhyp_bag_hi =  bag.from_sequence(train['hypothesis'].tolist()).map(lambda x: translation(x, lang = 'hi'))\n\n#translate to Bulgarian\nprem_bag_bg = bag.from_sequence(train['premise'].tolist()).map(lambda x: translation(x, lang = 'bg'))\nhyp_bag_bg =  bag.from_sequence(train['hypothesis'].tolist()).map(lambda x: translation(x, lang = 'bg'))\n\n#and compute\nif GEN_UPSAMPLE:\n    with diagnostics.ProgressBar():\n        print('Translating train to Vietnamese...')\n        prems_vi = prem_bag_vi.compute()\n        hyps_vi = hyp_bag_vi.compute()\n        print('Done'); print('')\n    \n        print('Translating train to Hindi...')\n        prems_hi = prem_bag_hi.compute()\n        hyps_hi = hyp_bag_hi.compute()\n        print('Done'); print('')\n    \n        print('Translating train to Bulgarian...')\n        prems_bg = prem_bag_bg.compute()\n        hyps_bg = hyp_bag_bg.compute()\n        print('Done')\n        \nelse:\n    train_vi = pd.read_csv(\"../input/contradictorytranslatedtrain/train_vi.csv\")\n    train_hi = pd.read_csv(\"../input/contradictorytranslatedtrain/train_hi.csv\")\n    train_bg = pd.read_csv(\"../input/contradictorytranslatedtrain/train_bg.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if GEN_UPSAMPLE:\n    #sanity check\n    train_vi = train\n    train_vi[['premise', 'hypothesis']] = list(zip(prems_vi, hyps_vi))\n    train_vi[['lang_abv', 'language']] = ['vi', 'Vietnamese']\n    train_vi.to_csv('train_vi.csv', index = False)\ntrain_vi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if GEN_UPSAMPLE:\n    #sanity check\n    train_hi = train\n    train_hi[['premise', 'hypothesis']] = list(zip(prems_hi, hyps_hi))\n    train_hi[['lang_abv', 'language']] = ['hi', 'Hindi']\n    train_hi.to_csv('train_hi.csv', index = False)\ntrain_hi.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if GEN_UPSAMPLE:\n    #sanity check\n    train_bg = train\n    train_bg[['premise', 'hypothesis']] = list(zip(prems_bg, hyps_bg))\n    train_bg[['lang_abv', 'language']] = ['bg', 'Bulgarian']\n    train_bg.to_csv('train_bg.csv', index = False)\ntrain_bg.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# English Only","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#translate to English\nprem_bag_en = bag.from_sequence(train['premise'].tolist()).map(lambda x: translation(x, lang = 'en'))\nhyp_bag_en =  bag.from_sequence(train['hypothesis'].tolist()).map(lambda x: translation(x, lang = 'en'))\n\nif GEN_EN_ONLY:\n    #sanity check\n    train_en = train\n    train_en[['premise', 'hypothesis']] = list(zip(prems_en, hyps_en))\n    train_en[['lang_abv', 'language']] = ['en', 'English']\n    train_en.to_csv('train_en.csv', index = False)\n\nelse:\n    train_en = pd.read_csv(\"../input/contradictorytranslatedtrain/train_en.csv\")\n    \n#sanity check\ntrain_en.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The English translated dataset can now be found [here](https://www.kaggle.com/tuckerarrants/contradictorytranslatedtrain)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}