{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport gc\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nprint(f\"Using Tensorflow version: {tf.__version__}\")\nfrom tensorflow.keras.utils import to_categorical \nimport tensorflow_addons as tfa\ntfa.register_all(custom_kernels=False) # otherwise TPU throws up error\n\n!pip install -q transformers==3.1.0\n\nimport tokenizers\nimport transformers\nprint(f\"Using Transformers version: {transformers.__version__}\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = 'TPU'\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else: DEVICE = \"GPU\"\n\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint('REPLICAS: {}'.format(REPLICAS))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 777\nDATA_PATH = \"../input/contradictory-my-dear-watson/\"\n\n\n#MODEL = \"bert-base-multilingual-cased\"   # the bert uncased model is trained on fewer languages,does not include Thai\nMODEL = 'jplu/tf-xlm-roberta-large' #microsoft/Multilingual-MiniLM-L12-H384\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL)\n\n#BERT_MODEL_PATH = \"../input/bert-tensorflow/\"\n\n\nMAX_LEN = 96 # for input encoding sequence, multiple of 8\nBATCH_SIZE = 16 * REPLICAS\nEPOCHS= 10\nFOLDS = 5 # num k-folds\n\nNUM_CLASSES = 3\nOPT_TYPE = \"Adam\" # \"Adam\" or \"RAdam\"\nLR = 1e-5 # initial learning rate\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DATA_PATH+\"train.csv\")\ntest = pd.read_csv(DATA_PATH+\"test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train.label);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Language distribution by target\nf = plt.figure(figsize = (10,10))\nf.add_subplot(211)\nsns.countplot(train.language,hue=train.label);\nplt.xticks(rotation=90);\n\n# Non-english languages\nf.add_subplot(212)\nsns.countplot(train.loc[train.language!='English', 'language'],hue=train.loc[train.language!='English', 'label']);\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As expected, most texts in the corpus are in English. Therefore, to get a stable cross-validation score, we must stratify the CV folds by language as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the language and label columns to perform stratified K-fold later\ntrain.loc[:, 'lang_label'] = train.loc[:, ['lang_abv', 'label']].apply(lambda row: row['lang_abv']+'-'+str(row['label']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Word count distribution in the texts, note that this does not correspond to number of tokens, especially for non-English languages\n\npd.concat([train.premise.apply(lambda text: len(text.split(' '))).describe(),train.hypothesis.apply(lambda text: len(text.split(' '))).describe()],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = TOKENIZER(train.loc[2,'premise'],train.loc[2,'hypothesis'],add_special_tokens=True, max_length=40, padding='max_length',truncation = True,return_token_type_ids=True)\nout","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input_data(idx, tokenizer, label = False, dataset = 'train',max_len = MAX_LEN):\n    \n# Tokenize and encode the question (sentiment) and the context (tweet) with special tokens\n    if dataset == 'train':\n        enc = TOKENIZER(train.loc[idx,['premise', 'hypothesis']].values.tolist(), max_length=max_len, padding= 'max_length', add_special_tokens=True,truncation = True,return_token_type_ids=True)\n            \n    elif dataset == 'test':\n        enc = TOKENIZER(test.loc[idx,['premise', 'hypothesis']].values.tolist(), max_length=max_len, padding= 'max_length', add_special_tokens=True,truncation = True,return_token_type_ids=True)\n    \n    attention_mask = enc.attention_mask\n    input_ids = enc.input_ids \n    token_type_ids = enc.token_type_ids\n    input_tokens = [tokenizer.convert_ids_to_tokens(enc.input_ids[i]) for i in range(len(enc.input_ids))]\n\n    output_dict = {'token_type_ids': np.array(token_type_ids).astype('int32'),\n                  'input_ids': np.array(input_ids).astype('int32'),\n                  'input_tokens': input_tokens,\n                   'attention_mask': np.array(attention_mask).astype('int32'),\n                   }\n    if label:\n         output_dict['labels'] = to_categorical(train.loc[idx, 'label'], num_classes = NUM_CLASSES).astype('int32')\n                        \n    return output_dict\n\n# pre-process the training set\nprocessed_dict = preprocess_input_data(np.arange(train.shape[0]), tokenizer=TOKENIZER, label=True, dataset = 'train',max_len = MAX_LEN)\ninput_ids, attention_mask, token_type_ids,labels = processed_dict['input_ids'],processed_dict['attention_mask'], processed_dict['token_type_ids'], processed_dict['labels']\n\n# pre-process the test set\nprocessed_dict = preprocess_input_data(np.arange(test.shape[0]), tokenizer=TOKENIZER, label=False, dataset = 'test',max_len = MAX_LEN)\ninput_ids_test, attention_mask_test, token_type_ids_test = processed_dict['input_ids'],processed_dict['attention_mask'], processed_dict['token_type_ids']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_loader(idx,data= 'train', labelled=True, return_ids=False, repeat=True, shuffle=True,cache=True, batch_size = BATCH_SIZE):\n    if data == 'train':\n        if labelled:\n            dataset = tf.data.Dataset.from_tensor_slices( ((input_ids[idx,],attention_mask[idx,],token_type_ids[idx,]),\n                                                   labels[idx,]) )                                                   \n        else:\n            dataset = tf.data.Dataset.from_tensor_slices( ((input_ids[idx,],attention_mask[idx,],token_type_ids[idx,]),) )\n            if return_ids:\n                dataset = tf.data.Dataset.from_tensor_slices( ((input_ids[idx,],attention_mask[idx,],token_type_ids[idx,], train.loc[idx,'id'].values),) )\n    elif data == 'test':\n        dataset = tf.data.Dataset.from_tensor_slices( ((input_ids_test,attention_mask_test,token_type_ids_test),) )\n    if cache:\n        dataset = dataset.cache()\n    \n    if shuffle:\n        dataset = dataset.shuffle(2048)\n    if repeat:\n        dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size).prefetch(AUTO)\n    return dataset\n\n# test the data loader\nfor out in data_loader(np.arange(10),batch_size=5).unbatch().take(1):\n    print(out[0], out[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model and LR"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lr_callback(PLOT_LR = False): # LR scheduler\n    lr_start   = 1e-5\n    lr_max     = 1.5e-5 \n    lr_min     = 1e-5\n    lr_ramp_ep = 3\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    if PLOT_LR:\n        plt.figure(figsize=(15, 5))\n        plt.subplot(1, 2, 1)\n        plt.plot([lrfn(e) for e in range(EPOCHS)]);\n        plt.xlabel('Epoch'); plt.ylabel('LR');\n        plt.subplot(1, 2, 2);\n        plt.plot([lrfn(e) for e in range(EPOCHS)]);\n        plt.xlabel('Epoch'); plt.ylabel('Log LR');\n        plt.yscale('log');\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n\n    return lr_callback\n\nget_lr_callback(PLOT_LR = True)\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n\n    \ndef build_model(opt = OPT_TYPE):\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n            \n    transformer_embedding = transformers.TFAutoModel.from_pretrained(MODEL)#(BERT_MODEL_PATH+MODEL+ \"-tf_model.h5\", config=BERT_MODEL_PATH+MODEL+\"-config.json\")\n    x = transformer_embedding({'input_ids':ids,'attention_mask': att, 'token_type_ids':tok})[0][:,0,:]\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(128, activation=\"relu\")(x) \n    x = tf.keras.layers.Dense(3)(x)    \n    x = tf.keras.layers.Activation('softmax')(x) \n    \n    model = tf.keras.Model(inputs = [ids,att,tok], outputs = x)\n    if opt == 'Adam':\n        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n    elif OPT_TYPE == \"RAdam\":\n        optimizer =  tfa.optimizers.RectifiedAdam(lr=LR)\n    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    model.compile(optimizer = optimizer, loss=loss_fn, metrics = ['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"build_model().summary()\n#tf.keras.utils.plot_model(build_model()) # for plotting the graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def memory():\n    import psutil\n    pid = os.getpid()\n    py = psutil.Process(pid)\n    memoryUse = py.memory_info()[0]/2.**30  \n    print('memory use:', memoryUse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_training_curve=True\noof_preds = np.zeros(shape = (train.shape[0],3))\nypreds_test = np.zeros(shape = (test.shape[0],3))\nval_ids = []\n\nprint(f'DEVICE: {DEVICE}')\nskf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\nfor fold,(idxT,idxV) in enumerate(skf.split(train,train.lang_label.values)):    \n    print('#'*25)\n    print(f'### FOLD {fold+1}')\n    print('#'*25)\n    print(f\"Training on {len(idxT)} examples with batch size = {BATCH_SIZE}, validate on {len(idxV)} examples\")\n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) # to reset TPU memory, otherwise there's a memory leak. Took me some time to figure this out.\n            \n    memory()  \n    train_dataset = data_loader(idxT,labelled=True,repeat=True, shuffle=True)\n    valid_dataset = data_loader(idxV,labelled=True,repeat=False, shuffle=False, cache=False)\n    \n    K.clear_session()\n    with strategy.scope():\n        model = build_model()\n        \n    mod_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"fold{}.h5\".format(fold+1), monitor=\"val_accuracy\", \n                                                 verbose=1, save_best_only=True,\n                                                 save_weights_only=True, mode='max', save_freq='epoch')\n    \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,verbose=1,mode='max',\n                              patience=2, min_lr=5e-6)\n        \n    history = model.fit(train_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, \n                        steps_per_epoch= len(idxT)//BATCH_SIZE, callbacks=[mod_checkpoint,get_lr_callback()],\n                        validation_data=valid_dataset)\n    print(\"-\"*5 +\" Loading model weights from best epoch \"+\"-\"*5)\n    try:\n        model.load_weights(\"fold{}.h5\".format(fold+1))\n        print('Done')\n    except OSError:\n        print(f'Unable to load model!')\n    \n    # Save oof preds from best epoch\n    valid_dataset_unlabelled = data_loader(idxV,labelled=False,return_ids=False,repeat=False, shuffle=False, cache=False)\n    oof_preds[idxV,] = model.predict(valid_dataset_unlabelled, steps = len(idxV)/BATCH_SIZE)\n    oof_acc = accuracy_score(np.argmax(oof_preds[idxV,],axis=1), train.label.values[idxV])\n    print(f' Out-of-fold accuracy score: {oof_acc}')\n    \n    valid_dataset_unlabelled = data_loader(idxV,labelled=False,return_ids=True,repeat=False, shuffle=False)\n    val_ids.extend([sample[0][3].numpy().decode('utf-8') for sample in valid_dataset_unlabelled.unbatch()])\n    \n    # Predict on the test set\n    test_dataset = data_loader(_,data='test', labelled=False,repeat=False, shuffle=False, cache=False)\n    ypreds_test += model.predict(test_dataset, steps = test.shape[0]/BATCH_SIZE)/FOLDS\n    \n    os.remove(\"fold{}.h5\".format(fold+1))\n    \n    if display_training_curve:\n        display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n        display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'Accuracy', 212)\n\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OOF predictions\n- Save out-of-fold predictions to disk"},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred_oof = np.argmax(oof_preds,axis=1)\nprint(f\"{FOLDS}-fold CV accuracy score = {accuracy_score(ypred_oof, train.label.values)}\")\noof_df = pd.DataFrame(list(zip(val_ids,ypred_oof.tolist())),columns = ['id','pred'])\noof_df.to_csv('oof.csv', index=False)\noof_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"ypred_sub = np.argmax(ypreds_test, axis=1) # Prediction labels\n\nsub_df = pd.read_csv(\"../input/contradictory-my-dear-watson/sample_submission.csv\")\nsub_df.loc[:,'prediction'] = ypred_sub\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}