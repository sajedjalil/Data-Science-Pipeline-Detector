{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Version History\n\nThis is a kernel that shows how to run reinforcement learning algorithms in Halite IV. A colleague of mine asked me to provide something so he could get started so I decided to make it publicly available. I think I will see how far I can push this kernel in terms of usefulness. Currently it is really just a toy example and  meant to be minimal\n\n## Version 1 (03. Aug 2020)\n\nThis is a first sketch of a simple DQN baseline. It has:\n* replay buffer\n* target network\n* simple reward function\n* self play \n* A couple convolutional layers\n\n\n### References\n\nThere is plenty of good resources out there. First and foremost checkout a couple other notebooks in this competition since there is other RL starter notebooks.\n\n* [Simple Reinforcement Learning with Tensorflow - Arthur Juliani](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)\n* [Official Pytorch: Reinforcement Learning (DQN) Tutorial - Adam Paszke](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n* [Couple of good ipynb notebooks for various rainbow DQN features higgsfield/RL-Adventure](https://github.com/higgsfield/RL-Adventure)\n* [Pytorch Examples on Github](https://github.com/pytorch/examples/tree/master/reinforcement_learning)\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from kaggle_environments import make\nfrom kaggle_environments.envs.halite.helpers import *\n\nfrom random import seed\nimport time\n\nimport numpy as np\n\nnp.set_printoptions(precision=3)\nnp.set_printoptions(suppress=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"NUM_AGENTS = 2\nBOARD_SIZE = 7\nTURNS = 6\n\nenv = make('halite', configuration={\"randomSeed\": 1, \"episodeSteps\": TURNS, \"size\": BOARD_SIZE}, debug=True)\n_ = env.reset(num_agents=NUM_AGENTS)\n\nACTIONS = [\n    ShipAction.NORTH,\n    ShipAction.EAST,\n    ShipAction.SOUTH,\n    ShipAction.WEST,\n    #ShipAction.CONVERT, #for our toy example we don't want to build new shipyards\n    None #None for collecting\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env.render(mode='ipython')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will create an output of\n#  size x size x features\n#  features are \n#   \"how much halite on the board\", \n#   \"where are ships\",\n#   \"how full are ships\",\n#   \"where are bases\",\n\n# could add features around how many turns are left\n# how much halite players have on the bank etc\n\ndef world_feature(board):\n    size = board.configuration.size\n    me = board.current_player\n    \n    ships = np.zeros((1, size, size))\n    ship_cargo = np.zeros((1, size, size))\n    bases = np.zeros((1, size, size))\n\n    map_halite = np.array(board.observation['halite']).reshape(1, size, size)/1000\n\n    for iid, ship in board.ships.items():\n        ships[0, ship.position[1], ship.position[0]] = 1 if ship.player_id == me.id else -1\n        ship_cargo[0, ship.position[1], ship.position[0]] = ship.halite/1000\n\n    for iid, yard in board.shipyards.items():\n        bases[0, yard.position[1], yard.position[0]] = 1 if yard.player_id == me.id else -1\n        \n    return np.concatenate([\n        map_halite, \n        ships, \n        ship_cargo, \n        bases\n    ], axis=0)\n\n\n#As example take the first frame of the game\nsample_obs = env.state[0].observation\nboard = Board(sample_obs, env.configuration)\n\nfeature = world_feature(board)\nfeature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we define an agent consisting of a neural network\nimport random\nimport torch\nimport torch.nn as nn\n\ntorch.set_printoptions(profile=\"short\")\n\n\n#Simple network with three conv layers and a linear output\n#We will interpret the output as a (n_actions, size, size) array and \n#do the .argmax over the actions to find the move we would take for every location on the board, if there was a ship\n\n#You want to change this to your liking e.g.\n#https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#q-network\n#or search for various pytorch DQN implementations\nclass SmallModel(nn.Module):\n    def __init__(self, input_channels, num_actions):\n        super(SmallModel, self).__init__()\n        self.input_channels = input_channels\n        self.num_actions = num_actions\n        \n        self.network = nn.Sequential(\n            nn.Conv2d(\n                in_channels=input_channels,\n                out_channels=16,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=3, #use this padding to give each pixel more vision\n                padding_mode='circular'\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=16,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=0, #this will make the padded first layer smaller again\n                padding_mode='circular'\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=16,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=0, #this will make the padded first layer smaller again\n                padding_mode='circular'\n            ),\n            nn.ReLU(),\n            nn.BatchNorm2d(16),\n        )\n        \n        self.output = nn.Linear(BOARD_SIZE*BOARD_SIZE*16, BOARD_SIZE*BOARD_SIZE*len(ACTIONS))\n        \n    def forward(self, features):\n        x = self.network(features)\n        x = x.view(features.shape[0], -1) #flatten\n        x = self.output(x) #pass through linear layer\n        \n        return x.reshape(features.shape[0], self.num_actions, BOARD_SIZE, BOARD_SIZE)\n                          \n\nmodel = SmallModel(\n    input_channels=4, #needs to be equal to the number of feature channels we have\n    num_actions=len(ACTIONS)\n)\n\ntarget_model = SmallModel(\n    input_channels=4, #needs to be equal to the number of feature channels we have\n    num_actions=len(ACTIONS)\n)\n\n#predicting the feature from the cell above\nfeature_tensor = torch.from_numpy(feature).float().unsqueeze(0)\nprediction = model(feature_tensor) \nprint(prediction.shape) \n# Shape: (1, 5, 6, 6) = (batch_size, n_actions, y, x)\n# move would then be model(feature_tensor).argmax(dim=1)\n# to select the Q-Values for a specific location you can do prediction[0, :, y, x]  where y,x will be the coordinates of our ships\nprediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_move(model, obs, configuration, EPSILON):\n    size = configuration.size\n    board = Board(obs, configuration)\n    me = board.current_player\n\n\n    #if we do not have ships but a shipyard build 1 ship\n    if len(me.ships)==0 and len(me.shipyards)>0:\n        me.shipyards[0].next_action = ShipyardAction.SPAWN\n\n    #if we have no shipyard build one\n    state = world_feature(board).astype(np.float32)\n    state_tensor = torch.from_numpy(state).unsqueeze(0)\n    \n    action_indices = model(state_tensor).detach().numpy().argmax(1).squeeze()\n    random_indices = np.random.choice(range(5), (size, size))\n    actions = np.zeros((size, size))-1\n    \n\n    for ship in me.ships: \n        if len(me.shipyards)==0:\n            action_index = -1\n            ship.next_action = ShipAction.CONVERT #in our toy example we handle this manually\n        else:\n            if random.random() < EPSILON:\n                action_index = random_indices[ship.position[1], ship.position[0]]\n            else:\n                action_index = action_indices[ship.position[1], ship.position[0]]\n            \n            ship.next_action = ACTIONS[action_index]\n            \n        actions[ship.position[1], ship.position[0]] = action_index\n            \n    return me.next_actions, state, actions\n\nmake_move(model, sample_obs, env.configuration, 0.0)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taken and slightly modified from here: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        \"\"\"Saves a transition.\"\"\"\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        \n        self.memory[self.position] = args[0]\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport torch.nn.functional as F\nimport copy\n\nEPSILON = 1.0 \nEPSILON_DECAY = 0.998\nTRAINING_ITERATIONS = 2300\nEPOCHS = 1\nREPLACE_TARGET_INTERVAL = 10\nLEARNING_RATE = 0.1\nREPLAY_CAPACITY = 100000\nWARM_START_SAMPLES = 32*20\nBATCH_SIZE = 32\nGAMMA = 0.99\nPRINT_INTERVAL = 100\n\nrunning_avg_reward = []\nepisode_rewards = []\n\nmemory = ReplayMemory(REPLAY_CAPACITY)\n\n#A bunch of stuff is happening here\n#We inialize the environment, set random seeds and define some variables we need for storing all the necessary information\n#We run one full game with 4 times the same agent (initially making random moves EPSILON=1)\n#We store all the data in the replay buffer\n#After the last round we need to properly set the done flag, calculate the last rewards and the next states for everything\n#We then store this in the replay buffer\n#If we have enough samples, we run EPOCHS times the training loop, random sampling data and training the network with it (r+GAMMA*target_q)\n\nfor episode in range(TRAINING_ITERATIONS+1): #+1 so its inclusive and we print a statement at the end\n    print(f'{episode} - {round(EPSILON,3)} - {len(memory)}', end='\\r')\n    _ = env.reset(num_agents=NUM_AGENTS)\n\n    #When we call env.reset it will set the random seeds of both python and numpy to our fixed value\n    #We want to do some real random exploration though, otherwise we will always end up with the same game\n    seed_time = int(time.time()*1000)%1000000000\n    np.random.seed(seed_time)\n    seed(seed_time)\n    size = env.configuration.size\n\n\n    player2states = defaultdict(list)\n    player2actions = defaultdict(list)\n\n    player2halite = defaultdict(list)\n    player2rewards = defaultdict(list)\n\n    player2dones = defaultdict(list)\n\n    #The gist of this loop is copied from Tom Van de Wiele's answer here: https://www.kaggle.com/c/halite/discussion/144844\n    while not env.done:\n        observation = env.state[0].observation\n        player_mapped_actions = []\n        for active_id in range(NUM_AGENTS):\n            agent_status = env.state[active_id].status\n            if agent_status == 'ACTIVE':\n                player_obs = env.state[0].observation.players[active_id]\n                observation['player'] = active_id\n                engine_commands, state, actions = make_move(model, observation, env.configuration, EPSILON)\n\n                player2states[active_id].append(state)\n                player2actions[active_id].append(actions)\n\n                #in the first round there was no previous reward, we will use 5000 so the diff is 0 and drop it later in post processing\n                #one big thing is that we only generate one reward per frame, probably we should generate a (size, size) reward array to \n                #properly attribute the rewards to the ships if we have a multi ship scenario later\n                prev_reward = 5000 if len(player2halite[active_id]) == 0 else player2halite[active_id][-1]\n                reward = player_obs[0] - prev_reward\n                reward = reward if reward > 0 else 0\n                player2rewards[active_id].append(reward) \n                player2halite[active_id].append(player_obs[0]) \n\n                player2dones[active_id].append(env.done)\n\n                player_mapped_actions.append(engine_commands)\n            else:\n                player_mapped_actions.append({})\n        env.step(player_mapped_actions)\n\n\n    #Postprocessing:\n    #We need to build (state(t), actions(t), reward(t+1), state(t+1), dones(t+1)) tuples\n    #After the env finished we want to set the last done to true\n    #We want to add the last reward and remove the reward t=0 (since we always need reward(t+1))\n    for active_id in range(NUM_AGENTS):\n        player_obs = env.state[0].observation.players[active_id]\n        player2dones[active_id][-1] = True #the main loop does not get called again when the env is done so we set it manually\n\n        prev_reward = player2halite[active_id][-1]\n        reward = player_obs[0] - prev_reward\n        reward = reward if reward > 0 else 0\n        player2rewards[active_id].append(reward) #append reward t+1\n        player2rewards[active_id].pop(0) #remove reward t=0\n\n        #For debugging: Make sure we have the same number of samples everywhere\n        #print(len(player2states[active_id]), len(player2actions[active_id]),len(player2rewards[active_id]),len(player2dones[active_id]),)\n        #Look at your rewards and compare with the replay below whether the reward matches the games that you see\n        #print(player2rewards[active_id])\n\n\n        states = player2states[active_id]\n        next_states = [x for x in states]\n        next_states = next_states[1:] + next_states[-1:]\n\n        for state, action, reward, next_state, done in zip(states, player2actions[active_id], player2rewards[active_id], next_states, player2dones[active_id]):\n            memory.push((state, action, reward, next_state, done))\n\n\n    episode_rewards.append(np.array([x for y in player2rewards.values() for x in y]))\n\n        \n        \n    running_avg_reward.append(episode_rewards[-1].sum()/episode_rewards[-1].shape)\n    if episode % PRINT_INTERVAL == 0:\n        episode_rewards = np.concatenate(episode_rewards)\n        print(f'ep:{episode}, '\n               f'mem_size:{len(memory)}, '\n               f'rew:{episode_rewards.sum()}, '\n               f'avg:{round(episode_rewards.sum()/episode_rewards.shape[0], 3)}, '\n               f'eps:{round(EPSILON, 2)}, '\n               f'running_avg_rew:{round(np.mean(running_avg_reward), 3)}'\n              )\n        episode_rewards = []\n    \n            \n            \n    if not len(memory)>WARM_START_SAMPLES:\n        continue\n        \n    for epoch in range(EPOCHS):\n        sample = memory.sample(BATCH_SIZE)\n\n        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for detailed explanation). \n        # This converts [(state1, action1, reward1, next_state1, done1), (state2, action2, reward2, next_state2, done2)]\n        # To:[(state1, state2), (action1, action2), (reward1, reward2), (next_state1, next_state2), (done1, done2)]\n        states, actions, rewards, next_states, dones = list(zip(*sample))\n\n        states = np.array(states)\n        actions = np.array(actions)\n        rewards = np.array(rewards)\n        next_states = np.array(next_states)\n        dones = np.array(dones)\n\n        #Its not a bad idea to check shapes again: states.shape, next_states.shape, actions.shape, rewards.shape, dones.shape\n        states = torch.from_numpy(states)\n        next_states = torch.from_numpy(next_states)\n        actions = torch.from_numpy(actions).long()\n        rewards = torch.from_numpy(rewards)\n        dones = torch.from_numpy(dones)\n\n        #our actions are a (size, size) array, we want to select all fields that are not -1 since this is where a ship was that took an action\n        #if we had mutliple ships then batch would contain the same frame multiple times with different x and y coordinates\n        batch, xs, ys = np.where(actions>-1)\n\n        taken_actions = actions[batch, xs, ys].unsqueeze(-1)\n\n        #We will train multiple epochs, here you would ideally want to sample from a replay buffer\n\n        current_qs = model(states)[batch, :, xs, ys].gather(1, taken_actions)\n        next_qs = target_model(next_states).detach().max(1)[0][batch, xs, ys]\n\n        # target_q = reward + 0.99 * next_state_max_q * (1 - done)\n        target_qs = rewards[batch] + GAMMA * next_qs * ~dones[batch]\n        loss = F.smooth_l1_loss(current_qs.squeeze(), target_qs.detach())\n        #if we turn this on we will see that the loss is actually not decreasing very much from epoch to epoch\n        #print(target_qs.shape, current_qs.shape, loss.mean(), end='\\n')\n        optimizer.zero_grad()\n        loss.mean().backward()\n        for param in model.parameters():\n            param.grad.data.clamp_(-1, 1)\n        optimizer.step()\n\n    if episode and episode % REPLACE_TARGET_INTERVAL:\n        target_model = copy.deepcopy(model)\n        \n    EPSILON *= EPSILON_DECAY ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#running_avg_rew=0.221 is random play, so we learned something!\n#Lets see how our bot fares on some sample games:\n\nenv = make('halite', configuration={\"randomSeed\": 1, \"episodeSteps\": TURNS, \"size\": BOARD_SIZE}, debug=True)\n_ = env.reset(num_agents=NUM_AGENTS)\n\nwhile not env.done:\n    observation = env.state[0].observation\n    player_mapped_actions = []\n    for active_id in range(NUM_AGENTS):\n        agent_status = env.state[active_id].status\n        if agent_status == 'ACTIVE':\n            player_obs = env.state[0].observation.players[active_id]\n            observation['player'] = active_id\n            engine_commands, state, actions = make_move(model, observation, env.configuration, 0.0)\n\n            player_mapped_actions.append(engine_commands)\n        else:\n            player_mapped_actions.append({})\n    env.step(player_mapped_actions)\n    \n    \nenv.render(mode='ipython', width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the yellow agent found the best move while the red agent did not.\nThis is a little surprising given that we played ~2300 games. But we can see that the agent is learning something\nFrom here on out we can maybe add two more turns to see if the agents can find the big halite chunk.\nAfter this I would add advances of DQN like rainbow DQN, still on the small problem, see  if it works fine\nThen up the size of the board a little, add multi ship, make sure this works and then scale out to full game.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}