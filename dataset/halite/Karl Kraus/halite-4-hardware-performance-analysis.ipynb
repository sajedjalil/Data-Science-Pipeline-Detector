{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Measuring performance of Halite 4 Runners\n\nCurrently, the performance of the online runners is much lower than one usually expects from an online competition. This notebook shows some measurements that have been done locally and on the online runners to get a feeling about what to expect.\n\nLocal evaluations are very consistent over different runs. Notebook and online evaluation show very unpredictable spikes in performance and an 80% safety margin (capping loops at 4s) is definitely not enough to prevent an agent from erroring out. Especially when starting to do heavy work in python, one should not use more than 10% of the maximum available time.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(tight_layout=True, figsize=(12,4))\nfor i, time in enumerate([13.70, 18.60, 2.7921 * 10, 42, 7 * 10]):\n    plt.bar(i, 420e3 * 350 / time, .5)\nplt.gca().set_xticks(range(5))\nplt.gca().set_xticklabels([\"workstation\", \"laptop\", \"codeforces\\n(extrapolated)\", \"notebook\", \"online\\n(extrapolated, optimistic)\"])\nplt.title(\"append(random()) on different hardware\")\nplt.ylabel(\"iterations/s\")\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setup","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import kaggle_environments\n\n# patch renderer: https://github.com/Kaggle/kaggle-environments/pull/55\nold_html = kaggle_environments.envs.halite.halite.html_renderer()\npatched_html = old_html.replace(\".action;\", \".action || {};\")\ndef evaluate(agent):\n    environment = kaggle_environments.make(\"halite\")\n    environment.html_renderer = lambda: patched_html\n    environment.run([agent] * 4)\n    environment.render(mode=\"ipython\")\n    return environment\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nTo be able to report performance we need some way to export the step timing from a running episode evaluation. Unfortunately, this information is not readily accessable right now. Replays are the only information that we get after an episode has been evaluated. Thereby we have to encode the measurements done during the evaluation as actions. A simple way to do that is to use the first 10 steps of the simulation to convert to a shipyard and spawn 9 ships that move to individual locations on the board. Also we make sure that the all agentes error out after step 350, so it won't play ranked games.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_actions(actions):\n\treturn { id: action for id, action in actions.items() if action is not None }\n\ndef run_base(act):\n    def run(observation):\n        step = observation[\"step\"]\n        player = observation[\"player\"]\n        players = observation[\"players\"]\n        my_halite, my_shipyards, my_ships = players[player]\n        if step == 0:\n            return { id: \"CONVERT\" for id in my_ships }\n        if step < 10:\n            return {\n                **{ id: \"SPAWN\" for id in my_shipyards },\n                **{ id: \"EAST\" for id in my_ships },\n            }\n        if step < 350 + player:\n            return filter_actions(act(step, player, my_ships))\n        raise Exception(42)\n    return run\n\ndef idle(step, player, my_ships):\n    return {}\n\nidle_steps = evaluate(run_base(idle)).steps\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have 4 agents with 9 ships each that have nothing to do between steps 10 and 350. We can now use this interval to issue actions and export information about the evaluation. For example, we can let the ships dance in binary, where an even y-position means 0 (up/north) and an odd y-position means 1 (down/south). The bit position of a ship can be derived from its spawn step. The spawn step is part of the ship's id: `\"{spawn_step}-{step_local_id}\"`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bit(id):\n\treturn int(id.split(\"-\")[0]) - 2\n\ndef display(position, value):\n\ty = position // 21\n\ttarget = y & 1\n\tif target == value:\n\t\treturn None\n\telif target == 0:\n\t\treturn \"NORTH\"\n\telif target == 1:\n\t\treturn \"SOUTH\"\n\ndef run_binary(measure):\n    def run(step, player, my_ships):\n        quantized = int(measure(step, player))\n        return { id: display(my_ships[id][0], (quantized >> bit(id)) & 1) for id in my_ships }\n    return run_base(run)\n\nimport math\ndef dance(step, player):\n    return step * (player + 1) + math.sin(step * math.pi / 10) * 10\n\ndance_steps = evaluate(run_binary(dance)).steps","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All 9 ships of each player can now move independently between two positions. Thereby we can encode 9bits per player and step. The last thing left to do is to decode this information and to render some nice debug graphs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def value(position):\n\ty = position // 21\n\ttarget = y & 1\n\treturn target\n\nplayers = 4\nstart_step = 10\nplayer_labels = lambda: plt.legend([\"1\", \"2\", \"3\", \"4\"])\n\ndef decode(steps):\n    counts = [[None] * players] * start_step\n    for step in steps[start_step + 1:]:\n        count = []\n        for player in step[0][\"observation\"][\"players\"]:\n            ships = player[2]\n            if len(ships) == 0:\n                z = None\n            else:\n                z = 0\n                for id in ships:\n                    z += value(ships[id][0]) << bit(id)\n            count.append(z)\n        counts.append(count)\n    return counts\n\ncounts = decode(dance_steps)\nfigure = plt.figure(tight_layout=True, figsize=(12, 4))\nfor player in range(4): # encoded counts\n    steps = range(50, 301, 50)\n    plt.scatter(steps, [dance(step, player) for step in steps])\nplt.plot(counts) # decoded counts\nplayer_labels()\nplt.xlabel(\"step\")\nplt.ylabel(\"value\")\nplt.grid()\nplt.show()\n\nprint(\"encoded\", dance(255, 0), \"decoded\", counts[255][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The diagram shows the encoded (dots) and decoded (lines) values for each player at the given timestep. Like the halite board, our encoding scheme exhibits modular wraparound. This happens whenever the value crosses the 9bit boundary at 0 and 512. When the values only change by a small amount, it is possible to recover the real absolute values that have been encoded by accumulating the change between timesteps.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_total(counts):\n    unrolled_counts = list(counts[:start_step + 1])\n    for last_count, count in zip(counts[start_step:-1], counts[start_step + 1:]):\n        unrolled = []\n        for total, y, z in zip(unrolled_counts[-1], last_count, count):\n            if z is None or total is None:\n                unrolled.append(None)\n            else:\n                delta = (z - y + 256) % 512 - 256\n                unrolled.append(total + delta)\n        unrolled_counts.append(unrolled)\n    return unrolled_counts\n\ntotal = get_total(counts)\nfigure = plt.figure(tight_layout=True, figsize=(12, 4))\nfor player in range(4): # encoded counts\n    steps = range(50, 301, 50)\n    plt.scatter(steps, [dance(step, player) for step in steps])\nplt.plot(total) # decoded counts\nplayer_labels()\nplt.xlabel(\"step\")\nplt.ylabel(\"value\")\nplt.grid()\nplt.show()\n\nfor i in [10, 11, 12, 255]:\n    print(i, \"encoded\", dance(i, 3), \"decoded\", total[i][3])\nfor i in range(20):\n    print(i, counts[i], total[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another way to look at the data is to decode the change of the data instead of accumulating the changes. This is used for converting an encoded timestamp to an interval. Following code does not exploit the fact that timestamps are monotonic. To do that, you can just remove \"+ 256\" and \"- 256\".","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_deltas(counts):\n    deltas = [[None] * players] * (start_step + 1)\n    for last_count, count in zip(counts[start_step:-1], counts[start_step + 1:]):\n        unrolled = []\n        for y, z in zip(last_count, count):\n            if z is None:\n                unrolled.append(None)\n            else:\n                delta = (z - y + 256) % 512 - 256\n                unrolled.append(delta)\n        deltas.append(unrolled)\n    return deltas\n\ndeltas = get_deltas(counts)\nfigure = plt.figure(tight_layout=True, figsize=(12, 4))\nfor player in range(4): # encoded counts\n    steps = range(50, 301, 50)\n    plt.scatter(steps, [dance(step, player) - dance(step - 1, player) for step in steps])\nplt.plot(deltas) # decoded counts\nplayer_labels()\nplt.xlabel(\"step\")\nplt.ylabel(\"change\")\nplt.grid()\nplt.show()\n\nprint(len(counts), len(total), len(deltas))\nfor i in [11, 12, 256]:\n    print(\"encoded\", int(dance(i, 3)) - int(dance(i-1, 3)), \"decoded\", deltas[i][3])\nfor i in range(20):\n    print(i, counts[i], total[i], deltas[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tests Performed\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Verify that timeouts are consistent with the local clock. Note that the timestamps are discretized to 1/10s. Therefore we have a resolution of 1/10s and can represent +-25.6s without overflow. Every agent measures the absolute time at the beginning of its step and then sleep for .15s and on every tenth step for (step)/50s. Therefore the timeout is tested at intervals of .2s and the interval at step 300 is exactly 6s.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndef test_timeout(step, player):\n    now = time.perf_counter()\n    if (step % 10) == 0:\n        print(player, step, step / 50)\n        time.sleep(step / 50)\n    else:\n        time.sleep(.15)\n    return now * 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Perform heavy work and measure loop interval as Player 2 + 3, Measure start and end as Player 1 + 4. Note that the loop interval with a resolution of 1/100s. Therefore we can represent a change of loop duration of +-2.56s per step without overflow. The work consists of a loop that queries a random number and appends it to an array. The amount of iterations increases linearly at a rate of 420e3 iterations per step. At step 350 this means that the array will contain 147e6 items that take up 5.7gb of memory.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ndef test_iterations(iterations_per_step, players):\n    def test(step, player):\n        now = time.perf_counter()\n        if player not in players:\n            print(player, step, \"%.5f\" % now)\n            return now * 10\n        else:\n            z = []\n            for i in range(iterations_per_step * step):\n                z.append(random.random())\n            then = time.perf_counter()\n            interval = then - now\n            print(player, step, \"%.5f\" % now, \"%.5f\" % interval)\n            return interval * 100\n    return test\n\ntest_performance = test_iterations(420000, [1, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Perform heavy work and measure duration as Player 2, Measure start and end as Player 1 + 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_performance_once = test_iterations(420000, [1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4. Same as 2. but 10x less work.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_performance_light = test_iterations(42000, [1, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hardware Specs\n\n- Workstation:\n    - 16gb DDR4\n    - i9 ten-core 3.7ghz\n    - (12.34s, 12.35s) for the the heavy loop at step 350 (without del z)\n    - (13.69s, 13.70s) (with del z)\n- Old Gaming Laptop:\n    - 32gb DDR3\n    - i7 quad-core 2.4ghz\n    - (17.38s, 17.39s)\n    - (18.58s, 18.60s)\n- Kaggle Notebook:\n    - 16gb\n    - 36-42s\n- Online Runner:\n    - > 70s (estimate from graphs in the next cells)\n- Codeforces: (only using 42000)\n    - (2.6520170000000003, 2.650512571)\n    - (2.7612177, 2.7920572359999998)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport random\n\ndef final_loop():\n    a = time.process_time()\n    at = time.perf_counter()\n    z = []\n    for i in range(420000 * 350):\n        z.append(random.random())\n    del z\n    b = time.process_time()\n    bt = time.perf_counter()\n    return b - a, bt - at\n\n#final_loop()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\ndef fix_scale(data, scales):\n    return [[None if x is None else x * scale for x, scale in zip(values, scales)] for values in data]\n\ndef load_result(file, scales):\n    with open(file, \"r\") as f:\n        data = json.load(f)\n    steps = data[\"steps\"]\n    counts = decode(steps)\n    total = fix_scale(get_total(counts), scales)\n    deltas = fix_scale(get_deltas(counts), scales)\n    return total, deltas\n\nresult_labels = [\"workstation\", \"laptop\", \"notebook\", \"online\"]\nresults_timeout = [load_result(file, [1/10, 1/10, 1/10, 1/10]) for file in [\n    \"/kaggle/input/halite-4-hardware-performance/local_test_timeout.json\",\n    \"/kaggle/input/halite-4-hardware-performance/laptop_test_timeout.json\",\n    \"/kaggle/input/halite-4-hardware-performance/notebook_test_timeout.json\",\n    \"/kaggle/input/halite-4-hardware-performance/1577246.json\",\n]]\nresults_performance = [load_result(file, [1/10, 1/100, 1/100, 1/10]) for file in [\n    \"/kaggle/input/halite-4-hardware-performance/local_test_performance.json\",\n    \"/kaggle/input/halite-4-hardware-performance/laptop_test_performance.json\",\n    \"/kaggle/input/halite-4-hardware-performance/notebook_test_performance.json\",\n    \"/kaggle/input/halite-4-hardware-performance/1577603.json\",\n]]\nresults_performance_once = [load_result(file, [1/10, 1/100, 1/10, 1/10]) for file in [\n    \"/kaggle/input/halite-4-hardware-performance/local_test_performance_once.json\",\n    \"/kaggle/input/halite-4-hardware-performance/laptop_test_performance_once.json\",\n    \"/kaggle/input/halite-4-hardware-performance/notebook_test_performance_once.json\",\n    \"/kaggle/input/halite-4-hardware-performance/1578314.json\",\n]]\nresults_performance_light = [load_result(file, [1/10, 1/100, 1/100, 1/10]) for file in [\n    \"/kaggle/input/halite-4-hardware-performance/local_test_performance_light.json\",\n    \"/kaggle/input/halite-4-hardware-performance/laptop_test_performance_light.json\",\n    \"/kaggle/input/halite-4-hardware-performance/notebook_test_performance_light.json\",\n    \"/kaggle/input/halite-4-hardware-performance/1581528.json\",\n]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Timeout","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for title, (total, deltas) in zip(result_labels, results_timeout):\n    plt.figure(tight_layout=True, figsize=(12, 4))\n    plt.title(title)\n    plt.grid()\n    plt.plot(deltas)\n    player_labels()\n    plt.axhline(23.2, color=\"black\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphs all show expected behaviour. They show that all agents sleep for .15s (therefore each agent measures `4*.15s` = .6s). And on every 10th step each agent sleeps longer. They all error out on step 300, as every agent wants to sleep for the maximum timeout of 6s. On step 290, all agents sleep for 5.8s which accumulates to `4*5.8s` = 23.2s, the peak value of player 1. This means that all agents are evaluated in sequence. Another thing to note is that players 2, 3 and 4 show lower peak values. Unlike player 1, they see the long waits distributed over 2 timesteps.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Performance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pick(data, players):\n    return [[x if player in players else None for player, x in enumerate(values)] for values in data]\n\ndef inverse_pick(data, players):\n    return [[x if player not in players else None for player, x in enumerate(values)] for values in data]\n\ndef show_performance(results, players):\n    for title, (total, deltas) in zip(result_labels, results):\n        plt.figure(tight_layout=True, figsize=(24, 4))\n        plt.subplot(121)\n        plt.title(title)\n        plt.grid()\n        plt.plot([None if values[0] is None or values[3] is None else values[3] - values[0] for values in total])\n        plt.legend([\"start(4)-start(1)\"])\n        plt.subplot(122)\n        plt.title(title)\n        plt.grid()\n        plt.plot(pick(total, players))\n        player_labels()\n    plt.show()\n\nshow_performance(results_performance, [1, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The left diagram shows the time between entering run of the the first player and entering run of the last player. The right diagram shows the tight timing of executing the loops.  The workstation and laptop runs show a smooth linear increasing time. However the notebook and especially the online runner perform very poorly with big spikes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Performance Single Agent","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_performance(results_performance_once, [1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Performance Light","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_performance(results_performance_light, [1, 2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance Histogram","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Following diagrams show iterations per second for each device. Black bar shows average iterations per second, red bar shows the minimum iterations per second. However, the minimum iterations per second does not include the step where the agent has failed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_iterations_per_second(results, players, ops):\n    results_speeds = []\n    for total, delta in results:\n        speeds = []\n        for step, values in enumerate(total):\n            for player in players:\n                interval = values[player]\n                if interval is not None:\n                    speeds.append(ops * step / interval)\n        results_speeds.append(speeds)\n    plt.figure(tight_layout=True, figsize=(12, 4))\n    plt.xlim(0, 1.5e7)\n    for speeds in results_speeds:\n        plt.hist(speeds, histtype=\"step\", bins=10, weights=[1/len(speeds)]*len(speeds))\n    plt.legend(result_labels)\n    plt.xlabel(\"iterations/s\")\n    for speeds in results_speeds:\n        plt.axvline(sum(speeds) / len(speeds), color=\"black\")\n        plt.axvline(min(speeds), color=\"red\")\n    plt.grid()\n    plt.show()\n    print(result_labels)\n    print([sum(x)/len(x) for x in results_speeds])\n    print([min(x) for x in results_speeds])\n    return results_speeds\n\nips_performance = show_iterations_per_second(results_performance, [1, 2], 420e3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ips_performance_light = show_iterations_per_second(results_performance_light, [1, 2], 42e3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluation in Notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_notebook = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_notebook:\n    result = evaluate(run_binary(test_timeout)).render(mode=\"json\")\n    with open(\"/kaggle/working/notebook_test_timeout.json\", \"w\") as f:\n        f.write(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_notebook:\n    result = evaluate(run_binary(test_performance)).render(mode=\"json\")\n    with open(\"/kaggle/working/notebook_test_performance.json\", \"w\") as f:\n        f.write(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_notebook:\n    result = evaluate(run_binary(test_performance_once)).render(mode=\"json\")\n    with open(\"/kaggle/working/notebook_test_performance_once.json\", \"w\") as f:\n        f.write(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if run_notebook:\n    result = evaluate(run_binary(test_performance_light)).render(mode=\"json\")\n    with open(\"/kaggle/working/notebook_test_performance_light.json\", \"w\") as f:\n        f.write(result)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}