{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Best Model for TPSAPR22 Without Neural Networks\n\nThis notebook shows how to solve TPSAPR22 with good feature engineering and a `HistGradientBoostingClassifier`. It furthermore shows how to cross-validate correctly without creating a data leak.\n\nSome features have been inspired by C4rl05/V's [XGBoost notebook](https://www.kaggle.com/code/cv13j0/tps-apr-2022-xgboost-model).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nfrom cycler import cycler\nfrom IPython.display import display\nimport datetime\nimport scipy.stats\n\nfrom sklearn.model_selection import GroupKFold, cross_val_score\nfrom sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import make_pipeline\n\npd.set_option(\"precision\", 3)\nplt.rcParams['axes.facecolor'] = '#0057b8' # blue\nplt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-07T07:22:19.488929Z","iopub.execute_input":"2022-04-07T07:22:19.489349Z","iopub.status.idle":"2022-04-07T07:22:20.868582Z","shell.execute_reply.started":"2022-04-07T07:22:19.489226Z","shell.execute_reply":"2022-04-07T07:22:20.867864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the data\n\nWe read the data and pivot the training data so that we have a dataframe with one row per sequence.","metadata":{}},{"cell_type":"code","source":"# Reading the data\ntrain = pd.read_csv('../input/tabular-playground-series-apr-2022/train.csv')\ntrain_labels = pd.read_csv('../input/tabular-playground-series-apr-2022/train_labels.csv')\ntest = pd.read_csv('../input/tabular-playground-series-apr-2022/test.csv')\n\nsensors = [col for col in train.columns if 'sensor_' in col]\n\ntrain_pivoted0 = train.pivot(index=['sequence', 'subject'], columns='step', values=sensors)\ndisplay(train_pivoted0)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:20.869886Z","iopub.execute_input":"2022-04-07T07:22:20.870576Z","iopub.status.idle":"2022-04-07T07:22:33.443756Z","shell.execute_reply.started":"2022-04-07T07:22:20.87054Z","shell.execute_reply":"2022-04-07T07:22:33.443028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering\n\nLet's keep it simple and calculate only the following features:\n- For every sensor, we calculate mean, standard deviation, interquartile range, standard deviation divided by mean, and kurtosis. This gives the first 5\\*13=65 features.\n- For the special sensor_02, we count how many times it goes up or down.\n- For sensor_02, we calculate the sum of all upward / downward steps, the maximum of all upward / downward steps, and the mean of all upward / downward steps. \n- For every subject, we count how many sequences belong to it, and we add this count as a feature to all sequences of the subject (the [EDA](https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense) gives the motivation for this feature). \n\nNow we have 74 features. ","metadata":{}},{"cell_type":"code","source":"# Feature engineering\ndef engineer(df):\n    new_df = pd.DataFrame([], index=df.index)\n    for sensor in sensors:\n        new_df[sensor + '_mean'] = df[sensor].mean(axis=1)\n        new_df[sensor + '_std'] = df[sensor].std(axis=1)\n        new_df[sensor + '_iqr'] = scipy.stats.iqr(df[sensor], axis=1)\n        new_df[sensor + '_sm'] = np.nan_to_num(new_df[sensor + '_std'] / \n                                               new_df[sensor + '_mean'].abs()).clip(-1e30, 1e30)\n        new_df[sensor + '_kurtosis'] = scipy.stats.kurtosis(df[sensor], axis=1)\n    new_df['sensor_02_up'] = (df.sensor_02.diff(axis=1) > 0).sum(axis=1)\n    new_df['sensor_02_down'] = (df.sensor_02.diff(axis=1) < 0).sum(axis=1)\n    new_df['sensor_02_upsum'] = df.sensor_02.diff(axis=1).clip(0, None).sum(axis=1)\n    new_df['sensor_02_downsum'] = df.sensor_02.diff(axis=1) .clip(None, 0).sum(axis=1)\n    new_df['sensor_02_upmax'] = df.sensor_02.diff(axis=1).max(axis=1)\n    new_df['sensor_02_downmax'] = df.sensor_02.diff(axis=1).min(axis=1)\n    new_df['sensor_02_upmean'] = np.nan_to_num(new_df['sensor_02_upsum'] / new_df['sensor_02_up'], posinf=40)\n    new_df['sensor_02_downmean'] = np.nan_to_num(new_df['sensor_02_downsum'] / new_df['sensor_02_down'], neginf=-40)\n    return new_df\n\ntrain_pivoted = engineer(train_pivoted0)\n\ntrain_shuffled = train_pivoted.sample(frac=1.0, random_state=1)\nlabels_shuffled = train_labels.reindex(train_shuffled.index.get_level_values('sequence'))\nlabels_shuffled = labels_shuffled[['state']].merge(train[['sequence', 'subject']].groupby('sequence').min(),\n                                                   how='left', on='sequence')\nlabels_shuffled = labels_shuffled.merge(labels_shuffled.groupby('subject').size().rename('sequence_count'),\n                                        how='left', on='subject')\ntrain_shuffled['sequence_count_of_subject'] = labels_shuffled['sequence_count'].values\n\nselected_columns = train_shuffled.columns\nprint(len(selected_columns))\n#train_shuffled.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:33.44546Z","iopub.execute_input":"2022-04-07T07:22:33.445833Z","iopub.status.idle":"2022-04-07T07:22:35.245758Z","shell.execute_reply.started":"2022-04-07T07:22:33.445789Z","shell.execute_reply":"2022-04-07T07:22:35.244867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get a first impression of the usefulness of the 74 features, we plot how the target depends on every feature, i.e., a diagram of $P(y=1|x)$.  To get a meaningful plot, we apply two transformations:\n- The x axis is not the value of the feature, but its index (when sorted by feature value).\n- The y axis is not the target value (which can be only 0 or 1), but a rolling mean over 1000 targets.\n\nThe diagram shows bad features with an almost horizontal line (the probability of the positive target is 0.5 independently of the feature value) (e.g. sensor_05_std). Good features have a curve with high y_max - y_min (e.g. sensor_02_std). ","metadata":{}},{"cell_type":"code","source":"# Plot dependence between every feature and the target\nncols = len(train_shuffled.columns) // 13\nplt.subplots(15, ncols, sharey=True, sharex=True, figsize=(15, 40))\nfor i, col in enumerate(train_shuffled.columns):\n    temp = pd.DataFrame({col: train_shuffled[col].values,\n                         'state': labels_shuffled.state.values})\n    temp = temp.sort_values(col)\n    temp.reset_index(inplace=True)\n    plt.subplot(15, ncols, i+1)\n    plt.scatter(temp.index, temp.state.rolling(1000).mean(), s=2)\n    plt.xlabel(col)\n    plt.xticks([])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:35.248014Z","iopub.execute_input":"2022-04-07T07:22:35.248356Z","iopub.status.idle":"2022-04-07T07:22:45.397188Z","shell.execute_reply.started":"2022-04-07T07:22:35.248313Z","shell.execute_reply":"2022-04-07T07:22:45.396224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature selection\n\nWe don't need all 74 features. In a first step we drop 26 features which proved to be useless in a previous run of the notebook.","metadata":{}},{"cell_type":"code","source":"# Drop some useless features\ndropped_features = ['sensor_05_kurtosis', 'sensor_08_mean',\n                    'sensor_05_std', 'sensor_06_kurtosis',\n                    'sensor_06_std', 'sensor_03_std',\n                    'sensor_02_kurtosis', 'sensor_03_kurtosis',\n                    'sensor_09_kurtosis', 'sensor_03_mean',\n                    'sensor_00_mean', 'sensor_02_iqr',\n                    'sensor_05_mean', 'sensor_06_mean',\n                    'sensor_07_std', 'sensor_10_iqr',\n                    'sensor_11_iqr', 'sensor_12_iqr',\n                    'sensor_09_mean', 'sensor_02_sm',\n                    'sensor_03_sm', 'sensor_05_iqr', \n                    'sensor_06_sm', 'sensor_09_iqr', \n                    'sensor_07_iqr', 'sensor_10_mean']\nselected_columns = [f for f in selected_columns if f not in dropped_features]\nlen(selected_columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:45.398577Z","iopub.execute_input":"2022-04-07T07:22:45.39904Z","iopub.status.idle":"2022-04-07T07:22:45.40689Z","shell.execute_reply.started":"2022-04-07T07:22:45.399004Z","shell.execute_reply":"2022-04-07T07:22:45.40636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we select features sequentially. We start with zero features and add one feature after the other. In every step we select the feature which increases the model's validation score the most. In this example, we select all features, and the output tells us which features are useful and which aren't.\n\nThe same algorithm can be run backward by setting `backward` to `True`. It then starts with all features and repeatedly deletes the feature which adds the least value to the model's validation score.\n\nThe model is a `HistGradientBoostingClassifier`.","metadata":{}},{"cell_type":"code","source":"# Sequential feature selection\n# This code is a more verbose form of scikit-learn's SequentialFeatureSelector\nestimator = HistGradientBoostingClassifier(learning_rate=0.05, max_leaf_nodes=25,\n                                       max_iter=1000, min_samples_leaf=500,\n                                       l2_regularization=1,\n                                       max_bins=255,\n                                       random_state=4, verbose=0)\n\nX, y = train_shuffled[selected_columns], labels_shuffled.state\nn_iterations, backward = 48, False\n\nif n_iterations != 0:\n    n_features = X.shape[1]\n    current_mask = np.zeros(shape=n_features, dtype=bool)\n    history = []\n    for _ in range(n_iterations):\n        candidate_feature_indices = np.flatnonzero(~current_mask)\n        scores = {}\n        for feature_idx in candidate_feature_indices:\n            candidate_mask = current_mask.copy()\n            candidate_mask[feature_idx] = True\n            X_new = X.values[:, ~candidate_mask if backward else candidate_mask]\n            scores[feature_idx] = cross_val_score(\n                estimator,\n                X_new,\n                y,\n                cv=GroupKFold(n_splits=5),\n                groups=train_shuffled.index.get_level_values('subject'),\n                scoring='roc_auc',\n                n_jobs=-1,\n            ).mean()\n            #print(f\"{str(X.columns[feature_idx]):30} {scores[feature_idx]:.3f}\")\n        new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])\n        current_mask[new_feature_idx] = True\n        history.append(scores[new_feature_idx])\n        new = 'Deleted' if backward else 'Added'\n        print(f'{new} feature: {str(X.columns[new_feature_idx]):30}'\n              f' {scores[new_feature_idx]:.3f}')\n\n    print()\n    plt.figure(figsize=(12, 6))\n    plt.scatter(np.arange(len(history)) + (0 if backward else 1), history)\n    plt.ylabel('AUC')\n    plt.xlabel('Features removed' if backward else 'Features added')\n    plt.title('Sequential Feature Selection')\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.show()\n\n    if backward:\n        current_mask = ~current_mask\n    selected_columns = np.array(selected_columns)[current_mask]\n    print(selected_columns)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:45.407878Z","iopub.execute_input":"2022-04-07T07:22:45.408481Z","iopub.status.idle":"2022-04-07T07:22:45.427602Z","shell.execute_reply.started":"2022-04-07T07:22:45.408449Z","shell.execute_reply":"2022-04-07T07:22:45.426924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cross-validation\n\nFor cross-validation, we use a GroupKFold grouped on subjects. If we didn't group on subjects, we'd have a data leak (see the [EDA](https://www.kaggle.com/code/ambrosm/tpsapr22-eda-which-makes-sense) for an explanation).\n\nThe model is a `HistGradientBoostingClassifier`; I got the same cv score using an `XGBClassifier`.","metadata":{}},{"cell_type":"code","source":"%%time\n# Cross-validation of the classifier\n\nprint(f\"{len(selected_columns)} features\")\nscore_list = []\nkf = GroupKFold(n_splits=5)\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(train_shuffled, groups=train_shuffled.index.get_level_values('subject'))):\n    X_tr = train_shuffled.iloc[idx_tr][selected_columns]\n    X_va = train_shuffled.iloc[idx_va][selected_columns]\n    y_tr = labels_shuffled.iloc[idx_tr].state\n    y_va = labels_shuffled.iloc[idx_va].state\n\n    model = HistGradientBoostingClassifier(learning_rate=0.05, max_leaf_nodes=25,\n                                           max_iter=1000, min_samples_leaf=500,\n                                           l2_regularization=1,\n                                           validation_fraction=0.05,\n                                           max_bins=63,\n                                           random_state=3, verbose=0)\n#     model = XGBClassifier(n_estimators=500, n_jobs=-1,\n#                           eval_metric=['logloss'],\n#                           #max_depth=10,\n#                           colsample_bytree=0.8,\n#                           #gamma=1.4,\n#                           reg_alpha=6, reg_lambda=1.5,\n#                           tree_method='hist',\n#                           learning_rate=0.03,\n#                           verbosity=1,\n#                           use_label_encoder=False, random_state=3)\n\n    if True or type(model) != XGBClassifier:\n        model.fit(X_tr.values, y_tr)\n    else:\n        model.fit(X_tr.values, y_tr, eval_set = [(X_va.values, y_va)], \n                  eval_metric = ['auc'], early_stopping_rounds=30, verbose=10)\n    try:\n        y_va_pred = model.decision_function(X_va.values) # HistGradientBoostingClassifier\n    except AttributeError:\n        try:\n            y_va_pred = model.predict_proba(X_va.values)[:,1] # XGBClassifier\n        except AttributeError:\n            y_va_pred = model.predict(X_va.values) # XGBRegressor\n    score = roc_auc_score(y_va, y_va_pred)\n    try:\n        print(f\"Fold {fold}: n_iter ={model.n_iter_:5d}    AUC = {score:.3f}\")\n    except AttributeError:\n        print(f\"Fold {fold}:                  AUC = {score:.3f}\")\n    score_list.append(score)\n    \nprint(f\"OOF AUC:                       {np.mean(score_list):.3f}\") # 0.944\n","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:45.4294Z","iopub.execute_input":"2022-04-07T07:22:45.429854Z","iopub.status.idle":"2022-04-07T07:22:59.679269Z","shell.execute_reply.started":"2022-04-07T07:22:45.429808Z","shell.execute_reply":"2022-04-07T07:22:59.678546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROC curve\n\nWe plot the ROC curve just because it looks nice. The area under the red curve is the score of our model.","metadata":{}},{"cell_type":"code","source":"# Plot the roc curve for the last fold\ndef plot_roc_curve(y_va, y_va_pred):\n    plt.figure(figsize=(8, 8))\n    fpr, tpr, _ = roc_curve(y_va, y_va_pred)\n    plt.plot(fpr, tpr, color='r', lw=2)\n    plt.plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\n    plt.gca().set_aspect('equal')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"Receiver operating characteristic\")\n    plt.show()\n\nplot_roc_curve(y_va, y_va_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:59.68279Z","iopub.execute_input":"2022-04-07T07:22:59.68354Z","iopub.status.idle":"2022-04-07T07:22:59.898917Z","shell.execute_reply.started":"2022-04-07T07:22:59.683482Z","shell.execute_reply":"2022-04-07T07:22:59.89808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test predictions and submission\n\nWe create a submission file as follows:\n1. We apply the same feature engineering to the test data as we did for the training data. Here it is important not to shuffle the test data so that the submission file is ordered correctly.\n2. We retrain the `HistGradientBoostingClassifier` 100 times with different seeds on 95 % of the training data.\n3. The decision functions of the 100 models can have different scales. To counter this, we convert the predictions to ranks using `scipy.stats.rankdata` and then submit the mean of the 100 ranks.","metadata":{}},{"cell_type":"code","source":"# Feature engineering for test\ntest_pivoted0 = test.pivot(index=['sequence', 'subject'], columns='step', values=sensors)\ntest_pivoted = engineer(test_pivoted0)\nsequence_count = test_pivoted.index.to_frame(index=False).groupby('subject').size().rename('sequence_count_of_subject')\n#display(test_pivoted.head(2))\nsubmission = pd.DataFrame({'sequence': test_pivoted.index.get_level_values('sequence')})\ntest_pivoted = test_pivoted.merge(sequence_count, how='left', on='subject')\ntest_pivoted.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:22:59.900082Z","iopub.execute_input":"2022-04-07T07:22:59.900352Z","iopub.status.idle":"2022-04-07T07:23:00.975384Z","shell.execute_reply.started":"2022-04-07T07:22:59.900323Z","shell.execute_reply":"2022-04-07T07:23:00.97458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Retrain, predict and write submission\nprint(f\"{len(selected_columns)} features\")\n\npred_list = []\nfor seed in range(100):\n    X_tr = train_shuffled[selected_columns]\n    y_tr = labels_shuffled.state\n\n    model = HistGradientBoostingClassifier(learning_rate=0.05, max_leaf_nodes=25,\n                                           max_iter=1000, min_samples_leaf=500,\n                                           validation_fraction=0.05,\n                                           l2_regularization=1,\n                                           max_bins=63,\n                                           random_state=seed, verbose=0)\n    model.fit(X_tr.values, y_tr)\n    pred_list.append(scipy.stats.rankdata(model.decision_function(test_pivoted[selected_columns].values)))\n    print(f\"{seed:2}\", pred_list[-1])\nprint()\nsubmission['state'] = sum(pred_list) / len(pred_list)\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-04-07T07:23:00.97769Z","iopub.execute_input":"2022-04-07T07:23:00.977926Z","iopub.status.idle":"2022-04-07T07:27:49.966113Z","shell.execute_reply.started":"2022-04-07T07:23:00.977897Z","shell.execute_reply":"2022-04-07T07:27:49.964664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}