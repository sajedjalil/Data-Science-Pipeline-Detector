{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\n\n* Create a validation scheme that can mesure the generalization capacity of our model\n* We need to predict unknown batches (in other words in the training section, i don't believe that training and predicting the same batch is a good validation scheme)                                      "},{"metadata":{},"cell_type":"markdown","source":"# Validation Scheme\n\nThis validation tries to align with the test set in the following way.\n\nTrain 10 models with the training set following this combination:\n    * Leave batch 1 out, train with batch 2, 3, 4, 5, 6, 7, 8, 9, 10 (stratified, shuffle true)\n    * Leave batch 2 out, train with batch 1, 3, 4, 5, 6, 7, 8, 9, 10 (stratified, shuffle true)\n    * ---------------------------------------------------------------------------------------------------\n    * Leave batch 10 out, train with batch 1, 2, 3, 4, 5, 6, 7, 8, 9 (stratified, shuffle true)\n    \nThe batches that were left out will be the ones we will predict (predict unknown batch)\n\nPlz let me know if you find a bug or have any comment! Thanks!."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading training, testing and submission data...')\n    train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\n    test = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')\n    submission = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time':str})\n    print('Train set has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n    print('Test set has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n    return train, test, submission\n\ntrain, test, submission = read_data()\n\n# concatenate data\nbatch = 50\ntotal_batches = 14\ntrain['set'] = 'train'\ntest['set'] = 'test'\ndata = pd.concat([train, test])\nfor i in range(int(total_batches)):\n    data.loc[(data['time'] > i * batch) & (data['time'] <= (i + 1) * batch), 'batch'] = i + 1\ntrain = data[data['set'] == 'train']\ntest = data[data['set'] == 'test']\ndel data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a lot of features\ndef preprocess(train, test):\n    \n    pre_train = train.copy()\n    pre_test = test.copy()\n    \n    for df in [pre_train, pre_test]:\n        for window in [10000, 20000, 30000, 40000]:\n            # roll backwards\n            df['signalmean_t' + str(window)] = df.groupby(['batch'])['signal'].shift(1).rolling(window).mean()\n            df['signalvar_t' + str(window)] = df.groupby(['batch'])['signal'].shift(1).rolling(window).var()\n            df['signalstd_t' + str(window)] = df.groupby(['batch'])['signal'].shift(1).rolling(window).std()\n            df['signalmin_t' + str(window)] = df.groupby(['batch'])['signal'].shift(1).rolling(window).min()\n            df['signalmax_t' + str(window)] = df.groupby(['batch'])['signal'].shift(1).rolling(window).max()\n\n            min_max = (df['signal'] - df['signalmin_t' + str(window)]) / (df['signalmax_t' + str(window)] - df['signalmin_t' + str(window)])\n            df['norm_t' + str(window)] = min_max * (np.floor(df['signalmax_t' + str(window)]) - np.ceil(df['signalmin_t' + str(window)]))\n\n            # roll forward\n            df['signalmean_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].shift(- window - 1).rolling(window).mean()\n            df['signalvar_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].shift(- window - 1).rolling(window).var()\n            df['signalstd_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].shift(- window - 1).rolling(window).std()\n            df['signalmin_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].shift(- window - 1).rolling(window).min()\n            df['signalmax_t' + str(window) + '_lead'] = df.groupby(['batch'])['signal'].shift(- window - 1).rolling(window).max()\n\n            min_max = (df['signal'] - df['signalmin_t' + str(window) + '_lead']) / (df['signalmax_t' + str(window) + '_lead'] - df['signalmin_t' + str(window) + '_lead'])\n            df['norm_t' + str(window) + '_lead'] = min_max * (np.floor(df['signalmax_t' + str(window) + '_lead']) - np.ceil(df['signalmin_t' + str(window) + '_lead']))        \n\n    del train, test, min_max\n    \n    return pre_train, pre_test\n\n# feature engineering\npre_train, pre_test = preprocess(train, test)\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(pre_train, pre_test, features, params, get_sample = True, get_metrics = True):\n    \n    pre_train = pre_train.copy()\n    pre_test = pre_test.copy()\n    \n    # get a random sample for faster training\n    if get_sample:\n        pre_train = pre_train.sample(frac = 0.1, random_state = 20)\n    \n    pre_train.reset_index(drop = True, inplace = True)\n    pre_test.reset_index(drop = True, inplace = True)\n    \n    target = 'open_channels'\n    \n    x_train, x_val, y_train, y_val = train_test_split(pre_train[features], pre_train[target], stratify = pre_train[target], \n                                                      random_state = 42)\n    train_set = lgb.Dataset(x_train, y_train)\n    val_set = lgb.Dataset(x_val, y_val)\n        \n    model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 50, \n                      valid_sets = [train_set, val_set], verbose_eval = 2000)\n    \n    val_pred = model.predict(x_val) \n    y_pred = model.predict(pre_test[features])\n        \n    rmse_score = np.sqrt(metrics.mean_squared_error(y_val, val_pred))\n    # want to clip and then round predictions\n    val_pred = np.round(np.clip(val_pred, 0, 10)).astype(int)\n    round_y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n    f1 = metrics.f1_score(y_val, val_pred, average = 'macro')\n    if get_metrics:\n        print(f'Our val f1_score is {f1}')\n        print(f'Our rmse score is {rmse_score}')\n    \n    return f1, round_y_pred, y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define hyperparammeter (some random hyperparammeters)\nparams = {'learning_rate': 0.2, \n          'feature_fraction': 0.75, \n          'bagging_fraction': 0.75,\n          'bagging_freq': 1,\n          'n_jobs': -1, \n          'seed': 50,\n          'metric': 'rmse'\n        }\n\n# trian and predict the test set with all the features that were build\nfeatures = [col for col in pre_train.columns if col not in ['open_channels', 'time', 'batch', 'set']]\nprint(f'Training with {len(features)} features')\ntest_f1, test_r_pred, test_pred = run_lgb(pre_train, pre_test, features, params, get_sample = False, get_metrics = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# leave one batch out, trian with 9 (predict all 10 batches as our out of folds)\ndef val_strat(pre_train, pre_test, features, params, get_sample = True, get_metrics = True):\n    \n    y_train = pre_train['open_channels']\n    \n    f1_1, r_y_1, y_1 = run_lgb(pre_train[pre_train['batch']!=1], pre_train[pre_train['batch']==1], features, params, get_sample, get_metrics)\n    f1_2, r_y_2, y_2 = run_lgb(pre_train[pre_train['batch']!=2], pre_train[pre_train['batch']==2], features, params, get_sample, get_metrics)\n    f1_3, r_y_3, y_3 = run_lgb(pre_train[pre_train['batch']!=3], pre_train[pre_train['batch']==3], features, params, get_sample, get_metrics)\n    f1_4, r_y_4, y_4 = run_lgb(pre_train[pre_train['batch']!=4], pre_train[pre_train['batch']==4], features, params, get_sample, get_metrics)\n    f1_5, r_y_5, y_5 = run_lgb(pre_train[pre_train['batch']!=5], pre_train[pre_train['batch']==5], features, params, get_sample, get_metrics)\n    f1_6, r_y_6, y_6 = run_lgb(pre_train[pre_train['batch']!=6], pre_train[pre_train['batch']==6], features, params, get_sample, get_metrics)\n    f1_7, r_y_7, y_7 = run_lgb(pre_train[pre_train['batch']!=7], pre_train[pre_train['batch']==7], features, params, get_sample, get_metrics)\n    f1_8, r_y_8, y_8 = run_lgb(pre_train[pre_train['batch']!=8], pre_train[pre_train['batch']==8], features, params, get_sample, get_metrics)\n    f1_9, r_y_9, y_9 = run_lgb(pre_train[pre_train['batch']!=9], pre_train[pre_train['batch']==9], features, params, get_sample, get_metrics)\n    f1_10, r_y_10, y_10 = run_lgb(pre_train[pre_train['batch']!=10], pre_train[pre_train['batch']==10], features, params, get_sample, get_metrics)\n    \n    round_y_pred = np.hstack([r_y_1, r_y_2, r_y_3, r_y_4, r_y_5, r_y_6, r_y_7, r_y_8, r_y_9, r_y_10])\n    y_pred = np.hstack([y_1, y_2, y_3, y_4, y_5, y_6, y_7, y_8, y_9, y_10])\n    macro_mean_f1_score = (f1_1 + f1_2 + f1_3 + f1_4 + f1_5 + f1_6 + f1_7 + f1_8 + f1_9 + f1_10) / 10\n    \n    macro_f1_score = metrics.f1_score(y_train, round_y_pred, average = 'macro')\n    rmse_score = np.sqrt(metrics.mean_squared_error(y_train, y_pred))\n    print(f'Our mean macro f1 score for the 10 folds is {macro_mean_f1_score}')\n    print(f'Our out of folds macro f1 score is {macro_f1_score}')\n    print(f'Our out of folds rmse score is {rmse_score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# using a sample of 10% for demonstration purpose (similar results)\nval_strat(pre_train, pre_test, features, params, get_sample = True, get_metrics = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validating with this strategy gives really bad results, be carefull!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.open_channels = test_r_pred\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}