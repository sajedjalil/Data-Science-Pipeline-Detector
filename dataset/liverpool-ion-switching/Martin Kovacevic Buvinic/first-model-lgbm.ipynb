{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Objective\n\n* Try different features and see if our oof align with public leaderboard\n* You can find my exploratory data analysis in this link: https://www.kaggle.com/ragnar123/exploratory-data-analysis-and-model-baseline"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport gc\nwarnings.filterwarnings('ignore')\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error, f1_score\nfrom sklearn.preprocessing import StandardScaler\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data():\n    print('Reading training, testing and submission data...')\n    train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\n    test = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')\n    submission = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time':str})\n    print('Train set has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n    print('Test set has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n    return train, test, submission\n\ntrain, test, submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the exploratory data analysis we know that we have 10 batches for training and 4 batches in the test. I believe this batches are independent from each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate data\nbatch = 50\ntotal_batches = 14\ntrain['set'] = 'train'\ntest['set'] = 'test'\ndata = pd.concat([train, test])\nfor i in range(int(total_batches)):\n    data.loc[(data['time'] > i * batch) & (data['time'] <= (i + 1) * batch), 'batch'] = i + 1\ntrain = data[data['set'] == 'train']\ntest = data[data['set'] == 'test']\ndel data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(train, test):\n    \n    pre_train = train.copy()\n    pre_test = test.copy()\n    \n    batch1 = pre_train[pre_train[\"batch\"] == 1]\n    batch2 = pre_train[pre_train[\"batch\"] == 2]\n    batch3 = pre_train[pre_train[\"batch\"] == 3]\n    batch4 = pre_train[pre_train[\"batch\"] == 4]\n    batch5 = pre_train[pre_train[\"batch\"] == 5]\n    batch6 = pre_train[pre_train[\"batch\"] == 6]\n    batch7 = pre_train[pre_train[\"batch\"] == 7]\n    batch8 = pre_train[pre_train[\"batch\"] == 8]\n    batch9 = pre_train[pre_train[\"batch\"] == 9]\n    batch10 = pre_train[pre_train[\"batch\"] == 10]\n    batch11 = pre_test[pre_test['batch'] == 11]\n    batch12 = pre_test[pre_test['batch'] == 12]\n    batch13 = pre_test[pre_test['batch'] == 13]\n    batch14 = pre_test[pre_test['batch'] == 14]\n    batches = [batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10, batch11, batch12, batch13, batch14]\n    \n    for batch in batches:\n        for feature in ['signal']:\n            # some random rolling features\n            for window in [50, 100, 1000, 5000, 10000, 25000]:\n                # roll backwards\n                batch[feature + 'mean_t' + str(window)] = batch[feature].shift(1).rolling(window).mean()\n                batch[feature + 'std_t' + str(window)] = batch[feature].shift(1).rolling(window).std()\n                batch[feature + 'min_t' + str(window)] = batch[feature].shift(1).rolling(window).min()\n                batch[feature + 'max_t' + str(window)] = batch[feature].shift(1).rolling(window).max()\n                min_max = (batch[feature] - batch[feature + 'min_t' + str(window)]) / (batch[feature + 'max_t' + str(window)] - batch[feature + 'min_t' + str(window)])\n                batch['norm_t' + str(window)] = min_max * (np.floor(batch[feature + 'max_t' + str(window)]) - np.ceil(batch[feature + 'min_t' + str(window)]))\n                \n#                 # roll forward\n#                 batch[feature + 'mean_t' + str(window) + '_lead'] = batch[feature].shift(- window - 1).rolling(window).mean()\n#                 batch[feature + 'std_t' + str(window) +'_lead'] = batch[feature].shift(- window - 1).rolling(window).std()\n#                 batch[feature + 'min_t' + str(window) + '_lead'] = batch[feature].shift(- window - 1).rolling(window).min()\n#                 batch[feature + 'max_t' + str(window) + '_lead'] = batch[feature].shift(- window - 1).rolling(window).max()\n#                 min_max = (batch[feature] - batch[feature + 'min_t' + str(window) + '_lead']) / (batch[feature + 'max_t' + str(window) + '_lead'] - batch[feature + 'min_t' + str(window) + '_lead'])\n#                 batch['norm_t' + str(window) + '_lead'] = min_max * (np.floor(batch[feature + 'max_t' + str(window) + '_lead']) - np.ceil(batch[feature + 'min_t' + str(window) + '_lead']))\n                \n    pre_train = pd.concat([batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10])\n    pre_test = pd.concat([batch11, batch12, batch13, batch14])\n    \n    del batches, batch1, batch2, batch3, batch4, batch5, batch6, batch7, batch8, batch9, batch10, batch11, batch12, batch13, batch14, train, test, min_max\n    \n    return pre_train, pre_test\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        if col!='open_channels':\n            col_type = df[col].dtypes\n            if col_type in numerics:\n                c_min = df[col].min()\n                c_max = df[col].max()\n                if str(col_type)[:3] == 'int':\n                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)  \n                else:\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float16)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\n\n\ndef scale_fillna(pre_train, pre_test):\n    features = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]\n    pre_train = pre_train.replace([np.inf, -np.inf], np.nan)\n    pre_test = pre_test.replace([np.inf, -np.inf], np.nan)\n    pre_train.fillna(0, inplace = True)\n    pre_test.fillna(0, inplace = True)\n#     scaler = StandardScaler()\n#     pre_train[features] = scaler.fit_transform(pre_train[features])\n#     pre_test[features] = scaler.transform(pre_test[features])\n    return pre_train, pre_test\n\n# feature engineering\npre_train, pre_test = preprocess(train, test)\n# reduce memory usage\npre_train = reduce_mem_usage(pre_train, verbose=True)\npre_test = reduce_mem_usage(pre_test, verbose=True)\n# scaling and filling missing values (this is not required for boosting algorithms, nevertheless i wanted to try and check)\npre_train, pre_test = scale_fillna(pre_train, pre_test)\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_lgb(pre_train, pre_test, usefull_features, params):\n    \n    kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n    target = 'open_channels'\n    oof_pred = np.zeros(len(pre_train))\n    y_pred = np.zeros(len(pre_test))\n    feature_importance = pd.DataFrame()\n    \n    # train a baseline model and record the weighted cohen kappa score \n    for fold, (tr_ind, val_ind) in enumerate(kf.split(pre_train, pre_train[target])):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = pre_train[usefull_features].iloc[tr_ind], pre_train[usefull_features].iloc[val_ind]\n        y_train, y_val = pre_train[target][tr_ind], pre_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n        \n        model = lgb.train(params, train_set, num_boost_round = 1000, early_stopping_rounds = 50, \n                         valid_sets = [train_set, val_set], verbose_eval = 100)\n        \n        oof_pred[val_ind] = model.predict(x_val)\n        \n        y_pred += model.predict(pre_test[usefull_features]) / kf.n_splits\n        \n        # get fold importance df\n        fold_importance = pd.DataFrame({'features': usefull_features})\n        fold_importance['fold'] = fold + 1\n        fold_importance['importance'] = model.feature_importance()\n        feature_importance = pd.concat([feature_importance, fold_importance])\n        \n    # round predictions\n    rmse_score = np.sqrt(mean_squared_error(pre_train[target], oof_pred))\n    print('Our oof rmse score is: ', rmse_score)\n    # want to clip and then round predictions\n    oof_pred = np.round(np.clip(oof_pred, 0, 10)).astype(int)\n    y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n    cohen_score = cohen_kappa_score(pre_train[target], oof_pred, weights = 'quadratic')\n    f1 = f1_score(pre_train[target], oof_pred, average = 'macro')\n    print('Our oof cohen kappa score is: ', cohen_score)\n    print('Our oof f1_macro score is: ', f1)\n    \n    # plot feature importance\n    fi_mean = feature_importance.groupby(['features'])['importance'].mean().reset_index()\n    fi_mean.sort_values('importance', ascending = False, inplace = True)\n    plt.figure(figsize = (12, 14))\n    sns.barplot(x = fi_mean['importance'], y = fi_mean['features'])\n    plt.xlabel('Importance', fontsize = 13)\n    plt.ylabel('Feature', fontsize = 13)\n    plt.tick_params(axis = 'x', labelsize = 11)\n    plt.tick_params(axis = 'y', labelsize = 11)\n    plt.title('Light Gradient Boosting Feature Importance (5 KFold)')\n    plt.show()\n    \n    \n    return oof_pred, y_pred, feature_importance\n\n\n# define hyperparammeter (some random hyperparammeters)\nparams = {'learning_rate': 0.1, \n          'feature_fraction': 0.75, \n          'bagging_fraction': 0.75,\n          'bagging_freq': 1,\n          'n_jobs': -1, \n          'seed': 50,\n          'metric': 'rmse'\n        }\n\n\n\n# define the features for training\nfeatures = [col for col in pre_train.columns if col not in ['open_channels', 'set', 'time', 'batch']]\n\noof_pred, y_pred, feature_importance = run_lgb(pre_train, pre_test, features, params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.open_channels = y_pred\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}