{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Liverpool ion switching: Permutation importance for feature selection ~part 1~\n\nFeature engineering and feature selection are the one of the important & difficult task for kaggler to improve model performance.\n\nHere I will introduce **permutation importance and model's feature importance (\"gain\" and \"split\") using LightGBM**. I think these are useful for feature selection.\n\nIf you don't know about permutation importance, please refer Machine Learning Explainability Home Page [Permutation importance](https://www.kaggle.com/dansbecker/permutation-importance).\nIt can be computed **after model is trained** (thus no additional training is necessary for calculating importance), by **randomly permuting one column's feature** for inference to see the effect on prediction value.\n![](https://i.imgur.com/h17tMUU.png)\n*Figures from [Permutation importance](https://www.kaggle.com/dansbecker/permutation-importance)"},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n**1. [Load data](#id1)**<br/>\n**2. [Clean drift](#id2)**<br/>\n**3. [Preprocessing: feature engineering](#id3)**<br/>\n**4. [Util functions for calculating model/permutation importance](#id4)**<br/>\n**5. [Training a model with GroupKFold cross validation](#id5)**<br/>\n**6. [Visualizing feature importance & permutation importance](#id55)**<br/>\n**7. [Prediction on test data](#id6)**<br/>\n**8. [References and further reading](#id7)**<br/>\n\nIf you are only interested in the code to calculate permutation importance, jump to [Util functions for calculating model/permutation importance](#id4).<br/>\nIf you are only interested in the visualization & observation of feature importance, jump to [Visualizing feature importance & permutation importance](#id55).<br/>"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm.notebook import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\n# --- setup ---\npd.set_option('max_columns', 50)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from contextlib import contextmanager\nfrom time import perf_counter\n\n\n@contextmanager\ndef timer(name):\n    t0 = perf_counter()\n    yield\n    t1 = perf_counter()\n    print('[{}] done in {:.3f} s'.format(name, t1-t0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very fast macro f1 score calculation methods.\nRefer [Fast macro f1 computation](https://www.kaggle.com/corochann/fast-macro-f1-computation) for details."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numba as nb\n\n\ndef macro_f1_score(y_true, y_pred, n_labels):\n    total_f1 = 0.\n    for i in range(n_labels):\n        yt = y_true == i\n        yp = y_pred == i\n\n        tp = np.sum(yt & yp)\n\n        tpfp = np.sum(yp)\n        tpfn = np.sum(yt)\n        if tpfp == 0:\n            print('[WARNING] F-score is ill-defined and being set to 0.0 in labels with no predicted samples.')\n            precision = 0.\n        else:\n            precision = tp / tpfp\n        if tpfn == 0:\n            print(f'[ERROR] label not found in y_true...')\n            recall = 0.\n        else:\n            recall = tp / tpfn\n\n        if precision == 0. or recall == 0.:\n            f1 = 0.\n        else:\n            f1 = 2 * precision * recall / (precision + recall)\n        total_f1 += f1\n    return total_f1 / n_labels\n\n\nmacro_f1_score_nb = nb.jit(nb.float64(nb.int32[:], nb.int32[:], nb.int64), nopython=True, nogil=True)(macro_f1_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id1\"></a><br>\n# Load data\n\nI use [Liverpool ion switching feather](https://www.kaggle.com/corochann/liverpool-ion-switching-feather) dataset to load the data much faster. You can also refer the kernel [Convert to feather format for fast data loading](https://www.kaggle.com/corochann/convert-to-feather-format-for-fast-data-loading)."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\ndatadir = Path('/kaggle/input/liverpool-ion-switching-feather')\n\ntrain = pd.read_feather(datadir/'train.feather')\ntest = pd.read_feather(datadir/'test.feather')\nsubmission = pd.read_feather(datadir/'sample_submission.feather')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"signal_array = train['signal'].values\nopen_channels = train['open_channels'].values\n\ntest_signal_array = test['signal'].values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --- configurations ---\n\ndebug = False\narch = 'lgb'\noutdir = Path('.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id2\"></a><br>\n# Clean drift\n\nPlease refer the discussion [What is Drift?](https://www.kaggle.com/c/liverpool-ion-switching/discussion/133874) and the kernel [One Feature Model Scores LB 0.930!](https://www.kaggle.com/cdeotte/one-feature-model-0-930) by @cdeotte for details.\n\nAs you can see in the below figures, signal contains baseline shift and it is called \"drift\". The method for manually removing drift is introduced in the above kernel."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"\"\"\"\nManually clean drift.\nPlot all train/test data.\nRef https://www.kaggle.com/cdeotte/one-feature-model-0-930 by @cdeotte\n\"\"\"\n\n\ndef clean_train(train):\n    def f(x, low, high, mid):\n        return -((-low + high) / 625) * (x - mid) ** 2 + high - low\n\n    # CLEAN TRAIN BATCH 2\n    a = 500000\n    b = 600000\n    train.loc[train.index[a:b], 'signal'] = train.signal[a:b].values - 3 * (train.time.values[a:b] - 50) / 10.\n\n    # CLEAN TRAIN BATCH 7\n    batch = 7\n    a = 500000 * (batch - 1)\n    b = 500000 * batch\n    train.loc[train.index[a:b], 'signal'] = train.signal.values[a:b] - f(train.time[a:b].values, -1.817, 3.186, 325)\n    # CLEAN TRAIN BATCH 8\n    batch = 8\n    a = 500000 * (batch - 1)\n    b = 500000 * batch\n    train.loc[train.index[a:b], 'signal'] = train.signal.values[a:b] - f(train.time[a:b].values, -0.094, 4.936, 375)\n    # CLEAN TRAIN BATCH 9\n    batch = 9\n    a = 500000 * (batch - 1)\n    b = 500000 * batch\n    train.loc[train.index[a:b], 'signal'] = train.signal.values[a:b] - f(train.time[a:b].values, 1.715, 6.689, 425)\n    # CLEAN TRAIN BATCH 10\n    batch = 10\n    a = 500000 * (batch - 1)\n    b = 500000 * batch\n    train.loc[train.index[a:b], 'signal'] = train.signal.values[a:b] - f(train.time[a:b].values, 3.361, 8.45, 475)\n    return train\n\n\ndef clean_test(test):\n    # REMOVE BATCH 1 DRIFT\n    start = 500\n    a = 0\n    b = 100000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - 3 * (test.time.values[a:b] - start) / 10.\n    start = 510\n    a = 100000\n    b = 200000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - 3 * (test.time.values[a:b] - start) / 10.\n    start = 540\n    a = 400000\n    b = 500000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - 3 * (test.time.values[a:b] - start) / 10.\n\n    # REMOVE BATCH 2 DRIFT\n    start = 560\n    a = 600000\n    b = 700000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - 3 * (test.time.values[a:b] - start) / 10.\n    start = 570\n    a = 700000\n    b = 800000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - 3 * (test.time.values[a:b] - start) / 10.\n    start = 580\n    a = 800000\n    b = 900000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - 3 * (test.time.values[a:b] - start) / 10.\n\n    # REMOVE BATCH 3 DRIFT\n    def f(x):\n        return -(0.00788) * (x - 625) ** 2 + 2.345 + 2.58\n\n    a = 1000000\n    b = 1500000\n    test.loc[test.index[a:b], 'signal'] = test.signal.values[a:b] - f(test.time[a:b].values)\n    return test\n\n\ndef plot_all_train_signal(train, title):\n    plt.figure(figsize=(20,5)); res = 1000\n    plt.plot(range(0,train.shape[0],res),train.signal[0::res])\n    for i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\n    for j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\n    plt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \n    plt.title(title,size=20)\n    plt.show()\n\n    \ndef plot_all_test_signal(test, title):\n    let = ['A','B','C','D','E','F','G','H','I','J']\n    plt.figure(figsize=(20,5))\n    res = 1000\n    plt.plot(range(0,test.shape[0],res),test.signal[0::res])\n    for i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\n    for i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\n    for k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\n    for k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\n    plt.title(title,size=20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_all_train_signal(train, title='Training Data Signal BEFORE clean drift - 10 batches')\nplot_all_test_signal(test, title='Test Data Signal BEFORE clean drift - 4 batches')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply it to see the behavior, the drift is removed and each batch mean value is now horizontally straight line."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = clean_train(train)\ntest = clean_test(test)\n\nplot_all_train_signal(train, title='Training Data Signal AFTER clean drift - 10 batches')\nplot_all_test_signal(test, title='Test Data Signal AFTER clean drift - 4 batches')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id3\"></a><br>\n# Preprocessing: feature engineering\n\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if debug:\n    s = 500000*4\n    train = train.iloc[s:s+50000]\n    test = test.iloc[:1000]\n    submission = submission.iloc[:1000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The util method for smoothing to calculate baseline. I wrote a separate kernel [Liverpool ion switching: smoothing visualization](https://www.kaggle.com/corochann/liverpool-ion-switching-smoothing-visualization) to explain how smoothing works."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from scipy import signal\n\n\ndef filter_wave(x, cutoff=(-1, -1), N=4, filtering='lfilter'):\n    \"\"\"Apply low pass/high pass/band pass filter on wave `x`\n\n    Args:\n        x (numpy.ndarray): original wave array.\n        cutoff (tuple): tuple of 2 int.\n            1st element is for lowest frequency to pass. -1 indicates to allow freq=0\n            2nd element is for highest frequency to pass. -1 indicates to allow freq=infty\n        N (int): order of filter\n        filtering (str): filtering method. `lfilter` or `filtfilt` method.\n\n    Returns:\n        filtered_x (numpy.ndarray): same shape with `x`, filter applied.\n    \"\"\"\n    assert x.ndim == 1\n    output = 'sos' if filtering == 'sos' else 'ba'\n    if cutoff[0] <= 0 and cutoff[1] <= 0:\n        # Do not apply filter\n        return x\n    elif cutoff[0] <= 0 and cutoff[1] > 0:\n        # Apply low pass filter\n        output = signal.butter(N, Wn=cutoff[1]/len(x), btype='lowpass', output=output)\n    elif cutoff[0] > 0 and cutoff[1] <= 0:\n        # Apply high pass filter\n        output = signal.butter(N, Wn=cutoff[0]/len(x), btype='highpass', output=output)\n    else:\n        # Apply band pass filter\n        output = signal.butter(N, Wn=(cutoff[0]/len(x), cutoff[1]/len(x)), btype='bandpass', output=output)\n\n    if filtering == 'lfilter':\n        b, a = output\n        return signal.lfilter(b, a, x)\n    elif filtering == 'filtfilt':\n        b, a = output\n        return signal.filtfilt(b, a, x)\n    elif filtering == 'sos':\n        sos = output\n        return signal.sosfilt(sos, x)\n    else:\n        raise ValueError(\"[ERROR] Unexpected value filtering={}\".format(filtering))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features originally inspired from [Ion Switching - 5kfold LGBM & Tracking](https://www.kaggle.com/robikscube/ion-switching-5kfold-lgbm-tracking) by @robikscube.\n\nBut I changed the following:\n\n - Make batch window slice length as variables\n - Take aggregated stats feature by `pd.rolling`, instead of making batch. (It takes time to compute though...)\n - Separate preprocessing window for index with signal jumped place\n - Added \"XXX_mbase\" feature, this is \"signal feature - baseline\" feature where baseline is calculated by smoothing function"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_separate_indices = [\n    0, 500_000, 600_000, 1_000000, 1_500000, 2_000000, 2_500000,\n    3_000000, 3_500000, 4_000000, 4_500000, 5_000000]\ntest_separate_indices = [\n    0, 100_000, 200_000, 300_000, 400_000, 500_000, 600_000, 700_000,\n    800_000, 900_000, 1_000000, 1_500000, 2_000000]\n\n\ndef _create_total_batch_indices(separate_indices):\n    s = separate_indices\n    total_batch = np.zeros((s[-1],), dtype=np.int64)\n    for i in range(len(s) - 1):\n        total_batch[s[i]:s[i+1]] = i\n    return total_batch\n\n\ndef create_total_batch_indices(train: bool = True):\n    separate_indices = train_separate_indices if train else test_separate_indices\n    return _create_total_batch_indices(separate_indices)\n\n\nclass Preprocessor(object):\n    \"\"\"Features ispired from: https://www.kaggle.com/robikscube/ion-switching-5kfold-lgbm-tracking\"\"\"\n\n    def __init__(self, feature_cols=None, debug=False, use_sep=True):\n        self.feature_cols = feature_cols\n        self.debug = debug\n        self.use_sep = use_sep\n        # window_list = [5000, 50000]\n        self.window_list = [5000, 10000, 50000]\n\n    def preprocess(self, tt, train=True):\n        feature_cols = self.feature_cols\n        window_list = self.window_list\n\n        window_str = '-'.join(list(map(str, window_list)))\n        cache_path = f'pp_tt_train{int(train)}_debug{int(self.debug)}_usesep{int(self.use_sep)}_w{window_str}.feather'\n\n        # --- default feature cols ---\n        msignal_cols = []\n        # 5000, 50_000 Batch Features\n        for window in window_list:\n            msignal_cols.extend([\n                f'signal_batch_{window}_min', f'signal_batch_{window}_max',\n                f'signal_batch_{window}_std', f'signal_batch_{window}_mean',\n                f'mean_abs_chg_batch_{window}',\n                f'abs_max_batch_{window}', f'abs_min_batch_{window}',\n                f'range_batch_{window}', f'maxtomin_batch_{window}', f'abs_avg_batch_{window}'\n            ])\n        msignal_cols.extend([\n            'signal_shift+1', 'signal_shift-1',\n            'signal_shift+2', 'signal_shift-2'])\n        default_feature_cols = ['signal', 'baseline']\n        default_feature_cols.extend(msignal_cols)\n        default_feature_cols.extend([f'{c}_msignal' for c in msignal_cols])\n        default_feature_cols.extend([f'{c}_mbase' for c in msignal_cols])\n        # --- default feature cols end ---\n\n        if not os.path.exists(cache_path):\n            tt = tt.sort_values(by=['time']).reset_index(drop=True)\n            tt.index = ((tt.time * 10_000) - 1).values\n            if self.use_sep:\n                separate_indices = train_separate_indices if train else test_separate_indices\n                separate_indices = separate_indices[:len(tt)]\n                tt['total_batch'] = create_total_batch_indices(train=train)[:len(tt)]\n            else:\n                raise NotImplementedError()\n                separate_indices = None\n                tt['total_batch'] = tt.index // 500_000\n            tt['batch'] = tt.index // 50_000\n            tt['batch_index'] = tt.index - (tt.batch * 50_000)\n            tt['batch_slices'] = tt['batch_index'] // 5_000\n            tt['batch_slices2'] = tt.apply(lambda r: '_'.join(\n                [str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n\n            # Calculate baseline\n            cutoff_high = 100\n            signal = tt['signal'].values\n            total_batch = tt['total_batch'].values\n\n            smooth_fn = lambda x: filter_wave(x, cutoff=(0, cutoff_high), filtering='filtfilt')\n            baseline = np.concatenate([smooth_fn(signal[separate_indices[k]:separate_indices[k+1]])\n                                       for k in range(total_batch[-1] + 1)])\n            tt['baseline'] = baseline\n\n            # 5000, 50_000 Batch Features\n            for window in window_list:\n                rolling_df = tt.groupby('total_batch')['signal'].rolling(\n                    window=window, min_periods=1, center=True)\n                tt[f'signal_batch_{window}_min'] = rolling_df.min().values\n                tt[f'signal_batch_{window}_max'] = rolling_df.max().values\n                tt[f'signal_batch_{window}_std'] = rolling_df.std().values\n                tt[f'signal_batch_{window}_mean'] = rolling_df.mean().values\n                tt[f'mean_abs_chg_batch_{window}'] = rolling_df.apply(\n                    lambda x: np.mean(np.abs(np.diff(x))), raw=True).values\n                tt[f'abs_max_batch_{window}'] = rolling_df.apply(\n                    lambda x: np.max(np.abs(x)), raw=True).values\n                tt[f'abs_min_batch_{window}'] = rolling_df.apply(\n                    lambda x: np.min(np.abs(x)), raw=True).values\n\n                tt[f'range_batch_{window}'] = \\\n                    tt[f'signal_batch_{window}_max'] - tt[f'signal_batch_{window}_min']\n                # It makes nan when min=0.\n                tt[f'maxtomin_batch_{window}'] = \\\n                    tt[f'signal_batch_{window}_max'] / tt[f'signal_batch_{window}_min']\n                tt[f'abs_avg_batch_{window}'] = \\\n                    (tt[f'abs_min_batch_{window}'] + tt[f'abs_max_batch_{window}']) / 2\n\n            # add shifts\n            # TODO: nan is not allowed to use permutation importance...\n            tt['signal_shift+1'] = tt.groupby(['batch']).shift(1, fill_value=0.)['signal']\n            tt['signal_shift-1'] = tt.groupby(['batch']).shift(-1, fill_value=0.)['signal']\n            tt['signal_shift+2'] = tt.groupby(['batch']).shift(2, fill_value=0.)['signal']\n            tt['signal_shift-2'] = tt.groupby(['batch']).shift(-2, fill_value=0.)['signal']\n\n            for c in msignal_cols:\n                tt[f'{c}_msignal'] = tt[c] - tt['signal']\n            for c in msignal_cols:\n                tt[f'{c}_mbase'] = tt[c] - tt['baseline']\n\n            tt = tt.replace([np.inf, -np.inf], np.nan)\n            print('Number of nan', tt.isna().sum().sum())\n            tt.fillna(-1, inplace=True)\n            print(f'saving to {cache_path}')\n            #tt.reset_index(drop=True).to_feather(cache_path)\n        else:\n            print(f'loading from {cache_path}')\n            tt = pd.read_feather(cache_path)\n\n        if feature_cols is None:\n            feature_cols = default_feature_cols\n\n        X_train = tt[feature_cols]\n        if train:\n            y_train = tt['open_channels'].values\n        else:\n            y_train = None\n        return X_train, y_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp = Preprocessor(debug=debug)\n\nwith timer('preprocess train'):\n    X_train, y_train = pp.preprocess(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id4\"></a><br>\n# Util functions for calculating model/permutation importance"},{"metadata":{},"cell_type":"markdown","source":"## Model feature importance\n\nLightGBM model has `feature_importance` method which calculates each feature's importance. Here I just made some util methods to summarize it in `DataFrame` format and plot it."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\n\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(8, 12)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Permutation importance\n\n`eli5` library provides `PermutationImportance` class to calculate permutation importance.<br/>\nIt assumes sklearn API, and thus **`fit` and `score` method need to be implemented in the `model` to calc permutation importance**.\n\nHere I made `LGBWrapper` class, which wraps LightGBM model to add the `fit` and `score` methods so that permutation importance can be calculated with LightGBM model.<br/>\n`score` function is implemented to calculate Macro F1 score so that it can calculate the permutation importance for this competition's evaluation metric."},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"from eli5.sklearn import PermutationImportance\n\n\nclass F1MacroScorerWrapper(object):\n    \"\"\"Base class\"\"\"\n    def fit(self, X, y):\n        print('F1MacroScorerWrapper fit called!')\n\n    def predict(self, X):\n        raise NotImplementedError()\n\n    def score(self, X, y):\n        y_pred = self.predict(X)\n        y_pred_valid_int = np.round(np.clip(y_pred, 0, 10))\n        macro_f1 = macro_f1_score_nb(y.astype(np.int32), y_pred_valid_int.astype(np.int32), 10)\n        return macro_f1\n\n    \nclass LGBWrapper(F1MacroScorerWrapper):\n    def __init__(self, lgb_model):\n        self.lgb_model = lgb_model\n\n    def predict(self, X):\n        return self.lgb_model.predict(X, num_iteration=self.lgb_model.best_iteration)\n\n    def feature_importance(self, **kwargs):\n        return self.lgb_model.feature_importance(**kwargs)\n\n\ndef calc_permutation_importance(model, valid_data, feature_names=None):\n    \"\"\"Calculates permutation importance\n\n    Args:\n        model: `score` method should be implemented to calc score used to calc permutation importance.\n            `fit` method should exist, eli5 library assumes sklearn API and\n            `fit` method existence is checked (though the method itself is not used).\n        valid_data: data used to calc perm importance.\n        feature_names: feature names, it is used to construct data frame.\n\n    Returns:\n        perm (PermutationImportance): eli5 object\n        perm_importance_df (pd.DataFrame):\n    \"\"\"\n    perm = PermutationImportance(\n        model, random_state=1, cv='prefit').fit(*valid_data)\n    perm_importance_df = pd.DataFrame({\n        'importance': perm.feature_importances_\n    }, index=feature_names).sort_values('importance', ascending=True)\n    return perm, perm_importance_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id5\"></a><br>\n# Training a model with GroupKFold cross validation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def fit_lgb(train, val, devices=(-1,), seed=None, cat_features=None,\n            num_rounds=1500, lr=0.1):\n    \"\"\"Train Light GBM model\n\n    Ref: https://www.kaggle.com/corochann/ashrae-training-lgbm-by-meter-type\n    \"\"\"\n    X_train, y_train = train\n    X_valid, y_valid = val\n\n    # https://www.kaggle.com/pestipeti/eda-ion-switching\n    metric = 'l1'\n    params = {'num_leaves': 128,\n              'min_data_in_leaf': 64,\n              'objective': 'huber',\n              'max_depth': -1,\n              'learning_rate': lr,  # 0.005,\n              \"boosting\": \"gbdt\",\n              \"bagging_freq\": 5,\n              \"bagging_fraction\": 0.8,\n              \"bagging_seed\": 11,\n              \"metric\": metric,\n              \"verbosity\": -1,\n              'reg_alpha': 0.1,\n              'reg_lambda': 0.3\n              }\n    device = devices[0]\n    if device == -1:\n        # use cpu\n        pass\n    else:\n        # use gpu\n        print(f'using gpu device_id {device}...')\n        params.update({'device': 'gpu', 'gpu_device_id': device})\n\n    params['seed'] = seed\n\n    early_stop = 20\n    verbose_eval = 20\n\n    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_features)\n    watchlist = [d_train, d_valid]\n\n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n\n    # predictions\n    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    print('best_score', model.best_score)\n    log = {'train/mae': model.best_score['training'][metric],\n           'valid/mae': model.best_score['valid_1'][metric]}\n    return model, y_pred_valid, log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is the proper CV in this competition? Since this is time-series data, randomly shuffling & splitting all data may cause overfitting to the training data.\nThe ideal way is to separate by measurement batch, for each 500000 points. However the label `open_channels=10` is contained only limited batch and this separation is too difficult to learn the label with few or no training data.\n\nHere I took the intermediate option. Signal is separated each 100000 steps, to obtain consective field as different batch."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\n\n# --- cross validation ---\nfolds = 5\nseed = 666\ncv_method = 'groupkfold'\nif cv_method == 'groupkfold':\n    group_batch_width = 500000\n    if debug:\n        groups = np.arange(folds).repeat(len(train) // folds)\n    else:\n        groups = np.tile(\n            np.arange(folds).repeat(group_batch_width // folds), 10)\n    kf = GroupKFold(n_splits=folds)\n    cv_iter = kf.split(X_train, y_train, groups=groups)\nelse:\n    raise ValueError(\"[ERROR] Unexpected value cv_method={}\"\n                     .format(cv_method))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# visualize group\nplt.figure(figsize=(20,5))\nres = 1000\n\nx_idx = np.arange(train.shape[0])\n\ncolor_list = ['blue', 'orange', 'green', 'red', 'purple']\ncurrent_idx = 0\none_batch_width = group_batch_width // folds\nwhile current_idx * one_batch_width < len(train):\n    group_idx = groups[current_idx * one_batch_width]\n    col = color_list[group_idx]\n    sl = slice(current_idx * one_batch_width, (current_idx + 1 ) * one_batch_width, res)\n    label = f'group {group_idx}' if current_idx < folds else ''\n    plt.plot(x_idx[sl], train.signal[sl], color=col, label=label)\n    current_idx += 1\n\nfor i in range(11): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor j in range(10): plt.text(j*500000+200000,10,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('GroupKFold separation',size=20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"calc_perm = True  # Note that it takes much long time than training!\n\ny_valid_pred_total = np.zeros(X_train.shape[0])\nmodels = []\nlog_list = []\nmacro_f1_list = []\n\ngain_importance_list = []\nsplit_importance_list = []\nperm_importance_list = []\nfor fold_idx, (train_idx, valid_idx) in enumerate(cv_iter):\n    train_data = X_train.iloc[train_idx, :], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx, :], y_train[valid_idx]\n\n    print('fold_idx', fold_idx, 'train', len(train_idx), 'valid', len(valid_idx))\n    model, y_pred_valid, log = fit_lgb(train_data, valid_data,\n                                       num_rounds=5000, lr=0.3)\n    model = LGBWrapper(model)\n    print('log', log)\n\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models.append(model)\n\n    y_true_valid = valid_data[1]\n    y_pred_valid_int = np.round(np.clip(y_pred_valid, 0, 10))\n    macro_f1 = macro_f1_score(y_true_valid, y_pred_valid_int, 10)\n    print('macro_f1', macro_f1)\n    log['fold_idx'] = fold_idx\n    log['macro_f1'] = macro_f1\n    log_list.append(log)\n    macro_f1_list.append(macro_f1)\n\n    # --- calc model feature importance ---\n    feature_names = train_data[0].columns.tolist()\n    gain_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='gain')\n    gain_importance_list.append(gain_importance_df)\n\n    split_importance_df = calc_model_importance(\n        model, feature_names=feature_names, importance_type='split')\n    split_importance_list.append(split_importance_df)\n\n    # --- calc permutation importance ---\n    if calc_perm:\n        print('calculating permutation importance...')\n        if (debug and fold_idx == 0) or not debug:\n            with timer('calc_permutation_importance'):\n                perm, perm_importance_df = calc_permutation_importance(\n                    model, valid_data, feature_names=feature_names)\n            perm_importance_list.append(perm_importance_df)\n\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check trained performance"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"log_df = pd.DataFrame(log_list)\nlog_df.to_csv(outdir/'log.csv', index=False)\nlogmean_df = log_df.mean(axis=0)\nlogmean_df.to_csv(outdir/'log_mean.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmean_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id55\"></a><br>\n# Visualizing feature importance & permutation importance"},{"metadata":{},"cell_type":"markdown","source":"Now we got 3 types of feature importance, for each fold in CV.\n\n - LightGBM Model feature importance by \"gain\"\n - LightGBM Model feature importance by \"split\"\n - LightGBM Model's permutation importance\n\nI will calulate mean of feature importance to get final result."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    return mean_df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_gain_df = calc_mean_importance(gain_importance_list)\nplot_importance(mean_gain_df, title='Model feature importance by gain')\nmean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_gain_df.to_csv(outdir / 'gain_importance_mean.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"mean_split_df = calc_mean_importance(split_importance_list)\nplot_importance(mean_split_df, title='Model feature importance by split')\nmean_split_df = mean_split_df.reset_index().rename(columns={'index': 'feature_names'})\nmean_split_df.to_csv(outdir / 'split_importance_mean.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if calc_perm:\n    mean_perm_df = calc_mean_importance(perm_importance_list)\n    plot_importance(mean_perm_df, title='Model permutation importance')\n    mean_perm_df = mean_perm_df.reset_index().rename(columns={'index': 'feature_names'})\n    mean_perm_df.to_csv(outdir / 'perm_importance_mean.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we compare these 3 figures, we can see following:\n\n - \"split\" importance ranges to almost all features, which means LightGBM model uses all features in their tree.\n - However, \"gain\" and \"permutation\" importance ranges to only aroud 1/4 of features. It means that even though all features are used in tree construction, many features do not contribute to improve the accuracy so much.\n   - These features may be removed without sacrifycing accuracy, or even it contributes to reduce overfitting.\n - When comparing \"gain\" importance and \"permutation\" importance, its ranking is actually different. \"gain\" importance is high with maxtomin features, while \"permutation\" is high with signal_max, abs_avg, baseline, maxtomin features..."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id6\"></a><br>\n# Prediction on test data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_signal_and_pred(signal_array, pred, gt=None, save_filepath=None,\n                         smooth_fn=None, label='smooth signal',\n                         target_indices=None, interval=50, debug=False):\n    if debug:\n        width = len(signal_array)\n    else:\n        width = 500000\n    if target_indices is None:\n        n_batch = len(signal_array) // width\n        target_indices = np.arange(n_batch)\n\n    n_figs = len(target_indices)\n    fig, axes = plt.subplots(n_figs, 1, figsize=(18, 5 * n_figs))\n    for i in target_indices:\n        ax = axes if n_figs == 1 else axes[i]\n        s = 500000 * i\n        y = signal_array[s:s + width][::interval]\n        t = pred[s:s + width][::interval]\n        ax.plot(y, label='signal', zorder=1)\n        if smooth_fn is not None:\n            y2 = smooth_fn(signal_array[s:s + width])[::interval]\n            ax.plot(y2, label=label, zorder=1)\n        if gt is not None:\n            ax.scatter(x=np.arange(t.shape[0]), y=gt[s:s + width][::interval],\n                       color='orange', label='ground truth', zorder=2)\n        ax.scatter(x=np.arange(t.shape[0]), y=t, color='green', label='pred', zorder=2)\n        ax.legend()\n        ax.set_title(f'prediction of batch={target_indices[i]}')\n    plt.tight_layout()\n    if save_filepath:\n        plt.savefig(save_filepath)\n    else:\n        plt.show()\n    plt.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid_pred_total_int = np.round(np.clip(y_valid_pred_total, 0, 10))\nmacro_f1 = macro_f1_score(y_train, y_valid_pred_total_int, 10)\nprint('Total CV: macro_f1', macro_f1, 'macro_f1_list', macro_f1_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- model prediction for test data ---\nprint('start model prediction...')\nwith timer('preprocess test'):\n    X_test, _ = pp.preprocess(test, train=False)\n\ny_test_pred_total = np.zeros(X_test.shape[0])\nfor i, model in enumerate(models):\n    print(f'predicting {i}-th model')\n    y_pred_test = model.predict(X_test)\n    y_test_pred_total += y_pred_test\ny_test_pred_total /= len(models)\ny_test_pred_total_int = np.round(np.clip(y_test_pred_total, 0, 10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- Save submission ---\nsubmission['open_channels'] = y_test_pred_total_int.astype(np.int64)\nsubmission.to_csv(outdir / 'submission.csv', index=False, float_format='%.4f')\n\n# --- Save prediction for future ensemble, optimize threshold etc ---\nnp.savez_compressed(\n    outdir / 'pred.npz',\n    valid=y_valid_pred_total, test=y_test_pred_total)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- plot dist ---\nax = sns.distplot(y_valid_pred_total, label='valid_pred')\nsns.distplot(y_train, ax=ax, label='train')\nsns.distplot(y_test_pred_total, ax=ax, label='test pred')\nplt.legend()\nplt.show()\n#plt.savefig(outdir / f'dist_total_test.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --- plot prediction ---\nplot_signal_and_pred(test['signal'].values, y_test_pred_total_int,\n                     debug=debug)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_signal_and_pred(train['signal'].values, y_valid_pred_total_int,\n                     gt=train['open_channels'].values,\n                     debug=debug)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"id7\"></a><br>\n# References and further reading\n\nThat's all! You can go these references for further reading.\n\n\nDiscussion:\n - [What is Drift?](https://www.kaggle.com/c/liverpool-ion-switching/discussion/133874) \n \nKernel:\n - Machine Learning Explainability Home Page [Permutation importance](https://www.kaggle.com/dansbecker/permutation-importance)\n - [One Feature Model Scores LB 0.930!](https://www.kaggle.com/cdeotte/one-feature-model-0-930)\n - [Ion Switching - 5kfold LGBM & Tracking](https://www.kaggle.com/robikscube/ion-switching-5kfold-lgbm-tracking)\n - [Convert to feather format for fast data loading](https://www.kaggle.com/corochann/convert-to-feather-format-for-fast-data-loading).\n - [Fast macro f1 computation](https://www.kaggle.com/corochann/fast-macro-f1-computation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}