{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Necessary imports"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the data and making bins"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"test_clean = pd.read_csv(\"../input/data-without-drift/test_clean.csv\")\ntest_clean['group'] = -1\nx = [[(0,100000),(300000,400000),(800000,900000),(1000000,2000000)],[(400000,500000)], \n     [(100000,200000),(900000,1000000)],[(200000,300000),(600000,700000)],[(500000,600000),(700000,800000)]]\nfor k in range(5):\n    for j in range(len(x[k])): test_clean.iloc[x[k][j][0]:x[k][j][1],2] = k\n\ntrain_clean = pd.read_csv(\"../input/data-without-drift/train_clean.csv\")\ntrain_clean['group'] = -1\nx = [(0,500000),(1000000,1500000),(1500000,2000000),(2500000,3000000),(2000000,2500000)]\nfor k in range(5): train_clean.iloc[x[k][0]:x[k][1],3] = k","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Memory Reduction\nElse the notebook will crash due to overhead of memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"window_sizes = [10, 25, 50, 100, 500, 1000, 5000, 10000, 25000]\n\nfor window in window_sizes:\n    train_clean[\"rolling_mean_\" + str(window)] = train_clean['signal'].rolling(window=window).mean()\n    #train_clean[\"rolling_std_\" + str(window)]  = train_clean['signal'].rolling(window=window).std()\n    train_clean[\"rolling_var_\" + str(window)]  = train_clean['signal'].rolling(window=window).var()\n    train_clean[\"rolling_min_\" + str(window)]  = train_clean['signal'].rolling(window=window).min()\n    train_clean[\"rolling_max_\" + str(window)]  = train_clean['signal'].rolling(window=window).max()\n    \n    train_clean[\"rolling_min_max_ratio_\" + str(window)] = train_clean[\"rolling_min_\" + str(window)] / train_clean[\"rolling_max_\" + str(window)]\n    train_clean[\"rolling_min_max_diff_\" + str(window)]  = train_clean[\"rolling_max_\"  + str(window)] - train_clean[\"rolling_min_\" + str(window)]\n    \n    a = (train_clean['signal'] - train_clean['rolling_min_' + str(window)]) / (train_clean['rolling_max_' + str(window)] - train_clean['rolling_min_' + str(window)])\n    train_clean[\"norm_\" + str(window)] = a * (np.floor(train_clean['rolling_max_' + str(window)]) - np.ceil(train_clean['rolling_min_' + str(window)]))\n    \ntrain_clean = train_clean.replace([np.inf, -np.inf], np.nan)\ntrain_clean.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for window in window_sizes:\n    \n    test_clean[\"rolling_mean_\" + str(window)] = test_clean['signal'].rolling(window=window).mean()\n    #test_clean[\"rolling_std_\" + str(window)]  = test_clean['signal'].rolling(window=window).std()\n    test_clean[\"rolling_var_\" + str(window)]  = test_clean['signal'].rolling(window=window).var()\n    test_clean[\"rolling_min_\" + str(window)]  = test_clean['signal'].rolling(window=window).min()\n    test_clean[\"rolling_max_\" + str(window)]  = test_clean['signal'].rolling(window=window).max()\n    \n    test_clean[\"rolling_min_max_ratio_\" + str(window)]  = test_clean[\"rolling_min_\" + str(window)] /  test_clean[\"rolling_max_\" + str(window)]\n    test_clean[\"rolling_min_max_diff_\"  + str(window)]  = test_clean[\"rolling_max_\"  + str(window)] - test_clean[\"rolling_min_\" + str(window)]\n    \n    a = (test_clean['signal'] - test_clean['rolling_min_' + str(window)]) / (test_clean['rolling_max_' + str(window)] - test_clean['rolling_min_' + str(window)])\n    test_clean[\"norm_\" + str(window)] = a * (np.floor(test_clean['rolling_max_' + str(window)]) - np.ceil(test_clean['rolling_min_' + str(window)]))\n    \ntest_clean = test_clean.replace([np.inf, -np.inf], np.nan)\ntest_clean.fillna(0, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_clean['signal_median'] = train_clean.groupby('group')['signal'].median()\n#train_clean['signal_mean']   = train_clean.groupby('group')['signal'].mean()\n#train_clean['signal_min']    = train_clean.groupby('group')['signal'].min()\n#train_clean['signal_max']    = train_clean.groupby('group')['signal'].max()\n\ntrain_clean['cum_sum_signal'] = train_clean['signal'].cumsum()\ntrain_clean['cum_perc_signal']= 100*train_clean['cum_sum_signal']/train_clean['signal'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_clean['signal_median'] = test_clean.groupby('group')['signal'].median()\n#test_clean['signal_mean']   = test_clean.groupby('group')['signal'].mean()\n#test_clean['signal_min']    = test_clean.groupby('group')['signal'].min()\n#test_clean['signal_max']    = test_clean.groupby('group')['signal'].max()\n\ntest_clean['cum_sum_signal'] = test_clean['signal'].cumsum()\ntest_clean['cum_perc_signal']= 100*test_clean['cum_sum_signal']/test_clean['signal'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean = reduce_mem_usage(train_clean)\ntest_clean  = reduce_mem_usage(test_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y     = train_clean['open_channels']\ntrain = train_clean.drop(['open_channels'],axis=1)\ntest  = test_clean\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = np.unique(y)\nprint(len(classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_clean   # Delete the copy of train data.\ndel test_clean    # Delete the copy of test data.\ngc.collect()      # Collect the garbage.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Group KFold Technique as a CV strategy."},{"metadata":{"trusted":true},"cell_type":"code","source":"id_train = train['time']\nid_test  = test['time']\n\ntrain = train.drop('time', axis = 1)\ntest  = test.drop( 'time', axis = 1)\n\nnfolds = 5\ngroups = np.array(train.signal.values)\nfolds = GroupKFold(n_splits = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'num_leaves': 129,\n         'min_data_in_leaf': 148, \n         'objective':'multiclass',\n         'max_depth': 7,\n         'learning_rate': 0.00987173774816051,\n         \"min_child_samples\": 24,\n         \"feature_fraction\": 0.7202,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8125 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'multi_logloss',\n         \"lambda_l1\": 0.3468,\n         \"verbosity\": -1, \n         'num_class': 11}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_class_weight(classes, exp=1):\n    '''\n    Weight of the class is inversely proportional to the population of the class.\n    There is an exponent for adding more weight.\n    '''\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()/np.power(hist, exp)\n    \n    return class_weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfeature_importance_df = np.zeros((train.shape[1], nfolds))\npreds = np.zeros(2000000*11).reshape((2000000, 11))\n\nf1s = []\naccuracies = []\nprecisions = []\nrecalls = []\n\n#mvalid = np.zeros([len(train), len(classes)])\n#mfull  = np.zeros([len(test), len(classes)])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, train.values, groups)):\n    print('----')\n    print(\"fold nÂ°{}\".format(fold_))\n    \n    x0,y0 = train.iloc[trn_idx], y[trn_idx]\n    x1,y1 = train.iloc[val_idx], y[val_idx]\n    \n    class_weight = get_class_weight(y0)\n    \n    trn_data = lgb.Dataset(x0, label=y0, weight=class_weight[y0])\n    val_data = lgb.Dataset(x1, label=y1, weight=class_weight[y1])\n    \n    num_round = 500\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \n                    verbose_eval=100, early_stopping_rounds = 100)\n    gbc_pred = clf.predict(x1, num_iteration=clf.best_iteration)\n    \n    f1 = f1_score(y1, np.argmax(gbc_pred, axis=1), average='macro')\n    precision = precision_score(y1, np.argmax(gbc_pred, axis=1), average='macro')\n    recall = recall_score(y1, np.argmax(gbc_pred, axis=1), average='macro')\n    accuracy = accuracy_score(y1, np.argmax(gbc_pred, axis=1))\n    \n    print(\"F1 Score for LGBM: \", str(f1))\n    print(\"Precision Score for LGBM: \", str(precision))\n    print(\"Recall Score for LGBM: \", str(recall))\n    print(\"Accuracy Score for LGBM: \", str(accuracy))\n    \n    f1s.append(f1)\n    accuracies.append(accuracy)\n    precisions.append(precision)\n    recalls.append(recall)\n    \n    preds += clf.predict(test, num_iteration=clf.best_iteration) / folds.n_splits\n    preds_rounded = np.asarray([np.argmax(line) for line in preds])\n    \nprint(\"F1 Scores over {} folds: {}\".format(fold_, f1s))\nprint(\"Accuracy Scores over {} folds: {}\".format(fold_, accuracies))\nprint(\"Precision Scores over {} folds: {}\".format(fold_, precisions))\nprint(\"Recall Scores over {} folds: {}\".format(fold_, recalls))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\")\n#sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare file for the submission\nWe have to conver the 'open_channels' column to make it look like a classifier problem, we used a regressor to predict the values. For this I did:\n1. The rounding of the values predicted.\n2. Converting the datatype of 'open_channels' from float to int."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['time']  = sub['time'] #id_test\nsubmission['open_channels'] = preds_rounded\nsubmission['open_channels'] = submission['open_channels'].round(decimals=0)   # Round the 'open_channels' values to the nearest decimal as we implemented a regressor.\nsubmission['open_channels'] = submission['open_channels'].astype(int)         # Convert the datatype of 'open_channels' from float to integer to match the requirements of submission.\nsubmission.to_csv('submission.csv', index = False,float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.tail()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}