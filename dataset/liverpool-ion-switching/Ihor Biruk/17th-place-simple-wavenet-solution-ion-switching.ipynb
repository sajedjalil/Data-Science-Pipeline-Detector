{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Here is our 17th place simple Wavenet solution (Private LB: 0.94494, Public LB: 0.94600). You can find a short description in [this topic](https://www.kaggle.com/c/liverpool-ion-switching/discussion/153829).\n\n### Acknowledgements:\n- Our solution is based on [WaveNet-Keras](https://www.kaggle.com/siavrez/wavenet-keras) and [Wavenet with SHIFTED-RFC Proba and CBR](https://www.kaggle.com/nxrprime/wavenet-with-shifted-rfc-proba-and-cbr) kernels.\n- We were using a dataset with [removed drift and Kalman filtering](https://www.kaggle.com/michaln/data-without-drift-with-kalman-filter) (which is based on [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and [kalman-filtering](https://www.kaggle.com/teejmahal20/a-signal-processing-approach-kalman-filtering)).\n- Also we used [ION-SHIFTED-RFC-PROBA](https://www.kaggle.com/sggpls/ion-shifted-rfc-proba) dataset (based on [SHIFTED-RFC Pipeline](https://www.kaggle.com/sggpls/shifted-rfc-pipeline) kernel, see [this discussion](https://www.kaggle.com/c/liverpool-ion-switching/discussion/144645)) as additional features.\n\nFor simplicity, in this kernel we use already created data from public datasets and not create it from scratch as it takes a while (all scripts can be found in the links above).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Code:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install tensorflow_addons==0.9.1\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nimport pandas as pd\nimport numpy as np\nimport random\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\nimport gc\nimport os\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"EPOCHS = 200\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nLR = 0.0015\nSPLITS = 5\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data():\n    train = pd.read_csv('/kaggle/input/data-without-drift-with-kalman-filter/train.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test = pd.read_csv('/kaggle/input/data-without-drift-with-kalman-filter/test.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n    \n    Y_train_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_test_proba.npy\")\n    \n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n\n    return train, test, sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batching(df, batch_size):\n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_signal_shift(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_feat_engineering(df, batch_size):\n    df = batching(df, batch_size = batch_size)\n    df = generate_signal_shift(df, [1, 2])\n    df['signal_2'] = df['signal'] ** 2\n    df['signal_3'] = df['signal'] ** 3\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Classifier(shape_):\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    x = BatchNormalization()(inp)\n    x = wave_block(x, 16, 3, 12)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 32, 3, 8)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 64, 3, 4)\n    x = BatchNormalization()(x)\n    x = wave_block(x, 128, 3, 1)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['categorical_accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_schedule(epoch):\n    if epoch < 30:\n        lr = LR\n    elif epoch < 40:\n        lr = LR / 3\n    elif epoch < 50:\n        lr = LR / 5\n    elif epoch < 60:\n        lr = LR / 7\n    elif epoch < 70:\n        lr = LR / 9\n    elif epoch < 80:\n        lr = LR / 11\n    elif epoch < 90:\n        lr = LR / 13\n    else:\n        lr = LR / 100\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class EarlyStoppingAtMaxMacroF1(Callback):\n    \"\"\"Stop training when the MacroF1 is at its max.\n    \"\"\"\n\n    def __init__(self, model, inputs, targets, epochs, patience=0):\n        super(EarlyStoppingAtMaxMacroF1, self).__init__()\n\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis=2).reshape(-1)\n        self.patience = patience\n\n        # best_weights to store the weights at which the minimum loss occurs.\n        self.best_weights = None\n        self.last_epoch = epochs - 1\n\n    def on_train_begin(self, logs=None):\n        # The number of epoch it has waited when loss is no longer minimum.\n        self.wait = 0\n        # The epoch the training stops at.\n        self.stopped_epoch = 0\n        # Initialize the best as negative infinity.\n        self.best = np.NINF\n\n    def on_epoch_end(self, epoch, logs=None):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        current_score = f1_score(self.targets, pred, average='macro')\n        print(f'F1 Macro Score: {current_score:.6f}')\n\n        if np.greater(current_score, self.best):\n            self.best = current_score\n            self.wait = 0\n            # Record the best weights if current results is better (less).\n            self.best_weights = self.model.get_weights()\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stopped_epoch = epoch\n                self.model.stop_training = True\n                print('Restoring model weights from the end of the best epoch.')\n                self.model.set_weights(self.best_weights)\n        if epoch == self.last_epoch:\n            self.model.set_weights(self.best_weights)\n\n    def on_train_end(self, logs=None):\n        if self.stopped_epoch > 0:\n            print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_cv_model_by_batch(train, test, n_splits, batch_col, feats, sample_submission, nn_epochs, nn_batch_size, seed):\n    \n    seed_everything(seed)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11)) # build out of folds matrix with 11 columns, they represent our target variables classes (from 0 to 10)\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=n_splits)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n    # pivot target columns to transform the net to a multiclass classification structure\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n\n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        print(f'Our training dataset shape is {train_x.shape}')\n        print(f'Our validation dataset shape is {valid_x.shape}')\n\n        gc.collect()\n        shape_ = (None, train_x.shape[2])\n        model = Classifier(shape_)\n        \n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        model.fit(train_x,train_y,\n                  epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, EarlyStoppingAtMaxMacroF1(model, valid_x, valid_y, nn_epochs, patience=40)],\n                  batch_size = nn_batch_size,verbose = 2,\n                  validation_data = (valid_x,valid_y))\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n        print(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :.6f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds / n_splits\n        model.save(f\"model_seed_{seed}_fold_{n_fold}.mdl\")\n    # calculate the oof macro f1_score\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro')\n    print(f'Training completed. oof macro f1 score : {f1_score_:.6f}')\n\n    # save predictions\n    np.save(f\"preds_{seed}.npf\", preds_)\n    sample_submission['open_channels'] = np.argmax(preds_, axis = 1).astype(int)\n    sample_submission.to_csv(f'submission_wavenet_{seed}.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_everything(seed):\n    \n    print('Reading Data Started.')\n    train, test, sample_submission = read_data()\n    print('Reading Data Completed.')\n    \n    print('Normalizing Data Started.')\n    train, test = normalize(train, test)\n    print('Normalizing Data Completed.')\n        \n    print('Feature Engineering Started.')\n    train = train[:3640000].append(train[3840000:], ignore_index=True) # removed noise from train data\n    print(f'Train shape: {train.shape}')\n    \n    train = run_feat_engineering(train, batch_size = GROUP_BATCH_SIZE)\n    test = run_feat_engineering(test, batch_size = GROUP_BATCH_SIZE)\n    train, test, features = feature_selection(train, test)\n    print('Feature Engineering Completed.')\n\n    print(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started.')\n    run_cv_model_by_batch(train, test, SPLITS, 'group', features, sample_submission, EPOCHS, NNBATCHSIZE, seed)\n    print('Training completed.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generating predictions:\nUnder the following section we generate seeds for our ensemble technique inspired by [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method).\n\n**Note**: for simplicity, here is an example which uses only 3 predictions, in our final submissions we used more seeds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"seeds = [1, 2, 3] # we used more seeds\nfor seed in seeds:\n    run_everything(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ensembling:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')\n\nfile_name = 'submission_wavenet_{}.csv'\nsubmissions = [pd.read_csv(file_name.format(i)) for i in seeds]\n\nprint(sample_sub.shape)\nfor s in submissions:\n    print(s.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [sub['open_channels'] for sub in submissions]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_round_median = np.round(np.median(predictions, axis=0)).astype(int)\nres_max = np.max(predictions, axis=0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the difference\ndiff = (res_max - res_round_median)\nunique, counts = np.unique(diff, return_counts=True)\ndict(zip(unique, counts))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create final submission:\nThe final estimator looks like `max(predictions)` for the group with the high signal average number (we denote this group as `D`) and `round(median(predictions))` for everything else.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df, sample_submission = read_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how we can split train data by groups\nplt.figure(figsize=(15, 8))\nplt.plot(train_df[\"time\"], train_df[\"signal\"], color=\"grey\")\nplt.title(\"Signals (Clean train data)\", fontsize=20)\nplt.xlabel(\"Time\", fontsize=18)\nplt.ylabel(\"Signal\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_groups_data(train_df):\n    A = train_df[:1000000]\n    B = train_df[1000000:1500000].append(train_df[3000000:3500000], ignore_index=True)\n    C = train_df[1500000:2000000].append(train_df[3500000:3640000], ignore_index=True)\\\n                                 .append(train_df[3840000:4000000], ignore_index=True) # removed noise for group C\n    D = train_df[2000000:2500000].append(train_df[4500000:5000000], ignore_index=True)\n    E = train_df[2500000:3000000].append(train_df[4000000:4500000], ignore_index=True)\n    return A, B, C, D, E","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A, B, C, D, E = get_groups_data(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nplt.plot(A.time, A.signal)\nplt.plot(B.time, B.signal)\nplt.plot(C.time, C.signal)\nplt.plot(D.time, D.signal)\nplt.plot(E.time, E.signal)\nplt.title(\"Signals (Clean train data without noise, colored by groups)\", fontsize=20)\nplt.xlabel(\"Time\", fontsize=18)\nplt.ylabel(\"Signal\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasets = {\n    'A': A,\n    'B': B,\n    'C': C,\n    'D': D,\n    'E': E,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_predictions(df, batch_length, datasets):\n    sub_final = pd.DataFrame()\n    sub_final['time'] = sample_sub['time']\n    sub_final['open_channels'] = sample_sub['open_channels']\n\n    batch_len = batch_length\n    for i in range(int(len(df) / batch_len)):\n        df_batch = df[i * batch_len:i * batch_len + batch_len]\n\n        batch_mean = df_batch['signal'].mean()\n        default_group_name, default_group = next(iter(datasets.items()))\n        min_mean_diff = np.abs(batch_mean - default_group['signal'].mean())\n        clf_name = default_group_name\n\n        for name, data in datasets.items():\n            dataset_mean = data['signal'].mean()\n            mean_diff = np.abs(batch_mean - dataset_mean)\n            if mean_diff < min_mean_diff:\n                min_mean_diff = mean_diff\n                clf_name = name\n\n        # use max(predictions) for the group with the high signal average number (D) and round(median(predictions)) for everything else (A, B, C, E).\n        if (clf_name=='A'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='B'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='C'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='D'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_max[i * batch_len:i * batch_len + batch_len]\n        elif (clf_name=='E'):\n            sub_final.loc[i * batch_len:i * batch_len + batch_len - 1, 'open_channels'] = res_round_median[i * batch_len:i * batch_len + batch_len]\n            \n        print(f\"group_{clf_name} prediction: {i} batch data\")\n    return sub_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_final = combine_predictions(test_df, 100000, datasets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if combine_predictions correctly defines each group\nplt.figure(figsize=(15, 8))\nplt.plot(test_df[\"time\"], test_df[\"signal\"], color=\"grey\")\nplt.title(\"Signals (Clean test data)\", fontsize=20)\nplt.xlabel(\"Time\", fontsize=18)\nplt.ylabel(\"Signal\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can also combine our predictions by simple hard-coded solution, so let's check if there is no difference:\nres_combined = np.concatenate([res_round_median[:500000], res_max[500000:600000], res_round_median[600000:700000], res_max[700000:800000], res_round_median[800000:]])\n\ndiff = (sub_final['open_channels'] - res_combined)\nunique, counts = np.unique(diff, return_counts=True)\ndict(zip(unique, counts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_final.to_csv('wavenet_monte_carlo.csv', index=False, float_format='%.4f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_final","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}