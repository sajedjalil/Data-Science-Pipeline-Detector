{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ChangeLog\n\nthe goal is to improve the models performance,so in \n\n* version 1 : i will try adding Stochastic weight averaging(swa) and adamW (lb 0.939)\n* version 2 : Adam with swa_lr=0.002 (lb 0.94)\n* version 3 : Adding LSTM layer before conv2\n* version 4 : our model was never using LSTM in version 3,i am trying to add LSTM again after wave_block4 (if i am making mistakes again,please help me in the comment box) [failed : waited more than 8 hours]\n\n* version 5 : 1 epoch for 5 fold takes  4min 23s so i will try 80 epochs instead of 150 (got lb 0.942) \n* version 6 : trying [Wavenet with SHIFTED-RFC Proba](https://www.kaggle.com/c/liverpool-ion-switching/discussion/144645) as [this kernel ](https://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba) for 90 epochs and batch size = 32\n* version 7 : solving SWA issue,trying cyclicLR and  solving model bug\n* version 8 : doing res = torch.add(res, x) instead of res+x and switching back to reducelronplateau scheduler and epoch = 150,swa_lr = 0.0011, added 1 more lstm before first wave block"},{"metadata":{},"cell_type":"markdown","source":"[ STOCHASTIC WEIGHT AVERAGING](https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/)\n\nStochastic Weight Averaging (SWA) is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorc\n\n![](https://scontent.fdac6-1.fna.fbcdn.net/v/t1.0-9/59705847_2248977985403173_8149245770332110848_o.png?_nc_cat=107&_nc_sid=8024bb&_nc_ohc=PMOb2aDgLVsAX9dRes8&_nc_ht=scontent.fdac6-1.fna&oh=13176a691e130400ac1229830ffc27cc&oe=5EBDC505)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nimport random\n#from tqdm import tqdm\nfrom tqdm.notebook import tqdm\nimport gc\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# configurations and main hyperparammeters\nEPOCHS = 150\nNNBATCHSIZE = 32\nGROUP_BATCH_SIZE = 4000\nSEED = 123\nLR = 0.001\nSPLITS = 5\n\noutdir = 'wavenet_models'\nflip = False\nnoise = False\n\n\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data\ndef read_data():\n    train = pd.read_csv('../input/data-without-drift/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('/kaggle/input/data-without-drift/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\n    #from https://www.kaggle.com/sggpls/wavenet-with-shifted-rfc-proba and\n    # https://www.kaggle.com/c/liverpool-ion-switching/discussion/144645\n    Y_train_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_train_proba.npy\")\n    Y_test_proba = np.load(\"/kaggle/input/ion-shifted-rfc-proba/Y_test_proba.npy\")\n    print(train.shape,Y_train_proba.shape)\n    for i in range(11):\n        train[f\"proba_{i}\"] = Y_train_proba[:, i]\n        test[f\"proba_{i}\"] = Y_test_proba[:, i]\n        \n    sub  = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n    return train, test, sub\n\n# create batches of 4000 observations\ndef batching(df, batch_size):\n    #print(df)\n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    return df\n\n# normalize the data (standard scaler). We can also try other scalers for a better score!\ndef normalize(train, test):\n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n    return train, test\n\n# get lead and lags features\ndef lag_with_pct_change(df, windows):\n    for window in windows:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n    return df\n\n# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\ndef run_feat_engineering(df, batch_size):\n    # create batches\n    df = batching(df, batch_size = batch_size)\n    # create leads and lags (1, 2, 3 making them 6 features)\n    df = lag_with_pct_change(df, [1, 2, 3])\n    # create signal ** 2 (this is the new feature)\n    df['signal_2'] = df['signal'] ** 2\n    return df\n\n# fillna with the mean and select features for training\ndef feature_selection(train, test):\n    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n    train = train.replace([np.inf, -np.inf], np.nan)\n    test = test.replace([np.inf, -np.inf], np.nan)\n    for feature in features:\n        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n        train[feature] = train[feature].fillna(feature_mean)\n        test[feature] = test[feature].fillna(feature_mean)\n    return train, test, features\n\n\ndef split(GROUP_BATCH_SIZE=4000, SPLITS=5):\n    print('Reading Data Started...')\n    train, test, sample_submission = read_data()\n    train, test = normalize(train, test)\n    print('Reading and Normalizing Data Completed')\n    print('Creating Features')\n    print('Feature Engineering Started...')\n    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n    train, test, features = feature_selection(train, test)\n    print(train.head())\n    print('Feature Engineering Completed...')\n\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=SPLITS)\n    splits = [x for x in kf.split(train, train[target], group)]\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])\n        new_splits.append(new_split)\n    target_cols = ['open_channels']\n    print(train.head(), train.shape)\n    train_tr = np.array(list(train.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[features].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[features].values)))\n    print(train.shape, test.shape, train_tr.shape)\n    return train, test, train_tr, new_splits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# wavenet "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import torch.nn.functional as F\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n        return x *( torch.tanh(F.softplus(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mish = Mish()\nmish","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/hanjoonchoe/wavenet-lstm-pytorch-ignite-ver        \nclass Wave_Block(nn.Module):\n    \n    def __init__(self,in_channels,out_channels,dilation_rates):\n        super(Wave_Block,self).__init__()\n        self.num_rates = dilation_rates\n        self.convs = nn.ModuleList()\n        self.filter_convs = nn.ModuleList()\n        self.gate_convs = nn.ModuleList()\n        self.mish = Mish()\n        self.convs.append(nn.Conv1d(in_channels,out_channels,kernel_size=1))\n        dilation_rates = [2**i for i in range(dilation_rates)]\n        for dilation_rate in dilation_rates:\n            self.filter_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.gate_convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=3,padding=dilation_rate,dilation=dilation_rate))\n            self.convs.append(nn.Conv1d(out_channels,out_channels,kernel_size=1))\n            \n    def forward(self,x):\n        x = self.convs[0](x)\n        res = x\n        for i in range(self.num_rates):\n            x = self.mish(self.filter_convs[i](x))*F.sigmoid(self.gate_convs[i](x))\n            x = self.convs[i+1](x)\n            #x += res\n            res = torch.add(res, x)\n        return res\n    \n    \n\n    \nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        input_size = 128\n        self.LSTM1 = nn.GRU(input_size=19,hidden_size=64,num_layers=2,batch_first=True,bidirectional=True)\n\n        self.LSTM = nn.GRU(input_size=input_size,hidden_size=64,num_layers=2,batch_first=True,bidirectional=True)\n        #self.attention = Attention(input_size,4000)\n        #self.rnn = nn.RNN(input_size, 64, 2, batch_first=True, nonlinearity='relu')\n        self.trfrmr = nn.TransformerEncoderLayer(128, 4, dim_feedforward=512, dropout=0.1, activation='gelu')\n        \n        self.wave_block1 = Wave_Block(128,16,12)\n        self.wave_block2 = Wave_Block(16,32,8)\n        self.wave_block3 = Wave_Block(32,64,4)\n        self.wave_block4 = Wave_Block(64, 128, 1)\n        self.fc = nn.Linear(128, 11)\n            \n    def forward(self,x):\n        x,_ = self.LSTM1(x)\n#         x = self.trfrmr(x)\n        x = x.permute(0, 2, 1)\n      \n        x = self.wave_block1(x)\n        x = self.wave_block2(x)\n        x = self.wave_block3(x)\n        \n        #x,_ = self.LSTM(x)\n        x = self.wave_block4(x)\n        x = x.permute(0, 2, 1)\n        x,_ = self.LSTM(x)\n        x = self.trfrmr(x)\n        #x = self.conv1(x)\n        #print(x.shape)\n        #x = self.rnn(x)\n        #x = self.attention(x)\n        x = self.fc(x)\n        return x\n\n   \n    \nclass EarlyStopping:\n    def __init__(self, patience=7, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n        self.counter, self.best_score = 0, None\n        self.is_maximize = is_maximize\n\n\n    def load_best_weights(self, model):\n        model.load_state_dict(torch.load(self.checkpoint_path))\n\n    def __call__(self, score, model):\n        if self.best_score is None or \\\n                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n            torch.save(model.state_dict(), self.checkpoint_path)\n            self.best_score, self.counter = score, 0\n            return 1\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return 2\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Classifier()\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nclass IronDataset(Dataset):\n    def __init__(self, data, labels, training=True, transform=None, seq_len=5000, flip=0.5, noise_level=0, class_split=0.0):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n        self.training = training\n        self.flip = flip\n        self.noise_level = noise_level\n        self.class_split = class_split\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        data = self.data[idx]\n        labels = self.labels[idx]\n\n        return [data.astype(np.float32), labels.astype(int)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, train_tr, new_splits = split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install torchcontrib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_toolbelt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_toolbelt import losses as L","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom torchcontrib.optim import SWA\nimport torchcontrib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest_y = np.zeros([int(2000000/GROUP_BATCH_SIZE), GROUP_BATCH_SIZE, 1])\ntest_dataset = IronDataset(test, test_y, flip=False)\ntest_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False)\ntest_preds_all = np.zeros((2000000, 11))\n\n\noof_score = []\nfor index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n    print(\"Fold : {}\".format(index))\n    train_dataset = IronDataset(train[train_index], train_tr[train_index], seq_len=GROUP_BATCH_SIZE, flip=flip, noise_level=noise)\n    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True,num_workers = 16)\n\n    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=False)\n    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False)\n\n    it = 0\n    model = Classifier()\n    model = model.cuda()\n\n    early_stopping = EarlyStopping(patience=40, is_maximize=True,\n                                   checkpoint_path=os.path.join(outdir, \"gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index,\n                                                                                                             it)))\n\n    weight = None#cal_weights()\n    criterion = L.FocalLoss()\n    \n\n    \n    #schedular = torch.optim.lr_scheduler.CyclicLR(optimizer,base_lr=LR, max_lr=0.003, step_size_up=len(train_dataset)/2, cycle_momentum=False)\n    \n#     schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.2)\n    \n#     schedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, [40,70], gamma=0.333, last_epoch=-1)\n    \n    avg_train_losses, avg_valid_losses = [], []\n\n    \n\n    for epoch in range( EPOCHS):\n        if epoch < 1:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n        elif epoch < 7:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/3)\n        elif epoch < 50:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/5)\n        elif epoch < 60:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/7)\n        elif epoch < 70:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/9)\n        elif epoch < 80:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/11)\n        elif epoch < 90:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/13)\n        else:\n            optimizer = torch.optim.Adam(model.parameters(), lr=LR/100)\n            optimizer = torchcontrib.optim.SWA(optimizer, swa_start=2, swa_freq=2, swa_lr=0.00011)\n    \n        train_losses, valid_losses = [], []\n        tr_loss_cls_item, val_loss_cls_item = [], []\n\n        model.train()  # prep model for training\n        train_preds, train_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()#.to(device)\n        \n        print('**********************************')\n        print(\"Folder : {} Epoch : {}\".format(index, epoch))\n        print(\"Curr learning_rate: {:0.9f}\".format(optimizer.param_groups[0]['lr']))\n        \n            #loss_fn(model(input), target).backward()\n        for x, y in tqdm(train_dataloader):\n            x = x.cuda()\n            y = y.cuda()\n            #print(x.shape)\n            \n         \n            \n            optimizer.zero_grad()\n            predictions = model(x)\n\n            predictions_ = predictions.view(-1, predictions.shape[-1])\n            y_ = y.view(-1)\n\n            loss = criterion(predictions_, y_)\n\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            \n#             schedular.step(loss)\n            # record training lossa\n            train_losses.append(loss.item())\n            train_true = torch.cat([train_true, y_], 0)\n            train_preds = torch.cat([train_preds, predictions_], 0)\n\n        #model.eval()  # prep model for evaluation\n        if epoch >= 90:\n            optimizer.update_swa()\n            optimizer.swap_swa_sgd()\n        val_preds, val_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()\n        print('EVALUATION')\n        with torch.no_grad():\n            for x, y in tqdm(valid_dataloader):\n                x = x.cuda()#.to(device)\n                y = y.cuda()#..to(device)\n\n                predictions = model(x)\n                predictions_ = predictions.view(-1, predictions.shape[-1])\n                y_ = y.view(-1)\n\n                loss = criterion(predictions_, y_)\n\n                valid_losses.append(loss.item())\n\n\n                val_true = torch.cat([val_true, y_], 0)\n                val_preds = torch.cat([val_preds, predictions_], 0)\n \n        \n        # calculate average loss over an epoch\n        train_loss = np.average(train_losses)\n        valid_loss = np.average(valid_losses)\n        avg_train_losses.append(train_loss)\n        avg_valid_losses.append(valid_loss)\n        print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n\n        train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1),\n                               labels=list(range(11)), average='macro')\n\n        val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1),\n                             labels=list(range(11)), average='macro')\n\n#         schedular.step()\n        print(\"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n        res = early_stopping(val_score, model)\n        #print('fres:', res)\n        if  res == 2:\n            print(\"Early Stopping\")\n            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n            break\n        elif res == 1:\n            print('save folder %d global val max f1 model score %f' % (index, val_score))\n    print('Folder {} finally best global max f1 score is {}'.format(index, early_stopping.best_score))\n    oof_score.append(round(early_stopping.best_score, 6))\n    \n    model.eval()\n    pred_list = []\n    with torch.no_grad():\n        for x, y in tqdm(test_dataloader):\n            \n            x = x.cuda()\n            y = y.cuda()\n\n            predictions = model(x)\n            predictions_ = predictions.view(-1, predictions.shape[-1]) # shape [128, 4000, 11]\n            #print(predictions.shape, F.softmax(predictions_, dim=1).cpu().numpy().shape)\n            pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy()) # shape (512000, 11)\n            #a = input()\n        test_preds = np.vstack(pred_list) # shape [2000000, 11]\n        test_preds_all += test_preds\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('all folder score is:%s'%str(oof_score))\nprint('OOF mean score is: %f'% (sum(oof_score)/len(oof_score)))\nprint('Generate submission.............')\nsubmission_csv_path = '/kaggle/input/liverpool-ion-switching/sample_submission.csv'\nss = pd.read_csv(submission_csv_path, dtype={'time': str})\ntest_preds_all = test_preds_all / np.sum(test_preds_all, axis=1)[:, None]\ntest_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n                                'open_channels': np.argmax(test_preds_all, axis=1)})\ntest_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\nprint('over')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lot more to come,i am new in this field, any suggestions in the comment box for improving this model is highly appreciated,thanks**"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"'''x = torch.randn((16,4000, 128))\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nattention = Attention(128,4000)\nattention(x)'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"\n'''x = torch.randn((2,64,300))\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nprint(x.shape)\n#x = x.permute(0, 2, 1)\nattention = Attention(300,64)\nattention(x)\n#attention'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}