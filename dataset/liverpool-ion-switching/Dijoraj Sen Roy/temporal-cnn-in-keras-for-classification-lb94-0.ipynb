{"cells":[{"metadata":{},"cell_type":"markdown","source":"### 1. Data Preprocessing","execution_count":null},{"metadata":{"_uuid":"c07e849d-99b2-4303-9b9e-91db4c0b41c8","_cell_guid":"45ec5c84-581a-46dc-85bf-3ec453403fcd","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np \nimport pandas as pd\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.layers import Input, Conv1D, Dense, Activation, Dropout,Multiply\nfrom tensorflow.keras.layers import BatchNormalization,Bidirectional,GRU,Multiply, Add\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nimport datetime\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GROUP_BATCH_SIZE = 4000\nWINDOWS = [10, 50]\n\n\nBASE_PATH = '/kaggle/input/liverpool-ion-switching'\nDATA_PATH = '/kaggle/input/data-without-drift'\nRFC_DATA_PATH = '/kaggle/input/ion-shifted-rfc-proba'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef create_rolling_features(df):\n    for window in WINDOWS:\n        df[\"rolling_mean_\" + str(window)] = df['signal'].rolling(window=window).mean()\n        df[\"rolling_std_\" + str(window)] = df['signal'].rolling(window=window).std()\n        df[\"rolling_var_\" + str(window)] = df['signal'].rolling(window=window).var()\n        df[\"rolling_min_\" + str(window)] = df['signal'].rolling(window=window).min()\n        df[\"rolling_max_\" + str(window)] = df['signal'].rolling(window=window).max()\n        df[\"rolling_min_max_ratio_\" + str(window)] = df[\"rolling_min_\" + str(window)] / df[\"rolling_max_\" + str(window)]\n        df[\"rolling_min_max_diff_\" + str(window)] = df[\"rolling_max_\" + str(window)] - df[\"rolling_min_\" + str(window)]\n\n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    return df\n\n\ndef create_features(df, batch_size):\n    \n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n    for window in WINDOWS:    \n        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n        \n    df['signal_2'] = df['signal'] ** 2\n    return df   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reading data\ntrain = pd.read_csv(f'{DATA_PATH}/train_clean.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\ntest  = pd.read_csv(f'{DATA_PATH}/test_clean.csv', dtype={'time': np.float32, 'signal': np.float32})\nsub  = pd.read_csv(f'{BASE_PATH}/sample_submission.csv', dtype={'time': np.float32})\n\n# loading and adding shifted-rfc-proba features\ny_train_proba = np.load(f\"{RFC_DATA_PATH}/Y_train_proba.npy\")\ny_test_proba = np.load(f\"{RFC_DATA_PATH}/Y_test_proba.npy\")\n\nfor i in range(11):\n    train[f\"proba_{i}\"] = y_train_proba[:, i]\n    test[f\"proba_{i}\"] = y_test_proba[:, i]\n\n    \ntrain = create_rolling_features(train) ##trying to not use feature engg for TCN\ntest = create_rolling_features(test)   \n    \n## normalizing features\ntrain_mean = train.signal.mean()\ntrain_std = train.signal.std()\ntrain['signal'] = (train.signal - train_mean) / train_std\ntest['signal'] = (test.signal - train_mean) / train_std\n\n\nprint('Shape of train is ',train.shape)\nprint('Shape of test is ',test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create features\n\nbatch_size = GROUP_BATCH_SIZE\n\ntrain = create_features(train, batch_size)\ntest = create_features(test, batch_size)\n\ncols_to_remove = ['time','signal','open_channels','batch','batch_index','batch_slices','batch_slices2', 'group']\ncols = [c for c in train.columns if c not in cols_to_remove]\nX = train[cols]\ny = train['open_channels']\nX_pred = test[cols]\nX_pred = X_pred.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndel train\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Model -- TCN edit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import inspect\nfrom typing import List\nfrom tensorflow.keras import backend as K, Model, Input, optimizers\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda, LSTM, Dropout\nfrom tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_power_of_two(num):\n    return num != 0 and ((num & (num - 1)) == 0)\n\n\ndef adjust_dilations(dilations):\n    if all([is_power_of_two(i) for i in dilations]):\n        return dilations\n    else:\n        new_dilations = [2 ** i for i in dilations]\n        return new_dilations\n\n\nclass ResidualBlock(Layer):\n\n    def __init__(self,\n                 dilation_rate,\n                 nb_filters,\n                 kernel_size,\n                 padding,\n                 activation='relu',\n                 dropout_rate=0,\n                 kernel_initializer='he_normal',\n                 use_batch_norm=False,\n                 use_layer_norm=False,\n                 **kwargs):\n\n        # type: (int, int, int, str, str, float, str, bool, bool, bool, dict) -> None\n        self.dilation_rate = dilation_rate\n        self.nb_filters = nb_filters\n        self.kernel_size = kernel_size\n        self.padding = padding\n        self.activation = activation\n        self.dropout_rate = dropout_rate\n        self.use_batch_norm = use_batch_norm\n        self.use_layer_norm = use_layer_norm\n        self.kernel_initializer = kernel_initializer\n        self.layers = []\n        self.layers_outputs = []\n        self.shape_match_conv = None\n        self.res_output_shape = None\n        self.final_activation = None\n\n        super(ResidualBlock, self).__init__(**kwargs)\n\n    def _add_and_activate_layer(self, layer):\n        \"\"\"Helper function for building layer\n        Args:\n            layer: Appends layer to internal layer list and builds it based on the current output\n                   shape of ResidualBlocK. Updates current output shape.\n        \"\"\"\n        self.layers.append(layer)\n        self.layers[-1].build(self.res_output_shape)\n        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n\n    def build(self, input_shape):\n\n        with K.name_scope(self.name):  # name scope used to make sure weights get unique names\n            self.layers = []\n            self.res_output_shape = input_shape\n\n            for k in range(2):\n                name = 'conv1D_{}'.format(k)\n                with K.name_scope(name):  # name scope used to make sure weights get unique names\n                    self._add_and_activate_layer(Conv1D(filters=self.nb_filters,\n                                                        kernel_size=self.kernel_size,\n                                                        dilation_rate=self.dilation_rate,\n                                                        padding=self.padding,\n                                                        name=name,\n                                                        kernel_initializer=self.kernel_initializer))\n\n                with K.name_scope('norm_{}'.format(k)):\n                    if self.use_batch_norm:\n                        self._add_and_activate_layer(BatchNormalization())\n                    elif self.use_layer_norm:\n                        self._add_and_activate_layer(LayerNormalization())\n\n                self._add_and_activate_layer(Activation('relu'))\n                self._add_and_activate_layer(SpatialDropout1D(rate=self.dropout_rate))\n\n            if self.nb_filters != input_shape[-1]:\n                # 1x1 conv to match the shapes (channel dimension).\n                name = 'matching_conv1D'\n                with K.name_scope(name):\n                    # make and build this layer separately because it directly uses input_shape\n                    self.shape_match_conv = Conv1D(filters=self.nb_filters,\n                                                   kernel_size=1,\n                                                   padding='same',\n                                                   name=name,\n                                                   kernel_initializer=self.kernel_initializer)\n\n            else:\n                name = 'matching_identity'\n                self.shape_match_conv = Lambda(lambda x: x, name=name)\n\n            with K.name_scope(name):\n                self.shape_match_conv.build(input_shape)\n                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n\n            self.final_activation = Activation(self.activation)\n            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n\n            # this is done to force Keras to add the layers in the list to self._layers\n            for layer in self.layers:\n                self.__setattr__(layer.name, layer)\n            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n            self.__setattr__(self.final_activation.name, self.final_activation)\n\n            super(ResidualBlock, self).build(input_shape)  # done to make sure self.built is set True\n\n    def call(self, inputs, training=None):\n        \"\"\"\n        Returns: A tuple where the first element is the residual model tensor, and the second\n                 is the skip connection tensor.\n        \"\"\"\n        x = inputs\n        self.layers_outputs = [x]\n        for layer in self.layers:\n            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n            x = layer(x, training=training) if training_flag else layer(x)\n            self.layers_outputs.append(x)\n        x2 = self.shape_match_conv(inputs)\n        self.layers_outputs.append(x2)\n        res_x = layers.add([x2, x])\n        self.layers_outputs.append(res_x)\n\n        res_act_x = self.final_activation(res_x)\n        self.layers_outputs.append(res_act_x)\n        return [res_act_x, x]\n\n    def compute_output_shape(self, input_shape):\n        return [self.res_output_shape, self.res_output_shape]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TCN(Layer):\n    def __init__(self,\n                 nb_filters=32,\n                 kernel_size=2,\n                 nb_stacks=1,\n                 dilations=(1, 2, 4, 8, 16),\n                 padding='same',\n                 use_skip_connections=False,\n                 dropout_rate=0.0,\n                 return_sequences=False,\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 use_batch_norm=False,\n                 use_layer_norm=False,\n                 **kwargs):\n\n        self.return_sequences = return_sequences\n        self.dropout_rate = dropout_rate\n        self.use_skip_connections = use_skip_connections\n        self.dilations = dilations\n        self.nb_stacks = nb_stacks\n        self.kernel_size = kernel_size\n        self.nb_filters = nb_filters\n        self.activation = activation\n        self.padding = padding\n        self.kernel_initializer = kernel_initializer\n        self.use_batch_norm = use_batch_norm\n        self.use_layer_norm = use_layer_norm\n        self.skip_connections = []\n        self.residual_blocks = []\n        self.layers_outputs = []\n        self.build_output_shape = None\n        self.lambda_layer = None\n        self.lambda_ouput_shape = None\n\n        if padding != 'causal' and padding != 'same':\n            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n\n        if not isinstance(nb_filters, int):\n            print('An interface change occurred after the version 2.1.2.')\n            print('Before: tcn.TCN(x, return_sequences=False, ...)')\n            print('Now should be: tcn.TCN(return_sequences=False, ...)(x)')\n            print('The alternative is to downgrade to 2.1.2 (pip install keras-tcn==2.1.2).')\n            raise Exception()\n\n        # initialize parent class\n        super(TCN, self).__init__(**kwargs)\n\n    @property\n    def receptive_field(self):\n        assert_msg = 'The receptive field formula works only with power of two dilations.'\n        assert all([is_power_of_two(i) for i in self.dilations]), assert_msg\n        return self.kernel_size * self.nb_stacks * self.dilations[-1]\n\n    def build(self, input_shape):\n\n        # member to hold current output shape of the layer for building purposes\n        self.build_output_shape = input_shape\n\n        # list to hold all the member ResidualBlocks\n        self.residual_blocks = []\n        total_num_blocks = self.nb_stacks * len(self.dilations)\n        if not self.use_skip_connections:\n            total_num_blocks += 1  # cheap way to do a false case for below\n\n        for s in range(self.nb_stacks):\n            for d in self.dilations:\n                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n                                                          nb_filters=self.nb_filters,\n                                                          kernel_size=self.kernel_size,\n                                                          padding=self.padding,\n                                                          activation=self.activation,\n                                                          dropout_rate=self.dropout_rate,\n                                                          use_batch_norm=self.use_batch_norm,\n                                                          use_layer_norm=self.use_layer_norm,\n                                                          kernel_initializer=self.kernel_initializer,\n                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n                # build newest residual block\n                self.residual_blocks[-1].build(self.build_output_shape)\n                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n\n        # this is done to force keras to add the layers in the list to self._layers\n        for layer in self.residual_blocks:\n            self.__setattr__(layer.name, layer)\n\n        # Author: @karolbadowski.\n        output_slice_index = int(self.build_output_shape.as_list()[1] / 2) if self.padding == 'same' else -1\n        self.lambda_layer = Lambda(lambda tt: tt[:, output_slice_index, :])\n        self.lambda_ouput_shape = self.lambda_layer.compute_output_shape(self.build_output_shape)\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n        Overridden in case keras uses it somewhere... no idea. Just trying to avoid future errors.\n        \"\"\"\n        if not self.built:\n            self.build(input_shape)\n        if not self.return_sequences:\n            return self.lambda_ouput_shape\n        else:\n            return self.build_output_shape\n\n    def call(self, inputs, training=None):\n        x = inputs\n        self.layers_outputs = [x]\n        self.skip_connections = []\n        for layer in self.residual_blocks:\n            x, skip_out = layer(x, training=training)\n            self.skip_connections.append(skip_out)\n            self.layers_outputs.append(x)\n\n        if self.use_skip_connections:\n            x = layers.add(self.skip_connections)\n            self.layers_outputs.append(x)\n\n        if not self.return_sequences:\n            x = self.lambda_layer(x)\n            self.layers_outputs.append(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Model implementation\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##running the model for each fold\nskf = StratifiedKFold(n_splits=25, random_state=None, shuffle=True)\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\nX_train = X_train.values\ny_train = y_train.values\nX_test = X_test.values\ny_test = y_test.values\n\n#One Hot Encoding Target Variables\nonh = OneHotEncoder(sparse=False)\ny_train = y_train.reshape(len(y_train),1)\ny_train = onh.fit_transform(y_train)\ny_test = y_test.reshape(len(y_test),1)\ny_test = onh.fit_transform(y_test)\n#Making 2D into 3D tensors\nX_train= X_train.reshape((X_train.shape[0],X_train.shape[1],1))\nX_test= X_test.reshape((X_test.shape[0],X_test.shape[1],1))\nX_pred=X_pred.reshape((X_pred.shape[0],X_pred.shape[1],1))\n\nprint(\"X_train shape:\",X_train.shape)\nprint(\"X_test shape:\",X_test.shape)\nprint(\"y_train shape:\",y_train.shape)\nprint(\"y_test shape:\",y_test.shape)\nprint(\"X_pred shape:\",X_pred.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"I = Input(shape=(X_train.shape[1],1))\nmodel = TCN(nb_filters=32,kernel_size=6,dilations=[1, 2, 4, 8, 16, 32],return_sequences=True)(I)\nmodel = GRU(256,return_sequences=True)(model)\nmodel = Dropout(0.2)(model)\nmodel = GRU(256)(model)\nmodel = Dropout(0.2)(model)\n#model = GRU(128)(model)\nmodel = Dense(512,activation='relu',kernel_initializer='uniform')(model)\nmodel = Dense(512,activation='relu',kernel_initializer='uniform')(model)\n#model = Dense(512,activation='relu',kernel_initializer='uniform')(model)\nmodel = Dense(11,activation='softmax')(model)\n\nclassifier = Model(inputs=[I], outputs=[model])\nclassifier.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##loading weights from a pre-trained model\nfrom tensorflow.keras.models import load_model\n#classifier.save_weights('TCNweights.h5')\nclassifier.load_weights('/kaggle/input/weightstcn/TCNweights.h5')\n##accuracy achieved was 96.80 on 16 epochs possibly not converged since val loss was decreasing slow--retrain##","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##compiling model and fitting data\nclassifier.compile(optimizer = Adam(learning_rate=0.003, amsgrad=True),\n                   loss = 'categorical_crossentropy',\n                   metrics =['accuracy'])\nclassifier.fit(X_train,y_train,\n          epochs=16,batch_size=4000,\n          validation_data=[X_test,y_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Prediction and reversing One Hot Encoding\ny_pred=classifier.predict(X_pred)\ny_pred =onh.inverse_transform(y_pred)\ny_pred #Should be 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making submission\nsub = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\nsub.iloc[:,1] = y_pred[:,0]\nsub.to_csv('submission.csv',index=False,float_format='%.4f')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}