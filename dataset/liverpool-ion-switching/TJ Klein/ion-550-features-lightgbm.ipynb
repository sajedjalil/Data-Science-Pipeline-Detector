{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ion 550 Features (LightGBM)\n\n### Credit for the Feature Engineering code goes to [artgor](https://www.kaggle.com/artgor). I took his FE template and implemented a simple Light GBM model to see how these aggregated features perform. Note that I had to reduce the number of Batch Size and Window Size options in order to avoid memory issues, so it isn't the full 550 feature set. I plan on trying out different Batch and Window Sizes to see if I can't land on ones that lead to best performance.\n\n* ### For access to the original 550 feature code, see the dataset in this link: https://www.kaggle.com/artgor/ion-features"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport h5py\nimport gc\nfrom sklearn.model_selection import KFold, train_test_split\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n                start_mem - end_mem) / start_mem))\n    return df\n\n\ndef generate_features(data: pd.DataFrame,\n                      batch_sizes: list,\n                      window_sizes: list) -> pd.DataFrame:\n    \"\"\"\n    Generate features for https://www.kaggle.com/c/liverpool-ion-switching\n\n    Generate various aggregations over the data.\n\n    Args:\n        window_sizes: window sizes for rolling features\n        batch_sizes: batch sizes for which features are aggregated\n        data: original dataframe\n\n    Returns:\n        dataframe with generated features\n    \"\"\"\n    for batch_size in batch_sizes:\n        data['batch'] = ((data['time'] * 10_000) - 1) // batch_size\n        data['batch_index'] = ((data['time'] * 10_000) - 1) - (data['batch'] * batch_size)\n        data['batch_slices'] = data['batch_index'] // (batch_size / 10)\n        data['batch_slices2'] = data['batch'].astype(str).str.zfill(3) + '_' + data['batch_slices'].astype(\n            str).str.zfill(3)\n        data['batch_slices3'] = data['batch_index'] // (batch_size / 5)\n        data['batch_slices4'] = data['batch'].astype(str).str.zfill(3) + '_' + data['batch_slices3'].astype(\n            str).str.zfill(3)\n        data['batch_slices5'] = data['batch_index'] // (batch_size / 2)\n        data['batch_slices6'] = data['batch'].astype(str).str.zfill(3) + '_' + data['batch_slices5'].astype(\n            str).str.zfill(3)\n\n        for agg_feature in ['batch', 'batch_slices2', 'batch_slices4', 'batch_slices6']:\n            data[f\"min_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('min')\n            data[f\"max_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('max')\n            data[f\"std_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('std')\n            data[f\"mean_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('mean')\n            data[f\"median_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].transform('median')\n\n            data[f\"mean_abs_chg_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].apply(\n                lambda x: np.mean(np.abs(np.diff(x))))\n            data[f\"abs_max_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].apply(\n                lambda x: np.max(np.abs(x)))\n            data[f\"abs_min_{agg_feature}_{batch_size}\"] = data.groupby(agg_feature)['signal'].apply(\n                lambda x: np.min(np.abs(x)))\n\n            data[f\"min_{agg_feature}_{batch_size}_diff\"] = data[f\"min_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"max_{agg_feature}_{batch_size}_diff\"] = data[f\"max_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"std_{agg_feature}_{batch_size}_diff\"] = data[f\"std_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"mean_{agg_feature}_{batch_size}_diff\"] = data[f\"mean_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"median_{agg_feature}_{batch_size}_diff\"] = data[f\"median_{agg_feature}_{batch_size}\"] - data[\n                'signal']\n\n            data[f\"range_{agg_feature}_{batch_size}\"] = data[f\"max_{agg_feature}_{batch_size}\"] - data[\n                f\"min_{agg_feature}_{batch_size}\"]\n            data[f\"maxtomin_{agg_feature}_{batch_size}\"] = data[f\"max_{agg_feature}_{batch_size}\"] / data[\n                f\"min_{agg_feature}_{batch_size}\"]\n            data[f\"abs_avg_{agg_feature}_{batch_size}\"] = (data[f\"abs_min_{agg_feature}_{batch_size}\"] + data[\n                f\"abs_max_{agg_feature}_{batch_size}\"]) / 2\n\n            data[f'signal_shift+1_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(1)['signal']\n            data[f'signal_shift-1_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(-1)['signal']\n            data[f'signal_shift+2_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(2)['signal']\n            data[f'signal_shift-2_{agg_feature}_{batch_size}'] = data.groupby([agg_feature]).shift(-2)['signal']\n\n            data[f\"signal_shift+1_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift+1_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"signal_shift-1_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift-1_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"signal_shift+2_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift+2_{agg_feature}_{batch_size}\"] - data['signal']\n            data[f\"signal_shift-2_{agg_feature}_{batch_size}_diff\"] = data[f\"signal_shift-2_{agg_feature}_{batch_size}\"] - data['signal']\n\n        for window in window_sizes:\n            window = min(batch_size, window)\n\n            data[\"rolling_mean_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).mean().reset_index()['signal']\n            data[\"rolling_std_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).std().reset_index()['signal']\n            data[\"rolling_min_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).min().reset_index()['signal']\n            data[\"rolling_max_\" + str(window) + '_batch_' + str(batch_size)] = \\\n                data.groupby('batch')['signal'].rolling(window=window).max().reset_index()['signal']\n\n            data[f'exp_Moving__{window}_{batch_size}'] = data.groupby('batch')['signal'].apply(\n                lambda x: x.ewm(alpha=0.5, adjust=False).mean())\n        data = reduce_mem_usage(data)\n    data.fillna(0, inplace=True)\n\n    return data\n\n\ndef read_data(path: str = ''):\n    \"\"\"\n    Read train, test data\n\n    Args:\n        path: path to the data\n\n    Returns:\n        two dataframes\n    \"\"\"\n    train_df = pd.read_csv(f'{path}/train.csv')\n    test_df = pd.read_csv(f'{path}/test.csv')\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"directory = '/kaggle/input/liverpool-ion-switching/'\ntrain,test = read_data(directory)\nsample_submission = pd.read_csv(f'{directory}/sample_submission.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_sizes = [25000]\nwindow_sizes = [10, 25, 50, 5000, 10000]\n\ngenerated_train = generate_features(train, batch_sizes, window_sizes)\n\ndel train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feats = generated_train.columns\nprint(feats)\nfeats = np.delete(feats,[2,3,4,5,6,7,8,9,10]) # Delete Target from features\ntarget = ['open_channels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generated_test = generate_features(test, batch_sizes, window_sizes)\n\ndel test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nparams = {'learning_rate': 0.05, 'max_depth': -1, 'num_leaves':200, 'metric': 'rmse', 'random_state': 42, 'n_jobs':-1, 'sample_fraction':0.33} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/siavrez/simple-eda-model\nfrom sklearn.metrics import confusion_matrix, f1_score, mean_absolute_error, make_scorer\ndef MacroF1Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average = 'macro')\n    return ('MacroF1Metric', score, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1, x2, y1, y2 = train_test_split(generated_train[feats], generated_train[target], test_size=0.3, random_state=42)\nmodel = lgb.train(params, lgb.Dataset(x1, y1), 2000,  lgb.Dataset(x2, y2), verbose_eval=100, early_stopping_rounds=100, feval=MacroF1Metric)\ndel x1, x2, y1, y2\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ = model.predict(generated_test[feats], num_iteration=model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['open_channels'] = np.round(np.clip(preds_, 0, 10)).astype(int)\nsample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\ndisplay(sample_submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig =  plt.figure(figsize = (25,25))\naxes = fig.add_subplot(111)\nlgb.plot_importance(model,ax = axes,height = 0.5)\nplt.show();plt.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}