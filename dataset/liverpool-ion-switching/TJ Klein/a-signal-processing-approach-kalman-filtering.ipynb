{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A Signal Processing Approach - Kalman Filtering\n\n\n### Acknowledgements:\n@FriedChips - https://www.kaggle.com/friedchips/the-viterbi-algorithm-a-complete-solution"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import datetime\nimport numpy as np\nimport scipy as sp\nimport scipy.fftpack\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter,filtfilt,freqz, iirnotch\nfrom sklearn import *\nfrom sklearn.metrics import f1_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import Pool,CatBoostRegressor\nimport time\nimport datetime\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom pykalman import KalmanFilter\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/data-without-drift/train_clean.csv')\ntest = pd.read_csv('../input/data-without-drift/test_clean.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_axes_grid(numplots_x, numplots_y, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    return fig, axes\n\ndef set_axes(axes, use_grid=True, x_val = [0,100,10,5], y_val = [-50,50,10,5]):\n    axes.grid(use_grid)\n    axes.tick_params(which='both', direction='inout', top=True, right=True, labelbottom=True, labelleft=True)\n    axes.set_xlim(x_val[0], x_val[1])\n    axes.set_ylim(y_val[0], y_val[1])\n    axes.set_xticks(np.linspace(x_val[0], x_val[1], np.around((x_val[1] - x_val[0]) / x_val[2] + 1).astype(int)))\n    axes.set_xticks(np.linspace(x_val[0], x_val[1], np.around((x_val[1] - x_val[0]) / x_val[3] + 1).astype(int)), minor=True)\n    axes.set_yticks(np.linspace(y_val[0], y_val[1], np.around((y_val[1] - y_val[0]) / y_val[2] + 1).astype(int)))\n    axes.set_yticks(np.linspace(y_val[0], y_val[1], np.around((y_val[1] - y_val[0]) / y_val[3] + 1).astype(int)), minor=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calc_markov_p_trans(states):\n    max_state = np.max(states)\n    states_next = np.roll(states, -1)\n    matrix = []\n    for i in range(max_state + 1):\n        current_row = np.histogram(states_next[states == i], bins=np.arange(max_state + 2))[0]\n        if np.sum(current_row) == 0: # if a state doesn't appear in states...\n            current_row = np.ones(max_state + 1) / (max_state + 1) # ...use uniform probability\n        else:\n            current_row = current_row / np.sum(current_row) # normalize to 1\n        matrix.append(current_row)\n    return np.array(matrix)\n\ndef calc_markov_p_signal(state, signal, num_bins = 1000):\n    states_range = np.arange(state.min(), state.max() + 1)\n    signal_bins = np.linspace(signal.min(), signal.max(), num_bins + 1)\n    p_signal = np.array([ np.histogram(signal[state == s], bins=signal_bins)[0] for s in states_range ])\n    p_signal = np.array([ p / np.sum(p) for p in p_signal ]) # normalize to 1\n    return p_signal, signal_bins\n\ndef digitize_signal(signal, signal_bins):\n    signal_dig = np.digitize(signal, bins=signal_bins) - 1 # these -1 and -2 are necessary because of the way...\n    signal_dig = np.minimum(signal_dig, len(signal_bins) - 2) # ... numpy.digitize works\n    return signal_dig\n\ndef viterbi(p_trans, p_signal, p_in, signal):\n\n    offset = 10**(-20) # added to values to avoid problems with log2(0)\n    \n    p_trans_tlog  = np.transpose(np.log2(p_trans  + offset)) # p_trans, logarithm + transposed\n    p_signal_tlog = np.transpose(np.log2(p_signal + offset)) # p_signal, logarithm + transposed\n    p_in_log      =              np.log2(p_in     + offset)  # p_in, logarithm\n    p_state_log = [ p_in_log + p_signal_tlog[signal[0]] ] # initial state probabilities for signal element 0 \n\n    for s in signal[1:]:\n        p_state_log.append(np.max(p_state_log[-1] + p_trans_tlog, axis=1) + p_signal_tlog[s]) # the Viterbi algorithm\n\n    states = np.argmax(p_state_log, axis=1) # finding the most probable states\n    \n    return states","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Kalman1D(observations,damping=1):\n    # To return the smoothed time series data\n    observation_covariance = damping\n    initial_value_guess = observations[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    initial_value_guess\n    kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(observations)\n    return pred_state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 500000\nnum_batches = 10\nres = 1000 # Resolution of signal plots\n\nfs = 10000       # sample rate, 10kHz\nnyq = 0.5 * fs  # Nyquist Frequency","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5));\nplt.plot(range(0,train.shape[0],res),train.signal[0::res])\nfor i in range(num_batches+1): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\nfor j in range(num_batches): plt.text(j*batch_size+200000,num_batches,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Signal - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5));\nplt.plot(range(0,train.shape[0],res),train.open_channels[0::res])\nfor i in range(num_batches+1): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\nfor j in range(num_batches): plt.text(j*batch_size+200000,num_batches,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 5\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"observation_covariance = .0015\nsignal_kalman_train = Kalman1D(train.signal.values,observation_covariance)\nkalman = pd.DataFrame(signal_kalman_train,columns=['signal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_unfiltered = []\nf1_unfiltered_avg = []\nf1_filtered = []\nf1_filtered_avg = []\n\nfor batch in range(1,10,1):\n    # Unfiltered Viterbi F1 Macro\n    p_trans = calc_markov_p_trans(train.open_channels[batch_size*(batch-1):batch_size*batch])\n    p_signal_unfiltered, signal_bins = calc_markov_p_signal(train.open_channels[batch_size*(batch-1):batch_size*batch], train.signal[batch_size*(batch-1):batch_size*batch])\n    signal_dig = digitize_signal(train.signal[batch_size*(batch-1):batch_size*batch], signal_bins)\n    p_in = np.ones(len(p_trans)) / len(p_trans)\n    viterbi_state = viterbi(p_trans, p_signal_unfiltered, p_in, signal_dig)\n    print(f'Batch = {batch}')\n    f1_unfiltered = f1_score(y_pred=viterbi_state, y_true=train.open_channels[batch_size*(batch-1):batch_size*batch], average='macro')\n    print(\"Unfiltered - F1 macro =\", f1_unfiltered)\n    f1_unfiltered_avg += f1_unfiltered\n    \n    # Kalman Filtered Viterbi F1 Macro\n    p_trans = calc_markov_p_trans(train.open_channels[batch_size*(batch-1):batch_size*batch])\n    p_signal_filtered, signal_bins = calc_markov_p_signal(train.open_channels[batch_size*(batch-1):batch_size*batch], kalman.signal[batch_size*(batch-1):batch_size*batch])\n    signal_dig = digitize_signal(kalman.signal[batch_size*(batch-1):batch_size*batch], signal_bins)\n    p_in = np.ones(len(p_trans)) / len(p_trans)\n    viterbi_state = viterbi(p_trans, p_signal_filtered, p_in, signal_dig)\n    f1_filtered = f1_score(y_pred=viterbi_state, y_true=train.open_channels[batch_size*(batch-1):batch_size*batch], average='macro')\n    print(\"Kalman Filtered - F1 macro =\", f1_filtered)\n    f1_filtered_avg += f1_filtered","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,1,30,7)\nset_axes(axes, x_val=[0, 1000, 100, 10], y_val=[0,0.02,0.005,0.001])\naxes.set_title('Signal probability distribution for each state (not normalized)')\nfor s,p in enumerate(p_signal_unfiltered):\n    axes.plot(p, label=\"Unfiltered - State \"+str(s));\nfor s,p in enumerate(p_signal_filtered):\n    axes.plot(p, label=\"Filtered - State \"+str(s));    \naxes.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),train.signal[batch_size*(batch-1):batch_size*batch:res])\nax[1].plot(range(0,batch_size,res),kalman.signal[batch_size*(batch-1):batch_size*batch:res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['signal', 'filtered signal'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fft = sp.fftpack.fft(train.signal[batch_size*(batch-1):batch_size*batch])\npsd = np.abs(fft) ** 2\nfftfreq = sp.fftpack.fftfreq(len(psd),1/fs)\ni = fftfreq > 0\n\nfig, ax = plt.subplots(2, 1, figsize=(10, 6))\nfig.subplots_adjust(hspace = .5)\nax[0].plot(fftfreq[i], 10 * np.log10(psd[i]))\nax[0].set_xlabel('Frequency (1/10000 seconds)')\nax[0].set_ylabel('PSD (dB)')\nax[0].set_title('Unfiltered')\n\n\nfft = sp.fftpack.fft(kalman.signal)\npsd = np.abs(fft) ** 2\nfftfreq = sp.fftpack.fftfreq(len(psd),1/fs)\ni = fftfreq > 0\n\nax[1].plot(fftfreq[i], 10 * np.log10(psd[i]))\nax[1].set_xlabel('Frequency (1/10000 seconds)')\nax[1].set_ylabel('PSD (dB)')\nax[1].set_title('Kalman Filter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P_trans')\nsns.heatmap(\n    p_trans,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\nax[0].plot(range(0,batch_size,res),train.open_channels[batch_size*(batch-1):batch_size*batch:res],color='g')\nax[1].plot(range(0,batch_size,res),viterbi_state[::res])\n\nax[0].legend(['open_channels'])\nax[1].legend(['Viterbi State Prediction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the filtered signal for train 'signal'\ntrain['signal_undrifted'] = signal_kalman_train\n\n# Apply Kalman filter to test data and set as test 'signal'\nsignal_kalman_test = Kalman1D(test.signal.values,observation_covariance)\ntest['signal_undrifted'] = signal_kalman_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index // 50_000\n    df['batch_index'] = df.index  - (df.batch * 50_000)\n    df['batch_slices'] = df['batch_index']  // 5_000\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal_undrifted'].mean()\n        d['median'+c] = df.groupby([c])['signal_undrifted'].median()\n        d['max'+c] = df.groupby([c])['signal_undrifted'].max()\n        d['min'+c] = df.groupby([c])['signal_undrifted'].min()\n        d['std'+c] = df.groupby([c])['signal_undrifted'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal_undrifted'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n        df['range'+c] = df['max'+c] - df['min'+c]\n        df['maxtomin'+c] = df['max'+c] / df['min'+c]\n        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) / 2\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal_undrifted'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal_undrifted'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    # add shifts_2\n    df['signal_shift_+2'] = [0,] + [1,] + list(df['signal_undrifted'].values[:-2])\n    df['signal_shift_-2'] = list(df['signal_undrifted'].values[2:]) + [0] + [1]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==1].index:\n        df['signal_shift_+2'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-2'][i] = np.nan\n    for i in df[df['batch_index']==49998].index:\n        df['signal_shift_-2'][i] = np.nan \n        \n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal_undrifted', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal_undrifted']\n        \n    return df\n\ntrain = features(train)\ntest = features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_score_calc(y_true, y_pred):\n    return f1_score(y_true, y_pred, average=\"macro\")\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = f1_score(labels, preds, average=\"macro\")\n    return ('KaggleMetric', score, True)\n\n\ndef train_model_classification(X, X_test, y, params, model_type='lgb', eval_metric='f1score',\n                               columns=None, plot_feature_importance=False, model=None,\n                               verbose=50, early_stopping_rounds=200, n_estimators=2000):\n\n    columns = X.columns if columns == None else columns\n    X_test = X_test[columns]\n    \n    # to set up scoring parameters\n    metrics_dict = {\n                    'f1score': {'lgb_metric_name': lgb_Metric,}\n                   }\n    \n    result_dict = {}\n    \n    # out-of-fold predictions on train data\n    oof = np.zeros(len(X) )\n    \n    # averaged predictions on train data\n    prediction = np.zeros((len(X_test)))\n    \n    # list of scores on folds\n    scores = []\n    feature_importance = pd.DataFrame()\n    \n    # split and train on folds\n    '''for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n        if type(X) == np.ndarray:\n            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n        else:\n            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]'''\n            \n    if True:        \n        X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X, y, test_size=0.3, random_state=7)    \n            \n        if model_type == 'lgb':\n            #model = lgb.LGBMClassifier(**params, n_estimators=n_estimators)\n            #model.fit(X_train, y_train, \n            #        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n            #       verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            model = lgb.train(params, lgb.Dataset(X_train, y_train),\n                              n_estimators,  lgb.Dataset(X_valid, y_valid),\n                              verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds, feval=lgb_Metric)\n            \n            \n            preds = model.predict(X, num_iteration=model.best_iteration) #model.predict(X_valid) \n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            \n        if model_type == 'xgb':\n            train_set = xgb.DMatrix(X_train, y_train)\n            val_set = xgb.DMatrix(X_valid, y_valid)\n            model = xgb.train(params, train_set, num_boost_round=2222, evals=[(train_set, 'train'), (val_set, 'val')], \n                                     verbose_eval=verbose, early_stopping_rounds=early_stopping_rounds)\n            \n            preds = model.predict(xgb.DMatrix(X)) \n\n            y_pred = model.predict(xgb.DMatrix(X_test))\n            \n\n        if model_type == 'cat':\n            # Initialize CatBoostRegressor\n            model = CatBoostRegressor(params)\n            # Fit model\n            model.fit(X_train, y_train)\n            # Get predictions\n            y_pred_valid = np.round(np.clip(preds, 0, 10)).astype(int)\n\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = np.round(np.clip(y_pred, 0, 10)).astype(int)\n\n \n        oof = preds\n        \n        scores.append(f1_score_calc(y, np.round(np.clip(preds,0,10)).astype(int) ) )\n\n        prediction += y_pred    \n        \n        if model_type == 'lgb' and plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    #prediction /= folds.n_splits\n    \n    print('FINAL score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    result_dict['oof'] = oof\n    result_dict['prediction'] = prediction\n    result_dict['scores'] = scores\n    result_dict['model'] = model\n    \n    if model_type == 'lgb':\n        if plot_feature_importance:\n            feature_importance[\"importance\"] /= folds.n_splits\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n            \n            result_dict['feature_importance'] = feature_importance\n        \n    return result_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_columns = [c for c in train.columns if c not in ['time', 'signal','open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\n\nX = train[good_columns].copy()\ny = train['open_channels']\nX_test = test[good_columns].copy()\n\ndel train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb = {'colsample_bytree': 0.375,'learning_rate': 0.1,'max_depth': 10, 'subsample': 1, 'objective':'reg:squarederror',\n          'eval_metric':'rmse'}\n\nresult_dict_xgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_xgb, model_type='xgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {'learning_rate': 0.1, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1}\n\nresult_dict_lgb = train_model_classification(X=X[0:500000*8-1], X_test=X_test, y=y[0:500000*8-1], params=params_lgb, model_type='lgb', eval_metric='f1score', plot_feature_importance=False,\n                                                      verbose=50, early_stopping_rounds=250, n_estimators=3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_ensemble = 0.50 * result_dict_lgb['prediction'] + 0.50 * result_dict_xgb['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')\nsub['open_channels'] =  np.array(np.round(preds_ensemble,0), np.int) \n\nsub.to_csv('submission.csv', index=False, float_format='%.4f')\nsub.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}