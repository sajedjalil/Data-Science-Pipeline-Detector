{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple two-layer bidirectional LSTM with Pytorch"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport time\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 250\nlr = 0.01\nn_folds = 5\nlstm_input_size = 1\nhidden_state_size = 30\nbatch_size = 30\nnum_sequence_layers = 2\noutput_dim = 11\nnum_time_steps = 4000\nrnn_type = 'LSTM'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Bi_RNN(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=11, num_layers=2, rnn_type='LSTM'):\n        super(Bi_RNN, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n\n        #Define the initial linear hidden layer\n        self.init_linear = nn.Linear(self.input_dim, self.input_dim)\n\n        # Define the LSTM layer\n        self.lstm = eval('nn.' + rnn_type)(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True, bidirectional=True)\n\n        # Define the output layer\n        self.linear = nn.Linear(self.hidden_dim * 2, output_dim)\n\n    def init_hidden(self):\n        # This is what we'll initialise our hidden state as\n        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n\n    def forward(self, input):\n        #Forward pass through initial hidden layer\n        linear_input = self.init_linear(input)\n\n        # Forward pass through LSTM layer\n        # shape of lstm_out: [batch_size, input_size ,hidden_dim]\n        # shape of self.hidden: (a, b), where a and b both\n        # have shape (batch_size, num_layers, hidden_dim).\n        lstm_out, self.hidden = self.lstm(linear_input)\n\n        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n        y_pred = self.linear(lstm_out)\n        return y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define data loaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ION_Dataset_Sequential(Dataset):\n    def __init__(self, input, output):\n        self.input = input\n        self.output = output\n\n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self, idx):\n        x = self.input[idx]\n        y = self.output[idx]\n        x = torch.tensor(x, dtype=torch.float)\n        y = torch.tensor(y, dtype=torch.float)\n        return x, y\n\nclass ION_Dataset_Sequential_test(Dataset):\n    def __init__(self, input):\n        self.input = input\n\n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self, idx):\n        x = self.input[idx]\n        x = torch.tensor(x, dtype=torch.float)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import data\n\nWe removed the drift following https://www.kaggle.com/cdeotte/one-feature-model-0-930/output"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/data-no-drift/train_detrend.csv')\ntest_df = pd.read_csv('/kaggle/input/data-no-drift/test_detrend.csv')\nX = train_df['signal'].values.reshape(-1, num_time_steps, 1)\ny = pd.get_dummies(train_df['open_channels']).values.reshape(-1, num_time_steps, output_dim)\ntest_input = test_df[\"signal\"].values.reshape(-1, num_time_steps, 1)\ntrain_input_mean = X.mean()\ntrain_input_sigma = X.std()\ntest_input = (test_input-train_input_mean)/train_input_sigma\ntest_preds = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\ntest = ION_Dataset_Sequential_test(test_input)\ntest_loader = DataLoader(test, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train, evaluate with 5-fold CV and keep best model on every fold"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iterate through folds\n\nkfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\nlocal_val_score = 0\nmodels = {}\n\nk=0 #initialize fold number\nfor tr_idx, val_idx in kfold.split(X, y):\n    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n\n    print('starting fold', k)\n    k += 1\n\n    print(6*'#', 'splitting and reshaping the data')\n    train_input = X[tr_idx]\n    print(train_input.shape)\n    train_target = y[tr_idx]\n    val_input = X[val_idx]\n    val_target = y[val_idx]\n    train_input_mean = train_input.mean()\n    train_input_sigma = train_input.std()\n    val_input = (val_input-train_input_mean)/train_input_sigma\n    train_input = (train_input-train_input_mean)/train_input_sigma\n\n    print(6*'#', 'Loading')\n    train = ION_Dataset_Sequential(train_input, train_target)\n    valid = ION_Dataset_Sequential(val_input, val_target)\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n\n    #Build tensor data for torch\n    train_preds = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n    val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n    best_val_preds = np.zeros((int(val_input.shape[0] * val_input.shape[1])))\n    train_targets = np.zeros((int(train_input.shape[0] * train_input.shape[1])))\n    avg_losses_f = []\n    avg_val_losses_f = []\n\n    #Define loss function\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    #Build model, initialize weights and define optimizer\n    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Using Adam optimizer\n    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n    temp_val_loss = 9999999999\n    reached_val_score = 0\n\n    #Iterate through epochs\n    for epoch in range(n_epochs):\n        start_time = time.time()\n\n        #Train\n        model.train()\n        avg_loss = 0.\n        for i, (x_batch, y_batch) in enumerate(train_loader):\n            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n            optimizer.zero_grad()\n            y_pred = model(x_batch.cuda())\n            loss = loss_fn(y_pred.cpu(), y_batch)\n            loss.backward()\n            optimizer.step()\n            avg_loss += loss.item() / len(train_loader)\n            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n            train_preds[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = pred.reshape((-1))\n            train_targets[i * batch_size * train_input.shape[1]:(i + 1) * batch_size * train_input.shape[1]] = y_batch.detach().cpu().numpy().argmax(axis=2).reshape((-1))\n            del y_pred, loss, x_batch, y_batch, pred\n\n        #Evaluate\n        model.eval()\n        avg_val_loss = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n            y_batch = y_batch.view(-1, num_time_steps, output_dim)\n            y_pred = model(x_batch.cuda()).detach()\n            avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n            pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n            val_preds[i * batch_size * val_input.shape[1]:(i + 1) * batch_size * val_input.shape[1]] = pred.reshape((-1))\n            del y_pred, x_batch, y_batch, pred\n        if avg_val_loss < temp_val_loss:\n            temp_val_loss = avg_val_loss\n\n        #Calculate F1-score\n        train_score = f1_score(train_targets, train_preds, average='macro')\n        val_score = f1_score(val_target.argmax(axis=2).reshape((-1)), val_preds, average='macro')\n\n        #Print output of epoch\n        elapsed_time = time.time() - start_time\n        scheduler.step(avg_val_loss)\n        if epoch%10 == 0:\n            print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, train_score, avg_val_loss, val_score, elapsed_time))\n\n        if val_score > reached_val_score:\n            reached_val_score = val_score\n            best_model = copy.deepcopy(model.state_dict())\n            best_val_preds = copy.deepcopy(val_preds)\n\n    #Calculate F1-score of the fold\n    val_score_fold = f1_score(val_target.argmax(axis=2).reshape((-1)), best_val_preds, average='macro')\n\n    #Save the fold's model in a dictionary\n    models[k] = best_model\n\n    #Print F1-score of the fold\n    print(\"BEST VALIDATION SCORE (F1): \", val_score_fold)\n    local_val_score += (1/n_folds) * val_score_fold\n\n#Print final average k-fold CV F1-score\nprint(\"Final Score \", local_val_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict test data by averaging model results from 5 folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Iterate through folds\n\nfor k in range(n_folds):\n    test_p = np.zeros((int(test_input.shape[0] * test_input.shape[1])))\n    k += 1\n\n    #Import model of fold k\n    model = Bi_RNN(lstm_input_size, hidden_state_size, batch_size=batch_size, output_dim=output_dim, num_layers=num_sequence_layers, rnn_type=rnn_type)  # (input_dim, hidden_state_size, batch_size, output_dim, num_seq_layers, rnn_type)\n    model = model.to(device)\n    model.load_state_dict(models[k])\n\n    #Make predictions on test data\n    model.eval()\n    for i, x_batch in enumerate(test_loader):\n        x_batch = x_batch.view(-1, num_time_steps, lstm_input_size)\n        y_pred = model(x_batch.cuda()).detach()\n        pred = F.softmax(y_pred, 2).detach().cpu().numpy().argmax(axis=-1)\n        test_p[i * batch_size * test_input.shape[1]:(i + 1) * batch_size * test_input.shape[1]] = pred.reshape((-1))\n        del y_pred, x_batch, pred\n    test_preds += (1/n_folds) * test_p","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generate submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create submission file\ndf_sub = pd.read_csv(\"/kaggle/input/liverpool-ion-switching/sample_submission.csv\", dtype = {'time': str})\ndf_sub.open_channels = np.array(test_preds, np.int)\ndf_sub.to_csv(\"submission_bilstm.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}