{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **On Monte Carlo Simulations**"},{"metadata":{},"cell_type":"markdown","source":"This kernel will cover: \n* Metropolis-Hastings algorithm\n* Hamiltonian Monte Carlo"},{"metadata":{},"cell_type":"markdown","source":"> # **1. On Metropolis-Hastings**"},{"metadata":{},"cell_type":"markdown","source":"## What is Metropolis-Hastings?\n\nMetropolis-Hastings approach allows us to obtain random samples from a distribution where direct samping is difficult. They are used for sampling from multi-dimensional distributions."},{"metadata":{},"cell_type":"markdown","source":"## Our application "},{"metadata":{},"cell_type":"markdown","source":"I am taking a **VERY SIMPLE** approach and using Ridge and Logistic regression."},{"metadata":{},"cell_type":"markdown","source":"I have also taken a look at <a href=\"https://bair.berkeley.edu/blog/2017/08/02/minibatch-metropolis-hastings/\">Bair's approach towards minibatch Metropolis.</a> Please take a look there if you wish to learn more."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('../input/liverpool-ion-switching/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Metropolis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_posterior(theta, X, p_mean, p_cov, x_var):\n    inv_cov = np.linalg.inv(p_cov)\n    determinant = np.prod(np.diagonal(p_cov))\n    prior = np.log(1. / (2*np.pi*np.sqrt(determinant)))\n\n    ll_constant = 1. / (np.sqrt(2 * np.pi * x_var))\n    L = 0.5 * np.exp(-(1./(2*x_var)) * (X - theta[0])**2) + \\\n        0.5 * np.exp(-(1./(2*x_var)) * (X - (theta[0]+theta[1]))**2)\n    L *= ll_constant\n    log_likelihood = np.sum(np.log(L))\n    \n    assert not np.isnan(prior + log_likelihood)\n    return np.squeeze(prior + log_likelihood)\n    \n\ndef get_noise(eps):\n    \"\"\" Returns a 2-D multivariate normal vector with covariance matrix diag(eps,eps). \"\"\"\n    return (np.random.multivariate_normal(np.array([0,0]), eps*np.eye(2))).reshape((2,1))\n\n\ndef get_info_for_contour(K=100):\n    \"\"\" For building the contour plots. \"\"\"\n    xlist = np.linspace(-1.5, 2.5, num=K)\n    ylist = np.linspace(-3, 3, num=K)\n    X_a,Y_a = np.meshgrid(xlist, ylist)\n    Z_a = np.zeros((K,K))\n    for i in range(K):\n        for j in range(K):\n            theta = np.array( [[X_a[i,j]],[Y_a[i,j]]] )\n            Z_a[i,j] = log_posterior(theta, X, prior_mean, prior_cov, sigmax_sq) \n    return X_a, Y_a, Z_a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.merge(df, test)\nx_train, y_train, x_test, y_test = train_test_split(df.head(2000000), test, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_run():\n    clf = LogisticRegression(random_state=0)\n    clf.fit(np.array(x_train.head(300000)['open_channels']).reshape(-1, 1), np.array(y_train.open_channels).reshape(-1, 1))\n    x = clf.predict(np.array(df['open_channels']).reshape(-1, 1))\n    x = np.array(x)\n    x = pd.DataFrame({'time': df.time, 'open_channels': x})\n    x.to_csv('log_sub.csv')\n    x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic_run()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBM"},{"metadata":{},"cell_type":"markdown","source":"## Credit to Rob Mulla"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport lightgbm as lgb\n#from tqdm.notebook import tqdm\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error, f1_score\nfrom datetime import datetime\nimport os\n\n###########\n# SETTINGS\n###########\n\n\nTARGET = 'open_channels'\n\nTOTAL_FOLDS = 5\nRANDOM_SEED = 529\nMODEL_TYPE = 'LGBM'\nLEARNING_RATE = 0.009\nSHUFFLE = True\nNUM_BOOST_ROUND = 500_000\nEARLY_STOPPING_ROUNDS = 100\nN_THREADS = -1\nOBJECTIVE = 'regression'\nMETRIC = 'rmse'\nNUM_LEAVES = 2**8+1\nMAX_DEPTH = -1\nFEATURE_FRACTION = 1\nBAGGING_FRACTION = 1\nBAGGING_FREQ = 0\n\n####################\n# READING IN FILES\n####################\n\ntrain = pd.read_csv('../input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('../input/liverpool-ion-switching/test.csv')\nss = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\ntrain['train'] = True\ntest['train'] = False\ntt = pd.concat([train, test], sort=False).reset_index(drop=True)\ntt['train'] = tt['train'].astype('bool')\n\n###########\n# TRACKING\n###########\n\nrun_id = \"{:%m%d_%H%M}\".format(datetime.now())\n\n\ndef update_tracking(\n        run_id, field, value, csv_file=\"./tracking.csv\",\n        integer=False, digits=None, nround=6,\n        drop_broken_runs=False):\n    \"\"\"\n    Tracking function for keep track of model parameters and\n    CV scores. `integer` forces the value to be an int.\n    \"\"\"\n    try:\n        df = pd.read_csv(csv_file, index_col=[0])\n    except:\n        df = pd.DataFrame()\n    if drop_broken_runs:\n        df = df.dropna(subset=['1_f1'])\n    if integer:\n        value = round(value)\n    elif digits is not None:\n        value = round(value, digits)\n    df.loc[run_id, field] = value  # Model number is index\n    df = df.round(nround)\n    df.to_csv(csv_file)\n\n\n# Update Tracking\n\nupdate_tracking(run_id, 'model_type', MODEL_TYPE)\nupdate_tracking(run_id, 'seed', RANDOM_SEED, integer=True)\nupdate_tracking(run_id, 'nfolds', TOTAL_FOLDS, integer=True)\nupdate_tracking(run_id, 'lr', LEARNING_RATE)\nupdate_tracking(run_id, 'shuffle', SHUFFLE)\nupdate_tracking(run_id, 'boost_rounds', NUM_BOOST_ROUND)\nupdate_tracking(run_id, 'es_rounds', EARLY_STOPPING_ROUNDS)\nupdate_tracking(run_id, 'threads', N_THREADS)\nupdate_tracking(run_id, 'objective', OBJECTIVE)\nupdate_tracking(run_id, 'metric', METRIC)\nupdate_tracking(run_id, 'num_leaves', NUM_LEAVES)\nupdate_tracking(run_id, 'max_depth', MAX_DEPTH)\nupdate_tracking(run_id, 'feature_fraction', FEATURE_FRACTION)\nupdate_tracking(run_id, 'bagging_fraction', BAGGING_FRACTION)\nupdate_tracking(run_id, 'bagging_freq', BAGGING_FREQ)\n\n###########\n# FEATURES\n###########\n\n# # Include batch\ntt = tt.sort_values(by=['time']).reset_index(drop=True)\ntt.index = ((tt.time * 10_000) - 1).values\ntt['batch'] = tt.index // 50_000\ntt['batch_index'] = tt.index - (tt.batch * 50_000)\ntt['batch_slices'] = tt['batch_index'] // 5_000\ntt['batch_slices2'] = tt.apply(lambda r: '_'.join(\n    [str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n\n# 50_000 Batch Features\ntt['signal_batch_min'] = tt.groupby('batch')['signal'].transform('min')\ntt['signal_batch_max'] = tt.groupby('batch')['signal'].transform('max')\ntt['signal_batch_std'] = tt.groupby('batch')['signal'].transform('std')\ntt['signal_batch_mean'] = tt.groupby('batch')['signal'].transform('mean')\ntt['mean_abs_chg_batch'] = tt.groupby(['batch'])['signal'].transform(\n    lambda x: np.mean(np.abs(np.diff(x))))\ntt['abs_max_batch'] = tt.groupby(\n    ['batch'])['signal'].transform(lambda x: np.max(np.abs(x)))\ntt['abs_min_batch'] = tt.groupby(\n    ['batch'])['signal'].transform(lambda x: np.min(np.abs(x)))\n\ntt['range_batch'] = tt['signal_batch_max'] - tt['signal_batch_min']\ntt['maxtomin_batch'] = tt['signal_batch_max'] / tt['signal_batch_min']\ntt['abs_avg_batch'] = (tt['abs_min_batch'] + tt['abs_max_batch']) / 2\n\n# 5_000 Batch Features\ntt['signal_batch_5k_min'] = tt.groupby(\n    'batch_slices2')['signal'].transform('min')\ntt['signal_batch_5k_max'] = tt.groupby(\n    'batch_slices2')['signal'].transform('max')\ntt['signal_batch_5k_std'] = tt.groupby(\n    'batch_slices2')['signal'].transform('std')\ntt['signal_batch_5k_mean'] = tt.groupby(\n    'batch_slices2')['signal'].transform('mean')\ntt['mean_abs_chg_batch_5k'] = tt.groupby(['batch_slices2'])[\n    'signal'].transform(lambda x: np.mean(np.abs(np.diff(x))))\ntt['abs_max_batch_5k'] = tt.groupby(['batch_slices2'])[\n    'signal'].transform(lambda x: np.max(np.abs(x)))\ntt['abs_min_batch_5k'] = tt.groupby(['batch_slices2'])[\n    'signal'].transform(lambda x: np.min(np.abs(x)))\n\ntt['range_batch_5k'] = tt['signal_batch_5k_max'] - tt['signal_batch_5k_min']\ntt['maxtomin_batch_5k'] = tt['signal_batch_5k_max'] / tt['signal_batch_5k_min']\ntt['abs_avg_batch_5k'] = (tt['abs_min_batch_5k'] + tt['abs_max_batch_5k']) / 2\n\n\n# add shifts\ntt['signal_shift+1'] = tt.groupby(['batch']).shift(1)['signal']\ntt['signal_shift-1'] = tt.groupby(['batch']).shift(-1)['signal']\ntt['signal_shift+2'] = tt.groupby(['batch']).shift(2)['signal']\ntt['signal_shift-2'] = tt.groupby(['batch']).shift(-2)['signal']\n\nfor c in ['signal_batch_min', 'signal_batch_max',\n          'signal_batch_std', 'signal_batch_mean',\n          'mean_abs_chg_batch', 'abs_max_batch',\n          'abs_min_batch',\n          'range_batch', 'maxtomin_batch', 'abs_avg_batch',\n          'signal_shift+1', 'signal_shift-1',\n          'signal_batch_5k_min', 'signal_batch_5k_max',\n          'signal_batch_5k_std',\n          'signal_batch_5k_mean', 'mean_abs_chg_batch_5k',\n          'abs_max_batch_5k', 'abs_min_batch_5k',\n          'range_batch_5k', 'maxtomin_batch_5k',\n          'abs_avg_batch_5k','signal_shift+2','signal_shift-2']:\n    tt[f'{c}_msignal'] = tt[c] - tt['signal']\n\n\n# FEATURES = [f for f in tt.columns if f not in ['open_channels','index','time','train','batch',\n#                                                'batch_index','batch_slices','batch_slices2']]\n\n\nFEATURES = ['signal',\n            'signal_batch_min',\n            'signal_batch_max',\n            'signal_batch_std',\n            'signal_batch_mean',\n            'mean_abs_chg_batch',\n            #'abs_max_batch',\n            #'abs_min_batch',\n            #'abs_avg_batch',\n            'range_batch',\n            'maxtomin_batch',\n            'signal_batch_5k_min',\n            'signal_batch_5k_max',\n            'signal_batch_5k_std',\n            'signal_batch_5k_mean',\n            'mean_abs_chg_batch_5k',\n            'abs_max_batch_5k',\n            'abs_min_batch_5k',\n            'range_batch_5k',\n            'maxtomin_batch_5k',\n            'abs_avg_batch_5k',\n            'signal_shift+1',\n            'signal_shift-1',\n            # 'signal_batch_min_msignal',\n            'signal_batch_max_msignal',\n            'signal_batch_std_msignal',\n            # 'signal_batch_mean_msignal',\n            'mean_abs_chg_batch_msignal',\n            'abs_max_batch_msignal',\n            'abs_min_batch_msignal',\n            'range_batch_msignal',\n            'maxtomin_batch_msignal',\n            'abs_avg_batch_msignal',\n            'signal_shift+1_msignal',\n            'signal_shift-1_msignal',\n            'signal_batch_5k_min_msignal',\n            'signal_batch_5k_max_msignal',\n            'signal_batch_5k_std_msignal',\n            'signal_batch_5k_mean_msignal',\n            'mean_abs_chg_batch_5k_msignal',\n            'abs_max_batch_5k_msignal',\n            'abs_min_batch_5k_msignal',\n            #'range_batch_5k_msignal',\n            'maxtomin_batch_5k_msignal',\n            'abs_avg_batch_5k_msignal',\n            'signal_shift+2',\n            'signal_shift-2']\n\nprint('....: FEATURE LIST :....')\nprint([f for f in FEATURES])\n\nupdate_tracking(run_id, 'n_features', len(FEATURES), integer=True)\nupdate_tracking(run_id, 'target', TARGET)\n\n###########\n# Metric\n###########\n\n\ndef lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    print(preds.shape)\n    print(preds)\n    preds = np.argmax(preds, axis=0)\n#     score = metrics.cohen_kappa_score(labels, preds, weights = 'quadratic')\n    score = f1_score(labels, preds, average='macro')\n    return ('KaggleMetric', score, True)\n\n\n###########\n# MODEL\n###########\ntt['train'] = tt['train'].astype('bool')\ntrain = tt.query('train').copy()\ntest = tt.query('not train').copy()\ntrain['open_channels'] = train['open_channels'].astype(int)\nX = train[FEATURES]\nX_test = test[FEATURES]\ny = train[TARGET].values\nsub = test[['time']].copy()\ngroups = train['batch']\n\nif OBJECTIVE == 'multiclass':\n    NUM_CLASS = 11\nelse:\n    NUM_CLASS = 1\n\n# define hyperparammeter (some random hyperparammeters)\nparams = {'learning_rate': LEARNING_RATE,\n          'max_depth': MAX_DEPTH,\n          'num_leaves': NUM_LEAVES,\n          'feature_fraction': FEATURE_FRACTION,\n          'bagging_fraction': BAGGING_FRACTION,\n          'bagging_freq': BAGGING_FREQ,\n          'n_jobs': N_THREADS,\n          'seed': RANDOM_SEED,\n          'metric': METRIC,\n          'objective': OBJECTIVE,\n          'num_class': NUM_CLASS\n          }\n\nkfold = KFold(n_splits=TOTAL_FOLDS, shuffle=SHUFFLE, random_state=RANDOM_SEED)\n\noof_df = train[['signal', 'open_channels']].copy()\nfi_df = pd.DataFrame(index=FEATURES)\n\nfold = 1  # init fold counter\nfor tr_idx, val_idx in kfold.split(X, y, groups=groups):\n    print(f'====== Fold {fold:0.0f} of {TOTAL_FOLDS} ======')\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y[tr_idx], y[val_idx]\n    train_set = lgb.Dataset(X_tr, y_tr)\n    val_set = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(params,\n                      train_set,\n                      num_boost_round=NUM_BOOST_ROUND,\n                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                      valid_sets=[train_set, val_set],\n                      verbose_eval=50)\n    # feval=lgb_Metric)\n\n    if OBJECTIVE == 'multi_class':\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        preds = np.argmax(preds, axis=1)\n        test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n        test_preds = np.argmax(test_preds, axis=1)\n    elif OBJECTIVE == 'regression':\n        preds = model.predict(X_val, num_iteration=model.best_iteration)\n        preds = np.round(np.clip(preds, 0, 10)).astype(int)\n        test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n        test_preds = np.round(np.clip(test_preds, 0, 10)).astype(int)\n\n    oof_df.loc[oof_df.iloc[val_idx].index, 'oof'] = preds\n    sub[f'open_channels_fold{fold}'] = test_preds\n\n    f1 = f1_score(oof_df.loc[oof_df.iloc[val_idx].index]['open_channels'],\n                  oof_df.loc[oof_df.iloc[val_idx].index]['oof'],\n                  average='macro')\n    rmse = np.sqrt(mean_squared_error(oof_df.loc[oof_df.index.isin(val_idx)]['open_channels'],\n                                      oof_df.loc[oof_df.index.isin(val_idx)]['oof']))\n\n    update_tracking(run_id, f'{fold}_best_iter', model.best_iteration, integer=True)\n    update_tracking(run_id, f'{fold}_rmse', rmse)\n    update_tracking(run_id, f'{fold}_f1', f1)\n    fi_df[f'importance_{fold}'] = model.feature_importance()\n    print(f'Fold {fold} - validation f1: {f1:0.5f}')\n    print(f'Fold {fold} - validation rmse: {rmse:0.5f}')\n\n    fold += 1\n\noof_f1 = f1_score(oof_df['open_channels'],\n                  oof_df['oof'],\n                  average='macro')\noof_rmse = np.sqrt(mean_squared_error(oof_df['open_channels'],\n                                      oof_df['oof']))\n\nupdate_tracking(run_id, f'oof_f1', oof_f1)\nupdate_tracking(run_id, f'oof_rmse', oof_rmse)\n\n###############\n# SAVE RESULTS\n###############\n\ns_cols = [s for s in sub.columns if 'open_channels' in s]\n\nsub['open_channels'] = sub[s_cols].median(axis=1).astype(int)\nsub.to_csv(f'./pred_x_{oof_f1:0.6}.csv', index=False)\nsub[['time', 'open_channels']].to_csv(f'./sub_x_{oof_f1:0.10f}.csv',\n                                      index=False,\n                                      float_format='%0.4f')\n\noof_df.to_csv(f'./oof_x_{oof_f1:0.6}.csv', index=False)\n\nfi_df['importance'] = fi_df.sum(axis=1)\nfi_df.to_csv(f'./fi_x_{oof_f1:0.6}.csv', index=True)\n\n\nfig, ax = plt.subplots(figsize=(15, 30))\nfi_df.sort_values('importance')['importance'] \\\n    .plot(kind='barh',\n          figsize=(15, 30),\n          title=f'x - Feature Importance',\n          ax=ax)\nplt.savefig(f'./x__{oof_f1:0.6}.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # **2. On Hamiltonian Monte Carlo**"},{"metadata":{},"cell_type":"markdown","source":"**Hamiltonian Monte Carlo** can produce distant proposals for the **Metropolis Algorithm** (an algorithm which obtains random samples from a distribution where direct sampling is difficult).\n\n**Hamiltonian Monte Carlo** avoids random walk behaviour by bringing in Hamiltonian Mechanics (where a system is defined by  a set of coordinates in space).\n\n---\n\n## Further results\n\nAfter experimentation, I have found out that it is not the result of a HMC. However, I am still experimenting on Metroplis algorithm for this dataset."},{"metadata":{},"cell_type":"markdown","source":"## Implementation\n\nWe can implement **Hamiltonian Monte Carlo** with `pyhmc`."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyhmc --quiet\n!pip install triangle --quiet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pyhmc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As Markus proved, the data is simply the result of a Markov process with Gaussian noise."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\n\ntrain_time   = df_train[\"time\"].values.reshape(-1,500000)\ntrain_signal = df_train[\"signal\"].values.reshape(-1,500000)\ntrain_opench = df_train[\"open_channels\"].values.reshape(-1,500000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Why Hamiltonian Monte Carlo?"},{"metadata":{},"cell_type":"markdown","source":"One might ask, \"why HMC instead of a regular Markov chain?\". HMC is built **not** to facilitate random walk behaviours."},{"metadata":{"trusted":true},"cell_type":"code","source":"def markov_p(data):\n    channel_range = np.unique(data)\n    channel_bins = np.append(channel_range, 11)\n    data_next = np.roll(data, -1)\n    matrix = []\n    for i in channel_range:\n        current_row = np.histogram(data_next[data == i], bins=channel_bins)[0]\n        current_row = current_row / np.sum(current_row)\n        matrix.append(current_row)\n    return np.array(matrix)\np03 = markov_p(train_opench[3])\np04 = markov_p(train_opench[4])\n\neig_values, eig_vectors = np.linalg.eig(np.transpose(p03))\nprint(\"Eigenvalues :\", eig_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. To really prove whether it's a HMC, we must identify the autocorrelation. \n2. How will this help?"},{"metadata":{"trusted":true},"cell_type":"code","source":"p03 = p03.flatten()\np03\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Auto correlation of HMC is:\", pd.Series(p03).autocorr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(p03).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(p03).plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(p03)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_axes_grid(numplots_x, numplots_y, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    return fig, axes\n    \ndef set_axes(axes, use_grid=True, x_val = [0,100,10,5], y_val = [-50,50,10,5]):\n    axes.grid(use_grid)\n    axes.tick_params(which='both', direction='inout', top=True, right=True, labelbottom=True, labelleft=True)\n    axes.set_xlim(x_val[0], x_val[1])\n    axes.set_ylim(y_val[0], y_val[1])\n    axes.set_xticks(np.linspace(x_val[0], x_val[1], np.around((x_val[1] - x_val[0]) / x_val[2] + 1).astype(int)))\n    axes.set_xticks(np.linspace(x_val[0], x_val[1], np.around((x_val[1] - x_val[0]) / x_val[3] + 1).astype(int)), minor=True)\n    axes.set_yticks(np.linspace(y_val[0], y_val[1], np.around((y_val[1] - y_val[0]) / y_val[2] + 1).astype(int)))\n    axes.set_yticks(np.linspace(y_val[0], y_val[1], np.around((y_val[1] - y_val[0]) / y_val[3] + 1).astype(int)), minor=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = create_axes_grid(1,1,10,10)\nset_axes(axes, x_val=[-4,2,1,.1], y_val=[-4,2,1,.1])\n\naxes.set_aspect('equal')\ndata = train_signal[4]\nplt.scatter(np.roll(data, -1), data)\nplt.savefig(\"results.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get rather odd results from the scatter plot."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_signal[4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob = [.3, .3, .2, .1, .05, .05]\ndata = train_signal[4][1:7]\nlen(data)\nstat = []\nfor i in range(1, 1000):\n\n    # Choose random inputs for the sales targets and percent to target\n    mc_signal = np.random.choice(data, 500, p=prob)\n\n    # Build the dataframe based on the inputs and number of reps\n    df = pd.DataFrame(index=range(500), data={'mc_signal': mc_signal,})\n\n    # We want to track sales,commission amounts and sales targets over all the simulations\n    stat.append([df['mc_signal'].sum().round(0)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df.head(11))\narsig = np.array(df.mc_signal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import kstest as ks\nks(arsig, 'norm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import kstest as ks\nks(p03, 'norm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seems like our Monte Carlo predicted value and our p03 array are not similar. So, it seems Monte Carlo is not so reliable after all. \n\nSo, it seems we do not have a Hamiltonian Markov Chain, because this is obviously random whereas a HMC is built not to become random."},{"metadata":{},"cell_type":"markdown","source":"Some resources to help you in this competition:\n\n* What is Drift? :- [Chris Deotte](https://www.kaggle.com/c/liverpool-ion-switching/discussion/137537)\n* Viterbi Algorithm explained:- [Markus F.](https://www.kaggle.com/c/liverpool-ion-switching/discussion/137388)\n* What a Markov chain actually is:- [Roman](https://www.kaggle.com/c/liverpool-ion-switching/discussion/137366)"},{"metadata":{},"cell_type":"markdown","source":"## Credits: Markus F."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}