{"cells":[{"metadata":{"id":"huixWaL42uZi"},"cell_type":"markdown","source":"The validation scheme is based on [seq2seq-rnn-with-gru](https://www.kaggle.com/brandenkmurray/seq2seq-rnn-with-gru/output), and cleaned data is from [data-without-drift](https://www.kaggle.com/cdeotte/data-without-drift) and Kalman filter is from [https://www.kaggle.com/teejmahal20/single-model-lgbm-kalman-filter](single-model-lgbm-kalman-filter) and the added feature is from [wavenet-with-1-more-feature](wavenet-with-1-more-feature). I also used ragnar's data in this version [clean-kalman](https://www.kaggle.com/ragnar123/clean-kalman). The Wavenet is based on [https://github.com/philipperemy/keras-tcn](https://github.com/philipperemy/keras-tcn), [https://github.com/peustr/wavenet](https://github.com/peustr/wavenet) and [https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet) and also [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm). If any refrence is not mentioned it was not intentional, please add them in comments.\n\nPrevious versions were mainly based on [https://www.kaggle.com/wimwim/wavenet-lstm](https://www.kaggle.com/wimwim/wavenet-lstm)  "},{"metadata":{"trusted":true,"_kg_hide-input":true,"id":"LqmWjeYJ2uZn"},"cell_type":"code","source":"!pip install --no-warn-conflicts -q tensorflow-addons","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"y1qOuodBfSxN","trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,\n                                     RepeatVector, Conv1D, MaxPooling1D, Concatenate, GlobalAveragePooling1D, UpSampling1D)\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.utils import Sequence, to_categorical\nfrom tensorflow.keras import losses, models, optimizers\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.metrics import f1_score, cohen_kappa_score, mean_squared_error\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom tqdm import tqdm_notebook as tqdm\nfrom contextlib import contextmanager\nfrom joblib import Parallel, delayed\nfrom IPython.display import display\nfrom sklearn import preprocessing\nimport tensorflow_addons as tfa\nimport scipy.stats as stats\nimport random as rn\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport itertools\nimport warnings\nimport time\nimport pywt\nimport os\nimport gc\n\n\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"UafJMtyefSxU","trusted":true},"cell_type":"code","source":"EPOCHS=120\nNNBATCHSIZE=20\nBATCHSIZE = 4000\nSEED = 321\nSELECT = True\nSPLITS = 5\nLR = 0.001\nfe_config = [\n    (True, 4000),\n]","execution_count":null,"outputs":[]},{"metadata":{"id":"EE4v8h1tfSxb","trusted":true},"cell_type":"code","source":"\ndef init_logger():\n    handler = StreamHandler()\n    handler.setLevel(INFO)\n    handler.setFormatter(Formatter(LOGFORMAT))\n    fh_handler = FileHandler('{}.log'.format(MODELNAME))\n    fh_handler.setFormatter(Formatter(LOGFORMAT))\n    logger.setLevel(INFO)\n    logger.addHandler(handler)\n    logger.addHandler(fh_handler)\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"7nxcdN_5fSxo","trusted":true},"cell_type":"code","source":"\n@contextmanager\ndef timer(name : Text):\n    t0 = time.time()\n    yield\n    logger.info(f'[{name}] done in {time.time() - t0:.0f} s')\n\nCOMPETITION = 'ION-Switching'\nlogger = getLogger(COMPETITION)\nLOGFORMAT = '%(asctime)s %(levelname)s %(message)s'\nMODELNAME = 'WaveNet'\n","execution_count":null,"outputs":[]},{"metadata":{"id":"OC5DOcDifSxx","trusted":true},"cell_type":"code","source":"\ndef seed_everything(seed : int) -> NoReturn :\n    \n    rn.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(SEED)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"adUHGQUTfSyA","trusted":true},"cell_type":"code","source":"\ndef read_data(base : os.path.abspath) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \n    train = pd.read_csv('/kaggle/input/clean-kalman/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n    test  = pd.read_csv('/kaggle/input/clean-kalman/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n    sub  = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv', dtype={'time': np.float32})\n    \n    return train, test, sub\n","execution_count":null,"outputs":[]},{"metadata":{"id":"HpDaJQ5yfSyI","trusted":true},"cell_type":"code","source":"\ndef batching(df : pd.DataFrame,\n             batch_size : int) -> pd.DataFrame :\n    \n    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n    df['group'] = df['group'].astype(np.uint16)\n        \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"id":"iQxOYF3tfSyj","trusted":true},"cell_type":"code","source":"\ndef reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n\n                if (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    reduction = (start_mem - end_mem) / start_mem\n\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"id":"dytmNmL_fSzj","trusted":true},"cell_type":"code","source":"\ndef lag_with_pct_change(df : pd.DataFrame,\n                        shift_sizes : Optional[List]=[1, 2],\n                        add_pct_change : Optional[bool]=False,\n                        add_pct_change_lag : Optional[bool]=False) -> pd.DataFrame:\n    \n    for shift_size in shift_sizes:    \n        df['signal_shift_pos_'+str(shift_size)] = df.groupby('group')['signal'].shift(shift_size).fillna(0)\n        df['signal_shift_neg_'+str(shift_size)] = df.groupby('group')['signal'].shift(-1*shift_size).fillna(0)\n\n    if add_pct_change:\n        df['pct_change'] = df['signal'].pct_change()\n        if add_pct_change_lag:\n            for shift_size in shift_sizes:    \n                df['pct_change_shift_pos_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(shift_size).fillna(0)\n                df['pct_change_shift_neg_'+str(shift_size)] = df.groupby('group')['pct_change'].shift(-1*shift_size).fillna(0)\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"id":"m-ULWLF_fS0B","trusted":true},"cell_type":"code","source":"\ndef run_feat_enginnering(df : pd.DataFrame,\n                         create_all_data_feats : bool,\n                         batch_size : int) -> pd.DataFrame:\n    \n    df = batching(df, batch_size=batch_size)\n    if create_all_data_feats:\n        df = lag_with_pct_change(df, [1, 2, 3],  add_pct_change=False, add_pct_change_lag=False)\n    df['signal_2'] = df['signal'] ** 2\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"id":"6E87jcu2fS0O","trusted":true},"cell_type":"code","source":"def feature_selection(df : pd.DataFrame,\n                      df_test : pd.DataFrame) -> Tuple[pd.DataFrame , pd.DataFrame, List]:\n    use_cols = [col for col in df.columns if col not in ['index','group', 'open_channels', 'time']]\n    df = df.replace([np.inf, -np.inf], np.nan)\n    df_test = df_test.replace([np.inf, -np.inf], np.nan)\n    for col in use_cols:\n        col_mean = pd.concat([df[col], df_test[col]], axis=0).mean()\n        df[col] = df[col].fillna(col_mean)\n        df_test[col] = df_test[col].fillna(col_mean)\n   \n    gc.collect()\n    return df, df_test, use_cols\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef augment(X: np.array, y:np.array) -> Tuple[np.array, np.array]:\n    \n    X = np.vstack((X, np.flip(X, axis=1)))\n    y = np.vstack((y, np.flip(y, axis=1)))\n    \n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"id":"e4QulGxHfS0n","trusted":true},"cell_type":"code","source":"\ndef run_cv_model_by_batch(train : pd.DataFrame,\n                          test : pd.DataFrame,\n                          splits : int,\n                          batch_col : Text,\n                          feats : List,\n                          sample_submission: pd.DataFrame,\n                          nn_epochs : int,\n                          nn_batch_size : int) -> NoReturn:\n    seed_everything(SEED)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11))\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    group = train['group']\n    kf = GroupKFold(n_splits=5)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n        \n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['group']]], axis=1)\n\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['group']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n    train = np.array(list(train.groupby('group').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('group').apply(lambda x: x[feats].values)))\n\n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        \n        if n_fold < 2:\n            train_x, train_y = augment(train_x, train_y)\n\n        gc.collect()\n        shape_ = (None, train_x.shape[2])\n        model = Classifier(shape_)\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n        cb_prg = tfa.callbacks.TQDMProgressBar(leave_epoch_progress=False,leave_overall_progress=False, show_epoch_progress=False,show_overall_progress=True)\n        model.fit(train_x,train_y,\n                  epochs=nn_epochs,\n                  callbacks=[cb_prg, cb_lr_schedule, MacroF1(model, valid_x,valid_y)],\n                  batch_size=nn_batch_size,verbose=0,\n                  validation_data=(valid_x,valid_y))\n        preds_f = model.predict(valid_x)\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n        logger.info(f'Training fold {n_fold + 1} completed. macro f1 score : {f1_score_ :1.5f}')\n        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n        oof_[val_orig_idx,:] += preds_f\n        te_preds = model.predict(test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds / SPLITS\n    f1_score_ =f1_score(np.argmax(train_tr, axis=2).reshape(-1),  np.argmax(oof_, axis=1), average = 'macro')\n    logger.info(f'Training completed. oof macro f1 score : {f1_score_:1.5f}')\n    sample_submission['open_channels'] = np.argmax(preds_, axis=1).astype(int)\n    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n    display(sample_submission.head())\n    np.save('oof.npy', oof_)\n    np.save('preds.npy', preds_)\n\n    return \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"dqt0VJAS2ucB"},"cell_type":"code","source":"def lr_schedule(epoch):\n    if epoch < 40:\n        lr = LR\n    elif epoch < 50:\n        lr = LR / 3\n    elif epoch < 60:\n        lr = LR / 6\n    elif epoch < 75:\n        lr = LR / 9\n    elif epoch < 85:\n        lr = LR / 12\n    elif epoch < 100:\n        lr = LR / 15\n    else:\n        lr = LR / 50\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"SjbrEQSa2ucN"},"cell_type":"code","source":"class Mish(tf.keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(Mish, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def call(self, inputs):\n        return inputs * K.tanh(K.softplus(inputs))\n\n    def get_config(self):\n        base_config = super(Mish, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\ndef mish(x):\n\treturn tf.keras.layers.Lambda(lambda x: x*K.tanh(K.softplus(x)))(x)\n \nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\nget_custom_objects().update({'mish': Activation(mish)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"dM8RiNfL2ucY"},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\n\nclass Attention(Layer):\n    \"\"\"Multi-headed attention layer.\"\"\"\n    \n    def __init__(self, hidden_size, \n                 num_heads = 8, \n                 attention_dropout=.1,\n                 trainable=True,\n                 name='Attention'):\n        \n        if hidden_size % num_heads != 0:\n            raise ValueError(\"Hidden size must be evenly divisible by the number of heads.\")\n            \n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.trainable = trainable\n        self.attention_dropout = attention_dropout\n        self.dense = tf.keras.layers.Dense(self.hidden_size, use_bias=False)\n        super(Attention, self).__init__(name=name)\n\n    def split_heads(self, x):\n        \"\"\"Split x into different heads, and transpose the resulting value.\n        The tensor is transposed to insure the inner dimensions hold the correct\n        values during the matrix multiplication.\n        Args:\n          x: A tensor with shape [batch_size, length, hidden_size]\n        Returns:\n          A tensor with shape [batch_size, num_heads, length, hidden_size/num_heads]\n        \"\"\"\n        with tf.name_scope(\"split_heads\"):\n            batch_size = tf.shape(x)[0]\n            length = tf.shape(x)[1]\n\n            # Calculate depth of last dimension after it has been split.\n            depth = (self.hidden_size // self.num_heads)\n\n            # Split the last dimension\n            x = tf.reshape(x, [batch_size, length, self.num_heads, depth])\n\n            # Transpose the result\n            return tf.transpose(x, [0, 2, 1, 3])\n    \n    def combine_heads(self, x):\n        \"\"\"Combine tensor that has been split.\n        Args:\n          x: A tensor [batch_size, num_heads, length, hidden_size/num_heads]\n        Returns:\n          A tensor with shape [batch_size, length, hidden_size]\n        \"\"\"\n        with tf.name_scope(\"combine_heads\"):\n            batch_size = tf.shape(x)[0]\n            length = tf.shape(x)[2]\n            x = tf.transpose(x, [0, 2, 1, 3])  # --> [batch, length, num_heads, depth]\n            return tf.reshape(x, [batch_size, length, self.hidden_size])        \n\n    def call(self, inputs):\n        \"\"\"Apply attention mechanism to inputs.\n        Args:\n          inputs: a tensor with shape [batch_size, length_x, hidden_size]\n        Returns:\n          Attention layer output with shape [batch_size, length_x, hidden_size]\n        \"\"\"\n        # Google developper use tf.layer.Dense to linearly project the queries, keys, and values.\n        q = self.dense(inputs)\n        k = self.dense(inputs)\n        v = self.dense(inputs)\n\n        q = self.split_heads(q)\n        k = self.split_heads(k)\n        v = self.split_heads(v)\n        \n        # Scale q to prevent the dot product between q and k from growing too large.\n        depth = (self.hidden_size // self.num_heads)\n        q *= depth ** -0.5\n        \n        logits = tf.matmul(q, k, transpose_b=True)\n        # logits += self.bias\n        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n        \n        if self.trainable:\n            weights = tf.nn.dropout(weights, 1.0 - self.attention_dropout)\n        \n        attention_output = tf.matmul(weights, v)\n        attention_output = self.combine_heads(attention_output)\n        attention_output = self.dense(attention_output)\n        return attention_output\n        \n    def compute_output_shape(self, input_shape):\n        return tf.TensorShape(input_shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"bG4cGcYYNVYw","trusted":true},"cell_type":"code","source":"def categorical_focal_loss(gamma=2.0, alpha=0.25):\n    \"\"\"\n    Implementation of Focal Loss from the paper in multiclass classification\n    Formula:\n        loss = -alpha*((1-p)^gamma)*log(p)\n    Parameters:\n        alpha -- the same as wighting factor in balanced cross entropy\n        gamma -- focusing parameter for modulating factor (1-p)\n    Default value:\n        gamma -- 2.0 as mentioned in the paper\n        alpha -- 0.25 as mentioned in the paper\n    \"\"\"\n    def focal_loss(y_true, y_pred):\n        # Define epsilon so that the backpropagation will not result in NaN\n        # for 0 divisor case\n        epsilon = K.epsilon()\n        # Add the epsilon to prediction value\n        #y_pred = y_pred + epsilon\n        # Clip the prediction value\n        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n        # Calculate cross entropy\n        cross_entropy = -y_true*K.log(y_pred)\n        # Calculate weight that consists of  modulating factor and weighting factor\n        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n        # Calculate focal loss\n        loss = weight * cross_entropy\n        # Sum the losses in mini_batch\n        loss = K.sum(loss, axis=1)\n        return loss\n    \n    return focal_loss","execution_count":null,"outputs":[]},{"metadata":{"id":"YSESlpaNwkkx","trusted":true},"cell_type":"code","source":"def WaveNetResidualConv1D(num_filters, kernel_size, stacked_layer):\n\n    def build_residual_block(l_input):\n        resid_input = l_input\n        for dilation_rate in [2**i for i in range(stacked_layer)]:\n            l_sigmoid_conv1d = Conv1D(\n              num_filters, kernel_size, dilation_rate=dilation_rate,\n              padding='same', activation='sigmoid')(l_input)\n            l_tanh_conv1d = Conv1D(\n             num_filters, kernel_size, dilation_rate=dilation_rate,\n             padding='same', activation='mish')(l_input)\n            l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n            l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n            resid_input = Add()([resid_input ,l_input])\n        return resid_input\n    return build_residual_block\ndef Classifier(shape_):\n    num_filters_ = 16\n    kernel_size_ = 3\n    stacked_layers_ = [12, 8, 4, 1]\n    l_input = Input(shape=(shape_))\n    x = Conv1D(num_filters_, 1, padding='same')(l_input)\n    x = WaveNetResidualConv1D(num_filters_, kernel_size_, stacked_layers_[0])(x)\n    x = Conv1D(num_filters_*2, 1, padding='same')(x)\n    x = WaveNetResidualConv1D(num_filters_*2, kernel_size_, stacked_layers_[1])(x)\n    x = Conv1D(num_filters_*4, 1, padding='same')(x)\n    x = WaveNetResidualConv1D(num_filters_*4, kernel_size_, stacked_layers_[2])(x)\n    x = Conv1D(num_filters_*8, 1, padding='same')(x)\n    x = WaveNetResidualConv1D(num_filters_*8, kernel_size_, stacked_layers_[3])(x)\n    l_output = Dense(11, activation='softmax')(x)\n    model = models.Model(inputs=[l_input], outputs=[l_output])\n    opt = Adam(lr=LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"9Cckpy4I2uco"},"cell_type":"code","source":"def Classifierx(shape_):        \n    \n    inp = Input(shape=(shape_))\n    x = Bidirectional(GRU(256, return_sequences=True))(inp)\n    x = Attention(512)(x)\n    x = TimeDistributed(Dense(256, activation='mish'))(x)\n    x = TimeDistributed(Dense(128, activation='mish'))(x)\n    out = TimeDistributed(Dense(11, activation='softmax', name='out'))(x)\n    \n    model = models.Model(inputs=inp, outputs=out) \n    \n    opt = Adam(lr=LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(loss=losses.CategoricalCrossentropy(), optimizer=opt, metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"1yfpldH_2ucw"},"cell_type":"code","source":"class MacroF1(Callback):\n    def __init__(self, model, inputs, targets):\n        self.model = model\n        self.inputs = inputs\n        self.targets = np.argmax(targets, axis=2).reshape(-1)\n\n    def on_epoch_end(self, epoch, logs):\n        pred = np.argmax(self.model.predict(self.inputs), axis=2).reshape(-1)\n        score = f1_score(self.targets, pred, average=\"macro\")\n        print(f' F1Macro: {score:.5f}')    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"skru4lPt2uc6"},"cell_type":"code","source":"def normalize(train, test):\n    \n    train_input_mean = train.signal.mean()\n    train_input_sigma = train.signal.std()\n    train['signal'] = (train.signal-train_input_mean)/train_input_sigma\n    test['signal'] = (test.signal-train_input_mean)/train_input_sigma\n\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"id":"02TDJtejfS0_","trusted":true},"cell_type":"code","source":"\ndef run_everything(fe_config : List) -> NoReturn:\n    not_feats_cols = ['time']\n    target_col = ['open_channels']\n    init_logger()\n    with timer(f'Reading Data'):\n        logger.info('Reading Data Started ...')\n        base = os.path.abspath('/kaggle/input/liverpool-ion-switching/')\n        train, test, sample_submission = read_data(base)\n        train, test = normalize(train, test)    \n        logger.info('Reading and Normalizing Data Completed ...')\n    with timer(f'Creating Features'):\n        logger.info('Feature Enginnering Started ...')\n        for config in fe_config:\n            train = run_feat_enginnering(train, create_all_data_feats=config[0], batch_size=config[1])\n            test  = run_feat_enginnering(test,  create_all_data_feats=config[0], batch_size=config[1])\n        train, test, feats = feature_selection(train, test)\n        logger.info('Feature Enginnering Completed ...')\n\n    with timer(f'Running Wavenet model'):\n        logger.info(f'Training Wavenet model with {SPLITS} folds of GroupKFold Started ...')\n        run_cv_model_by_batch(train, test, splits=SPLITS, batch_col='group', feats=feats, sample_submission=sample_submission, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)\n        logger.info(f'Training completed ...')\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_w4_OJJtfS1K","outputId":"7ea711e0-3974-419e-85fc-af53090561df","trusted":true},"cell_type":"code","source":"run_everything(fe_config)","execution_count":null,"outputs":[]},{"metadata":{"id":"m43NcOnF7Jl8","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"WaveNet_Keras_(4) (1).ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}