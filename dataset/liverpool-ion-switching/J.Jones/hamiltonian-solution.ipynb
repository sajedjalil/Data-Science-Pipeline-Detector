{"cells":[{"metadata":{},"cell_type":"raw","source":"## Derive the equations of motion y(t) and y'(t)\n\nThis problem might be solved using optimal control mathematics. We want to control a detector of the system's output states (how many open channels) so that the f1 score of the detector is maximized. The Kalman filter is based on this approach yet it has a slightly different measure of performance **J(x)**.\n\nThe Hamiltonian equation offers a solution. The solution of the minumized partial derivative of the Hamiltonian is the solution for this problem. It tries to provide an estimate of \"open_channels\" at the highest possible f1 score. This requires that our performance score is diffenciable on both time and x.\n\n![A System Model](https://www.elmtreegarden.com/wp-content/uploads/2020/05/ion-channel-system-model-2.jpg)\n\n## The HJB equation, now called the Hamiltonian\n\nIn this case the Hamiltonian is the energy equation of the system. **H = T - V** = kinetic energy - potential energy. And we know energy is conserved.\n\nConsider charge q. Electric current is movement(change of) of charge over time. The current i(t) = dq / dt (C/s Coulomns/second). Since charge is conserved at the node where probe, injected current and the signal are connected, then injected charge, qi = qs + qp (sum of signal charge, qs, and probe charge qp). And the charge moving through the probe is steady state charge qq (quiet when no channels are open) minus the charge of ions moving through the probe q_ions. If r1=r2 then the ion charge splits evenly into the signal current and into the signal generator.\n\nqp = qq - q_ions = qq - y * q_ion.   <br>\nwhere y is number of ions (open channels) and q_ion is the charge of the ion. Then by substitution into our conservation of charge formula, we get:\n\nqi = qs + qq - y * q_ion\n\nqs = qi - qq + y * q_ion\n\ny * q_ion = qs + qq - qi\n\n\nGiven that a unit of charge (charge of electron) in coulombs: 1.602176634√ó10‚àí19 C. A single open channel represents a curent dq_ion / dt. So:\n\nThe current indicating one open channel  = one open channel = 1.602176634√ó10‚àí19 C / 0.0001 s = 1.602 x 10^-15 A = 0.0016 pA<br>\nk = .0016 pC/ion passage. This amount of charge must pass through probe every 0.0001s to be measured as an open channel.\n\nThis means that the current that indicates one open channel change = dq/dt = .00001602 pC/ion / 0.0001s = 0.0016pA\n\n\nqs = qi - qq + k y   y = {0,1,2,,,10,,,} (why must 10 be limit?)\n\ntake derivative with time\n\ndqs/dt = dqi/dt - dqq/dt + k dy/dt  and rearrange\n\ny'(t) = 1/k ( dqs/dt - dqi/dt + dqq/dt )\n\ny'(t) = 1/k * (signal_current(t) - injected_current(t) + quiet_probe_current(t))  **[1]**\n\nBut we really only know signal current. But we can make a few assumptions about resistors r, r1 and r2 in the model diagram above. The injection source (assumed to be signal generator) may be about 50 Ohms which is a typical value for such equipment. I'll also assume that the probe cable (which feeds an amplifier) will have a similar resistance for good coupling to the probe. Assume r2 = r1 = 50 Ohms. To find r, I will manually seek a value r that agrees with the data.\n\nThe voltage at the node = r1 injected_current = r2 signal current so:<br>\ninjected current = r2/r1 signal current\n\nand  r (quiet_probe current - k dy/dt) = r2 signal current  so:<br>\nquiet_probe current = r2/r (signal current )\n\ny'(t) = 1/k ( signal_current(t) - r2/r1 signal_current(t) + r2/r (signal current)\n\nI assume r2 = r1</br>\n\ny'(t) = 1/k  r2/r signal_current(t)    **[2]**\n\n \n\n![The Hamiltonian](https://www.elmtreegarden.com/wp-content/uploads/2020/05/pontryagin.jpg)\n\nLet,\n\nx0(t) = signal(t) which has been comb filtered to remove noise<br>\nx1(t) = energy of the x0 current = r x0(t)^2 * dt<br>\nx2(t) = injected energy to probe = a1 * x1(t-Ta:t).min() * dt<br>\ny(t) = open_channels<br>\n\nThe energy of open channels is k * open_channels * dt. This energy is the energy required to transport the ion through the electric field in the probe. \n\n### to conserve enery we know that total energy is zero, so\n\n    0 = Signal Energy + Injected Energy - energy of transition\n    \n    0 = x1(t) dt + x2(t) dt - k y'(t)\n    y'(t) = 1/k * (x1(t) dt + x2(t) dt) \n    y'(t) = 1/k * ( x1(t) + a1 * x1(t-Ta:t).min)) * dt\n    y'(t) = 1/k * ( r x0(t)^2 + a1 r x0(t-Ta:t)^2.min()) * dt\n\nSimplify x0(t) = x(t)\n    \n    y'(t) = dy(t)/dt = r/k ( x(t)^2 + a1 x(t-Ta:t)^2.min() ) * dt\n    \nfinally we get   \n \n### g(x,u,t) = y'(t) = r/k (x(t)^2 + a1 x(t-Ta:t)^2.min() + u(t)) * dt\n\n\n\n### Measure of performance\n\nThe performance function, **J(t)** , is the f_1 score, f_1(y, y_true) over time period T. It is a measure of energy. Unfortunately the f1 score only measures an ensemble's score (over a period of time). There is no microscopic f1_score. That is to say the f1_score needs more than a single pair(y_est,y_true) It is not differentiable with time: there is no function f = d(f1_score)/dt.\n             \n\n### Initial conditions\nInitial conditions will be assumed are<br>\nX0(0) = X0_dot(0) = 0<br>\ny'(0) = 0\n\nReferences:\n1. Kirk, \"Optimal Control Theory\", 1970 Chapters 1-3<br>\n2. [E.T. Jaynes: Minimum Entropy Principle](https://pdfs.semanticscholar.org/b326/6b25cb2ff34634aff48434652bacb3fede9c.pdf)\n3. [Toward Data Science Blog](https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d)\n4. Hamill, \"A Student's Guide to Lagrangians and Hamiltonians\", Cambridge Press (Nice concise and modern!)\n5. This community of programmers, Kaggle, for much of this code. \n6. KitchinGroup, Using Lagrangian Multipliers [source](http://kitchingroup.cheme.cmu.edu/blog/2013/02/03/Using-Lagrange-multipliers-in-optimization/)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom numpy.fft import *\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport os.path as path\nimport scipy as sp\nimport scipy.fftpack\nfrom scipy import signal\nfrom pykalman import KalmanFilter\nfrom sklearn import tree\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport gc\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n\nDATA_PATH = \"../input/liverpool-ion-switching\"\n\nx = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n#test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n#submission_df = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference: http://kitchingroup.cheme.cmu.edu/blog/2013/02/03/Using-Lagrange-multipliers-in-optimization/\n# Calculate Hamiltonian\ndef func(X):\n    x = X[0]\n    y = X[1]\n    L = X[2] \n    return x + y + L * (x**2 + k * y)\n\n# dirvitive of Hamiltionian with respect to x\ndef dfunc(X):\n    dL = np.zeros(len(X))\n    d = 1e-4 \n    for i in range(len(X)):\n        dX = np.zeros(len(X))\n        dX[i] = d\n        dL[i] = (func(X+dX)-func(X-dX))/(2*d);\n    return dL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ion_plotter(x, examples):\n\n    fig, ax = plt.subplots(nrows=len(examples), ncols=1, figsize=(25, 3.5*len(examples)))\n    fig.subplots_adjust(hspace = .5)\n    ax = ax.ravel()\n    colors = plt.rcParams[\"axes.prop_cycle\"]()\n\n    for i in range(len(examples)):\n    \n        c = next(colors)[\"color\"]\n        ax[i].grid()\n        if examples[i] in ['signal_f','signal','message_current','injected_current']:\n            ax[i].plot(x['time'], x[examples[i]],color=c, linewidth= 2)\n            ax[i].set_ylabel('current (pA)', fontsize=14)\n        if examples[i] in ['y_var','y_var_est','y_var_k']:\n            ax[i].scatter(x['time'][::100], x[examples[i]][::100],marker ='.', color=c, linewidth=0)\n            ax[i].set_ylabel('Open Channels', fontsize=14)\n            ax[i].set_ylim(-5,5)\n        if examples[i] in ['y_mean_est','y_mean','y_pred','y_mean_k']:\n            ax[i].scatter(x['time'][::100], (x[examples[i]][::100]),marker ='.', color=c, linewidth=0)\n            ax[i].set_ylabel('Open Channels', fontsize=14)\n            ax[i].set_ylim(0,10)\n        if examples[i] in ['y','y_est','y_est_k']:\n            ax[i].scatter(x['time'][::100], (x[examples[i]][::100]),marker ='.', color=c, linewidth=0)\n            ax[i].set_ylabel('Open Channels', fontsize=14)\n            ax[i].set_ylim(0,10)\n        if examples[i] in ['signal_energy','injected_energy','message_energy']:\n            ax[i].plot(x['time'], x[examples[i]],color=c, linewidth= 1)\n            ax[i].set_ylabel('Energy 10^-24 Joules', fontsize=14)                     \n        ax[i].plot(x['time'], x[examples[i]],color=c, linewidth=.5)\n        ax[i].set_title(examples[i], fontsize=24)\n        ax[i].set_xlabel('Time (seconds)', fontsize=14)\n        #ax[i].set_ylabel('current (pA)', fontsize=24)\n        #ax[i].set_ylim(0,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.rename(columns = {'open_channels':'y'})\n\n# Find quiet_probe_current  when y = 0\nx['qs'] = x['signal'] - x['signal'].shift(1)\nx.loc[0,'qs'] = 0.\nqsm = x.loc[x.y == 0.,['qs']].values\nqq = np.mean(qsm) \n\nprint(f'{qq:.3f} pA = Quiet_probe_current')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reduce known non-gaussian noise.\n\nTo eliminate low frequency waveform noise, the filter should block DC and VLF. In train data from 365s to 385s exists obvious 100Hz and harmonics of 100Hz (n x 100Hz).\n\nTho remove both of these noises I'll first try a Comb filter using it's difference equation:\n\nx<sub>1</sub>(t) = a0 x<sub>0</sub>(t) + a1 x<sub>0</sub>(t-K)<br>\nK = fs/f0<br>\na0 = 1<br>\na1 = -0.99","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nx['y'] = x['y'].astype('int')\ndt = 0.0001\nfs = 10000\nf0 = 100\n\nK = np.int(fs/f0)\na1 = -0.99 \nx['signal_f'] = 0.\nx['signal_f'] = x['signal'] + a1 * x['signal'].shift(K)\nx.loc[0:K-1,'signal_f'] = x.loc[0:K-1,'signal']\n\nfilter_gain =  np.sqrt( (x['signal']**2).sum()/(x['signal_f']**2).sum()) \nprint(f'filter gain is {filter_gain:.3f}')\nprint(f'filter loss is {1./filter_gain:.3f}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate variance: y'(t) and mean: Expected{y(t)}\ny'(t) = 1/k  r2/r signal_current(t)\n\nThe injection current raises the mean of y. y_mean(t) = - 1/k * insertion_current(t) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Injection energy is converted into open channel Potential Energy y'(t) * k\n\nx2 above, the injection energy (IE) of current IC, frees ion channels to open with more vigor. When IE is lowest (0.1 x10^-24 Joules), it is rare for more than 1 channel to be open. As IE increase, so does ion channel freedom to open. At IE = 6.0 x10^-24 J, we see up to 10 open channels.\n\nOther notebooks have shown that given the mode {0,1,2,3,4}, the state of open channels are gaussian distributions. We also make the assumption that after initial comb filtering (Drift removal) that our signal also has a gaussian distribution. I assume that the injection current is DC and has variance=0. Gaussians have continuous first and second derivatives (a HJB requirement). This also inspires me to try to imagine the \"True\" reciever's transfer function H(z) as a gaussian and as the sum of two gaussians:\n\n     gaussian(y (mean,var)) = gaussian(injection_energy(mean,var=0)) + gaussian(message_energy(mean,var))\n     y(mean) = injection_energy_mean + message_energy_mean\n     y(var) = message_energy_variance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r = 70000 # Ohms\nr2 = 50 # Ohms\nr1 = 50 # Ohms\nk = 0.0016 # (pC/ion channel change)\n\n# First the true variance of y\nx['y_var'] = x['y'] - x['y'].shift(1)\nx.loc[0,'y_var'] = 0\nx['y_var'] = x['y_var'].astype('int')\nx['y_mean'] = x['y'].rolling(window=100, min_periods=5).mean()\nx.loc[0:4,'y_mean'] = 0\n\n\nx['y_var_est'] = r2 * x['signal_f'] / (k * r)\nx['y_var_est'] = x['y_var_est'].round(0).clip(-10,10).astype('int')\n\n# the injection current can be estimated as the most negative, min() of the signal\n\nx['injected_current'] = x['signal_f'].rolling(window=7500,min_periods=5).min()\nx.loc[0:4,'injected_current'] = 0.\n\nx['injected_energy'] = r1 * x['injected_current']**2 * dt/32 # I think injected current is square wave\n\nx['y_mean_est'] =  1/k * x['injected_energy']\nx.loc[0:4,'y_mean_est'] = 0\n\n# And now estimate y(t) = y_var(t) + y_mean(t)\n\nx['y_est'] = x['y_mean_est'] + x['y_var_est']\nx['y_est'] = x['y_est'].round(0).clip(0,10).astype('int')\n\nexamples = ['signal_f', 'y_var', 'y_var_est', 'y_mean','y_mean_est','y','y_est']\nion_plotter(x,examples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.close\n# Calculate the f1_score for the estimate\nf1 = f1_score(x.y, x.y_est, average='macro')\nprint(f'f1_score is {f1:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference 3\nfrom collections import namedtuple\ngaussian = namedtuple('Gaussian', ['mean', 'var'])\ngaussian.__repr__ = lambda s: 'ùí©(Œº={:.3f}, ùúé¬≤={:.3f})'.format(s[0], s[1])\n\ndef update(prior, measurement):\n    x, P = prior        # mean and variance of prior of x (system)\n    z, R = measurement  # mean and variance of measurement (open_channels) with ion probe\n    \n    J = z-x          #1 - f1_score(z,x)        # residual - This is error we want to minumize\n    K = P / (P + R)              # Kalman gain\n\n    x = x + K*J      # posterior\n    P = (1 - K) * P  # posterior variance\n    return gaussian(x, P)\n\ndef predict(posterior, movement):\n    x, P = posterior # mean and variance of posterior\n    dx, Q = movement # mean and variance of movement\n    x = x + dx\n    P = P + Q\n    return gaussian(x, P)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set initial conditions\nprior = gaussian(1,1)\n\n\nfor i in range(len(x)):\n    measurement = gaussian(x.loc[i,'y_mean_est'],x.loc[i,'y_var_est'])        \n    \n    post = update(prior,measurement)\n    \n    movement = gaussian(x.loc[i,'y_var'],x.loc[i,'y_mean'])\n    est, P = predict(post,movement)\n    x.loc[i,'y_mean_k'] = est\n    x.loc[i,'y_var_k'] = P\n\nx['y_est_k'] = x['y_mean_k'] + x['y_var_k']\nx['y_est_k'] = x['y_est_k'].round(0).clip(0,10).astype('int')\nexamples = ['y','y_est','y_est_k']\nion_plotter(x,examples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the f1_score for the estimate\n# first descretize the estimate by rounding to ones digit.\n\nf1 = f1_score(x.y, x.y_est, average='macro')\nprint(f'starting f1_score is {f1:.3f}')\nf1 = f1_score(x.y, x.y_pred_k, average='macro')\nprint(f'f1_score after optimization is {f1:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from itertools import islice\n\ndef window(seq, n=2):\n    \"Sliding window width n from seq.  From old itertools recipes.\"\"\"\n    it = iter(seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\n        \npairs = pd.DataFrame(window(x.loc[:,'y']), columns=['state1', 'state2'])\ncounts = pairs.groupby('state1')['state2'].value_counts()\nalpha = 1 # Laplacian smoothing is when alpha=1\ncounts = counts + 1\n#counts = counts.fillna(0)\nP = ((counts + alpha )/(counts.sum()+alpha)).unstack()\nP","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npairs = pd.DataFrame(window(x.loc[:,'signal_energy']), columns=['state1', 'state2'])\nmeans = pairs.groupby('state1')['state2'].mean()\nalpha = 1 # Laplacian smoothing is when alpha=1\nmeans = means.unstack()\nmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Occurence Table of State Transitions')\not = counts.unstack().fillna(0)\not","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"P = (ot)/(ot.sum())\nCal = - P * np.log(P)\nCal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Caliber = Cal.sum().sum()\nsns.heatmap(\n    Cal,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference https://www.kaggle.com/friedchips/on-markov-chains-and-the-competition-data\ndef create_axes_grid(numplots_x, numplots_y, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n    return fig, axes\n\nfig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Markov Transition Matrix P for all of train')\nsns.heatmap(\n    P,\n    annot=True, fmt='.3f', cmap='Blues', cbar=False,\n    ax=axes, vmin=0, vmax=0.5, linewidths=2);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eig_values, eig_vectors = np.linalg.eig(np.transpose(P))\nprint(\"Eigenvalues :\", eig_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lagrangian analysis seeks to find minimums and maxima for prediction purposes. First find the Lagrangian L such that:\n\nf(x,y) = L g(x,y)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import fsolve\n\n# this is the max\nX1 = fsolve(dfunc, [1, 1, 0])\nprint(X1, func(X1))\n\n# this is the min\nX2 = fsolve(dfunc, [-1, -1, 0])\nprint(X2, func(X2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}