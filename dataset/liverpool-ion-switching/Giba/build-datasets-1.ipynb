{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import f1_score\n#%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/data-without-drift/train_clean.csv', dtype={'time':'str','signal':'float','open_channels':'int'} )\ntest  = pd.read_csv('../input/data-without-drift/test_clean.csv' , dtype={'time':'str','signal':'float'} )\n\ntest ['open_channels'] = 0\ntrain['group'] = np.arange(train.shape[0])//500_000\ntest ['group'] = np.arange( test.shape[0])//100_000\ntrain.loc[ (train.group==4)|(train.group==9) ,'signal'] += np.exp(1)\ntest.loc[  ( test.group==5)|( test.group==7) ,'signal'] += np.exp(1)\n\ntrain['group'] = np.arange(train.shape[0])//100_000\n\ntrain['signal'] = train['signal'] + np.exp(1)\ntrain['signal'] = train['signal'] / 1.239\n\ntest['signal'] = test['signal'] + np.exp(1)\ntest['signal'] = test['signal'] / 1.239","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train['signal_round'] = np.clip( train['signal'].round() , 0 , 10 )\ntrain['noise'] = train['signal'] - train['open_channels']\ntrain['noise_round'] = train['signal'] - train['signal'].round()\ntrain['noise_std'] = train.groupby(['group','signal_round'])['noise_round'].transform('std')\ntrain['gaussian_noise'] = train['noise_std'].apply( lambda x: np.random.normal(0,x,1)[0]  )\ntrain['gaussian_noise'] = train['gaussian_noise'].fillna(0.20)\nprint( train.head(2) )\n\ntest['signal_round'] = np.clip( test['signal'].round() , 0 , 10 )\ntest['noise'] = 0\ntest['noise_round'] = test['signal'] - test['signal'].round()\ntest['noise_std'] = test.groupby(['group','signal_round'])['noise_round'].transform('std')\ntest['gaussian_noise'] = test['noise_std'].apply( lambda x: np.random.normal(0,x,1)[0]  )\ntest['gaussian_noise'] = test['gaussian_noise'].fillna(0.20)\nprint( test.head(2) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('group')['noise_std'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby('group')['noise_std'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN = []\nfor i, dt in train.groupby( 'group' ):\n    tmp = dt.copy()\n    varf = np.fft.fft( tmp['gaussian_noise'].values )\n    mmax = np.max( np.abs( varf ) )\n\n    varf = np.fft.fft( tmp['noise_round'].values )\n    filter_ind = np.where( np.abs(varf) >= 1.10*mmax )[0]\n    print( i , dt.shape, len(filter_ind) )\n    \n    filter_ind = np.concatenate( [filter_ind,\n                                  filter_ind+1,filter_ind-1,\n                                  filter_ind+2,filter_ind-2,\n                                  filter_ind+3,filter_ind-3,\n                                 ] )\n    filter_ind = filter_ind[filter_ind<100000]\n        \n    varf[filter_ind] = 0\n    tmp['signal_filter'] = np.fft.ifft( varf ).real + tmp[\"signal\"].round()\n    \n    TRAIN.append( tmp )\n    \nTRAIN = pd.concat( TRAIN, sort=True )\n#TRAIN.sort_values( 'time', inplace=True )\nTRAIN = TRAIN.reset_index( drop=True )\nTRAIN = TRAIN[['time','signal','signal_filter','open_channels']].copy()\nTRAIN.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( f1_score( TRAIN.open_channels, np.clip(TRAIN.signal.round(),0,10)        , average='macro' ) )\nprint( f1_score( TRAIN.open_channels, np.clip(TRAIN.signal_filter.round(),0,10) , average='macro' ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN['group'] = np.arange(TRAIN.shape[0])//500_000\nTRAIN['hit1'] = 1*(TRAIN.open_channels == TRAIN.signal.round())\nTRAIN['hit2'] = 1*(TRAIN.open_channels == TRAIN.signal_filter.round())\nTRAIN.groupby( 'group' )[['hit1','hit2']].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST = []\nfor i, dt in test.groupby( 'group' ):\n    tmp = dt.copy()\n    varf = np.fft.fft( tmp['gaussian_noise'].values )\n    mmax = np.max( np.abs( varf ) )\n\n    varf = np.fft.fft( tmp['noise_round'].values )\n    filter_ind = np.where( np.abs(varf)>= mmax )[0]\n    print( i , dt.shape, len(filter_ind) )\n    \n    filter_ind = np.concatenate( [filter_ind,\n                                  filter_ind+1,filter_ind-1,\n                                  filter_ind+2,filter_ind-2,\n                                  filter_ind+3,filter_ind-3,\n                                 ] )\n    filter_ind = filter_ind[filter_ind<100000]\n    \n    varf[filter_ind] = 0\n    tmp['signal_filter'] = np.fft.ifft( varf ).real + tmp[\"signal\"].round()\n    \n    TEST.append( tmp )\n    \nTEST = pd.concat( TEST, sort=True )\n#TEST.sort_values( 'time', inplace=True )\nTEST = TEST.reset_index( drop=True )\nTEST = TEST[['time','signal','signal_filter']].copy()\nTEST.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN['signal'] = TRAIN['signal_filter'].values\nTEST ['signal'] = TEST ['signal_filter'].values\nTRAIN[['time','signal','open_channels']].to_csv( 'train_clean_dropfreq.csv.zip', index=False, compression='zip' )\nTEST[['time','signal']].to_csv( 'test_clean_dropfreq.csv.zip', index=False, compression='zip' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l ../working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now create augmented trainset using shifted original noise"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv( 'train_clean_dropfreq.csv.zip', dtype={'time':'str','signal':'float','open_channels':'int'}, compression='zip' )\ntrain['group'] = np.arange(train.shape[0])//500_000\ntrain['noise'] = train['signal'] - train['open_channels']\ntrain.groupby( 'group' )['noise'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for N in range(5):\n    np.random.seed(N*13+11)\n    for i in range(10):\n        noise = train.loc[ train.group==i,'noise'].values.copy()\n        \n        if i==7:\n            noise = np.concatenate( [noise[:142000],noise[-177000:] ] )#Filter just the good part of noise for group 7\n        noise = np.concatenate( [noise,noise,noise,noise] )#Augment with inverse negative noise\n        window = np.random.randint(0, len(noise)-500000,1 )[0]\n        noise = noise[ window:(window+500000) ]\n        \n        train.loc[ train.group==i,'noise'] = noise\n    \n    train['signal'] = train['open_channels'] + train['noise']    \n    print( f1_score( train.open_channels, np.clip(train.signal.round(),0,10) , average='macro' ) )\n    \n    train[['time','signal','open_channels']].to_csv('train-add-noise-'+str(N)+'.csv.zip', index=False, compression='zip' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby( 'group' )['noise'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l ../working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now create trainset using custom FSM"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv( 'train_clean_dropfreq.csv.zip', dtype={'time':'str','signal':'float','open_channels':'int'}, compression='zip' )\ntrain['group'] = np.arange(train.shape[0])//500_000\ntrain['noise'] = train['signal'] - train['open_channels']\ntrain.groupby( 'group' )['noise'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StateMachineA:\n    def __init__(self):\n        self.cursts = 'C1'\n        \n        self.outputs = {}\n        self.outputs['C1'] = 0\n        self.outputs['C2'] = 0\n        self.outputs['C3'] = 0\n        self.outputs['O1'] = 1\n        self.outputs['O2'] = 1\n        \n        self.states = {}\n        self.states['C1'] = { 'C1':0.9997786 ,'O2':0.0002214, 'O1':0.00000   , 'C2':0.00000  , 'C3':0.00000 }\n        self.states['C2'] = { 'C1':0.000000  ,'O2':0.3814155, 'O1':0.00000   , 'C2':0.6185845, 'C3':0.00000 }\n        self.states['C3'] = { 'C1':0.000000  ,'O2':0.000000 , 'O1':0.3237174 , 'C2':0.00000  , 'C3':0.6762826 }\n        self.states['O1'] = { 'C1':0.000000  ,'O2':0.761767 , 'O1':0.0499013 , 'C2':0.00000  , 'C3':0.1883317 }\n        self.states['O2'] = { 'C1':0.0133812 ,'O2':0.0014535, 'O1':0.8260250 , 'C2':0.1591403, 'C3':0.00000 }          \n        \n        self.choices = {}\n        self.choices['C1'] = ['C1'] * int( self.states['C1']['C1']*1000000 ) + ['C2'] * int( self.states['C1']['C2']*1000000 ) + ['C3'] * int( self.states['C1']['C3']*1000000 ) + ['O1'] * int( self.states['C1']['O1']*1000000 ) + ['O2'] * int( self.states['C1']['O2']*1000000 )\n        self.choices['C2'] = ['C1'] * int( self.states['C2']['C1']*1000000 ) + ['C2'] * int( self.states['C2']['C2']*1000000 ) + ['C3'] * int( self.states['C2']['C3']*1000000 ) + ['O1'] * int( self.states['C2']['O1']*1000000 ) + ['O2'] * int( self.states['C2']['O2']*1000000 )\n        self.choices['C3'] = ['C1'] * int( self.states['C3']['C1']*1000000 ) + ['C2'] * int( self.states['C3']['C2']*1000000 ) + ['C3'] * int( self.states['C3']['C3']*1000000 ) + ['O1'] * int( self.states['C3']['O1']*1000000 ) + ['O2'] * int( self.states['C3']['O2']*1000000 )\n        self.choices['O1'] = ['C1'] * int( self.states['O1']['C1']*1000000 ) + ['C2'] * int( self.states['O1']['C2']*1000000 ) + ['C3'] * int( self.states['O1']['C3']*1000000 ) + ['O1'] * int( self.states['O1']['O1']*1000000 ) + ['O2'] * int( self.states['O1']['O2']*1000000 )\n        self.choices['O2'] = ['C1'] * int( self.states['O2']['C1']*1000000 ) + ['C2'] * int( self.states['O2']['C2']*1000000 ) + ['C3'] * int( self.states['O2']['C3']*1000000 ) + ['O1'] * int( self.states['O2']['O1']*1000000 ) + ['O2'] * int( self.states['O2']['O2']*1000000 )\n        \n    def run(self):\n        randint = np.random.randint( 0, len(self.choices[self.cursts]), 1 )[0]\n        #Next State\n        self.cursts = self.choices[self.cursts][ randint ]\n        return self.outputs[self.cursts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class StateMachineB:\n    def __init__(self):\n        self.cursts = 'C1'\n\n        self.outputs = {}\n        self.outputs['C1'] = 0\n        self.outputs['C2'] = 0\n        self.outputs['C3'] = 0\n        self.outputs['O1'] = 1\n        self.outputs['O2'] = 1\n        self.outputs['O3'] = 1\n\n        self.states = {}\n        self.states['C1'] = { 'C1':0.718306,'C2':0.0223246, 'C3':0.00000 , 'O1':0.2593693 , 'O2':0.00000  , 'O3':0.00000 }\n        self.states['C2'] = { 'C1':0.172716,'C2':0.226948 , 'C3':0.600336, 'O1':0.00000   , 'O2':0.00000  , 'O3':0.00000 }\n        self.states['C3'] = { 'C1':0.000000,'C2':0.317792 , 'C3':0.682208, 'O1':0.00000   , 'O2':0.00000  , 'O3':0.00000 }\n\n        self.states['O1'] = { 'C1':0.297278,'C2':0.000000 , 'C3':0.00000 , 'O1':0.6483142 , 'O2':0.0544078, 'O3':0.00000 }\n        self.states['O2'] = { 'C1':0.000000,'C2':0.000000 , 'C3':0.00000 , 'O1':0.0565354 , 'O2':0.085590 , 'O3':0.8578746}\n        self.states['O3'] = { 'C1':0.000000,'C2':0.000000 , 'C3':0.00000 , 'O1':0.00000   , 'O2':0.305837 , 'O3':0.694163}\n\n        self.choices = {}\n        self.choices['C1'] = ['C1'] * int( self.states['C1']['C1']*9000000 ) + ['C2'] * int( self.states['C1']['C2']*9000000 ) + ['C3'] * int( self.states['C1']['C3']*9000000 ) + ['O1'] * int( self.states['C1']['O1']*9000000 ) + ['O2'] * int( self.states['C1']['O2']*9000000 ) + ['O3'] * int( self.states['C1']['O3']*9000000 )\n        self.choices['C2'] = ['C1'] * int( self.states['C2']['C1']*9000000 ) + ['C2'] * int( self.states['C2']['C2']*9000000 ) + ['C3'] * int( self.states['C2']['C3']*9000000 ) + ['O1'] * int( self.states['C2']['O1']*9000000 ) + ['O2'] * int( self.states['C2']['O2']*9000000 ) + ['O3'] * int( self.states['C2']['O3']*9000000 )\n        self.choices['C3'] = ['C1'] * int( self.states['C3']['C1']*9000000 ) + ['C2'] * int( self.states['C3']['C2']*9000000 ) + ['C3'] * int( self.states['C3']['C3']*9000000 ) + ['O1'] * int( self.states['C3']['O1']*9000000 ) + ['O2'] * int( self.states['C3']['O2']*9000000 ) + ['O3'] * int( self.states['C3']['O3']*9000000 )\n        self.choices['O1'] = ['C1'] * int( self.states['O1']['C1']*9000000 ) + ['C2'] * int( self.states['O1']['C2']*9000000 ) + ['C3'] * int( self.states['O1']['C3']*9000000 ) + ['O1'] * int( self.states['O1']['O1']*9000000 ) + ['O2'] * int( self.states['O1']['O2']*9000000 ) + ['O3'] * int( self.states['O1']['O3']*9000000 )\n        self.choices['O2'] = ['C1'] * int( self.states['O2']['C1']*9000000 ) + ['C2'] * int( self.states['O2']['C2']*9000000 ) + ['C3'] * int( self.states['O2']['C3']*9000000 ) + ['O1'] * int( self.states['O2']['O1']*9000000 ) + ['O2'] * int( self.states['O2']['O2']*9000000 ) + ['O3'] * int( self.states['O2']['O3']*9000000 )\n        self.choices['O3'] = ['C1'] * int( self.states['O3']['C1']*9000000 ) + ['C2'] * int( self.states['O3']['C2']*9000000 ) + ['C3'] * int( self.states['O3']['C3']*9000000 ) + ['O1'] * int( self.states['O3']['O1']*9000000 ) + ['O2'] * int( self.states['O3']['O2']*9000000 ) + ['O3'] * int( self.states['O3']['O3']*9000000 )\n\n    def run(self):\n        randint = np.random.randint( 0, len(self.choices[self.cursts]), 1 )[0]\n        #Next State\n        self.cursts = self.choices[self.cursts][ randint ]\n        return self.outputs[self.cursts]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for N in range(1):\n    print( N )\n    \n    np.random.seed(N*17+19)\n    \n    print('.')\n    fsm = StateMachineA()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==0, 'open_channels' ] = tmp0\n    \n    print('.')\n    fsm = StateMachineA()\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==1, 'open_channels' ] = tmp1\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==2, 'open_channels' ] = tmp2\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp3 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==6, 'open_channels' ] = tmp3\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==3, 'open_channels' ] = tmp0+tmp1+tmp2\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==7, 'open_channels' ] = tmp0+tmp1+tmp2\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    tmp3 = np.array( [fsm.run() for i in range(500000)] )\n    tmp4 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==5, 'open_channels' ] = tmp0+tmp1+tmp2+tmp3+tmp4\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    tmp3 = np.array( [fsm.run() for i in range(500000)] )\n    tmp4 = np.array( [fsm.run() for i in range(500000)] )\n    train.loc[ train.group==8, 'open_channels' ] = tmp0+tmp1+tmp2+tmp3+tmp4\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    tmp3 = np.array( [fsm.run() for i in range(500000)] )\n    tmp4 = np.array( [fsm.run() for i in range(500000)] )\n    tmp0 = tmp0+tmp1+tmp2+tmp3+tmp4\n    \n    tmp5 = np.array( [fsm.run() for i in range(500000)] )\n    tmp6 = np.array( [fsm.run() for i in range(500000)] )\n    tmp7 = np.array( [fsm.run() for i in range(500000)] )\n    tmp8 = np.array( [fsm.run() for i in range(500000)] )\n    tmp9 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = tmp5+tmp6+tmp7+tmp8+tmp9\n    train.loc[ train.group==4, 'open_channels' ] = tmp0+tmp1\n    \n    print('.')\n    fsm = StateMachineB()\n    tmp0 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = np.array( [fsm.run() for i in range(500000)] )\n    tmp2 = np.array( [fsm.run() for i in range(500000)] )\n    tmp3 = np.array( [fsm.run() for i in range(500000)] )\n    tmp4 = np.array( [fsm.run() for i in range(500000)] )\n    tmp0 = tmp0+tmp1+tmp2+tmp3+tmp4\n    \n    tmp5 = np.array( [fsm.run() for i in range(500000)] )\n    tmp6 = np.array( [fsm.run() for i in range(500000)] )\n    tmp7 = np.array( [fsm.run() for i in range(500000)] )\n    tmp8 = np.array( [fsm.run() for i in range(500000)] )\n    tmp9 = np.array( [fsm.run() for i in range(500000)] )\n    tmp1 = tmp5+tmp6+tmp7+tmp8+tmp9\n    train.loc[ train.group==9, 'open_channels' ] = tmp0+tmp1\n    \n    \n    #Now add original noise back to open_channels\n    for i in range(10):\n        noise = train.loc[ train.group==i,'noise'].values.copy()\n        oc    = train.loc[ train.group==i,'open_channels'].values.copy()\n        \n        if i==7:\n            noise = np.concatenate( [noise[:142000],noise[-177000:] ] )#Filter just the good part of noise for group 7\n        noise = np.concatenate( [noise,noise,noise,noise] )#Augment with inverse negative noise\n        window = np.random.randint(0, len(noise)-500000,1 )[0]\n        noise = noise[ window:(window+500000) ]\n    \n        train.loc[ train.group==i,'signal'] = oc + noise    \n    \n    print( f1_score( train.open_channels, np.clip(train.signal.round(),0,10) , average='macro' ) )\n    \n    train[['time','signal','open_channels']].to_csv( 'train-fsmAB-artificial-'+str(N)+'.csv.zip' , index=False, compression='zip'  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l ../working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 0\ntrain0 = pd.read_csv( 'train_clean_dropfreq.csv.zip', compression='zip'  )\ntrain1 = pd.read_csv( 'train-add-noise-'+str(i)+'.csv.zip', compression='zip'  )\ntrain2 = pd.read_csv( 'train-fsmAB-artificial-'+str(i)+'.csv.zip', compression='zip'  )\n\ntrain0.sort_values( 'time', inplace=True )\ntrain1.sort_values( 'time', inplace=True )\ntrain2.sort_values( 'time', inplace=True )\n\ntrain0['group'] = np.arange(train0.shape[0])//500_000\ntrain1['group'] = np.arange(train1.shape[0])//500_000\ntrain2['group'] = np.arange(train2.shape[0])//500_000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot( train0.signal.values[:1000] )\nplt.plot( train1.signal.values[:1000] )\nplt.plot( train2.signal.values[:1000] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train0.groupby('group')['open_channels'].agg(['mean','std','min','max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.groupby('group')['open_channels'].agg(['mean','std','min','max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.groupby('group')['open_channels'].agg(['mean','std','min','max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( f1_score( train0.open_channels, np.clip(train0.signal.round(),0,10) , average='macro' ) )\nprint( f1_score( train1.open_channels, np.clip(train1.signal.round(),0,10) , average='macro' ) )\nprint( f1_score( train2.open_channels, np.clip(train2.signal.round(),0,10) , average='macro' ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train0['hit1'] = 1*(train0.open_channels == train0.signal.round())\ntrain0.groupby( 'group' )['hit1'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1['hit1'] = 1*(train1.open_channels == train1.signal.round())\ntrain1.groupby( 'group' )['hit1'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2['hit1'] = 1*(train2.open_channels == train2.signal.round())\ntrain2.groupby( 'group' )['hit1'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Create Artificial Public Testset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv( 'train_clean_dropfreq.csv.zip', dtype={'time':'str','signal':'float','open_channels':'int'}, compression='zip' )\n# train['group'] = np.arange(train.shape[0])//500_000\n# train['noise'] = train['signal'] - train['open_channels']\n# train.groupby( 'group' )['noise'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Public Test\n# Group, sum\n# 0, 3A (or maybe 4A)\n# 1, 3B \n# 2, 5B \n# 3, 3A (or maybe 4A) \n# 4,  B\n# 5,10B\n\n# group\n# 0     0.191035\n# 1     0.214339\n# 2     0.223478\n# 3     0.190773\n# 4     0.193667\n# 5     0.267836\n\n# Train\n# Group, sum\n# 0,  A \n# 1,  A \n# 2,  B \n# 3, 3B \n# 4,10B \n# 5, 5B \n# 6,  B \n# 7, 3B \n# 8, 5B \n# 9,10B \n\n# group\n# 0    0.197584\n# 1    0.199169\n# 2    0.198097\n# 3    0.215502\n# 4    0.333838\n# 5    0.232669\n# 6    0.197967\n# 7    0.231789\n# 8    0.230806\n# 9    0.333575","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for N in range(1):\n#     print( N )\n#     np.random.seed(N*171+191)\n\n#     print('.')\n#     fsm = StateMachineA()\n#     tmp0 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp1 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp2 = np.array( [fsm.run() for i in range(500000)] )\n#     train.loc[ train.group==0, 'open_channels' ] = tmp0 + tmp1 + tmp2\n\n#     print('.')\n#     fsm = StateMachineB()\n#     tmp0 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp1 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp2 = np.array( [fsm.run() for i in range(500000)] )\n#     train.loc[ train.group==1, 'open_channels' ] = tmp0 + tmp1 + tmp2\n\n#     print('.')\n#     fsm = StateMachineB()\n#     tmp0 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp1 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp2 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp3 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp4 = np.array( [fsm.run() for i in range(500000)] )\n#     train.loc[ train.group==2, 'open_channels' ] = tmp0+tmp1+tmp2+tmp3+tmp4\n\n#     print('.')\n#     fsm = StateMachineA()\n#     tmp0 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp1 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp2 = np.array( [fsm.run() for i in range(500000)] )\n#     train.loc[ train.group==3, 'open_channels' ] = tmp0 + tmp1 + tmp2\n\n#     print('.')\n#     fsm = StateMachineB()\n#     tmp0 = np.array( [fsm.run() for i in range(500000)] )\n#     train.loc[ train.group==4, 'open_channels' ] = tmp0\n\n#     print('.')\n#     fsm = StateMachineB()\n#     tmp0 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp1 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp2 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp3 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp4 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp0 = tmp0+tmp1+tmp2+tmp3+tmp4\n\n#     tmp5 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp6 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp7 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp8 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp9 = np.array( [fsm.run() for i in range(500000)] )\n#     tmp1 = tmp5+tmp6+tmp7+tmp8+tmp9\n#     train.loc[ train.group==5, 'open_channels' ] = tmp0+tmp1\n\n\n#     #Now add original noise back to open_channels\n#     mapnoise = [0,3,8,0,2,4]\n#     for i in range(6):\n#         noise = train.loc[ train.group==mapnoise[i],'noise'].values.copy()\n#         oc    = train.loc[ train.group==i,'open_channels'].values.copy()\n\n#         if i==7:\n#             noise = np.concatenate( [noise[:142000],noise[-177000:] ] )#Pick just the good part of noise for group 7\n#         noise = np.concatenate( [noise,-1*noise] )#Augment with inverse negative noise\n#         noise = np.random.choice( noise, 500000, replace=False )\n\n#         train.loc[ train.group==i,'signal'] = oc + noise    \n\n#     py = np.where( train.group <=5 )[0]\n#     print( f1_score( train.open_channels.values[py], np.clip(train.signal.values[py].round(),0,10) , average='macro' ) )\n\n#     train[['time','signal','open_channels']].iloc[py].to_csv( 'train-PublicLB-fsmAB-artificial-'+str(N)+'.csv.zip' , index=False, compression='zip'  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Real Public Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Public Test\n# Group, sum\n# 0, 3A (or maybe 4A)\n# 1, 3B \n# 2, 5B \n# 3, 3A (or maybe 4A) \n# 4,  B\n# 5,10B\n\n# group\n# 0     0.191035\n# 1     0.214339\n# 2     0.223478\n# 3     0.190773\n# 4     0.193667\n# 5     0.267836\n\n\n# Train\n# Group, sum\n# 0,  A \n# 1,  A \n# 2,  B \n# 3, 3B \n# 4,10B \n# 5, 5B \n# 6,  B \n# 7, 3B \n# 8, 5B \n# 9,10B \n\n# group\n# 0    0.197584\n# 1    0.199169\n# 2    0.198097\n# 3    0.215502\n# 4    0.333838\n# 5    0.232669\n# 6    0.197967\n# 7    0.231789\n# 8    0.230806\n# 9    0.333575","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv( 'train_clean_dropfreq.csv.zip', dtype={'time':'str','signal':'float','open_channels':'int'}, compression='zip' )\ntrain['group'] = np.arange(train.shape[0])//500_000\ntrain['noise'] = train['signal'] - train['open_channels']\n\ntmp = train.loc[ train.group==7 ].copy()\ntmp['noise'] = tmp['signal'] - tmp['open_channels']\ntmp['signal'].iloc[142000:323000] = tmp['open_channels'].iloc[142000:323000]\ntmp['noise'].iloc[142000:242000] = tmp['noise'].iloc[:100000].values\ntmp['noise'].iloc[242000:323000] = tmp['noise'].iloc[323000:404000].values\ntrain.loc[ train.group==7 ] = tmp.copy()\n\ntrain.groupby( 'group' )['noise'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( f1_score( train.open_channels.values, np.clip(train.signal.round().values,0,10) , average='macro' ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for N in range(10):\n    print( N )\n    np.random.seed(N*171+191)\n    TRAIN = []\n\n    A_base = pd.concat( (train.loc[ train.group==0 ].copy() , train.loc[ train.group==1 ].copy() , train.loc[ train.group==0 ].copy() , train.loc[ train.group==1 ].copy()) )\n    B_base = pd.concat( (train.loc[ train.group==2 ].copy() , train.loc[ train.group==6 ].copy() , train.loc[ train.group==2 ].copy() , train.loc[ train.group==6 ].copy()) )\n    B3_base = pd.concat( (train.loc[ train.group==3 ].copy() , train.loc[ train.group==7 ].copy() , train.loc[ train.group==3 ].copy() , train.loc[ train.group==7 ].copy()) )\n    B5_base = pd.concat( (train.loc[ train.group==5 ].copy() , train.loc[ train.group==8 ].copy() , train.loc[ train.group==5 ].copy() , train.loc[ train.group==8 ].copy()) )\n    B10_base = pd.concat( (train.loc[ train.group==4 ].copy() , train.loc[ train.group==9 ].copy() , train.loc[ train.group==4 ].copy() , train.loc[ train.group==9 ].copy()) )\n    \n    #B1\n    n0,n1 = np.random.randint( 0, B_base.shape[0]-500000, 2 )\n    B0 = B_base.iloc[ n0:(n0+500000) ].copy()\n    B1 = B_base.iloc[ n1:(n1+500000) ].copy()\n    B0['noise1'] = B_base['noise'].iloc[ n1:(n1+500000) ].values.copy()\n    NOISE = B0[['noise','noise1']].values\n    ind = np.random.randint(0,2, 500000)\n    B0['signal'] = [NOISE[i,ind[i]] for i in range(500000) ]\n    B0['signal'] = B0['signal'] + B0['open_channels']\n    B0 = B0.reset_index(drop=True)\n    TRAIN.append( B0[['signal','open_channels']] )\n    del B0, B1, NOISE, ind\n    gc.collect()    \n    \n    #A4\n    n0,n1,n2,n3 = np.random.randint( 0, A_base.shape[0]-500000, 4  )\n    A0 = A_base.iloc[ n0:(n0+500000) ].copy()\n    A1 = A_base.iloc[ n1:(n1+500000) ].copy()\n    A2 = A_base.iloc[ n2:(n2+500000) ].copy()\n    A3 = A_base.iloc[ n3:(n3+500000) ].copy()\n    A0['open_channels'] = A0['open_channels'].values+A1['open_channels'].values+A2['open_channels'].values+A3['open_channels'].values\n    A0['noise1'] = A1['noise'].values\n    A0['noise2'] = A2['noise'].values\n    A0['noise3'] = A3['noise'].values\n    NOISE = A0[['noise','noise1','noise2','noise3']].values\n    ind = np.random.randint(0,4, 500000)\n    A0['signal'] = [NOISE[i,ind[i]] for i in range(500000) ]\n    A0['signal'] = A0['signal'] + A0['open_channels']\n    A0 = A0.reset_index(drop=True)\n    TRAIN.append( A0[['signal','open_channels']] )\n    del A0, A1, A2, A3, NOISE, ind\n    gc.collect()\n    \n    #A4\n    n0,n1,n2,n3 = np.random.randint( 0, A_base.shape[0]-500000, 4  )\n    A0 = A_base.iloc[ n0:(n0+500000) ].copy()\n    A1 = A_base.iloc[ n1:(n1+500000) ].copy()\n    A2 = A_base.iloc[ n2:(n2+500000) ].copy()\n    A3 = A_base.iloc[ n3:(n3+500000) ].copy()\n    A0['open_channels'] = A0['open_channels'].values+A1['open_channels'].values+A2['open_channels'].values+A3['open_channels'].values\n    A0['noise1'] = A1['noise'].values\n    A0['noise2'] = A2['noise'].values\n    A0['noise3'] = A3['noise'].values\n    NOISE = A0[['noise','noise1','noise2','noise3']].values\n    ind = np.random.randint(0,4, 500000)\n    A0['signal'] = [NOISE[i,ind[i]] for i in range(500000) ]\n    A0['signal'] = A0['signal'] + A0['open_channels']\n    A0 = A0.reset_index(drop=True)\n    TRAIN.append( A0[['signal','open_channels']] )\n    del A0, A1, A2, A3, NOISE, ind\n    gc.collect()\n    \n    #B3\n    n0,n1,n2 = np.random.randint( 0, A_base.shape[0]-500000, 3 )\n    B0 = B_base.iloc[ n0:(n0+500000) ].copy()\n    B1 = B_base.iloc[ n1:(n1+500000) ].copy()\n    B2 = B_base.iloc[ n2:(n2+500000) ].copy()\n    B0['open_channels'] = B0['open_channels'].values+B1['open_channels'].values+B2['open_channels'].values\n    B0['noise']  = B3_base['noise'].iloc[ n0:(n0+500000) ].values.copy()\n    B0['noise1'] = B3_base['noise'].iloc[ n1:(n1+500000) ].values.copy()\n    B0['noise2'] = B3_base['noise'].iloc[ n2:(n2+500000) ].values.copy()\n    NOISE = B0[['noise','noise1','noise2']].values\n    ind = np.random.randint(0,3, 500000)\n    B0['signal'] = [NOISE[i,ind[i]] for i in range(500000) ]\n    B0['signal'] = B0['signal'] + B0['open_channels']\n    B0 = B0.reset_index(drop=True)\n    TRAIN.append( B0[['signal','open_channels']] )\n    del B0, B1, B2, NOISE, ind\n    gc.collect()\n    \n    #B5\n    n0,n1,n2 = np.random.randint( 0, B_base.shape[0]-500000, 3 )\n    B0 = B3_base.iloc[ n0:(n0+500000) ].copy()\n    B1 = B_base.iloc[ n1:(n1+500000) ].copy()\n    B2 = B_base.iloc[ n2:(n2+500000) ].copy()\n    B0['open_channels'] = B0['open_channels'].values+B1['open_channels'].values+B2['open_channels'].values\n    B0['noise']  = B5_base['noise'].iloc[ n0:(n0+500000) ].values.copy()\n    B0['noise1'] = B5_base['noise'].iloc[ n1:(n1+500000) ].values.copy()\n    B0['noise2'] = B5_base['noise'].iloc[ n2:(n2+500000) ].values.copy()\n    NOISE = B0[['noise','noise1','noise2']].values\n    ind = np.random.randint(0,3, 500000)\n    B0['signal'] = [NOISE[i,ind[i]] for i in range(500000) ]\n    B0['signal'] = B0['signal'] + B0['open_channels']\n    B0 = B0.reset_index(drop=True)\n    TRAIN.append( B0[['signal','open_channels']] )\n    del B0, B1, B2, NOISE, ind\n    gc.collect()    \n    \n    #B10\n    n0,n1,n2,n3 = np.random.randint( 0, B_base.shape[0]-500000, 4 )\n    B0 = B5_base.iloc[ n0:(n0+500000) ].copy()\n    B1 = B3_base.iloc[ n1:(n1+500000) ].copy()\n    B2 = B_base.iloc[ n2:(n2+500000) ].copy()\n    B3 = B_base.iloc[ n3:(n3+500000) ].copy()\n    B0['open_channels'] = B0['open_channels'].values+B1['open_channels'].values+B2['open_channels'].values+B3['open_channels'].values\n    B0['noise']  = B10_base['noise'].iloc[ n0:(n0+500000) ].values.copy()\n    B0['noise1'] = B10_base['noise'].iloc[ n1:(n1+500000) ].values.copy()\n    B0['noise2'] = B10_base['noise'].iloc[ n2:(n2+500000) ].values.copy()\n    B0['noise3'] = B10_base['noise'].iloc[ n3:(n3+500000) ].values.copy()\n    NOISE = B0[['noise','noise1','noise2','noise3']].values\n    ind = np.random.randint(0,4, 500000)\n    B0['signal'] = [NOISE[i,ind[i]] for i in range(500000) ]\n    B0['signal'] = B0['signal'] + B0['open_channels']\n    B0 = B0.reset_index(drop=True)\n    TRAIN.append( B0[['signal','open_channels']] )\n    del B0, B1, B2, B3, NOISE, ind\n    gc.collect()\n\n    TRAIN = pd.concat( TRAIN )\n    TRAIN['time'] = [ \"{0:.4f}\".format( i/10000 ) for i in range(TRAIN.shape[0]) ]\n\n    TRAIN[['time','signal','open_channels']].to_csv( 'train-PublicLB-mix-real-data-'+str(N)+'.csv.zip' , index=False, compression='zip'  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 0\ntrain0 = pd.read_csv( 'train-PublicLB-mix-real-data-'+str(N)+'.csv.zip', compression='zip' )\nprint( f1_score( train0.open_channels.values, np.clip(train0.signal.round().values,0,10) , average='macro' ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train0['group'] = np.arange(train0.shape[0])//500_000\ntrain0['noise'] = train0['signal'] - train0['open_channels']\ntrain0.groupby( 'group' )['noise'].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby( 'group' )[['signal','open_channels']].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train0.groupby( 'group' )[['signal','open_channels']].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby( 'group' )[['signal']].agg(['mean','std'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}