{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# University of Liverpool - Ion Switching Tutorial\n\n**Hello everybody, in this tutorial i will do this competition:** https://www.kaggle.com/c/liverpool-ion-switching\n\n**Since you can only use the \"late submission\" option to still submit to this competition, this competition will NOT be shown in your profile!**\n\n**The results of this notebook so far look like this (the ranks vary as time goes on):**\n\n* **public score:   0.92874**\n* **public rank:  1888/2618**\n\n* **private score:  0.91612**\n* **private rank: 1711/2618**\n\n**What I do is strongly oriented on this notebook:** https://www.kaggle.com/cdeotte/one-feature-model-0-930\n\n**I will explain certain things with more detail and more explanations, that's the main purpose of this tutorial.**\n\n**The focus of this notebook lies on the modification of the signal curves, these measured curves must be altered in a certain way.**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T12:07:00.495346Z","iopub.execute_input":"2021-06-02T12:07:00.495699Z","iopub.status.idle":"2021-06-02T12:07:00.503383Z","shell.execute_reply.started":"2021-06-02T12:07:00.49567Z","shell.execute_reply":"2021-06-02T12:07:00.502433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Load and analyze data","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/liverpool-ion-switching/train.csv')\ntest = pd.read_csv('/kaggle/input/liverpool-ion-switching/test.csv')\n\n\nprint(\"loading successful!\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape, \"\\n\")\nprint(train.info(), \"\\n\")\nprint(train.columns, \"\\n\")\nprint(train.index, \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test.shape, \"\\n\")\nprint(test.info(), \"\\n\")\nprint(test.columns, \"\\n\")\nprint(test.index, \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Check for missing values","metadata":{}},{"cell_type":"code","source":"for i in train.columns:\n    print(i, train[i].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in test.columns:\n    print(i, test[i].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No missing values, that is very good.**","metadata":{}},{"cell_type":"markdown","source":"# 3. Analyze distribution of target","metadata":{}},{"cell_type":"code","source":"print(train.open_channels.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ok, there are 0 to 10 open channels, but we have to plot and look at the data to really understand it.**","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Plot the data","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(train.time[::100], train.signal[::100])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(train.time[::1000], train.open_channels[::1000], color = 'red')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Analyze correlation between time and signal","metadata":{}},{"cell_type":"code","source":"corr_dataframe = train[[\"time\", \"signal\"]]\n\ncorr_mat = corr_dataframe.corr()\n\nprint(corr_mat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ok, the correlation value between time and signal is 0.831, which is quite high, which is quite good, since these are the only 2 features we have.**\n\n**But this value does not say much, because it's simply the correlation value of the entire time series, and as we can see from the plot, there are many local differences and the signal curve does weird things, which the open_channel curve does not relate to, hence we must remove these weird effects and shapes in the signal curve.** \n\n**The open_channel curve only seems to correlate with the height of the plateaus of the signal curve and with the rapid ups and downs of the signal curve.**\n\n\n**We can see that there are 10 distinct parts of our time series, hence let's look at the plots in these 10 parts:**","metadata":{}},{"cell_type":"code","source":"a = 500000\ndist = 100\n\nfor i in range(0,10):\n    \n    print(i, \"min: \", min(train.signal[0+i*a:(i+1)*a:dist].values), \"max: \", max(train.signal[0+i*a:(i+1)*a:dist]))\n    plt.figure(figsize=(20,5))\n    plt.plot(train.time[0+i*a:(i+1)*a:dist], train.signal[0+i*a:(i+1)*a:dist])\n    plt.plot(train.time[0+i*a:(i+1)*a:dist], train.open_channels[0+i*a:(i+1)*a:dist], color = 'red')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see there are some distinct shapes, we have to remove from the signal curve:**\n\n* **In the second plot there is a weird short linear increasing from 50 to 60, where the open_channel curve does not follow this linear trend.**\n* **In plots 7,8,9,10 there is this parabolic shape of the signal curve, but the open_channel curve stays horizontial.**\n* **The sharp peaks and dips standing out of the parabola shape in the 8th part of the signal curve do not correlate at all with the open_channel curve, hence we should remove them as well.**\n\n\n**When we remove these weird shapes from the train signal curve, we have to remove these from the test signal curve as well, let's look how the test signal curve looks like:**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(test.time[::100], test.signal[::100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see there are 10 distinct parts as well, but they look different than the train signal curve, and they are not equidistant, because the last 2 parts are much longer than the first 8 parts.** \n\n**The test signal curve has many linear increasing segments, the train signal curve only had one tiny part of a linearly increasing signal in the second part between 50 and 60.**\n\n**But since the open_channel curve did not react at all to the linear increasing segment in the train signal curve, we must remove all linear increasing segments from the test signal curve.**\n\n**Besides that we must remove the parabolic shape of the 9th part of the test signal curve and the 4 parabolic shapes of the train signal curve.**\n\n\n**These two tasks (remove linear increasing and remove parabolic shape) offer a huge potential to solve these with a lot of elegance and mathematics, you could fit the slope of the linear increasing segment as well as the parabolic shapes and then  automate everything afterwards,  I will try it fast and dirty by hand and let's see if it works and how fast we can progress.**","metadata":{}},{"cell_type":"markdown","source":"# 4. Correct linear increasing slope\n## 4.1 Correct linear increasing slope of train\n\n**To remove the linear increasing a simple formula should suffice.**\n\n**The slope of the linear increasing segment of the train curve in the 2nd part between 50 and 60 can be measured to rougly 3 y-units on 10 x-units, hence our slope rougly is 3/10.** \n\n**We want to modify the train.signal values by subtracting the train.time values, because the train.time values are a perfect linear slope, hence we can use this linear increasing character of these values and do not have to implement any linear increasing function.**\n\n**And the only 2 values we need to remove/correct the linear increasing segment are the slope and the starting point of the train.time values, which is 50 in this case.**\n\n**Hence let's try the simple formula  slope * (train.time - 50), by doing this we correct the offset of the train.time values by subtracting 50, and the linear increasing character of the train.time values is compensated by multiplying with 0.3.**\n\n**Let's see if it works:**","metadata":{}},{"cell_type":"code","source":"a = 500000 \nb = 600000 \n\n\nplt.plot(train.time[0+1*a:(1+1)*a:dist], train.signal[0+1*a:(1+1)*a:dist])\nplt.plot(train.time[0+1*a:(1+1)*a:dist], train.open_channels[0+1*a:(1+1)*a:dist], color = 'red')\nplt.show()\n\n#####################################\ntrain2 = train.copy()\n\nc = 0.3\nd = 50\n\ntrain2.signal[a:b] = train2.signal[a:b].values - c*(train2.time[a:b].values - d)\ntrain.signal[a:b] = train2.signal[a:b]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's see if it worked:**","metadata":{}},{"cell_type":"code","source":"a = 500000\ndist = 100\n\nplt.plot(train.time[0+1*a:(1+1)*a:dist], train.signal[0+1*a:(1+1)*a:dist])\nplt.plot(train.time[0+1*a:(1+1)*a:dist], train.open_channels[0+1*a:(1+1)*a:dist], color = 'red')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Correct linear increasing slope of test","metadata":{}},{"cell_type":"markdown","source":"**Beautiful.  Sadly this kind of method cannot be used for the parabolic shapes, since parabolic shapes are not linear.**\n\n**And now we have to remove all the linear increasing slopes of the test data.**\n\n**Let's look at the test data again:**\n\n\n![](https://i.imgur.com/C3WPCew.png)\n","metadata":{}},{"cell_type":"markdown","source":"**Above we have already splitted this test signal curve up into the 10 distinct parts.**\n\n**The following parts contain linear increasing slopes, which we have to compensate in order for our models to predict accurately:**\n\n* **part 1**\n* **part 2**\n* **part 5**\n* **part 7**\n* **part 8**\n* **part 9**\n\n\n**We will use the same procedure as we did for the train data.**\n\n**We will measure the slope easily by hand, it seems to be the same slope for the entire test signal curve anyway.**\n\n**The offset value of the test.time feature can easily be read out from the x-axis.**\n\n**With these 2 values we can easily correct the slopes and end up with a horizontal plateau at different heights.**","metadata":{}},{"cell_type":"code","source":"a = 100000\ndist = 100\n\nprint(\"part 1\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[0:1*a:dist], test.signal[0:1*a:dist])\nplt.show()\n\nprint(\"part 2\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[1*a:2*a:dist], test.signal[1*a:2*a:dist])\nplt.show()\n\nprint(\"part 3\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[2*a:3*a:dist], test.signal[2*a:3*a:dist])\nplt.show()\n\nprint(\"part 4\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[3*a:4*a:dist], test.signal[3*a:4*a:dist])\nplt.show()\n\nprint(\"part 5\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[4*a:5*a:dist], test.signal[4*a:5*a:dist])\nplt.show()\n\nprint(\"part 6\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[5*a:6*a:dist], test.signal[5*a:6*a:dist])\nplt.show()\n\nprint(\"part 7\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[6*a:7*a:dist], test.signal[6*a:7*a:dist])\nplt.show()\n\nprint(\"part 8\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[7*a:8*a:dist], test.signal[7*a:8*a:dist])\nplt.show()\n\nprint(\"part 9\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[8*a:9*a:dist], test.signal[8*a:9*a:dist])\nplt.show()\n\nprint(\"part 10\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[9*a:10*a:dist], test.signal[9*a:10*a:dist])\nplt.show()\n\nprint(\"part 11\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[10*a:15*a:dist], test.signal[10*a:15*a:dist])\nplt.show()\n\nprint(\"part 12\")\nplt.figure(figsize=(20,5))\nplt.plot(test.time[15*a:20*a:dist], test.signal[15*a:20*a:dist])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################\n\ntest2 = test.copy()\n\n################\n# part 1:\n\na = 0\nb = 100000\n\nc = 0.3\nd = 500\n\ntest2.signal[a:b] = test2.signal[a:b].values - c*(test2.time[a:b].values - d)\ntest.signal[a:b]  = test2.signal[a:b]\n################\n# part 2:\n\na = 100000\nb = 200000\n\nd =  510\n\ntest2.signal[a:b] = test2.signal[a:b].values - c*(test2.time[a:b].values - d)\ntest.signal[a:b]  = test2.signal[a:b]\n################\n# part 5:\n\na = 400000\nb = 500000\n\nd =  540\n\ntest2.signal[a:b] = test2.signal[a:b].values - c*(test2.time[a:b].values - d)\ntest.signal[a:b]  = test2.signal[a:b]\n################\n# part 7:\n\na = 600000\nb = 700000\n\nd =  560\n\ntest2.signal[a:b] = test2.signal[a:b].values - c*(test2.time[a:b].values - d)\ntest.signal[a:b]  = test2.signal[a:b]\n################\n# part 8:\n\n# slope  =  3/10\n\na = 700000\nb = 800000\n\nd =  570\n\ntest2.signal[a:b] = test2.signal[a:b].values - c*(test2.time[a:b].values - d)\ntest.signal[a:b]  = test2.signal[a:b]\n################\n# part 9:\n\na = 800000\nb = 900000\n\nd =  580\n\ntest2.signal[a:b] = test2.signal[a:b].values - c*(test2.time[a:b].values - d)\ntest.signal[a:b]  = test2.signal[a:b]\n################\n\nprint(\"correcting linear slopes in test successful!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's see if it worked:**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(test.time[::100], test.signal[::100])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Correct the parabolic shape\n\n**I looked up this notebook to see how other people removed the parabolic shape:** https://www.kaggle.com/cdeotte/one-feature-model-0-930#Remove-Training-Data-Drift\n\n**We will try to construct a parabolic shape, we will need 3 values for this:  minimum, maximum, middle.**\n\n**I printed the minimum and maximum values when i plotted the 10 parts of the train signal curve, the middle can be easily detected by hand, since it's just the time stamp in the middle of the parabolic shape, where it has its peak.**","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Correct parabolic shape in train data","metadata":{}},{"cell_type":"code","source":"def remove_parabolic_shape(values, minimum, middle, maximum):\n    \n    a = maximum - minimum\n    return -(a/625)*(values - middle)**2+a\n\n################################################\n\n# I really want to find out, how he found these perfectly working\n# numbers, because I can't imagine, that he sat around for hours,\n# tweaking these low and high values until it worked.\n\n# idea1: get the min and max value by calculating the mean\n# of a certain window at the beginning of the batch\n# and at the middle of the batch.\n\n\n################################################\n# part 7 goes from 3000k to 3500k\n\n#his values\n#low  = -1.817\n#high =  3.186\n\n#my values\n#min:   -2.9517 \n#max:    4.366\n\na = 3000000\nb = 3500000\nminimum = -1.817\nmiddle = 325\nmaximum = 3.186\n\ntrain2.signal[a:b] = train2.signal[a:b].values - remove_parabolic_shape(train2.time[a:b].values, minimum, middle, maximum)\ntrain.signal[a:b] = train2.signal[a:b]\n\n################################################\n# part 8 goes from 3500k to 4000k\n\n#his values\n#low  = -0.094\n#high =  4.936\n\n#my values\n#min:   -3.0399 \n#max:    9.9986\n\na = 3500000\nb = 4000000\nminimum = -0.094\nmiddle = 375\nmaximum = 4.936\n\ntrain2.signal[a:b] = train2.signal[a:b].values - remove_parabolic_shape(train2.time[a:b].values, minimum, middle, maximum)\ntrain.signal[a:b] = train2.signal[a:b]\n\n################################################\n# part 9 goes from 4000k to 4500k\n\n#his values\n#low  =  1.715\n#high =  6.689\n\n#my values\n#min:   -2.0985 \n#max:    9.0889\n\na = 4000000\nb = 4500000\nminimum = 1.715\nmiddle = 425\nmaximum = 6.689\n\ntrain2.signal[a:b] = train2.signal[a:b].values - remove_parabolic_shape(train2.time[a:b].values, minimum, middle, maximum)\ntrain.signal[a:b] = train2.signal[a:b]\n\n################################################\n# part10 goes from 4500k to 5000k\n\n#his values\n#low  =  3.361\n#high =  8.45\n\n#my values\n#min:   -1.5457 \n#max:   12.683\n\na = 4500000\nb = 5000000\nminimum = 3.361\nmiddle = 475\nmaximum = 8.45\n\ntrain2.signal[a:b] = train2.signal[a:b].values - remove_parabolic_shape(train2.time[a:b].values, minimum, middle, maximum)\ntrain.signal[a:b] = train2.signal[a:b]\n\n################################################","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 500000\ndist = 100\n\nfor i in range(6,10):    \n    plt.figure(figsize=(20,5))\n    plt.plot(train.time[0+i*a:(i+1)*a:dist], train.signal[0+i*a:(i+1)*a:dist])\n    plt.plot(train.time[0+i*a:(i+1)*a:dist], train.open_channels[0+i*a:(i+1)*a:dist], color = 'red')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see, this method worked quite well, all parabolic shapes are gone.**\n\n**Now we have to do the same thing for the big parabolic shape in the test signal curve.**","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Correct parabolic shape in test data","metadata":{}},{"cell_type":"code","source":"#######################################################\n# his magical function full of magical numbers\n\ndef f(x):\n    return -(0.00788)*(x-625)**2+2.345 +2.58\n\n\n#test2.loc[test2.index[a:b],'signal'] = test2.signal.values[a:b] - f(test2.time[a:b].values)\n#######################################################\n\ntest2 = test.copy()\n\n\na = 1000000\nb = 1500000\n\nplt.figure(figsize=(20,5))\nplt.plot(test.time[a:b], test.signal[a:b])\nplt.show()\n\ntest2.signal[a:b] = test2.signal[a:b].values - f(test2.time[a:b].values)\n#test2.signal[a:b] = test2.signal[a:b].values - remove_parabolic_shape(test2.time[a:b].values, minimum, middle, maximum)\ntest.signal[a:b] = test2.signal[a:b]\n\nplt.figure(figsize=(20,5))\nplt.plot(test.time[a:b], test.signal[a:b])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Choose and train models\n\n**In this notebook:**  https://www.kaggle.com/cdeotte/one-feature-model-0-930#Make-Five-Simple-Models\n\n**he identified the 5 different parts of the signal curves by the number of open channel and how fast the number switches.**\n\n* **1 slow open channel**\n* **1 fast open channel**\n* **3 open channels**\n* **5 open channels**\n* **10 open channels**\n\n\n**The differentiation between slow and fast will only be made for the parts where there is only 1 or 0 open channels.**\n\n**In all the other parts the switching is always fast compared to the first part from 0 to 100 where the one open channel switches slowly.**\n\n**This can be seen in this picture:**\n\n![](https://i.imgur.com/gdoz3nE.png)","metadata":{}},{"cell_type":"markdown","source":"**He then wisely chooses 5 different models, optimizes the parameters for that exact model and then trains the model with the correct part of the training data.**\n\n**He chooses DecisionTreeClassifier models and mainly adjusts the max_depth parameter to the number of open channels.**\n\n**For the training of the models we will only use the signal feature, since the time feature does not contain any valuable info, it is simply the linear increasing time that belongs to the measurement of the signal curve.**\n\n**Let's go:**","metadata":{}},{"cell_type":"markdown","source":"## 6.1 1 slow open channel\n\n**Only 0 or 1 channels are open in this time window, hence we only need max_depth = 1.**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport graphviz\nfrom sklearn import tree","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 slow open channel\n\na =  0\nb =  500000\nc =  500000\nd = 1000000\n\nX_train = np.concatenate([train.signal.values[a:b],train.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train.open_channels.values[a:b],train.open_channels.values[c:d]]).reshape((-1,1))\n\nmodel_1_slow_channel = tree.DecisionTreeClassifier(max_depth=1)\nmodel_1_slow_channel.fit(X_train,y_train)\n\nprint('Training model_1_slow_open_channel...')\npreds = model_1_slow_channel.predict(X_train)\n\n\nprint('has f1 validation score =', f1_score(y_train,preds, average='macro'))\n\n\n#tree_graph = tree.export_graphviz(model_1_slow_channel, out_file=None, max_depth = 10,\n#    impurity = False, feature_names = ['signal'], class_names = ['0', '1'],\n#    rounded = True, filled= True )\n#graphviz.Source(tree_graph)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2 1 fast open channel\n\n**Again only 0 or 1 channels are open in that time window, hence we use max_depth = 1 again.**","metadata":{}},{"cell_type":"code","source":"a = 1000000\nb = 1500000\n\nc = 3000000 \nd = 3500000\n\nX_train = np.concatenate([train.signal.values[a:b],train.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train.open_channels.values[a:b],train.open_channels.values[c:d]]).reshape((-1,1))\n\nmodel_1_fast_channel = tree.DecisionTreeClassifier(max_depth=1)\n\nmodel_1_fast_channel.fit(X_train, y_train)\n\nprint('Training model_1_fast_channel...')\npreds = model_1_fast_channel.predict(X_train)\n\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\n#tree_graph = tree.export_graphviz(clf1f, out_file=None, max_depth = 10,\n#    impurity = False, feature_names = ['signal'], class_names = ['0', '1'],\n#    rounded = True, filled= True )\n#graphviz.Source(tree_graph) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.3 3 open channels","metadata":{}},{"cell_type":"markdown","source":"**In these time windows there are 0,1,2 or 3 open channels, hence we need max_depth = 4.**\n\n**He uses max_leaf_nodes = 4 and get's a f1 score of 0.9321.**\n\n**I replaced max_leaf_nodes = 4 by max_depth = 4 and got 0.9454,  so I stick with max_depth.**","metadata":{}},{"cell_type":"code","source":"a = 1500000 \nb = 2000000\n\nc = 3500000 \nd = 4000000\n\nX_train = np.concatenate([train.signal.values[a:b],train.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train.open_channels.values[a:b],train.open_channels.values[c:d]]).reshape((-1,1))\n\nmodel_3_channels = tree.DecisionTreeClassifier(max_depth=4)\nmodel_3_channels.fit(X_train,y_train)\nprint('Training model_3_open_channels')\n\npreds = model_3_channels.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\n#tree_graph = tree.export_graphviz(clf3, out_file=None, max_depth = 10,\n#    impurity = False, feature_names = ['signal'], class_names = ['0', '1','2','3'],\n#    rounded = True, filled= True )\n#graphviz.Source(tree_graph) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.4 5 open channels","metadata":{}},{"cell_type":"markdown","source":"**In these time windows there are 0,1,2,3,4,5 or 6 open channels,  he uses max_leaf_nodes = 6, probably because there is only one dip down to 0 channels in the second time window from 400 to 450.**\n\n**I again tried replacing max_leaf_nodes with max_depth, and again my F1 score got slightly better when using max_depth.**\n\n**And here I tried max_depth = 6 and max_depth = 7, and the F1 score with max_depth = 7  was better at the 4th digit after the comma.**\n\n**But so far we are only calculating the F1-score with the data we used to train the model,  hence using max_depth = 7 instead of 6 can lead to overfitting, which results in a better score here on the X_train data, but will result in a worse score later on the unknown test data.**\n\n**Hence I will use the smaller value of max_depth to prevent overfitting.**","metadata":{}},{"cell_type":"code","source":"a = 2500000\nb = 3000000\n\nc = 4000000 \nd = 4500000\n\n\nX_train = np.concatenate([train.signal.values[a:b],train.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train.open_channels.values[a:b],train.open_channels.values[c:d]]).reshape((-1,1))\n\nmodel_5_channels = tree.DecisionTreeClassifier(max_depth=6)\nmodel_5_channels.fit(X_train, y_train)\nprint('Training model_5_open_channels')\npreds = model_5_channels.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\n#tree_graph = tree.export_graphviz(clf5, out_file=None, max_depth = 10,\n#    impurity = False, feature_names = ['signal'], class_names = ['0', '1','2','3','4','5'],\n#    rounded = True, filled= True )\n#graphviz.Source(tree_graph) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.5 10 open channels","metadata":{}},{"cell_type":"markdown","source":"**Again the same effect was observable:  using max_depth instead of max_leaf_nodes yields a slightly better F1 score.**\n\n**He used max_leaf_nodes = 8,  I tried max_depth = 9, because there are 2,3,4,5,6,7,8,9,10 open channels, but it only dips down to 2 channels one time per time window.**\n\n**Using max_depth = 9 yielded F1: 0.8597**\n\n**Using max_depth = 8 yielded F1: 0.7674**\n\n**Because the F1 score of max_depth = 9 is so much better here, I will use max_depth = 9 and hope it doesnt cause much overfitting.**\n\n**I can still play with these parameters and submit, to see what the public/private score will be in that competition.**","metadata":{}},{"cell_type":"code","source":"a = 2000000\nb = 2500000\n\nc = 4500000 \nd = 5000000\n\nX_train = np.concatenate([train.signal.values[a:b],train.signal.values[c:d]]).reshape((-1,1))\ny_train = np.concatenate([train.open_channels.values[a:b],train.open_channels.values[c:d]]).reshape((-1,1))\n\nmodel_10_channels = tree.DecisionTreeClassifier(max_depth=9)  # max_depth = 9 may be overfitting, try 8 and see if priv/pub score gets better\nmodel_10_channels.fit(X_train, y_train)\n\nprint('Training model_10_open_channels')\npreds = model_10_channels.predict(X_train)\nprint('has f1 validation score =',f1_score(y_train,preds,average='macro'))\n\n#tree_graph = tree.export_graphviz(clf10, out_file=None, max_depth = 10,\n#    impurity = False, feature_names = ['signal'], class_names = [str(x) for x in range(11)],\n#    rounded = True, filled= True )\n#graphviz.Source(tree_graph) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Predict with test data\n\n**I liked his way of loading in the sample_submission.csv,  and then simply replace all the values in that dataframe, and then submit it.**\n\n**By doing this we dont have to construct a new dataframe with the correct column names etc.**\n\n**What we have to do in order to use the 5 separate models correctly, is to assign the 5 different models to the correct parts of the test signal.**\n\n**For the corrected train signal curve we have trained the following 5 models for the different parts of the train signal curve:**\n\n![](https://i.imgur.com/0kcC2xQ.png)","metadata":{}},{"cell_type":"markdown","source":"**Hence we look at our test signal curve, try to identify parts that look similar to the train signal curve, and then assign that corresponding model to that part of the test signal curve.**\n\n**Then we end up with the following image:**\n\n![](https://i.imgur.com/sE6vmMj.png)","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\n\na = 100000\n\n# part 1\nsub.iloc[0*a:1*a,1] = model_1_slow_channel.predict(test.signal.values[0*a:1*a].reshape((-1,1)))\n\n# part 2\nsub.iloc[1*a:2*a,1] = model_3_channels.predict(test.signal.values[1*a:2*a].reshape((-1,1)))\n\n# part 3\nsub.iloc[2*a:3*a,1] = model_5_channels.predict(test.signal.values[2*a:3*a].reshape((-1,1)))\n\n# part 4\nsub.iloc[3*a:4*a,1] = model_1_slow_channel.predict(test.signal.values[3*a:4*a].reshape((-1,1)))\n\n# part 5\nsub.iloc[4*a:5*a,1] = model_1_fast_channel.predict(test.signal.values[4*a:5*a].reshape((-1,1)))\n\n# part 6\nsub.iloc[5*a:6*a,1] = model_10_channels.predict(test.signal.values[5*a:6*a].reshape((-1,1)))\n\n# part 7\nsub.iloc[6*a:7*a,1] = model_5_channels.predict(test.signal.values[6*a:7*a].reshape((-1,1)))\n\n# part 8\nsub.iloc[7*a:8*a,1] = model_10_channels.predict(test.signal.values[7*a:8*a].reshape((-1,1)))\n\n# part 9\nsub.iloc[8*a:9*a,1] = model_1_slow_channel.predict(test.signal.values[8*a:9*a].reshape((-1,1)))\n\n# part 10\nsub.iloc[9*a:10*a,1] = model_3_channels.predict(test.signal.values[9*a:10*a].reshape((-1,1)))\n\n# part 11\nsub.iloc[10*a:20*a,1] = model_1_slow_channel.predict(test.signal.values[10*a:20*a].reshape((-1,1)))\n\nprint(\"training successful!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's plot the predictions to see if it worked:**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,5))\nres = 1000\nlet = ['A','B','C','D','E','F','G','H','I','J']\nplt.plot(range(0,test.shape[0],res),sub.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**And now let's save the predictions and submit :)**","metadata":{}},{"cell_type":"code","source":"sub.to_csv('submission.csv', index = False, float_format='%.4f')\n\nprint(\"submission.csv saved successfully!\")\n\n\n\n#################################################\n# result so far:\n\n# public uses 30% of the test data\n# public score:   0.92874  \n# public rank:  1888/2618\n\n\n# private uses 70% of the test data\n# private score:  0.91612\n# private rank: 1711/2618\n\n#################################################","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# things to do:\n\n# 1.) remove all these warnings, maybe try the df.loc[df.column, 'signal']  alternative\n# 2.) understand how he got the parabola values to work so nicely\n\n# things to improve performance:\n\n# 1.) tweak max_depth  of the decisiontree models, then submit, and see if the private/public score improves","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}