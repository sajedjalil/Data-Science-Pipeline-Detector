{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n* Ref Kernel https://www.kaggle.com/kmat2019/u-net-1d-cnn-with-keras, many thanks @K_mat shared 1DCNN keras version\n* Write the simple pytorch version for U-Net (1D CNN)"},{"metadata":{},"cell_type":"markdown","source":"## Import Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import cohen_kappa_score,f1_score\nfrom sklearn.model_selection import KFold, train_test_split\nfrom keras.callbacks import Callback\ndevice = torch.device(\"cuda\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and Split Dataset\nSimply split the input data into certain length."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/liverpool-ion-switching/train.csv\")\ndf_test = pd.read_csv(\"../input/liverpool-ion-switching/test.csv\")\n\n# I don't use \"time\" feature\ntrain_input = df_train[\"signal\"].values.reshape(-1,4000,1)#number_of_data:1250 x time_step:4000\ntrain_input_mean = train_input.mean()\ntrain_input_sigma = train_input.std()\ntrain_input = (train_input-train_input_mean)/train_input_sigma\ntest_input = df_test[\"signal\"].values.reshape(-1,10000,1)\ntest_input = (test_input-train_input_mean)/train_input_sigma\n\ntrain_target = pd.get_dummies(df_train[\"open_channels\"]).values.reshape(-1,4000,11)#classification\n\nidx = np.arange(train_input.shape[0])\ntrain_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\n\nval_input = train_input[val_idx]\ntrain_input = train_input[train_idx] \nval_target = train_target[val_idx]\ntrain_target = train_target[train_idx] \n\nprint(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Model\nThis section defines U-Net(se-resnet base).\nInput and output of the U-Net are follows:\n* Input: 4000 time steps of \"signal\"\n* Output: (4000,11) time steps of \"open_channels\""},{"metadata":{"trusted":true},"cell_type":"code","source":"class conbr_block(nn.Module):\n    def __init__(self, in_layer, out_layer, kernel_size, stride, dilation):\n        super(conbr_block, self).__init__()\n\n        self.conv1 = nn.Conv1d(in_layer, out_layer, kernel_size=kernel_size, stride=stride, dilation = dilation, padding = 3, bias=True)\n        self.bn = nn.BatchNorm1d(out_layer)\n        self.relu = nn.ReLU()\n    \n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        out = self.relu(x)\n        \n        return out       \n\nclass se_block(nn.Module):\n    def __init__(self,in_layer, out_layer):\n        super(se_block, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_layer, out_layer//8, kernel_size=1, padding=0)\n        self.conv2 = nn.Conv1d(out_layer//8, in_layer, kernel_size=1, padding=0)\n        self.fc = nn.Linear(1,out_layer//8)\n        self.fc2 = nn.Linear(out_layer//8,out_layer)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self,x):\n\n        x_se = nn.functional.adaptive_avg_pool1d(x,1)\n        x_se = self.conv1(x_se)\n        x_se = self.relu(x_se)\n        x_se = self.conv2(x_se)\n        x_se = self.sigmoid(x_se)\n        \n        x_out = torch.add(x, x_se)\n        return x_out\n\nclass re_block(nn.Module):\n    def __init__(self, in_layer, out_layer, kernel_size, dilation):\n        super(re_block, self).__init__()\n        \n        self.cbr1 = conbr_block(in_layer,out_layer, kernel_size, 1, dilation)\n        self.cbr2 = conbr_block(out_layer,out_layer, kernel_size, 1, dilation)\n        self.seblock = se_block(out_layer, out_layer)\n    \n    def forward(self,x):\n\n        x_re = self.cbr1(x)\n        x_re = self.cbr2(x_re)\n        x_re = self.seblock(x_re)\n        x_out = torch.add(x, x_re)\n        return x_out          \n\nclass UNET_1D(nn.Module):\n    def __init__(self ,input_dim,layer_n,kernel_size,depth):\n        super(UNET_1D, self).__init__()\n        self.input_dim = input_dim\n        self.layer_n = layer_n\n        self.kernel_size = kernel_size\n        self.depth = depth\n        \n        self.AvgPool1D1 = nn.AvgPool1d(input_dim, stride=5)\n        self.AvgPool1D2 = nn.AvgPool1d(input_dim, stride=25)\n        self.AvgPool1D3 = nn.AvgPool1d(input_dim, stride=125)\n        \n        self.layer1 = self.down_layer(self.input_dim, self.layer_n, self.kernel_size,1, 2)\n        self.layer2 = self.down_layer(self.layer_n, int(self.layer_n*2), self.kernel_size,5, 2)\n        self.layer3 = self.down_layer(int(self.layer_n*2)+int(self.input_dim), int(self.layer_n*3), self.kernel_size,5, 2)\n        self.layer4 = self.down_layer(int(self.layer_n*3)+int(self.input_dim), int(self.layer_n*4), self.kernel_size,5, 2)\n        self.layer5 = self.down_layer(int(self.layer_n*4)+int(self.input_dim), int(self.layer_n*5), self.kernel_size,4, 2)\n\n        self.cbr_up1 = conbr_block(int(self.layer_n*7), int(self.layer_n*3), self.kernel_size, 1, 1)\n        self.cbr_up2 = conbr_block(int(self.layer_n*5), int(self.layer_n*2), self.kernel_size, 1, 1)\n        self.cbr_up3 = conbr_block(int(self.layer_n*3), self.layer_n, self.kernel_size, 1, 1)\n        self.upsample = nn.Upsample(scale_factor=5, mode='nearest')\n        self.upsample1 = nn.Upsample(scale_factor=5, mode='nearest')\n        \n        self.outcov = nn.Conv1d(self.layer_n, 11, kernel_size=self.kernel_size, stride=1,padding = 3)\n    \n        \n    def down_layer(self, input_layer, out_layer, kernel, stride, depth):\n        block = []\n        block.append(conbr_block(input_layer, out_layer, kernel, stride, 1))\n        for i in range(depth):\n            block.append(re_block(out_layer,out_layer,kernel,1))\n        return nn.Sequential(*block)\n            \n    def forward(self, x):\n        \n        pool_x1 = self.AvgPool1D1(x)\n        pool_x2 = self.AvgPool1D2(x)\n        pool_x3 = self.AvgPool1D3(x)\n        \n        #############Encoder#####################\n        \n        out_0 = self.layer1(x)\n        out_1 = self.layer2(out_0)\n        \n        x = torch.cat([out_1,pool_x1],1)\n        out_2 = self.layer3(x)\n        \n        x = torch.cat([out_2,pool_x2],1)\n        x = self.layer4(x)\n        \n        #############Decoder####################\n        \n        up = self.upsample1(x)\n        up = torch.cat([up,out_2],1)\n        up = self.cbr_up1(up)\n        \n        up = self.upsample(up)\n        up = torch.cat([up,out_1],1)\n        up = self.cbr_up2(up)\n        \n        up = self.upsample(up)\n        up = torch.cat([up,out_0],1)\n        up = self.cbr_up3(up)\n        \n        out = self.outcov(up)\n        \n        #out = nn.functional.softmax(out,dim=2)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Dataset with augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ION_Dataset(Dataset):\n    def __init__(self, train_input, train_output,mode='train'):\n        self.train_input = train_input\n        self.train_output = train_output\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.train_input)\n    \n    def _augmentations(self,input_data, target_data):\n        #flip\n        if np.random.rand()<0.5:    \n            input_data = input_data[::-1]\n            target_data = target_data[::-1]\n        return input_data, target_data\n    \n    def __getitem__(self, idx):\n        x = self.train_input[idx]\n        y = self.train_output[idx]\n        if self.mode =='train':\n            x,y = self._augmentations(x,y)\n        out_x = torch.tensor(np.transpose(x.copy(),(1,0)), dtype=torch.float)\n        out_y = torch.tensor(np.transpose(y.copy(),(1,0)), dtype=torch.float)\n        return out_x, out_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 8\ntrain = ION_Dataset(train_input, train_target,mode='train')\nvalid = ION_Dataset(val_input, val_target,mode='valid')\n\nx_test = torch.tensor(np.transpose(test_input,(0,2,1)), dtype=torch.float).cuda()\ntest = torch.utils.data.TensorDataset(x_test)\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, y = next(iter(train_loader))\nx_train.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR,StepLR,ReduceLROnPlateau\nimport time\nfrom tqdm import tqdm\n## Hyperparameter\nn_epochs = 100\nlr = 0.001\n\n## Build tensor data for torch\ntrain_preds = np.zeros((int(train_input.shape[0]*train_input.shape[1])))\nval_preds = np.zeros((int(val_input.shape[0]*val_input.shape[1])))\n\ntrain_targets = np.zeros((int(train_input.shape[0]*train_input.shape[1])))\n\navg_losses_f = []\navg_val_losses_f = []\n\n##Loss function\nloss_fn = torch.nn.BCEWithLogitsLoss()\n\n#Build model, initial weight and optimizer\nmodel = UNET_1D(1,128,7,3) #(input_dim, hidden_layer, kernel_size, depth)\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = lr,weight_decay=1e-5) # Using Adam optimizer\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.8, min_lr=1e-8) # Using ReduceLROnPlateau schedule\ntemp_val_loss = 9999999999\n\n\nfor epoch in range(n_epochs):\n    \n    start_time = time.time()\n    model.train()\n    avg_loss = 0.\n    for i, (x_batch, y_batch) in enumerate(train_loader):\n        y_pred = model(x_batch.cuda())\n        \n        loss = loss_fn(y_pred.cpu(), y_batch)\n        \n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n        avg_loss += loss.item()/len(train_loader)\n\n        pred = F.softmax(y_pred, 1).detach().cpu().numpy().argmax(axis=1)\n        train_preds[i * batch_size*train_input.shape[1]:(i+1) * batch_size*train_input.shape[1]] = pred.reshape((-1))\n        train_targets[i * batch_size*train_input.shape[1]:(i+1) * batch_size*train_input.shape[1]] = y_batch.detach().cpu().argmax(axis=1).reshape((-1))\n        del y_pred, loss, x_batch, y_batch, pred\n        \n        \n    model.eval()\n\n    avg_val_loss = 0.\n    for i, (x_batch, y_batch) in enumerate(valid_loader):\n        y_pred = model(x_batch.cuda()).detach()\n\n        avg_val_loss += loss_fn(y_pred.cpu(), y_batch).item() / len(valid_loader)\n        pred = F.softmax(y_pred, 1).detach().cpu().numpy().argmax(axis=1)\n        val_preds[i * batch_size*val_input.shape[1]:(i+1) * batch_size*val_input.shape[1]] = pred.reshape((-1))\n        del y_pred, x_batch, y_batch, pred\n        \n    if avg_val_loss<temp_val_loss:\n        #print ('checkpoint_save')\n        temp_val_loss = avg_val_loss\n        torch.save(model.state_dict(), 'ION_train_checkpoint.pt')\n        \n    train_score = f1_score(train_targets,train_preds,average = 'macro')\n    val_score = f1_score(val_target.argmax(axis=2).reshape((-1)),val_preds,average = 'macro')\n    \n    elapsed_time = time.time() - start_time \n    scheduler.step(avg_val_loss)\n    \n    print('Epoch {}/{} \\t loss={:.4f} \\t train_f1={:.4f} \\t val_loss={:.4f} \\t val_f1={:.4f} \\t time={:.2f}s'.format(\n        epoch + 1, n_epochs, avg_loss,train_score, avg_val_loss,val_score, elapsed_time))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"VALIDATION_SCORE (QWK): \", cohen_kappa_score(val_target.argmax(axis=2).reshape((-1)),val_preds, weights=\"quadratic\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"VALIDATION_SCORE (F1): \", f1_score(val_target.argmax(axis=2).reshape((-1)),val_preds ,average = 'macro'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict and Submit\nThis is not the main topic of this kernel, so I just round predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(torch.load('ION_train_checkpoint.pt'))\nmodel.eval()\ntest_preds = np.zeros((int(test_input.shape[0]*test_input.shape[1])))\nfor i, x_batch in enumerate(test_loader):\n    y_pred = model(x_batch[0]).detach()\n\n    pred = F.softmax(y_pred, 1).detach().cpu().numpy().argmax(axis=1)\n    test_preds[i * batch_size*test_input.shape[1]:(i+1) * batch_size*test_input.shape[1]] = pred.reshape((-1))\n    del y_pred, x_batch, pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = pd.read_csv(\"../input/liverpool-ion-switching/sample_submission.csv\", dtype={'time':str})\ndf_sub.open_channels = np.array(test_preds,np.int)\ndf_sub.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Next plan\n* K-fold split training\n* Change loss function\n* Try RNN or LSTM model"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}