{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nLGB_SEED = 1337\nXGB_SEED = 5041992\nCB_SEED = 7021991\n\nprint(f'LightGBM Version: {lgb.__version__} - Seed: {LGB_SEED}')\nprint(f'XGBoost Version: {xgb.__version__} - Seed: {XGB_SEED}')\nprint(f'CatBoost Version: {cb.__version__} - Seed: {CB_SEED}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_pickle('../input/ion-switching-preprocessing/train.pkl')\ndf_test = pd.read_pickle('../input/ion-switching-preprocessing/test.pkl')\n    \ndf_test.loc[df_test['model'] == 1.5, 'model'] = 2\ndf_train['is_filtered'] = df_train['is_filtered'].astype(np.uint8)\ndf_train['model'] = df_train['model'].astype(np.uint8)\ndf_test['model'] = df_test['model'].astype(np.uint8)\n    \nfor df in [df_train, df_test]:\n    df.drop(columns=['signal', 'time_scaled', 'signal_processed_kalman', 'batch'], inplace=True)\n    \ndf_train['signal_processed_original'] = df_train['signal_processed']\ndf_test['signal_processed_original'] = df_test['signal_processed']\ndf_train['signal_processed'] = df_train['signal_processed_denoised']\ndf_test['signal_processed'] = df_test['signal_processed_denoised']\n\nfor df in [df_train, df_test]:\n    df.drop(columns=['signal_processed_denoised'], inplace=True)\n\nprint(f'Training Set Shape: {df_train.shape}')\nprint(f'Test Set Shape: {df_test.shape}')\nprint(f'Training Set Memory Usage: {df_train.memory_usage().sum() / 1024 ** 2:.2f} MB')\nprint(f'Test Set Memory Usage: {df_test.memory_usage().sum() / 1024 ** 2:.2f} MB')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **1. Models and Cross-validation**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **1.1. Models**\n\n`model` is a categorical feature created in this preprocessing kernel [here](https://www.kaggle.com/gunesevitan/ion-switching-preprocessing#4.-Models). It is very similar to `batch` feature but it groups similar distributions together.\n\n* Model 0 (Training Batch 0 and 1)\n* Model 1 (Training Batch 2 and 6)\n* Model 1.5 (Test Batch 2 and 3)\n* Model 2 (Training Batch 3 and 7)\n* Model 3 (Training Batch 5 and 8)\n* Model 4 (Training Batch 4 and 9)\n\nThere are **6** different distributions in the dataset. Those distributions are named **Model 0**, **Model 1**, **Model 1.5**, **Model 2**, **Model 3**, **Model 4**. 6th distribution is **Model 1.5** and it only exists in test set. Those distributions are not completely different from each other which makes it possible to train them separately and together. **Model 1.5** doesn't exist in training set so it is named as **Model 2** for training convenience.\n\n* Training them separately works better for **Model 0** and **Model 1** because when those distributions are isolated from rest of the data, models can learn only the relevant things about them. They are both binary classification problems but regression models worked better in every case. When those models are trained with additional data, their cross-validation scores doesn't improve too much.\n* Training them together works better for **Model 2**, **Model 3**, **Model 4** and probably **Model 1.5** because preceeding models' classes also exist in the current models. Data augmentation from the previous models improve the current models' cross-validation scores.\n\nFor example **Model 0**'s and **Model 1**'s data exist in **Model 2**, however **Model 2** doesn't exist in those two. In this case; **Model 0** trained with only **Model 0** data performs better than **Model 0** trained with **Model 0**, **Model 1** and **Model 2** data. However, **Model 0** and **Model 1** exist in **Model 2**, so **Model 2** trained with **Model 0**, **Model 1** and **Model 2** data performs better than **Model 2** trained with only **Model 2** data.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, figsize=(20, 14), dpi=100)\n\ndf_train.set_index('time')['signal_processed'].plot(ax=axes[0])\nfor batch in np.arange(0, 550, 50):\n    axes[0].axvline(batch, color='r', linestyle='--', lw=2)\n    \ndf_test.set_index('time')['signal_processed'].plot(ax=axes[1])\n\nfor batch in np.arange(500, 600, 10):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\nfor batch in np.arange(600, 700, 50):\n    axes[1].axvline(batch, color='r', linestyle='--', lw=2)\n    \naxes[1].axvline(560, color='y', linestyle='dotted', lw=8)\n\nfor i in range(2):            \n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', labelsize=15)\n    axes[i].tick_params(axis='y', labelsize=15)\n    \naxes[0].set_title('Training Set Batches and Models', size=18, pad=18)\naxes[1].set_title('Public/Private Test Set Batches and Models', size=18, pad=18)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though separately trained **Model 2**, **Model 3** and **Model 4** perform worse, they can be used for blending. They can still take the larger models to a next level. In order to make an efficient ensemble, there should be **27** gbdt models and those models are:\n\n#### **Solo Models (15 Models)**\n\n  * Model 0 (Batch 0 and 1) LightGBM, XGBoost, CatBoost\n  * Model 1 (Batch 2 and 6) LightGBM, XGBoost, CatBoost\n  * Model 2 (Batch 3 and 7) LightGBM, XGBoost, CatBoost\n  * Model 3 (Batch 5 and 8) LightGBM, XGBoost, CatBoost\n  * Model 4 (Batch 4 and 9) LightGBM, XGBoost, CatBoost\n  \n#### **Multiple Models (12 Models)**\n\n  * Duo Model 0-1 (Batch 0, 1, 2, 6) LightGBM, XGBoost, CatBoost\n  * Trio Model 0-1-2 (Batch 0, 1, 2, 3, 6, 7) LightGBM, XGBoost, CatBoost\n  * Quad Model 0-1-2-3 (Batch 0, 1, 2, 3, 5, 6, 7, 8) LightGBM, XGBoost, CatBoost\n  * Penta Model 0-1-2-3-4 (All Batches) LightGBM, XGBoost, CatBoost\n\nEvery different distribution is a blend of **6** different model predictions. For **Model 0** and **Model 1** solo models' predictions are given equal weights with multiple models' predictions. For **Model 2**, **Model 3** and **Model 4** multiple models' predictions are given higher weights and solo models' predictions are given lower weights.\n\n* **Model 0** is predicted by Solo Model 0 LightGBM, XGBoost, CatBoost (Equal Weights) and Duo Model LightGBM, XGBoost, CatBoost (Equal Weights)\n* **Model 1** is predicted by Solo Model 1 LightGBM, XGBoost, CatBoost (Equal Weights) and Duo Model LightGBM, XGBoost, CatBoost (Equal Weights)\n* **Model 2** and **Model 1.5** are predicted by Solo Model 2 LightGBM, XGBoost, CatBoost (Lower Weights) and Trio Model LightGBM, XGBoost, CatBoost (Higher Weights)\n* **Model 3** is predicted by Solo Model 3 LightGBM, XGBoost, CatBoost (Lower Weights) and Quad Model LightGBM, XGBoost, CatBoost (Higher Weights)\n* **Model 4** is predicted by Solo Model 4 LightGBM, XGBoost, CatBoost (Lower Weights) and Penta Model LightGBM, XGBoost, CatBoost (Higher Weights)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **1.2. Cross-validation**\n\nSolo and multiple model setup requires two different cross-validation techniques but they have the same logic. That logic is training in one batch and validate on the other similar batch. It is very simple for solo models and it can be implemented with regular **2** `KFold`. For multiple models, `StratifiedKFold` has to be used because training and validation sets have to be stratified on `model` feature.\n\n#### **Solo Models' Cross-validation**\n\n* Train on Batch **0** - Validate on Batch **1** then Train on Batch **1** - Validate on Batch **0** (**Solo Model 0**)\n* Train on Batch **2** - Validate on Batch **6** then Train on Batch **6** - Validate on Batch **2** (**Solo Model 1**)\n* Train on Batch **3** - Validate on Batch **7** then Train on Batch **7** - Validate on Batch **3** (**Solo Model 2**)\n* Train on Batch **5** - Validate on Batch **8** then Train on Batch **8** - Validate on Batch **5** (**Solo Model 3**)\n* Train on Batch **4** - Validate on Batch **9** then Train on Batch **9** - Validate on Batch **4** (**Solo Model 4**)\n\n#### **Multiple Models' Cross-validation**\n\n* Train on Batch **0**, **2** - Validate on Batch **1**, **6** then Train on Batch **1**, **6** - Validate on Batch **0**, **2** (**Duo Model**)\n* Train on Batch **0**, **2**, **3** - Validate on Batch **1**, **6**, **7** then Train on Batch **1**, **6**, **7** - Validate on Batch **0**, **2**, **3** (**Trio Model**)\n* Train on Batch **0**, **2**, **3**, **5** - Validate on Batch **1**, **6**, **7**, **8** then Train on Batch **1**, **6**, **7**, **8** - Validate on Batch **0**, **2**, **3**, **5** (**Quad Model**)\n* Train on Batch **0**, **2**, **3**, **4**, **5** - Validate on Batch **1**, **6**, **7**, **8**, **9** then Train on Batch **1**, **6**, **7**, **8**, **9** - Validate on Batch **0**, **2**, **3**, **4**, **5** (**Penta Model**)\n\nI believe this is the perfect cross-validation technique for this competition because batches are independent. It can observe how good the models are generalizing on different similar distributions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **2. Features**\nAll of the models are trained with different features for the sake of diversity but some of the features are too good that they can't be changed. Those are the type of features used in the models:\n\n* `signal_processed` (Drift, ghost drift and electric noise removed `signal`)\n* `model`(A categorical feature which identifies the 5 different distributions)\n* Lag and Lead Features (Features created with `shift`, `diff` and `pct_change`)\n* Brute Forced Rolling Window Features (Features created with both centered and regular rolling window between `5` to `200` window sizes) \n* Transformation Features on `signal_processed` (Features created with `square`, `square root`, `cube`, `cube root`, `abs`, `exp` and etc.)\n* Features that don't make any sense but work on synthetic data (Fraction of `signal_processed`)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## **3. Solo Models**\n`MultiModelGBDTEnsemble` is a wrapper that incorporates the cross-validation stated above for solo models. Solo models are good at predicting binary distributions which are **Model 0** and **Model 1**, but they are bad at predicting **Model 2**, **Model 3**, and **Model 4**.\n\nThe wrapper has LightGBM, XGBoost and CatBoost models. It can be run with `LightGBM`, `XGBoost` or `CatBoost` parameter so the corresponding model trains and predicts the given distribution. If `Blend` is selected for `model` parameter, `MultiModelGBDTEnsemble` runs all the models together and then blends their scores equally.\n\nAfter training and predicting the given distribution, `MultiModelGBDTEnsemble` plots the feature importance of model(s) by gain.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiModelGBDTEnsemble:\n        \n    def __init__(self,lgb_parameters, xgb_parameters, cb_parameters, model):  \n        \n        self.K = 2\n        self.kf = KFold(n_splits=self.K)\n        self.model = model\n        \n        self.lgb_parameters = lgb_parameters        \n        self.xgb_parameters = xgb_parameters        \n        self.cb_parameters = cb_parameters\n        \n    def _lightgbm_train_and_predict(self, X_train, y_train, X_test, weight, model):\n        \n        print(f'\\n########## Running Solo LightGBM Model {model} ##########')\n        \n        self.lgb_scores = []\n        self.lgb_oof = np.zeros(X_train.shape[0])\n        self.lgb_y_pred = pd.DataFrame(np.zeros((X_test.shape[0], self.K)), columns=[f'Fold_{i}_Predictions' for i in range(1, self.K + 1)]) \n        self.lgb_importance = pd.DataFrame(np.zeros((X_train.shape[1], self.K)), columns=[f'Fold_{i}_Importance' for i in range(1, self.K + 1)], index=X_train.columns)\n        \n        for fold, (trn_idx, val_idx) in enumerate(self.kf.split(X_train, y_train), 1):\n            print(f'\\nFold {fold}')\n\n            lgb_trn_data = lgb.Dataset(X_train.iloc[trn_idx, :], label=y_train.iloc[trn_idx], weight=weight[trn_idx])\n            lgb_val_data = lgb.Dataset(X_train.iloc[val_idx, :], label=y_train.iloc[val_idx])    \n            lgb_model = lgb.train(self.lgb_parameters, lgb_trn_data, valid_sets=[lgb_trn_data, lgb_val_data], verbose_eval=50)\n\n            lgb_oof_predictions = lgb_model.predict(X_train.iloc[val_idx, :], num_iteration=lgb_model.best_iteration)\n            self.lgb_oof[val_idx] = lgb_oof_predictions\n            df_train.loc[X_train.iloc[val_idx, :].index, 'lgb_solo_model_oof'] = lgb_oof_predictions\n\n            lgb_test_predictions = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n            self.lgb_y_pred.iloc[:, fold - 1] = lgb_test_predictions\n            df_test.loc[X_test.index, f'fold{fold}_lgb_solo_model_predictions'] = lgb_test_predictions\n\n            self.lgb_importance.iloc[:, fold - 1] = lgb_model.feature_importance(importance_type='gain')\n            lgb_score = f1_score(y_train.iloc[val_idx].values, np.round(np.clip(lgb_oof_predictions, y_train.min(), y_train.max())), average='macro')\n            self.lgb_scores.append(lgb_score)            \n            print('\\nSolo LGB Fold {} Macro F1-Score {}\\n'.format(fold, lgb_score))\n            \n        print('--------------------')\n        print(f'Solo LGB Mean Macro F1-Score {np.mean(self.lgb_scores):.6} [STD:{np.std(self.lgb_scores):.6}]')\n        print(f'Solo LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_train, np.round(np.clip(self.lgb_oof, y_train.min(), y_train.max())), average=\"macro\"):.6}')\n        print('--------------------')\n        \n    def _xgboost_train_and_predict(self, X_train, y_train, X_test, num_boosting_round, early_stopping_rounds, weight, model):\n        \n        print(f'\\n########## Running Solo XGBoost Model {model} ##########')\n        \n        self.xgb_scores = []\n        self.xgb_oof = np.zeros(X_train.shape[0])\n        self.xgb_y_pred = pd.DataFrame(np.zeros((X_test.shape[0], self.K)), columns=[f'Fold_{i}_Predictions' for i in range(1, self.K + 1)])\n        self.xgb_importance = pd.DataFrame(np.zeros((X_train.shape[1], self.K)), columns=[f'Fold_{i}_Importance' for i in range(1, self.K + 1)], index=X_train.columns)\n        \n        for fold, (trn_idx, val_idx) in enumerate(self.kf.split(X_train, y_train), 1):\n            print(f'\\nFold {fold}')\n            \n            xgb_trn_data = xgb.DMatrix(X_train.iloc[trn_idx, :], label=y_train.iloc[trn_idx], weight=weight[trn_idx])\n            xgb_val_data = xgb.DMatrix(X_train.iloc[val_idx, :], label=y_train.iloc[val_idx])\n            xgb_model = xgb.train(self.xgb_parameters, xgb_trn_data, num_boosting_round, evals=[(xgb_trn_data, 'train'), (xgb_val_data, 'val')], verbose_eval=50, early_stopping_rounds=early_stopping_rounds)\n\n            xgb_oof_predictions = xgb_model.predict(xgb.DMatrix(X_train.iloc[val_idx, :]))\n            self.xgb_oof[val_idx] = xgb_oof_predictions\n            df_train.loc[X_train.iloc[val_idx, :].index, 'xgb_solo_model_oof'] = xgb_oof_predictions\n\n            xgb_test_predictions = xgb_model.predict(xgb.DMatrix(X_test))\n            self.xgb_y_pred.iloc[:, fold - 1] = xgb_test_predictions\n            df_test.loc[X_test.index, f'fold{fold}_xgb_solo_model_predictions'] = xgb_test_predictions\n\n            self.xgb_importance.iloc[:, fold - 1] = list(xgb_model.get_score(importance_type='gain').values())\n            xgb_score = f1_score(y_train.iloc[val_idx].values, np.round(np.clip(xgb_oof_predictions, y_train.min(), y_train.max())), average='macro')\n            self.xgb_scores.append(xgb_score)            \n            print('Solo XGB Fold {} Macro F1-Score {}\\n'.format(fold, xgb_score))\n            \n        print('--------------------')\n        print(f'Solo XGB Mean Macro F1-Score {np.mean(self.xgb_scores):.6} [STD:{np.std(self.xgb_scores):.6}]')\n        print(f'Solo XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_train, np.round(np.clip(self.xgb_oof, y_train.min(), y_train.max())), average=\"macro\"):.6}')\n        print('--------------------')\n        \n    def _catboost_train_and_predict(self, X_train, y_train, X_test, weight, model):\n        \n        print(f'\\n########## Running Solo CatBoost Model {model} ##########')\n        \n        self.cb_scores = []      \n        self.cb_oof = np.zeros(X_train.shape[0])        \n        self.cb_y_pred = pd.DataFrame(np.zeros((X_test.shape[0], self.K)), columns=[f'Fold_{i}_Predictions' for i in range(1, self.K + 1)]) \n        self.cb_importance = pd.DataFrame(np.zeros((X_train.shape[1], self.K)), columns=[f'Fold_{i}_Importance' for i in range(1, self.K + 1)], index=X_train.columns)\n        \n        for fold, (trn_idx, val_idx) in enumerate(self.kf.split(X_train, y_train), 1):\n            print(f'\\nFold {fold}')\n            \n            cb_trn_data = cb.Pool(X_train.iloc[trn_idx, :], label=y_train.iloc[trn_idx], weight=weight[trn_idx]) \n            cb_val_data = cb.Pool(X_train.iloc[val_idx, :], label=y_train.iloc[val_idx])            \n            cb_model = cb.CatBoostRegressor(**self.cb_parameters)\n            cb_model.fit(cb_trn_data)\n            \n            cb_oof_predictions = cb_model.predict(cb_val_data)\n            self.cb_oof[val_idx] = cb_oof_predictions\n            df_train.loc[X_train.iloc[val_idx, :].index, 'cb_solo_model_oof'] = cb_oof_predictions\n            \n            cb_test_predictions = cb_model.predict(cb.Pool(X_test))\n            self.cb_y_pred.iloc[:, fold - 1] = cb_test_predictions\n            df_test.loc[X_test.index, f'fold{fold}_cb_solo_model_predictions'] = cb_test_predictions\n            \n            self.cb_importance.iloc[:, fold - 1] = cb_model.get_feature_importance()\n            cb_score = f1_score(y_train.iloc[val_idx].values, np.round(np.clip(cb_oof_predictions, y_train.min(), y_train.max())), average='macro')\n            self.cb_scores.append(cb_score)            \n            print('\\nSolo CB Fold {} Macro F1-Score {}\\n'.format(fold, cb_score))\n            \n        print('--------------------')\n        print(f'Solo CB Mean Macro F1-Score {np.mean(self.cb_scores):.6} [STD:{np.std(self.cb_scores):.6}]')\n        print(f'Solo CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_train, np.round(np.clip(self.cb_oof, y_train.min(), y_train.max())), average=\"macro\"):.6}')\n        print('--------------------')\n                \n    def train_and_predict(self, \n                          lgb_X_train, lgb_y_train, lgb_X_test,\n                          xgb_X_train, xgb_y_train, xgb_X_test,\n                          cb_X_train, cb_y_train, cb_X_test,\n                          num_boosting_round=100, early_stopping_rounds=25,\n                          lgb_sample_weight=None, xgb_sample_weight=None, cb_sample_weight=None, model=None):\n\n        if self.model == 'LightGBM':\n            self._lightgbm_train_and_predict(lgb_X_train, lgb_y_train, lgb_X_test, weight=lgb_sample_weight, model=model)\n        elif self.model == 'XGBoost':\n            self._xgboost_train_and_predict(xgb_X_train, xgb_y_train, xgb_X_test, num_boosting_round=num_boosting_round, early_stopping_rounds=early_stopping_rounds, weight=xgb_sample_weight, model=model)\n        elif self.model == 'CatBoost':\n            self._catboost_train_and_predict(cb_X_train, cb_y_train, cb_X_test, weight=cb_sample_weight, model=model)\n        elif self.model == 'Blend':\n            self._lightgbm_train_and_predict(lgb_X_train, lgb_y_train, lgb_X_test, weight=lgb_sample_weight, model=model)\n            self._xgboost_train_and_predict(xgb_X_train, xgb_y_train, xgb_X_test, num_boosting_round=num_boosting_round, early_stopping_rounds=early_stopping_rounds, weight=xgb_sample_weight, model=model)\n            self._catboost_train_and_predict(cb_X_train, cb_y_train, cb_X_test, weight=cb_sample_weight, model=model)\n            \n            self.blend_oof = (self.lgb_oof * 0.34) + (self.xgb_oof * 0.33) + (self.cb_oof * 0.33)\n            print(f'Solo Model {model} Equal Blend OOF (Rounded) Macro F1-Score {f1_score(lgb_y_train, np.round(np.clip(self.blend_oof, lgb_y_train.min(), lgb_y_train.max())), average=\"macro\")}\\n')\n            \n    def plot_importance(self):\n        \n        if self.model == 'Blend':\n            self.lgb_importance['Mean_Importance'] = self.lgb_importance.sum(axis=1) / self.K\n            self.lgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n            self.xgb_importance['Mean_Importance'] = self.xgb_importance.sum(axis=1) / self.K\n            self.xgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n            self.cb_importance['Mean_Importance'] = self.cb_importance.sum(axis=1) / self.K\n            self.cb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n            \n            fig, axs = plt.subplots(nrows=3, figsize=(20, 40))\n            \n            sns.barplot(x='Mean_Importance', y=self.lgb_importance.index, data=self.lgb_importance, ax=axs[0])\n            sns.barplot(x='Mean_Importance', y=self.xgb_importance.index, data=self.xgb_importance, ax=axs[1])\n            sns.barplot(x='Mean_Importance', y=self.cb_importance.index, data=self.cb_importance, ax=axs[2])\n            \n            for i in range(3):\n                axs[i].set_xlabel('')\n                axs[i].tick_params(axis='x', labelsize=20)\n                axs[i].tick_params(axis='y', labelsize=20)\n                \n            axs[0].set_title(f'Solo LightGBM Feature Importance (Gain)', size=20, pad=20)\n            axs[1].set_title(f'Solo XGBoost Feature Importance (Gain)', size=20, pad=20)\n            axs[2].set_title(f'Solo CatBoost Feature Importance (Gain)', size=20, pad=20)\n            \n        else:\n            plt.figure(figsize=(20, 6))\n        \n        if self.model == 'LightGBM':\n            self.lgb_importance['Mean_Importance'] = self.lgb_importance.sum(axis=1) / self.K\n            self.lgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n            sns.barplot(x='Mean_Importance', y=self.lgb_importance.index, data=self.lgb_importance)\n        elif self.model == 'XGBoost':\n            self.xgb_importance['Mean_Importance'] = self.xgb_importance.sum(axis=1) / self.K\n            self.xgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n            sns.barplot(x='Mean_Importance', y=self.xgb_importance.index, data=self.xgb_importance)\n        elif self.model == 'CatBoost':\n            self.cb_importance['Mean_Importance'] = self.cb_importance.sum(axis=1) / self.K\n            self.cb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\n            sns.barplot(x='Mean_Importance', y=self.cb_importance.index, data=self.cb_importance)\n        \n        if self.model != 'Blend':\n            plt.xlabel('')\n            plt.tick_params(axis='x', labelsize=20)\n            plt.tick_params(axis='y', labelsize=20)\n            plt.title(f'Solo {self.model} Feature Importance (Gain)', size=20, pad=20)\n\n        plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.1. Solo Model 0**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### SOLO MODEL 0 ####################\n\nprint('#################### SOLO MODEL 0 ####################')\nprint('------------------------------------------------------')\n\n########## SOLO MODEL 0 LGB ##########\n\nmodel0_lgb_features = ['signal_processed']\nmodel0_lgb_X_train = df_train[df_train['model'] == 0][model0_lgb_features].copy(deep=True)\nmodel0_lgb_y_train = df_train[df_train['model'] == 0]['open_channels'].copy(deep=True)\nmodel0_lgb_X_test = df_test[df_test['model'] == 0][model0_lgb_features].copy(deep=True)\n\nmodel0_lgb_sample_weight = model0_lgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel0_lgb_sample_weight.loc[model0_lgb_sample_weight == 0] = 1\nmodel0_lgb_sample_weight.loc[model0_lgb_sample_weight == 1] = 1\n\nfor df in [model0_lgb_X_train, model0_lgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_mean_50'] = df['signal_processed'].rolling(window=50, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_center_std_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_mean_70_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=70, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_center_max_25_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=25, min_periods=1, center=True).max()\n    df['signal_processed_diff+1'] = df['signal_processed'].diff(1)\n    df['signal_processed_rolling_forward_center_min_35'] = df['signal_processed'].rolling(window=35, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_35'] = df['signal_processed'].rolling(window=35, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_maxmin_ratio_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).max() / df['signal_processed'].rolling(window=15, min_periods=1, center=False).min()\n\nmodel0_lgb_parameters = {\n    'num_iterations': 250,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 7, \n    'learning_rate': 0.075,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.5,\n    'feature_fraction_bynode': 0.5,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0.5,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 0 XGB ##########\n\nmodel0_xgb_features = ['signal_processed']\nmodel0_xgb_X_train = df_train[df_train['model'] == 0][model0_xgb_features].copy(deep=True)\nmodel0_xgb_y_train = df_train[df_train['model'] == 0]['open_channels'].copy(deep=True)\nmodel0_xgb_X_test = df_test[df_test['model'] == 0][model0_xgb_features].copy(deep=True)\n\nmodel0_xgb_sample_weight = model0_xgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel0_xgb_sample_weight.loc[model0_xgb_sample_weight == 0] = 1\nmodel0_xgb_sample_weight.loc[model0_xgb_sample_weight == 1] = 1\n\nfor df in [model0_xgb_X_train, model0_xgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_rolling_forward_mean_50'] = df['signal_processed'].rolling(window=50, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_center_std_45'] = df['signal_processed'].rolling(window=45, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_45'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_45'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_maxminrange_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).max() - df['signal_processed'].rolling(window=30, min_periods=1, center=True).min()\n    df['signal_processed_diff+1'] = df['signal_processed'].diff(1).fillna(0)\n    df['signal_processed_rolling_forward_center_mean_50_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=20, min_periods=1, center=False).mean()\n\nmodel0_xgb_parameters = {\n    'n_estimators': 2 ** 8, \n    'learning_rate': 0.05,\n    'colsample_bytree': 0.8, \n    'colsample_bylevel': 0.8,\n    'colsample_bynode': 0.8,\n    'sumbsample': 0.9,\n    'max_depth': 5,\n    'gamma': 0.75,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0.1,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 0 CB ##########\n\nmodel0_cb_features = ['signal_processed']\nmodel0_cb_X_train = df_train[df_train['model'] == 0][model0_cb_features].copy(deep=True)\nmodel0_cb_y_train = df_train[df_train['model'] == 0]['open_channels'].copy(deep=True)\nmodel0_cb_X_test = df_test[df_test['model'] == 0][model0_cb_features].copy(deep=True)\n\nmodel0_cb_sample_weight = model0_cb_y_train.copy(deep=True).reset_index(drop=True)\nmodel0_cb_sample_weight.loc[model0_cb_sample_weight == 0] = 1\nmodel0_cb_sample_weight.loc[model0_cb_sample_weight == 1] = 1\n\nfor df in [model0_cb_X_train, model0_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_rolling_forward_center_mean_40'] = df['signal_processed'].rolling(window=40, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_min_45'] = df['signal_processed'].rolling(window=45, min_periods=1, center=False).min()\n    df['signal_processed_rolling_forward_center_max_50'] = df['signal_processed'].rolling(window=50, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_maxminratio_10'] = df['signal_processed'].rolling(window=25, min_periods=1, center=True).max() / df['signal_processed'].rolling(window=25, min_periods=1, center=True).min()\n    \nmodel0_cb_parameters = {\n    'n_estimators': 250, \n    'learning_rate': 0.06,\n    'depth': 6,\n    'subsample': 0.7,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.6,\n    'l2_leaf_reg': 0.04,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\n########## SOLO MODEL 0 TRAINING ##########\n\nsolo_model0 = MultiModelGBDTEnsemble(lgb_parameters=model0_lgb_parameters,\n                                     xgb_parameters=model0_xgb_parameters, \n                                     cb_parameters=model0_cb_parameters,\n                                     model='Blend')\n\nsolo_model0.train_and_predict(lgb_X_train=model0_lgb_X_train, lgb_y_train=model0_lgb_y_train, lgb_X_test=model0_lgb_X_test,\n                              xgb_X_train=model0_xgb_X_train, xgb_y_train=model0_xgb_y_train, xgb_X_test=model0_xgb_X_test,\n                              cb_X_train=model0_cb_X_train, cb_y_train=model0_cb_y_train, cb_X_test=model0_cb_X_test,\n                              num_boosting_round=300, early_stopping_rounds=15,\n                              lgb_sample_weight=model0_lgb_sample_weight, xgb_sample_weight=model0_xgb_sample_weight, cb_sample_weight=model0_cb_sample_weight,\n                              model=0)\nsolo_model0.plot_importance()\n\ndel model0_lgb_X_train, model0_lgb_y_train, model0_lgb_X_test, model0_lgb_sample_weight, model0_lgb_parameters\ndel model0_xgb_X_train, model0_xgb_y_train, model0_xgb_X_test, model0_xgb_sample_weight, model0_xgb_parameters\ndel model0_cb_X_train, model0_cb_y_train, model0_cb_X_test, model0_cb_sample_weight, model0_cb_parameters\ndel solo_model0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.2. Solo Model 1**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### SOLO MODEL 1 ####################\n\nprint('#################### SOLO MODEL 1 ####################')\nprint('------------------------------------------------------')\n\n########## SOLO MODEL 1 LGB ##########\n\nmodel1_lgb_features = ['signal_processed']\nmodel1_lgb_X_train = df_train[df_train['model'] == 1][model1_lgb_features].copy(deep=True)\nmodel1_lgb_y_train = df_train[df_train['model'] == 1]['open_channels'].copy(deep=True)\nmodel1_lgb_X_test = df_test[df_test['model'] == 1][model1_lgb_features].copy(deep=True)\n\nmodel1_lgb_sample_weight = model1_lgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel1_lgb_sample_weight.loc[model1_lgb_sample_weight == 0] = 1\nmodel1_lgb_sample_weight.loc[model1_lgb_sample_weight == 1] = 1\n\nfor df in [model1_lgb_X_train, model1_lgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_mean_45'] = df['signal_processed'].rolling(window=45, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_std_10'] = df['signal_processed'].rolling(window=20, min_periods=1, center=False).std()\n    df['signal_processed_rolling_forward_center_min_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_maxmindifference_10'] = df['signal_processed'].rolling(window=5, min_periods=1, center=True).max() - df['signal_processed'].rolling(window=5, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_min_15_shift-15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).min().shift(-15)\n    df['signal_processed_rolling_forward_max_15_shift-15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).max().shift(-15)\n    df['signal_processed_rolling_forward_mean_15_shift-15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).mean().shift(-15)\n\nmodel1_lgb_parameters = {\n    'num_iterations': 250,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 5, \n    'learning_rate': 0.085,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'feature_fraction_bynode': 0.5,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0.1,\n    'lambda_l2': 0.5,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 1 XGB ##########\n\nmodel1_xgb_features = ['signal_processed']\nmodel1_xgb_X_train = df_train[df_train['model'] == 1][model1_xgb_features].copy(deep=True)\nmodel1_xgb_y_train = df_train[df_train['model'] == 1]['open_channels'].copy(deep=True)\nmodel1_xgb_X_test = df_test[df_test['model'] == 1][model1_xgb_features].copy(deep=True)\n\nmodel1_xgb_sample_weight = model1_xgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel1_xgb_sample_weight.loc[model1_xgb_sample_weight == 0] = 1\nmodel1_xgb_sample_weight.loc[model1_xgb_sample_weight == 1] = 1\n\nfor df in [model1_xgb_X_train, model1_xgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_median_25'] = df['signal_processed'].rolling(window=25, min_periods=1, center=False).median()\n    df['signal_processed_rolling_forward_std_10'] = df['signal_processed'].rolling(window=25, min_periods=1, center=False).std()\n    df['signal_processed_rolling_forward_center_min_10'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=False).max()\n    df['signal_processed_rolling_forward_center_maxminratio_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).max() / df['signal_processed'].rolling(window=15, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_min_15_shift-15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).min().shift(-15)\n    df['signal_processed_rolling_forward_max_15_shift-15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).max().shift(-15)\n    df['signal_processed_rolling_forward_mean_15_shift-15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).mean().shift(-15)\n\nmodel1_xgb_parameters = {\n    'n_estimators': 2 ** 6, \n    'learning_rate': 0.085,\n    'colsample_bytree': 0.8, \n    'colsample_bylevel': 0.8,\n    'colsample_bynode': 0.7,\n    'sumbsample': 0.8,\n    'max_depth': 6,\n    'gamma': 0.4,\n    'min_child_weight': 1,\n    'lambda': 0.6,\n    'alpha': 0,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 1 CB ##########\n\nmodel1_cb_features = ['signal_processed']\nmodel1_cb_X_train = df_train[df_train['model'] == 1][model1_cb_features].copy(deep=True)\nmodel1_cb_y_train = df_train[df_train['model'] == 1]['open_channels'].copy(deep=True)\nmodel1_cb_X_test = df_test[df_test['model'] == 1][model1_cb_features].copy(deep=True)\n\nmodel1_cb_sample_weight = model1_cb_y_train.copy(deep=True).reset_index(drop=True)\nmodel1_cb_sample_weight.loc[model1_cb_sample_weight == 0] = 1\nmodel1_cb_sample_weight.loc[model1_cb_sample_weight == 1] = 1\n\nfor df in [model1_cb_X_train, model1_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_45'] = df['signal_processed'].rolling(window=50, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_std_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=False).std()\n    df['signal_processed_rolling_forward_min_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=False).min()\n    df['signal_processed_rolling_forward_center_max_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_min_15_shift-15'] = df['signal_processed'].rolling(window=10).min().shift(-10)\n    df['signal_processed_rolling_forward_max_15_shift-15'] = df['signal_processed'].rolling(window=10).max().shift(-10)\n    df['signal_processed_rolling_forward_mean_15_shift-15'] = df['signal_processed'].rolling(window=10).mean().shift(-10)\n\nmodel1_cb_parameters = {\n    'n_estimators': 200, \n    'learning_rate': 0.055,\n    'depth': 6,\n    'subsample': 0.6,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,\n    'l2_leaf_reg': 0.5,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\n########## SOLO MODEL 1 TRAINING ##########\n\nsolo_model1 = MultiModelGBDTEnsemble(lgb_parameters=model1_lgb_parameters,\n                                     xgb_parameters=model1_xgb_parameters, \n                                     cb_parameters=model1_cb_parameters,\n                                     model='Blend')\n\nsolo_model1.train_and_predict(lgb_X_train=model1_lgb_X_train, lgb_y_train=model1_lgb_y_train, lgb_X_test=model1_lgb_X_test,\n                              xgb_X_train=model1_xgb_X_train, xgb_y_train=model1_xgb_y_train, xgb_X_test=model1_xgb_X_test, \n                              cb_X_train=model1_cb_X_train, cb_y_train=model1_cb_y_train, cb_X_test=model1_cb_X_test,                         \n                              num_boosting_round=250, early_stopping_rounds=10,\n                              lgb_sample_weight=model1_lgb_sample_weight, xgb_sample_weight=model1_xgb_sample_weight, cb_sample_weight=model1_cb_sample_weight,\n                              model=1)\nsolo_model1.plot_importance()\n\ndel model1_lgb_X_train, model1_lgb_y_train, model1_lgb_X_test, model1_lgb_sample_weight, model1_lgb_parameters\ndel model1_xgb_X_train, model1_xgb_y_train, model1_xgb_X_test, model1_xgb_sample_weight, model1_xgb_parameters\ndel model1_cb_X_train, model1_cb_y_train, model1_cb_X_test, model1_cb_sample_weight, model1_cb_parameters\ndel solo_model1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.3. Solo Model 2**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### SOLO MODEL 2 ####################\n\nprint('#################### SOLO MODEL 2 ####################')\nprint('------------------------------------------------------')\n\n########## SOLO MODEL 2 LGB ##########\n\nmodel2_lgb_features = ['signal_processed']\nmodel2_lgb_X_train = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)][model2_lgb_features].copy(deep=True)\nmodel2_lgb_y_train = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel2_lgb_X_test = df_test[df_test['model'] == 2][model2_lgb_features].copy(deep=True)\n\nmodel2_lgb_sample_weight = model2_lgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel2_lgb_sample_weight.loc[model2_lgb_sample_weight == 0] = 1\nmodel2_lgb_sample_weight.loc[model2_lgb_sample_weight == 1] = 1\nmodel2_lgb_sample_weight.loc[model2_lgb_sample_weight == 2] = 1\nmodel2_lgb_sample_weight.loc[model2_lgb_sample_weight == 3] = 1\n\nfor df in [model2_lgb_X_train, model2_lgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n    df['signal_processed_rolling_forward_center_mean_45'] = df['signal_processed'].rolling(window=45, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_median_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).median()\n    df['signal_processed_rolling_forward_center_std_20'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_35'] = df['signal_processed'].rolling(window=35, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_median_50_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=50, min_periods=1, center=True).median()\n    df['signal_processed_rolling_forward_min_30_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=30, min_periods=1, center=False).min()\n    df['signal_processed_rolling_forward_maxminratio_50'] = df['signal_processed'].rolling(window=50, min_periods=1, center=False).max() / df['signal_processed'].rolling(window=50, min_periods=1, center=False).min()\n    df['signal_processed_diff+1_rolling_forward_center_mean_20'] = df['signal_processed'].diff(1).fillna(0).rolling(window=20, min_periods=1, center=True).mean()\n\nmodel2_lgb_parameters = {\n    'num_iterations': 250,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 7, \n    'learning_rate': 0.05,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.5,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0,\n    'lambda_l2': 0.05,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 2 XGB ##########\n\nmodel2_xgb_features = ['signal_processed']\nmodel2_xgb_X_train = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)][model2_xgb_features].copy(deep=True)\nmodel2_xgb_y_train = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel2_xgb_X_test = df_test[(df_test['model'] == 2) | (df_test['model'] == 1.5)][model2_xgb_features].copy(deep=True)\n\nmodel2_xgb_sample_weight = model2_xgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel2_xgb_sample_weight.loc[model2_xgb_sample_weight == 0] = 1\nmodel2_xgb_sample_weight.loc[model2_xgb_sample_weight == 1] = 1\nmodel2_xgb_sample_weight.loc[model2_xgb_sample_weight == 2] = 1\nmodel2_xgb_sample_weight.loc[model2_xgb_sample_weight == 3] = 1\n\nfor df in [model2_xgb_X_train, model2_xgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_rolling_forward_center_mean_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_std_25'] = df['signal_processed'].rolling(window=25, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_40'] = df['signal_processed'].rolling(window=40, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).max()\n    \nmodel2_xgb_parameters = {\n    'n_estimators': 2 ** 7, \n    'learning_rate': 0.09,\n    'colsample_bytree': 0.9, \n    'colsample_bylevel': 0.9,\n    'colsample_bynode': 0.8,\n    'sumbsample': 0.6,\n    'max_depth': 6,\n    'gamma': 0.01,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0.005,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 2 CB ##########\n\nmodel2_cb_features = ['signal_processed']\nmodel2_cb_X_train = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)][model2_cb_features].copy(deep=True)\nmodel2_cb_y_train = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel2_cb_X_test = df_test[(df_test['model'] == 2) | (df_test['model'] == 1.5)][model2_cb_features].copy(deep=True)\n\nmodel2_cb_sample_weight = model2_cb_y_train.copy(deep=True).reset_index(drop=True)\nmodel2_cb_sample_weight.loc[model2_cb_sample_weight == 0] = 1\nmodel2_cb_sample_weight.loc[model2_cb_sample_weight == 1] = 1\nmodel2_cb_sample_weight.loc[model2_cb_sample_weight == 2] = 1\nmodel2_cb_sample_weight.loc[model2_cb_sample_weight == 3] = 1\n\nfor df in [model2_cb_X_train, model2_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_rolling_forward_center_mean_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_std_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_35'] = df['signal_processed'].rolling(window=35, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_mean_30_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=30, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_center_min_45_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=45, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_median_45'] = df['signal_processed'].rolling(window=45, min_periods=1, center=True).median()\n    df['signal_processed_rolling_forward_min_25_shift-25'] = df['signal_processed'].rolling(window=25, min_periods=1, center=False).min().shift(-25)\n    df['signal_processed_rolling_forward_max_25_shift-25'] = df['signal_processed'].rolling(window=25, min_periods=1, center=False).max().shift(-25)\n    df['signal_processed_rolling_forward_mean_25_shift-25'] = df['signal_processed'].rolling(window=25, min_periods=1, center=False).mean().shift(-25)\n    df['signal_processed_fraction'] = np.abs(df['signal_processed'] - df['signal_processed'].astype(int))\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    \nmodel2_cb_parameters = {\n    'n_estimators': 300, \n    'learning_rate': 0.08,\n    'depth': 6,\n    'subsample': 0.6,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.8,\n    'l2_leaf_reg': 0.06,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\n########## SOLO MODEL 2 TRAINING ##########\n\nsolo_model2 = MultiModelGBDTEnsemble(lgb_parameters=model2_lgb_parameters,\n                                     xgb_parameters=model2_xgb_parameters, \n                                     cb_parameters=model2_cb_parameters,\n                                     model='Blend')\n\nsolo_model2.train_and_predict(lgb_X_train=model2_lgb_X_train, lgb_y_train=model2_lgb_y_train, lgb_X_test=model2_lgb_X_test,\n                              xgb_X_train=model2_xgb_X_train, xgb_y_train=model2_xgb_y_train, xgb_X_test=model2_xgb_X_test, \n                              cb_X_train=model2_cb_X_train, cb_y_train=model2_cb_y_train, cb_X_test=model2_cb_X_test,\n                              num_boosting_round=300, early_stopping_rounds=25, \n                              lgb_sample_weight=model2_lgb_sample_weight, xgb_sample_weight=model2_xgb_sample_weight, cb_sample_weight=model2_cb_sample_weight, \n                              model=2)\nsolo_model2.plot_importance()\n\ndel model2_lgb_X_train, model2_lgb_y_train, model2_lgb_X_test, model2_lgb_sample_weight, model2_lgb_parameters\ndel model2_xgb_X_train, model2_xgb_y_train, model2_xgb_X_test, model2_xgb_sample_weight, model2_xgb_parameters\ndel model2_cb_X_train, model2_cb_y_train, model2_cb_X_test, model2_cb_sample_weight, model2_cb_parameters\ndel solo_model2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.4. Solo Model 3**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### SOLO MODEL 3 ####################\n\nprint('#################### SOLO MODEL 3 ####################')\nprint('------------------------------------------------------')\n\n########## SOLO MODEL 3 LGB ##########\n\nmodel3_lgb_features = ['signal_processed']\nmodel3_lgb_X_train = df_train[df_train['model'] == 3][model3_lgb_features].copy(deep=True)\nmodel3_lgb_y_train = df_train[df_train['model'] == 3]['open_channels'].copy(deep=True)\nmodel3_lgb_X_test = df_test[df_test['model'] == 3][model3_lgb_features].copy(deep=True)\n\nmodel3_lgb_sample_weight = model3_lgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel3_lgb_sample_weight.loc[model3_lgb_sample_weight == 0] = 1\nmodel3_lgb_sample_weight.loc[model3_lgb_sample_weight == 1] = 1\nmodel3_lgb_sample_weight.loc[model3_lgb_sample_weight == 2] = 1\nmodel3_lgb_sample_weight.loc[model3_lgb_sample_weight == 3] = 1\nmodel3_lgb_sample_weight.loc[model3_lgb_sample_weight == 4] = 1\nmodel3_lgb_sample_weight.loc[model3_lgb_sample_weight == 5] = 1\n\nfor df in [model3_lgb_X_train, model3_lgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_rolling_forward_mean_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_std_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=False).std()\n    df['signal_processed_rolling_forward_center_min_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_35'] = df['signal_processed'].rolling(window=35, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_maxmindifference_35'] = df['signal_processed'].rolling(window=25, min_periods=1, center=False).max() - df['signal_processed'].rolling(window=25, min_periods=1, center=False).min()\n    df['signal_processed_rolling_forward_center_mean_15_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=15, min_periods=1, center=False).mean()\n    df['signal_processed_pctchange-1'] = df['signal_processed'].pct_change(-1)\n    \nmodel3_lgb_parameters = {\n    'num_iterations': 200,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 7, \n    'learning_rate': 0.06,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.8,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0.5,\n    'lambda_l2': 0,\n    'max_depth': -1,   \n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 3 XGB ##########\n\nmodel3_xgb_features = ['signal_processed']\nmodel3_xgb_X_train = df_train[df_train['model'] == 3][model3_xgb_features].copy(deep=True)\nmodel3_xgb_y_train = df_train[df_train['model'] == 3]['open_channels'].copy(deep=True)\nmodel3_xgb_X_test = df_test[df_test['model'] == 3][model3_xgb_features].copy(deep=True)\n\nmodel3_xgb_sample_weight = model3_xgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel3_xgb_sample_weight.loc[model3_xgb_sample_weight == 0] = 1\nmodel3_xgb_sample_weight.loc[model3_xgb_sample_weight == 1] = 1\nmodel3_xgb_sample_weight.loc[model3_xgb_sample_weight == 2] = 1\nmodel3_xgb_sample_weight.loc[model3_xgb_sample_weight == 3] = 1\nmodel3_xgb_sample_weight.loc[model3_xgb_sample_weight == 4] = 1\nmodel3_xgb_sample_weight.loc[model3_xgb_sample_weight == 5] = 1\n\nfor df in [model3_xgb_X_train, model3_xgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_rolling_forward_center_mean_35'] = df['signal_processed'].rolling(window=35, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_std_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_mean_45_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=45, min_periods=1, center=False).mean()\n    \nmodel3_xgb_parameters = {\n    'n_estimators': 2 ** 7, \n    'learning_rate': 0.09,\n    'colsample_bytree': 0.9, \n    'colsample_bylevel': 0.9,\n    'colsample_bynode': 0.9,\n    'sumbsample': 0.6,\n    'max_depth': 6,\n    'gamma': 0,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 3 CB ##########\n\nmodel3_cb_features = ['signal_processed']\nmodel3_cb_X_train = df_train[df_train['model'] == 3][model3_cb_features].copy(deep=True)\nmodel3_cb_y_train = df_train[df_train['model'] == 3]['open_channels'].copy(deep=True)\nmodel3_cb_X_test = df_test[df_test['model'] == 3][model3_cb_features].copy(deep=True)\n\nmodel3_cb_sample_weight = model3_cb_y_train.copy(deep=True).reset_index(drop=True)\nmodel3_cb_sample_weight.loc[model3_cb_sample_weight == 0] = 1\nmodel3_cb_sample_weight.loc[model3_cb_sample_weight == 1] = 1\nmodel3_cb_sample_weight.loc[model3_cb_sample_weight == 2] = 1\nmodel3_cb_sample_weight.loc[model3_cb_sample_weight == 3] = 1\nmodel3_cb_sample_weight.loc[model3_cb_sample_weight == 4] = 1\nmodel3_cb_sample_weight.loc[model3_cb_sample_weight == 5] = 1\n\nfor df in [model3_cb_X_train, model3_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift).fillna(0)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift).fillna(0)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_rolling_forward_center_mean_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_std_30'] = df['signal_processed'].rolling(window=30, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_25'] = df['signal_processed'].rolling(window=25, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_mean_25_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=25, min_periods=1, center=False).mean()\n    df['signal_processed_rolling_forward_min_15_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=15, min_periods=1, center=False).min()\n    df['signal_processed_rolling_forward_center_maxminratio_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).max() / df['signal_processed'].rolling(window=15, min_periods=1, center=True).min()\n    \nmodel3_cb_parameters = {\n    'n_estimators': 550, \n    'learning_rate': 0.07,\n    'depth': 6,\n    'subsample': 0.7,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,    \n    'l2_leaf_reg': 0.05,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\n########## SOLO MODEL 3 TRAINING ##########\n\nsolo_model3 = MultiModelGBDTEnsemble(lgb_parameters=model3_lgb_parameters,\n                                     xgb_parameters=model3_xgb_parameters, \n                                     cb_parameters=model3_cb_parameters,\n                                     model='Blend')\n\nsolo_model3.train_and_predict(lgb_X_train=model3_lgb_X_train, lgb_y_train=model3_lgb_y_train, lgb_X_test=model3_lgb_X_test,\n                              xgb_X_train=model3_xgb_X_train, xgb_y_train=model3_xgb_y_train, xgb_X_test=model3_xgb_X_test, \n                              cb_X_train=model3_cb_X_train, cb_y_train=model3_cb_y_train, cb_X_test=model3_cb_X_test,\n                              num_boosting_round=400, early_stopping_rounds=20, \n                              lgb_sample_weight=model3_lgb_sample_weight, xgb_sample_weight=model3_xgb_sample_weight, cb_sample_weight=model3_cb_sample_weight,\n                              model=3)\nsolo_model3.plot_importance()\n\ndel model3_lgb_X_train, model3_lgb_y_train, model3_lgb_X_test, model3_lgb_sample_weight, model3_lgb_parameters\ndel model3_xgb_X_train, model3_xgb_y_train, model3_xgb_X_test, model3_xgb_sample_weight, model3_xgb_parameters\ndel model3_cb_X_train, model3_cb_y_train, model3_cb_X_test, model3_cb_sample_weight, model3_cb_parameters\ndel solo_model3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.5. Solo Model 4**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### SOLO MODEL 4 ####################\n\nprint('#################### SOLO MODEL 4 ####################')\nprint('------------------------------------------------------')\n\n########## SOLO MODEL 4 LGB ##########\n\nmodel4_lgb_features = ['signal_processed']\nmodel4_lgb_X_train = df_train[df_train['model'] == 4][model4_lgb_features].copy(deep=True)\nmodel4_lgb_y_train = df_train[df_train['model'] == 4]['open_channels'].copy(deep=True)\nmodel4_lgb_X_test = df_test[df_test['model'] == 4][model4_lgb_features].copy(deep=True)\n\nmodel4_lgb_sample_weight = model4_lgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 0] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 1] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 2] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 3] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 4] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 5] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 6] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 7] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 8] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 9] = 1\nmodel4_lgb_sample_weight.loc[model4_lgb_sample_weight == 10] = 1\n\nfor df in [model4_lgb_X_train, model4_lgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2      \n    df['signal_processed_rolling_forward_center_mean_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_min_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_min_35_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=35, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_min_140'] = df['signal_processed'].rolling(window=140, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_max_140'] = df['signal_processed'].rolling(window=140, min_periods=1, center=True).max()\n    \nmodel4_lgb_parameters = {\n    'num_iterations': 500,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 7, \n    'learning_rate': 0.08,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.8,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 4 XGB ##########\n\nmodel4_xgb_features = ['signal_processed']\nmodel4_xgb_X_train = df_train[df_train['model'] == 4][model4_xgb_features].copy(deep=True)\nmodel4_xgb_y_train = df_train[df_train['model'] == 4]['open_channels'].copy(deep=True)\nmodel4_xgb_X_test = df_test[df_test['model'] == 4][model4_xgb_features].copy(deep=True)\n\nmodel4_xgb_sample_weight = model4_xgb_y_train.copy(deep=True).reset_index(drop=True)\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 0] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 1] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 2] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 3] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 4] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 5] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 6] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 7] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 8] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 9] = 1\nmodel4_xgb_sample_weight.loc[model4_xgb_sample_weight == 10] = 1\n\nfor df in [model4_xgb_X_train, model4_xgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n    df['signal_processed_rolling_forward_center_mean_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_std_10'] = df['signal_processed'].rolling(window=10, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_min_5'] = df['signal_processed'].rolling(window=5, min_periods=1, center=True).min()\n    df['signal_processed_rolling_forward_center_mean_45_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=45, min_periods=1, center=True).mean()\n\nmodel4_xgb_parameters = {\n    'n_estimators': 2 ** 7, \n    'learning_rate': 0.08,\n    'colsample_bytree': 0.9, \n    'colsample_bylevel': 0.9,\n    'colsample_bynode': 0.8,\n    'sumbsample': 0.9,\n    'max_depth': 6,\n    'gamma': 0,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0.5,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\n########## SOLO MODEL 4 CB ##########\n\nmodel4_cb_features = ['signal_processed']\nmodel4_cb_X_train = df_train[df_train['model'] == 4][model4_cb_features].copy(deep=True)\nmodel4_cb_y_train = df_train[df_train['model'] == 4]['open_channels'].copy(deep=True)\nmodel4_cb_X_test = df_test[df_test['model'] == 4][model4_cb_features].copy(deep=True)\n\nmodel4_cb_sample_weight = model4_cb_y_train.copy(deep=True).reset_index(drop=True)\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 0] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 1] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 2] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 3] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 4] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 5] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 6] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 7] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 8] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 9] = 1\nmodel4_cb_sample_weight.loc[model4_cb_sample_weight == 10] = 1\n\nfor df in [model4_cb_X_train, model4_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df['signal_processed'].shift(-shift)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n    df['signal_processed_rolling_forward_center_mean_15'] = df['signal_processed'].rolling(window=15, min_periods=1, center=True).mean()\n    df['signal_processed_rolling_forward_center_std_55'] = df['signal_processed'].rolling(window=55, min_periods=1, center=True).std()\n    df['signal_processed_rolling_forward_center_max_20'] = df['signal_processed'].rolling(window=20, min_periods=1, center=True).max()\n    df['signal_processed_rolling_forward_center_median_15_difference'] = df['signal_processed'] - df['signal_processed'].rolling(window=15, min_periods=1, center=True).median()\n\nmodel4_cb_parameters = {\n    'n_estimators': 450, \n    'learning_rate': 0.065,\n    'depth': 6,\n    'subsample': 0.7,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,\n    'l2_leaf_reg': 0.01,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\n########## SOLO MODEL 4 TRAINING ##########\n\nsolo_model4 = MultiModelGBDTEnsemble(lgb_parameters=model4_lgb_parameters,\n                                     xgb_parameters=model4_xgb_parameters, \n                                     cb_parameters=model4_cb_parameters,\n                                     model='Blend')\n\nsolo_model4.train_and_predict(lgb_X_train=model4_lgb_X_train, lgb_y_train=model4_lgb_y_train, lgb_X_test=model4_lgb_X_test,\n                              xgb_X_train=model4_xgb_X_train, xgb_y_train=model4_xgb_y_train, xgb_X_test=model4_xgb_X_test, \n                              cb_X_train=model4_cb_X_train, cb_y_train=model4_cb_y_train, cb_X_test=model4_cb_X_test,\n                              lgb_sample_weight=model4_lgb_sample_weight, xgb_sample_weight=model4_xgb_sample_weight, cb_sample_weight=model4_cb_sample_weight,\n                              num_boosting_round=500, early_stopping_rounds=20,\n                              model=4)\nsolo_model4.plot_importance()\n\ndel model4_lgb_X_train, model4_lgb_y_train, model4_lgb_X_test, model4_lgb_sample_weight, model4_lgb_parameters\ndel model4_xgb_X_train, model4_xgb_y_train, model4_xgb_X_test, model4_xgb_sample_weight, model4_xgb_parameters\ndel model4_cb_X_train, model4_cb_y_train, model4_cb_X_test, model4_cb_sample_weight, model4_cb_parameters\ndel solo_model4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **3.6. Solo Model Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('#################### SOLO MODEL EVALUATION ####################')\nprint('---------------------------------------------------------------')\n\nfor model in range(5):\n    print(f'\\n########## Solo Model {model} ##########')\n    print('----------------------------------')\n    \n    y_true = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    lgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_solo_model_oof']\n    xgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'xgb_solo_model_oof']\n    cb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_solo_model_oof']\n    equal_blend_oof = (lgb_oof * 0.34) + (xgb_oof * 0.33) + (cb_oof * 0.33)\n    \n    print(f'Solo LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(lgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Solo XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(xgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Solo CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(cb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Solo Model {model} Equal Blend OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(equal_blend_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    \nprint(f'\\n########## Solo Model Global ##########')\nprint('---------------------------------------')\n\nglobal_y_true = df_train.loc[df_train['is_filtered'] == 0, 'open_channels']\nglobal_lgb_oof = df_train.loc[df_train['is_filtered'] == 0, 'lgb_solo_model_oof']\nglobal_xgb_oof = df_train.loc[df_train['is_filtered'] == 0, 'xgb_solo_model_oof']\nglobal_cb_oof = df_train.loc[df_train['is_filtered'] == 0, 'cb_solo_model_oof']\nglobal_equal_blend_oof = (global_lgb_oof * 0.34) + (global_xgb_oof * 0.33) + (global_cb_oof * 0.33)\n\nprint(f'Solo LGB Model Global OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_lgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Solo XGB Model Global OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_xgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Solo CB Model Global OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_cb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Solo Model Global Equal Blend OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_equal_blend_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\n\ndel global_y_true, global_lgb_oof, global_xgb_oof, global_cb_oof, global_equal_blend_oof","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4. Duo Models**\nDuo models are trained on only **Model 0** and **Model 1** together. The objective of duo models is maximizing the F1 score calculated on those two groups together because they have the same classes. Duo models are performing slightly better than solo models in both of those groups, and since they are trained on a different distribution, they are useful for blending. Their weights in blends are equal with solo models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **4.1. Duo Model LightGBM**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"################### DUO MODEL 0-1 ####################\n\nprint('#################### DUO MODEL 0-1 ####################')\nprint('-------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## DUO MODEL 0-1 LGB ##########\n\nprint(f'\\n########## Running Duo LightGBM Model 0-1 ##########')\n\nmodel01_lgb_features = ['signal_processed', 'model']\nmodel01_lgb_X_train = df_train[df_train['model'] <= 1][model01_lgb_features].copy(deep=True)\nmodel01_lgb_y_train = df_train[df_train['model'] <= 1]['open_channels'].copy(deep=True)\nmodel01_lgb_X_test = df_test[df_train['model'] <= 1][model01_lgb_features].copy(deep=True)\n\nfor df in [model01_lgb_X_train, model01_lgb_X_test]:\n    for shift in range(1, 3):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_std_15'] = df.groupby('model')['signal_processed'].rolling(window=15, min_periods=1, center=False).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n\nmodel01_lgb_parameters = {\n    'num_iterations': 500,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 6, \n    'learning_rate': 0.03,\n    'bagging_fraction': 0.6,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'feature_fraction_bynode': 0.7,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0.6,\n    'lambda_l2': 0.6,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\nmodel01_lgb_scores = []\nmodel01_lgb_oof = np.zeros(model01_lgb_X_train.shape[0])\nmodel01_lgb_y_pred = pd.DataFrame(np.zeros((model01_lgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel01_lgb_importance = pd.DataFrame(np.zeros((model01_lgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model01_lgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model01_lgb_X_train, model01_lgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model01_lgb_trn_data = lgb.Dataset(model01_lgb_X_train.iloc[trn_idx, :], label=model01_lgb_y_train.iloc[trn_idx], categorical_feature=['model'])\n    model01_lgb_val_data = lgb.Dataset(model01_lgb_X_train.iloc[val_idx, :], label=model01_lgb_y_train.iloc[val_idx], categorical_feature=['model'])  \n\n    model01_lgb_model = lgb.train(model01_lgb_parameters, model01_lgb_trn_data, valid_sets=[model01_lgb_trn_data, model01_lgb_val_data], verbose_eval=50)\n\n    model01_lgb_oof_predictions = model01_lgb_model.predict(model01_lgb_X_train.iloc[val_idx, :], num_iteration=model01_lgb_model.best_iteration)\n    model01_lgb_oof[val_idx] = model01_lgb_oof_predictions\n    df_train.loc[model01_lgb_X_train.iloc[val_idx, :].index, 'lgb_duo_model_oof'] = model01_lgb_oof_predictions\n\n    model01_lgb_test_predictions = model01_lgb_model.predict(model01_lgb_X_test, num_iteration=model01_lgb_model.best_iteration)\n    model01_lgb_y_pred.iloc[:, fold - 1] = model01_lgb_test_predictions\n    df_test.loc[model01_lgb_X_test.index, f'fold{fold}_lgb_duo_model_predictions'] = model01_lgb_test_predictions\n\n    model01_lgb_importance.iloc[:, fold - 1] = model01_lgb_model.feature_importance(importance_type='gain')\n    model01_lgb_score = f1_score(model01_lgb_y_train.iloc[val_idx].values, np.round(np.clip(model01_lgb_oof_predictions, model01_lgb_y_train.min(), model01_lgb_y_train.max())), average='macro')\n    model01_lgb_scores.append(model01_lgb_score)            \n    print('\\nDuo LGB Fold {} Macro F1-Score {}\\n'.format(fold, model01_lgb_score))\n\nprint('--------------------')\nprint(f'Duo LGB Mean Macro F1-Score {np.mean(model01_lgb_scores):.6} [STD:{np.std(model01_lgb_scores):.6}]')\nfor model in [0, 1]:\n    model_oof = df_train.loc[df_train['model'] == model, 'lgb_duo_model_oof']\n    model_labels = df_train.loc[df_train['model'] == model, 'open_channels']\n    print(f'Duo LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model01_lgb_y_train.min(), model01_lgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Duo LGB Model 0-1 OOF (Rounded) Macro F1-Score {f1_score(model01_lgb_y_train, np.round(np.clip(model01_lgb_oof, model01_lgb_y_train.min(), model01_lgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 6))\n\nmodel01_lgb_importance['Mean_Importance'] = model01_lgb_importance.sum(axis=1) / K\nmodel01_lgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model01_lgb_importance.index, data=model01_lgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Duo LightGBM Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model01_lgb_X_train, model01_lgb_y_train, model01_lgb_X_test, model01_lgb_parameters\ndel model01_lgb_scores, model01_lgb_oof, model01_lgb_y_pred, model01_lgb_importance\ndel model01_lgb_trn_data, model01_lgb_val_data, model01_lgb_model, model01_lgb_oof_predictions, model01_lgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.2. Duo Model XGBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### DUO MODEL 0-1 ####################\n\nprint('#################### DUO MODEL 0-1 ####################')\nprint('-------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## DUO MODEL 0-1 XGB ##########\n    \nprint(f'\\n########## Running Duo XGBoost Model 0-1 ##########')\n\nmodel01_xgb_features = ['signal_processed', 'model']\nmodel01_xgb_X_train = df_train[df_train['model'] <= 1][model01_xgb_features].copy(deep=True)\nmodel01_xgb_y_train = df_train[df_train['model'] <= 1]['open_channels'].copy(deep=True)\nmodel01_xgb_X_test = df_test[df_train['model'] <= 1][model01_xgb_features].copy(deep=True)\n\nfor df in [model01_xgb_X_train, model01_xgb_X_test]:\n    for shift in range(1, 3):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_std_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=False).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=False).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_pct_change+1'] = df.groupby('model')['signal_processed'].pct_change(1).fillna(0)\n\nmodel01_xgb_parameters = {\n    'n_estimators': 2 ** 7, \n    'learning_rate': 0.07,\n    'colsample_bytree': 0.8, \n    'colsample_bylevel': 0.8,\n    'colsample_bynode': 0.7,\n    'sumbsample': 0.7,\n    'max_depth': 6,\n    'gamma': 0.8,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0.1,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\nmodel01_xgb_scores = []\nmodel01_xgb_oof = np.zeros(model01_xgb_X_train.shape[0])\nmodel01_xgb_y_pred = pd.DataFrame(np.zeros((model01_xgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel01_xgb_importance = pd.DataFrame(np.zeros((model01_xgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model01_xgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model01_xgb_X_train, model01_xgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model01_xgb_trn_data = xgb.DMatrix(model01_xgb_X_train.iloc[trn_idx, :], label=model01_xgb_y_train.iloc[trn_idx])\n    model01_xgb_val_data = xgb.DMatrix(model01_xgb_X_train.iloc[val_idx, :], label=model01_xgb_y_train.iloc[val_idx])\n    model01_xgb_model = xgb.train(model01_xgb_parameters, model01_xgb_trn_data, 500, evals=[(model01_xgb_trn_data, 'train'), (model01_xgb_val_data, 'val')], verbose_eval=50, early_stopping_rounds=25)\n\n    model01_xgb_oof_predictions = model01_xgb_model.predict(xgb.DMatrix(model01_xgb_X_train.iloc[val_idx, :]))\n    model01_xgb_oof[val_idx] = model01_xgb_oof_predictions\n    df_train.loc[model01_xgb_X_train.iloc[val_idx, :].index, 'xgb_duo_model_oof'] = model01_xgb_oof_predictions\n\n    model01_xgb_test_predictions = model01_xgb_model.predict(xgb.DMatrix(model01_xgb_X_test))\n    model01_xgb_y_pred.iloc[:, fold - 1] = model01_xgb_test_predictions\n    df_test.loc[model01_xgb_X_test.index, f'fold{fold}_xgb_duo_model_predictions'] = model01_xgb_test_predictions\n\n    model01_xgb_importance.iloc[:, fold - 1] = list(model01_xgb_model.get_score(importance_type='gain').values())\n    model01_xgb_score = f1_score(model01_xgb_y_train.iloc[val_idx].values, np.round(np.clip(model01_xgb_oof_predictions, model01_xgb_y_train.min(), model01_xgb_y_train.max())), average='macro')\n    model01_xgb_scores.append(model01_xgb_score)            \n    print('\\nDuo XGB Fold {} Macro F1-Score {}\\n'.format(fold, model01_xgb_score))\n\nprint('--------------------')\nprint(f'Duo XGB Mean Macro F1-Score {np.mean(model01_xgb_scores):.6} [STD:{np.std(model01_xgb_scores):.6}]')\nfor model in [0, 1]:\n    model_oof = df_train.loc[df_train['model'] == model, 'xgb_duo_model_oof']\n    model_labels = df_train.loc[df_train['model'] == model, 'open_channels']\n    print(f'Duo XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model01_xgb_y_train.min(), model01_xgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Duo XGB Model 0-1 OOF (Rounded) Macro F1-Score {f1_score(model01_xgb_y_train, np.round(np.clip(model01_xgb_oof, model01_xgb_y_train.min(), model01_xgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 6))\n\nmodel01_xgb_importance['Mean_Importance'] = model01_xgb_importance.sum(axis=1) / K\nmodel01_xgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model01_xgb_importance.index, data=model01_xgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Duo XGBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model01_xgb_X_train, model01_xgb_y_train, model01_xgb_X_test, model01_xgb_parameters\ndel model01_xgb_scores, model01_xgb_oof, model01_xgb_y_pred, model01_xgb_importance\ndel model01_xgb_trn_data, model01_xgb_val_data, model01_xgb_model, model01_xgb_oof_predictions, model01_xgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.3. Duo Model CatBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### DUO MODEL 0-1 ####################\n\nprint('#################### DUO MODEL 0-1 ####################')\nprint('-------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\nprint(f'\\n########## Running Duo CatBoost Model 0-1 ##########')\n\nmodel01_cb_features = ['signal_processed', 'model']\nmodel01_cb_X_train = df_train[df_train['model'] <= 1][model01_cb_features].copy(deep=True)\nmodel01_cb_y_train = df_train[df_train['model'] <= 1]['open_channels'].copy(deep=True)\nmodel01_cb_X_test = df_test[df_train['model'] <= 1][model01_cb_features].copy(deep=True)\n\nfor df in [model01_cb_X_train, model01_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_15'] = df.groupby('model')['signal_processed'].rolling(window=15, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_15'] = df.groupby('model')['signal_processed'].rolling(window=60, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_35_difference'] = df['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=False).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_35_difference'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=False).max().reset_index(level='model')['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=False).min().reset_index(level='model')['signal_processed']\n\nmodel01_cb_parameters = {\n    'n_estimators': 400, \n    'learning_rate': 0.08,\n    'depth': 6,\n    'subsample': 0.9,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,\n    'l2_leaf_reg': 0,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\nmodel01_cb_scores = []\nmodel01_cb_oof = np.zeros(model01_cb_X_train.shape[0])\nmodel01_cb_y_pred = pd.DataFrame(np.zeros((model01_cb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel01_cb_importance = pd.DataFrame(np.zeros((model01_cb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model01_cb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model01_cb_X_train, model01_cb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model01_cb_trn_data = cb.Pool(model01_cb_X_train.iloc[trn_idx, :], label=model01_cb_y_train.iloc[trn_idx], cat_features=[1]) \n    model01_cb_val_data = cb.Pool(model01_cb_X_train.iloc[val_idx, :], label=model01_cb_y_train.iloc[val_idx], cat_features=[1])            \n    model01_cb_model = cb.CatBoostRegressor(**model01_cb_parameters)\n    model01_cb_model.fit(model01_cb_trn_data)\n\n    model01_cb_oof_predictions = model01_cb_model.predict(model01_cb_val_data)\n    model01_cb_oof[val_idx] = model01_cb_oof_predictions\n    df_train.loc[model01_cb_X_train.iloc[val_idx, :].index, 'cb_duo_model_oof'] = model01_cb_oof_predictions\n\n    model01_cb_test_predictions = model01_cb_model.predict(cb.Pool(model01_cb_X_test, cat_features=[1]))\n    model01_cb_y_pred.iloc[:, fold - 1] = model01_cb_test_predictions\n    df_test.loc[model01_cb_X_test.index, f'fold{fold}_cb_duo_model_predictions'] = model01_cb_test_predictions\n\n    model01_cb_importance.iloc[:, fold - 1] = model01_cb_model.get_feature_importance()\n    model01_cb_score = f1_score(model01_cb_y_train.iloc[val_idx].values, np.round(np.clip(model01_cb_oof_predictions, model01_cb_y_train.min(), model01_cb_y_train.max())), average='macro')\n    model01_cb_scores.append(model01_cb_score)            \n    print('\\nDuo CB Fold {} Macro F1-Score {}\\n'.format(fold, model01_cb_score))\n\nprint('--------------------')\nprint(f'Duo CB Mean Macro F1-Score {np.mean(model01_cb_scores):.6} [STD:{np.std(model01_cb_scores):.6}]')\nfor model in [0, 1]:\n    model_oof = df_train.loc[df_train['model'] == model, 'cb_duo_model_oof']\n    model_labels = df_train.loc[df_train['model'] == model, 'open_channels']\n    print(f'Duo CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model01_cb_y_train.min(), model01_cb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Duo CB Model 0-1 OOF (Rounded) Macro F1-Score {f1_score(model01_cb_y_train, np.round(np.clip(model01_cb_oof, model01_cb_y_train.min(), model01_cb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 6))\n\nmodel01_cb_importance['Mean_Importance'] = model01_cb_importance.sum(axis=1) / K\nmodel01_cb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model01_cb_importance.index, data=model01_cb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Duo CatBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model01_cb_X_train, model01_cb_y_train, model01_cb_X_test, model01_cb_parameters\ndel model01_cb_scores, model01_cb_oof, model01_cb_y_pred, model01_cb_importance\ndel model01_cb_trn_data, model01_cb_val_data, model01_cb_model, model01_cb_oof_predictions, model01_cb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **4.4. Duo Model Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('#################### DUO MODEL EVALUATION ####################')\nprint('--------------------------------------------------------------')\n\nfor model in range(2):\n    print(f'\\n########## Duo Model {model} ##########')\n    print('---------------------------------')\n    \n    y_true = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    lgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_duo_model_oof']\n    xgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'xgb_duo_model_oof']\n    cb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_duo_model_oof']\n    equal_blend_oof = (lgb_oof * 0.34) + (xgb_oof * 0.33) + (cb_oof * 0.33)\n    \n    print(f'Duo LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(lgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Duo XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(xgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Duo CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(cb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Duo Model {model} Equal Blend OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(equal_blend_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    \nprint(f'\\n########## Duo Model Global ##########')\nprint('--------------------------------------')\n\nglobal_y_true = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 1), 'open_channels']\nglobal_lgb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 1), 'lgb_duo_model_oof']\nglobal_xgb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 1), 'xgb_duo_model_oof']\nglobal_cb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 1), 'cb_duo_model_oof']\nglobal_equal_blend_oof = (global_lgb_oof * 0.34) + (global_xgb_oof * 0.33) + (global_cb_oof * 0.33)\n\nprint(f'Duo LGB Model Global (0-1) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_lgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Duo XGB Model Global (0-1) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_xgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Duo CB Model Global (0-1) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_cb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Duo Model Global (0-1) Equal Blend OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_equal_blend_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\n\ndel global_y_true, global_lgb_oof, global_xgb_oof, global_cb_oof, global_equal_blend_oof","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5. Trio Models**\nTrio models are trained on only **Model 0**, **Model 1** and **Model 2** together. The objective of trio models is maximizing the F1 score calculated on only **Model 2** because the class distribution in each model are different, so feature selection is done based on that objective. Trio models are performing better than Solo Model 2 since they are trained on a larger dataset. Their weights in blends are equal to solo models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **5.1. Trio Model LightGBM**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### TRIO MODEL 0-1-2 ####################\n\nprint('#################### TRIO MODEL 0-1-2 ####################')\nprint('----------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## TRIO MODEL 0-1-2 LGB ##########\n\nprint(f'\\n########## Running Trio LightGBM Model 0-1-2 ##########')\n\nmodel012_lgb_features = ['signal_processed', 'model']\nmodel012_lgb_X_train = df_train[(df_train['model'] <= 2) & (df_train['is_filtered'] == 0)][model012_lgb_features].copy(deep=True)\nmodel012_lgb_y_train = df_train[(df_train['model'] <= 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel012_lgb_X_test = df_test[df_test['model'] <= 2][model012_lgb_features].copy(deep=True)\n\nfor df in [model012_lgb_X_train, model012_lgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)  \n    df['signal_processed_rolling_forward_center_mean_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_20'] = df.groupby('model')['signal_processed'].rolling(window=20, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_40'] = df.groupby('model')['signal_processed'].rolling(window=40, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_40'] = df.groupby('model')['signal_processed'].rolling(window=40, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_median_35_difference'] = df['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).median().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_50_difference'] = df['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=50, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_maxmin_10_difference'] = df.groupby('model')['signal_processed'].rolling(window=10, min_periods=1, center=True).max().reset_index(level='model')['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=10, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n\nmodel012_lgb_parameters = {\n    'num_iterations': 300,\n    'early_stopping_round': 50,\n    'num_leaves': 2 ** 7, \n    'learning_rate': 0.045,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.7,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0,\n    'lambda_l2': 0.5,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n    \nmodel012_lgb_scores = []\nmodel012_lgb_oof = np.zeros(model012_lgb_X_train.shape[0])\nmodel012_lgb_y_pred = pd.DataFrame(np.zeros((model012_lgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel012_lgb_importance = pd.DataFrame(np.zeros((model012_lgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model012_lgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model012_lgb_X_train, model012_lgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model012_lgb_trn_data = lgb.Dataset(model012_lgb_X_train.iloc[trn_idx, :], label=model012_lgb_y_train.iloc[trn_idx], categorical_feature=['model'])\n    model012_lgb_val_data = lgb.Dataset(model012_lgb_X_train.iloc[val_idx, :], label=model012_lgb_y_train.iloc[val_idx], categorical_feature=['model'])  \n\n    model012_lgb_model = lgb.train(model012_lgb_parameters, model012_lgb_trn_data, valid_sets=[model012_lgb_trn_data, model012_lgb_val_data], verbose_eval=50)\n\n    model012_lgb_oof_predictions = model012_lgb_model.predict(model012_lgb_X_train.iloc[val_idx, :], num_iteration=model012_lgb_model.best_iteration)\n    model012_lgb_oof[val_idx] = model012_lgb_oof_predictions\n    df_train.loc[model012_lgb_X_train.iloc[val_idx, :].index, 'lgb_trio_model_oof'] = model012_lgb_oof_predictions\n\n    model012_lgb_test_predictions = model012_lgb_model.predict(model012_lgb_X_test, num_iteration=model012_lgb_model.best_iteration)\n    model012_lgb_y_pred.iloc[:, fold - 1] = model012_lgb_test_predictions\n    df_test.loc[model012_lgb_X_test.index, f'fold{fold}_lgb_trio_model_predictions'] = model012_lgb_test_predictions\n\n    model012_lgb_importance.iloc[:, fold - 1] = model012_lgb_model.feature_importance(importance_type='gain')\n    model012_lgb_score = f1_score(model012_lgb_y_train.iloc[val_idx].values, np.round(np.clip(model012_lgb_oof_predictions, model012_lgb_y_train.min(), model012_lgb_y_train.max())), average='macro')\n    model012_lgb_scores.append(model012_lgb_score)            \n    print('\\nTrio LGB Fold {} Macro F1-Score {}\\n'.format(fold, model012_lgb_score))\n\nprint('--------------------')\nprint(f'Trio LGB Mean Macro F1-Score {np.mean(model012_lgb_scores):.6} [STD:{np.std(model012_lgb_scores):.6}]')\nfor model in [0, 1, 2]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_trio_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Trio LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model012_lgb_y_train.min(), model012_lgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Trio LGB Model 0-1-2 OOF (Rounded) Macro F1-Score {f1_score(model012_lgb_y_train, np.round(np.clip(model012_lgb_oof, model012_lgb_y_train.min(), model012_lgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 10))\n\nmodel012_lgb_importance['Mean_Importance'] = model012_lgb_importance.sum(axis=1) / K\nmodel012_lgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model012_lgb_importance.index, data=model012_lgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Trio LightGBM Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model012_lgb_X_train, model012_lgb_y_train, model012_lgb_X_test, model012_lgb_parameters\ndel model012_lgb_scores, model012_lgb_oof, model012_lgb_y_pred, model012_lgb_importance\ndel model012_lgb_trn_data, model012_lgb_val_data, model012_lgb_model, model012_lgb_oof_predictions, model012_lgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **5.2. Trio Model XGBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### TRIO MODEL 0-1-2 ####################\n\nprint('#################### TRIO MODEL 0-1-2 ####################')\nprint('----------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## TRIO MODEL 0-1-2 XGB ##########\n\nprint(f'\\n########## Running Trio XGBoost Model 0-1-2 ##########')\n\nmodel012_xgb_features = ['signal_processed', 'model']\nmodel012_xgb_X_train = df_train[(df_train['model'] <= 2) & (df_train['is_filtered'] == 0)][model012_xgb_features].copy(deep=True)\nmodel012_xgb_y_train = df_train[(df_train['model'] <= 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel012_xgb_X_test = df_test[df_test['model'] <= 2][model012_xgb_features].copy(deep=True)\n\nfor df in [model012_xgb_X_train, model012_xgb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_55'] = df.groupby('model')['signal_processed'].rolling(window=55, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_maxminratio_15'] = df.groupby('model')['signal_processed'].rolling(window=15, min_periods=1, center=True).max().reset_index(level='model')['signal_processed'] / df.groupby('model')['signal_processed'].rolling(window=15, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    \nmodel012_xgb_parameters = {\n    'n_estimators': 2 ** 7, \n    'learning_rate': 0.095,\n    'colsample_bytree': 0.9, \n    'colsample_bylevel': 0.9,\n    'colsample_bynode': 0.9,\n    'sumbsample': 0.7,\n    'max_depth': 7,\n    'gamma': 0,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\nmodel012_xgb_scores = []\nmodel012_xgb_oof = np.zeros(model012_xgb_X_train.shape[0])\nmodel012_xgb_y_pred = pd.DataFrame(np.zeros((model012_xgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel012_xgb_importance = pd.DataFrame(np.zeros((model012_xgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model012_xgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model012_xgb_X_train, model012_xgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model012_xgb_trn_data = xgb.DMatrix(model012_xgb_X_train.iloc[trn_idx, :], label=model012_xgb_y_train.iloc[trn_idx])\n    model012_xgb_val_data = xgb.DMatrix(model012_xgb_X_train.iloc[val_idx, :], label=model012_xgb_y_train.iloc[val_idx])\n    \n    model012_xgb_model = xgb.train(model012_xgb_parameters, model012_xgb_trn_data, 500, evals=[(model012_xgb_trn_data, 'train'), (model012_xgb_val_data, 'val')], verbose_eval=50, early_stopping_rounds=25)\n\n    model012_xgb_oof_predictions = model012_xgb_model.predict(xgb.DMatrix(model012_xgb_X_train.iloc[val_idx, :]))\n    model012_xgb_oof[val_idx] = model012_xgb_oof_predictions\n    df_train.loc[model012_xgb_X_train.iloc[val_idx, :].index, 'xgb_trio_model_oof'] = model012_xgb_oof_predictions\n\n    model012_xgb_test_predictions = model012_xgb_model.predict(xgb.DMatrix(model012_xgb_X_test))\n    model012_xgb_y_pred.iloc[:, fold - 1] = model012_xgb_test_predictions\n    df_test.loc[model012_xgb_X_test.index, f'fold{fold}_xgb_trio_model_predictions'] = model012_xgb_test_predictions\n\n    model012_xgb_importance.iloc[:, fold - 1] = list(model012_xgb_model.get_score(importance_type='gain').values())\n    model012_xgb_score = f1_score(model012_xgb_y_train.iloc[val_idx].values, np.round(np.clip(model012_xgb_oof_predictions, model012_xgb_y_train.min(), model012_xgb_y_train.max())), average='macro')\n    model012_xgb_scores.append(model012_xgb_score)            \n    print('\\nTrio XGB Fold {} Macro F1-Score {}\\n'.format(fold, model012_xgb_score))\n\nprint('--------------------')\nprint(f'Trio XGB Mean Macro F1-Score {np.mean(model012_xgb_scores):.6} [STD:{np.std(model012_xgb_scores):.6}]')\nfor model in [0, 1, 2]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'xgb_trio_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Trio XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model012_xgb_y_train.min(), model012_xgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Trio XGB Model 0-1-2 OOF (Rounded) Macro F1-Score {f1_score(model012_xgb_y_train, np.round(np.clip(model012_xgb_oof, model012_xgb_y_train.min(), model012_xgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 10))\n\nmodel012_xgb_importance['Mean_Importance'] = model012_xgb_importance.sum(axis=1) / K\nmodel012_xgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model012_xgb_importance.index, data=model012_xgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Trio XGBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model012_xgb_X_train, model012_xgb_y_train, model012_xgb_X_test, model012_xgb_parameters\ndel model012_xgb_scores, model012_xgb_oof, model012_xgb_y_pred, model012_xgb_importance\ndel model012_xgb_trn_data, model012_xgb_val_data, model012_xgb_model, model012_xgb_oof_predictions, model012_xgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **5.3. Trio Model CatBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### TRIO MODEL 0-1-2 ####################\n\nprint('#################### TRIO MODEL 0-1-2 ####################')\nprint('----------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## TRIO MODEL 0-1-2 XGB ##########\n\nprint(f'\\n########## Running Trio CatBoost Model 0-1-2 ##########')\n\nmodel012_cb_features = ['signal_processed', 'model']\nmodel012_cb_X_train = df_train[(df_train['model'] <= 2) & (df_train['is_filtered'] == 0)][model012_cb_features].copy(deep=True)\nmodel012_cb_y_train = df_train[(df_train['model'] <= 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel012_cb_X_test = df_test[df_test['model'] <= 2][model012_cb_features].copy(deep=True)\n\nfor df in [model012_cb_X_train, model012_cb_X_test]:\n    for shift in range(1, 2):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_20'] = df.groupby('model')['signal_processed'].rolling(window=20, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_10_difference'] = df['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=10, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_50_difference'] = df['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=50, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_maxmindiff_55'] = df.groupby('model')['signal_processed'].rolling(window=55, min_periods=1, center=False).max().reset_index(level='model')['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=55, min_periods=1, center=False).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n    df['signal_processed_pctchange+1'] = df['signal_processed'].pct_change(1)\n\nmodel012_cb_parameters = {\n    'n_estimators': 500, \n    'learning_rate': 0.085,\n    'depth': 7,\n    'subsample': 0.8,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,\n    'l2_leaf_reg': 0.6,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\nmodel012_cb_scores = []\nmodel012_cb_oof = np.zeros(model012_cb_X_train.shape[0])\nmodel012_cb_y_pred = pd.DataFrame(np.zeros((model012_cb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel012_cb_importance = pd.DataFrame(np.zeros((model012_cb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model012_cb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model012_cb_X_train, model012_cb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model012_cb_trn_data = cb.Pool(model012_cb_X_train.iloc[trn_idx, :], label=model012_cb_y_train.iloc[trn_idx], cat_features=[1]) \n    model012_cb_val_data = cb.Pool(model012_cb_X_train.iloc[val_idx, :], label=model012_cb_y_train.iloc[val_idx], cat_features=[1])            \n    model012_cb_model = cb.CatBoostRegressor(**model012_cb_parameters)\n    model012_cb_model.fit(model012_cb_trn_data)\n\n    model012_cb_oof_predictions = model012_cb_model.predict(model012_cb_val_data)\n    model012_cb_oof[val_idx] = model012_cb_oof_predictions\n    df_train.loc[model012_cb_X_train.iloc[val_idx, :].index, 'cb_trio_model_oof'] = model012_cb_oof_predictions\n\n    model012_cb_test_predictions = model012_cb_model.predict(cb.Pool(model012_cb_X_test, cat_features=[1]))\n    model012_cb_y_pred.iloc[:, fold - 1] = model012_cb_test_predictions\n    df_test.loc[model012_cb_X_test.index, f'fold{fold}_cb_trio_model_predictions'] = model012_cb_test_predictions\n\n    model012_cb_importance.iloc[:, fold - 1] = model012_cb_model.get_feature_importance()\n    model012_cb_score = f1_score(model012_cb_y_train.iloc[val_idx].values, np.round(np.clip(model012_cb_oof_predictions, model012_cb_y_train.min(), model012_cb_y_train.max())), average='macro')\n    model012_cb_scores.append(model012_cb_score)   \n    print('\\nTrio CB Fold {} Macro F1-Score {}\\n'.format(fold, model012_cb_score))\n\nprint('--------------------')\nprint(f'Trio CB Mean Macro F1-Score {np.mean(model012_cb_scores):.6} [STD:{np.std(model012_cb_scores):.6}]')\nfor model in [0, 1, 2]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_trio_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Trio CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model012_cb_y_train.min(), model012_cb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Trio CB Model 0-1-2 OOF (Rounded) Macro F1-Score {f1_score(model012_cb_y_train, np.round(np.clip(model012_cb_oof, model012_cb_y_train.min(), model012_cb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 10))\n\nmodel012_cb_importance['Mean_Importance'] = model012_cb_importance.sum(axis=1) / K\nmodel012_cb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model012_cb_importance.index, data=model012_cb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Trio CatBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model012_cb_X_train, model012_cb_y_train, model012_cb_X_test, model012_cb_parameters\ndel model012_cb_scores, model012_cb_oof, model012_cb_y_pred, model012_cb_importance\ndel model012_cb_trn_data, model012_cb_val_data, model012_cb_model, model012_cb_oof_predictions, model012_cb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **5.4. Trio Model Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('#################### TRIO MODEL EVALUATION ####################')\nprint('---------------------------------------------------------------')\n\nfor model in range(3):\n    print(f'\\n########## Trio Model {model} ##########')\n    print('----------------------------------')\n    \n    y_true = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    lgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_trio_model_oof']\n    xgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'xgb_trio_model_oof']\n    cb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_trio_model_oof']\n    equal_blend_oof = (lgb_oof * 0.34) + (xgb_oof * 0.33) + (cb_oof * 0.33)\n    \n    print(f'Trio LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(lgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Trio XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(xgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Trio CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(cb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Trio Model {model} Equal Blend OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(equal_blend_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    \nprint(f'\\n########## Trio Model Global ##########')\nprint('---------------------------------------')\n\nglobal_y_true = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 2), 'open_channels']\nglobal_lgb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 2), 'lgb_trio_model_oof']\nglobal_xgb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 2), 'xgb_trio_model_oof']\nglobal_cb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 2), 'cb_trio_model_oof']\nglobal_equal_blend_oof = (global_lgb_oof * 0.34) + (global_xgb_oof * 0.33) + (global_cb_oof * 0.33)\n\nprint(f'Trio LGB Model Global (0-1-2) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_lgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Trio XGB Model Global (0-1-2) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_xgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Trio CB Model Global (0-1-2) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_cb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Trio Model Global (0-1-2) Equal Blend OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_equal_blend_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\n\ndel global_y_true, global_lgb_oof, global_xgb_oof, global_cb_oof, global_equal_blend_oof","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **6. Quad Models**\nQuad models are trained on only **Model 0**, **Model 1**, **Model 2** and **Model 3** together. The objective of quad models is maximizing the F1 score calculated on only **Model 3** because the class distribution in each model are different, so feature selection is done based on that objective. Quad models are performing better than Solo Model 3 since they are trained on a larger dataset. Their weights in blends are higher than solo models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **6.1. Quad Model LightGBM**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### QUAD MODEL 0-1-2-3 ####################\n\nprint('#################### QUAD MODEL 0-1-2-3 ####################')\nprint('------------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## QUAD MODEL 0-1-2-3 LGB ##########\n\nprint(f'\\n########## Running Quad LightGBM Model 0-1-2-3 ##########')\n\nmodel0123_lgb_features = ['signal_processed', 'model']\nmodel0123_lgb_X_train = df_train[(df_train['model'] <= 3) & (df_train['is_filtered'] == 0)][model0123_lgb_features].copy(deep=True)\nmodel0123_lgb_y_train = df_train[(df_train['model'] <= 3) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel0123_lgb_X_test = df_test[df_test['model'] <= 3][model0123_lgb_features].copy(deep=True)\n\nfor df in [model0123_lgb_X_train, model0123_lgb_X_test]:\n    for shift in range(1, 5):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_rolling_forward_center_mean_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_50'] = df.groupby('model')['signal_processed'].rolling(window=50, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_60'] = df.groupby('model')['signal_processed'].rolling(window=60, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n\nmodel0123_lgb_parameters = {\n    'num_iterations': 400,\n    'early_stopping_round': 100,\n    'num_leaves': 2 ** 7, \n    'learning_rate': 0.1,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.9,\n    'feature_fraction_bynode': 0.9,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\nmodel0123_lgb_scores = []\nmodel0123_lgb_oof = np.zeros(model0123_lgb_X_train.shape[0])\nmodel0123_lgb_y_pred = pd.DataFrame(np.zeros((model0123_lgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel0123_lgb_importance = pd.DataFrame(np.zeros((model0123_lgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model0123_lgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model0123_lgb_X_train, model0123_lgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model0123_lgb_trn_data = lgb.Dataset(model0123_lgb_X_train.iloc[trn_idx, :], label=model0123_lgb_y_train.iloc[trn_idx])\n    model0123_lgb_val_data = lgb.Dataset(model0123_lgb_X_train.iloc[val_idx, :], label=model0123_lgb_y_train.iloc[val_idx])  \n\n    model0123_lgb_model = lgb.train(model0123_lgb_parameters, model0123_lgb_trn_data, valid_sets=[model0123_lgb_trn_data, model0123_lgb_val_data], verbose_eval=50)\n\n    model0123_lgb_oof_predictions = model0123_lgb_model.predict(model0123_lgb_X_train.iloc[val_idx, :], num_iteration=model0123_lgb_model.best_iteration)\n    model0123_lgb_oof[val_idx] = model0123_lgb_oof_predictions\n    df_train.loc[model0123_lgb_X_train.iloc[val_idx, :].index, 'lgb_quad_model_oof'] = model0123_lgb_oof_predictions\n\n    model0123_lgb_test_predictions = model0123_lgb_model.predict(model0123_lgb_X_test, num_iteration=model0123_lgb_model.best_iteration)\n    model0123_lgb_y_pred.iloc[:, fold - 1] = model0123_lgb_test_predictions\n    df_test.loc[model0123_lgb_X_test.index, f'fold{fold}_lgb_quad_model_predictions'] = model0123_lgb_test_predictions\n\n    model0123_lgb_importance.iloc[:, fold - 1] = model0123_lgb_model.feature_importance(importance_type='gain')\n    model0123_lgb_score = f1_score(model0123_lgb_y_train.iloc[val_idx].values, np.round(np.clip(model0123_lgb_oof_predictions, model0123_lgb_y_train.min(), model0123_lgb_y_train.max())), average='macro')\n    model0123_lgb_scores.append(model0123_lgb_score)            \n    print('\\nQuad LGB Fold {} Macro F1-Score {}\\n'.format(fold, model0123_lgb_score))\n\nprint('--------------------')\nprint(f'Quad LGB Mean Macro F1-Score {np.mean(model0123_lgb_scores):.6} [STD:{np.std(model0123_lgb_scores):.6}]')\nfor model in [0, 1, 2, 3]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_quad_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Quad LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model0123_lgb_y_train.min(), model0123_lgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Quad LGB Model 0-1-2-3 OOF (Rounded) Macro F1-Score {f1_score(model0123_lgb_y_train, np.round(np.clip(model0123_lgb_oof, model0123_lgb_y_train.min(), model0123_lgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 10))\n\nmodel0123_lgb_importance['Mean_Importance'] = model0123_lgb_importance.sum(axis=1) / K\nmodel0123_lgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model0123_lgb_importance.index, data=model0123_lgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Quad LightGBM Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model0123_lgb_X_train, model0123_lgb_y_train, model0123_lgb_X_test, model0123_lgb_parameters\ndel model0123_lgb_scores, model0123_lgb_oof, model0123_lgb_y_pred, model0123_lgb_importance\ndel model0123_lgb_trn_data, model0123_lgb_val_data, model0123_lgb_model, model0123_lgb_oof_predictions, model0123_lgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **6.2. Quad Model XGBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### QUAD MODEL 0-1-2-3 ####################\n\nprint('#################### QUAD MODEL 0-1-2-3 ####################')\nprint('------------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## QUAD MODEL 0-1-2-3 XGB ##########\n\nprint(f'\\n########## Running Quad XGBoost Model 0-1-2-3 ##########')\n\nmodel0123_xgb_features = ['signal_processed', 'model']\nmodel0123_xgb_X_train = df_train[(df_train['model'] <= 3) & (df_train['is_filtered'] == 0)][model0123_xgb_features].copy(deep=True)\nmodel0123_xgb_y_train = df_train[(df_train['model'] <= 3) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel0123_xgb_X_test = df_test[df_test['model'] <= 3][model0123_xgb_features].copy(deep=True)\n\nfor df in [model0123_xgb_X_train, model0123_xgb_X_test]:\n    for shift in range(1, 3):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_rolling_forward_center_mean_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_20'] = df.groupby('model')['signal_processed'].rolling(window=20, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_40'] = df.groupby('model')['signal_processed'].rolling(window=40, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n\nmodel0123_xgb_parameters = {\n    'n_estimators': 2 ** 8, \n    'learning_rate': 0.1,\n    'colsample_bytree': 0.9, \n    'colsample_bylevel': 0.9,\n    'colsample_bynode': 0.9,\n    'sumbsample': 0.8,\n    'max_depth': 6,\n    'gamma': 0,\n    'min_child_weight': 1,\n    'lambda': 1,\n    'alpha': 0,\n    'objective': 'reg:squarederror',\n    'seed': XGB_SEED,\n    'boosting_type': 'gbtree',\n    'tree_method': 'auto',\n    'silent': True,\n    'verbose': 1,\n    'n_jobs': -1,\n}\n\nmodel0123_xgb_scores = []\nmodel0123_xgb_oof = np.zeros(model0123_xgb_X_train.shape[0])\nmodel0123_xgb_y_pred = pd.DataFrame(np.zeros((model0123_xgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel0123_xgb_importance = pd.DataFrame(np.zeros((model0123_xgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model0123_xgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model0123_xgb_X_train, model0123_xgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model0123_xgb_trn_data = xgb.DMatrix(model0123_xgb_X_train.iloc[trn_idx, :], label=model0123_xgb_y_train.iloc[trn_idx])\n    model0123_xgb_val_data = xgb.DMatrix(model0123_xgb_X_train.iloc[val_idx, :], label=model0123_xgb_y_train.iloc[val_idx])\n\n    model0123_xgb_model = xgb.train(model0123_xgb_parameters, model0123_xgb_trn_data, 500, evals=[(model0123_xgb_trn_data, 'train'), (model0123_xgb_val_data, 'val')], verbose_eval=50, early_stopping_rounds=25)\n\n    model0123_xgb_oof_predictions = model0123_xgb_model.predict(xgb.DMatrix(model0123_xgb_X_train.iloc[val_idx, :]))\n    model0123_xgb_oof[val_idx] = model0123_xgb_oof_predictions\n    df_train.loc[model0123_xgb_X_train.iloc[val_idx, :].index, 'xgb_quad_model_oof'] = model0123_xgb_oof_predictions\n\n    model0123_xgb_test_predictions = model0123_xgb_model.predict(xgb.DMatrix(model0123_xgb_X_test))\n    model0123_xgb_y_pred.iloc[:, fold - 1] = model0123_xgb_test_predictions\n    df_test.loc[model0123_xgb_X_test.index, f'fold{fold}_xgb_quad_model_predictions'] = model0123_xgb_test_predictions\n\n    model0123_xgb_importance.iloc[:, fold - 1] = list(model0123_xgb_model.get_score(importance_type='gain').values())\n    model0123_xgb_score = f1_score(model0123_xgb_y_train.iloc[val_idx].values, np.round(np.clip(model0123_xgb_oof_predictions, model0123_xgb_y_train.min(), model0123_xgb_y_train.max())), average='macro')\n    model0123_xgb_scores.append(model0123_xgb_score)            \n    print('\\nQuad XGB Fold {} Macro F1-Score {}\\n'.format(fold, model0123_xgb_score))\n\nprint('--------------------')\nprint(f'Quad XGB Mean Macro F1-Score {np.mean(model0123_xgb_scores):.6} [STD:{np.std(model0123_xgb_scores):.6}]')\nfor model in [0, 1, 2, 3]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'xgb_quad_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Quad XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model0123_xgb_y_train.min(), model0123_xgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Quad XGB Model 0-1-2-3 OOF (Rounded) Macro F1-Score {f1_score(model0123_xgb_y_train, np.round(np.clip(model0123_xgb_oof, model0123_xgb_y_train.min(), model0123_xgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 10))\n\nmodel0123_xgb_importance['Mean_Importance'] = model0123_xgb_importance.sum(axis=1) / K\nmodel0123_xgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model0123_xgb_importance.index, data=model0123_xgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Quad XGBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model0123_xgb_X_train, model0123_xgb_y_train, model0123_xgb_X_test, model0123_xgb_parameters\ndel model0123_xgb_scores, model0123_xgb_oof, model0123_xgb_y_pred, model0123_xgb_importance\ndel model0123_xgb_trn_data, model0123_xgb_val_data, model0123_xgb_model, model0123_xgb_oof_predictions, model0123_xgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **6.3. Quad Model CatBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### QUAD MODEL 0-1-2-3 ####################\n\nprint('#################### QUAD MODEL 0-1-2-3 ####################')\nprint('------------------------------------------------------------')\n\nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## QUAD MODEL 0-1-2-3 CB ##########\n\nprint(f'\\n########## Running Quad CatBoost Model 0-1-2-3 ##########')\n\nmodel0123_cb_features = ['signal_processed', 'model']\nmodel0123_cb_X_train = df_train[(df_train['model'] <= 3) & (df_train['is_filtered'] == 0)][model0123_cb_features].copy(deep=True)\nmodel0123_cb_y_train = df_train[(df_train['model'] <= 3) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\nmodel0123_cb_X_test = df_test[df_test['model'] <= 3][model0123_cb_features].copy(deep=True)\n\nfor df in [model0123_cb_X_train, model0123_cb_X_test]:\n    for shift in range(1, 4):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_rolling_forward_center_mean_30'] = df.groupby('model')['signal_processed'].rolling(window=30, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_20'] = df.groupby('model')['signal_processed'].rolling(window=20, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_55'] = df.groupby('model')['signal_processed'].rolling(window=55, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n\nmodel0123_cb_parameters = {\n    'n_estimators': 1000, \n    'learning_rate': 0.09,\n    'depth': 7,\n    'subsample': 0.8,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,\n    'l2_leaf_reg': 0,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\nmodel0123_cb_scores = []\nmodel0123_cb_oof = np.zeros(model0123_cb_X_train.shape[0])\nmodel0123_cb_y_pred = pd.DataFrame(np.zeros((model0123_cb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel0123_cb_importance = pd.DataFrame(np.zeros((model0123_cb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model0123_cb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model0123_cb_X_train, model0123_cb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model0123_cb_trn_data = cb.Pool(model0123_cb_X_train.iloc[trn_idx, :], label=model0123_cb_y_train.iloc[trn_idx], cat_features=[1]) \n    model0123_cb_val_data = cb.Pool(model0123_cb_X_train.iloc[val_idx, :], label=model0123_cb_y_train.iloc[val_idx], cat_features=[1])            \n    model0123_cb_model = cb.CatBoostRegressor(**model0123_cb_parameters)\n    model0123_cb_model.fit(model0123_cb_trn_data)\n\n    model0123_cb_oof_predictions = model0123_cb_model.predict(model0123_cb_val_data)\n    model0123_cb_oof[val_idx] = model0123_cb_oof_predictions\n    df_train.loc[model0123_cb_X_train.iloc[val_idx, :].index, 'cb_quad_model_oof'] = model0123_cb_oof_predictions\n\n    model0123_cb_test_predictions = model0123_cb_model.predict(cb.Pool(model0123_cb_X_test, cat_features=[1]))\n    model0123_cb_y_pred.iloc[:, fold - 1] = model0123_cb_test_predictions\n    df_test.loc[model0123_cb_X_test.index, f'fold{fold}_cb_quad_model_predictions'] = model0123_cb_test_predictions\n\n    model0123_cb_importance.iloc[:, fold - 1] = model0123_cb_model.get_feature_importance()\n    model0123_cb_score = f1_score(model0123_cb_y_train.iloc[val_idx].values, np.round(np.clip(model0123_cb_oof_predictions, model0123_cb_y_train.min(), model0123_cb_y_train.max())), average='macro')\n    model0123_cb_scores.append(model0123_cb_score)   \n    print('\\nQuad CB Fold {} Macro F1-Score {}\\n'.format(fold, model0123_cb_score))\n\nprint('--------------------')\nprint(f'Quad CB Mean Macro F1-Score {np.mean(model0123_cb_scores):.6} [STD:{np.std(model0123_cb_scores):.6}]')\nfor model in [0, 1, 2, 3]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_quad_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Quad CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model0123_cb_y_train.min(), model0123_cb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Quad CB Model 0-1-2-3 OOF (Rounded) Macro F1-Score {f1_score(model0123_cb_y_train, np.round(np.clip(model0123_cb_oof, model0123_cb_y_train.min(), model0123_cb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 10))\n\nmodel0123_cb_importance['Mean_Importance'] = model0123_cb_importance.sum(axis=1) / K\nmodel0123_cb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model0123_cb_importance.index, data=model0123_cb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Quad CatBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model0123_cb_X_train, model0123_cb_y_train, model0123_cb_X_test, model0123_cb_parameters\ndel model0123_cb_scores, model0123_cb_oof, model0123_cb_y_pred, model0123_cb_importance\ndel model0123_cb_trn_data, model0123_cb_val_data, model0123_cb_model, model0123_cb_oof_predictions, model0123_cb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **6.4. Quad Model Evaluation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('#################### QUAD MODEL EVALUATION ####################')\nprint('---------------------------------------------------------------')\n\nfor model in range(4):\n    print(f'\\n########## Quad Model {model} ##########')\n    print('----------------------------------')\n    \n    y_true = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    lgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_quad_model_oof']\n    xgb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'xgb_quad_model_oof']\n    cb_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_quad_model_oof']\n    equal_blend_oof = (lgb_oof * 0.34) + (xgb_oof * 0.33) + (cb_oof * 0.33)\n    \n    print(f'Quad LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(lgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Quad XGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(xgb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Quad CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(cb_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    print(f'Quad Model {model} Equal Blend OOF (Rounded) Macro F1-Score {f1_score(y_true, np.round(np.clip(equal_blend_oof, y_true.min(), y_true.max())), average=\"macro\"):.6}')\n    \nprint(f'\\n########## Quad Model Global ##########')\nprint('---------------------------------------')\n\nglobal_y_true = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 3), 'open_channels']\nglobal_lgb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 3), 'lgb_quad_model_oof']\nglobal_xgb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 3), 'xgb_quad_model_oof']\nglobal_cb_oof = df_train.loc[(df_train['is_filtered'] == 0) & (df_train['model'] <= 3), 'cb_quad_model_oof']\nglobal_equal_blend_oof = (global_lgb_oof * 0.34) + (global_xgb_oof * 0.33) + (global_cb_oof * 0.33)\n\nprint(f'Quad LGB Model Global (0-1-2-3) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_lgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Quad XGB Model Global (0-1-2-3) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_xgb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Quad CB Model Global (0-1-2-3) OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_cb_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\nprint(f'Quad Model Global (0-1-2-3) Equal Blend OOF (Rounded) Macro F1-Score {f1_score(global_y_true, np.round(np.clip(global_equal_blend_oof, global_y_true.min(), global_y_true.max())), average=\"macro\"):.6}')\n\ndel global_y_true, global_lgb_oof, global_xgb_oof, global_cb_oof, global_equal_blend_oof","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **7. Penta Models**\n\nPenta models are trained on the whole dataset so they contain all different distributions. The objective of penta models is maximizing the F1 score calculated on the whole dataset, so feature selection is done based on that objective. Penta models are performing way better than Solo Model 4 since they contain all different distributions. Their weights in blends are higher than solo models.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **7.1. Penta Model LightGBM**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### PENTA MODEL 0-1-2-3-4 ####################\n\nprint('#################### PENTA MODEL 0-1-2-3-4 ####################')\nprint('---------------------------------------------------------------')\n    \nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## PENTA MODEL 0-1-2-3-4 LGB ##########\n\nprint(f'\\n########## Running Penta LightGBM Model 0-1-2-3-4 ##########')\n\nmodel01234_lgb_features = ['signal_processed', 'model']\nmodel01234_lgb_X_train = df_train[df_train['is_filtered'] == 0][model01234_lgb_features].copy(deep=True)\nmodel01234_lgb_y_train = df_train[df_train['is_filtered'] == 0]['open_channels'].copy(deep=True)\nmodel01234_lgb_X_test = df_test[df_train['is_filtered'] == 0][model01234_lgb_features].copy(deep=True)\n\nfor df in [model01234_lgb_X_train, model01234_lgb_X_test]:\n    for shift in range(1, 6):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    for diff in range(1, 3):\n        df[f'signal_processed_diff+{diff}'] = df.groupby('model')['signal_processed'].diff(diff)\n        df[f'signal_processed_diff-{diff}'] = df.groupby('model')['signal_processed'].diff(-diff)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n    df['signal_processed_abs'] = np.abs(df['signal_processed'])\n    df['signal_processed_fraction'] = np.abs(df['signal_processed'] - df['signal_processed'].astype(int))\n    df['signal_processed_rolling_forward_center_mean_50'] = df.groupby('model')['signal_processed'].rolling(window=50, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_var_60'] = df.groupby('model')['signal_processed'].rolling(window=60, min_periods=1, center=True).var().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_mean_40_difference'] = df['signal_processed'] - df.groupby('model')['signal_processed'].rolling(window=40, min_periods=1, center=False).mean().reset_index(level='model')['signal_processed']\n\nmodel01234_lgb_parameters = {\n    'num_iterations': 1500,\n    'early_stopping_round': 150,\n    'num_leaves': 2 ** 7,\n    'learning_rate': 0.025,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 1,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.6,\n    'min_data_in_leaf': 20,\n    'lambda_l1': 0,\n    'lambda_l2': 0,\n    'max_depth': -1,\n    'objective': 'regression',\n    'seed': LGB_SEED,\n    'feature_fraction_seed': LGB_SEED,\n    'bagging_seed': LGB_SEED,\n    'drop_seed': LGB_SEED,\n    'data_random_seed': LGB_SEED,\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'rmse',\n    'n_jobs': -1,\n}\n\nmodel01234_lgb_scores = []\nmodel01234_lgb_oof = np.zeros(model01234_lgb_X_train.shape[0])\nmodel01234_lgb_y_pred = pd.DataFrame(np.zeros((model01234_lgb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel01234_lgb_importance = pd.DataFrame(np.zeros((model01234_lgb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model01234_lgb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model01234_lgb_X_train, model01234_lgb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model01234_lgb_trn_data = lgb.Dataset(model01234_lgb_X_train.iloc[trn_idx, :], label=model01234_lgb_y_train.iloc[trn_idx], categorical_feature=['model'])\n    model01234_lgb_val_data = lgb.Dataset(model01234_lgb_X_train.iloc[val_idx, :], label=model01234_lgb_y_train.iloc[val_idx], categorical_feature=['model'])  \n\n    model01234_lgb_model = lgb.train(model01234_lgb_parameters, model01234_lgb_trn_data, valid_sets=[model01234_lgb_trn_data, model01234_lgb_val_data], verbose_eval=50)\n\n    model01234_lgb_oof_predictions = model01234_lgb_model.predict(model01234_lgb_X_train.iloc[val_idx, :], num_iteration=model01234_lgb_model.best_iteration)\n    model01234_lgb_oof[val_idx] = model01234_lgb_oof_predictions\n    df_train.loc[model01234_lgb_X_train.iloc[val_idx, :].index, 'lgb_penta_model_oof'] = model01234_lgb_oof_predictions\n\n    model01234_lgb_test_predictions = model01234_lgb_model.predict(model01234_lgb_X_test, num_iteration=model01234_lgb_model.best_iteration)\n    model01234_lgb_y_pred.iloc[:, fold - 1] = model01234_lgb_test_predictions\n    df_test.loc[model01234_lgb_X_test.index, f'fold{fold}_lgb_penta_model_predictions'] = model01234_lgb_test_predictions\n\n    model01234_lgb_importance.iloc[:, fold - 1] = model01234_lgb_model.feature_importance(importance_type='gain')\n    model01234_lgb_score = f1_score(model01234_lgb_y_train.iloc[val_idx].values, np.round(np.clip(model01234_lgb_oof_predictions, model01234_lgb_y_train.min(), model01234_lgb_y_train.max())), average='macro')\n    model01234_lgb_scores.append(model01234_lgb_score)            \n    print('\\nPenta LGB Fold {} Macro F1-Score {}\\n'.format(fold, model01234_lgb_score))\n\nprint('--------------------')\nprint(f'Penta LGB Mean Macro F1-Score {np.mean(model01234_lgb_scores):.6} [STD:{np.std(model01234_lgb_scores):.6}]')\nfor model in [0, 1, 2, 3, 4]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'lgb_penta_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Penta LGB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model01234_lgb_y_train.min(), model01234_lgb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Penta LGB Model 0-1-2-3-4 OOF (Rounded) Macro F1-Score {f1_score(model01234_lgb_y_train, np.round(np.clip(model01234_lgb_oof, model01234_lgb_y_train.min(), model01234_lgb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 16))\n\nmodel01234_lgb_importance['Mean_Importance'] = model01234_lgb_importance.sum(axis=1) / K\nmodel01234_lgb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model01234_lgb_importance.index, data=model01234_lgb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Penta LightGBM Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model01234_lgb_X_train, model01234_lgb_y_train, model01234_lgb_X_test, model01234_lgb_parameters\ndel model01234_lgb_scores, model01234_lgb_oof, model01234_lgb_y_pred, model01234_lgb_importance\ndel model01234_lgb_trn_data, model01234_lgb_val_data, model01234_lgb_model, model01234_lgb_oof_predictions, model01234_lgb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **7.2. Penta Model XGBoost**\nPenta Model XGBoost is not implemented because it takes too much time to train it on the whole dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **7.3. Penta Model CatBoost**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### PENTA MODEL 0-1-2-3-4 ####################\n\nprint('#################### PENTA MODEL 0-1-2-3-4 ####################')\nprint('---------------------------------------------------------------')\n    \nK = 2\nskf = StratifiedKFold(n_splits=K)\n\n########## PENTA MODEL 0-1-2-3-4 CB ##########\n\nprint(f'\\n########## Running Penta CatBoost Model 0-1-2-3-4 ##########')\n\nmodel01234_cb_features = ['signal_processed', 'model']\nmodel01234_cb_X_train = df_train[df_train['is_filtered'] == 0][model01234_cb_features].copy(deep=True)\nmodel01234_cb_y_train = df_train[df_train['is_filtered'] == 0]['open_channels'].copy(deep=True)\nmodel01234_cb_X_test = df_test[df_train['is_filtered'] == 0][model01234_cb_features].copy(deep=True)\n\nfor df in [model01234_cb_X_train, model01234_cb_X_test]:\n    for shift in range(1, 6):\n        df[f'signal_processed_shift+{shift}'] = df.groupby('model')['signal_processed'].shift(shift)\n        df[f'signal_processed_shift-{shift}'] = df.groupby('model')['signal_processed'].shift(-shift)\n    df['signal_processed_squared'] = df['signal_processed'] ** 2\n    df['signal_processed_squarerooted'] = df['signal_processed'] ** (1 / 2)\n    df['signal_processed_fraction'] = np.abs(df['signal_processed'] - df['signal_processed'].astype(int))\n    df['signal_processed_rolling_forward_center_mean_50'] = df.groupby('model')['signal_processed'].rolling(window=50, min_periods=1, center=True).mean().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_std_25'] = df.groupby('model')['signal_processed'].rolling(window=25, min_periods=1, center=True).std().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_min_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).min().reset_index(level='model')['signal_processed']\n    df['signal_processed_rolling_forward_center_max_35'] = df.groupby('model')['signal_processed'].rolling(window=35, min_periods=1, center=True).max().reset_index(level='model')['signal_processed']\n\nmodel01234_cb_parameters = {\n    'n_estimators': 1000, \n    'learning_rate': 0.075,\n    'depth': 7,\n    'subsample': 0.6,\n    'bagging_temperature': 1,\n    'colsample_bylevel': 0.9,\n    'l2_leaf_reg': 0.1,\n    'metric_period': 50,\n    'boost_from_average': True,\n    'eval_metric': 'RMSE',\n    'loss_function': 'RMSE',    \n    'random_seed': CB_SEED,\n    'verbose': 1,\n}\n\nmodel01234_cb_scores = []\nmodel01234_cb_oof = np.zeros(model01234_cb_X_train.shape[0])\nmodel01234_cb_y_pred = pd.DataFrame(np.zeros((model01234_cb_X_test.shape[0], K)), columns=[f'Fold_{i}_Predictions' for i in range(1, K + 1)]) \nmodel01234_cb_importance = pd.DataFrame(np.zeros((model01234_cb_X_train.shape[1], K)), columns=[f'Fold_{i}_Importance' for i in range(1, K + 1)], index=model01234_cb_X_train.columns)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(model01234_cb_X_train, model01234_cb_X_train['model']), 1):\n    print(f'\\nFold {fold}')\n\n    model01234_cb_trn_data = cb.Pool(model01234_cb_X_train.iloc[trn_idx, :], label=model01234_cb_y_train.iloc[trn_idx], cat_features=[1]) \n    model01234_cb_val_data = cb.Pool(model01234_cb_X_train.iloc[val_idx, :], label=model01234_cb_y_train.iloc[val_idx], cat_features=[1])            \n    model01234_cb_model = cb.CatBoostRegressor(**model01234_cb_parameters)\n    model01234_cb_model.fit(model01234_cb_trn_data)\n\n    model01234_cb_oof_predictions = model01234_cb_model.predict(model01234_cb_val_data)\n    model01234_cb_oof[val_idx] = model01234_cb_oof_predictions\n    df_train.loc[model01234_cb_X_train.iloc[val_idx, :].index, 'cb_penta_model_oof'] = model01234_cb_oof_predictions\n\n    model01234_cb_test_predictions = model01234_cb_model.predict(cb.Pool(model01234_cb_X_test, cat_features=[1]))\n    model01234_cb_y_pred.iloc[:, fold - 1] = model01234_cb_test_predictions\n    df_test.loc[model01234_cb_X_test.index, f'fold{fold}_cb_penta_model_predictions'] = model01234_cb_test_predictions\n\n    model01234_cb_importance.iloc[:, fold - 1] = model01234_cb_model.get_feature_importance()\n    model01234_cb_score = f1_score(model01234_cb_y_train.iloc[val_idx].values, np.round(np.clip(model01234_cb_oof_predictions, model01234_cb_y_train.min(), model01234_cb_y_train.max())), average='macro')\n    model01234_cb_scores.append(model01234_cb_score)         \n    print('\\nPenta CB Fold {} Macro F1-Score {}\\n'.format(fold, model01234_cb_score))\n\nprint('--------------------')\nprint(f'Penta CB Mean Macro F1-Score {np.mean(model01234_cb_scores):.6} [STD:{np.std(model01234_cb_scores):.6}]')\nfor model in [0, 1, 2, 3, 4]:\n    model_oof = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'cb_penta_model_oof']\n    model_labels = df_train.loc[(df_train['model'] == model) & (df_train['is_filtered'] == 0), 'open_channels']\n    print(f'Penta CB Model {model} OOF (Rounded) Macro F1-Score {f1_score(model_labels, np.round(np.clip(model_oof, model01234_cb_y_train.min(), model01234_cb_y_train.max())), average=\"macro\"):.6}')\nprint(f'Penta CB Model 0-1-2-3-4 OOF (Rounded) Macro F1-Score {f1_score(model01234_cb_y_train, np.round(np.clip(model01234_cb_oof, model01234_cb_y_train.min(), model01234_cb_y_train.max())), average=\"macro\"):.6}')\nprint('--------------------')\n\nplt.figure(figsize=(20, 16))\n\nmodel01234_cb_importance['Mean_Importance'] = model01234_cb_importance.sum(axis=1) / K\nmodel01234_cb_importance.sort_values(by='Mean_Importance', inplace=True, ascending=False)\nsns.barplot(x='Mean_Importance', y=model01234_cb_importance.index, data=model01234_cb_importance)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=18)\nplt.tick_params(axis='y', labelsize=18)\nplt.title('Penta CatBoost Feature Importance (Gain)', size=20, pad=20)\n\nplt.show()\n\ndel model01234_cb_X_train, model01234_cb_y_train, model01234_cb_X_test, model01234_cb_parameters\ndel model01234_cb_scores, model01234_cb_oof, model01234_cb_y_pred, model01234_cb_importance\ndel model01234_cb_trn_data, model01234_cb_val_data, model01234_cb_model, model01234_cb_oof_predictions, model01234_cb_test_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **8. Blending**\n\nOOF Macro F1 scores are calculated with simple rounding operations without any optimization. Predictions are clipped based on the distributions' minimum and maximum `open_channels` value. Noisy part in Batch 7 (`is_filtered`) is excluded from OOF Macro F1 calculation.\n\n* **Model 0** - Solo Models (50%) + Duo Models (50%)\n* **Model 1** - Solo Models (50%) + Duo Models (50%)\n* **Model 2** - Solo Models (40%) + Trio Models (60%)\n* **Model 3** - Solo Models (40%) + Quad Models (60%)\n* **Model 4** - Solo Models (10%) + Penta Models (90%)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### MODEL 0 ####################\n\nprint('---------- Model 0 ----------\\n')\n\nmodel0_labels = df_train[df_train['model'] == 0]['open_channels'].copy(deep=True)\n\nmodel0_solo_lgb_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['lgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel0_solo_xgb_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['xgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel0_solo_cb_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['cb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel0_duo_lgb_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['lgb_duo_model_oof']).astype(np.uint8), average='macro')\nmodel0_duo_xgb_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['xgb_duo_model_oof']).astype(np.uint8), average='macro')\nmodel0_duo_cb_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['cb_duo_model_oof']).astype(np.uint8), average='macro')\n\ndf_train.loc[df_train['model'] == 0, 'blend_oof'] = (df_train.loc[df_train['model'] == 0, 'lgb_solo_model_oof'] * 0.17) +\\\n                                                    (df_train.loc[df_train['model'] == 0, 'xgb_solo_model_oof'] * 0.165) +\\\n                                                    (df_train.loc[df_train['model'] == 0, 'cb_solo_model_oof'] * 0.165) +\\\n                                                    (df_train.loc[df_train['model'] == 0, 'lgb_duo_model_oof'] * 0.17) +\\\n                                                    (df_train.loc[df_train['model'] == 0, 'xgb_duo_model_oof'] * 0.165) +\\\n                                                    (df_train.loc[df_train['model'] == 0, 'cb_duo_model_oof'] * 0.165)\n\nmodel0_blend_oof_score = f1_score(model0_labels, np.round(df_train[df_train['model'] == 0]['blend_oof']).astype(np.uint8), average='macro')\n\nprint(f'Solo LGB Model 0 OOF (Rounded) Macro F1-Score {model0_solo_lgb_oof_score:.6}')\nprint(f'Solo XGB OOF Model 0 (Rounded) Macro F1-Score {model0_solo_xgb_oof_score:.6}')\nprint(f'Solo CB OOF Model 0 (Rounded) Macro F1-Score {model0_solo_cb_oof_score:.6}')\nprint(f'Duo LGB Model 0 OOF (Rounded) Macro F1-Score {model0_duo_lgb_oof_score:.6}')\nprint(f'Duo XGB OOF Model 0 (Rounded) Macro F1-Score {model0_duo_xgb_oof_score:.6}')\nprint(f'Duo CB OOF Model 0 (Rounded) Macro F1-Score {model0_duo_cb_oof_score:.6}')\nprint(f'Blend OOF Model 0 (Rounded) Macro F1-Score {model0_blend_oof_score:.6}')\n\n#################### MODEL 1 ####################\n\nprint('\\n---------- Model 1 ----------\\n')\n\nmodel1_labels = df_train[df_train['model'] == 1]['open_channels'].copy(deep=True)\n\nmodel1_solo_lgb_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['lgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel1_solo_xgb_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['xgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel1_solo_cb_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['cb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel1_duo_lgb_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['lgb_duo_model_oof']).astype(np.uint8), average='macro')\nmodel1_duo_xgb_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['xgb_duo_model_oof']).astype(np.uint8), average='macro')\nmodel1_duo_cb_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['cb_duo_model_oof']).astype(np.uint8), average='macro')\n\ndf_train.loc[df_train['model'] == 1, 'blend_oof'] = (df_train.loc[df_train['model'] == 1, 'lgb_solo_model_oof'] * 0.17) +\\\n                                                    (df_train.loc[df_train['model'] == 1, 'xgb_solo_model_oof'] * 0.165) +\\\n                                                    (df_train.loc[df_train['model'] == 1, 'cb_solo_model_oof'] * 0.165) +\\\n                                                    (df_train.loc[df_train['model'] == 1, 'lgb_duo_model_oof'] * 0.17) +\\\n                                                    (df_train.loc[df_train['model'] == 1, 'xgb_duo_model_oof'] * 0.165) +\\\n                                                    (df_train.loc[df_train['model'] == 1, 'cb_duo_model_oof'] * 0.165)\n                    \nmodel1_blend_oof_score = f1_score(model1_labels, np.round(df_train[df_train['model'] == 1]['blend_oof']).astype(np.uint8), average='macro')\n\nprint(f'Solo LGB Model 1 OOF (Rounded) Macro F1-Score {model1_solo_lgb_oof_score:.6}')\nprint(f'Solo XGB OOF Model 1 (Rounded) Macro F1-Score {model1_solo_xgb_oof_score:.6}')\nprint(f'Solo CB OOF Model 1 (Rounded) Macro F1-Score {model1_solo_cb_oof_score:.6}')\nprint(f'Duo LGB Model 1 OOF (Rounded) Macro F1-Score {model1_duo_lgb_oof_score:.6}')\nprint(f'Duo XGB OOF Model 1 (Rounded) Macro F1-Score {model1_duo_xgb_oof_score:.6}')\nprint(f'Duo CB OOF Model 1 (Rounded) Macro F1-Score {model1_duo_cb_oof_score:.6}')\nprint(f'Blend OOF Model 1 (Rounded) Macro F1-Score {model1_blend_oof_score:.6}')\n\n#################### MODEL 2 ####################\n\nprint('\\n---------- Model 2 ----------\\n')\n\nmodel2_labels = df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['open_channels'].copy(deep=True)\n\nmodel2_solo_lgb_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['lgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel2_solo_xgb_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['xgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel2_solo_cb_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['cb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel2_trio_lgb_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['lgb_trio_model_oof']).astype(np.uint8), average='macro')\nmodel2_trio_xgb_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['xgb_trio_model_oof']).astype(np.uint8), average='macro')\nmodel2_trio_cb_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['cb_trio_model_oof']).astype(np.uint8), average='macro')\n\ndf_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'blend_oof'] = (df_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'lgb_solo_model_oof'] * 0.14) +\\\n                                                                                       (df_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'xgb_solo_model_oof'] * 0.13) +\\\n                                                                                       (df_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'cb_solo_model_oof'] * 0.13) +\\\n                                                                                       (df_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'lgb_trio_model_oof'] * 0.2) +\\\n                                                                                       (df_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'xgb_trio_model_oof'] * 0.2) +\\\n                                                                                       (df_train.loc[(df_train['model'] == 2) & (df_train['is_filtered'] == 0), 'cb_trio_model_oof'] * 0.2)\n                    \nmodel2_blend_oof_score = f1_score(model2_labels, np.round(df_train[(df_train['model'] == 2) & (df_train['is_filtered'] == 0)]['blend_oof']).astype(np.uint8), average='macro')\n\nprint(f'Solo LGB Model 2 OOF (Rounded) Macro F1-Score {model2_solo_lgb_oof_score:.6}')\nprint(f'Solo XGB OOF Model 2 (Rounded) Macro F1-Score {model2_solo_xgb_oof_score:.6}')\nprint(f'Solo CB OOF Model 2 (Rounded) Macro F1-Score {model2_solo_cb_oof_score:.6}')\nprint(f'Trio LGB Model 2 OOF (Rounded) Macro F1-Score {model2_trio_lgb_oof_score:.6}')\nprint(f'Trio XGB OOF Model 2 (Rounded) Macro F1-Score {model2_trio_xgb_oof_score:.6}')\nprint(f'Trio CB OOF Model 2 (Rounded) Macro F1-Score {model2_trio_cb_oof_score:.6}')\nprint(f'Blend OOF Model 2 (Rounded) Macro F1-Score {model2_blend_oof_score:.6}')\n\n#################### MODEL 3 ####################\n\nprint('\\n---------- Model 3 ----------\\n')\n\nmodel3_labels = df_train[df_train['model'] == 3]['open_channels'].copy(deep=True)\n\nmodel3_solo_lgb_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['lgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel3_solo_xgb_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['xgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel3_solo_cb_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['cb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel3_quad_lgb_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['lgb_quad_model_oof']).astype(np.uint8), average='macro')\nmodel3_quad_xgb_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['xgb_quad_model_oof']).astype(np.uint8), average='macro')\nmodel3_quad_cb_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['cb_quad_model_oof']).astype(np.uint8), average='macro')\n\ndf_train.loc[df_train['model'] == 3, 'blend_oof'] = (df_train.loc[df_train['model'] == 3, 'lgb_solo_model_oof'] * 0.14) +\\\n                                                    (df_train.loc[df_train['model'] == 3, 'xgb_solo_model_oof'] * 0.13) +\\\n                                                    (df_train.loc[df_train['model'] == 3, 'cb_solo_model_oof'] * 0.13) +\\\n                                                    (df_train.loc[df_train['model'] == 3, 'lgb_quad_model_oof'] * 0.2) +\\\n                                                    (df_train.loc[df_train['model'] == 3, 'xgb_quad_model_oof'] * 0.2) +\\\n                                                    (df_train.loc[df_train['model'] == 3, 'cb_quad_model_oof'] * 0.2)\n                    \nmodel3_blend_oof_score = f1_score(model3_labels, np.round(df_train[df_train['model'] == 3]['blend_oof']).astype(np.uint8), average='macro')\n\nprint(f'Solo LGB Model 3 OOF (Rounded) Macro F1-Score {model3_solo_lgb_oof_score:.6}')\nprint(f'Solo XGB OOF Model 3 (Rounded) Macro F1-Score {model3_solo_xgb_oof_score:.6}')\nprint(f'Solo CB OOF Model 3 (Rounded) Macro F1-Score {model3_solo_cb_oof_score:.6}')\nprint(f'Quad LGB Model 3 OOF (Rounded) Macro F1-Score {model3_quad_lgb_oof_score:.6}')\nprint(f'Quad XGB OOF Model 3 (Rounded) Macro F1-Score {model3_quad_xgb_oof_score:.6}')\nprint(f'Quad CB OOF Model 3 (Rounded) Macro F1-Score {model3_quad_cb_oof_score:.6}')\nprint(f'Blend OOF Model 3 (Rounded) Macro F1-Score {model3_blend_oof_score:.6}')\n\n#################### MODEL 4 ####################\n\nprint('\\n---------- Model 4 ----------\\n')\n\nmodel4_labels = df_train[df_train['model'] == 4]['open_channels'].copy(deep=True)\n\nmodel4_solo_lgb_oof_score = f1_score(model4_labels, np.round(df_train[df_train['model'] == 4]['lgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel4_solo_xgb_oof_score = f1_score(model4_labels, np.round(df_train[df_train['model'] == 4]['xgb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel4_solo_cb_oof_score = f1_score(model4_labels, np.round(df_train[df_train['model'] == 4]['cb_solo_model_oof']).astype(np.uint8), average='macro')\nmodel4_penta_lgb_oof_score = f1_score(model4_labels, np.round(df_train[df_train['model'] == 4]['lgb_penta_model_oof']).astype(np.uint8), average='macro')\nmodel4_penta_cb_oof_score = f1_score(model4_labels, np.round(df_train[df_train['model'] == 4]['cb_penta_model_oof']).astype(np.uint8), average='macro')\n\ndf_train.loc[df_train['model'] == 4, 'blend_oof'] = (df_train.loc[df_train['model'] == 4, 'lgb_solo_model_oof'] * 0.034) +\\\n                                                    (df_train.loc[df_train['model'] == 4, 'xgb_solo_model_oof'] * 0.033) +\\\n                                                    (df_train.loc[df_train['model'] == 4, 'cb_solo_model_oof'] * 0.033) +\\\n                                                    (df_train.loc[df_train['model'] == 4, 'lgb_penta_model_oof'] * 0.45) +\\\n                                                    (df_train.loc[df_train['model'] == 4, 'cb_penta_model_oof'] * 0.45)\n                    \nmodel4_blend_oof_score = f1_score(model4_labels, np.round(df_train[df_train['model'] == 4]['blend_oof']).astype(np.uint8), average='macro')\n\nprint(f'Solo LGB Model 4 OOF (Rounded) Macro F1-Score {model4_solo_lgb_oof_score:.6}')\nprint(f'Solo XGB OOF Model 4 (Rounded) Macro F1-Score {model4_solo_xgb_oof_score:.6}')\nprint(f'Solo CB OOF Model 4 (Rounded) Macro F1-Score {model4_solo_cb_oof_score:.6}')\nprint(f'Penta LGB Model 4 OOF (Rounded) Macro F1-Score {model4_penta_lgb_oof_score:.6}')\nprint(f'Penta CB OOF Model 4 (Rounded) Macro F1-Score {model4_penta_cb_oof_score:.6}')\nprint(f'Blend OOF Model 4 (Rounded) Macro F1-Score {model4_blend_oof_score:.6}')\n\n#################### GLOBAL ####################\n\nprint('\\n---------- Global ----------\\n')\n\nglobal_blend_oof_score = f1_score(df_train[df_train['is_filtered'] == 0]['open_channels'], np.round(df_train[df_train['is_filtered'] == 0]['blend_oof']).astype(np.uint8), average='macro')\n\nprint(f'Global Blend OOF (Rounded) Macro F1-Score {global_blend_oof_score:.6}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['lgb_solo_model_predictions'] = (df_test['fold1_lgb_solo_model_predictions'] + df_test['fold2_lgb_solo_model_predictions']) / 2\ndf_test['xgb_solo_model_predictions'] = (df_test['fold1_xgb_solo_model_predictions'] + df_test['fold2_xgb_solo_model_predictions']) / 2\ndf_test['cb_solo_model_predictions'] = (df_test['fold1_cb_solo_model_predictions'] + df_test['fold2_cb_solo_model_predictions']) / 2\ndf_test['lgb_duo_model_predictions'] = (df_test['fold1_lgb_duo_model_predictions'] + df_test['fold2_lgb_duo_model_predictions']) / 2\ndf_test['xgb_duo_model_predictions'] = (df_test['fold1_xgb_duo_model_predictions'] + df_test['fold2_xgb_duo_model_predictions']) / 2\ndf_test['cb_duo_model_predictions'] = (df_test['fold1_cb_duo_model_predictions'] + df_test['fold2_cb_duo_model_predictions']) / 2\ndf_test['lgb_trio_model_predictions'] = (df_test['fold1_lgb_trio_model_predictions'] + df_test['fold2_lgb_trio_model_predictions']) / 2\ndf_test['xgb_trio_model_predictions'] = (df_test['fold1_xgb_trio_model_predictions'] + df_test['fold2_xgb_trio_model_predictions']) / 2\ndf_test['cb_trio_model_predictions'] = (df_test['fold1_cb_trio_model_predictions'] + df_test['fold2_cb_trio_model_predictions']) / 2\ndf_test['lgb_quad_model_predictions'] = (df_test['fold1_lgb_quad_model_predictions'] + df_test['fold2_lgb_quad_model_predictions']) / 2\ndf_test['xgb_quad_model_predictions'] = (df_test['fold1_xgb_quad_model_predictions'] + df_test['fold2_xgb_quad_model_predictions']) / 2\ndf_test['cb_quad_model_predictions'] = (df_test['fold1_cb_quad_model_predictions'] + df_test['fold2_cb_quad_model_predictions']) / 2\ndf_test['lgb_penta_model_predictions'] = (df_test['fold1_lgb_penta_model_predictions'] + df_test['fold2_lgb_penta_model_predictions']) / 2\ndf_test['cb_penta_model_predictions'] = (df_test['fold1_cb_penta_model_predictions'] + df_test['fold2_cb_penta_model_predictions']) / 2\n\nfor fold in range(1, 3):\n    for gbdt in ['lgb', 'xgb', 'cb']:\n        for model_type in ['solo', 'duo', 'trio', 'quad']:\n            df_test.drop(columns=[f'fold{fold}_{gbdt}_{model_type}_model_predictions'], inplace=True)            \ndf_test.drop(columns=['fold1_lgb_penta_model_predictions', 'fold2_lgb_penta_model_predictions', 'fold1_cb_penta_model_predictions', 'fold2_cb_penta_model_predictions'], inplace=True)\n\n#################### MODEL 0 ####################\n            \ndf_test.loc[df_test['model'] == 0, 'blend_predictions'] = (df_test.loc[df_test['model'] == 0, 'lgb_solo_model_predictions'] * 0.17) +\\\n                                                          (df_test.loc[df_test['model'] == 0, 'xgb_solo_model_predictions'] * 0.165) +\\\n                                                          (df_test.loc[df_test['model'] == 0, 'cb_solo_model_predictions'] * 0.165) +\\\n                                                          (df_test.loc[df_test['model'] == 0, 'lgb_duo_model_predictions'] * 0.17) +\\\n                                                          (df_test.loc[df_test['model'] == 0, 'xgb_duo_model_predictions'] * 0.165) +\\\n                                                          (df_test.loc[df_test['model'] == 0, 'cb_duo_model_predictions'] * 0.165)\n\n#################### MODEL 1 ####################\n\ndf_test.loc[df_test['model'] == 1, 'blend_predictions'] = (df_test.loc[df_test['model'] == 1, 'lgb_solo_model_predictions'] * 0.17) +\\\n                                                          (df_test.loc[df_test['model'] == 1, 'xgb_solo_model_predictions'] * 0.165) +\\\n                                                          (df_test.loc[df_test['model'] == 1, 'cb_solo_model_predictions'] * 0.165) +\\\n                                                          (df_test.loc[df_test['model'] == 1, 'lgb_duo_model_predictions'] * 0.17) +\\\n                                                          (df_test.loc[df_test['model'] == 1, 'xgb_duo_model_predictions'] * 0.165) +\\\n                                                          (df_test.loc[df_test['model'] == 1, 'cb_duo_model_predictions'] * 0.165)\n\n#################### MODEL 2 ####################\n\ndf_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'blend_predictions'] = (df_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'lgb_solo_model_predictions'] * 0.14) +\\\n                                                                                        (df_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'xgb_solo_model_predictions'] * 0.13) +\\\n                                                                                        (df_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'cb_solo_model_predictions'] * 0.13) +\\\n                                                                                        (df_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'lgb_trio_model_predictions'] * 0.2) +\\\n                                                                                        (df_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'xgb_trio_model_predictions'] * 0.2) +\\\n                                                                                        (df_test.loc[(df_test['model'] == 1.5) | (df_test['model'] == 2), 'cb_trio_model_predictions'] * 0.2)\n\n#################### MODEL 3 ####################\n\ndf_test.loc[df_test['model'] == 3, 'blend_predictions'] = (df_test.loc[df_test['model'] == 3, 'lgb_solo_model_predictions'] * 0.14) +\\\n                                                          (df_test.loc[df_test['model'] == 3, 'xgb_solo_model_predictions'] * 0.13) +\\\n                                                          (df_test.loc[df_test['model'] == 3, 'cb_solo_model_predictions'] * 0.13) +\\\n                                                          (df_test.loc[df_test['model'] == 3, 'lgb_quad_model_predictions'] * 0.2) +\\\n                                                          (df_test.loc[df_test['model'] == 3, 'xgb_quad_model_predictions'] * 0.2) +\\\n                                                          (df_test.loc[df_test['model'] == 3, 'cb_quad_model_predictions'] * 0.2)\n\n#################### MODEL 4 ####################\n\ndf_test.loc[df_test['model'] == 4, 'blend_predictions'] = (df_test.loc[df_test['model'] == 4, 'lgb_solo_model_predictions'] * 0.034) +\\\n                                                          (df_test.loc[df_test['model'] == 4, 'xgb_solo_model_predictions'] * 0.033) +\\\n                                                          (df_test.loc[df_test['model'] == 4, 'cb_solo_model_predictions'] * 0.033) +\\\n                                                          (df_test.loc[df_test['model'] == 4, 'lgb_penta_model_predictions'] * 0.45) +\\\n                                                          (df_test.loc[df_test['model'] == 4, 'cb_penta_model_predictions'] * 0.45)\n# Clipping test set predictions\ndf_test.loc[df_test['model'] == 0, 'blend_predictions'] = np.clip(df_test.loc[df_test['model'] == 0, 'blend_predictions'], 0, 1)\ndf_test.loc[df_test['model'] == 1, 'blend_predictions'] = np.clip(df_test.loc[df_test['model'] == 1, 'blend_predictions'], 0, 1)\ndf_test.loc[df_test['model'] == 1.5, 'blend_predictions'] = np.clip(df_test.loc[df_test['model'] == 1.5, 'blend_predictions'], 0, 3)\ndf_test.loc[df_test['model'] == 2, 'blend_predictions'] = np.clip(df_test.loc[df_test['model'] == 2, 'blend_predictions'], 0, 3)\ndf_test.loc[df_test['model'] == 3, 'blend_predictions'] = np.clip(df_test.loc[df_test['model'] == 3, 'blend_predictions'], 0, 5)\ndf_test.loc[df_test['model'] == 4, 'blend_predictions'] = np.clip(df_test.loc[df_test['model'] == 4, 'blend_predictions'], 0, 10)\n\n# Rounding test set predictions\ndf_test['blend_predictions'] =  df_test['blend_predictions'].round().astype(np.uint8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **9. Conclusion**\n* `blend_predictions` is used for submissions\n* `lgb_solo_model`, `xgb_solo_model` and `cb_solo_model` are predictions made by the solo models stacked on top of each other (Batch 7 noisy part excluded)\n* `lgb_multi_model`, `xgb_multi_model` (Model 4 missing) and `cb_multi_model` are predictions made by duo, trio, quad and penta models stacked on top of each other (Batch 7 noisy part excluded)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#################### TRAINING SET ####################\n\ndf_train.rename(columns={\n    'lgb_solo_model_oof': 'lgb_solo_model',\n    'xgb_solo_model_oof': 'xgb_solo_model',\n    'cb_solo_model_oof': 'cb_solo_model'\n}, inplace=True)\n\ndf_train['lgb_multi_model'] = 0\ndf_train.loc[df_train['model'] == 0, 'lgb_multi_model'] = df_train.loc[df_train['model'] == 0, 'lgb_duo_model_oof']\ndf_train.loc[df_train['model'] == 1, 'lgb_multi_model'] = df_train.loc[df_train['model'] == 1, 'lgb_duo_model_oof']\ndf_train.loc[df_train['model'] == 2, 'lgb_multi_model'] = df_train.loc[df_train['model'] == 2, 'lgb_trio_model_oof']\ndf_train.loc[df_train['model'] == 3, 'lgb_multi_model'] = df_train.loc[df_train['model'] == 3, 'lgb_quad_model_oof']\ndf_train.loc[df_train['model'] == 4, 'lgb_multi_model'] = df_train.loc[df_train['model'] == 4, 'lgb_penta_model_oof']\ndf_train['lgb_multi_model'] = df_train['lgb_multi_model'].astype(np.float32)\n\ndf_train['xgb_multi_model'] = 0\ndf_train.loc[df_train['model'] == 0, 'xgb_multi_model'] = df_train.loc[df_train['model'] == 0, 'xgb_duo_model_oof']\ndf_train.loc[df_train['model'] == 1, 'xgb_multi_model'] = df_train.loc[df_train['model'] == 1, 'xgb_duo_model_oof']\ndf_train.loc[df_train['model'] == 2, 'xgb_multi_model'] = df_train.loc[df_train['model'] == 2, 'xgb_trio_model_oof']\ndf_train.loc[df_train['model'] == 3, 'xgb_multi_model'] = df_train.loc[df_train['model'] == 3, 'xgb_quad_model_oof']\ndf_train.loc[df_train['model'] == 4, 'xgb_multi_model'] = np.nan\ndf_train['xgb_multi_model'] = df_train['xgb_multi_model'].astype(np.float32)\n\ndf_train['cb_multi_model'] = 0\ndf_train.loc[df_train['model'] == 0, 'cb_multi_model'] = df_train.loc[df_train['model'] == 0, 'cb_duo_model_oof']\ndf_train.loc[df_train['model'] == 1, 'cb_multi_model'] = df_train.loc[df_train['model'] == 1, 'cb_duo_model_oof']\ndf_train.loc[df_train['model'] == 2, 'cb_multi_model'] = df_train.loc[df_train['model'] == 2, 'cb_trio_model_oof']\ndf_train.loc[df_train['model'] == 3, 'cb_multi_model'] = df_train.loc[df_train['model'] == 3, 'cb_quad_model_oof']\ndf_train.loc[df_train['model'] == 4, 'cb_multi_model'] = df_train.loc[df_train['model'] == 4, 'cb_penta_model_oof']\ndf_train['cb_multi_model'] = df_train['cb_multi_model'].astype(np.float32)\n\n#################### TEST SET ####################\n\ndf_test.rename(columns={\n    'lgb_solo_model_predictions': 'lgb_solo_model',\n    'xgb_solo_model_predictions': 'xgb_solo_model',\n    'cb_solo_model_predictions': 'cb_solo_model'\n}, inplace=True)\n\ndf_test['lgb_multi_model'] = 0\ndf_test.loc[df_test['model'] == 0, 'lgb_multi_model'] = df_test.loc[df_test['model'] == 0, 'lgb_duo_model_predictions']\ndf_test.loc[df_test['model'] == 1, 'lgb_multi_model'] = df_test.loc[df_test['model'] == 1, 'lgb_duo_model_predictions']\ndf_test.loc[df_test['model'] == 2, 'lgb_multi_model'] = df_test.loc[df_test['model'] == 2, 'lgb_trio_model_predictions']\ndf_test.loc[df_test['model'] == 3, 'lgb_multi_model'] = df_test.loc[df_test['model'] == 3, 'lgb_quad_model_predictions']\ndf_test.loc[df_test['model'] == 4, 'lgb_multi_model'] = df_test.loc[df_test['model'] == 4, 'lgb_penta_model_predictions']\ndf_test['lgb_multi_model'] = df_test['lgb_multi_model'].astype(np.float32)\n\ndf_test['xgb_multi_model'] = 0\ndf_test.loc[df_test['model'] == 0, 'xgb_multi_model'] = df_test.loc[df_test['model'] == 0, 'xgb_duo_model_predictions']\ndf_test.loc[df_test['model'] == 1, 'xgb_multi_model'] = df_test.loc[df_test['model'] == 1, 'xgb_duo_model_predictions']\ndf_test.loc[df_test['model'] == 2, 'xgb_multi_model'] = df_test.loc[df_test['model'] == 2, 'xgb_trio_model_predictions']\ndf_test.loc[df_test['model'] == 3, 'xgb_multi_model'] = df_test.loc[df_test['model'] == 3, 'xgb_quad_model_predictions']\ndf_test.loc[df_test['model'] == 4, 'xgb_multi_model'] = np.nan\ndf_test['xgb_multi_model'] = df_test['xgb_multi_model'].astype(np.float32)\n\ndf_test['cb_multi_model'] = 0\ndf_test.loc[df_test['model'] == 0, 'cb_multi_model'] = df_test.loc[df_test['model'] == 0, 'cb_duo_model_predictions']\ndf_test.loc[df_test['model'] == 1, 'cb_multi_model'] = df_test.loc[df_test['model'] == 1, 'cb_duo_model_predictions']\ndf_test.loc[df_test['model'] == 2, 'cb_multi_model'] = df_test.loc[df_test['model'] == 2, 'cb_trio_model_predictions']\ndf_test.loc[df_test['model'] == 3, 'cb_multi_model'] = df_test.loc[df_test['model'] == 3, 'cb_quad_model_predictions']\ndf_test.loc[df_test['model'] == 4, 'cb_multi_model'] = df_test.loc[df_test['model'] == 4, 'cb_penta_model_predictions']\ndf_test['cb_multi_model'] = df_test['cb_multi_model'].astype(np.float32)\n\nPREDICTION_COLS = ['lgb_solo_model', 'xgb_solo_model', 'cb_solo_model', 'lgb_multi_model', 'xgb_multi_model', 'cb_multi_model']\ndf_train[PREDICTION_COLS].to_pickle('train_predictions.pkl')\ndf_test[PREDICTION_COLS].to_pickle('test_predictions.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/liverpool-ion-switching/sample_submission.csv')\nsubmission['open_channels'] = df_test['blend_predictions'].copy()\nsubmission.to_csv('gbdt_blend_submission.csv', index=False, float_format='%.4f')\nprint(f'GBDT Blend Submission\\n\\n{submission[\"open_channels\"].describe()}')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}