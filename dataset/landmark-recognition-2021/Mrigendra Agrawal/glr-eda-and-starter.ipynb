{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://1.bp.blogspot.com/-5pULKUERnIc/Wpc7qPnUCuI/AAAAAAAACao/4YOtEQb_1gEweHRf8-drmi7KBEa1BmBTgCLcBGAs/s1600/image2.png\">","metadata":{}},{"cell_type":"markdown","source":"# About the Competitionüö©\n<p style=\"font-size:15px\">Welcome to the fourth Landmark Recognition competition! This year, we introduce a lot more diversity in the challenge‚Äôs test images in order to measure global landmark recognition performance in a fairer manner. And following last year‚Äôs success, we set this up as a code competition.\n<br></p>\n<p>\nHave you ever gone through your vacation photos and asked yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. This competition challenges Kagglers to build models that recognize the correct landmark (if any) in a dataset of challenging test images.<br></p>\n<p>\nMany Kagglers are familiar with image classification challenges like the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims to recognize 1K general object categories. Landmark recognition is a little different from that: it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. Landmark recognition is challenging in its own way\n<br>\n</p>","metadata":{}},{"cell_type":"markdown","source":"# Data Description\n<div style=\"font-size:15px\">\n there are 2 folder train and test which contains images in jpg format and 2 csv files:-\n<ul>\n    <li><code>train:</code> contains train images in jpf format \n</li>\n    <li><code>test:</code>contains test images in jpf format</li>\n    <li><code>train.csv:</code> labels of train images</li>\n    <li><code>sample_submission.csv:</code> a sample submission file in the correct format\n</li>\n</ul>    \n</div>","metadata":{}},{"cell_type":"markdown","source":"# Evaluation\nSubmissions are evaluated using Global Average Precision (GAP) at (k), where (k=1). This metric is also known as micro Average Precision (\\mu AP), as per references 1 and 2 below. It works as follows:\n\nFor each test image, you will predict one landmark label and a corresponding confidence score. The evaluation treats each prediction as an individual data point in a long list of predictions, sorted in descending order by confidence scores, and computes the Average Precision based on this list.\n\nReferences:\n\n1) F. Perronnin, Y. Liu, and J.-M. Renders, \"A Family of Contextual Measures of Similarity between Distributions with Application to Image Retrieval,\" Proc. CVPR'09\n\n2) T. Weyand, A. Araujo, B. Cao and J. Sim, \"Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval,\" Proc. CVPR'20","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport tensorflow as tf\nimport keras\nimport keras.layers as L\nimport math\nimport cv2\nfrom keras.utils import Sequence\nfrom keras.preprocessing import image\nfrom random import shuffle\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:43.171165Z","iopub.execute_input":"2021-08-12T04:56:43.171583Z","iopub.status.idle":"2021-08-12T04:56:53.743358Z","shell.execute_reply.started":"2021-08-12T04:56:43.1715Z","shell.execute_reply":"2021-08-12T04:56:53.742285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels = pd.read_csv('../input/landmark-recognition-2021/train.csv')\nsample_submission = pd.read_csv('../input/landmark-recognition-2021/sample_submission.csv')\ncounts = train_labels.landmark_id.value_counts()\ncounts = counts[counts >=50].index #indexing only classes which have atleast 50 samples\ntrain_labels = train_labels.loc[train_labels.landmark_id.isin(counts)]\nnum_classes = counts.shape[0] \nprint(num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:53.745152Z","iopub.execute_input":"2021-08-12T04:56:53.745611Z","iopub.status.idle":"2021-08-12T04:56:55.879997Z","shell.execute_reply.started":"2021-08-12T04:56:53.745543Z","shell.execute_reply":"2021-08-12T04:56:55.878926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def id2path(idx,is_train=True):\n    path = '../input/landmark-recognition-2021'\n    if is_train:\n        path += '/train/'+idx[0]+'/'+idx[1]+'/'+idx[2]+'/'+idx+'.jpg'\n    else:\n        path += '/test/'+idx[0]+'/'+idx[1]+'/'+idx[2]+'/'+idx+'.jpg'\n    return path\ntrain_labels['file_path'] = train_labels['id'].apply(id2path)\nsample_submission['file_path'] = sample_submission['id'].apply(id2path,False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:55.881691Z","iopub.execute_input":"2021-08-12T04:56:55.882029Z","iopub.status.idle":"2021-08-12T04:56:56.673168Z","shell.execute_reply.started":"2021-08-12T04:56:55.881994Z","shell.execute_reply":"2021-08-12T04:56:56.672026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_image(idx):\n    image = cv2.imread(idx)\n    image = image/255.\n    image = cv2.resize(image,(256,256))\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:56.674965Z","iopub.execute_input":"2021-08-12T04:56:56.675301Z","iopub.status.idle":"2021-08-12T04:56:56.680506Z","shell.execute_reply.started":"2021-08-12T04:56:56.675267Z","shell.execute_reply":"2021-08-12T04:56:56.679365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(landmark_id=27): #plot images by image_id\n    landmark = train_labels[train_labels['landmark_id']==landmark_id].head(25)\n    imgs = [read_image(x) for x in landmark['file_path']]\n    _, axs = plt.subplots(5,5, figsize=(12, 12))\n    axs = axs.flatten()\n    for i, (img, ax) in enumerate(zip(imgs, axs)):\n        ax.title.set_text(str(landmark['id'].iloc[i]))\n        ax.imshow(img)\n        ax.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:56.682113Z","iopub.execute_input":"2021-08-12T04:56:56.682453Z","iopub.status.idle":"2021-08-12T04:56:56.699831Z","shell.execute_reply.started":"2021-08-12T04:56:56.682412Z","shell.execute_reply":"2021-08-12T04:56:56.698647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:56.70127Z","iopub.execute_input":"2021-08-12T04:56:56.701612Z","iopub.status.idle":"2021-08-12T04:56:59.068601Z","shell.execute_reply.started":"2021-08-12T04:56:56.701559Z","shell.execute_reply":"2021-08-12T04:56:59.067464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(136)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:56:59.070091Z","iopub.execute_input":"2021-08-12T04:56:59.070414Z","iopub.status.idle":"2021-08-12T04:57:01.418049Z","shell.execute_reply.started":"2021-08-12T04:56:59.070383Z","shell.execute_reply":"2021-08-12T04:57:01.416855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(139)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:01.420489Z","iopub.execute_input":"2021-08-12T04:57:01.420843Z","iopub.status.idle":"2021-08-12T04:57:03.666815Z","shell.execute_reply.started":"2021-08-12T04:57:01.42081Z","shell.execute_reply":"2021-08-12T04:57:03.665664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_images(203071)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:03.668815Z","iopub.execute_input":"2021-08-12T04:57:03.66923Z","iopub.status.idle":"2021-08-12T04:57:06.061586Z","shell.execute_reply.started":"2021-08-12T04:57:03.669191Z","shell.execute_reply":"2021-08-12T04:57:06.060767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(Sequence):\n    def __init__(self,idx,y=None,batch_size=32,shuffle=True):\n        self.idx = idx\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        if y is not None:\n            self.is_train=True\n        else:\n            self.is_train=False\n        self.y = y\n    def __len__(self):\n        return math.ceil(len(self.idx)/self.batch_size)\n    def __getitem__(self,ids):\n        batch_ids = self.idx[ids * self.batch_size:(ids + 1) * self.batch_size]\n        if self.y is not None:\n            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n            \n        list_x = np.array([read_image(x) for x in batch_ids])\n        batch_X = np.stack(list_x)\n        if self.is_train:\n            return batch_X, batch_y\n        else:\n            return batch_X\n    \n    def on_epoch_end(self):\n        if self.shuffle and self.is_train:\n            ids_y = list(zip(self.idx, self.y))\n            shuffle(ids_y)\n            self.idx, self.y = list(zip(*ids_y))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:06.062853Z","iopub.execute_input":"2021-08-12T04:57:06.063315Z","iopub.status.idle":"2021-08-12T04:57:06.073541Z","shell.execute_reply.started":"2021-08-12T04:57:06.063276Z","shell.execute_reply":"2021-08-12T04:57:06.072705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_idx =  train_labels['file_path'].values\ny = train_labels['landmark_id'].values\ntest_idx = sample_submission['file_path'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:06.074687Z","iopub.execute_input":"2021-08-12T04:57:06.075153Z","iopub.status.idle":"2021-08-12T04:57:06.098688Z","shell.execute_reply.started":"2021-08-12T04:57:06.075107Z","shell.execute_reply":"2021-08-12T04:57:06.097843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_valid,y_train,y_valid = train_test_split(train_idx,y,test_size=0.05,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:06.099905Z","iopub.execute_input":"2021-08-12T04:57:06.100406Z","iopub.status.idle":"2021-08-12T04:57:06.198728Z","shell.execute_reply.started":"2021-08-12T04:57:06.100366Z","shell.execute_reply":"2021-08-12T04:57:06.197609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(x_train,y_train)\nvalid_dataset = Dataset(x_valid,y_valid)\ntest_dataset = Dataset(test_idx)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:06.200087Z","iopub.execute_input":"2021-08-12T04:57:06.20042Z","iopub.status.idle":"2021-08-12T04:57:06.204827Z","shell.execute_reply.started":"2021-08-12T04:57:06.20038Z","shell.execute_reply":"2021-08-12T04:57:06.203995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/keras-efficientnet-whl/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ../input/keras-efficientnet-whl/efficientnet-1.1.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:57:06.206105Z","iopub.execute_input":"2021-08-12T04:57:06.206672Z","iopub.status.idle":"2021-08-12T04:58:02.644312Z","shell.execute_reply.started":"2021-08-12T04:57:06.206626Z","shell.execute_reply":"2021-08-12T04:58:02.643131Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import efficientnet.keras as efn","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-12T04:58:02.646094Z","iopub.execute_input":"2021-08-12T04:58:02.646516Z","iopub.status.idle":"2021-08-12T04:58:02.946116Z","shell.execute_reply.started":"2021-08-12T04:58:02.646468Z","shell.execute_reply":"2021-08-12T04:58:02.945049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([efn.EfficientNetB0(include_top=False,input_shape=(256,256,3),weights='../input/efficientnet-keras-weights-b0b5/efficientnet-b0_imagenet_1000_notop.h5'),\n        L.GlobalAveragePooling2D(),\n        L.Dense(32,activation='relu'),\n        L.Dense(num_classes, activation='sigmoid')])\nmodel.summary()\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n              loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[keras.metrics.SparseCategoricalAccuracy()])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:58:02.947379Z","iopub.execute_input":"2021-08-12T04:58:02.947681Z","iopub.status.idle":"2021-08-12T04:58:06.231298Z","shell.execute_reply.started":"2021-08-12T04:58:02.947653Z","shell.execute_reply":"2021-08-12T04:58:06.230172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.fit(train_dataset,epochs=1,validation_data=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T04:58:06.232947Z","iopub.execute_input":"2021-08-12T04:58:06.233375Z","iopub.status.idle":"2021-08-12T04:58:06.237795Z","shell.execute_reply.started":"2021-08-12T04:58:06.233329Z","shell.execute_reply":"2021-08-12T04:58:06.236619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preds = model.predict(test_dataset)\n#preds = preds.reshape(-1)\nsample_submission = pd.read_csv('../input/landmark-recognition-2021/sample_submission.csv')\nsample_submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:04:28.190461Z","iopub.execute_input":"2021-08-12T05:04:28.19092Z","iopub.status.idle":"2021-08-12T05:04:28.263858Z","shell.execute_reply.started":"2021-08-12T05:04:28.190829Z","shell.execute_reply":"2021-08-12T05:04:28.262215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><center>If you found this notebook useful please upvote</center></h2>","metadata":{}},{"cell_type":"markdown","source":"<h2><center>Work in Progress ... ‚è≥</center></h2>","metadata":{}}]}