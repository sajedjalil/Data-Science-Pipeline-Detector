{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pretraining knowledge priors with Transformers and adaptation to unseen tasks\n\nThis is an attempt to learn core knowledge priors for ARC using relational neural networks. A lot of fun working on this!\n- Build Transformer neural networks to encode and reason about the relations between objects among the input-output pairs.\n- Pretrain the model on the training set tasks to build representations for core knowledge priors. \n- At test time, first train on the demostration examples of the unseen tasks, then predict the test output. \n- So far, a pretrained model + 2-hr test-time training solves 9% of validation tasks and (only..) 1% of hidden test tasks (0.99 on LB). \n- Longer pretraining time makes adaptation to unseen tasks more efficient. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Learning representations for ARC\n\nFor a neural network to reason about (unseen) ARC tasks, it likely needs a network structure with built-in inductive bias of core knowledge priors and the relational reasoning process, as well as large amounts of pretraining to encode the abstract knowledge, similar to a human's exposure to vast amount of early-life experiences before being able to solve these tasks.\n\n### - Graph networks and Transformers\n\nAs the ARC tasks involve a lot of reasoning of the relationships between objects, [graph networks](https://arxiv.org/pdf/1806.01261.pdf) seem to be a suitable for this problem. A recent work is [Abstract Diagrammatic Reasoning with Multiplex Graph Networks](https://openreview.net/forum?id=ByxQB1BKwH). \n\nHowever, detecting and encoding objects are not easy, as well as synthesizing the output image. For the relatively small grid sizes in ARC, we can think of every grid point as a putative object and use [Transformers](https://arxiv.org/abs/1706.03762) (and [non-local neural networks](https://arxiv.org/abs/1711.07971) in general) as graph-structured neural networks. An example of using transformers on image data is [Image Transformers](https://arxiv.org/abs/1802.05751). \n\n### - Encoding input-output pairs and predicting the test output\n\nPredicting the test output given the test input, conditioned on a set of context input-output pairs, is conceptually a regression problem. Methods such as\n[Neural Processes](https://arxiv.org/abs/1807.01622), [Attentive Neural Processes](https://arxiv.org/abs/1901.05761) produces a series of distributions conditioned on the context pairs to predict the test output. One difference is that each ARC task has only one correct output, so it's not clear if it will benefit from the probablistic nature of neural processes. But we can still use the idea of conditioning.\n\n### - Learning to solve the demostration examples before predicting the test output\n\nSince we're given a few demonstration examples, we could train the neural network to predict the demonstration outputs of the unseen task before attempting on the test output. This is limited by the 2-hour GPU time of the Kaggle kernel. Thus, a faster learning curve here will give a better performance. (I trained all 100 tasks together. Maybe it's also a good idea to train on each task separately..)\n\n### - Pretraining to build representations for core knowledge priors\n\nAs ARC tasks are very hard, we can pretrain the model on the training set with data augmentations. This might help the model learn to better encode core knowledge priors and reason about common object relationships. The training set includes all training data and half of the evaluation data, the other half is used for validation. Of course, the pretrained model can also be directly used on the unseen test tasks without test-time training, but the accuracy is pretty low (validation 1~2%, LB 0%). When using a pretrained model (>8000 epochs) to initialize the weights for test-time training on demonstraction examples, we can increase the validation accuracy to 9% and get 1% on LB. I think longer pretraining with larger models on wide variaties of common sense visual tasks will help build better representations for core knowledge prior. I'm also interested in ideas from meta-learning, curriculum learning, etc, but haven't got a chance to try them yet. \n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model\n\n### - Encoding and summarization of demonstration input/output pairs\n\nEach demonstraction (demo) input ($x_i^{d}$) or output ($y_i^{d}$) image first goes through an embedding layer, a convolution block (optional), and a 2D positional embedding layer before being flattened into 1D to produce $x_i^{d,enc}$, $y_i^{d,enc}$. \n\nEach encoded image then passes through a self-attention layer and a cross-attention layer (input attends to output, and output attends to input) to encode the relationships between objects within the input-output pair, producing $x_i^{d,attn}$, $y_i^{d,attn}$. \n\nTheses representation then goes through two paths. \n\nIn one path, the input/output presentation are mean-aggregated across the node axis and across demo examples as a summarized representation of the task context, $x_i^{d,aggr} = mean(x_i^{d,attn})$, $y_i^{d,aggr} = mean(y_i^{d,attn})$, $x^{d,aggr} = \\frac{1}{n} \\sum_i x_i^{d,aggr}$, $y^{d,aggr} = \\frac{1}{n} \\sum_i y_i^{d,aggr}$. The generation of test output given the test input will be conditioned on these aggregated context pairs.\n\nIn the other path, input/output representation across different demo pairs are concatenated ($x^{d,cat}$, $y^{d,cat}$).\n\n### - Predicting test outputs conditioned on the context pairs\n\nThe test input ($x^{t}$) passes through the same initial encoding as demo inputs to produce $x^{t,enc}$. The test output ($y^{t}$) is initialized as blank with added positional encoding to produce $y^{t,enc}$. To condition on the context pairs, the aggregated demo input/output representations $x^{d,aggr}$, $y^{d,aggr}$ are added to test input/output encodings. The conditioning can also be made probablistic as in neural processes. \n\nFinaly the encoded test input, the concatenated demo input/output representations, and the encoded test output goes through a series of configurable self- and cross-attention blocks to generate the output image. \n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport time\nimport os\nimport sys\nimport argparse\nimport logging\nimport random\nimport numpy as np\nimport pickle\nimport collections\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom types import SimpleNamespace\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport plot_utils\nimport data_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_DIR = \"../input/abstraction-and-reasoning-challenge/test/\"\nEVAL_EVERY = None  # set eval_every to None\n\nPRESUBMIT = False\nfake_test_files = os.listdir(TEST_DIR)\nif \"0c9aba6e.json\" in fake_test_files:\n    PRESUBMIT = True\n    TEST_DIR = \"../input/arc-eval-100/eval_100/\"\n    EVAL_EVERY = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KAGGLE_KERNEL = True\nINIT_MODEL = \"../input/arc-pretrained-13990/pretrained_13990.pt\"\nTIME_LIMIT = 110  # minutes\nCONFIG = {\n    \"init_model\": INIT_MODEL,  # pretrained model path\n    \"n_ensemble\": 3,  # number of final predictions\n    \"name\": \"Kaggle\",  # extra name\n    \"message\": \"\",  # notes\n    \"n_epochs\": 3000,  # number of epochs (if within time limit)\n    \"time_limit\": TIME_LIMIT,  # time limit before stop (in minutes)\n    \"lr\": 0.0015,  # learning rate\n    \"half_lr_after\": None,  # reduce the lr by half after ? epochs\n    \"optim_update_every\": 150,  # accumulate gradient for ? iterations before optimizer update\n    \"eval_every\": EVAL_EVERY,  # eval every ? epochs\n    \"save_every\": 10,  # save every ? epochs\n    \"print_every\": 20,  # eval every ? epochs\n    \"n_checkpoints_kept\": 5,  # how many checkpoints to keep\n    \"resume\": \"\",  # checkpoint path to resume from\n    \"gpu\": 0,  # which gpu\n    \"aux_ratio\": 0.3  # auxiliary loss ratio\n}\nargs = SimpleNamespace(**CONFIG)  # to mimic argparse api","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logging\nlog = logging.getLogger()\nlog.setLevel(logging.INFO)\nif (KAGGLE_KERNEL and len(log.handlers) <=2) or not log.handlers:\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(logging.INFO)\n    log.addHandler(handler)\n\nSTART_TIME = datetime.now()\nlog.info(\"Current time: \" + START_TIME.strftime(\"%Y%m%d %H:%M\"))\nDEVICE = torch.device(\"cuda:\" + str(args.gpu) if torch.cuda.is_available() else \"cpu\")\nlog.info(\"Using device {}\".format(DEVICE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributions as dist\nimport math\n\nimport numpy as np\n\n\ndef build_model(device=\"cuda\"):\n    d_enc, d_ff = 128, 512\n    conv_encoder = None\n    conv_encoder = ConvImageEncoder(d_enc, n_blocks=1)\n    demo_encoder = SCAB_Encoder_Batch(\n        n_self_attn=1, n_cross_attn=1, d_enc=d_enc, n_heads=4, d_ff=d_ff, drop=0.1)\n    test_decoder = SCAB_Decoder(\n        layers=[\"xy\", \"dy\", \"xy\"], d_enc=d_enc, n_heads=4, d_ff=d_ff, drop=0.1)\n    model = ArcTransformer_MultiPath_Wrapper(\n        d_enc=d_enc, demo_encoder=demo_encoder, test_decoder=test_decoder, conv_encoder=conv_encoder,\n        aggr_x=True, aggr_y=True, aggr_x_cat=False, aggr_y_cat=False, aggr_shrink=3, device=device).to(device)\n    return model\n\n\nclass ArcTransformer_MultiPath_Wrapper(nn.Module):\n    def __init__(self, d_enc, demo_encoder, test_decoder, conv_encoder=None, n_colors=10,\n                 aggr_x=True, aggr_y=True, aggr_x_cat=False, aggr_y_cat=False, aggr_shrink=1, device=\"cuda\"):\n        super().__init__()\n        self.d_enc = d_enc\n        self.aggr_x = aggr_x\n        self.aggr_y = aggr_y\n        self.aggr_x_cat = aggr_x_cat\n        self.aggr_y_cat = aggr_y_cat\n        self.aggr_shrink = aggr_shrink\n        self.positional_encoder = GridPositionalEncoderWithCache(\n            d_enc=d_enc, max_img_shape=(30, 30), device=device)\n        \n        self.conv_encoder = conv_encoder\n\n        self.enc1 = nn.Sequential(  # (w_in, h_in)\n            UnsqueezeBatch(),  # (1, w_in, h_in)\n            nn.Embedding(n_colors, d_enc),  # (1, w_in, h_in, d_embed)\n        )\n\n        self.enc2 = nn.Sequential(\n            self.positional_encoder,  # (1, w_in, h_in, d_embed)\n            nn.Flatten(start_dim=-3, end_dim=-2),  # (1, n_nodes, d_embed)\n        )\n        self.enc_y_test = nn.Sequential(  # (w_in, h_in, d_embed)\n            UnsqueezeBatch(),  # (1, w_in, h_in, d_embed)\n            # no embedding\n            self.positional_encoder,  # (1, w_in, h_in, d_embed)\n            nn.Flatten(start_dim=-3, end_dim=-2),  # (1, n_nodes, d_embed)\n        )\n\n        self.demo_encoder = demo_encoder\n        self.test_decoder = test_decoder\n\n        self.fc_out = nn.Linear(d_enc, n_colors)\n\n    def forward(self, x_demos_raw, y_demos_raw, x_test, y_test_shape):\n        n_demos = len(x_demos_raw)\n\n        x_demos, y_demos = [None] * n_demos, [None] * n_demos\n        for i in range(n_demos):\n            x_demos[i] = self.enc1(x_demos_raw[i])  # (1, w_in, h_in, d_enc)\n            y_demos[i] = self.enc1(y_demos_raw[i])  # (1, w_in, h_in, d_enc)\n        x_test = self.enc1(x_test)\n        \n        if self.conv_encoder is not None:\n            all_demos = batch_2d_layer(self.conv_encoder, x_demos + y_demos + [x_test])\n            x_demos, y_demos, x_test = all_demos[:len(all_demos)//2], all_demos[len(all_demos)//2:-1], all_demos[-1]\n\n        for i in range(n_demos):\n            x_demos[i] = self.enc2(x_demos[i])  # (1, x_nodes, d_enc)\n            y_demos[i] = self.enc2(y_demos[i])  # (1, y_nodes, d_enc)\n        x_test = self.enc2(x_test)\n\n        y_test = torch.zeros((*y_test_shape, self.d_enc),\n                             dtype=torch.float, device=x_test.device)\n        \n        \n        y_test = self.enc_y_test(y_test)  # (1, n_nodes, d_enc)\n\n        # transformer layers\n        y_aggr, (x_demos_cat, y_demos_cat, x_aggr) = \\\n            self.demo_encoder(x_demos, y_demos)\n        \n        if self.aggr_x:\n            if self.aggr_x_cat:\n                x_test = torch.cat([x_test, x_aggr.repeat(1, x_test.shape[1], 1)], axis=-1)\n            else:\n                x_test += x_aggr / self.aggr_shrink\n        \n        if self.aggr_y:\n            if self.aggr_y_cat:\n                y_test = torch.cat([y_test, y_aggr.repeat(1, y_test.shape[1], 1)], axis=-1)\n            else:\n                y_test += y_aggr / self.aggr_shrink\n        \n        y_test, x_test, auxiliary = \\\n            self.test_decoder(x_demos_cat, y_demos_cat, x_test, y_test)\n\n        y_test = self.fc_out(y_test)\n        y_test = F.log_softmax(y_test, dim=-1)  # (1, n_nodes, n_colors)\n        y_test = y_test.view((1, *y_test_shape, y_test.shape[-1]))\n\n        aux_out = []\n        for y in auxiliary:\n            aux = self.fc_out(y)\n            aux = F.log_softmax(aux, dim=-1)  # (1, n_nodes, n_colors)\n            aux = aux.view((1, *y_test_shape, aux.shape[-1]))\n            aux_out.append(aux)\n\n        return y_test, (x_demos_cat, y_demos_cat, x_test), aux_out  # (1, w_out, h_out, d_out)\n\n\nclass SCAB_Encoder_Batch(nn.Module):\n    def __init__(self, n_self_attn=1, n_cross_attn=1, d_enc=128, n_heads=4, d_ff=None, drop=0, ln=True):\n        super().__init__()\n        self.n_self_attn = n_self_attn\n        self.n_cross_attn = n_cross_attn\n        # aggr\n        self.aggr_x = nn.Sequential(\n            nn.Linear(d_enc, d_enc),\n            nn.ReLU(),\n            nn.Linear(d_enc, d_enc))\n        self.aggr_y = nn.Sequential(\n            nn.Linear(d_enc, d_enc),\n            nn.ReLU(),\n            nn.Linear(d_enc, d_enc))\n\n        # enc demo inputs\n        self.demo_self_enc = nn.ModuleList(\n            [SAB(d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(n_self_attn)])\n        self.demo_yx_enc = nn.ModuleList(\n            [SCAB(d_enc, d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(n_cross_attn)])\n        self.demo_xy_enc = nn.ModuleList(\n            [SCAB(d_enc, d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(n_cross_attn)])\n\n    def forward(self, x_demos, y_demos):\n        # encode demos\n        n_demos = len(x_demos)\n\n        for i in range(self.n_self_attn):\n            all_demos = batch_layer(self.demo_self_enc[i], x_demos + y_demos)\n            x_demos, y_demos = all_demos[:len(all_demos)//2], all_demos[len(all_demos)//2:]\n        \n        for i in range(self.n_cross_attn):\n            x_demos, y_demos = \\\n                batch_cross_attn(self.demo_xy_enc[i], x_demos, y_demos), \\\n                batch_cross_attn(self.demo_yx_enc[i], y_demos, x_demos)\n\n        inds_x = np.cumsum([0] + [d.shape[1] for d in x_demos]).tolist()  # [0, 3, 7, ...]\n        inds_y = np.cumsum([0] + [d.shape[1] for d in y_demos]).tolist()\n\n        x_demos_cat = torch.cat(x_demos, axis=1)  # (1, n_total, d_enc)\n        y_demos_cat = torch.cat(y_demos, axis=1)\n\n        x_demos_aggr = x_demos_cat + self.aggr_x(x_demos_cat)\n        y_demos_aggr = y_demos_cat + self.aggr_y(y_demos_cat)\n\n        x_aggr = 0\n        for i in range(n_demos):\n            x_aggr += x_demos_aggr[:, inds_x[i]:inds_x[i+1]].mean(1) / n_demos\n        y_aggr = 0\n        for i in range(n_demos):\n            y_aggr += y_demos_aggr[:, inds_y[i]:inds_y[i+1]].mean(1) / n_demos\n\n        return y_aggr, (x_demos_cat, y_demos_cat, x_aggr)\n\n\nclass SCAB_Decoder(nn.Module):\n    \"\"\"\n    layers: 'x' means self-attn(x_test), 'y' means cross-attn(y_test, x_test),\n        'd' -- cross-attn(ytest or x_test, y_demos_cat or x_demos_cat)\n    \"\"\"\n\n    def __init__(self, layers=[\"x\", \"xy\", \"dy\", \"dy\"], d_enc=128, n_heads=4, d_ff=None, drop=0,\n                 ln=True):\n        super().__init__()\n        assert(\"y\" in layers[-1])\n\n        self.layers = layers\n        self.n_x_enc = len([s for s in layers if \"x\" in s])\n        self.n_yx_dec = len([s for s in layers if \"y\" in s])\n        self.n_demo_dec = len([s for s in layers if \"d\" in s])\n\n        # enc x_test\n        self.x_enc = nn.ModuleList(\n            [SAB(d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(self.n_x_enc)])\n        \n        # dec y_test\n        self.yx_dec = nn.ModuleList(\n            [SCAB(d_enc, d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(self.n_yx_dec)])\n        \n        self.x_vert_dec = nn.ModuleList(\n            [SCAB(d_enc, d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(self.n_demo_dec)])\n        self.y_vert_dec = nn.ModuleList(\n            [SCAB(d_enc, d_enc, d_enc, num_heads=n_heads, dim_F=d_ff, drop=drop, ln=ln) \\\n                for _ in range(self.n_demo_dec)])\n\n    def forward(self, x_demos_cat, y_demos_cat, x_test, y_test):\n\n        auxiliary = []\n        xi, di, yi = 0, 0, 0\n        for i, layer in enumerate(self.layers):\n            if \"x\" in layer:\n                x_test = self.x_enc[xi](x_test)\n                xi += 1\n            if \"d\" in layer:\n                x_test = self.x_vert_dec[di](x_test, x_demos_cat)\n                y_test = self.y_vert_dec[di](y_test, y_demos_cat)\n                di += 1\n            if \"y\" in layer:\n                y_test = self.yx_dec[yi](y_test, x_test)\n                yi += 1\n                if i != len(self.layers) - 1:\n                    auxiliary.append(y_test)\n\n        return y_test, x_test, auxiliary\n\n\ndef batch_layer(function, tensors):\n    \"\"\"\n    function: a neural network that takes a signle tensor\n    tensors: list of tensors of shape [1, n_nodes, n_dims], each tensor can have \n        different n_nodes, but all need to have the same n_dims\n    \"\"\"\n    n_tensors = len(tensors)\n    lengths = [x.shape[1] for x in tensors]\n    n_dims = tensors[0].shape[2]\n    batch = torch.zeros((n_tensors, max(lengths), n_dims), device=tensors[0].device)\n    mask = torch.zeros((n_tensors, max(lengths)), dtype=torch.bool, device=tensors[0].device)\n    for i, tensor in enumerate(tensors):\n        batch[i, 0:lengths[i]] = tensor\n        mask[i, 0:lengths[i]] = 1\n    batch = function(batch, mask=mask)\n\n    out = [None for _ in range(len(tensors))]\n    for i in range(len(tensors)):\n        out[i] = batch[i, 0:lengths[i]].unsqueeze(0)\n    return out\n\n\ndef batch_2d_layer(function, tensors):\n    \"\"\"\n    function: a neural network that takes a signle tensor\n    tensors: list of tensors of shape [1, n_nodes, n_dims], each tensor can have \n        different n_nodes, but all need to have the same n_dims\n    \"\"\"\n    n_tensors = len(tensors)\n    widths, heights = [x.shape[1] for x in tensors], [x.shape[2] for x in tensors]\n    n_dims = tensors[0].shape[3]\n    batch = torch.zeros((n_tensors, max(widths), max(heights), n_dims), device=tensors[0].device)\n    mask = torch.zeros((n_tensors, max(widths), max(heights)), dtype=torch.bool, device=tensors[0].device)\n    for i, tensor in enumerate(tensors):\n        batch[i, 0:widths[i], 0:heights[i]] = tensor\n        mask[i, 0:widths[i], 0:heights[i]] = 1\n    batch = function(batch, mask=mask)\n\n    out = [None for _ in range(len(tensors))]\n    for i in range(len(tensors)):\n        out[i] = batch[i, 0:widths[i], 0:heights[i]].unsqueeze(0)\n    return out\n\n\ndef batch_cross_attn(function, this, other):\n    \"\"\"\n    function: a neural network that takes a two tensors and returns a single tensor\n    this: list of tensors of shape [1, n_nodes, n_dims], each tensor can have\n        different n_nodes, but all need to have the same n_dims.\n    other: must be the same length as this\n    \"\"\"\n    assert(len(this) == len(other))\n    n_tensors = len(this)\n\n    len_this = [x.shape[1] for x in this]\n    n_dims = this[0].shape[2]\n    batch_this = torch.zeros((n_tensors, max(len_this), n_dims), device=this[0].device)\n    mask_this = torch.zeros((n_tensors, max(len_this)), dtype=torch.bool, device=this[0].device)\n    for i, tensor in enumerate(this):\n        batch_this[i, 0:len_this[i]] = tensor\n        mask_this[i, 0:len_this[i]] = 1\n\n    len_other = [x.shape[1] for x in other]\n    n_dims = other[0].shape[2]\n    batch_other = torch.zeros((n_tensors, max(len_other), n_dims), device=other[0].device)\n    mask_other = torch.zeros((n_tensors, max(len_other)), dtype=torch.bool, device=other[0].device)\n    for i, tensor in enumerate(other):\n        batch_other[i, 0:len_other[i]] = tensor\n        mask_other[i, 0:len_other[i]] = 1\n\n    batch_this = function(batch_this, batch_other, mask_this, mask_other)\n    # should not mutates the input tensor array\n    out = [None for _ in range(len(this))]\n    for i in range(len(this)):\n        out[i] = batch_this[i, 0:len_this[i]].unsqueeze(0)\n    return out\n\n\nclass MAB(nn.Module):\n    # adapted from https://github.com/juho-lee/set_transformer\n    def __init__(self, dim_Q, dim_K, dim_V, num_heads, dim_F=None, drop=0, ln=False, ff=True):\n        super(MAB, self).__init__()\n        self.dim_Q = dim_Q\n        self.dim_V = dim_V\n        if dim_F is None:\n            dim_F = dim_V * 4\n        self.num_heads = num_heads\n        self.feed_forward = ff\n        self.fc_q = nn.Linear(dim_Q, dim_V)\n        self.fc_k = nn.Linear(dim_K, dim_V)\n        self.fc_v = nn.Linear(dim_K, dim_V)\n        if ln:\n            self.ln0 = nn.LayerNorm(dim_V)\n            self.ln1 = nn.LayerNorm(dim_V)\n        self.fc_o1 = nn.Linear(dim_V, dim_F)\n        self.fc_o2 = nn.Linear(dim_F, dim_V)\n        self.dropout1 = nn.Dropout(drop)\n        self.dropout2 = nn.Dropout(drop)\n\n    def forward(self, Q, K, mask=None):\n        Q_ori = Q\n        Q = self.fc_q(Q)\n        K, V = self.fc_k(K), self.fc_v(K)\n\n        dim_split = self.dim_V // self.num_heads\n        Q_ = torch.cat(Q.split(dim_split, 2), 0)  # (batch*n_heads, n, dim_split)\n        K_ = torch.cat(K.split(dim_split, 2), 0)\n        V_ = torch.cat(V.split(dim_split, 2), 0)\n\n        scores = Q_.bmm(K_.transpose(1, 2))/math.sqrt(self.dim_V)  # (batch, n_q, n_k)\n        if mask is not None:  # mask the scores along the K axis\n            mask = mask.repeat(self.num_heads, 1).unsqueeze(1)  # (batch*n_heads, 1, n_K)\n            # scores = scores.masked_fill(mask == 0, -1e9)\n            scores.masked_fill_(mask == 0, -1e9)\n\n        A = torch.softmax(scores, 2)\n        # original MAB implementation\n        # O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n        # changed to real residual connection (next two lines)\n        O = torch.cat(A.bmm(V_).split(Q.size(0), 0), 2)\n        if self.dim_Q == self.dim_V:\n            O = Q_ori + self.dropout1(O)  # this requires dim_Q == dim_V\n        else:\n            O = Q + self.dropout1(O)\n\n        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n        if self.feed_forward:\n            O = O + self.dropout2(self.fc_o2(F.relu(self.fc_o1(O))))\n            O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n        return O\n\n\nclass SAB(nn.Module):\n    def __init__(self, dim_in, dim_out, num_heads, dim_F=None, drop=0, ln=False):\n        super(SAB, self).__init__()\n        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, dim_F, drop=drop, ln=ln)\n\n    def forward(self, X, mask=None):\n        return self.mab(X, X, mask=mask)\n\n\nclass SCAB(nn.Module):\n    def __init__(self, dim_q, dim_k, dim_out, num_heads, dim_F=None, drop=0, ln=False):\n        super().__init__()\n        self.mab0 = MAB(dim_q, dim_q, dim_out, num_heads, dim_F, drop=drop, ln=ln, ff=False)\n        self.mab1 = MAB(dim_out, dim_k, dim_out, num_heads, dim_F, drop=drop, ln=ln)\n\n    def forward(self, Xq, Xk, mask_q=None, mask_k=None):\n        return self.mab1(self.mab0(Xq, Xq, mask=mask_q), Xk, mask=mask_k)\n\n\nclass UnsqueezeBatch(nn.Module):\n    def forward(self, X):\n        return X.unsqueeze(0)\n\n\nclass GridPositionalEncoderWithCache(nn.Module):\n    # adapted from https://github.com/sahajgarg/image_transformer\n    def __init__(self, d_enc, max_img_shape, min_timescale=1.0, max_timescale=1.0e4,\n                 num_dims=2, device=\"cpu\"):\n        super().__init__()\n        self.d_enc = d_enc\n        self.min_timescale = min_timescale\n        self.max_timescale = max_timescale\n        self.num_dims = num_dims\n        self.full_size_signal = self.positional_signal(max_img_shape, device)\n\n    def positional_signal(self, shape, device=\"cpu\"):\n        # X: (batch, x, y, hidden)\n        # return: add in the range of (-1, 1)\n        num_timescales = self.d_enc // (self.num_dims * 2)\n        log_timescale_increment = np.log(\n            self.max_timescale / self.min_timescale) / (num_timescales - 1)\n        inv_timescales = self.min_timescale * \\\n            torch.exp((torch.arange(num_timescales).float()\n                       * -log_timescale_increment))\n        inv_timescales = inv_timescales.to(device)\n        total_signal = torch.zeros((1, shape[0], shape[1], self.d_enc), device=device)\n        for dim in range(self.num_dims):\n            length = shape[dim]\n            position = torch.arange(length).float().to(device)\n            scaled_time = position.view(-1, 1) * inv_timescales.view(1, -1)\n            signal = torch.cat(\n                [torch.sin(scaled_time), torch.cos(scaled_time)], 1)\n            prepad = dim * 2 * num_timescales\n            postpad = self.d_enc - (dim + 1) * 2 * num_timescales\n            signal = F.pad(signal, (prepad, postpad))\n            for _ in range(1 + dim):\n                signal = signal.unsqueeze(0)\n            for _ in range(self.num_dims - 1 - dim):\n                signal = signal.unsqueeze(-2)\n            # X += signal\n            total_signal += signal\n        return total_signal\n\n    def forward(self, X):\n        # X: (batch, width, height, d_enc)\n        X = X + self.full_size_signal[:, :X.shape[1], :X.shape[2]]\n        return X\n\n\nclass ConvImageEncoder(nn.Module):\n    def __init__(self, d_enc, n_blocks=1):\n        super(ConvImageEncoder, self).__init__()\n        self.n_blocks = n_blocks\n        self.blocks = nn.ModuleList([BasicBlock(d_enc, d_enc, stride=1) for _ in range(n_blocks)])\n\n    def forward(self, x, mask=None):\n        out = x.permute(0, 3, 1, 2).contiguous()\n        for i in range(self.n_blocks):\n            out = self.blocks[i](out, mask=mask)\n        out = out.permute(0, 2, 3, 1).contiguous()\n        return out\n\n\nclass BasicBlock(nn.Module):\n    # https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.InstanceNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.InstanceNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.InstanceNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x, mask=None):\n        out = F.relu(self.bn1(self.conv1(x)))\n        if mask is not None:\n            out.masked_fill_(mask.unsqueeze(1) == 0, -1e9)\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        if mask is not None:\n            out.masked_fill_(mask.unsqueeze(1) == 0, -1e9)\n        out = F.relu(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(training_set, test_set, model_id):\n    # build model (must be the same model as the pretrained checkpoint)\n    model = build_model(DEVICE)\n\n    training_ids = sorted(list(training_set.keys()))\n    test_ids = sorted(list(test_set.keys()))\n    # initialize stats\n    train_losses = []\n    eval_losses = []\n    train_top1 = []\n    eval_top1 = []\n    train_soft_acc = []\n    eval_soft_acc = []\n    min_eval_loss = np.inf  # not a good indicator of improvement\n    max_eval_soft_acc = 0\n    max_eval_soft_acc_fname = \"\"\n    fname_queue = collections.deque()\n\n    # training parameters\n    lr = args.lr\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=lr, weight_decay=0)\n    criterion = nn.NLLLoss()\n\n    epoch_start = 1\n    n_iter = 0\n\n    if not args.resume:\n        if args.init_model:\n            # start from a pretrained checkpoint\n            checkpoint = torch.load(args.init_model, map_location='cuda:{}'.format(args.gpu))\n            model.load_state_dict(checkpoint['model_state_dict'])\n            model.to(DEVICE)\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n            model_id += \"e{}\".format(checkpoint['epoch'])\n        else:\n            # start from scratch\n            log.warning(\"Training from scratch.\")\n            model_id += \"e0\"\n    else:\n        # resume from partially trainded model\n        model_checkpoint = args.resume\n        checkpoint = torch.load(model_checkpoint)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.to(DEVICE)\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n        epoch_start = checkpoint['epoch'] + 1\n        n_iter = checkpoint['epoch'] * len(training_ids)\n        with open(model_checkpoint[:-3] + \"_stats.pkl\", 'rb') as f:\n            train_losses, eval_losses, train_top1, eval_top1 = pickle.load(f)\n        model_id = os.path.dirname(model_checkpoint).split('/')[-1]\n    \n    # save checkpoints in model_dir, save stats in log_dir\n    model_dir = log_dir = model_id + \"/\"\n    if not os.path.isdir(model_dir):\n        os.mkdir(model_dir)\n    if not os.path.isdir(log_dir):\n        os.mkdir(log_dir)\n    if args.message:\n        with open(os.path.join(log_dir, \"README_MICROTRAIN.md\"), \"w\") as f:\n            f.write(args.message)\n    writer = SummaryWriter(log_dir=log_dir)\n    \n    optimizer.zero_grad()\n\n    # run the training loop\n    for epoch in range(epoch_start, args.n_epochs + 1):\n        current_time = datetime.now()\n        since_start = (current_time - START_TIME).total_seconds() / 60  # in minutes\n        if since_start >= args.time_limit:\n            log.info(\"Stopping training at {} epochs.\".format(epoch))\n            log.info(\"Current time: {}, start time: {}\".format(\n                current_time.strftime(\"%Y%m%d %H:%M\"), START_TIME.strftime(\"%Y%m%d %H:%M\")))\n            break\n\n        if args.half_lr_after == epoch:\n            lr /= 2.0\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n\n        t_start = time.time()\n        shuffled_ids = training_ids[:]\n        random.shuffle(shuffled_ids)\n\n        model.train()\n        losses_epoch = []\n        top1_epoch = 0\n        soft_acc_epoch = 0\n        aux_losses_epoch = []\n\n        for train_id in shuffled_ids:\n            n_iter += 1\n            data = training_set[train_id]\n\n            # data augmentation\n            data = data_utils.shuffle_demo_test(data, demo_only=True)  # only using demo data\n            # add flip/roate augmentation\n            data = data_utils.random_flip(data)\n            data = data_utils.random_transpose(data)\n            data, perm_colors = data_utils.permute_colors(data, device=DEVICE)\n\n            x_demos, y_demos = [], []\n            for demo in data['train']:\n                x_demos.append(demo['input'])\n                y_demos.append(demo['output'])\n\n            test_id = random.randrange(len(data['test']))\n            test_pair = data['test'][test_id]\n\n            x_test, y_test = test_pair['input'], test_pair['output']\n\n            y_test_shape = y_test.shape  # provide known shape in training\n\n            y_prob, _, auxiliary = model(x_demos, y_demos, x_test, y_test_shape)\n            assert(y_prob.argmax(-1).squeeze(0).shape == y_test_shape)\n\n            loss = criterion(y_prob.permute(0, 3, 1, 2), y_test.unsqueeze(0))\n            y_pred = y_prob.argmax(-1).squeeze(0)\n            top1_epoch += torch.equal(y_pred, y_test)\n            soft_acc_epoch += torch.mean((y_pred == y_test).float()).item()\n\n            if args.aux_ratio > 0:\n                # if use auxiliary loss\n                if auxiliary is None:\n                    raise AssertionError(\"aux ratio > 0 but no Aux loss detected.\")\n                total_loss = 0\n                for aux in auxiliary:\n                    total_loss += args.aux_ratio * criterion(aux.permute(0, 3, 1, 2), y_test.unsqueeze(0))\n                total_loss += loss\n                total_loss.backward()\n                aux_losses_epoch.append(total_loss.item())\n            else:\n                # normal loss\n                loss.backward()\n\n            # batch optimizer update (TODO: parallelize batch update)\n            if n_iter % args.optim_update_every == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n            losses_epoch.append(loss.item())\n\n        # stats\n        losses_epoch = np.mean(losses_epoch)\n        top1_epoch = top1_epoch / float(len(training_ids))\n        soft_acc_epoch = soft_acc_epoch / float(len(training_ids))\n        train_losses.append(losses_epoch)\n        train_top1.append(top1_epoch)\n        train_soft_acc.append(soft_acc_epoch)\n        time_per_epoch = (time.time() - t_start)\n        time_per_iter = time_per_epoch / len(shuffled_ids)\n        time_now = datetime.now().strftime(\"%m%d%H%M\")\n        log_str = \"{} ep:{} it:{} NLL:{:.3f} soft acc: {:.3f} top1:{:.4f} t/ep:{:.1f} t/it:{:.3f} lr:{}\".format(\n            time_now, epoch, n_iter, losses_epoch, soft_acc_epoch, top1_epoch, time_per_epoch, time_per_iter, lr)\n        if args.aux_ratio:\n            log_str += \", NLL+Aux:{:.3f}\".format(np.mean(aux_losses_epoch))\n        if args.print_every and epoch % args.print_every == 0:\n            log.info(log_str)\n        writer.add_scalar('Loss/Train', losses_epoch, epoch)\n        writer.add_scalar('Accuracy_top1/Train', top1_epoch, epoch)\n        writer.add_scalar('Soft_Accuracy/Train', soft_acc_epoch, epoch)\n\n        # Evaluation\n        if args.eval_every and epoch % args.eval_every == 0:\n            torch.cuda.empty_cache()\n            model.eval()\n\n            eval_loss = []\n            top1_epoch = 0\n            soft_acc_epoch = 0\n            for test_id in test_ids:\n                data = test_set[test_id]\n                x_demos, y_demos = [], []\n                for demo in data['train']:\n                    x_demos.append(demo['input'])\n                    y_demos.append(demo['output'])\n\n                test_id = 0\n                test_pair = data['test'][test_id]\n\n                x_test, y_test = test_pair['input'], test_pair['output']\n                \n                y_test_shape = y_test.shape  # provide known shape during training\n\n                y_prob, _, _ = model(x_demos, y_demos, x_test, y_test_shape)\n                assert(y_prob.argmax(-1).squeeze(0).shape == y_test_shape)\n\n                loss = criterion(y_prob.permute(0, 3, 1, 2), y_test.unsqueeze(0))\n                y_pred = y_prob.argmax(-1).squeeze(0)\n                top1_epoch += torch.equal(y_pred, y_test)\n                soft_acc_epoch += torch.mean((y_pred == y_test).float()).item()\n\n                eval_loss.append(loss.item())\n\n            eval_loss = np.mean(eval_loss)\n            top1_epoch = top1_epoch / float(len(test_ids))\n            soft_acc_epoch = soft_acc_epoch / float(len(test_ids))\n            eval_losses.append((epoch, eval_loss))\n            eval_top1.append((epoch, top1_epoch))\n            eval_soft_acc.append((epoch, soft_acc_epoch))\n            log.info(\"{} Eval, NLL:{:.3f} soft acc: {:.3f} top1:{:.4f}\".format(model_id, eval_loss, soft_acc_epoch, top1_epoch))\n            writer.add_scalar('Loss/Validation', eval_loss, epoch)\n            writer.add_scalar('Accuracy_top1/Validation', top1_epoch, epoch)\n            writer.add_scalar('Soft_Accuracy/Validation', soft_acc_epoch, epoch)\n\n        if args.save_every and epoch % args.save_every == 0:\n            # remove previous checkpoints\n            if len(fname_queue) >= args.n_checkpoints_kept:\n                fname = fname_queue.popleft()\n                if os.path.isfile(fname):\n                    os.remove(fname)\n                pickle_fname = fname.strip(\".pt\") + \"_stats.pkl\"\n                if os.path.isfile(pickle_fname):\n                    os.remove(pickle_fname)\n            \n            # save model checkpoint\n            fname = os.path.join(model_dir, model_id + '_' + str(epoch) + '.pt')\n            torch_save_dict = {\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict()\n            }\n            torch.save(torch_save_dict, fname)\n            fname_queue.append(fname)\n\n            # save stats in pickle file\n            pickle_fname = '{}_stats.pkl'.format(fname.strip(\".pt\"))\n            with open(pickle_fname, 'wb') as f:\n                pickle.dump([train_losses, eval_losses, train_top1, eval_top1], f)\n            \n            # save best validation checkpoint\n            if args.eval_every and soft_acc_epoch > max_eval_soft_acc:\n                if os.path.isfile(max_eval_soft_acc_fname):\n                    os.remove(max_eval_soft_acc_fname)\n                max_eval_soft_acc_fname = os.path.join(model_dir, '_best_eval_' + str(epoch) + '.pt')\n                torch.save(torch_save_dict, max_eval_soft_acc_fname)\n                max_eval_soft_acc = soft_acc_epoch\n\n    return model, (fname_queue, max_eval_soft_acc_fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, test_set, plot=False):\n    model.eval()\n    test_ids = list(test_set.keys())\n    res = []\n    for test_id in test_ids:\n        data = test_set[test_id]\n\n        x_demos, y_demos = [], []\n        for demo in data['train']:\n            x_demos.append(demo['input'])\n            y_demos.append(demo['output'])\n\n        # test_output = data_utils.create_dummy_test_output(test_data)\n        for i, test_pair in enumerate(data['test']):\n            x_test = test_pair['input']\n            # TODO: add better shape prediction\n            y_test_shape = data_utils.predict_shape(data, x_test)\n            y_prob, _, _ = model(x_demos, y_demos, x_test, y_test_shape)\n            y_pred1 = y_prob.argmax(-1).squeeze(0)\n            test_pair['pred'] = y_pred1.detach().cpu()\n            \n            y_pred1 = data_utils.flattener(y_pred1.cpu().numpy().tolist())\n            \n            output_string = y_pred1\n            # top3 = [y_pred1, y_pred1, y_pred1]  # TODO: get the top 3\n            # output_string = ' '.join(top3)\n            output_id = test_id + '_' + str(i)\n            res.append({'output_id': output_id, 'output': output_string})\n        \n        if plot:\n            plot_utils.plot_task(data)\n            \n    return pd.DataFrame(res, columns=['output_id', 'output'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = \"Microtrain\"\nmodel_id = model_name + datetime.now().strftime(\"x%Y%m%dx%H%M\")\nif args.name:\n    model_id += \"x\" + args.name\n\n# load test data\ntest_raw = data_utils.load_dataset(TEST_DIR)\ntest_set = data_utils.dataset_to_tensor(test_raw, device=DEVICE)\n# load micro-training data (without test input/output)\ntraining_set = data_utils.dataset_to_tensor(\n    test_raw, device=DEVICE, include_test=False)\n\nlog.info(\"Loaded {} test examples.\".format(len(test_set)))\n\nmodel, (fname_queue, _) = train(training_set, test_set, model_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()\nres = predict(model, test_set, plot=True)\n\nif args.n_ensemble > 1:\n    res = [res]\n\n    # other single checkpoints\n    n_models = args.n_ensemble - 1\n    checkpoint_fnames = list(fname_queue)[-(n_models+1): -1]\n    for model_checkpoint in checkpoint_fnames:\n        del model\n        torch.cuda.empty_cache()\n        model = build_model(DEVICE)\n        checkpoint = torch.load(model_checkpoint)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        model.to(DEVICE)\n        res.append(predict(model, test_set))\n    res = data_utils.concat_results(res)\nprint(res)\nres.to_csv('submission.csv', index=False)\nlog.info(\"Done. {}\".format(datetime.now().strftime(\"%Y%m%d %H:%M\")))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if PRESUBMIT:\n    import shutil\n    shutil.copyfile('submission.csv', 'val_submission.csv')\n    sample = pd.read_csv(\"../input/abstraction-and-reasoning-challenge/sample_submission.csv\")\n    sample.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}