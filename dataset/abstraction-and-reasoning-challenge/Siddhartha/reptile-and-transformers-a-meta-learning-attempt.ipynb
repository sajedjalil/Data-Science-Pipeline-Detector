{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The challenge as I see it:\nLearn a function $f(x, K) = y$, where $x$ is the input sampled from the ARC dataset, $K$ is the core knowledge prior and $y$ is the output from the ARC dataset. It should be noted that only $x$ and $K$ are the inputs here and nothing else.\nFor each task we already have some training examples $x$ and $y$. We can easily overfit a neural network and get 100% accuracy. But by doing so\nwe are not learning anything and getting nowhere closer to AGI. Even in a DSL approach, the algorithm is not learning anything! In such an approach\nwe have created handcrafted constraints on the type of functions i.e. we have manually engineered K. \n\nA smart algorithm should have a representation of cognitive priors and should automatically decide which specific\nprior is suitable for a particular task. The tasks are independent of each other but they have something in common i.e. the cognitive priors.\nWe can use this common thread among all the tasks to learn a representation of cognitive priors. And whenever we want to do prediction on a new task, we can finetune and select our learnt priors for that task. So how to do this ?\n\n# Meta Learning\nMeta-learning is the process of learning how to learn. A meta-learning algorithm takes in a distribution of tasks, where each task is a learning problem, and it produces a quick learner — a learner that can generalize from a small number of examples. MAML is one of the famous meta-learning approaches\nout there. But it requires us to compute Hessians (which is hard). Another interesting approach is the Reptile Algorithm.\nIt's mathematically similar to first order MAML and performs a very similar update. The performance of reptile is similar to MAML\non the Omniglot and Mini-ImageNet benchmarks. So, i decided to stick with Reptile instead of MAML.\n\n\n\n# About the code\n\nIn this kernel, I am only doing prediction on the test set. You can find the full code for training, validation and testing in [my github repo](https://github.com/sidml/reptile-transformer).\nThe ARC Dataset is divided into training, evaluation and test sets. 100 examples from the evaluation set are part of the test set. I use these 100 tasks\nfor validation. Each task can have 3 to 5 training images and images can vary in size across and within tasks from 2x2 to 30x30.\nThe model weights are going to be shared among the tasks. So, we need a model that is independent of image size. So, i decided to \nmake an extra class (11). This class represents non existent pixels for a task. So, let's say task size is 10, then i pad the image with class 1 such\nthat image size is again 15. This has two advantages (hopefully)\n1. Allows us to have a common size across tasks (important if you want to experiment with cnn)\n2. Introduces the concept of emptiness and varying size to our model. \n   \nI experimented with cnn's but i found their performance to be lacking. In the current code, you can see that i am using transformer model.\nI reshape the image and pass it to the embedding layer first, followed by a positional encoding layer to account for the order of the pixel. I \nuse Pytorch's implementation of TransformerEncoder. The output of TransformerEncoder model is sent to the final Linear layer, which gives\nus logits for each class (we have total 11 classes). Then we use the usual cross entropy for model training.\n\n\nSince we are aiming to learn parameters that optimizes over all the tasks, during the model training we need an outer loop which iterates over the tasks.\nWe train a transformer model for each task using the task specific inputs and outputs. This is our inner loop.\nThen we perform a gradient update to interpolate between current model weights and trained weights from this task.\nI found this picture from MAML paper very useful to get an intuition about ![how training works](https://bair.berkeley.edu/blog/assets/maml/maml.png)\n\n\nI use the Transformer model. You can easily replace the model with any cnn or deep learning model that you desire.\nI just wanted to introduce the idea of meta-learning and how it may be pertinent to the ARC dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport json\nfrom glob import glob\n\nimport os\nimport random\nimport pickle\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport pandas as pd\nimport torch.nn as nn\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 42\nprint(f'setting everything to seed {seed}')\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\ncmap = colors.ListedColormap(\n    ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25', '#FFFFFF'])\nnorm = colors.Normalize(vmin=0, vmax=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class ARCTest(Dataset):\n\n    def __init__(self, root, imgsz=30):\n        \"\"\"\n\n        :param root: root path of mini-imagenet\n        :param mode: train, val or test\n        :param batchsz: batch size of sets, not batch of imgs\n        :param n_way:\n        :param k_query: num of qeruy imgs per class\n        :param resize: resize to\n        :param startidx: start to index label from startidx\n        \"\"\"\n        super(ARCTest, self).__init__()\n        self.out_rows, self.out_cols = imgsz, imgsz\n        task_paths = sorted(glob(f'{root}/test/*.json')) \n        self.support_x_batch, self.support_y_batch, self.query_x_batch, self.task_paths = self.create_batch(task_paths)\n\n    def pad_im(self, task, out_rows, out_cols, cval=10):\n\n        ip = []\n        op = []\n        for mode in ['train']:\n            num_pairs = len(task[mode])\n            input_im = np.zeros((num_pairs, 1, out_rows, out_cols))\n            output_im = np.zeros(\n                (num_pairs, 1, out_rows, out_cols), dtype=np.long)\n            for task_num in range(num_pairs):\n                im = np.array(task[mode][task_num]['input'])\n                nrows, ncols = im.shape\n                if (nrows > out_rows) or (ncols > out_cols):\n                    return 0, 0, 1, 0\n                im = np.pad(im, ((out_rows-nrows, 0), (out_cols-ncols, 0)), mode='constant',\n                            constant_values=(cval, cval))\n\n                input_im[task_num, 0] = im\n                im = np.array(task[mode][task_num]['output'])\n                nrows, ncols = im.shape\n                if (nrows > out_rows) or (ncols > out_cols):\n                    return 0, 0, 1, 0\n                im = np.pad(im, ((out_rows-nrows, 0), (out_cols-ncols, 0)), mode='constant',\n                            constant_values=(cval, cval))\n                output_im[task_num, 0] = im\n            ip.extend(input_im)\n            op.extend(output_im)\n\n            test_ip = []\n            num_pairs = len(task['test'])\n            input_im = np.zeros((num_pairs, 1, out_rows, out_cols))\n            for task_num in range(num_pairs):\n                im = np.array(task['test'][task_num]['input'])\n                nrows, ncols = im.shape\n                if (nrows > out_rows) or (ncols > out_cols):\n                    return 0, 0, 1, 0\n                im = np.pad(im, ((out_rows-nrows, 0), (out_cols-ncols, 0)), mode='constant',\n                            constant_values=(cval, cval))\n\n                input_im[task_num, 0] = im\n            test_ip.extend(input_im)\n\n        return np.vstack(ip), np.vstack(op), 0, np.vstack(test_ip)\n\n    def create_batch(self, task_paths):\n        \"\"\"\n        create batch for meta-learning.\n        ×episode× here means batch, and it means how many sets we want to retain.\n        :param episodes: batch size\n        :return:\n        \"\"\"\n        x_batch, y_batch,query_x_batch  = [], [], []\n        all_task_paths = []\n        for task_file in task_paths:\n            with open(task_file, 'r') as f:\n                task = json.load(f)\n            input_im, output_im, not_valid, query_im = self.pad_im(task, self.out_rows,\n                                                                   self.out_cols)\n            if not_valid:\n                continue\n            x_batch.extend(input_im[None])\n            y_batch.extend(output_im[None])\n            query_x_batch.extend(query_im[None])\n            all_task_paths.append(task_file)\n        return x_batch, y_batch, query_x_batch, all_task_paths\n\n    def __getitem__(self, index):\n        \"\"\"\n        index means index of sets, 0<= index <= batchsz-1\n        :param index:\n        :return:\n        \"\"\"\n        # print('global:', support_y, query_y)\n        # support_y: [setsz]\n        # query_y: [querysz]\n        # unique: [n-way], sorted\n        support_x = torch.tensor(\n            self.support_x_batch[index], dtype=torch.float32)\n        support_y = torch.tensor(\n            self.support_y_batch[index], dtype=torch.long)\n        return support_x[:, None], support_y.reshape(-1), self.task_paths[index]\n\n    def __len__(self):\n        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n        # return self.batchsz\n        return len(self.support_x_batch)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Defining the Transformer Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(\n            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n        self.model_type = 'Transformer'\n        self.src_mask = None\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntoken)\n\n    def _generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float(\n            '-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        if self.src_mask is None or self.src_mask.size(0) != len(src):\n            device = src.device\n            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n            self.src_mask = mask\n\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, self.src_mask)\n\n        output = self.decoder(output)\n        return output\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/inversion/abstraction-and-reasoning-starter-notebook\ndef flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred\n\ndef plot_figure(x_spt, y_spt, x_qry,\n                pred_q, im_num, img_sz=30):\n\n    plt.figure(figsize=(15,15))\n    plt.subplot(2, 2, 1)\n    plt.imshow(x_spt[0].cpu().numpy().reshape(img_sz, img_sz),\n               cmap=cmap, norm=norm)\n    plt.title('train input 1')\n    plt.subplot(2, 2, 2)\n    plt.imshow(y_spt[:img_sz*img_sz].cpu().numpy().reshape(img_sz, img_sz),\n               cmap=cmap, norm=norm)\n    plt.title('ideal output 1')\n    plt.subplot(2, 2, 3)\n    plt.title('test input 1')\n    plt.imshow(x_qry[0].cpu().numpy().reshape(img_sz, img_sz),\n               cmap=cmap, norm=norm)\n    plt.title('model prediction')\n\n    # do visualization only for the first input.\n    pred_q = pred_q[0, :img_sz*img_sz].cpu().numpy().reshape(img_sz, img_sz)\n    frow = np.nonzero(np.count_nonzero(pred_q-10, axis=1))[0][0]\n    fcol = np.nonzero(np.count_nonzero(pred_q-10, axis=0))[0][0]\n    a = np.copy(pred_q[frow:, fcol:])\n    a[a == 10] = 0\n    plt.subplot(2, 2, 4)\n    plt.imshow(a,\n               cmap=cmap, norm=norm)\n    plt.suptitle(f'{im_num}')\n\n    plt.savefig(f'.epoch_30_preds_{im_num}.png')\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntokens = 11  # the size of vocabulary\nemsize = 32  # embedding dimension\nnhid = 64  # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 4  # the number of heads in the multiheadattention models\ndropout = 0.5  # the dropout value\ndevice = 'cuda'\n\n# ntoken, ninp, nhead, nhid, nlayers, dropout=0.5\nmodel = TransformerModel(ntokens, emsize, nhead,\n                         nhid, nlayers, dropout).cpu()\n\nsample_sub = pd.read_csv('/kaggle/input/abstraction-and-reasoning-challenge/sample_submission.csv')\nsample_sub = sample_sub.set_index('output_id')\nsample_sub.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Predictions on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# device = torch.device('cpu')\ninnerstepsize = 1e-2  # stepsize in inner SGD\ninnerepochs = 1000  # number of epochs of each inner SGD\n\n\ndevice = torch.device('cpu')\n\n# batchsz here means total episode number\narc_dataset = ARCTest(\n    root='/kaggle/input/abstraction-and-reasoning-challenge/', imgsz=15)\n\nall_train_acc = []\nimgsz, num_class = 15, 11\nfor step, ((x, y, task_path), q) in enumerate(zip(arc_dataset, arc_dataset.query_x_batch)):\n\n    task_id = task_path.split('/')[-1]\n    state = torch.load('../input/arctransformermodel/epoch_1_step_264_acc_0.688.pth', map_location='cpu')\n    model.load_state_dict(state)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=innerstepsize)\n    x, y = x.to(device), y.to(device)\n    x = x.to(device).reshape(-1, imgsz*imgsz).long()\n\n    train_losses = []\n    train_acc = []\n    model.train()\n    for _ in range(innerepochs):\n        optimizer.zero_grad()\n        outputs = model(x).reshape(-1, num_class)\n        loss = F.cross_entropy(outputs, y)\n        loss.backward()\n        optimizer.step()\n        train_losses.append(loss.item())\n        acc = (outputs.argmax(1) == y).float().mean().item()\n        train_acc.append(acc)\n    print('\\ttraining loss:',\n          np.mean(train_losses), '\\ttraining acc:', np.mean(train_acc))\n\n    all_train_acc.append(np.mean(train_acc))\n    model.eval()\n    with torch.no_grad():\n        q = torch.tensor(\n            q.reshape(-1, imgsz*imgsz)).to(device).long()\n        # print(q.shape)\n        outputs = F.softmax(model(q), dim=1)\n        outputs = outputs.argmax(2).reshape(-1, imgsz, imgsz)\n        plot_figure(x, y, q, outputs, im_num=task_id, img_sz=imgsz)\n\n    for task_num, preds in enumerate(outputs.cpu().numpy()):\n        frow = np.nonzero(np.count_nonzero(preds-10, axis=1))[0][0]\n        fcol = np.nonzero(np.count_nonzero(preds-10, axis=0))[0][0]\n        preds = np.copy(preds[frow:, fcol:])\n        preds[preds == 10] = 0\n        sample_sub.loc[f'{task_id[:-5]}_{task_num}',\n                    'output'] = flattener(preds.astype(int).tolist())\n#     if step > 3: break\nprint('\\nmean train acc:', np.mean(all_train_acc),\n      'stddev train acc:', np.std(all_train_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub.head()\nsample_sub.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}