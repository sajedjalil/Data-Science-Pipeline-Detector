{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AutoEncoder for Abstraction and Reasoning (Keras)\n\n![](https://github.com/seriousran/img_link/blob/master/kg/anr/img_1.PNG?raw=true)\n\nI have tried to solve it with autoencoder with Keras.\nBut it is not that looks useful yet.\n\nPlease give me some advice and vote.\n\n### updates\n- v3?\n    - add cutout augmentation\n- v4\n    - CAE to VAE (and no data augmentation)\n- v5~6\n    - submission error correction\n- v7\n    - Dropout\n- v8\n    - knn"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport gc\nimport random\nimport json\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom pathlib import Path\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\nfrom sklearn.model_selection import train_test_split,KFold\n\nfrom keras.layers import Input, Dense, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, UpSampling1D, UpSampling2D, Lambda, Embedding, Flatten, Add,Concatenate, Dropout, LSTM\nfrom keras.models import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Nadam\nimport keras.backend as K\n\nfrom keras.applications.vgg16 import VGG16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\n\ntraining_tasks = sorted(os.listdir(training_path))\nevaluation_tasks = sorted(os.listdir(evaluation_path))\ntest_tasks = sorted(os.listdir(test_path))\nprint(len(training_tasks), len(evaluation_tasks), len(test_tasks))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_data(task_filename):\n    with open(task_filename, 'r') as f:\n        task = json.load(f)\n    return task\n\nnum2color = [\"black\", \"blue\", \"red\", \"green\", \"yellow\", \"gray\", \"magenta\", \"orange\", \"sky\", \"brown\"]\ncolor2num = {c: n for n, c in enumerate(num2color)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(400):\n    print(get_data(str(test_path / test_tasks[i])))\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(400):\n    print(get_data(str(training_path / training_tasks[i]))['test'])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = []\ny_train = []\nx_test = []\ny_test = []\n\nox_train = []\noy_train = []\nox_test = []\n\nfor i in range(400):\n    for train_data in get_data(str(training_path / training_tasks[i]))['train']:\n        x_train.append(cv2.resize(np.asarray(train_data['input']), dsize=(32, 32), interpolation=cv2.INTER_NEAREST))\n        y_train.append(cv2.resize(np.asarray(train_data['output']), dsize=(32, 32), interpolation=cv2.INTER_NEAREST))\n        ox_train.append(np.asarray(train_data['input']))\n        oy_train.append(np.asarray(train_data['output']))\n        \nfor i in range(100):\n    for test_data in get_data(str(test_path / test_tasks[i]))['test']:\n        x_test.append(cv2.resize(np.asarray(test_data['input']), dsize=(32, 32), interpolation=cv2.INTER_NEAREST))\n        ox_test.append(np.asarray(test_data['input']))\n    for train_data in get_data(str(test_path / test_tasks[i]))['train']:\n        x_train.append(cv2.resize(np.asarray(train_data['input']), dsize=(32, 32), interpolation=cv2.INTER_NEAREST))\n        y_train.append(cv2.resize(np.asarray(train_data['output']), dsize=(32, 32), interpolation=cv2.INTER_NEAREST))\n        ox_train.append(np.asarray(train_data['input']))\n        oy_train.append(np.asarray(train_data['output']))\n        \nx_train = np.asarray(x_train) / 10. \ny_train = np.asarray(y_train) / 10.\nx_test = np.asarray(x_test) / 10.\n\nprint('length of x_train:', len(x_train))\nprint('length of x_test:', len(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ref: https://github.com/yu4u/cutout-random-erasing/blob/master/cifar10_resnet.py\ndef get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n    def eraser(input_img):\n        img_h, img_w, img_c = input_img.shape\n        p_1 = np.random.rand()\n\n        if p_1 > p:\n            return input_img\n\n        while True:\n            s = np.random.uniform(s_l, s_h) * img_h * img_w\n            r = np.random.uniform(r_1, r_2)\n            w = int(np.sqrt(s / r))\n            h = int(np.sqrt(s * r))\n            left = np.random.randint(0, img_w)\n            top = np.random.randint(0, img_h)\n\n            if left + w <= img_w and top + h <= img_h:\n                break\n\n        if pixel_level:\n            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n        else:\n            c = np.random.uniform(v_l, v_h)\n\n        input_img[top:top + h, left:left + w, :] = c\n\n        return input_img\n\n    return eraser","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = ImageDataGenerator(\n    width_shift_range=0.5,\n    height_shift_range=0.5,\n    #horizontal_flip=True,\n    #vertical_flip=True\n    dtype=float,\n    fill_mode='nearest',\n    preprocessing_function = get_random_eraser(p=0.8, s_l=0.0009765625, s_h=0.0009765625, r_1=0.03124, r_2=0.03124, v_l=0, v_h=9, pixel_level=True),\n)\n\nx_train = x_train.reshape(x_train.shape + (1,) )\ndatagen.fit(x_train)\n\ny_train = y_train.reshape(y_train.shape + (1,))\n\nx_test = x_test.reshape(x_test.shape + (1,))\ndatagen.fit(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sampling(args):\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean = 0 and std = 1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_dim = 32 * 32\ninput_shape_vae = (original_dim, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_vae_train = np.reshape(x_train, [-1, original_dim])\nx_vae_test = np.reshape(x_test, [-1, original_dim])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_vae_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"intermediate_dim = 512\nbatch_size = 32\nlatent_dim = 256\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def create_vae_model(batch_size, input_shape):\n\ninputs = Input(shape=input_shape_vae)\nx = Dense(intermediate_dim, activation='relu')(inputs)\nx = Dropout(0.5)(x)\nx = Dense(intermediate_dim, activation='relu')(x)\nx = Dropout(0.3)(x)\nx = Dense(intermediate_dim, activation='relu')(x)\n\n#x = LSTM(512)(x)\n\nz_mean = Dense(latent_dim)(x)\nz_log_var = Dense(latent_dim)(x)\n\nz = Lambda(sampling, output_shape=(latent_dim, ))([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()\n\n# build decoder model\nlatent_inputs = Input(shape=(latent_dim, ), name='z_sampling')\nx = Dense(intermediate_dim, activation='relu')(latent_inputs)\n#x = LSTM(512, return_sequences=True)(x)\nx = Dropout(0.3)(x)\nx = Dense(intermediate_dim, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(intermediate_dim, activation='relu')(x)\noutputs = Dense(original_dim, activation='sigmoid')(x)\n\n# instantiate decoder model\ndecoder = Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()\n\n# instantiate VAE model\noutputs = decoder(encoder(inputs)[2])\nvae = Model(inputs, outputs, name='vae_mlp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.losses import mse, binary_crossentropy\n\n#reconstruction_loss = mse(inputs, outputs)\nreconstruction_loss = binary_crossentropy(inputs, outputs)\n\nreconstruction_loss *= original_dim\nkl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\nkl_loss = K.sum(kl_loss, axis=-1)\nkl_loss *= -0.3\nvae_loss = K.mean(reconstruction_loss + kl_loss)\nvae.add_loss(vae_loss)\n\nnadam = Nadam(learning_rate=0.003, beta_1=0.999, beta_2=0.999999)\nvae.compile(optimizer=nadam)\n\nvae.fit(x_vae_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded_train = vae.predict(x_vae_train)\n\ndecoded_train = np.reshape(decoded_train, [-1, 32, 32, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_max = np.amax(decoded_train)\n_min = np.amin(decoded_train)\n\n_range = _max - _min\n_step = _range / 10\n\n#decoded_train = (decoded_train - _min) / _range\n\ndecoded_train = (decoded_train * 18)\ndecoded_train = decoded_train.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rd_train = []\nfor i in range(len(decoded_train)):\n    w = ox_train[i].shape[0]\n    h = ox_train[i].shape[1]\n    if (decoded_train[i].shape[0] != h) | (decoded_train[i].shape[1] != w) :\n        rd_train.append( cv2.resize(decoded_train[i], dsize=(h, w), interpolation=cv2.INTER_NEAREST) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1,n+1):\n    # 입력 출력\n    ax = plt.subplot(3, n, i)\n    plt.imshow(ox_train[i])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # 정답 출력\n    ax = plt.subplot(3, n, i + n)\n    plt.imshow(oy_train[i])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # 생성 출력\n    ax = plt.subplot(3, n, i + 2 * n)\n    plt.imshow(rd_train[i])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoded_test = vae.predict(x_vae_test)\n\ndecoded_test = np.reshape(decoded_test, [-1, 32, 32, 1])\n\n_max = np.amax(decoded_test)\n_min = np.amin(decoded_test)\n\n_range = _max - _min\n_step = _range / 10\n\n#decoded_test = (decoded_test - _min) / _range\n\ndecoded_test = (decoded_test * 19)\ndecoded_test = decoded_test.astype(int)\n\nprint( np.amax(decoded_test) )\nprint( np.amin(decoded_test) )\n\nrd_test = []\nfor i in range(len(decoded_test)):\n    w = ox_test[i].shape[0]\n    h = ox_test[i].shape[1]\n    if (decoded_test[i].shape[0] != h) | (decoded_test[i].shape[1] != w) :\n        rd_test.append( cv2.resize(decoded_test[i], dsize=(h, w), interpolation=cv2.INTER_NEAREST) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 10\nplt.figure(figsize=(20, 4))\nfor i in range(1,n+1):\n    # 입력 출력\n    ax = plt.subplot(2, n, i)\n    plt.imshow(ox_test[i])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    # 생성 출력\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(rd_test[i])\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv(\"../input/abstraction-and-reasoning-challenge/sample_submission.csv\")\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission['output'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'|' + str(rd_test[0]).replace('[','').replace(']','').replace('\\n','|').replace(' ','') + '|'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, row in df_submission.iterrows():\n    cand_0 = ''\n    cand_1 = ''\n    cand_2 = '|' + str(rd_test[i]).replace('[','').replace(']','').replace('\\n','|').replace(' ','') + '|'\n    answer = ''\n    for j, cand in enumerate(row[1].split(' ')):\n        #print(j)\n        #print('-', cand)\n        #print('=', cand_2)\n        if j == 0: #cand_0\n            cand_0 = cand\n            answer += cand_0\n            #print(answer)\n        elif j == 1: #cand_1\n            nums = []\n            for c in cand_0.replace('|', ''):\n                nums.append(int(c))\n            for k, c in enumerate(cand_0):\n                #print(k, c, cand_0)\n                if c == '|':\n                    cand_1 += '|'\n                elif int(c) == np.amax(nums):\n                    if np.amax(rd_test[i]) == cand_2[k]:\n                        cand_1 += c\n                    else:\n                        cand_1 += cand_2[k]\n                else:\n                    cand_1 += c\n            answer += ' ' + cand_1\n            #print('+2', answer)\n        elif j == 2: #cand_2\n            answer += ' ' + cand_2\n            #print(answer)\n    #print(answer)\n    #print()\n    df_submission.at[i,'output'] = answer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To Do\n\n1. fine-tune?\n    - Test file has also train part. I think we should use it with just one epoch or more.\n\n2. Data Augmentation\n    - shift\n\n2. post-process\n    - quantization way\n        - resize first or rescale first\n        - following exist value\n    - how to have the answer for top-3?"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}