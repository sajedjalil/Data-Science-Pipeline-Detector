{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Checking Grid Sizes in the Abstraction and Reasoning Challenge\n\nIn the interactive version of the ARC, the grid size can be changed.  This indicates that there may not be a one-to-one correspondence between grid sizes of input and output pairs.  This notebook looks into that possibility and we find that, indeed, only 63% of training examples have such a one-to-one correspondence.  However, of those that did not have one-to-one correspondence, 55% had identical grid sizes across the training outputs.\n\nThe consequence of this is that the ARC problem space can be broken into three parts of (most likely) increasing difficulty:\n1. Tasks where the grid size of the inputs can just be copied to the grid size for the outputs (63% of the training tasks).\n2. Tasks where the grid sizes of the inputs do not match the grid sizes of the outputs, but where all the outputs have the same grid size (20% of the training data).\n3. Tasks where the grid sizes of the inputs do not match the grid sizes of the outputs, and the input grid somehow informs the grid size of the output (17% of the training data)\n\nThis notebook does not create a submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport re # Regular expressions\nimport os # To walk through the data files provided\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Specify the different directories of data files\ntestDirectory = \"/kaggle/input/abstraction-and-reasoning-challenge/test/\"\ntrainingDirectory = \"/kaggle/input/abstraction-and-reasoning-challenge/training/\"\nevaluationDirectory = \"/kaggle/input/abstraction-and-reasoning-challenge/training/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to read a task file and return the parsed data\ndef readTaskFile(filename):\n    #print(\"Reading file: \"+filename)\n    \n    # Open the file\n    f = open(filename, \"r\")\n    \n    # Parse the JSON\n    data = json.loads(f.read())\n    \n    # Add in an 'id' that's extracted from the filename\n    data[\"id\"] = re.sub(\"(.*/)|(\\.json)\", \"\", filename)\n    \n    # Close the file\n    f.close()\n    \n    # Return the parsed data\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick test to see that our readTaskFile function is working.\n# Note the addition of the 'id' field.\nfilename = testDirectory+\"19bb5feb.json\"\nreadTaskFile(filename)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Function to compare the grid sizes of the input and output fields.\n# Returns True if the sizes are the same, and False otherwise.\ndef getGridSizeComparison(filename):\n    data = readTaskFile(filename)\n    #print(data)\n    trainSection = data[\"train\"]\n    ident = data[\"id\"]\n    \n    numTrain = len(trainSection)\n    result = {}\n    for i in range(numTrain):\n        trainCase = trainSection[i]\n        trainCaseInput = trainCase[\"input\"]\n        trainCaseOutput = trainCase[\"output\"]\n        sameY = len(trainCaseInput) == len(trainCaseOutput)\n        sameX = len(trainCaseInput[0]) == len(trainCaseOutput[0])\n        result[ident + \"_train_\" + str(i)] = sameX and sameY\n        \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Quick test with the first file \"19bb5feb.json\"\n# \"3b4c2228.json\" has two tests\nfilename = testDirectory+\"19bb5feb.json\"\ngetGridSizeComparison(filename)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to loop through the questions in the given directory\n# applying a function 'f' to each question.\ndef getResults(directory, f):\n    results = {}\n    for _, _, filenames in os.walk(directory):\n        for filename in filenames:\n            results.update(f(directory+filename))\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Execute our comparison function on the training directory\nresults = getResults(trainingDirectory, getGridSizeComparison)\n\nprint(str(results)[1:1000]+\"[...]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What proportion of training examples have the same input:output grid sizes?\ncount = 0\nfor _, value in results.items():\n    if value: count+=1\n\nprint(\"Proportion of training examples with the same grid size: \"+str(round(count/len(results), 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualise the training cases for a task\n# Code inspiration from https://www.kaggle.com/inversion/abstraction-and-reasoning-starter-notebook\ndef plotTaskTraining(task):\n    \"\"\"\n    Plots the training pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    \n    # Plot all the training cases\n    nTrainingCases = len(task[\"train\"])\n    scale = 3\n    fig, axs = plt.subplots(nTrainingCases, 2, figsize=(2*scale,nTrainingCases*scale))\n    for i in range(nTrainingCases):\n        axs[i][0].imshow(task['train'][i]['input'], cmap=cmap, norm=norm)\n        axs[i][0].axis('off')\n        axs[i][0].set_title('Train Input')\n        axs[i][1].imshow(task['train'][i]['output'], cmap=cmap, norm=norm)\n        axs[i][1].axis('off')\n        axs[i][1].set_title('Train Output')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task ID 19bb5feb gives an example of a task where the input:output grid sizes are not the same\nfilename = testDirectory+\"19bb5feb.json\"\n\ntask = readTaskFile(filename)\nplotTaskTraining(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above example shows that the grid sizes for all the **outputs** of the training cases of the task were identical.  Perhaps a significant proportion of tasks that have differing input:output grid sizes have fixed output grid sizes that do not change across the training cases?  The following code considers that possibility."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to compare the grid sizes of the input and output fields.\n# Returns a dict with the following fields:\n#   allCorrespond: True iff the input and output grid sizes are the same for all training cases\n#   outputsSame: True iff the output gird sizes are the same for all training cases \ndef getGridSizeComparison2(filename):\n    data = readTaskFile(filename)\n    #print(data)\n    trainSection = data[\"train\"]\n    ident = data[\"id\"]\n    \n    numTrain = len(trainSection)\n    result = {\"allCorrespond\": True,\n              \"outputsSame\": True}\n    \n    # Check for allCorrespond\n    for i in range(numTrain):\n        trainCase = trainSection[i]\n        trainCaseInput = trainCase[\"input\"]\n        trainCaseOutput = trainCase[\"output\"]\n        sameY = len(trainCaseInput) == len(trainCaseOutput)\n        sameX = len(trainCaseInput[0]) == len(trainCaseOutput[0])\n        if not (sameX and sameY):\n            result[\"allCorrespond\"] = False\n            break\n\n    # Check for outputsSame\n    outputX = None\n    outputY = None\n    for i in range(numTrain):\n        trainCase = trainSection[i]\n        trainCaseOutput = trainCase[\"output\"]\n        same = True\n        if outputY == None:\n            outputY = len(trainCaseOutput)\n        else:\n            if not outputY == len(trainCaseOutput):\n                same = False\n            \n        if outputX == None:\n            outputX = len(trainCaseOutput[0])\n        else:\n            if not outputX == len(trainCaseOutput[0]):\n                same = False\n\n        if not same:\n            result[\"outputsSame\"] = False\n            break\n        \n    return {ident: result}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task ID 19bb5feb gives an example of a task where the \n# input:output grid sizes are not the same, but all the \n# outputs are the same size\n\nfilename = testDirectory+\"19bb5feb.json\"\nprint(getGridSizeComparison2(filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task ID 0b148d64 gives an example of a task where both the \n# input:output grid sizes are not the same, and all the \n# outputs are not the same size\nfilename = trainingDirectory+\"0b148d64.json\"\nprint(getGridSizeComparison2(filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Task ID 0b148d64 gives an example of a task where the input:output grid sizes are not the same\nfilename = trainingDirectory+\"0b148d64.json\"\n\ntask = readTaskFile(filename)\nplotTaskTraining(task)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Execute our comparison function on the training directory\nresults = getResults(trainingDirectory, getGridSizeComparison2)\n\nprint(str(results)[1:1000]+\"[...]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Of the training examples where the input:output grid sizes\n# are not all the same, what proportion have all the same size\n# training grids?\ncountAllCorrespondFalse = 0\nfor _, value in results.items():\n    if not value[\"allCorrespond\"]: countAllCorrespondFalse+=1\n        \ncountAllCorrespondFalseOutputsSameTrue = 0\nfor _, value in results.items():\n    if (value[\"allCorrespond\"]==False and \n        value[\"outputsSame\"]==True): countAllCorrespondFalseOutputsSameTrue+=1\n\nprint(\"Of the \"+str(countAllCorrespondFalse)+\" tasks where the input:output \"+\n      \"grid sizes were not the same,\\n\"+\n      str(countAllCorrespondFalseOutputsSameTrue)+\" had identical grid sizes \"+\n      \"for all their outputs, or \"+\n      str(round(countAllCorrespondFalseOutputsSameTrue/countAllCorrespondFalse*100))+\n      \"%.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}