{"cells":[{"metadata":{},"cell_type":"markdown","source":"In my [previous notebook](https://www.kaggle.com/teddykoker/training-cellular-automata-part-i-game-of-life) we explored how we could use a CNN to create a cellular automata (CA) by recurrently passing the state of the grid through itself. Now we'll solve one of the tasks [arseny-n](https://www.kaggle.com/arsenynerinovsky/cellular-automata-as-a-language-for-reasoning) solved with a hard coded CA by learning the CA instead!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge')\ntrain_path = data_path / 'training'\nvalid_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\n\ntrain_tasks = { task.stem: json.load(task.open()) for task in train_path.iterdir() } \nvalid_tasks = { task.stem: json.load(task.open()) for task in valid_path.iterdir() } ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n    \ndef plot_pictures(pictures, labels):\n    fig, axs = plt.subplots(1, len(pictures), figsize=(2*len(pictures),32))\n    for i, (pict, label) in enumerate(zip(pictures, labels)):\n        axs[i].imshow(np.array(pict), cmap=cmap, norm=norm)\n        axs[i].set_title(label)\n    plt.show()\n    \ndef plot_sample(sample, predict=None):\n    if predict is None:\n        plot_pictures([sample['input'], sample['output']], ['Input', 'Output'])\n    else:\n        plot_pictures([sample['input'], sample['output'], predict], ['Input', 'Output', 'Predict'])\n        \ndef inp2img(inp):\n    inp = np.array(inp)\n    img = np.full((10, inp.shape[0], inp.shape[1]), 0, dtype=np.uint8)\n    for i in range(10):\n        img[i] = (inp==i)\n    return img\n\ndef input_output_shape_is_same(task):\n    return all([np.array(el['input']).shape == np.array(el['output']).shape for el in task['train']])\n\n\ndef calk_score(task_test, predict):\n    return [int(np.equal(sample['output'], pred).all()) for sample, pred in zip(task_test, predict)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First Task: db3e9e38\n\nThe task we'll first try is relitively straight foward; given a central orange \"pillar\", form stairs of alternating blue and orange in each direction. `arseny-n` showed that this could be solved with a CA consisting of three rules."},{"metadata":{"trusted":true},"cell_type":"code","source":"task = train_tasks[\"db3e9e38\"][\"train\"]\nfor sample in task:\n    plot_sample(sample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Model\n\nThe model consists of a single 3x3 convolutional layer, followed by a 1x1 convolutional layer, just like my last notebook. Here `num_states` represents how many values a single cell could have; in this case 10, one for each color. Down the road, we may want to add a hidden state, concatinating it to the input, then removing it from the output.\n\nThe foward pass of the model will repeatedly pass the grid state through the CA transition for `steps` number of times."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CAModel(nn.Module):\n    def __init__(self, num_states):\n        super(CAModel, self).__init__()\n        self.transition = nn.Sequential(\n            nn.Conv2d(num_states, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, num_states, kernel_size=1)\n        )\n        \n    def forward(self, x, steps=1):\n        for _ in range(steps):\n            x = self.transition(torch.softmax(x, dim=1))\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n\nThis \"recurrent CNN\" can be quite to difficult to train. After trying a few ideas, this seemed to be the best approach that I encountered:\n\n* For every value $n$ = $1, ..., N$:\n    1. Train the model with $n$ `steps` to produce the output from input\n    2. Train the model with 1 `steps` to produce output from output\n        * This enforces that the CA stabilizes after reaching a solution\n        \nIn this way the model will try to get as close to a solution as possible in 1 step, then try to get closer in the next step, and so on until $N$ steps. For now I will use $N = 10$ = `max_steps`. I will also set the learning rate to decay with each additional step: $LR = 0.1 / (n * 2) $"},{"metadata":{"trusted":true},"cell_type":"code","source":"def solve_task(task, max_steps=10):\n    model = CAModel(10).to(device)\n    num_epochs = 100\n    criterion = nn.CrossEntropyLoss()\n    losses = np.zeros((max_steps - 1) * num_epochs)\n\n    for num_steps in range(1, max_steps):\n        optimizer = torch.optim.Adam(model.parameters(), lr=(0.1 / (num_steps * 2)))\n        \n        for e in range(num_epochs):\n            optimizer.zero_grad()\n            loss = 0.0\n\n            for sample in task:\n                # predict output from input\n                x = torch.from_numpy(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device)\n                y = torch.tensor(sample[\"output\"]).long().unsqueeze(0).to(device)\n                y_pred = model(x, num_steps)\n                loss += criterion(y_pred, y)\n                \n                # predit output from output\n                # enforces stability after solution is reached\n                y_in = torch.from_numpy(inp2img(sample[\"output\"])).unsqueeze(0).float().to(device)\n                y_pred = model(y_in, 1) \n                loss += criterion(y_pred, y)\n\n            loss.backward()\n            optimizer.step()\n            losses[(num_steps - 1) * num_epochs + e] = loss.item()\n    return model, num_steps, losses\n                \n@torch.no_grad()\ndef predict(model, task):\n    predictions = []\n    for sample in task:\n        x = torch.from_numpy(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device)\n        pred = model(x, 100).argmax(1).squeeze().cpu().numpy()\n        predictions.append(pred)\n    return predictions\n    \ntask = train_tasks[\"db3e9e38\"][\"train\"]\nmodel, num_steps, losses = solve_task(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$n$ is incremented every 100 epochs, so we can see that it reaches a good solution after 3 steps (epoch 300)."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(losses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets see if it at least correctly outputs the training set. To be save we'll give the model $n=100$ steps:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predict(model, task)\nfor i in range(len(task)):\n    plot_sample(task[i], predictions[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It works! Now lets see if it generalized to the test question:"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = train_tasks[\"db3e9e38\"][\"test\"]\npredictions = predict(model, test)\nfor i in range(len(test)):\n    plot_sample(test[i], predictions[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fantastic! The coolest part now is that we can animate our solution to see the CA in action:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def animate_solution(model, sample):\n    x = torch.from_numpy(inp2img(sample[\"input\"])).unsqueeze(0).float().to(device)\n\n    @torch.no_grad()\n    def animate(i):\n        pred = model(x, i)\n        im.set_data(pred.argmax(1).squeeze().cpu().numpy())\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(x.argmax(1).squeeze().cpu().numpy(), cmap=cmap, norm=norm)\n    return animation.FuncAnimation(fig, animate, frames=100, interval=120)\n    \nanim = animate_solution(model, train_tasks[\"db3e9e38\"][\"test\"][0])\nHTML(anim.to_jshtml())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see that the CA quickly gets to a solution and then stabilizes."},{"metadata":{},"cell_type":"markdown","source":"## More Tasks\n\nNow that we know we can train a CA for one task, will it work on others?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(tasks):\n    result = []\n    predictions = []\n    for idx, task in tqdm(tasks.items()):\n        if input_output_shape_is_same(task):\n            model, _, _ = solve_task(task[\"train\"])\n            pred = predict(model, task[\"test\"])\n            score = calk_score(task[\"test\"], pred)\n        else:\n            pred = [el[\"input\"] for el in task[\"test\"]]\n            score = [0] * len(task[\"test\"])\n\n        predictions.append(pred)\n        result.append(score)\n    return result, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_result, train_predictions = evaluate(train_tasks)\ntrain_solved = [any(score) for score in train_result]\n\ntotal = sum([len(score) for score in train_result])\nprint(f\"solved : {sum(train_solved)} from {total} ({sum(train_solved)/total})\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We solve many of the tasks within the training set using our Neural Cellular Automata model! I did test on the validation set as well, and it correctly solved 17 of the tasks. There are a number of ways this model could be improved. Please let me know if you'd be interested in collaboration!\n\n## Solved Tasks"},{"metadata":{"trusted":true},"cell_type":"code","source":"for task, prediction, solved in tqdm(zip(train_tasks.values(), train_predictions, train_solved)):\n    if solved:\n        for i in range(len(task['train'])):\n            plot_sample(task['train'][i])\n            \n        for i in range(len(task['test'])):\n            plot_sample(task['test'][i], prediction[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"That's all for now, thanks for reading!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}