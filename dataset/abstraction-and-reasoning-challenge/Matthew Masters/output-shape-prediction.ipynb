{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is based on the findings by @capiru [here](https://www.kaggle.com/c/abstraction-and-reasoning-challenge/discussion/134030). He points out you can predict the output shape with high accuracy by simply answering these two questions:\n - Are all training outputs the same size?\n - Are outputs the same size as input?\n \nHe found that by answering these questions you could predict 84.5% of the output shapes. Here I present the code for answering these two questions as well as some additions that increase accuracy to 350/400 (87.5%)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport os\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\n\ntraining_tasks = sorted(glob(str(training_path / '*')))\nevaluation_tasks = sorted(os.listdir(evaluation_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def getData(task_filename):\n    with open(task_filename, 'r') as f:\n        task = json.load(f)\n    return task","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def plotOne(ax,task,i,train_or_test,input_or_output):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    \n    input_matrix = task[train_or_test][i][input_or_output]\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n    ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n    ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(train_or_test + ' '+input_or_output)\n    \n\ndef plotTask(task):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"    \n    num_train = len(task['train'])\n    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n    for i in range(num_train):     \n        plotOne(axs[0,i],task,i,'train','input')\n        plotOne(axs[1,i],task,i,'train','output')        \n    plt.tight_layout()\n    plt.show()        \n        \n    num_test = len(task['test'])\n    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n    if num_test==1: \n        plotOne(axs[0],task,0,'test','input')\n        plotOne(axs[1],task,0,'test','output')     \n    else:\n        for i in range(num_test):      \n            plotOne(axs[0,i],task,i,'test','input')\n            plotOne(axs[1,i],task,i,'test','output')  \n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct = 0\ntotal = len(training_tasks)\nunsolved = []\n\nfor task in training_tasks:\n    data = getData(task)\n    \n    x = [np.array(x['input']) for x in data['train']]\n    y = [np.array(x['output']) for x in data['train']]\n    x_shapes = [x.shape for x in x]\n    y_shapes = [x.shape for x in y]\n    io_ratio = [(j[0] / i[0], j[1] / i[1]) for i,j in zip(x_shapes,y_shapes)]\n    io_diff =  [(j[0] - i[0], j[1] - i[1]) for i,j in zip(x_shapes,y_shapes)]\n    \n    x_test = [np.array(x['input']) for x in data['test']]\n    test_x_shapes = [x.shape for x in x_test]\n    output_shapes = [(3,3)] * len(test_x_shapes)\n    \n    if len(list(set(io_ratio))) == 1: # Output shapes have the same input/output ratio\n        io_ratio = io_ratio[0]\n        output_shapes = [(shape[0] * io_ratio[0], shape[1] * io_ratio[1]) for shape in test_x_shapes]\n    elif len(list(set(io_diff))) == 1: # Output shapes have the same input/output difference\n        io_diff = io_diff[0]\n        output_shapes = [(shape[0] + io_diff[0], shape[1] + io_diff[1]) for shape in test_x_shapes]\n    elif len(list(set(y_shapes))) == 1: # Outputs have the same shape\n        output_shapes = [y_shapes[0]] * len(test_x_shapes)\n    \n    # Check if output_shapes prediction is correct for all test examples\n    y_test_shapes = [np.array(x['output']).shape for x in data['test']]\n    solved = sum([1 for idx,test_shapes in enumerate(y_test_shapes) if test_shapes == output_shapes[idx]]) == len(y_test_shapes)\n    if not solved: unsolved.append(task)\n    correct += solved\n\nprint('%d/%d (%.1f' % (correct,total,100*correct/total) + r' %)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's take a look at the unsolved cases. You will see that predicting the output shape in these examples will require some kind of higher logic."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for task in unsolved:\n    data = getData(task)\n    plotTask(data)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}