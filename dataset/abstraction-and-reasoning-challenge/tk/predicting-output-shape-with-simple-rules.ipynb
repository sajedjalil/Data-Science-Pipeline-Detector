{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook shows that you can predict the shape of test output by approx. 80% acc with simple rules.\n\nThe rules are:\n\n1. If train outputs have the same shape as their input, test output has the same shape as its input.\n2. If train outputs have constant shape, test output has the same shape as train outputs.\n3. If train outputs have constant factor to their input, test output shape will be `(test_input_shape[0] * shape_fector[0], test_input_shape[1] * shape_fector[1]`"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from pathlib import Path\n\ndata_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\n\ntraining_tasks = sorted(os.listdir(training_path))\nevaluation_tasks = sorted(os.listdir(evaluation_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# kudo for: https://www.kaggle.com/boliu0/visualizing-all-task-pairs-with-gridline\n\ndef plot_one(task, ax, i,train_or_test,input_or_output):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    \n    input_matrix = task[train_or_test][i][input_or_output]\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n    ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n    ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(train_or_test + ' '+input_or_output)\n    \n\ndef plot_task(task):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"    \n    num_train = len(task['train'])\n    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n    for i in range(num_train):     \n        plot_one(task, axs[0,i],i,'train','input')\n        plot_one(task, axs[1,i],i,'train','output')        \n    plt.tight_layout()\n    plt.show()        \n        \n    num_test = len(task['test'])\n    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n    if num_test==1: \n        plot_one(task, axs[0],0,'test','input')\n        plot_one(task, axs[1],0,'test','output')     \n    else:\n        for i in range(num_test):      \n            plot_one(task, axs[0,i],i,'test','input')\n            plot_one(task, axs[1,i],i,'test','output')  \n    plt.tight_layout()\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_constant_factor(input_lengths, output_lengths):\n    factors = [o / i for i, o in zip(input_lengths, output_lengths)]\n    \n    factor_consistent = len(list(set(factors))) == 1\n    if factor_consistent:\n        return factors[0]\n    else:\n        return None\n\ndef predict_shape(train_inputs, train_outputs, test_input):\n    input_shapes = [x.shape for x in train_inputs] \n    output_shapes = [x.shape for x in train_outputs]\n    \n    is_same_shape_x = all([shape_i[0] == shape_o[0] for shape_i, shape_o in zip(input_shapes, output_shapes)])\n    is_same_shape_y = all([shape_i[1] == shape_o[1] for shape_i, shape_o in zip(input_shapes, output_shapes)])\n    if is_same_shape_x and is_same_shape_y:\n        return test_input.shape, \"same shape\"\n    \n    is_output_shape_consistent = len(list(set(output_shapes))) == 1\n    if is_output_shape_consistent:\n        return output_shapes[0], \"output shape consistent\"\n\n    x_factor = find_constant_factor([e[0] for e in input_shapes], [e[0] for e in output_shapes])\n    y_factor = find_constant_factor([e[1] for e in input_shapes], [e[1] for e in output_shapes])\n    if x_factor is not None and y_factor is not None:\n        description = \"(x_out, y_out) =  (x_in * {}, y_in * {})\".format(x_factor, y_factor)\n        return (test_input.shape[0] * x_factor, test_input.shape[1] * y_factor), description\n    \n    return None, \"no suitable rule\"\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction for training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_correct = 0\ntasks_mistaken_train = []\ntasks_unknown_train = []\n\nfor i, filename in enumerate(training_tasks):\n    task_file = str(training_path / filename)\n    \n    with open(task_file, 'r') as f:\n        task = json.load(f)\n        \n    inputs = [np.array(t[\"input\"]) for t in task[\"train\"]]\n    outputs = [np.array(t[\"output\"]) for t in task[\"train\"]]\n    test_input = np.array(task[\"test\"][0][\"input\"])\n    test_output_shape = np.array(task[\"test\"][0][\"output\"]).shape\n    \n    test_output_shape_predicted, description = predict_shape(inputs, outputs, test_input)\n    \n    if test_output_shape == test_output_shape_predicted:\n        n_correct += 1\n    else:\n#         print(\"task: \", filename)\n#         print(description)\n        if test_output_shape_predicted is not None:\n            tasks_mistaken_train.append((filename, description, test_output_shape, test_output_shape_predicted))\n#             print(\"**wrong: expected {} but got {}\".format(test_output_shape, test_output_shape_predicted))\n        else:\n            tasks_unknown_train.append(filename)\nprint(\"{} out of {} correct\".format(n_correct, len(training_tasks)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"86% accuracy! Well done for these simple rules.\n\nLet's look into mistaken samples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (filename, description, _, _) in enumerate(tasks_mistaken_train):\n    task_file = str(training_path / filename)\n    print(description)\n    \n    with open(task_file, 'r') as f:\n        task = json.load(f)\n    plot_task(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This task has the constant shape in the train output. With simple rules we can't predict this task's output.\n\nLet's Look into some of the tasks we couldn't predict with our simple rules:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, filename in enumerate(tasks_unknown_train[:3]):\n    task_file = str(training_path / filename)\n    \n    with open(task_file, 'r') as f:\n        task = json.load(f)\n    plot_task(task)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems hard to predict output shapes for them with simple rules."},{"metadata":{},"cell_type":"markdown","source":"## Prediction for evaluation set"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_correct_eval = 0\ntasks_mistaken_eval = []\ntasks_unknown_eval = []\n\nfor i, filename in enumerate(evaluation_tasks):\n    task_file = str(evaluation_path / filename)\n    \n    with open(task_file, 'r') as f:\n        task = json.load(f)\n        \n    inputs = [np.array(t[\"input\"]) for t in task[\"train\"]]\n    outputs = [np.array(t[\"output\"]) for t in task[\"train\"]]\n    test_input = np.array(task[\"test\"][0][\"input\"])\n    test_output_shape = np.array(task[\"test\"][0][\"output\"]).shape\n    \n    test_output_shape_predicted, description = predict_shape(inputs, outputs, test_input)\n    \n    if test_output_shape == test_output_shape_predicted:\n        n_correct_eval += 1\n    else:\n#         print(\"task: \", filename)\n#         print(description)\n        if test_output_shape_predicted is not None:\n            tasks_mistaken_eval.append((filename, description, test_output_shape, test_output_shape_predicted))\n#             print(\"**wrong: expected {} but got {}\".format(test_output_shape, test_output_shape_predicted))\n        else:\n            tasks_unknown_eval.append(filename)\nprint(\"{} out of {} correct\".format(n_correct_eval, len(evaluation_tasks)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"87.5% accuracy for eval sets. Well Done!\n\nIf you like this notebook please upvote, thanks."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}