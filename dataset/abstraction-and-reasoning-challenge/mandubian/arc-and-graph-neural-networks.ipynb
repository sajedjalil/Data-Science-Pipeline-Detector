{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Playing with Graph Neural Networks on ARC\n(Not winning :D)\n\nFor months of lockdown, I wanted to play with ARC and graph neural networks but couldn't find the time until now 3 days before the end of the competition.\nAnyway, I'm not here to win the competition, I'm here to experiment and have fun.\n\nThe original notebook can be found there:\nhttps://github.com/mandubian/ARC/blob/master/graph_geometric.ipynb\n\nYou can contact me on twitter https://twitter.com/@mandubian\n\n> I'll play more with these concepts in the incoming weeks IMHO.\n\nThis notebook provides in early draft versions:\n- Utils to convert ARC input/output task to Pytorch Geometric batches of Graphs.\n- Utils to display ARC graphs and tasks.\n- Pytorch dataset/dataloader to manage ARC Tasks.\n- Sample of GraphNN based GCN to train with custom ARC Graph Dataset.\n\n> I haven't yet submitted any result because results are very bad as I expected on such very-low data intensive tasks. But the important is not the result, it's the ideas and the experimentations.\n\nSo for now, I do something very stupid:\n- In a ARC task, there are several input/output samples.\n- Take one input and convert it into a graph (1 pixel = 1 node and each pixel is connected to its neighbors).\n- Take one output and convert it into a graph.\n- Take a pair of input/output in a ARC task and merge them into a big graph interconnecting each pixel of input to its sibling in output.\n- Take all input/output graphs of a task and merged into a big graph representing the task (every sample graph is a disjoint graph in it).\n- In one task's graph, hide one output of a sample and then ask the Graph Neural Network to learn to rebuild the full graph.\n- Then do it again\n- etc...\n\n\nThis idea is very basic but in 2days, I had not much time to do better. I have other ideas based on encoding/embedding/decoding strategies. I'll test them later.\n\nThis is just a draft to open reflections. Code is barely working and not at all optimized in anyway.\n1. I love those ideas even if they don't work for now...\n\nYou can see my broken trainings on WanDB for demo: https://app.wandb.ai/mandubian/mandubian-arc-graph?workspace=user-mandubian\n\nPytorch Geometric is a great library that you can find there https://github.com/rusty1s/pytorch_geometric","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Install Torch geometric\n\n**Very long process** as it recompiles sparse/scatter and CPU in instance are not very fast.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install torch_geometric\n!pip install torch_sparse\n!pip install torch_scatter\n!pip install pytorch_lightning\n!pip install wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom pathlib import Path\nfrom torch_geometric.data import Data, Batch\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_geometric.utils import to_networkx\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nfrom typing import Tuple, Dict, List\n\nimport networkx as nx\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading ARC paths","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_path = Path('/kaggle/input/abstraction-and-reasoning-challenge/')\ntraining_path = data_path / 'training'\nevaluation_path = data_path / 'evaluation'\ntest_path = data_path / 'test'\nprint(\"training_path\", training_path)\nprint(\"evaluation_path\", evaluation_path)\nprint(\"test_path\", test_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert ARC sample to Geometric Graph","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Modified Geometric code for Pytorch 1.5\n\n`.to(non_blocking)` doesn't with code in Pytorch Geometric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# COPIED from https://github.com/rusty1s/pytorch_geometric\n# and very lightly modified to use `non_blocking` param for Tensor.to(...)\n# Maybe I'll do a PR with it\nimport re\nimport copy\nimport warnings\n\nimport torch\nimport torch_geometric\nfrom torch_sparse import coalesce, SparseTensor\nfrom torch_geometric.utils import (contains_isolated_nodes,\n                                   contains_self_loops, is_undirected)\n\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\n\n__num_nodes_warn_msg__ = (\n    'The number of nodes in your data object can only be inferred by its {} '\n    'indices, and hence may result in unexpected batch-wise behavior, e.g., '\n    'in case there exists isolated nodes. Please consider explicitly setting '\n    'the number of nodes for this data object by assigning it to '\n    'data.num_nodes.')\n\n\ndef size_repr(key, item, indent=0):\n    indent_str = ' ' * indent\n    if torch.is_tensor(item):\n        out = str(list(item.size()))\n    elif isinstance(item, SparseTensor):\n        out = str(item.sizes())[:-1] + f', nnz={item.nnz()}]'\n    elif isinstance(item, list) or isinstance(item, tuple):\n        out = str([len(item)])\n    elif isinstance(item, dict):\n        lines = [indent_str + size_repr(k, v, 2) for k, v in item.items()]\n        out = '{\\n' + ',\\n'.join(lines) + '\\n' + indent_str + '}'\n    else:\n        out = str(item)\n\n    return f'{indent_str}{key}={out}'\n\n\nclass Data(object):\n    r\"\"\"A plain old python object modeling a single graph with various\n    (optional) attributes:\n    Args:\n        x (Tensor, optional): Node feature matrix with shape :obj:`[num_nodes,\n            num_node_features]`. (default: :obj:`None`)\n        edge_index (LongTensor, optional): Graph connectivity in COO format\n            with shape :obj:`[2, num_edges]`. (default: :obj:`None`)\n        edge_attr (Tensor, optional): Edge feature matrix with shape\n            :obj:`[num_edges, num_edge_features]`. (default: :obj:`None`)\n        y (Tensor, optional): Graph or node targets with arbitrary shape.\n            (default: :obj:`None`)\n        pos (Tensor, optional): Node position matrix with shape\n            :obj:`[num_nodes, num_dimensions]`. (default: :obj:`None`)\n        norm (Tensor, optional): Normal vector matrix with shape\n            :obj:`[num_nodes, num_dimensions]`. (default: :obj:`None`)\n        face (LongTensor, optional): Face adjacency matrix with shape\n            :obj:`[3, num_faces]`. (default: :obj:`None`)\n    The data object is not restricted to these attributes and can be extented\n    by any other additional data.\n    Example::\n        data = Data(x=x, edge_index=edge_index)\n        data.train_idx = torch.tensor([...], dtype=torch.long)\n        data.test_mask = torch.tensor([...], dtype=torch.bool)\n    \"\"\"\n    def __init__(self, x=None, edge_index=None, edge_attr=None, y=None,\n                 pos=None, norm=None, face=None, **kwargs):\n        self.x = x\n        self.edge_index = edge_index\n        self.edge_attr = edge_attr\n        self.y = y\n        self.pos = pos\n        self.norm = norm\n        self.face = face\n        for key, item in kwargs.items():\n            if key == 'num_nodes':\n                self.__num_nodes__ = item\n            else:\n                self[key] = item\n\n        if edge_index is not None and edge_index.dtype != torch.long:\n            raise ValueError(\n                (f'Argument `edge_index` needs to be of type `torch.long` but '\n                 f'found type `{edge_index.dtype}`.'))\n\n        if face is not None and face.dtype != torch.long:\n            raise ValueError(\n                (f'Argument `face` needs to be of type `torch.long` but found '\n                 f'type `{face.dtype}`.'))\n\n        if torch_geometric.is_debug_enabled():\n            self.debug()\n\n    @classmethod\n    def from_dict(cls, dictionary):\n        r\"\"\"Creates a data object from a python dictionary.\"\"\"\n        data = cls()\n\n        for key, item in dictionary.items():\n            data[key] = item\n\n        if torch_geometric.is_debug_enabled():\n            data.debug()\n\n        return data\n\n    def __getitem__(self, key):\n        r\"\"\"Gets the data of the attribute :obj:`key`.\"\"\"\n        return getattr(self, key, None)\n\n    def __setitem__(self, key, value):\n        \"\"\"Sets the attribute :obj:`key` to :obj:`value`.\"\"\"\n        setattr(self, key, value)\n\n    @property\n    def keys(self):\n        r\"\"\"Returns all names of graph attributes.\"\"\"\n        keys = [key for key in self.__dict__.keys() if self[key] is not None]\n        keys = [key for key in keys if key[:2] != '__' and key[-2:] != '__']\n        return keys\n\n    def __len__(self):\n        r\"\"\"Returns the number of all present attributes.\"\"\"\n        return len(self.keys)\n\n    def __contains__(self, key):\n        r\"\"\"Returns :obj:`True`, if the attribute :obj:`key` is present in the\n        data.\"\"\"\n        return key in self.keys\n\n    def __iter__(self):\n        r\"\"\"Iterates over all present attributes in the data, yielding their\n        attribute names and content.\"\"\"\n        for key in sorted(self.keys):\n            yield key, self[key]\n\n    def __call__(self, *keys):\n        r\"\"\"Iterates over all attributes :obj:`*keys` in the data, yielding\n        their attribute names and content.\n        If :obj:`*keys` is not given this method will iterative over all\n        present attributes.\"\"\"\n        for key in sorted(self.keys) if not keys else keys:\n            if key in self:\n                yield key, self[key]\n\n    def __cat_dim__(self, key, value):\n        r\"\"\"Returns the dimension for which :obj:`value` of attribute\n        :obj:`key` will get concatenated when creating batches.\n        .. note::\n            This method is for internal use only, and should only be overridden\n            if the batch concatenation process is corrupted for a specific data\n            attribute.\n        \"\"\"\n        # `*index*` and `*face*` should be concatenated in the last dimension,\n        # everything else in the first dimension.\n        return -1 if bool(re.search('(index|face)', key)) else 0\n\n    def __inc__(self, key, value):\n        r\"\"\"\"Returns the incremental count to cumulatively increase the value\n        of the next attribute of :obj:`key` when creating batches.\n        .. note::\n            This method is for internal use only, and should only be overridden\n            if the batch concatenation process is corrupted for a specific data\n            attribute.\n        \"\"\"\n        # Only `*index*` and `*face*` should be cumulatively summed up when\n        # creating batches.\n        return self.num_nodes if bool(re.search('(index|face)', key)) else 0\n\n    @property\n    def num_nodes(self):\n        r\"\"\"Returns or sets the number of nodes in the graph.\n        .. note::\n            The number of nodes in your data object is typically automatically\n            inferred, *e.g.*, when node features :obj:`x` are present.\n            In some cases however, a graph may only be given by its edge\n            indices :obj:`edge_index`.\n            PyTorch Geometric then *guesses* the number of nodes\n            according to :obj:`edge_index.max().item() + 1`, but in case there\n            exists isolated nodes, this number has not to be correct and can\n            therefore result in unexpected batch-wise behavior.\n            Thus, we recommend to set the number of nodes in your data object\n            explicitly via :obj:`data.num_nodes = ...`.\n            You will be given a warning that requests you to do so.\n        \"\"\"\n        if hasattr(self, '__num_nodes__'):\n            return self.__num_nodes__\n        for key, item in self('x', 'pos', 'norm', 'batch'):\n            return item.size(self.__cat_dim__(key, item))\n        if self.face is not None:\n            warnings.warn(__num_nodes_warn_msg__.format('face'))\n            return maybe_num_nodes(self.face)\n        if self.edge_index is not None:\n            warnings.warn(__num_nodes_warn_msg__.format('edge'))\n            return maybe_num_nodes(self.edge_index)\n        return None\n\n    @num_nodes.setter\n    def num_nodes(self, num_nodes):\n        self.__num_nodes__ = num_nodes\n\n    @property\n    def num_edges(self):\n        r\"\"\"Returns the number of edges in the graph.\"\"\"\n        for key, item in self('edge_index', 'edge_attr'):\n            return item.size(self.__cat_dim__(key, item))\n        return None\n\n    @property\n    def num_faces(self):\n        r\"\"\"Returns the number of faces in the mesh.\"\"\"\n        if self.face is not None:\n            return self.face.size(self.__cat_dim__('face', self.face))\n        return None\n\n    @property\n    def num_node_features(self):\n        r\"\"\"Returns the number of features per node in the graph.\"\"\"\n        if self.x is None:\n            return 0\n        return 1 if self.x.dim() == 1 else self.x.size(1)\n\n    @property\n    def num_features(self):\n        r\"\"\"Alias for :py:attr:`~num_node_features`.\"\"\"\n        return self.num_node_features\n\n    @property\n    def num_edge_features(self):\n        r\"\"\"Returns the number of features per edge in the graph.\"\"\"\n        if self.edge_attr is None:\n            return 0\n        return 1 if self.edge_attr.dim() == 1 else self.edge_attr.size(1)\n\n    def is_coalesced(self):\n        r\"\"\"Returns :obj:`True`, if edge indices are ordered and do not contain\n        duplicate entries.\"\"\"\n        edge_index, _ = coalesce(self.edge_index, None, self.num_nodes,\n                                 self.num_nodes)\n        return self.edge_index.numel() == edge_index.numel() and (\n            self.edge_index != edge_index).sum().item() == 0\n\n    def coalesce(self):\n        r\"\"\"\"Orders and removes duplicated entries from edge indices.\"\"\"\n        self.edge_index, self.edge_attr = coalesce(self.edge_index,\n                                                   self.edge_attr,\n                                                   self.num_nodes,\n                                                   self.num_nodes)\n        return self\n\n    def contains_isolated_nodes(self):\n        r\"\"\"Returns :obj:`True`, if the graph contains isolated nodes.\"\"\"\n        return contains_isolated_nodes(self.edge_index, self.num_nodes)\n\n    def contains_self_loops(self):\n        \"\"\"Returns :obj:`True`, if the graph contains self-loops.\"\"\"\n        return contains_self_loops(self.edge_index)\n\n    def is_undirected(self):\n        r\"\"\"Returns :obj:`True`, if graph edges are undirected.\"\"\"\n        return is_undirected(self.edge_index, self.edge_attr, self.num_nodes)\n\n    def is_directed(self):\n        r\"\"\"Returns :obj:`True`, if graph edges are directed.\"\"\"\n        return not self.is_undirected()\n\n    def __apply__(self, item, func):\n        if torch.is_tensor(item) or isinstance(item, SparseTensor):\n            return func(item)\n        elif isinstance(item, (tuple, list)):\n            return [self.__apply__(v, func) for v in item]\n        elif isinstance(item, dict):\n            return {k: self.__apply__(v, func) for k, v in item.items()}\n        else:\n            return item\n\n    def apply(self, func, *keys):\n        r\"\"\"Applies the function :obj:`func` to all tensor attributes\n        :obj:`*keys`. If :obj:`*keys` is not given, :obj:`func` is applied to\n        all present attributes.\n        \"\"\"\n        for key, item in self(*keys):\n            self[key] = self.__apply__(item, func)\n        return self\n\n    def contiguous(self, *keys):\n        r\"\"\"Ensures a contiguous memory layout for all attributes :obj:`*keys`.\n        If :obj:`*keys` is not given, all present attributes are ensured to\n        have a contiguous memory layout.\"\"\"\n        return self.apply(lambda x: x.contiguous(), *keys)\n\n    #def to(self, device, *keys, **kwargs):\n    #    r\"\"\"Performs tensor dtype and/or device conversion to all attributes\n    #    :obj:`*keys`.\n    #    If :obj:`*keys` is not given, the conversion is applied to all present\n    #    attributes.\"\"\"\n    #    return self.apply(lambda x: x.to(device, **kwargs), *keys)\n\n    def to(self, device, non_blocking, *keys, **kwargs):\n        r\"\"\"Performs tensor dtype and/or device conversion to all attributes\n        :obj:`*keys`.\n        If :obj:`*keys` is not given, the conversion is applied to all present\n        attributes.\"\"\"\n        return self.apply(lambda x: x.to(device=device, non_blocking=non_blocking, **kwargs), *keys)\n\n    def clone(self):\n        return self.__class__.from_dict({\n            k: v.clone() if torch.is_tensor(v) else copy.deepcopy(v)\n            for k, v in self.__dict__.items()\n        })\n\n    def debug(self):\n        if self.edge_index is not None:\n            if self.edge_index.dtype != torch.long:\n                raise RuntimeError(\n                    ('Expected edge indices of dtype {}, but found dtype '\n                     ' {}').format(torch.long, self.edge_index.dtype))\n\n        if self.face is not None:\n            if self.face.dtype != torch.long:\n                raise RuntimeError(\n                    ('Expected face indices of dtype {}, but found dtype '\n                     ' {}').format(torch.long, self.face.dtype))\n\n        if self.edge_index is not None:\n            if self.edge_index.dim() != 2 or self.edge_index.size(0) != 2:\n                raise RuntimeError(\n                    ('Edge indices should have shape [2, num_edges] but found'\n                     ' shape {}').format(self.edge_index.size()))\n\n        if self.edge_index is not None and self.num_nodes is not None:\n            if self.edge_index.numel() > 0:\n                min_index = self.edge_index.min()\n                max_index = self.edge_index.max()\n            else:\n                min_index = max_index = 0\n            if min_index < 0 or max_index > self.num_nodes - 1:\n                raise RuntimeError(\n                    ('Edge indices must lay in the interval [0, {}]'\n                     ' but found them in the interval [{}, {}]').format(\n                         self.num_nodes - 1, min_index, max_index))\n\n        if self.face is not None:\n            if self.face.dim() != 2 or self.face.size(0) != 3:\n                raise RuntimeError(\n                    ('Face indices should have shape [3, num_faces] but found'\n                     ' shape {}').format(self.face.size()))\n\n        if self.face is not None and self.num_nodes is not None:\n            if self.face.numel() > 0:\n                min_index = self.face.min()\n                max_index = self.face.max()\n            else:\n                min_index = max_index = 0\n            if min_index < 0 or max_index > self.num_nodes - 1:\n                raise RuntimeError(\n                    ('Face indices must lay in the interval [0, {}]'\n                     ' but found them in the interval [{}, {}]').format(\n                         self.num_nodes - 1, min_index, max_index))\n\n        if self.edge_index is not None and self.edge_attr is not None:\n            if self.edge_index.size(1) != self.edge_attr.size(0):\n                raise RuntimeError(\n                    ('Edge indices and edge attributes hold a differing '\n                     'number of edges, found {} and {}').format(\n                         self.edge_index.size(), self.edge_attr.size()))\n\n        if self.x is not None and self.num_nodes is not None:\n            if self.x.size(0) != self.num_nodes:\n                raise RuntimeError(\n                    ('Node features should hold {} elements in the first '\n                     'dimension but found {}').format(self.num_nodes,\n                                                      self.x.size(0)))\n\n        if self.pos is not None and self.num_nodes is not None:\n            if self.pos.size(0) != self.num_nodes:\n                raise RuntimeError(\n                    ('Node positions should hold {} elements in the first '\n                     'dimension but found {}').format(self.num_nodes,\n                                                      self.pos.size(0)))\n\n        if self.norm is not None and self.num_nodes is not None:\n            if self.norm.size(0) != self.num_nodes:\n                raise RuntimeError(\n                    ('Node normals should hold {} elements in the first '\n                     'dimension but found {}').format(self.num_nodes,\n                                                      self.norm.size(0)))\n\n    def __repr__(self):\n        cls = str(self.__class__.__name__)\n        has_dict = any([isinstance(item, dict) for _, item in self])\n\n        if not has_dict:\n            info = [size_repr(key, item) for key, item in self]\n            return '{}({})'.format(cls, ', '.join(info))\n        else:\n            info = [size_repr(key, item, indent=2) for key, item in self]\n            return '{}(\\n{}\\n)'.format(cls, ',\\n'.join(info))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# COPIED from https://github.com/rusty1s/pytorch_geometric\n# Maybe I'll do a PR with it\nimport torch\nimport torch_geometric\n#from torch_geometric.data import Data\n\n\nclass Batch(Data):\n    r\"\"\"A plain old python object modeling a batch of graphs as one big\n    (dicconnected) graph. With :class:`torch_geometric.data.Data` being the\n    base class, all its methods can also be used here.\n    In addition, single graphs can be reconstructed via the assignment vector\n    :obj:`batch`, which maps each node to its respective graph identifier.\n    \"\"\"\n    def __init__(self, batch=None, **kwargs):\n        super(Batch, self).__init__(**kwargs)\n\n        self.batch = batch\n        self.__data_class__ = Data\n        self.__slices__ = None\n\n    @staticmethod\n    def from_data_list(data_list, follow_batch=[]):\n        r\"\"\"Constructs a batch object from a python list holding\n        :class:`torch_geometric.data.Data` objects.\n        The assignment vector :obj:`batch` is created on the fly.\n        Additionally, creates assignment batch vectors for each key in\n        :obj:`follow_batch`.\"\"\"\n\n        keys = [set(data.keys) for data in data_list]\n        keys = list(set.union(*keys))\n        assert 'batch' not in keys\n\n        batch = Batch()\n        batch.__data_class__ = data_list[0].__class__\n        batch.__slices__ = {key: [0] for key in keys}\n\n        for key in keys:\n            batch[key] = []\n\n        for key in follow_batch:\n            batch['{}_batch'.format(key)] = []\n\n        cumsum = {key: 0 for key in keys}\n        batch.batch = []\n        for i, data in enumerate(data_list):\n            for key in data.keys:\n                item = data[key]\n                if torch.is_tensor(item) and item.dtype != torch.bool:\n                    item = item + cumsum[key]\n                if torch.is_tensor(item):\n                    size = item.size(data.__cat_dim__(key, data[key]))\n                else:\n                    size = 1\n                batch.__slices__[key].append(size + batch.__slices__[key][-1])\n                cumsum[key] = cumsum[key] + data.__inc__(key, item)\n                batch[key].append(item)\n\n                if key in follow_batch:\n                    item = torch.full((size, ), i, dtype=torch.long)\n                    batch['{}_batch'.format(key)].append(item)\n\n            num_nodes = data.num_nodes\n            if num_nodes is not None:\n                item = torch.full((num_nodes, ), i, dtype=torch.long)\n                batch.batch.append(item)\n\n        if num_nodes is None:\n            batch.batch = None\n\n        for key in batch.keys:\n            item = batch[key][0]\n            if torch.is_tensor(item):\n                batch[key] = torch.cat(batch[key],\n                                       dim=data_list[0].__cat_dim__(key, item))\n            elif isinstance(item, int) or isinstance(item, float):\n                batch[key] = torch.tensor(batch[key])\n\n        # Copy custom data functions to batch (does not work yet):\n        # if data_list.__class__ != Data:\n        #     org_funcs = set(Data.__dict__.keys())\n        #     funcs = set(data_list[0].__class__.__dict__.keys())\n        #     batch.__custom_funcs__ = funcs.difference(org_funcs)\n        #     for func in funcs.difference(org_funcs):\n        #         setattr(batch, func, getattr(data_list[0], func))\n\n        if torch_geometric.is_debug_enabled():\n            batch.debug()\n\n        return batch.contiguous()\n\n    def to_data_list(self):\n        r\"\"\"Reconstructs the list of :class:`torch_geometric.data.Data` objects\n        from the batch object.\n        The batch object must have been created via :meth:`from_data_list` in\n        order to be able reconstruct the initial objects.\"\"\"\n\n        if self.__slices__ is None:\n            raise RuntimeError(\n                ('Cannot reconstruct data list from batch because the batch '\n                 'object was not created using Batch.from_data_list()'))\n\n        keys = [key for key in self.keys if key[-5:] != 'batch']\n        cumsum = {key: 0 for key in keys}\n        data_list = []\n        for i in range(len(self.__slices__[keys[0]]) - 1):\n            data = self.__data_class__()\n            for key in keys:\n                if torch.is_tensor(self[key]):\n                    data[key] = self[key].narrow(\n                        data.__cat_dim__(key,\n                                         self[key]), self.__slices__[key][i],\n                        self.__slices__[key][i + 1] - self.__slices__[key][i])\n                    if self[key].dtype != torch.bool:\n                        data[key] = data[key] - cumsum[key]\n                else:\n                    data[key] = self[key][self.__slices__[key][i]:self.\n                                          __slices__[key][i + 1]]\n                cumsum[key] = cumsum[key] + data.__inc__(key, data[key])\n            data_list.append(data)\n\n        return data_list\n\n    @property\n    def num_graphs(self):\n        \"\"\"Returns the number of graphs in the batch.\"\"\"\n        return self.batch[-1].item() + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ARC to Graph utilities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To refactor all of this code because my aim was just to be quick there\n# Remark that I code Python with types but here types are not everywhere by lack of time ;)\n\ndef sample_to_graph(sample):\n    \"\"\"\n    ARC sample to Geometric Graph.\n    \n    Every pixel becomes a node and it has edges to its neighbor pixels.\n    \"\"\"\n    height = len(sample)\n    width = len(sample[0])\n    data_x = torch.tensor(sample, dtype=torch.float)\n    data_x = data_x.flatten().unsqueeze(dim=1).contiguous()\n    edge_index=[]\n    pos = []\n    # terrible code now...\n    for y in range(height-1, -1, -1):\n        for x in range(width):\n            local_edges = []\n            pos.append([x, y])\n            x0 = x - 1\n            y0 = y - 1\n            x1 = x + 1\n            y1 = y + 1\n            pt = x + width * y\n            if x0 >= 0:\n                local_edges.append([x0 + width * y, pt])\n                if y0 >= 0:\n                    local_edges.append([x0 + width * y0, pt])\n                if y1 <= height - 1:\n                    local_edges.append([x0 + width * y1, pt])\n                                                       \n            if x1 <= width - 1:\n                local_edges.append([x1 + width * y, pt])\n                if y0 >= 0:\n                    local_edges.append([x1 + width * y0, pt])\n                if y1 <= height - 1:\n                    local_edges.append([x1 + width * y1, pt])\n\n            if y0 >= 0:\n                local_edges.append([x + width * y0, pt])\n            if y1 <= height - 1:\n                local_edges.append([x + width * y1, pt])\n            \n            edge_index.extend(local_edges)\n                    \n    edge_index = torch.tensor(edge_index, dtype=torch.long) #\n    pos = torch.tensor(pos, dtype=torch.long)\n    data = Data(x=data_x, edge_index=edge_index.t().contiguous(), pos=pos)\n    return data\n\ndef sample_to_graph_padded(sample, target, pad_value = 10) -> Data:\n    \"\"\"\n    ARC sample to Geometric Graph padding sample to fit size of target if it is bigger.\n    \n    Fake nodes are linked to other nodes exactly as pixels and their value is\n    pad_value (which is 10 to be out of ARC classes).\n    \"\"\"\n    target_height = len(target)\n    target_width = len(target[0])\n    \n    height = len(sample)\n    width = len(sample[0])\n    \n    # same size\n    if target_height == height and target_width == width:\n        return sample_to_graph(sample)\n    # rest is padding\n    else:\n        data_x = torch.tensor(sample, dtype=torch.float)\n\n        diff_height = max(target_height, height) - height\n        if diff_height % 2 == 0:\n            pad_top = pad_bottom = int(diff_height / 2)\n        else:\n            pad_top = int(diff_height / 2)\n            pad_bottom = pad_top + 1\n            \n        diff_width = max(target_width, width) - width\n        if diff_width % 2 == 0:\n            pad_left = pad_right = int(diff_width / 2)\n        else:\n            pad_left =int(diff_width / 2)\n            pad_right = pad_left + 1\n\n        \n        data_x = F.pad(data_x, pad=(pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=pad_value)\n        data_x = data_x.flatten().unsqueeze(dim=1).contiguous()\n        edge_index=[]\n        pos = []\n        final_height = max(target_height, height)\n        final_width = max(target_width, width)\n        for y in range(final_height-1, -1, -1):\n            for x in range(final_width):\n                local_edges = []\n                pos.append([x, y])\n                x0 = x - 1\n                y0 = y - 1\n                x1 = x + 1\n                y1 = y + 1\n                pt = x + final_width * y\n                if x0 >= 0:\n                    local_edges.append([x0 + final_width * y, pt])\n                    if y0 >= 0:\n                        local_edges.append([x0 + final_width * y0, pt])\n                    if y1 <= final_height - 1:\n                        local_edges.append([x0 + final_width * y1, pt])\n\n                if x1 <= final_width - 1:\n                    local_edges.append([x1 + final_width * y, pt])\n                    if y0 >= 0:\n                        local_edges.append([x1 + final_width * y0, pt])\n                    if y1 <= final_height - 1:\n                        local_edges.append([x1 + final_width * y1, pt])\n\n                if y0 >= 0:\n                    local_edges.append([x + final_width * y0, pt])\n                if y1 <= final_height - 1:\n                    local_edges.append([x + final_width * y1, pt])\n\n                edge_index.extend(local_edges)\n\n        edge_index = torch.tensor(edge_index, dtype=torch.long) #\n        pos = torch.tensor(pos, dtype=torch.long)\n        data = Data(x=data_x, edge_index=edge_index.t().contiguous(), pos=pos)\n        return data\n\ndef build_image_edges(final_height: int, final_width: int,\n                      start_height: int, start_width: int,\n                      end_height: int, end_width: int):\n    \"\"\"\n    Utilities to build all edges for an image to graph in which pixel are nodes\n    and pixel is linked to all neighbors.\n    \"\"\"\n    edge_index=[]\n    #final_height = start_height + final_height\n    #final_width = start_width + final_width\n    for y in range(end_height-1, start_height-1, -1):\n        for x in range(start_width, end_width):\n            local_edges = []\n            x0 = x - 1\n            y0 = y - 1\n            x1 = x + 1\n            y1 = y + 1\n            pt = x + final_width * y\n            if x0 >= start_width:\n                local_edges.append([x0 + final_width * y, pt])\n                if y0 >= start_height:\n                    local_edges.append([x0 + final_width * y0, pt])\n                if y1 <= end_height - 1:\n                    local_edges.append([x0 + final_width * y1, pt])\n\n            if x1 <= end_width - 1:\n                local_edges.append([x1 + final_width * y, pt])\n                if y0 >= start_height:\n                    local_edges.append([x1 + final_width * y0, pt])\n                if y1 <= end_height - 1:\n                    local_edges.append([x1 + final_width * y1, pt])\n\n            if y0 >= start_height:\n                local_edges.append([x + final_width * y0, pt])\n            if y1 <= end_height - 1:\n                local_edges.append([x + final_width * y1, pt])\n\n            edge_index.extend(local_edges)\n    edge_index = torch.tensor(edge_index, dtype=torch.long) #\n    \n    return edge_index\n    \ndef build_image_pos(final_height: int, final_width: int):\n    \"\"\"Building basic image positions for graph drawing.\"\"\"\n    pos = []\n    for y in range(final_height-1, -1, -1):\n        for x in range(final_width):\n            pos.append([x, y])\n            \n    pos = torch.tensor(pos, dtype=torch.long)\n    \n    return pos\n    \ndef build_edges_image_to_image(final_height: int, final_width: int):\n    \"\"\"\n    In the context of input/output graphs merged horizontally, connect both graphs\n    by drawing edges from every node of graph to every node of other graph.\n    \"\"\"\n    edge_index=[]\n    for y in range(final_height-1, -1, -1):\n        for x in range(final_width):\n            edge_index.append([x + 2*final_width * y, x + 2*final_width * y + final_width])\n            edge_index.append([x + 2*final_width * y + final_width, x + 2*final_width * y])\n    edge_index = torch.tensor(edge_index, dtype=torch.long)\n    return edge_index\n    \ndef sample_to_graph_merged(sample, target, empty_target: bool=False, pad_value = 10) -> Data:\n    \"\"\"Concat sample & target padded graphs horizontally and draw edges between every node.\"\"\"\n    target_height = len(target)\n    target_width = len(target[0])\n    \n    height = len(sample)\n    width = len(sample[0])\n    \n    # Pad Sample\n    data_x = torch.tensor(sample, dtype=torch.float)\n    diff_height = max(target_height, height) - height\n    if diff_height % 2 == 0:\n        pad_top = pad_bottom = int(diff_height / 2)\n    else:\n        pad_top = int(diff_height / 2)\n        pad_bottom = pad_top + 1\n\n    diff_width = max(target_width, width) - width\n    if diff_width % 2 == 0:\n        pad_left = pad_right = int(diff_width / 2)\n    else:\n        pad_left =int(diff_width / 2)\n        pad_right = pad_left + 1\n\n    data_x = F.pad(data_x, pad=(pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=pad_value)\n    \n    # Pad Target\n    data_y = torch.tensor(target, dtype=torch.float)\n    diff_target_height = max(target_height, height) - target_height\n    if diff_target_height % 2 == 0:\n        pad_target_top = pad_target_bottom = int(diff_target_height / 2)\n    else:\n        pad_target_top = int(diff_target_height / 2)\n        pad_target_bottom = pad_target_top + 1\n\n    diff_target_width = max(target_width, width) - target_width\n    if diff_target_width % 2 == 0:\n        pad_target_left = pad_target_right = int(diff_target_width / 2)\n    else:\n        pad_target_left =int(diff_target_width / 2)\n        pad_target_right = pad_target_left + 1\n    data_y = F.pad(data_y, pad=(pad_target_left, pad_target_right, pad_target_top, pad_target_bottom), mode='constant', value=pad_value)\n    if empty_target:\n        data_y = data_y.fill_(pad_value)\n\n    # horizontally cat\n    data_full = torch.cat((data_x, data_y), dim=1)\n    data_full = data_full.flatten().unsqueeze(dim=1).contiguous()\n\n    final_height = max(target_height, height)\n    final_width = max(target_width, width) # twice as we concat horizontally\n\n    edges_index_x = build_image_edges(final_height, 2*final_width, 0, 0, final_height, final_width)\n    edges_index_y = build_image_edges(final_height, 2*final_width, 0, final_width, final_height, 2*final_width)\n    pos = build_image_pos(final_height, 2*final_width)\n    edges_image_to_image = build_edges_image_to_image(final_height, final_width)\n    #edges_index = torch.cat((edges_index_x, edges_index_y, edges_image_to_image), dim=0)\n    edges_index = torch.cat((edges_index_x, edges_index_y, edges_image_to_image), dim=0)\n    #pos = torch.cat((pos_x, pos_y), dim=0)\n\n    data = Data(x=data_full, edge_index=edges_index.t().contiguous(), pos=pos)\n    return data\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Code copied from master version because the one in v1.4.3 is buggy\ndef to_networkx(data, node_attrs=None, edge_attrs=None, to_undirected=False,\n                remove_self_loops=False):\n    r\"\"\"Converts a :class:`torch_geometric.data.Data` instance to a\n    :obj:`networkx.DiGraph` if :attr:`to_undirected` is set to :obj:`True`, or\n    an undirected :obj:`networkx.Graph` otherwise.\n    Args:\n        data (torch_geometric.data.Data): The data object.\n        node_attrs (iterable of str, optional): The node attributes to be\n            copied. (default: :obj:`None`)\n        edge_attrs (iterable of str, optional): The edge attributes to be\n            copied. (default: :obj:`None`)\n        to_undirected (bool, optional): If set to :obj:`True`, will return a\n            a :obj:`networkx.Graph` instead of a :obj:`networkx.DiGraph`. The\n            undirected graph will correspond to the upper triangle of the\n            corresponding adjacency matrix. (default: :obj:`False`)\n        remove_self_loops (bool, optional): If set to :obj:`True`, will not\n            include self loops in the resulting graph. (default: :obj:`False`)\n    \"\"\"\n\n    if to_undirected:\n        G = nx.Graph()\n    else:\n        G = nx.DiGraph()\n\n    G.add_nodes_from(range(data.num_nodes))\n\n    values = {}\n    for key, item in data:\n        if torch.is_tensor(item):\n            values[key] = item.squeeze().tolist()\n        else:\n            values[key] = item\n        if isinstance(values[key], (list, tuple)) and len(values[key]) == 1:\n            values[key] = item[0]\n\n    for i, (u, v) in enumerate(data.edge_index.t().tolist()):\n        if to_undirected and v > u:\n            continue\n\n        if remove_self_loops and u == v:\n            continue\n\n        G.add_edge(u, v)\n        for key in edge_attrs if edge_attrs is not None else []:\n            G[u][v][key] = values[key][i]\n    for key in node_attrs if node_attrs is not None else []:\n        for i, feat_dict in G.nodes(data=True):\n            feat_dict.update({key: values[key][i]})\n\n    return G\n\nDEFAULT_CMAP = [\n    '#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n    '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25',\n    #_1 color\n    '#FFFFFF'\n]\n\ndef draw_graph(data: Data, title: str, ax=None, cmap=DEFAULT_CMAP):\n    \"\"\"Draw simple graph\"\"\"\n    G = to_networkx(data, to_undirected=True, node_attrs=[\"x\", \"pos\"])\n    # Kamada-Kawaii layout usually looks pretty for arbitrary graphs\n    #pos = nx.kamada_kawai_layout(nx_G)\n    node_labels = nx.get_node_attributes(G, 'x')\n    node_pos = nx.get_node_attributes(G, 'pos')\n    pos = nx.spring_layout(G, pos=node_pos) #, fixed = fixed_nodes)\n    node_labels = { i: int(x) for (i, x) in node_labels.items()}\n    nx.draw(G, pos, ax=ax)\n    node_pos = { i:i for i, _ in node_pos.items() }\n    nx.draw_networkx_nodes(G, pos, node_color=[cmap[i] for i in node_labels.values()], ax=ax)\n    nx.draw_networkx_labels(G, pos, labels=node_pos, font_color=\"w\", ax=ax)\n    if ax is not None:\n        ax.set_title(title)\n    else:\n        plt.title(title)\n\n\n    \ndef draw_batch(batch: Batch, title: str, figsize=(15, 5), cmap=DEFAULT_CMAP):\n    \"\"\"Draw batch of graphs\"\"\"\n    data_list = batch.to_data_list()\n    fig, axs = plt.subplots(len(data_list), 2, figsize=figsize)\n    plt.box(False)\n    for idx, data in enumerate(data_list):\n        draw_graph(data, f\"{title}_{idx}_input\", axs[idx, 0], cmap)\n        draw_graph(data.y[0], f\"{title}_{idx}_output\", axs[idx, 1], cmap)\n\n    #plt.savefig('this.png')\n    plt.show()\n\n    \ndef draw_batch_images(batch_images, title: str, use_pad: bool = True, merging: bool = False, figsize=(15, 5), cmap=DEFAULT_CMAP):\n    \"\"\"Draw batch from ARC dataset.\"\"\"\n    norm = colors.Normalize(vmin=0, vmax=9)\n\n    if not merging:\n        batch, batch_out, batch_pad, batch_out_pad, images = batch_images\n\n        if use_pad:\n            data_list = batch_pad.to_data_list()\n            data_out_list = batch_out_pad.to_data_list()\n        else:\n            data_list = batch.to_data_list()\n            data_out_list = batch_out.to_data_list()\n    else:\n        batch, batch_out, batch_pad, batch_out_pad, batch_merged, batch_merged_out, images = batch_images\n        data_list = batch_pad.to_data_list()\n        data_out_list = batch_out_pad.to_data_list()\n        \n    lg = len(data_list)+ len(images)\n    figsize = (figsize[0], figsize[1] * lg)\n    fig, axs = plt.subplots(lg, 2, figsize=figsize)\n    plt.box(False)\n    for idx, (data, data_out, imgs) in enumerate(zip(data_list, data_out_list, images)):\n        axs[2*idx, 0].imshow(imgs['input'], cmap=colors.ListedColormap(cmap), norm=norm)\n        axs[2*idx, 0].set_title(f\"{title}_{idx}_input\")\n        #axs[idx, 0].set_title('Train Input')\n        draw_graph(data, f\"{title}_{idx}_input\", axs[2*idx+1, 0], cmap)\n        axs[2*idx, 1].imshow(imgs['output'], cmap=colors.ListedColormap(cmap), norm=norm)\n        axs[2*idx, 1].set_title(f\"{title}_{idx}_output\")\n        draw_graph(data_out, f\"{title}_{idx}_output\", axs[2*idx+1, 1], cmap)\n\n    #draw_graph(batch_pad, f\"{title}_batch_input\", axs[lg-1, 1], cmap)\n    #draw_graph(batch_out_pad, f\"{title}_batch_output\", axs[lg, 1], cmap)\n\n    #plt.savefig('this.png')\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"task_file = str(training_path / '007bbfb7.json')\n\nwith open(task_file, 'r') as f:\n    task = json.load(f)\n\nprint(task.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_in = sample_to_graph(task['train'][0][\"input\"])\ndata_out = sample_to_graph(task['train'][0][\"output\"])\nprint(\"data_in\", data_in)\nprint(\"data_out\", data_out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drawing Basic input/output Graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15, 5))\nplt.box(False)\ndraw_graph(data_in, \"input\", axs[0])\ndraw_graph(data_out, \"output\", axs[1])\n\n#plt.savefig('this.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drawing Padded Graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_in_padded = sample_to_graph_padded(task['train'][0][\"input\"], task['train'][0][\"output\"])\ndata_out_padded = sample_to_graph(task['train'][0][\"output\"])\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 5))\nplt.box(False)\ndraw_graph(data_in_padded, \"input\", axs[0])\ndraw_graph(data_out_padded, \"output\", axs[1])\n\n#plt.savefig('this.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drawing Merged input/output Graph\n\nIt's not easy to see it but both graph are horizontally placed and then each pixel of the first is linked to each pixel of the second.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_merged = sample_to_graph_merged(task['train'][0][\"input\"], task['train'][0][\"output\"])\n\n#fig, axs = plt.subplots(1, 2, figsize=(15, 5))\nplt.figure(figsize=(20,10)) \nplt.box(False)\ndraw_graph(data_merged, \"merged\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Arc Graph Dataset\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os.path as osp\nimport torch\nfrom torch_geometric.data import Dataset\nimport torch_geometric\nfrom torch._six import container_abcs, string_classes, int_classes\n\n\nclass ARCGeometricDataset(Dataset):\n    \"\"\"\n    ARC Geometric Dataset loads all ARC tasks in memory and builds a Geometric Batch aggregated graph\n    for each task.\n    Each element of the dataset is tuple of a Geometric Batch of all train samples in task and original images.\n    \n    Default class for padding of graph is 11 (10 + 1)\n    \"\"\"\n    def __init__(self, arc_path: str, root, mode: str = \"train\", transform=None, pre_transform=None,\n                 pad_value=10, merging: bool = False):\n        self.arc_path = Path(arc_path)\n        self.mode = mode\n        self.task_files = [self.arc_path / f for f in sorted(os.listdir(arc_path))]\n        self.tasks = [task_file.stem for task_file in self.task_files]\n        self.graph_files = [Path(root) / f\"{task}.pt\" for task in self.tasks]\n        self.pad_value = pad_value\n        self.merging = merging\n        super(ARCGeometricDataset, self).__init__(root, transform, pre_transform)\n\n    @property\n    def raw_file_names(self):\n        return self.tasks\n\n    @property\n    def processed_file_names(self):\n        return self.graph_files\n\n    @property\n    def raw_dir(self):\n        return  self.arc_path    \n    \n    @property\n    def processed_dir(self):\n        return osp.join(self.root, 'processed')\n    \n    @property\n    def num_node_features(self):\n        r\"\"\"Returns the number of features per node in the dataset.\"\"\"\n        return self[0][0].num_node_features    \n    #def download(self):\n        # Download to `self.raw_dir`.\n\n        \n    def process(self):\n        batches = []\n        batches_out = []\n        batches_pad = []\n        batches_out_pad = []\n        originals = []\n        batches_merged = []\n        batches_merged_out = []\n        nb = 0\n        for (task, task_file, graph_file) in zip(self.tasks, self.task_files, self.graph_files):\n            with open(task_file, 'r') as f:\n                data_batch = []\n                data_batch_out = []\n                data_batch_pad = []\n                data_batch_out_pad = []\n                img_batch = []\n                task = json.load(f)\n                for t in task[\"train\"]:\n                    img_batch.append(t)\n                    data = sample_to_graph(t[\"input\"])\n                    data_out = sample_to_graph(t[\"output\"])\n                    data_batch_out.append(data_out)\n                    \n                    # pad input to output\n                    data_pad = sample_to_graph_padded(t[\"input\"], t[\"output\"], pad_value = self.pad_value)\n                    # pad output to input to be sure to have same size at the end\n                    data_out_pad = sample_to_graph_padded(t[\"output\"], t[\"input\"], pad_value = self.pad_value)\n                    #data_pad.y = data_out_pad\n                    data_batch_out_pad.append(data_out_pad)\n\n                    data_batch.append(data)\n                    data_batch_pad.append(data_pad)\n\n                batch = Batch.from_data_list(data_batch, follow_batch = [])\n                batches.append(batch)\n\n                batch_out = Batch.from_data_list(data_batch_out, follow_batch = [])\n                batches_out.append(batch_out)\n\n                batch_pad = Batch.from_data_list(data_batch_pad, follow_batch = [])\n                batches_pad.append(batch_pad)\n\n                batch_out_pad = Batch.from_data_list(data_batch_out_pad, follow_batch = [])\n                batches_out_pad.append(batch_out_pad)\n                \n                originals.append(img_batch)\n                \n                if self.merging:\n                    # Create merged input + output graphs\n                    # for each index, replace the output by empty output to force it to learn generating them\n                    # concatenated in a batch\n                    data_merged = []\n                    data_merged_out = []\n                    for i in range(len(task[\"train\"])):\n                        for idx, t in enumerate(task[\"train\"]):\n                            merged = sample_to_graph_merged(t[\"input\"], t[\"output\"],\n                                                             empty_target=False,\n                                                             pad_value = self.pad_value)\n                            data_merged_out.append(merged)\n                            \n                            if i != idx:\n                                data_merged.append(merged)\n                            else:\n                                merged = sample_to_graph_merged(t[\"input\"], t[\"output\"],\n                                                                 empty_target=True,\n                                                                 pad_value = self.pad_value)\n                                data_merged.append(merged)\n                                \n                    data_merged = Batch.from_data_list(data_merged, follow_batch = [])\n                    data_merged_out = Batch.from_data_list(data_merged_out, follow_batch = [])\n                    batches_merged.append(data_merged)\n                    batches_merged_out.append(data_merged_out)\n\n                #torch.save(batch, graph_file)\n                nb += 1\n        self.batches = batches\n        self.batches_out = batches_out\n        \n        self.batches_pad = batches_pad\n        self.batches_out_pad = batches_out_pad\n        \n        self.batches_merged = batches_merged\n        self.batches_merged_out = batches_merged_out\n        \n        self.originals = originals\n\n        print(f\"Read {nb} files\")\n \n    def len(self):\n        return len(self.batches)\n\n    def get(self, idx):\n        if not self.merging:\n            return (self.batches[idx], self.batches_out[idx],\n                    self.batches_pad[idx], self.batches_out_pad[idx],\n                    self.originals[idx])\n        else:\n            return (self.batches[idx], self.batches_out[idx],\n                    self.batches_pad[idx], self.batches_out_pad[idx],\n                    self.batches_merged[idx], self.batches_merged_out[idx],\n                    self.originals[idx])\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = ARCGeometricDataset(training_path, \"./geometric\", pad_value=10, merging=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Task 1: Input and output matrix have same size\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_batch_images(ds[1], \"1\", use_pad=True, merging = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample Task 2: output matrix is bigger than input matrix\n\n> White node in input graph are padded node to make it same size as output graph\n> This is artificial and we could position original graph anywhere but it's easier\n> to test GraphNN since generating new nodes is quite hard with GraphNN out-of-the-box\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_batch_images(ds[0], \"0\", use_pad=True, merging = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sample 3: Output is a vector when input is a matrix\n\n> White node in input graph are padded node to make it same size as output graph\n> This is artificial and we could position original graph anywhere but it's easier\n> to test GraphNN since generating new nodes is quite hard with GraphNN out-of-the-box\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_batch_images(ds[338], \"338\", use_pad=True, merging=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DataLoader\n\nI override a bit Collater because every ARC task is a batch graph in my current use-case. So I didn't want to use default Pytorch Geometric dataloader which tries to rebuild Graph Batches.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n        \n# Customized        \nclass ARCCollater(object):\n    def __init__(self, follow_batch, merging: bool = False):\n        self.follow_batch = follow_batch\n        self.merging = merging\n\n    def collate(self, batch):\n        #res = []\n        batches_out = []\n        batches_out_pad = []\n        batches = []\n        batches_pad = []\n        batches_merged = []\n        batches_merged_out = []\n        images = []\n        for e in batch:\n            if not self.merging:\n                batch_data, batch_out_data, batch_pad, batch_out_pad, imgs = e\n            else:\n                batch_data, batch_out_data, batch_pad, batch_out_pad, batch_merged, batch_merged_out, imgs = e\n                batches_merged.append(batch_merged)\n                batches_merged_out.append(batch_merged_out.x)\n            batches.append(batch_data)\n            batches_pad.append(batch_pad)\n            batches_out.append(batch_out_data.x)\n            batches_out_pad.append(batch_out_pad.x)\n            images.append(imgs)\n\n        batches_out = torch.cat(batches_out, dim=0).long().squeeze()\n        batches_out_pad = torch.cat(batches_out_pad, dim=0).long().squeeze()\n            \n        if not self.merging:\n            return (batches, batches_out, batches_pad, batches_out_pad, images)\n        else:\n            batches_merged_out = torch.cat(batches_merged_out, dim=0).long().squeeze()\n            return (batches, batches_out, batches_pad, batches_out_pad, batches_merged, batches_merged_out, images)\n\n    def __call__(self, batch):\n        return self.collate(batch)\n\n\nclass ARCDataLoader(torch.utils.data.DataLoader):\n    r\"\"\"Data loader which merges data objects from a\n    :class:`torch_geometric.data.dataset` to a mini-batch.\n    Args:\n        dataset (Dataset): The dataset from which to load the data.\n        batch_size (int, optional): How many samples per batch to load.\n            (default: :obj:`1`)\n        shuffle (bool, optional): If set to :obj:`True`, the data will be\n            reshuffled at every epoch. (default: :obj:`False`)\n        follow_batch (list or tuple, optional): Creates assignment batch\n            vectors for each key in the list. (default: :obj:`[]`)\n    \"\"\"\n\n    def __init__(self, dataset, batch_size=1, shuffle=False, follow_batch=[], merging: bool = True,\n                 **kwargs):\n        super(ARCDataLoader,\n              self).__init__(dataset, batch_size, shuffle,\n                             collate_fn=ARCCollater(follow_batch, merging), **kwargs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GraphNN basic Model\n\n- 2 layers of Graph Convolution Network https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv with 16 features.\n\n- Using Pytorch Lightning facilities\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom torch_geometric import utils\n\n\nclass FocalLoss(nn.Module):\n\n    \"\"\"Focal Loss.\"\"\"\n\n    def __init__(self, alpha=1, gamma=2, reduce=True):\n        \"\"\"Constructor.\"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduce = reduce\n        self.ce_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n\n    def forward(self, inputs, targets):\n        \"\"\"Forward propagation.\"\"\"\n        BCE_loss = self.ce_loss_fn(inputs, targets)\n\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss  # type: ignore\n\n        if self.reduce:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n\n\nclass Net(pl.LightningModule):\n    def __init__(self, hparams):\n        self.hparams = hparams\n        print(\"hparams\", hparams)\n        super(Net, self).__init__()\n        \n        self.data_path = Path(hparams.data_path)\n        self.training_path = self.data_path / 'training'\n        self.evaluation_path = self.data_path / 'evaluation'\n        self.test_path = self.data_path / 'test'\n        self.use_pad = hparams.use_pad\n        self.merging = hparams.merging\n        self.num_classes = hparams.num_classes\n        self.lr = hparams.lr\n        self.focal_alpha = hparams.focal_alpha\n        self.focal_lambda = hparams.focal_lambda\n        \n        self.conv1 = GCNConv(hparams.num_node_features, 32)\n        self.conv2 = GCNConv(32, 32)\n#        self.conv3 = GCNConv(64, 32)\n#        self.conv4 = GCNConv(128, 256)\n#        self.conv5 = GCNConv(256, 128)\n#        self.conv6 = GCNConv(128, 64)\n#        self.conv7 = GCNConv(64, 32)\n        self.conv_last = GCNConv(32, 11)\n        #self.loss_fn = torch.nn.CrossEntropyLoss()\n        self.loss_fn = FocalLoss(alpha=self.focal_alpha, gamma=self.focal_lambda)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, training=self.training)\n#        x = self.conv3(x, edge_index)\n#        x = F.relu(x)\n#        x = F.dropout(x, training=self.training)\n#        x = self.conv4(x, edge_index)\n#        x = F.relu(x)\n#        x = F.dropout(x, training=self.training)\n#        x = self.conv5(x, edge_index)\n#        x = F.relu(x)\n#        x = F.dropout(x, training=self.training)\n#        x = self.conv6(x, edge_index)\n#        x = F.relu(x)\n#        x = F.dropout(x, training=self.training)\n#        x = self.conv7(x, edge_index)\n#        x = F.relu(x)\n#        x = F.dropout(x, training=self.training)\n        x = self.conv_last(x, edge_index)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        data_batches = []\n        images = []\n        if not self.merging:\n            if self.use_pad:\n                _, _, data_batch, data_target, _ = batch\n            else:\n                data_batch, data_target, _, _, _ = batch\n        else:\n            _, _, _, _, data_batch, data_target, _ = batch\n        out = []\n        for db in data_batch:\n            out.append(self(db))\n        out = torch.cat(out, dim=0)\n        loss = self.loss_fn(out, data_target)\n        tensorboard_logs = {'train_loss': loss}\n        return {'loss': loss, 'log': tensorboard_logs}\n\n    def validation_step(self, batch, batch_idx):\n        data_batches = []\n        images = []\n        if not self.merging:\n            if self.use_pad:\n                _, _, data_batch, data_target, images = batch\n            else:\n                data_batch, data_target, _, _, images = batch\n        else:\n            _, _, _, _, data_batch, data_target, images = batch\n        out = []\n        for db in data_batch:\n            db.to(device, non_blocking=True)\n            out.append(self(db))\n        out = torch.cat(out, dim=0)\n        loss = self.loss_fn(out, data_target)\n        out = torch.argmax(out, dim=1)\n        tp = utils.true_positive(out, data_target, self.num_classes-1)\n        tn = utils.true_negative(out, data_target, self.num_classes-1)\n        fp = utils.false_positive(out, data_target, self.num_classes-1)\n        fn = utils.false_negative(out, data_target, self.num_classes-1)\n\n        return {'val_loss': loss, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        tps = torch.sum(torch.cat([x['tp'] for x in outputs])).to(torch.float)\n        tns = torch.sum(torch.cat([x['tn'] for x in outputs])).to(torch.float)\n        fps = torch.sum(torch.cat([x['fp'] for x in outputs])).to(torch.float)\n        fns = torch.sum(torch.cat([x['fn'] for x in outputs])).to(torch.float)\n        \n        precision = tps / (tps + fps)\n        precision[torch.isnan(precision)] = 0\n        \n        recall = tps / (tps + fns)\n        recall[torch.isnan(recall)] = 0\n        \n        f1_score = 2 * (precision * recall) / (precision + recall)\n        f1_score[torch.isnan(f1_score)] = 0\n    \n        tensorboard_logs = {'val_loss': avg_loss, 'precision': precision, 'recall': recall, 'f1_score': f1_score}\n        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=5e-4)\n        \n    def train_dataloader(self):\n        ds = ARCGeometricDataset(self.training_path, \"./geometric\", merging=self.merging, pad_value=10)\n        \n        train_dataloader = ARCDataLoader(ds, batch_size=1, shuffle=True, num_workers=5, merging=self.merging)\n        return train_dataloader\n\n    def val_dataloader(self):\n        ds = ARCGeometricDataset(self.evaluation_path, \"./geometric\", merging=self.merging, pad_value=10)\n        \n        val_dataloader = ARCDataLoader(ds, batch_size=1, shuffle=False, num_workers=5, merging=self.merging)\n        return val_dataloader\n\n    @staticmethod\n    def add_model_specific_args(parent_parser):\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n        parser.add_argument(\"--use_pad\", action=\"store_true\", help=\"Use graph padding.\")\n        parser.add_argument(\"--merging\", action=\"store_true\", help=\"Use graph merging.\")\n        parser.add_argument('--data_path', type=str)\n        parser.add_argument('--num_classes', type=int, default=10)\n        parser.add_argument('--num_node_features', type=int, default=1)\n        parser.add_argument('--lr', type=float, default=0.001)\n        parser.add_argument('--focal_alpha', type=float, default=0.5)\n        parser.add_argument('--focal_lambda', type=float, default=2.0)\n        return parser","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lightning Trainer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\nfrom argparse import ArgumentParser\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"device\", device)\n\n#, use_pad: bool = True, merging: bool = True,\n#num_classes=10, num_node_features: int = 1,\n#                 data_path = Path('kaggle/input/abstraction-and-reasoning-challenge/')\n                                  \nargs = [\n    \"--use_pad\",\n    \"--merging\",\n    \"--num_classes\", \"10\",\n    \"--num_node_features\", \"1\",\n    \"--data_path\", \"kaggle/input/abstraction-and-reasoning-challenge/\",\n    \"--lr\", \"0.000001\",\n    \"--focal_alpha\", \"0.5\",\n    \"--focal_lambda\", \"2.0\",\n]\n\nparser = ArgumentParser()\nparser = Trainer.add_argparse_args(parser)\nparser = Net.add_model_specific_args(parser)\nhparams = parser.parse_args(args)\n\nmodel = Net(hparams)\n\nexp = \"arc_geometric_v7\"\nwandb_logger = WandbLogger(project=\"mandubian-arc-graph\", name=exp)\n#logger = TensorBoardLogger(\"tb_logs\", name=exp)\n\n\n# DEFAULTS used by the Trainer\ncheckpoint_callback = ModelCheckpoint(\n    filepath=f\"./checkpoints/{exp}\",\n    save_top_k=True,\n    verbose=False,\n    monitor='val_loss',\n    mode='min',\n    #prefix=exp + \"_\"\n)\n\ntrainer = Trainer(max_epochs=400, logger=wandb_logger, pin_memory=True,\n                  distributed_backend=None, checkpoint_callback=checkpoint_callback, gpus=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wandb\nwandb.init()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Eval","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = Net.load_from_checkpoint(checkpoint_path=\"./checkpoints/arc_geometric_v1_epoch=99.ckpt\")\nbest_model.freeze()\nbest_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(data_path / 'sample_submission.csv', index_col='output_id')\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for output_id in submission.index:\n    task_id = output_id.split('_')[0]\n    pair_id = int(output_id.split('_')[1])\n    f = str(test_path / str(task_id + '.json'))\n    print(\"reading file\", f)\n    with open(f, 'r') as read_file:\n        task = json.load(read_file)\n    # skipping over the training examples, since this will be naive predictions\n    # we will use the test input grid as the base, and make some modifications\n    # print(\"task\", task)\n    data = task['test'][pair_id]['input'] # test pair input\n    \n    data_merged = []\n    output_size = 0\n    task_test = task['test'][pair_id] # test task\n    test_height_input = len(task_test[\"input\"])\n    test_width_input = len(task_test[\"input\"][0])\n    min_height_diff = 1e20\n    ref_output = None\n    ref_output_height = None\n    ref_output_width = None\n    ref_output_max = None\n    ref_output_rate = None\n    ref_rate_diff = 1e10\n    for idx, t in enumerate(task[\"train\"]):\n        t_height_input = len(t[\"input\"])\n        t_width_input = len(t[\"input\"][0])\n        t_height_output = len(t[\"output\"])\n        t_width_output = len(t[\"output\"][0])\n        # engineering output size here ;)\n        if test_height_input == t_height_input and test_width_input == t_width_input:\n            ref_output = t[\"output\"]\n        elif test_height_input == t_height_input:\n            ref_output_height = t[\"output\"]\n        elif test_width_input == t_width_input:\n            ref_output_width = t[\"output\"]\n        else:\n            r = t_height_input / t_width_input - test_height_input / test_width_input\n            if r < ref_rate_diff:\n                ref_rate_diff = r\n                ref_output_rate = t[\"output\"]\n        \n        merged = sample_to_graph_merged(t[\"input\"], t[\"output\"],\n                                        empty_target=False,\n                                        pad_value = 10)\n        data_merged.append(merged)\n\n    ref_output = ref_output or ref_output_height or ref_output_width or ref_output_rate\n    ref_height_output = len(ref_output)\n    ref_width_output = len(ref_output[0])\n    test_merged = sample_to_graph_merged(task_test[\"input\"], ref_output,\n                                    empty_target=True,\n                                    pad_value = 10)\n    data_merged.append(test_merged)\n    \n    batch = Batch.from_data_list(data_merged)\n    \n    #print(\"batch\", batch)\n    output = best_model(batch)\n    #print(\"output\", output)\n    output = torch.nn.functional.softmax(output)\n    #print(\"output\", output)\n    output = torch.argmax(output, dim=1)\n    output = output[:-ref_height_output * -ref_width_output]\n    output = output.view(ref_height_output, ref_width_output)\n    output = output.cpu().numpy().tolist()\n    pred_1 = flattener(output)\n    pred = pred_1\n    submission.loc[output_id, 'output'] = pred\n    #graph = sample_to_graph(data)\n    # for the first guess, predict that output is unchanged\n    #pred_1 = flattener(data)\n    # for the second guess, change all 0s to 5s\n    #data = [[5 if i==0 else i for i in j] for j in data]\n    #pred_2 = flattener(data)\n    # for the last gues, change everything to 0\n    #data = [[0 for i in j] for j in data]\n    #pred_3 = flattener(data)\n    # concatenate and add to the submission output\n    #pred = pred_1 + ' ' + pred_2 + ' ' + pred_3 + ' ' \n    #submission.loc[output_id, 'output'] = pred\nprint(submission)\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}