{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/KXtGwDB.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**THIS IS MY FIRST PYTHON KERNEL (on Kaggle) AND A WORK IN PROGRESS (31/05/19)**\n\nTo whom it may concern,\n\nI'm currently looking for full-time work in North America, Europe, or Remote. If you or someone you know is hiring for the type of work displayed in this kernel I'd be interested in speaking with you. All of my past work experience is in digital marketing but I have >1000hrs building full-stack ReactJS apps in NodeJS and a handfull of kernels / notebooks I can share that display knowledge of Computer Vision, Natural Language Processing, Sequence Mining, Advanced Visualization & other data science-type stuff written in R & Python.\n\nI can be reached at: Eric@Casey(dot)Works\n\n**The Problem:**\n\nWe have 5,310 pairs of single photos that we have to submit probabilities of being related to eachother between 0-1. With 0 being not related at all to 1 being related.\n\n**The Data:**\n\nWe have been given a training dataset of 3,598 pairs of individuals who ARE related to eachother and part of one of 470 families in the dataset. Each of these individuals has at least one photo.\n\nI've decided to add another kaggle-hosted dataset with 381 other people to use to balance the pairs that are related to some that are unrelated to anyone in the training dataset. Also to make this more wild.\n\n**Feature Statuses:**\n\n* [WORKING] - Age Detection 1 - https://github.com/spmallick/learnopencv\n* [WORKING] - Gender Detection 1 - https://github.com/spmallick/learnopencv\n* [WORKING] - FaceNet - https://arxiv.org/abs/1503.03832\n* [WORKING] - B&W Image SSIM - https://en.wikipedia.org/wiki/Structural_similarity\n* [WORKING] - Oxford Visual Geometry Group - http://www.robots.ox.ac.uk/~vgg/data/vgg_face/\n* [ NEXT ] - Selecting Optimal Comparison Photos (finding frontal pose)\n* [ NEXT ] - Generating Family / Descendance Clusters Somehow\n\n**Dataset Notes:**\n\n* Mix of photos with different genders, makeup, ages & headwear. \n* Some are photos B&W, some are color, different image qualities as well.\n* They also have different head poses, expressions & lighting. (but the 3rd-party dataset appears to have just a green background)\n\n**Model Notes:**\n\n* Using two different pre-trained facial verification / recognition models doesn't help here (and just takes way longer).\n\n**TODOS:**\n\n- Face angle detection, for getting aligned images? just use image symmetry? align nose points to center of img?\n- Eye color?\n- Hair color?\n- Aligning age? What's the age distributions of the list of photos for each individual?\n- Ethnicity detection?\n- Delaunay triangulation of each family? Then clustering?\n- Expression detection?\n- Average RGB in image?\n- Landmarks? (doesn't seem to help much in other public kernels)\n\n**Links:**\n\n- https://github.com/wondonghyeon/face-classification\n- http://www.tomgibara.com/computer-vision/symmetry-detection-algorithm\n- https://github.com/usc-sail/mica-race-from-face\n- https://en.wikipedia.org/wiki/Delaunay_triangulation\n- http://www.robots.ox.ac.uk/~vgg/data/vgg_face/\n- https://github.com/AaronJackson/vrn\n- https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n- http://vis-www.cs.umass.edu/lfw/\n- https://github.com/rcmalli/keras-vggface\n- https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/\n"},{"metadata":{},"cell_type":"markdown","source":"** [0] IMPORTS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install dlib\n!pip install git+https://github.com/rcmalli/keras-vggface.git\n!pip install imutils","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys \nimport cv2\nimport glob\nimport dlib\nimport math\nimport random\nimport sklearn\nimport imutils\nimport imageio\nimport itertools\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom random import choice, sample\nfrom cv2 import imread\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom imutils import face_utils\nfrom PIL import Image\n\nfrom IPython.display import HTML, display\nfrom matplotlib.pyplot import imshow\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d import proj3d\nfrom scipy.spatial import distance\n\nimport keras_vggface\nfrom keras_vggface.vggface import VGGFace       \nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom collections import defaultdict\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Activation\nfrom keras.models import Model, Sequential, model_from_json, load_model\nfrom keras.optimizers import Adam\nfrom keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.preprocessing import image\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom skimage.measure import compare_ssim\nfrom sklearn.decomposition import PCA                    \nfrom skimage.transform import resize          \nfrom sklearn.model_selection import train_test_split     \n\nimport xgboost as xgb\n\nprint(sys.executable)\n# print(os.sys.path)\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Version Check\n# TODO: Table of Contents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[1] PRE-TRAINED COMPUTER VISION MODELS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# VGG Face\nmodel = Sequential()\nmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\nmodel.add(Convolution2D(64, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n \nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n \nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n \nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(512, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n \nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(512, (3, 3), activation='relu'))\nmodel.add(ZeroPadding2D((1,1)))\nmodel.add(Convolution2D(512, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n \nmodel.add(Convolution2D(4096, (7, 7), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Convolution2D(4096, (1, 1), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Convolution2D(2622, (1, 1)))\nmodel.add(Flatten())\nmodel.add(Activation('softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age Detection - https://github.com/spmallick/learnopencv\nageProto = \"../input/learnopencvcom-model-weights/age_deploy.prototxt\"\nageModel = \"../input/learnopencvcom-model-weights/age_net.caffemodel\"\n\n# Gender Detection - https://github.com/spmallick/learnopencv\ngenderProto = \"../input/learnopencvcom-model-weights/gender_deploy.prototxt\"\ngenderModel = \"../input/learnopencvcom-model-weights/gender_net.caffemodel\"\n\n# FaceNet - https://arxiv.org/abs/1503.03832\nfacenet_model = '../input/facenetkeras/facenet_keras_1.h5'\nfacenetModel = load_model(facenet_model)\n\n# VGG - http://www.robots.ox.ac.uk/~vgg/data/vgg_face/\nmodel.load_weights('../input/vgg-face-weights/vgg_face_weights.h5')\nvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n\n# 68 Landmarks - https://github.com/spmallick/learnopencv\npredictor_path_68 = '../input/utilities/shape_predictor_68_face_landmarks.dat'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[3] IMPORTING THE TRAIN & TEST DATA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST & TRAIN FILES\ntrain_df = pd.read_csv('../input/recognizing-faces-in-the-wild/train_relationships.csv')\ntest_df = pd.read_csv('../input/recognizing-faces-in-the-wild/sample_submission.csv')\n\n# BASE PATHS FOR IMAGES\ntrain_img_path = Path('../input/recognizing-faces-in-the-wild/train/')\ntest_img_path = Path('../input/recognizing-faces-in-the-wild/test/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[4] DATASET INSPECTION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def inspect_pair(index):\n\n    img_1 = mpimg.imread( test_img_path / test_df.loc[index, 'img_pair'].split('-')[0] )\n    img_2 = mpimg.imread( test_img_path / test_df.loc[index, 'img_pair'].split('-')[1] )\n\n    fig = plt.figure()\n    fig.suptitle('Are they related?', fontsize=16)\n    a = fig.add_subplot(1, 2, 1)\n    imgplot = plt.imshow(img_1)\n    a.set_title('Person 1 - ' + test_df.loc[index, 'img_pair'].split('-')[0])\n    a = fig.add_subplot(1, 2, 2)\n    imgplot = plt.imshow(img_2)\n    imgplot.set_clim(0.0, 0.7)\n    a.set_title('Person 2 - ' + test_df.loc[index, 'img_pair'].split('-')[1])\n    \ninspect_pair(1) \ninspect_pair(2) # Celebrities.\ninspect_pair(3) # Same Background, probably related.  1\ninspect_pair(4) \n\n## TODO: Make this 2x2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def inspect_individual(index):\n#     # loop through all photos of that individual in the training data\n#     print(index)\n#     print(train_img_path)\n#     print(train_df.loc[index, 'p1'])\n    \n#     img_list = os.listdir(train_img_path / train_df.loc[index, 'p1'])\n    \n#     print(\"this individual has \", len(img_list), \" photos\")\n    \n#     for img in img_list:\n#         print(img)\n#         img = mpimg.imread( train_img_path / train_df.loc[index, 'p1'] / img )\n#         plt.imshow(img)\n\n# inspect_individual(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def inspect_family(id):\n#     print('Family ID: ', id)\n#     print('Family Members: ')\n    \n# inspect_family('F0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_profile(individual):\n    if len(individual.split('/')) == 2: # ORIGINAL DATASET\n        if len(train_df[train_df['p1'] == individual]['p1_photo_count'].head(1).values) == 0:\n            return {\n                'photo_list': train_df[train_df['p2'] == individual]['p2_photo_list'].head(1).values[0],\n                'photo_count': train_df[train_df['p2'] == individual]['p2_photo_count'].head(1).values[0]\n            }\n        else:\n            return {                                                        # it's not stupid if it works\n                'photo_list': train_df[train_df['p1'] == individual]['p1_photo_list'].head(1).values[0],\n                'photo_count': train_df[train_df['p1'] == individual]['p1_photo_count'].head(1).values[0]\n            }\n    else:                              # 3RD PARTY DATASET\n        return {\n            'photo_list': third_party[third_party['p1'] == individual]['p1_photo_list'].head(1).values[0],\n            'photo_count': third_party[third_party['p1'] == individual]['p1_photo_count'].head(1).values[0]\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[5] REFORMATTING TRAINING DATASET**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# SETUP SOME COLUMNS TO USE LATER\ntrain_df['is_related'] = 1\ntrain_df['p1_photo_count'] = 0\ntrain_df['p2_photo_count'] = 0\ntrain_df['p1_photo_list'] = ''\ntrain_df['p2_photo_list'] = ''\ntrain_df['combo'] = ''\n\nprint(train_df.shape[0], ' rows of confirmed kinship for model training')\n\n# LOOP THROUGH EACH KNOWN KINSHIP\nfor i in range(0, train_df.shape[0]) :\n    p1_path = '../input/recognizing-faces-in-the-wild/train/' + train_df.loc[i, 'p1'] + \"/\"\n    p2_path = '../input/recognizing-faces-in-the-wild/train/' + train_df.loc[i, 'p2'] + \"/\"\n\n    # CHECK IF DIRECTORY EXISTS (sometimes it's not present, unless I've done something wrong)\n    if os.path.isdir(p1_path) & os.path.isdir(p2_path):\n        train_df.loc[i, 'p1_photo_count'] = len(os.listdir(train_img_path / train_df.loc[i, 'p1'] ))\n        train_df.loc[i, 'p2_photo_count'] = len(os.listdir(train_img_path / train_df.loc[i, 'p2'] ))\n        train_df.at[i, 'p1_photo_list'] = os.listdir(train_img_path / train_df.loc[i, 'p1'] )\n        train_df.at[i, 'p2_photo_list'] = os.listdir(train_img_path / train_df.loc[i, 'p2'] )\n        train_df.loc[i, 'combo'] = train_df.loc[i, 'p1'] + '-' + train_df.loc[i, 'p2']\n\ntrain_columns = list(train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[6] ADD 3RD PARTY DATASET TO MAKE THIS *MORE WILD***"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = {}\nindividual_list = []\nthird_party = pd.DataFrame(columns = train_columns)\n\nimg_list_3rd = os.listdir('../input/faces-data-new/images/images')\n\nprint(len(img_list_3rd), ' photos in the given dataset')\n\nfor i in range(0, len(img_list_3rd)):\n    if img_list_3rd[i].split('.')[0] not in individual_list:\n        individual_list.append(img_list_3rd[i].split('.')[0]) \n        new_data[img_list_3rd[i].split('.')[0]] = { \n            'photo_count': 1, \n            'photo_list': [ img_list_3rd[i] ] \n        }\n    else:\n        files = new_data[img_list_3rd[i].split('.')[0]]['photo_list']\n        files.append(img_list_3rd[i])\n        new_data[img_list_3rd[i].split('.')[0]] = { \n            'photo_count': new_data[img_list_3rd[i].split('.')[0]]['photo_count'] + 1, \n            'photo_list': files\n        }\n\nprint(len(individual_list), ' individuals from 3rd party sources to be added')\n\nfor i in range(0, len(individual_list)):\n    third_party.loc[i, 'p1'] = individual_list[i]\n    third_party.loc[i, 'is_related'] = 0\n    third_party.loc[i, 'p1_photo_count'] = new_data[individual_list[i]]['photo_count']\n    third_party.loc[i, 'p1_photo_list'] = new_data[individual_list[i]]['photo_list']\n        \n# print(third_party.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[7] GENERATING ALL POSSIBLE COMBINATIONS OF INDIVIDUALS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GET A LIST OF ALL INDIVIDUALS IN THE TRAINING DATA\ntrain_individuals = list(set(list(train_df['p1']) + list(train_df['p2'])))\nprint(len(train_individuals), ' individuals in training dataset')\nprint(len(individual_list), ' individuals from the 3rd party data')\nall_individuals = train_individuals + individual_list\nprint(len(all_individuals), ' total individuals in the new training data')\n\n# GENERATE ALL POSSIBLE COMBINATIONS OF INDIVIDUALS\ncombinations = list(itertools.combinations(train_individuals + individual_list, 2))\nprint(len(combinations), ' possible 2-way combinations of individuals')\n\n# CLEAR OUT ANY RELATED COMBINATIONS\nfor i in tqdm(range(0, train_df.shape[0])):\n    combo = ( train_df.loc[i, 'p1'], train_df.loc[i, 'p2'] )\n    if combo in combinations:\n        combinations.remove(combo)\nprint(len(combinations), ' possible 2-way combinations of individuals after removing duplicates/related people')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[8] GENERATING NEW UNRELATED ROWS FOR TRAINING DATASET**"},{"metadata":{"trusted":true},"cell_type":"code","source":"goal_n = 10000  # 10k this gives us a 64/36-ish ratio for our target of 'is_related'\n\nprint(goal_n - train_df.shape[0], ' new rows to be created for training dataset')\n\n# generating a list of random numbers for indexes to grab from the combinations list\nrandom_list = []\nfor i in range(0, goal_n - train_df.shape[0]): # for each of the number of rows I need to add\n    x = random.randint(0, len(combinations))   # generate a random number between 0 and combinations length\n    random_list.append(x)\n\nrows_list = []\n\nfor i in range(0, goal_n - train_df.shape[0]):\n\n    combo = combinations[random_list[i]][0] + '-' + combinations[random_list[i]][1]\n    \n    p1 = get_profile(combo.split('-')[0])\n    p2 = get_profile(combo.split('-')[1])\n\n    new_row = {\n        'p1': combinations[random_list[i]][0],\n        'p2': combinations[random_list[i]][1],\n        'is_related': 0, \n        'p1_photo_count': p1['photo_count'], \n        'p2_photo_count': p2['photo_count'], \n        'p1_photo_list': p1['photo_list'], \n        'p2_photo_list': p2['photo_list'],  \n        'combo': combo\n    }\n    \n    rows_list.append(new_row)\n\nnew_rows = pd.DataFrame(rows_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[9] MASH RELATED AND UNRELATED TOGETHER**"},{"metadata":{"trusted":true},"cell_type":"code","source":"if train_df.shape[0] <= goal_n:\n    train_df = pd.concat( [ train_df, new_rows ], ignore_index=True )\n\nprint(train_df.shape[0],' rows after generating new, unrelated pairs of people')\nprint('the sum of the is_related column is: ', sum(list(train_df['is_related'])))\nprint('target ratio is: ', sum(list(train_df['is_related'])) /  train_df.shape[0])\n\ntrain_df = train_df[train_df.p1_photo_count != 0]\ntrain_df = train_df[train_df.p2_photo_count != 0]\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True) # random sort training data (easier to debug)\n\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[10] FEATURE CREATION FUNCTIONS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_age(path):\n    # https://github.com/spmallick/learnopencv\n    # This returns the index of the age bucket based on a pre-trained model by Satya Mallick\n    \n    cv_face = cv2.imread(path, cv2.COLOR_BGR2GRAY)\n    ageNet = cv2.dnn.readNet(ageModel, ageProto)\n    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746) # why? where tf are these from?\n    #             1,          2,         3,          4,           5,            6,          7,            8\n    ageList = ['(0 - 2)', '(4 - 6)', '(8 - 12)', '(15 - 20)', '(25 - 32)', '(38 - 43)', '(48 - 53)', '(60 - 100)']\n    blob = cv2.dnn.blobFromImage(cv_face, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n    ageNet.setInput(blob)\n    agePreds = ageNet.forward()\n    age = ageList[agePreds[0].argmax()]\n    return ageList.index(age) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_gender(path):\n    # https://github.com/spmallick/learnopencv\n    # This returns the % male based on a pre-trained model by Satya Mallick\n    \n    cv_face = cv2.imread(path, cv2.COLOR_BGR2GRAY)\n    genderNet = cv2.dnn.readNet(genderModel, genderProto)\n\n    MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746) # why? where tf are these from?\n    genderList = ['Male', 'Female']\n\n    blob = cv2.dnn.blobFromImage(cv_face, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n    genderNet.setInput(blob)\n    genderPreds = genderNet.forward()\n    gender = genderList[genderPreds[0].argmax()]\n    return genderPreds[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_pairdiff(path_1, path_2):\n    # https://www.pyimagesearch.com/2017/06/19/image-difference-with-opencv-and-python/\n    # This is straight up just the % difference between the two images in gayscale\n\n    imageA = cv2.imread(path_1)\n    imageB = cv2.imread(path_2)\n\n    grayA = cv2.resize(cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY),(277,277))\n    grayB = cv2.resize(cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY),(277,277))\n    \n    # Structural Similarity Index (SSIM)\n    (score, diff) = compare_ssim(grayA, grayB, full=True)\n    diff = (diff * 255).astype(\"uint8\")\n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_imqual(path):\n    # Simply the size of the image.\n    \n    img = cv2.imread(path, cv2.COLOR_BGR2GRAY)\n    return img.size\n\n# get_imqual('../input/recognizing-faces-in-the-wild/train/F0002/MID2/P00017_face2.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_symmetry(path):\n    ## Just splits the image in half, then flips one, then compares the difference between the two\n    \n    img = cv2.imread(path, cv2.COLOR_BGR2GRAY)\n    center = img.shape[1]\n    right_side = img[:,int(img.shape[1]/2):int(img.shape[1])]\n    left_side = img[:,0:int(img.shape[1]/2)]\n    right_flipped = cv2.flip( right_side, 1 )\n    grayA = cv2.cvtColor(left_side, cv2.COLOR_BGR2GRAY)\n    grayB = cv2.cvtColor(right_flipped, cv2.COLOR_BGR2GRAY)\n    (score, diff) = compare_ssim(grayA, grayB, full=True)\n#     imshow(diff)\n    return score\n\n# get_symmetry('../input/recognizing-faces-in-the-wild/train/F0002/MID2/P00017_face2.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Get 68 Points\ndef get_68_points(path):\n    print(path)\n\n    detector = dlib.get_frontal_face_detector()\n    predictor = dlib.shape_predictor(predictor_path_68)\n    image = dlib.load_rgb_image(path)\n    \n    dets = detector(image, 1)\n          \n    for k, d in enumerate(dets):\n\n        shape = predictor(image, d)\n        shape = face_utils.shape_to_np(shape)\n\n        for (x, y) in shape:\n            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n\n        return shape\n\n    \nresult = get_68_points('../input/recognizing-faces-in-the-wild/train/F0002/MID2/P00017_face2.jpg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## https://www.kaggle.com/suicaokhoailang/facenet-baseline-in-keras-0-749-lb\n## Huge shoutout to Khoi Nguyen for this part. \n\ndef prewhiten(x):\n    if x.ndim == 4:\n        axis = (1, 2, 3)\n        size = x[0].size\n    elif x.ndim == 3:\n        axis = (0, 1, 2)\n        size = x.size\n    else:\n        raise ValueError('Dimension should be 3 or 4')\n\n    mean = np.mean(x, axis=axis, keepdims=True)\n    std = np.std(x, axis=axis, keepdims=True)\n    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n    y = (x - mean) / std_adj\n    return y\n\ndef l2_normalize(x, axis=-1, epsilon=1e-10):\n    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n    return output\n\ndef load_and_align_images(filepaths, margin,image_size = 160):\n    aligned_images = []\n    for filepath in filepaths:\n        img = imread(filepath)\n        aligned = resize(img, (image_size, image_size), mode='reflect')\n        aligned_images.append(aligned)\n            \n    return np.array(aligned_images)\n\ndef calc_embs(filepaths, margin = 10, batch_size = 512):\n    pd = []\n    for start in range(0, len(filepaths)):\n        aligned_images = prewhiten(load_and_align_images(filepaths[start:start+batch_size], margin))\n        pd.append(facenetModel.predict_on_batch(aligned_images)) # ON BATCH! COOL!\n    embs = l2_normalize(np.concatenate(pd))\n\n    return embs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_facenet_diff(p1_path, p2_path):\n\n    p1_emb = calc_embs([p1_path])\n    p2_emb = calc_embs([p2_path])\n\n    pair_embs = [p1_emb, p2_emb]\n    \n    img2idx = dict()                                                   # Khoi Nguyen\n    for idx, img in enumerate([p1_path, p2_path]):                     # Khoi Nguyen\n        img2idx[img] = idx                                             # Khoi Nguyen\n\n    imgs = [pair_embs[img2idx[img]] for img in [p1_path, p2_path]]     # Khoi Nguyen\n    emb_euc_dist = distance.euclidean(*imgs)                           # Khoi Nguyen\n\n    return emb_euc_dist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VGG Model\n# https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(224, 224))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img\n\ndef findCosineSimilarity(source_representation, test_representation):\n    a = np.matmul(np.transpose(source_representation), test_representation)\n    b = np.sum(np.multiply(source_representation, source_representation))\n    c = np.sum(np.multiply(test_representation, test_representation))\n    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))\n \ndef findEuclideanDistance(source_representation, test_representation):\n    euclidean_distance = source_representation - test_representation\n    euclidean_distance = np.sum(np.multiply(euclidean_distance, euclidean_distance))\n    euclidean_distance = np.sqrt(euclidean_distance)\n    return euclidean_distance\n\n# epsilon = 0.40 # cosine similarity     - these are the cutoff values if we're looking to\n# epsilon = 120  # euclidean distance    - verify that these two images are the same person\n \ndef get_vgg_diff(img1, img2):\n    img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]\n    img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]\n    \n    cosine_similarity = findCosineSimilarity(img1_representation, img2_representation)\n    euclidean_distance = findEuclideanDistance(img1_representation, img2_representation)\n\n    return { \n        'vgg_cosine': cosine_similarity,\n        'vgg_euclid': euclidean_distance\n    }\n\n# path_1 = '../input/recognizing-faces-in-the-wild/train/F0680/MID4/P07097_face2.jpg'\n# path_2 = '../input/recognizing-faces-in-the-wild/train/F0680/MID4/P07102_face3.jpg'\n\n# get_vgg_diff(path_1, path_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_source(photo, comboID):\n    if photo[0:1] == 'P':\n        return '../input/recognizing-faces-in-the-wild/train/' + comboID + '/'\n    if photo[0:4] == 'face':\n        return '../input/recognizing-faces-in-the-wild/test/'\n    else:\n        return '../input/faces-data-new/images/images/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_best_photo(photo_list, comboID):\n    \n    score_list = []\n    \n    for photo in photo_list:\n\n        source = get_source(photo, comboID)\n\n        path = source + photo\n        \n        symmetry_score = get_symmetry(source + photo)\n        \n        score_list.append(symmetry_score)\n\n    max_index = score_list.index(max(score_list))\n    \n    return max_index\n\n# get_best_photo(['P00009_face3.jpg', 'P00013_face2.jpg', 'P00010_face4.jpg'], 'F0002/MID1')\n# imshow(cv2.imread('../input/recognizing-faces-in-the-wild/train/F0002/MID1/P00010_face4.jpg'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: get the head pose of the image.\ndef get_pose(path):\n    print(path)\n    \n# get_pose('path here')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: https://github.com/usc-sail/mica-race-from-face\n# def get_ethnicity(path):\n#     print(path)\n    \n# get_ethnicity('../input/recognizing-faces-in-the-wild/test/' + test_df.loc[3, 'img_pair'].split('-')[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: get eye color\n# def get_eyecolor(path):\n#     print(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[11] COMPARE INDIVIDUALS FUNCTION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_individuals(p1_photo_list, p2_photo_list, combo):\n    # this function receives two lists of multiple image paths for the same individual and returns all of the features\n    # I think this could count as a one-shot siamese model as it is right now.?\n    \n    # TODO: find a way to find the 'best' photo or use all photos somehow.\n    #       * figure out how symmetrical the test photos are and match training data to it?\n    #       * group multiple images that are similar, remove duplicates?\n    #       * Can I get a 3D mesh?\n    \n    p1_best_photo = get_best_photo(p1_photo_list, combo.split('-')[0])\n    p2_best_photo = get_best_photo(p2_photo_list, combo.split('-')[1])\n    \n    p1_photo = p1_photo_list[p1_best_photo] \n    p2_photo = p2_photo_list[p2_best_photo]\n    p1_source = get_source(p1_photo, combo.split('-')[0])\n    p2_source = get_source(p2_photo, combo.split('-')[1])\n    \n    p1_age = get_age(p1_source + p1_photo)\n    p2_age = get_age(p2_source + p2_photo)\n    p1_size = get_imqual(p1_source + p1_photo)\n    p2_size = get_imqual(p2_source + p2_photo)\n    p1_gender = get_gender(p1_source + p1_photo)\n    p2_gender = get_gender(p2_source + p2_photo)  \n    vgg_diff = get_vgg_diff(p1_source + p1_photo, p2_source + p2_photo)\n    perc_diff = get_pairdiff(p1_source + p1_photo, p2_source + p2_photo)\n    facenet_euclid = get_facenet_diff(p1_source + p1_photo, p2_source + p2_photo)\n    \n    return {\n        'p1_age': p1_age,\n        'p2_age': p2_age,\n        'p1_size': p1_size,\n        'p2_size': p2_size,\n        'p1_gender': p1_gender[0],\n        'p2_gender': p2_gender[0],\n        'perc_diff': perc_diff,\n        'vgg_cosine': vgg_diff['vgg_cosine'],\n        'vgg_euclid': vgg_diff['vgg_euclid'],\n        'facenet_euclid': facenet_euclid\n    }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[12] TRIMMING TEST & TRAIN DATA FOR DEBUGGING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# debug_rows = 100\n\n# if debug_rows != 0:\n#     train_df = train_df.head(debug_rows)\n#     test_df = test_df.head(debug_rows)\n    \n# print(train_df.shape)\n# print(test_df.shape)\n\n# train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[13] FEATURE CREATION FOR TRAINING DATASET**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rows = train_df.shape[0]\n\nfor i in tqdm(range(0, train_rows)): \n\n    if isinstance(train_df.loc[i, 'p1_photo_list'], str):\n        p1_list = list(eval(train_df.loc[i, 'p1_photo_list']))\n    else:\n        p1_list = list(train_df.loc[i, 'p1_photo_list'])\n\n    if isinstance(train_df.loc[i, 'p2_photo_list'], str):\n        p2_list = list(eval(train_df.loc[i, 'p2_photo_list']))\n    else:\n        p2_list = list(train_df.loc[i, 'p2_photo_list'])\n\n    results = compare_individuals(p1_list, p2_list, train_df.loc[i, 'combo'])\n\n    train_df.loc[i, 'p1_age'] = results['p1_age']\n    train_df.loc[i, 'p2_age'] = results['p2_age']\n    train_df.loc[i, 'p1_size'] = results['p1_size']\n    train_df.loc[i, 'p2_size'] = results['p2_size']\n    train_df.loc[i, 'p1_gender'] = results['p1_gender']\n    train_df.loc[i, 'p2_gender'] = results['p2_gender']\n    train_df.loc[i, 'perc_diff'] = results['perc_diff']\n    train_df.loc[i, 'vgg_cosine'] = results['vgg_cosine']\n    train_df.loc[i, 'vgg_euclid'] = results['vgg_euclid']\n    train_df.loc[i, 'facenet_euclid'] = results['facenet_euclid']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[14] FEATURE CREATION FOR TEST DATASET**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['p1_age'] = 0\ntest_df['p2_age'] = 0\ntest_df['p1_size'] = 0\ntest_df['p2_size'] = 0\ntest_df['p1_gender'] = 0\ntest_df['p2_gender'] = 0\ntest_df['perc_diff'] = 0\ntest_df['vgg_cosine'] = 0\ntest_df['vgg_euclid'] = 0\ntest_df['facenet_euclid'] = 0\n\nfor i in tqdm(range(0, test_df.shape[0])):\n\n    combo = test_df.loc[i, 'img_pair'].split('-')\n    \n    results = compare_individuals([combo[0]], [combo[1]] , test_df.loc[i, 'img_pair'])\n    \n    test_df.loc[i, 'p1_age'] = results['p1_age']\n    test_df.loc[i, 'p2_age'] = results['p2_age']\n    test_df.loc[i, 'p1_size'] = results['p1_size']\n    test_df.loc[i, 'p2_size'] = results['p2_size']\n    test_df.loc[i, 'p1_gender'] = results['p1_gender']\n    test_df.loc[i, 'p2_gender'] = results['p2_gender']\n    test_df.loc[i, 'perc_diff'] = results['perc_diff']\n    test_df.loc[i, 'vgg_cosine'] = results['vgg_cosine']\n    test_df.loc[i, 'vgg_euclid'] = results['vgg_euclid']\n    test_df.loc[i, 'facenet_euclid'] = results['facenet_euclid']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[15] TESTING & TRAINING DATA FORMALIZATION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df = train_df.dropna()\nfinal_train_df = final_train_df.reset_index()\n\nfinal_test_df = test_df\n\nkeeper_columns = ['is_related', # target\n                  'p1_age',\n                  'p2_age',\n                  'p1_size',\n                  'p2_size',\n                  'p1_gender',           \n                  'p2_gender',\n                  'vgg_cosine',\n                  'vgg_euclid',\n                  'perc_diff',\n                  'facenet_euclid'\n                 ]\n\nfinal_train_df = final_train_df[keeper_columns]\n\nfinal_test_df = test_df[keeper_columns]\nfinal_test_df = final_test_df.drop('is_related', 1)\n\ntest_cols = ['p1_age',\n             'p2_age',\n             'p1_size',\n             'p2_size',\n             'p1_gender',\n             'p2_gender',\n             'vgg_cosine',\n             'vgg_euclid',\n             'perc_diff',\n             'facenet_euclid'\n            ]\n\nfinal_test_df[test_cols] = final_test_df[test_cols].astype(float)\n\nprint(final_train_df.shape)\nprint(final_train_df.dtypes)\n\nfinal_train_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_train_df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_df.tail(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[16] EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Avg Faces OR Delaunay Triangulations? \n# TODO: Cluster Families\n# TODO: Correlograms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[17] FINAL RECTANGULAR XGBOOST MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.datacamp.com/community/tutorials/xgboost-in-python\n\nprint(final_train_df.columns)\n\n# Separate target from features (is_related ~ *)\nX, y = final_train_df.iloc[:,1:], final_train_df.iloc[:,:1]  # because price is the target, the last column\n\n# print(X) # features\n# print(y) # target\n\ndata_dmatrix = xgb.DMatrix(data = X, label = y)\n\n# split test & train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42) # DON'T PANIC\n\nparams = { 'objective':'reg:linear',\n           'colsample_bytree': 0.3,\n           'n_estimators' : 10,\n           'learning_rate': 0.1, \n           'max_depth': 10, \n           'alpha': 10 }\n\n# Cross Validation\ncv_results = xgb.cv(dtrain = data_dmatrix, \n                    params = params, \n                    nfold = 3,\n                    num_boost_round = 50, \n                    early_stopping_rounds = 10,\n                    metrics = \"rmse\", \n                    as_pandas = True, \n                    seed = 42) # DON'T PANIC!\n\nprint(cv_results.head())\n\nxg_reg = xgb.XGBRegressor(params = params, \n                          dtrain = data_dmatrix, \n                          num_boost_round = 10)\n\nxg_reg.fit(X_train, y_train)\n\n# Predict On In-Sample Test Data (20% of trainig dataset)\npreds = xg_reg.predict(X_test)   # print(len(preds), ' predictions')\n\n# In-Sample Test RMSE\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"IN-SAMPLE TEST RMSE: %f\" % (rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_reg, num_trees=0)\nplt.rcParams['figure.figsize'] = [100, 100]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [100, 100]\nplt.title(\"Variable Importance\", fontsize=100)\nplt.xlabel(\"Importance\", fontsize=100)\nplt.ylabel(\"Variable\", fontsize=100)\nplt.tick_params(labelsize=100);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[18] PREDICTIONS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_predictions = xg_reg.predict(final_test_df)\n\nprint(len(submission_predictions), ' final predictions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**[19] SUBMISSION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")\nsub_df.is_related = submission_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index = False)\n\nprint(\"SUBMISSION COMPLETE!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}