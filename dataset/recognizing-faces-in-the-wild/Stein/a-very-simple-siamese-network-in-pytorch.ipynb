{"cells":[{"metadata":{},"cell_type":"markdown","source":"Glad to finish my first neural network! Thanks to all kindly people who shared their kernal and write guidance for beginners. This kernal is written in Pytorch, and ResNet50 of fastai models is used to build siamese network.\nSpecial thanks to [this article](https://www.kaggle.com/leonbora/kinship-recognition-transfer-learning-vggface). It is the first time for me to see data organized like this, and his kernal gives me an example to deal with them. I copied the data processing code from his kernal , as can be seen in the 4th code block.\nAlso thanks to [harveyslash](https://github.com/harveyslash/Facial-Similarity-with-Siamese-Networks-in-Pytorch/tree/master), his code is taught [here](https://www.pytorchtutorial.com/pytorch-one-shot-learning/) in Chinese. By reading his tutorial I have a thorough understanding about how to write pytorch code, and the basic use of dataloader and network. After finish this work the first time, I accidently found that most of his code is similar to the [official tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?highlight=cifar). Though I ran this tutorial a month ago, I didn't learn a lot from it.\nFinally, thanks to kaggle for offering this platform and 16G GPU for us. I really appreciate it. \nIf you are also beginner as I do, just get your hands dirty. Once finish a competition by yourself, you will learn a lot from it.\nBest wishes to you!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#some of blocks below are not used.\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Data visualisation\nimport matplotlib.pyplot as plt\n\n# Fastai\nfrom fastai.vision import *\nfrom fastai.vision.models import *\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.utils\nimport torchvision.datasets as dset\n\nfrom torch import optim\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision.models import *\nfrom torchvision.datasets import ImageFolder\nfrom torch.autograd import Variable\n#import pretrainedmodels\n\nfrom pathlib import Path\nimport sys\n\nfrom glob import glob\nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"np.random.seed(42)#To make sure that each time you run this kernal, you will get the same beginning parameters.\n\nBATCH_SIZE=64\nNUMBER_EPOCHS=100\nIMG_SIZE=100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def imshow(img,text=None,should_save=False):#for showing the data you loaded to dataloader\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(75, 8, text, style='italic',fontweight='bold',\n            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()    \n\ndef show_plot(iteration,loss):# for showing loss value changed with iter\n    plt.plot(iteration,loss)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#F09xx are used for validation.\nval_famillies = \"F09\"\n\n#An example of data:\"../input/train/F00002/MID1/P0001_face1.jpg\"\nall_images = glob(\"../input/train/*/*/*.jpg\")\n\ntrain_images = [x for x in all_images if val_famillies not in x]\nval_images = [x for x in all_images if val_famillies in x]\n\ntrain_person_to_images_map = defaultdict(list)#Put the link of each picture under the key word of a person such as \"F0002/MID1\"\nfor x in train_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nval_person_to_images_map = defaultdict(list)\nfor x in val_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\nppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\nrelationships = pd.read_csv(\"../input/train_relationships.csv\")\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))#For a List like[p1 p2], zip can return a result like [(p1[0],p2[0]),(p1[1],p2[1]),...]\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]#filter unused relationships\n\ntrain = [x for x in relationships if val_famillies not in x[0]]\nval = [x for x in relationships if val_famillies in x[0]]\n\nprint(\"Total train pairs:\", len(train))    \nprint(\"Total val pairs:\", len(val))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class trainingDataset(Dataset):#Get two images and whether they are related.\n    \n    def __init__(self,imageFolderDataset, relationships, transform=None):\n        self.imageFolderDataset = imageFolderDataset    \n        self.relationships = relationships #choose either train or val dataset to use\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        img0_info = self.relationships[index][0]#for each relationship in train_relationships.csv, the first img comes from first row, and the second is either specially choosed related person or randomly choosed non-related person\n        img0_path = glob(\"../input/train/\"+img0_info+\"/*.jpg\")\n        img0_path = random.choice(img0_path)\n        \n        cand_relationships = [x for x in self.relationships if x[0]==img0_info or x[1]==img0_info]#found all candidates related to person in img0\n        if cand_relationships==[]:#in case no relationship is mensioned. But it is useless here because I choose the first person line by line.\n            should_get_same_class = 0\n        else:\n            should_get_same_class = random.randint(0,1) \n\n        if should_get_same_class==1:#1 means related, and 0 means non-related.\n            img1_info = random.choice(cand_relationships)#choose the second person from related relationships\n            if img1_info[0]!=img0_info:\n                img1_info=img1_info[0]\n            else:\n                img1_info=img1_info[1]\n            img1_path = glob(\"../input/train/\"+img1_info+\"/*.jpg\")#randomly choose a img of this person\n            img1_path = random.choice(img1_path)\n        else:#0 means non-related\n            randChoose = True#in case the chosen person is related to first person\n            while randChoose:\n                img1_path = random.choice(self.imageFolderDataset.imgs)[0]\n                img1_info = img1_path.split(\"/\")[-3] + \"/\" + img1_path.split(\"/\")[-2]\n                randChoose = False\n                for x in cand_relationships:#if so, randomly choose another person\n                    if x[0]==img1_info or x[1]==img1_info:\n                        randChoose = True\n                        break\n                    \n        img0 = Image.open(img0_path)\n        img1 = Image.open(img1_path)\n        \n        if self.transform is not None:#I think the transform is essential if you want to use GPU, because you have to trans data to tensor first.\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        \n        return img0, img1 , should_get_same_class #the returned data from dataloader is img=[batch_size,channels,width,length], should_get_same_class=[batch_size,label]\n    \n    def __len__(self):\n        return len(self.relationships)#essential for choose the num of data in one epoch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_dataset = dset.ImageFolder(root='../input/train')\n\ntrainset = trainingDataset(imageFolderDataset=folder_dataset,\n                                        relationships=train,\n                                        transform=transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),\n                                                                      transforms.ToTensor()\n                                                                      ]))\ntrainloader = DataLoader(trainset,\n                        shuffle=True,#whether randomly shuffle data in each epoch, but cannot let data in one batch in order.\n                        num_workers=8,\n                        batch_size=BATCH_SIZE)\nvalset = trainingDataset(imageFolderDataset=folder_dataset,\n                                        relationships=val,\n                                        transform=transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),\n                                                                      transforms.ToTensor()\n                                                                      ]))\nvalloader = DataLoader(valset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=BATCH_SIZE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#only for visualize data in dataloader, it won't matters if you delete this block.\nvis_dataloader = DataLoader(trainset,\n                        shuffle=True,\n                        num_workers=8,\n                        batch_size=8)\ndataiter = iter(vis_dataloader)\n\n\nexample_batch = next(dataiter)\nconcatenated = torch.cat((example_batch[0],example_batch[1]),0)\nimshow(torchvision.utils.make_grid(concatenated))\nprint(example_batch[2].numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SiameseNetwork(nn.Module):# A simple implementation of siamese network, ResNet50 is used, and then connected by three fc layer.\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        #self.cnn1 = models.resnet50(pretrained=True)#resnet50 doesn't work, might because pretrained model recognize all faces as the same.\n        self.cnn1 = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(3, 64, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(p=.2),\n            \n            nn.ReflectionPad2d(1),\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.Dropout2d(p=.2),\n\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(64, 32, kernel_size=3),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            nn.Dropout2d(p=.2),\n        )\n        self.fc1 = nn.Linear(2*32*100*100, 500)\n        #self.fc1 = nn.Linear(2*1000, 500)\n        self.fc2 = nn.Linear(500, 500)\n        self.fc3 = nn.Linear(500, 2)\n\n\n    def forward(self, input1, input2):#did not know how to let two resnet share the same param.\n        output1 = self.cnn1(input1)\n        output1 = output1.view(output1.size()[0], -1)#make it suitable for fc layer.\n        output2 = self.cnn1(input2)\n        output2 = output2.view(output2.size()[0], -1)\n        \n        output = torch.cat((output1, output2),1)\n        output = F.relu(self.fc1(output))\n        output = F.relu(self.fc2(output))\n        output = self.fc3(output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = SiameseNetwork().cuda()\ncriterion = nn.CrossEntropyLoss() # use a Classification Cross-Entropy loss\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\ncounter = []\nloss_history = [] \niteration_number= 0\n\nfor epoch in range(0,NUMBER_EPOCHS):\n    print(\"Epoch：\", epoch, \" start.\")\n    for i, data in enumerate(trainloader,0):\n        img0, img1 , labels = data #img=tensor[batch_size,channels,width,length], label=tensor[batch_size,label]\n        img0, img1 , labels = img0.cuda(), img1.cuda() , labels.cuda()#move to GPU\n        #print(\"epoch：\", epoch, \"No.\" , i, \"th inputs\", img0.data.size(), \"labels\", labels.data.size())\n        optimizer.zero_grad()#clear the calculated grad in previous batch\n        outputs = net(img0,img1)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        if i %10 == 0 :#show changes of loss value after each 10 batches\n            #print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,loss.item()))\n            iteration_number +=10\n            counter.append(iteration_number)\n            loss_history.append(loss.item())\n    \n    #test the network after finish each epoch, to have a brief training result.\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():#essential for testing!!!!\n        for data in valloader:\n            img0, img1 , labels = data\n            img0, img1 , labels = img0.cuda(), img1.cuda() , labels.cuda()\n            outputs = net(img0,img1)\n            _, predicted = torch.max(outputs.data, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n            \n    print('Accuracy of the network on the', total_val, 'val pairs in',val_famillies, ': %d %%' % (100 * correct_val / total_val))\n    show_plot(counter,loss_history)\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class testDataset(Dataset): #different from train dataset, because the data organized in submission.csv is different from train.csv\n    \n    def __init__(self,transform=None):\n        self.test_df = pd.read_csv('../input/sample_submission.csv')#pandas用来读取csv文件\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        #data in submission.csv:\n        #       img_pair               is_related\n        #face05508.jpg-face01210.jpg       0\n        #face05820.jpg-face03938.jpg       0\n        \n        img0_path = self.test_df.iloc[index].img_pair.split(\"-\")[0]\n        img1_path = self.test_df.iloc[index].img_pair.split(\"-\")[1]\n        #print(img0_path,'-',img1_path) #reserved to check whether test data is in order.\n        \n        img0 = Image.open('../input/test/'+img0_path)\n        img1 = Image.open('../input/test/'+img1_path)\n\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n        \n        return img0, img1\n    \n    def __len__(self):\n        return len(self.test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testset = testDataset(transform=transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)),\n                                                                      transforms.ToTensor()\n                                                                      ]))\ntestloader = DataLoader(testset,\n                        shuffle=False,\n                        num_workers=0,\n                        batch_size=1)#Both extra workers and batch size lead to data out of order, the submission.csv will be out of order\n#if you have better method, please tell me! thanks a lot!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/sample_submission.csv')#pandas用来读取csv文件\npredictions=[]\nwith torch.no_grad():\n    for data in testloader:\n        img0, img1 = data\n        img0, img1 = img0.cuda(), img1.cuda()\n        outputs = net(img0,img1)\n        _, predicted = torch.max(outputs, 1)\n        predictions = np.concatenate((predictions,predicted.cpu().numpy()),0)#taking care of here, the output data format is important for transfer\n        \ntest_df['is_related'] = predictions\ntest_df.to_csv(\"submission.csv\", index=False)#submission.csv should be placed directly in current fold.\ntest_df.head(50)#show the result to be committed\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}