{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following notebook uses OpenFace as a feature extractor and then uses bcnn as way to merge these features and make a final sigmoid layer for classifying the kinship between two pair of images.\n\nMost of the code has been derived from the following resources:\n * https://github.com/krasserm/face-recognition : This repository has been inspired by openface which in turn has been based on facenet[https://cmusatyalab.github.io/openface/]\n * https://github.com/tkhs3/BCNN_keras : Following the feature extraction face I have placed a BCNN(http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf) head\n * The basic image generators and train val distribution has been taken from https://www.kaggle.com/suicaokhoailang/facenet-baseline-in-keras-0-749-lb\n \n For merging the feature extractor and the BCNN head, I have simply popped off some last layers from the OpenFace model and attached the BCNN head to it"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\nfrom keras.layers.core import Lambda, Flatten, Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D, AveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.preprocessing import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nfrom tqdm import tqdm\nimport glob\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\n\n# from keras.applications.nasnet import preprocess_input, decode_predictions\n# from keras.applications.densenet import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport sys\n\nimport cv2\nfrom random import choice, sample\nimport datetime\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport threading\n\nfrom keras.initializers import glorot_normal\n\nimport tensorflow as tf\nimport numpy as np\nimport os\n\nfrom numpy import genfromtxt\nfrom keras.layers import Conv2D, ZeroPadding2D, Activation\nfrom keras.layers.normalization import BatchNormalization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef outer_product(x):\n    \"\"\"\n    calculate outer-products of 2 tensors\n\n        args \n            x\n                list of 2 tensors\n                , assuming each of which has shape = (size_minibatch, total_pixels, size_filter)\n    \"\"\"\n    return keras.backend.batch_dot(\n                x[0]\n                , x[1]\n                , axes=[1,1]\n            ) / x[0].get_shape().as_list()[1] \n\ndef signed_sqrt(x):\n    \"\"\"\n    calculate element-wise signed square root\n\n        args\n            x\n                a tensor\n    \"\"\"\n    return keras.backend.sign(x) * keras.backend.sqrt(keras.backend.abs(x) + 1e-9)\n\ndef L2_norm(x, axis=-1):\n    \"\"\"\n    calculate L2-norm\n\n        args \n            x\n                a tensor\n    \"\"\"\n    return keras.backend.l2_normalize(x, axis=axis)\n\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\n\n        \ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\ndef create_base_network(input_shape):\n    new_mod = Model(mod.input,mod.layers[-5].output)\n    return new_mod\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() < 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndef LRN2D(x):\n    return tf.nn.lrn(x, alpha=1e-4, beta=0.75)\n\ndef conv2d_bn(\n  x,\n  layer=None,\n  cv1_out=None,\n  cv1_filter=(1, 1),\n  cv1_strides=(1, 1),\n  cv2_out=None,\n  cv2_filter=(3, 3),\n  cv2_strides=(1, 1),\n  padding=None,\n):\n    num = '' if cv2_out == None else '1'\n    tensor = Conv2D(cv1_out, cv1_filter, strides=cv1_strides, name=layer+'_conv'+num)(x)\n    tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+num)(tensor)\n    tensor = Activation('relu')(tensor)\n    if padding == None:\n        return tensor\n    tensor = ZeroPadding2D(padding=padding)(tensor)\n    if cv2_out == None:\n        return tensor\n    tensor = Conv2D(cv2_out, cv2_filter, strides=cv2_strides, name=layer+'_conv'+'2')(tensor)\n    tensor = BatchNormalization(axis=3, epsilon=0.00001, name=layer+'_bn'+'2')(tensor)\n    tensor = Activation('relu')(tensor)\n    return tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nROOT = \"../input/recognizing-faces-in-the-wild/\"\n\n\ndef read_img(im_path):\n    im1 = image.load_img(im_path, target_size=(96, 96))\n    im1 = image.img_to_array(im1)\n    im1 = np.expand_dims(im1, axis=0)\n    im1 = preprocess_input(im1,mode='tf')\n    return im1[0]\n\nfrom collections import defaultdict\n#keeps all photos path in a dictionary\nallPhotos = defaultdict(list)\nfor family in glob.glob(ROOT+\"train/*\"):\n    for mem in glob.glob(family+'/*'):\n        for photo in glob.glob(mem+'/*'):\n            allPhotos[mem].append(photo)\n\n#list of all members with valid photo\nppl = list(allPhotos.keys())\nlen(ppl)\n\ndata = pd.read_csv(ROOT+'train_relationships.csv')\ndata.p1 = data.p1.apply( lambda x: ROOT+'train/'+x )\ndata.p2 = data.p2.apply( lambda x: ROOT+'train/'+x )\nprint(data.shape)\ndata.head()\n\ndata = data[ ( (data.p1.isin(ppl)) & (data.p2.isin(ppl)) ) ]\ndata = [ ( x[0], x[1]  ) for x in data.values ]\nlen(data)\n\ntrain = [ x for x in data if 'F09' not in x[0]  ]\nval = [ x for x in data if 'F09' in x[0]  ]\nlen(train), len(val)\n\ndef getImages(p1,p2):\n    p1 = read_img(choice(allPhotos[p1]))\n    p2 = read_img(choice(allPhotos[p2]))\n    return p1,p2\n\ndef getMiniBatch(batch_size=16, data=train):\n    p1 = []; p2 = []; Y = []\n    batch = sample(data, batch_size//2)\n    for x in batch:\n        _p1, _p2 = getImages(*x)\n        p1.append(_p1);p2.append(_p2);Y.append(1)\n    while len(Y) < batch_size:\n        _p1,_p2 = tuple(np.random.choice(ppl,size=2, replace=False))\n        if (_p1,_p2) not in train+val and (_p2,_p1) not in train+val:\n            _p1,_p2 = getImages(_p1,_p2)\n            p1.append(_p1);p2.append(_p2);Y.append(0) \n    return [np.array(p1),np.array(p2)], np.array(Y)\n\ndef create_model():\n    myInput = Input(shape=(96, 96, 3))\n\n    x = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\n    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\n    x = Activation('relu')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D(pool_size=3, strides=2)(x)\n    x = Lambda(LRN2D, name='lrn_1')(x)\n    x = Conv2D(64, (1, 1), name='conv2')(x)\n    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\n    x = Activation('relu')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = Conv2D(192, (3, 3), name='conv3')(x)\n    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\n    x = Activation('relu')(x)\n    x = Lambda(LRN2D, name='lrn_2')(x)\n    x = ZeroPadding2D(padding=(1, 1))(x)\n    x = MaxPooling2D(pool_size=3, strides=2)(x)\n\n    # Inception3a\n    inception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\n    inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\n    inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n    inception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n    inception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\n    inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\n    inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n\n    inception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\n    inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\n    inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n    inception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n    inception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\n    inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\n    inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n\n    inception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n    inception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\n    inception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\n    inception_3a_pool = Activation('relu')(inception_3a_pool)\n    inception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n\n    inception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\n    inception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\n    inception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n\n    inception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n\n    # Inception3b\n    inception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\n    inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\n    inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n    inception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n    inception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\n    inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\n    inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n\n    inception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\n    inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\n    inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n    inception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n    inception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\n    inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\n    inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n\n    inception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3a)\n    inception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\n    inception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\n    inception_3b_pool = Activation('relu')(inception_3b_pool)\n    inception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n\n    inception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\n    inception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\n    inception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n\n    inception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n\n    # Inception3c\n    inception_3c_3x3 = conv2d_bn(inception_3b,\n                                       layer='inception_3c_3x3',\n                                       cv1_out=128,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=256,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(2, 2),\n                                       padding=(1, 1))\n\n    inception_3c_5x5 = conv2d_bn(inception_3b,\n                                       layer='inception_3c_5x5',\n                                       cv1_out=32,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=64,\n                                       cv2_filter=(5, 5),\n                                       cv2_strides=(2, 2),\n                                       padding=(2, 2))\n\n    inception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n    inception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n\n    inception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n\n    #inception 4a\n    inception_4a_3x3 = conv2d_bn(inception_3c,\n                                       layer='inception_4a_3x3',\n                                       cv1_out=96,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=192,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(1, 1),\n                                       padding=(1, 1))\n    inception_4a_5x5 = conv2d_bn(inception_3c,\n                                       layer='inception_4a_5x5',\n                                       cv1_out=32,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=64,\n                                       cv2_filter=(5, 5),\n                                       cv2_strides=(1, 1),\n                                       padding=(2, 2))\n\n    inception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3c)\n    inception_4a_pool = conv2d_bn(inception_4a_pool,\n                                        layer='inception_4a_pool',\n                                        cv1_out=128,\n                                        cv1_filter=(1, 1),\n                                        padding=(2, 2))\n    inception_4a_1x1 = conv2d_bn(inception_3c,\n                                       layer='inception_4a_1x1',\n                                       cv1_out=256,\n                                       cv1_filter=(1, 1))\n    inception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n\n    #inception4e\n    inception_4e_3x3 = conv2d_bn(inception_4a,\n                                       layer='inception_4e_3x3',\n                                       cv1_out=160,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=256,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(2, 2),\n                                       padding=(1, 1))\n    inception_4e_5x5 = conv2d_bn(inception_4a,\n                                       layer='inception_4e_5x5',\n                                       cv1_out=64,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=128,\n                                       cv2_filter=(5, 5),\n                                       cv2_strides=(2, 2),\n                                       padding=(2, 2))\n    inception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n    inception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n\n    inception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n\n    #inception5a\n    inception_5a_3x3 = conv2d_bn(inception_4e,\n                                       layer='inception_5a_3x3',\n                                       cv1_out=96,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=384,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(1, 1),\n                                       padding=(1, 1))\n\n    inception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4e)\n    inception_5a_pool = conv2d_bn(inception_5a_pool,\n                                        layer='inception_5a_pool',\n                                        cv1_out=96,\n                                        cv1_filter=(1, 1),\n                                        padding=(1, 1))\n    inception_5a_1x1 = conv2d_bn(inception_4e,\n                                       layer='inception_5a_1x1',\n                                       cv1_out=256,\n                                       cv1_filter=(1, 1))\n\n    inception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n\n    #inception_5b\n    inception_5b_3x3 = conv2d_bn(inception_5a,\n                                       layer='inception_5b_3x3',\n                                       cv1_out=96,\n                                       cv1_filter=(1, 1),\n                                       cv2_out=384,\n                                       cv2_filter=(3, 3),\n                                       cv2_strides=(1, 1),\n                                       padding=(1, 1))\n    inception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n    inception_5b_pool = conv2d_bn(inception_5b_pool,\n                                        layer='inception_5b_pool',\n                                        cv1_out=96,\n                                        cv1_filter=(1, 1))\n    inception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n\n    inception_5b_1x1 = conv2d_bn(inception_5a,\n                                       layer='inception_5b_1x1',\n                                       cv1_out=256,\n                                       cv1_filter=(1, 1))\n    inception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n\n    av_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n    reshape_layer = Flatten()(av_pool)\n    dense_layer = Dense(128, name='dense_layer')(reshape_layer)\n    norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n\n    return Model(inputs=[myInput], outputs=norm_layer)\n\n\n\nmod = create_model()\nmod.load_weights('../input/kin-detection/nn4.small2.v1.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ModelCheckpoint('model_best_checkpoint.h5', save_best_only=True,\n                                    save_weights_only=True, monitor='val_auc', mode='max', verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n                           patience=8,\n                           verbose=1,\n                           min_delta=1e-4),\n             keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                               factor=0.1,\n                               patience=4,\n                               verbose=1,\n                               epsilon=1e-4),\n            keras.callbacks.ModelCheckpoint('model_best_checkpoint.h5',save_best_only=True,\n                                            verbose=0, save_weights_only=True,monitor='val_auc',mode='max'),\n        ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ninput_shape = (96,96,3)\n\ninput_a = Input(shape=input_shape)\ninput_b = Input(shape=input_shape)\n\nbase_network = create_base_network(input_shape)\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\n\n# extract features from detector\nx_detector = processed_a\nshape_detector = processed_a.shape\n\n# extract features from extractor , same with detector for symmetry DxD model\nshape_extractor = processed_b.shape\nx_extractor = processed_b\n\n# rehape to (minibatch_size, total_pixels, filter_size)\nx_detector = keras.layers.Reshape([shape_detector[1] * shape_detector[2] , shape_detector[-1]])(x_detector)\nx_extractor = keras.layers.Reshape([shape_extractor[1] * shape_extractor[2] , shape_extractor[-1]])(x_extractor)\n\n# outer products of features, output shape=(minibatch_size, filter_size_detector*filter_size_extractor)\nx = keras.layers.Lambda(outer_product)([x_detector, x_extractor])\n\n# rehape to (minibatch_size, filter_size_detector*filter_size_extractor)\nx = keras.layers.Reshape([shape_detector[-1]*shape_extractor[-1]])(x)\n# signed square-root \n\nx = keras.layers.Lambda(signed_sqrt)(x)\n# L2 normalization\nx = keras.layers.Lambda(L2_norm)(x)\n\n### \n### attach FC-Layer\n###\n\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.01)(x)\n\nout = Dense(units=1,kernel_regularizer=keras.regularizers.l2(1e-8),kernel_initializer='glorot_normal',activation=\"sigmoid\")(x)\n\n\n\nmodel = Model([input_a, input_b], out)\n\nmodel.compile(loss=\"binary_crossentropy\", metrics=['acc',auc], optimizer=Adam(0.001))\n\n\n# model.load_weights(\"/home/datumx/data_science_experiments/detect_kinship/logs/kin_relation_2019_05_28_21_53_51966472/siamese_kins_detection_13-val_loss_0.4720-val_acc_0.7680.h5\")\n# train\n\n\nprint(model.summary())\n# train_batches =batch_generator(data_train,batch_size=8)\n# valid_batches =batch_generator(data_valid,batch_size=8)\n\ndef Generator(batch_size, data ):\n    while True:\n        yield getMiniBatch(batch_size=batch_size, data=data)\n\ntrain_gen = Generator(batch_size=16,data=train)\nval_gen = Generator(batch_size=16,data=val)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.fit_generator(train_gen,steps_per_epoch=200,use_multiprocessing=True,\n          epochs=100,validation_data=val_gen,validation_steps=100,callbacks=callbacks)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = \"../input/recognizing-faces-in-the-wild/test/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model_best_checkpoint.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\nfrom tqdm import tqdm\n\nsubmission = pd.read_csv('../input/recognizing-faces-in-the-wild/sample_submission.csv')\n\npredictions = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor batch in tqdm(chunker(submission.img_pair.values)):\n    X1 = [x.split(\"-\")[0] for x in batch]\n    X1 = np.array([read_img(test_path + x) for x in X1])\n\n    X2 = [x.split(\"-\")[1] for x in batch]\n    X2 = np.array([read_img(test_path + x) for x in X2])\n\n    pred = model.predict([X1, X2]).ravel().tolist()\n    predictions += pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['is_related'] = predictions\n\nsubmission.to_csv(\"sub_detect_kinship.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}