{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom keras.preprocessing import image\nimport cv2\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-08T15:49:26.376739Z","iopub.execute_input":"2021-06-08T15:49:26.37699Z","iopub.status.idle":"2021-06-08T15:49:28.473197Z","shell.execute_reply.started":"2021-06-08T15:49:26.376942Z","shell.execute_reply":"2021-06-08T15:49:28.472404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" !cp -r ../input/kerasvggface/* ./","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-06-08T15:49:28.476922Z","iopub.execute_input":"2021-06-08T15:49:28.477175Z","iopub.status.idle":"2021-06-08T15:49:29.306612Z","shell.execute_reply.started":"2021-06-08T15:49:28.477119Z","shell.execute_reply":"2021-06-08T15:49:29.305617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.chdir(\"../input/kerasvggface/keras-vggface-master\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:29.308363Z","iopub.execute_input":"2021-06-08T15:49:29.308671Z","iopub.status.idle":"2021-06-08T15:49:29.313595Z","shell.execute_reply.started":"2021-06-08T15:49:29.308621Z","shell.execute_reply":"2021-06-08T15:49:29.312649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:29.314938Z","iopub.execute_input":"2021-06-08T15:49:29.315437Z","iopub.status.idle":"2021-06-08T15:49:29.335205Z","shell.execute_reply.started":"2021-06-08T15:49:29.315346Z","shell.execute_reply":"2021-06-08T15:49:29.334615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file_path = \"/kaggle/input/rfiwsmile/recognizing-faces-in-the-wild/train_relationships.csv\"\ntrain_folders_path = \"/kaggle/input/rfiwsmile/recognizing-faces-in-the-wild/train\"","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:29.338678Z","iopub.execute_input":"2021-06-08T15:49:29.339092Z","iopub.status.idle":"2021-06-08T15:49:29.342719Z","shell.execute_reply.started":"2021-06-08T15:49:29.338927Z","shell.execute_reply":"2021-06-08T15:49:29.341786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_famillies_list = [\"F07\", \"F08\", \"F09\"]\n# val_famillies_list = [\"F09\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:29.344268Z","iopub.execute_input":"2021-06-08T15:49:29.344787Z","iopub.status.idle":"2021-06-08T15:49:29.35291Z","shell.execute_reply.started":"2021-06-08T15:49:29.344736Z","shell.execute_reply":"2021-06-08T15:49:29.352084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nall_images = glob(train_folders_path + \"*/*/*/*\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T15:49:29.356154Z","iopub.execute_input":"2021-06-08T15:49:29.356392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_train_val(family_name):\n    # Get val_person_image_map\n    val_famillies = family_name\n    train_images = [x for x in all_images if val_famillies not in x] # train_img if val_fam not given\n    val_images = [x for x in all_images if val_famillies in x] # val_img if val_fam given\n\n    train_person_to_images_map = defaultdict(list) # initialises a default dict which will not give key error\n\n    ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images] # EXTRACT NAME VALUES\n\n    for x in train_images:\n        train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x) \n\n    val_person_to_images_map = defaultdict(list)\n\n    for x in val_images:\n        val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n        \n    # Get the train and val dataset\n    relationships = pd.read_csv(train_file_path)\n    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\n    train = [x for x in relationships if val_famillies not in x[0]]\n    val = [x for x in relationships if val_famillies in x[0]]\n    \n    return train, val, train_person_to_images_map, val_person_to_images_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\nimport tensorflow as tf\nfrom keras import backend as K\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\n\ndef baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = False\n    for x in base_model.layers[-3:]:\n        x.trainable=True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n    \n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4 = Subtract()([x1_, x2_])\n    x = Concatenate(axis=-1)([x4, x3])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n    \n    # loss=\"binary_crossentropy\"\n    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], \n                  metrics=['acc'], \n                  optimizer=Adam(0.00003))\n\n    model.summary()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = baseline_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Train","metadata":{}},{"cell_type":"code","source":"n_val_famillies_list = len(val_famillies_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_val_famillies_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in tqdm_notebook(range(n_val_famillies_list)):\n    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n    file_path = f\"/kaggle/workingvgg_face_{i}.h5\"\n    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=5, verbose=1)\n    es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=15, verbose=1)\n    callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n    history = model.fit_generator(gen(train, train_person_to_images_map, batch_size=32), \n                                  use_multiprocessing=True,\n                                  validation_data=gen(val, val_person_to_images_map, batch_size=32), \n                                  epochs=100, verbose=1,\n                                  workers=4, callbacks=callbacks_list, \n                                  steps_per_epoch=400, \n                                  validation_steps=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Inference","metadata":{}},{"cell_type":"code","source":"test_path = \"../input/newtest/test/\"\n\nsubmission = pd.read_csv('../input/rfiwsmile/recognizing-faces-in-the-wild/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_for_sub = np.zeros(submission.shape[0])\nfor i in tqdm_notebook(range(n_val_famillies_list)):\n    file_path = f\"vgg_face_{i}.h5\"\n    model.load_weights(file_path)\n    # Get the predictions\n    predictions = []\n\n    for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n        X1 = [x.split(\"-\")[0] for x in batch]\n        X1 = np.array([read_img(test_path + x) for x in X1])\n\n        X2 = [x.split(\"-\")[1] for x in batch]\n        X2 = np.array([read_img(test_path + x) for x in X2])\n\n        pred = model.predict([X1, X2]).ravel().tolist()\n        predictions += pred\n    preds_for_sub += np.array(predictions) / n_val_famillies_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission['is_related'] = preds_for_sub\nsubmission.to_csv(\"vgg_face.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(submission)\n\ndf.to_csv('submission.csv', index=False)\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}