{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install opencv-python\n!pip install imutils\n!pip install dlib","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# An implimentation of Facial landmark Detection using the trained data from iBUG's 68 landmark model\n[Code Curtesy of Adrian Rosebrock's pyimage blog](https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/)\nThis model identifies 68 landmarks around the face; 17 defining the shape of the jaw, 10 for the brows (5 each), 4 along the bridge of the nose, 5 along the tip and base of the nose, 12 for eye shape (6 each), 12 for the outer lip outline, 8 for the inside lip outline\n![](https://www.pyimagesearch.com/wp-content/uploads/2017/04/facial_landmarks_68markup-768x619.jpg)\nThe results of the model are 68 (x,y) points for each given face, 136 data points overall"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import cv2,matplotlib.pyplot as plt,dlib,imutils\nfrom imutils import face_utils\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(\"../input/dlib-68/shape_predictor_68_face_landmarks.dat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image=plt.imread(\"../input/recognizing-faces-in-the-wild/train/F0002/MID1/P00009_face3.jpg\")\n# image = imutils.resize(image, width=500)\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nrects = detector(gray, 1)\n\nfor rect in rects:\n    pred=predictor(gray,rect)\n    fig, ax1 = plt.subplots()\n\n    ax1.imshow(image)\n    ax1.scatter(face_utils.shape_to_np(pred)[:,0],face_utils.shape_to_np(pred)[:,1])\n    \n# del predictor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Referential Association of relationships\n> ~~An assumption is made that for triplett loss, simply using the un-related family members is good enough to use as negative cases, if this shows to be a bad assumption, changes can be made~~\n\n> Revision: the model had troubles coping with the trasitive non-equivelance (Person A related to B, B related to C, but C not related to A) as the only things to train on;added 3 random people to each person as unrelated persons and the model now trains nicely"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random, itertools,glob\nclass Person:\n    def __init__(self,name,Family):\n        self.name=name\n        self.family=Family\n        self.related=set()\n        self.unrelated=set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relationlist=open(\"../input/recognizing-faces-in-the-wild/train_relationships.csv\").read().split(\"\\n\")[1:-1]\nFamilies={k.split(\"/\")[0]:{} for k in relationlist}\nfor each in relationlist:\n    p1=each.split(\",\")[0].split(\"/\")[1]\n    p2=each.split(\"/\")[2]\n    Fam=each.split(\"/\")[0]\n    Families[Fam].update({p1:Person(p1,Fam),p2:Person(p2,Fam)})\nFamilies\nfor Fam in Families:\n    Family=Families[Fam]\n    for Pers in Family:\n        Families[Fam][Pers].unrelated.update([k for k in set(Family.values()) if k.name!=Pers])\nfor relation in relationlist:\n    A,B=[Families[A.split(\"/\")[0]][A.split(\"/\")[1]] for A in relation.split(\",\")]\n    a,b=[r.split(\"/\") for r in relation.split(\",\")]\n    Families[a[0]][a[1]].unrelated=A.unrelated - set([B])\n    Families[a[0]][a[1]].related=A.related   | set([B])\n    Families[b[0]][b[1]].unrelated=B.unrelated - set([A])\n    Families[b[0]][b[1]].related=B.related   | set([A])\n    \nfor F in Families:\n    for P in Families[F]:\n        if len(Families[F][P].unrelated)==0:\n            # For those that are fully related to those in the family, randomly choose 3 other people to be unrelated to\n            Families[F][P].unrelated= Families[F][P].unrelated | set([k for k in\n                                                                           [random.choice(list(random.choice(list(Families.values())).values())) for s in range(len(Families[F][P].related)+3)]\n                                                                          if k not in Families[F][P].related])\n        if len(Families[F][P].related)==0:\n            #ensure there are not any marooned individuals that are not related to anyone\n            print(\"Related\",F,P)\n        \n        Families[F][P].unrelated= Families[F][P].unrelated | set([random.choice(list(random.choice(list(Families.values())).values())) for s in range(3)])\n# Families","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_parts=[]\n[[train_data_parts.append(Families[F][P]) for P in Families[F]] for F in Families]\ndel Families\ntrain_data_parts[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metaglob(lis):\n    ret=[]\n    [ret.extend(glob.glob(\"../input/recognizing-faces-in-the-wild/train/\"+A.family+\"/\"+A.name+\"/*.jpg\")) for A in lis]\n    return ret\n\npairs=[]\n[pairs.extend(itertools.product(*[metaglob([A]),\n                                  metaglob(A.related),\n                                  metaglob(A.unrelated)])) for A in train_data_parts]\ndel train_data_parts\nprint(\"Done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pairs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*OOFF* 65-75 Million photo pairs, probably excessive, lets go with some subset for now"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.generic_utils import Progbar\n\ndef SixtyEight(image,k):\n    k.add(1)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    rects = detector(gray, 1)\n    for rect in rects:\n        pred=predictor(gray,rects[0])\n        re=face_utils.shape_to_np(pred)\n        re=(re-re.min(0))/(re.max(0)-re.min(0))\n        return re\n    return None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class callable_dict:\n    def __init__(self,fun):\n        self.dict=dict()\n        self.function=fun\n    def __getitem__(self, key):\n        if key in self.dict.keys():\n            return self.dict[key]\n        else:\n            self.dict[key]=self.function(plt.imread(key),Progbar(target=1, verbose=0))\n            return self.dict[key]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finalpair=pairs[::300]\n\ntrain=[[] for i in range(3)]\ns=Progbar(target=len(finalpair))\n\nmyDict=callable_dict(SixtyEight)\n\nfor p in finalpair:\n    s.add(1)\n    one,two,three=[myDict[x] for x in p]\n    if False not in [type(k)==np.ndarray for k in [one,two,three]]:\n        train[0].append(one)\n        train[1].append(two)\n        train[2].append(three)\n\ntrain=[np.array(k) for k in train]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[1].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Triplet loss biometric loss function\n[Curtousey of Towards DataScience](https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24)"},{"metadata":{},"cell_type":"markdown","source":">![Demonstration](https://cdn-images-1.medium.com/max/800/0*_WNBFcRVEOz6QM7R.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ndef triplet_loss(y_true, y_pred, alpha = 400,N=5):\n    \"\"\"\n    Implementation of the triplet loss function\n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor data\n            positive -- the encodings for the positive data (similar to anchor)\n            negative -- the encodings for the negative data (different from anchor)\n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n#     print((y_pred[0]))\n    N=y_pred.shape[1]//3\n    anchor = y_pred[:,0:N]\n    positive = y_pred[:,N:N*2]\n    negative = y_pred[:,N*2:N*3]\n\n    # distance between the anchor and the positive\n    pos_dist = K.sqrt(K.sum(K.square(anchor-positive),axis=1)+.01)\n\n    # distance between the anchor and the negative\n    neg_dist = K.sqrt(K.sum(K.square(anchor-negative),axis=1)+.01)\n\n    # compute loss\n    basic_loss = (pos_dist-neg_dist+alpha)\n    loss = K.maximum(basic_loss,0.0)\n \n    return loss\n\ndef Neg_Dist(y_true, y_pred):\n    \"\"\"\n    Implementation of the triplet loss function\n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor data\n            positive -- the encodings for the positive data (similar to anchor)\n            negative -- the encodings for the negative data (different from anchor)\n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n#     print((y_pred[0]))\n    N=y_pred.shape[1]//3\n    anchor = y_pred[:,0:N]\n    negative = y_pred[:,N*2:N*3]\n\n\n\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n\n \n    return neg_dist\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model, Input\nfrom keras.layers import Dense, Dropout, Activation, Flatten, concatenate, Conv2D\nfrom keras.optimizers import Adagrad, Adam\nfrom keras.metrics import K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pear=set(myDict.dict.keys())\n\ndef setup(pairs,pear):\n    ret=[]\n    prog=Progbar(2001)\n    while len(ret)<=2000 and pairs!=[]:\n        X=pairs.pop()\n        if len(set(X)-pear)==3:\n            prog.add(1)\n            ret.append(X)\n    return ret\n            \npear=setup(pairs,pear)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testset=[[] for i in range(3)]\ns=Progbar(target=len(pear))\n\n\nfor X in pear:\n    s.add(1)\n    one,two,three=[myDict[x] for x in X]\n    if False not in [type(k)==np.ndarray for k in [one,two,three]]:\n        testset[0].append(one)\n        testset[1].append(two)\n        testset[2].append(three)\n\nholdout=[np.array(k) for k in testset]\n\ndel testset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_mod(inpu,outpu):\n    model= Sequential()\n    model.add(Dense(256, input_shape=(68,2)))\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(rate=0.01))\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n#     model.add(Dropout(rate=0.1))\n\n    model.add(Dense(outpu))\n    model.add(Activation(\"relu\"))\n    return model\n\nwith tf.device('/device:GPU:1'):\n    anchor_in,pos_in,neg_in = Input(shape=(68,2)),Input(shape=(68,2)),Input(shape=(68,2))\n\n    mod=create_mod(224,100)\n\n    anchor_out=mod(anchor_in)\n    pos_out=mod(pos_in)\n    neg_out=mod(neg_in)\n\n    merged= concatenate([anchor_out,pos_out,neg_out], axis=-1)\n\n    model=Model(inputs=[anchor_in,pos_in,neg_in],outputs=merged)\n\n    model.compile(loss=triplet_loss,optimizer=Adam())\n\nmodel.fit(train,np.zeros(train[0].shape[0]),batch_size=10,epochs=2,validation_data=(holdout,np.zeros(holdout[0].shape[0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre=model.predict([train[0],train[1],train[2]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    N=pre.shape[1]//3\n    anchor = pre[:,0:N]\n    positive = pre[:,N:N*2]\n    negative = pre[:,N*2:N*3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(np.square(anchor-positive).sum(1)).mean(),np.sqrt(np.square(anchor-negative).sum(1)).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p=pre.reshape((train[1].shape[0],3,N))\nHO=model.predict(holdout).reshape((holdout[1].shape[0],3,N))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linalg.norm(p[400,0]-p[0,0],2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p[0,0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=list(pd.read_csv(\"../input/recognizing-faces-in-the-wild/sample_submission.csv\")[\"img_pair\"])\ntests=set()\nfor k in test:\n    tests=tests | set(k.split(\"-\")) \nsorted(tests)\nk=Progbar(len(tests))\ncomp={fil:SixtyEight(plt.imread(\"../input/recognizing-faces-in-the-wild/test/\"+fil),k) for fil in tests}\n# comp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1=[]\ntest2=[]\nkeys=[]\nfor face in test:\n    one=comp[face.split(\"-\")[0]]\n    two=comp[face.split(\"-\")[1]]\n    if type(one) == np.ndarray and type(two) == np.ndarray:\n        test1.append(one)\n        test2.append(two)\n        keys.append(face)\ntest1=np.array(test1)\ntest2=np.array(test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"te=model.predict([test1,test1,test2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1.shape\no=te.reshape((test1.shape[0],3,N))\nnp.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in o]).mean(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kk=np.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in p])\nplt.hist(kk,bins=20)\nplt.legend([\"Related\",\"Unrelated\"])\nplt.title(\"Training Data Distributions\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kk=np.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in HO])\nplt.hist(kk,bins=20)\nplt.legend([\"Related\",\"Unrelated\"])\nplt.title(\"Holdout Data Distributions\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kk=np.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in o])[:,1]\nplt.hist(kk,bins=20,color=\"k\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ppd=pd.DataFrame({\"img_pair\":keys,\"is_related\":1-(kk-kk.min())/(kk.max()-kk.min())})\nppd=ppd.append(pd.DataFrame({\"img_pair\":list(set(test)-set(keys)),\"is_related\":[1-(kk.mean()-kk.min())/(kk.max()-kk.min()) for i in range(len(list(set(test)-set(keys))))]}))\nppd.to_csv(\"submission.csv\",index=False,header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}