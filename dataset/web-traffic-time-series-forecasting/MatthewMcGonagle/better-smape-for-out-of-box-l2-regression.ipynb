{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3","name":"python","version":"3.5.2","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThe purpose of this notebook is to show some simple transformations that allow out of box non-parametric L2 regressors to achieve better SMAPE scores. By default, these algorithms are not optimizing for a better SMAPE score. However, by changing the target variables by an invertible transformation, we can get these algorithms to get closer to optimizing SMAPE. However a transformation alone won't make the algorithm actually optimize on SMAPE. Furthermore, fitting on transformed target variables will change a parametric model (e.g. linear regression), but a non-parametric model (e.g. decision tree or neural network) will be okay as it has no strict structure.\n\nFor this notebook, we will be using a simple decision tree for our model.\n\n# Imports\n\nImports that we will need."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"import pandas as pd # Reading csv file\nimport numpy as np # Linear algebra\nfrom matplotlib import pyplot as plt # Graphing\nfrom sklearn.model_selection import train_test_split # We will do a simple train, validate, test split.\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SMAPE Investigation\n\nWe won't go into great detail here as there are great resources elsewhere, but here we will just take a look at the nature of SMAPE. First we note that SMAPE measures an average of |p - t| / (|p| + |t|) for scalars p and t. Here, we think of p as being a predicted value and t being a true value. Furthermore, this quantity depends only on the ratio of p and t (with exception being when either are zero). So, let p = r * t. The quantity becomes a function of the single variable r, |1 - r| / (1 + |r|). Let us graph this function for non-negative ratios r."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"def ratioSMAPE(r):\n    return np.abs(1 - r) / (1 + np.abs(r))\n\nrvals = np.arange(0, 10, 0.1)\nplt.plot(rvals, ratioSMAPE(rvals))\nplt.title('Graph of |1 - r| / (1 + |r|)')\nplt.show()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we see that when the true values are non-negative (as is the case in our data), we have the following:\n* SMAPE punishes underpredictions more than over predictions. \n* As the prediction gets closer closer to 0, the SMAPE error approaches 1. \n* For over predicitions, as the size of the prediction gets really large, the SMAPE error again approaches 1.\n\nWe use these properties to guide us towards finding some transformation functions z = f(y) that will let an L2 regression for target z values more closely emulate minimizing SMAPE.\n\n# Get the Data"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"all_df = pd.read_csv('../input/train_1.csv')\nall_df.shape","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this notebook, we wish to concentrate on just getting better regression results. So we will simply set every NaN value to 0."},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"# Set all NaN to 0.\n\nall_df.fillna(0, inplace = True)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dividing the data\n\nThe competition has us predicting values 64 days into the future. To measure the accuracy of our models, we will simply let the last 64 days of our training data be the target values Y to predict using the data from the previous days X. Also, this notebook is meant to be as simple as possible, so we will not look at extracting features from the Page data. So we will only be using the time series data."},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"# Separate into training data into features and targets.\n\nfutureT = 64\nX_all = all_df.drop('Page', axis = 1).values[:, :-futureT]\nY_all = all_df.drop('Page', axis = 1).values[:, -futureT:]","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For simplicity, we will split our data into a training set, a validation set, and a test set."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# First split into test and a combination of training and validation.\n\nX_trainvalid, X_test, Y_trainvalid, Y_test = train_test_split(X_all, Y_all, test_size = 0.33, random_state = 32)\n\n# Now split up the training and validation sets.\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_trainvalid, Y_trainvalid, test_size = 0.33, random_state = 35)\n\nprint('X_train.shape = ', X_train.shape, '\\tX_valid.shape = ', X_valid.shape, '\\tX_test.shape = ', X_test.shape)\nprint('Y_train.shape = ', Y_train.shape, '\\tY_valid.shape = ', Y_valid.shape, '\\tY_test.shape = ', Y_test.shape)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Set up SMAPE error function\n\nNow we define a SMAPE error function that we will use through out the rest of the notebook."},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"def smape(Y_predict, Y_test):\n    result = np.linalg.norm(Y_predict - Y_test, axis = 1)\n    result = np.abs(result)\n    denom = np.linalg.norm(Y_predict, axis = 1)\n    denom += np.linalg.norm(Y_test, axis = 1)\n    result /= denom\n    result *= 100 * 2\n    result = np.mean(result)\n    return result","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normal L2 Regression Benchmark\n\nHere we do a simple L2 Regression using a decision tree on the mean of sample over the entire time history of the time series."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"model = Pipeline([ ('means', FunctionTransformer(lambda X : X.mean(axis = 1, keepdims = True))),\n                   ('tree', DecisionTreeRegressor(max_depth = 10)) ])\nmodel.fit(X_trainvalid, Y_trainvalid)\nY_predict = model.predict(X_test)\ntest_smape = {} # We will be storing all of the results of our tests for comparison in a dictionary.\ntest_smape['benchmark'] = smape(Y_predict, Y_test)\nprint('SMAPE = ', test_smape['benchmark'])","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Z Transformation Regressions\n\nNow we use simple functions f(Y) to make new target variables Z = f(Y). Then we do regression using the transformed targets Z. To compute the SMAPE score, we invert to get the predicted Y values and then compute the SMAPE score.\n\nFor each function f(Y), we will actually search over a hyper-parameter to minimize the SMAPE score.\n\n## First Z Transformation\n\nTo motivate the first Z transformation, we consider the mean of the Y values, Y_mean. We plug it into the formula for each term in SMAPE, and we get an average of |y - Y_mean| / (|y| + Y_mean). Fix Y_mean to be a constant C and consider this to be a function of y, that is f(y) = |y - C| / (|y| + C). Now, we should have non-negative y so should be able to ignore the absolute values in the denominator. Let us also ignore the absolute value in the numerator (you can kind of justify this by the fact that the squared nature of L2 means we should have ignored the absolute value from the very beginning). So we are lead to z = f(y) = (y - C) / (y + C) = 1 - 2C / ( y + C ).\n\nNow, doing L2 regression for this z is the same as doing L2 regression for the slightly different f(y) = 1 / (y + C) (which we probably could have simply guessed from the very beginning). This has the nice property of making smaller values larger and therefore more punishing for differences in small values. Furthermore, f(y) goes to zero as y gets really large (the main point being it approaches a bounded value)."},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"# definition of ztransformation.\n\ndef ztransform1(Y, param):\n    return 1 / (param + Y)\n\n# inverse transformation, Y = inverseZ(Z)\n\ndef inverseZ1(Z, param):\n    return -param + 1 / Z","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us try searching for the optimal value of C to minimize the SMAPE score when doing L2 regression on Z. We will do a simple grid search.\n\nNow, sometimes our fit will give a predicted value of 0 while the true value is 0. Our SMAPE function is simple and so it will return a NaN value. Let's bypass this by adding a small value of epsilon to the predicted Y values that will not affect the SMAPE score that much."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# Values to try for param.\n\nparam_search = np.arange(20, 420, 20)\n\n# To record results of fits\n\nsmapes = []\nepsilon = 1e-6\n\nfor param in param_search:\n    Z_train = ztransform1(Y_train, param)\n    model.fit(X_train, Z_train)\n    Z_predict = model.predict(X_valid)\n    Y_predict = inverseZ1(Z_predict, param)\n    newsmape = smape(epsilon + Y_predict, Y_valid)\n    smapes.append(newsmape)\n    print('param = ', param, ' smape = ', newsmape, ',\\t', end = '')\n    \nplt.plot(param_search, smapes)\nplt.title('SMAPES vs param values for Regression on ztransform1')\nplt.show()\n    ","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's do a test for param = 240. We will train on all of the training and validation data."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"param = 240\nZ_trainvalid = ztransform1(Y_trainvalid, param)\nmodel.fit(X_trainvalid, Z_trainvalid)\nZ_predict = model.predict(X_test)\nY_predict = inverseZ1(Z_predict, param)\ntest_smape['z1'] = smape(epsilon + Y_predict, Y_test)\nprint('SMAPE = ', test_smape['z1'])","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Second Z Transformation\n\nWe justify this one by putting an upper bound on a SMAPE term |p - t| / (|p| + |t|). For simplicity, consider p and t non-negative, so we have |p - t| / (p + t). Now, one can write p - t = (sqrt(p) - sqrt(t)) \\* (sqrt(p) + sqrt(t)) and then use the Cauchy-Schwarz inequality on this to give the following upper bound on the original expression in p and t: sqrt(2) * ( sqrt(p) - sqrt(t) ) / sqrt( p + t ).\n\nSimilarly to before, we are led to f(y) = (sqrt(y) - np.sqrt(C)) / sqrt(y + C). After some algebra and playing with the possibilities of plus and minus, the inverse function is given by y = C \\* ( -1 + z \\* sqrt(2 - z\\*\\*2 ) )\\*\\*2 / (z\\*\\*2 - 1)\\*\\*2. Let's double check this using some graphs.\n\nNow, for non-negative y values, f(y) is between -1 and 1, so the inverse function is always defined. However, we could possibly predict a value of y that is outside the definition of the inverse function. To prevent this, we first force z to be between -1 + epsilon and 1 - epsilon."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"def ztransform2(Y, param):\n    return (np.sqrt(Y) - np.sqrt(param)) / np.sqrt(Y + param)\n\ndef inverseZ2(Z, param):\n    Z2 = np.minimum(Z, 1 - epsilon)\n    Z2 = np.maximum(Z2, -1 + epsilon)\n    result = -1 - Z2 * np.sqrt(2 - Z2**2)\n    result = result / (Z2**2 - 1)\n    result = param * result**2\n    return result\n\n# Plot ztransform2 for param = 2.0.\n\nplt.figure(figsize = (15, 5))\nplt.subplot(121)\ndomain = np.arange(0, 10, 0.1)\nplt.plot(domain, ztransform2(domain, 2.0))\nplt.title('ztransform2 for param = 2.0')\n\n# Graph inverseZ2 for param = 1.0 and reflection of graph of ztransform2.\n\nplt.subplot(122)\ndomain2 = np.arange(-0.7, 0.7, 0.1)\nplt.plot(domain2, inverseZ2(domain2, 2.0))\nplt.plot(ztransform2(domain, 2.0), domain, color = 'red')\nplt.title('Graph of inverseZ2 and reflection of graph of ztransform2')\nplt.legend(['inverseZ2','reflection ztransform2'])\nplt.show()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's search over param for the optimal value to minimize SMAPE for our model."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# Values to try for param.\n\nparam_search = np.arange(100, 2500, 100)\n\n# To record results of fits\n\nsmapes = []\nepsilon = 1e-6\n\nfor param in param_search:\n    Z_train = ztransform2(Y_train, param)\n    model.fit(X_train, Z_train)\n    Z_predict = model.predict(X_valid)\n    Y_predict = inverseZ2(Z_predict, param)\n    newsmape = smape(epsilon + Y_predict, Y_valid)\n    smapes.append(newsmape)\n    print('param = ', param, ' smape = ', newsmape, ',\\t', end = '')\n    \nplt.plot(param_search, smapes)\nplt.title('SMAPES vs param values for Regression on ztransform2')\nplt.show()\n    ","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's do a test with param = 1000. We will fit the model on both the training and validation data."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"param = 1000\nZ_trainvalid = ztransform2(Y_trainvalid, param)\nmodel.fit(X_trainvalid, Z_trainvalid)\nZ_predict = model.predict(X_test)\nY_predict = inverseZ2(Z_predict, param)\ntest_smape['z2'] = smape(epsilon + Y_predict, Y_test)\nprint('SMAPE = ', test_smape['z2'])","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Review of Test Results\n\nHere we give a review of the test results for our z transformations."},{"metadata":{},"execution_count":null,"cell_type":"code","source":"# Code to generate simple table using notebook output.\n\ndescription = ['None (Benchmark)', 'Z Transformation 1', 'Z Transformation 2']\nkeys = ['benchmark', 'z1', 'z2']\nscores = [test_smape[key] for key in keys]\noutput = [x for x in zip(description, scores)]\n\noutput","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that there is some improvement in the SMAPE scores when using the Z transformations."}],"nbformat_minor":1}