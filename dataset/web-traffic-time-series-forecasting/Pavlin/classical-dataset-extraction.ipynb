{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"98664f9fefe523ff31830f01d4dfa46dd2c7734c","_cell_guid":"91f6a07a-c345-4ae8-a256-32b17aabf018"},"source":"# Classical dataset extraction\n\nTime series data can't be directly input into classical machine learning algorithms, since the data points are highly dependent on previous values. A typical approach to solve this is to introduce lag variables.\n\nThis kernel demonstrates an approach how to convert time series data into a dataset appropriate for classical machine learning algorithms."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"47eacce0bd928cf3e810bfe85bd07fec9782c640","_cell_guid":"329cb90c-b4ca-4122-9c29-5a4a6a14e132","collapsed":true},"source":"import re\n\nimport numpy as np\nimport pandas as pd"},{"cell_type":"markdown","metadata":{"_uuid":"cc7d9dc2111ac662aabb16e5cb1de7b9369c757f","_cell_guid":"1b9192df-21a4-479c-810c-9a512da3ca9d"},"source":"Firstly, we need to load our data files. Since this is meant only for demonstration purposes, we'll only use a small fraction of the data to speed up the process and so it's easier to verify that it works properly.\n\nAs you may imagine, running this on the entire dataset of time series may take a while and use up quite a bit of memory."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"d08290d2df7b807238a5264b66b10c0f91cfa748","_cell_guid":"d48a8bc5-d0db-4845-98c9-7ea375a327ef"},"source":"data = pd.read_csv('../input/train_1.csv').iloc[:256]\n\ndata.head()"},{"cell_type":"markdown","metadata":{"_uuid":"682af557c40a9aa16fcb5e1f91159f7d1131be71","_cell_guid":"a4419145-4cf3-494b-8440-63fba1f35cc9"},"source":"We'll extract the date columns from the data, as they will come in useful later for indexing and extracting lag variables."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"6b7f2ffd5f0a68a12aceda71a48aab513572c241","_cell_guid":"08f28617-4dc9-410b-a28d-0ce6ae067f70"},"source":"date_columns = [c for c in data.columns if re.match(r'\\d{4}-\\d{2}-\\d{2}', c)]\n\nprint(date_columns[:5])\nprint(date_columns[-5:])"},{"cell_type":"markdown","metadata":{"_uuid":"961b42d225c6a506345e627fd8fd23c4f8a765aa","_cell_guid":"1d3f32a1-2dfd-4141-ac67-8c2c3a9eb71a"},"source":"We can specify how many lag variables we want."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"e48fdcaef0159d0edde8e2db130de908f7bb3064","_cell_guid":"84ad46b0-3c3c-41f9-91ac-9ff72be10ffb","collapsed":true},"source":"LAG_DAYS = 7"},{"cell_type":"markdown","metadata":{"_uuid":"401cf2b1ba354dd282e0a337d28ab935b212d4d0","_cell_guid":"f9546a8e-1dcd-4b0e-92f3-6a0d4cd9ab45"},"source":"Clearly, since we're using a number of lag variables, we can't use the first N days in the series, since their lag values will be unknown."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"f2c7f63c2c4f1cc7dafb5404c2c9af3f42087a34","_cell_guid":"7b151081-6802-4ac0-83f4-176ce1ae5e8e","collapsed":true},"source":"used_data = data[['Page'] + date_columns[LAG_DAYS:]]"},{"cell_type":"markdown","metadata":{"_uuid":"11fadb50a820ac9309ef6a9106deb414da267a39","_cell_guid":"8f0a5dbd-c05c-4430-98be-73795f5d095b"},"source":"We now convert the original table to individual entries. Since doing this produces a very large amount of data points, we can drop any rows with NaN values."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"947e920e4aeff08eb3d539eaa70da1570ebffdbb","_cell_guid":"4632c0fb-4ffe-405e-997e-572e892e0ec0"},"source":"flattened = pd.melt(used_data, id_vars='Page', var_name='date', value_name='Visits')\nflattened.dropna(how='any', inplace=True)\n\nflattened.head()"},{"cell_type":"markdown","metadata":{"_uuid":"422e1722245a08b6c3cd4706e96746f56403910c","_cell_guid":"fd5d04f7-9edc-4ce1-844d-2d771d5bb476"},"source":"We will also want to get date indices, which we will later use to select the correct date values from the data matrix."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"ac09357a658d84e9bf381c87ff9cc2081524555c","_cell_guid":"5fea1c28-9b5d-40e1-bb9f-a4cfd8e94603","collapsed":true},"source":"date_indices = {d: i for i, d in enumerate(date_columns)}"},{"cell_type":"markdown","metadata":{"_uuid":"d5dd320631a27d712890cb318ba380f85c36c8be","_cell_guid":"3d5c8be4-eda1-481e-921d-8b74c5814568"},"source":"We now prepare the data needed to extract the lag values from the data.\n- `page_indices` will store the row index\n- `date_indices` will store the column index"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"23035fce053ba67239d9c4b4b0bf522a21186f3e","_cell_guid":"375049c6-c909-4623-8079-8c3dd94c109f"},"source":"# We will need the page indices to tell us which row to look at\ndata['page_indices'] = data.index\n# We set the index to page so we can merge with `flattened` easily\ndata.set_index('Page', inplace=True)\n\nflattened['date_indices'] = flattened['date'].apply(date_indices.get)\nflattened = flattened.set_index('Page').join(data['page_indices']).reset_index()\n\nflattened.iloc[538:548] # 543 happens to be the index where the second time series begins"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"9393c279e4ff2fc82b75c8ff185d4385a46c6d22","_cell_guid":"4907074b-6656-4c1c-b728-494e0625894a","collapsed":true},"source":"for lag in range(1, LAG_DAYS + 1):\n    flattened['lag_%d' % lag] = data[date_columns].values[\n        flattened['page_indices'],\n        flattened['date_indices'] - lag\n    ]"},{"cell_type":"markdown","metadata":{"_uuid":"a1dfb43eac6f72db26191ee499032559ef32b7a7","_cell_guid":"6a35c017-2f6d-46c4-9391-a57883e6f859"},"source":"Again, since we've got plenty of data to work with, we can drop any rows containing NaNs. This ensures that we don't incur any error with any non-optimal imputation strategy. "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"e69f63dfe94309ee8387ac81994c38faf98fbfce","_cell_guid":"79da3ab9-36d3-48f4-bc48-55920ab723e5","collapsed":true},"source":"flattened.dropna(how='any', inplace=True)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"d36a7623dc78f77dde17cd00616ef19cc1d0a676","_cell_guid":"3497490b-80bb-4c5d-9fec-007852746b78"},"source":"flattened.shape"},{"cell_type":"markdown","metadata":{"_uuid":"fa4f78bd6260247e60c899c03e79564c270eb384","_cell_guid":"12b83625-b077-479c-8ae0-a6f6c9d25c89"},"source":"In the three following cells, we verify that we got the desired results. We examine the first five data points and see, that we were in fact, successful."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"1a4a8c79d1b473c935790fa0a232c623279c3b6d","_cell_guid":"1db8dac6-0e1b-4003-bf8f-52fefc230c2e"},"source":"flattened.head()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"149dfab85452ad0ed7cfccfe81e39d3675408f43","_cell_guid":"3f9283d7-7004-438b-84ac-d9698fa1382b"},"source":"flattened.iloc[543:548]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"45e85def546d47b0bffb7c508419c42ad5ea77a0","_cell_guid":"c9583a91-bc40-4c58-96fc-65db8443bb2e"},"source":"data.iloc[:2]"},{"cell_type":"markdown","metadata":{"_uuid":"1c239187b6f608aa3d4f457f812b927dddd3e0de","_cell_guid":"32cc4320-b940-43f2-b30e-b39239d09b1f"},"source":"Finally, we can drop the index columns `page_indices` and `date_indices`. `flattened` now contains data that we can use with typical machine learning algorithms. We can save this to a csv file for later use."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"f4800be5a1bbecad6cac97c872f525702e1eab08","_cell_guid":"00616260-bd18-4891-87f7-f535e075eac9"},"source":"flattened.drop(['page_indices', 'date_indices'], inplace=True, axis=1)\n\nflattened.head()"}],"nbformat_minor":1}