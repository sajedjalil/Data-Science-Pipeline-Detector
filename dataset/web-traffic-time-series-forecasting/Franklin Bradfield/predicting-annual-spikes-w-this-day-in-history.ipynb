{"cells":[{"source":"# ** Predicting Annual Spikes w/ \"This Day in History\"**\n\n> *When this competition started, I entered with much enthusiasm, but about halfway through, I got caught up with something that prohibited me from giving it my all. With that said, I've decided to publish an approach that I was pursuing that I believe could have [some] value at this point to those who are still working hard on cracking this. With over 145k individual time series, it likely won't make enough of a difference to move spots on the leaderboard, but could prove valuable in Stage 2 for those seeking to push the limits of their models.*\n\nI noticed early in the competition that the main approaches that competitors were taking involved ignoring spikes - deeming them unreliable, unpredictable, and out of the scope of forecasting. It seems that the baseline method involves forecasting with a lagged median and then informing it all kinds of domain insight such as weekends, holidays, etc. \n\nThe purpose of this module is to demonstrate that annual \"spikes\" in the traffic on certain (but not all) pages can indeed be identified and forecasted. This code below is not intended to help one formulate a model from scratch, but merely to supplement an existing approach with additional information. For getting started with a baseline approach, there are a wealth of great kernels to work through.\n\nBecause this kernel involves web scraping from Wikipedia, this module cannot be run in its entirety. I will comment out and make notes on the areas where a web connection is required. You will need to run the notebook locally to gather and apply the data that the code can generate in an otherwise connected environment.\n\nLet's do our necessary imports, fetch the training data, and get started:","metadata":{"_uuid":"5b603b4e23f00a08515f6acd9114fa5076963f3a","_cell_guid":"f115c3c7-d868-4cd7-9f19-7178fbe5a0af"},"cell_type":"markdown"},{"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport re\nimport requests\nimport pandas as pd\nimport os\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nfrom matplotlib.pylab import rcParams\n\nrcParams['figure.figsize'] = 15, 6\n\ndef get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org',page)\n    if res:\n        return res.group()[:2]\n    return 'na'","execution_count":null,"metadata":{"_uuid":"6fa7f46522d47119ed93d0ffe50689f5fc08160e","collapsed":true,"_cell_guid":"fe042574-4610-42cd-b7b0-e6749033a409"},"cell_type":"code"},{"outputs":[],"source":"train = pd.read_csv('../input/train_1.csv').fillna(0)\nfor col in train.columns[1:]:\n    train[col] = pd.to_numeric(train[col],downcast='integer')\ntrain['lang'] = train.Page.map(get_language)","execution_count":null,"metadata":{"_uuid":"6968ef1666521fa8406116b296c3ae1f91342f68","collapsed":true,"_cell_guid":"9a3197d4-3776-4188-9079-bf68d8e7428b"},"cell_type":"code"},{"source":"The motivation for this module  is based on the assumption that if annual \"spikes\" in traffic can be observed to occur on the same date or at least in the same date range for two or more years, then there's a good reason to believe that a spike will occur in the following year.\\*\\* Since SMAPE heavily punishes under-forecasting, it doesn't make sense to just neglect spike trends and forecast normal medians or something to that effect. Furthermore, its doubtful that any statistical or ML model, .e.g ARIMA, RNN, XGBoost, etc. could adequately predict these spike with such limited data. It's best to go with common sense and manually add them in after your baseline forecasts have been established.\n\n\\*\\* *Spikes going back two years cannot be validated in the range of interest for Stage 1 because data only for the first quarter of 2016 is available. However, spikes for the past two years can be validated and used for prediction in Stage 2, because data for the third and fourth quarters of 2015 and 2016 is available.*\n\n","metadata":{"_uuid":"17e2366d7ee627f48352d846e5bb375cbadd4489","_cell_guid":"235f088b-d323-468e-9a5a-4909646a3b28"},"cell_type":"markdown"},{"source":"There are several outstanding events in the Stage 2 date range (09/10/2017 - 11/10-2017) that we can all agree should have predictable spikes:\n\n1. September 11th Attacks (September 11th)\n2. Halloween (October 31st)\n3. Release of iPhone 7s (or iPhone 8) (previous models were typically released in mid to late September)\n\nHowever, there are a few major events that garner attention that one may not be aware of. For example, how about the final release of John F. Kennedy's assassination documents on October 26th 2017, which on a side note also happens to fall within the anniversary range of the Cuban Missile Crisis? In any case, while some events are obvious, there are too many to keep track of, so it is inevitable that some will \"slip through the cracks.\" \n\nThankfully, using Wikipedia pages for each day of the year, we can generate dictionaries that capture many events that could be of recurring interest year after year. Each page can be thought of as \"This Day in History\", as it contains links to key anniversaries, events, births, deaths, holidays, etc. that fall on the particular date that a page covers. Of course, not all events are significant to generate recurring traffic year after year, so this kernel is at best a starting point for further investigation into spike forecasting.\n\nBefore going further, let's establish that certain articles exist that have very clear and predictable traffic spikes:","metadata":{"_uuid":"576d97107320e2aa6fee9a77de96d5ffad8cdbb1","_cell_guid":"1260bb44-6c11-49c8-b650-e1879f712ade"},"cell_type":"markdown"},{"outputs":[],"source":"def range_analysis(article, start, end):\n\n    series_2015 = train[train['Page'] == '{0}_en.wikipedia.org_all-access_all-agents'.format(article)].loc[:,'2015-{}'.format(start):'2015-{}'.format(end)]\n    series_2016 = train[train['Page'] == '{0}_en.wikipedia.org_all-access_all-agents'.format(article)].loc[:,'2016-{}'.format(start):'2016-{}'.format(end)]\n    try: # Some topics don't have series for the 'all-access_all-agents' suffix\n        series_2015.transpose().plot(kind='bar', title='2015 Pattern')\n        series_2016.transpose().plot(kind='bar', title='2016 Pattern')                                                                                       \n    except TypeError: # If not, then 'mobile-web_all-agents' can be attempted,\n        # but note that this could throw an error as well, although very rarely\n        series_2015 = train[train['Page'] == '{0}_en.wikipedia.org_mobile-web_all-agents'.format(article)].loc[:,'2015-{}'.format(start):'2015-{}'.format(end)]\n        series_2016 = train[train['Page'] == '{0}_en.wikipedia.org_mobile-web_all-agents'.format(article)].loc[:,'2016-{}'.format(start):'2016-{}'.format(end)]\n        series_2015.transpose().plot(kind='bar', title='2015 Pattern')\n        series_2016.transpose().plot(kind='bar', title='2016 Pattern')\n    plt.show()","execution_count":null,"metadata":{"_uuid":"d87988fc9ada23e623ab6866993c0a05e9ea10b2","collapsed":true,"_cell_guid":"05cc0697-cb67-47a9-8371-ee0b5a748126"},"cell_type":"code"},{"outputs":[],"source":"range_analysis('Halloween', '09-10','11-10')","execution_count":null,"metadata":{"_uuid":"b122945abfca4fc4f7e1d450241936eba136607f","collapsed":true,"_cell_guid":"b437419a-db6b-4e66-b4af-a1ed73670cf2"},"cell_type":"code"},{"outputs":[],"source":"# 'Casualties_of_the_September_11_attacks' exists in the data as well\n# Time series shows similar spike pattern\nrange_analysis('September_11_attacks', '09-10','11-10')","execution_count":null,"metadata":{"_uuid":"a5120a1f8eed88c4970ac5d1ec76eff886007d5a","collapsed":true,"_cell_guid":"0df67ccf-380f-4895-ab0c-0c3e5bc1941b"},"cell_type":"code"},{"source":"As we can see from these two examples, the spikes from 2015 and 2016 are nearly a mirror image of each other. The build-up and fade have a very similar structure as well. Now are you sure you wouldn't want to at least attempt adding in a spike for 2017, versus naively predicting an informed median or going with what your model says?\n\nNow let's investigate the structure of a typical Wikipedia date page URL so that we can automate a web scraping process that will fetch topics of interest for every day of the year: \n> https://en.wikipedia.org/wiki/January_1\n\nThis https:// `language` -wikipedia.org.wiki/ `month`_`day` structure is valid for Wikipedia articles for every date of the year, provided you can spell out every date and every month in all seven languages for which time series are available.\n\nI've assembled dictionaries below that perform such mapping:","metadata":{"_uuid":"a53999d26fb3ae864cc23bc10c49d13d601bb881","_cell_guid":"f92bf525-ba9b-4bac-b620-30e6b127faa5"},"cell_type":"markdown"},{"outputs":[],"source":"ENGLISH_MONTHS = {\n          '01': 'January',\n          '02': 'February',\n          '03': 'March',\n          '04': 'April',\n          '05': 'May',\n          '06': 'June',\n          '07': 'July',\n          '08': 'August',\n          '09': 'September',\n          '10': 'October',\n          '11': 'November',\n          '12': 'December'\n         }\n\nFRENCH_MONTHS = {\n          '01': 'janvier',\n          '02': 'février',\n          '03': 'mars',\n          '04': 'avril',\n          '05': 'mai',\n          '06': 'juin',\n          '07': 'juillet',\n          '08': 'août',\n          '09': 'septembre',\n          '10': 'octobre',\n          '11': 'novembre',\n          '12': 'décembre'\n         }\n\nJAPANESE_MONTHS = {\n          '01': '1月',\n          '02': '2月',\n          '03': '3月',\n          '04': '4月',\n          '05': '5月',\n          '06': '6月',\n          '07': '7月',\n          '08': '8月',\n          '09': '9月',\n          '10': '10月',\n          '11': '11月',\n          '12': '12月'\n         }\n\nCHINESE_MONTHS = {\n          '01': '1月',\n          '02': '2月',\n          '03': '3月',\n          '04': '4月',\n          '05': '5月',\n          '06': '6月',\n          '07': '7月',\n          '08': '8月',\n          '09': '9月',\n          '10': '10月',\n          '11': '11月',\n          '12': '12月'\n         }\n\nRUSSIAN_MONTHS = {\n          '01': 'января',\n          '02': 'февраля',\n          '03': 'марта',\n          '04': 'апреля',\n          '05': 'мая',\n          '06': 'июня',\n          '07': 'июля',\n          '08': 'августа',\n          '09': 'сентября',\n          '10': 'октября',\n          '11': 'ноября',\n          '12': 'декабря'\n         }\n\nGERMAN_MONTHS = {\n          '01': 'Januar',\n          '02': 'Februar',\n          '03': 'März',\n          '04': 'April',\n          '05': 'Mai',\n          '06': 'Juni',\n          '07': 'Juli',\n          '08': 'August',\n          '09': 'September',\n          '10': 'Oktober',\n          '11': 'November',\n          '12': 'Dezember'\n         }\n\nSPANISH_MONTHS = {\n          '01': 'enero',\n          '02': 'febrero',\n          '03': 'marzo',\n          '04': 'abril',\n          '05': 'mayo',\n          '06': 'junio',\n          '07': 'julio',\n          '08': 'agosto',\n          '09': 'septiembre',\n          '10': 'octubre',\n          '11': 'noviembre',\n          '12': 'diciembre'\n         }","execution_count":null,"metadata":{"_uuid":"e99c6d7c5f6d664687fb05ecb8dd6490d77a003c","collapsed":true,"_cell_guid":"16d13e7b-5fd7-4261-a324-8c0e66636521"},"cell_type":"code"},{"source":"Now let's use the month dictionaries along with some helper functions to create date ranges for 366 days and 7 languages. Note that the pandas date range involve using the year 2016, however this can be ignored as we only need the month and day to request and scrape articles. Discrete functions for each language are necessary because each language has its own quirks when it comes to dates.  French adds \"-er\" to the first day of every month for example, resulting in `1er_janvier`.","metadata":{"_uuid":"c527ac5ff4ed6859a3bba7029827704513ea07d5","_cell_guid":"dd1e6336-4bc0-4f28-9f6f-7d78251bab5e"},"cell_type":"markdown"},{"outputs":[],"source":"# English\ndef fetch_dates_en(start, end):\n    en_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        en_dates.append(ENGLISH_MONTHS[month_day[0]] + '_{}'.format(str(int(month_day[1]))))\n    return en_dates\n\n# French\ndef fetch_dates_fr(start, end):\n    fr_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        if int(month_day[1]) == 1:\n            fr_dates.append('{}er_'.format(str(int(month_day[1]))) + FRENCH_MONTHS[month_day[0]])\n        else:\n            fr_dates.append('{}_'.format(str(int(month_day[1]))) + FRENCH_MONTHS[month_day[0]])\n    return fr_dates\n\n# Japanese\ndef fetch_dates_ja(start, end):\n    ja_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        ja_dates.append(JAPANESE_MONTHS[month_day[0]] + '{}日'.format(str(int(month_day[1]))))\n    return ja_dates\n\n# Chinese\ndef fetch_dates_zh(start, end):\n    zh_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        zh_dates.append(CHINESE_MONTHS[month_day[0]] + '{}日'.format(str(int(month_day[1]))))\n    return zh_dates\n\n# Russian\ndef fetch_dates_ru(start, end):\n    ru_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        ru_dates.append('{}_'.format(str(int(month_day[1]))) + RUSSIAN_MONTHS[month_day[0]])\n    return ru_dates\n\n# German\ndef fetch_dates_de(start, end):\n    de_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        de_dates.append('{}._'.format(str(int(month_day[1]))) + GERMAN_MONTHS[month_day[0]])\n    return de_dates\n\n# Spanish\ndef fetch_dates_es(start, end):\n    es_dates = []\n    date_range = pd.date_range(start,end)\n    for date in date_range:\n        month_day = datetime.strftime(date, '%Y-%m-%d')[-5:].split('-')\n        es_dates.append('{}_de_'.format(str(int(month_day[1]))) + SPANISH_MONTHS[month_day[0]])\n    return es_dates\n\nstart_date, end_date = '2016-01-01', '2016-12-31'\n\n# There are 366 dates for each year because 2016 was a leap year (includes Feb. 29th)\n\nenglish_dates = fetch_dates_en(start_date, end_date)\nassert len(english_dates) == 366\n\nfrench_dates = fetch_dates_fr(start_date, end_date)\nassert len(french_dates) == 366\n\njapanese_dates = fetch_dates_ja(start_date, end_date)\nassert len(japanese_dates) == 366\n\nchinese_dates = fetch_dates_zh(start_date, end_date)\nassert len(chinese_dates) == 366\n\nrussian_dates = fetch_dates_ru(start_date, end_date)\nassert len(russian_dates) == 366\n\ngerman_dates = fetch_dates_de(start_date, end_date)\nassert len(german_dates) == 366\n\nspanish_dates = fetch_dates_es(start_date, end_date)\nassert len(spanish_dates) == 366","execution_count":null,"metadata":{"_uuid":"eabe2b81ae8d85946d585669f5a5ca762019d2a3","collapsed":true,"_cell_guid":"f1ae4a91-9b69-4574-91fe-7aad27d7fd1a"},"cell_type":"code"},{"source":"Before scraping the Wikipedia date pages looking for articles of interest, we need to extract the article information from the dataset in each language group, so we can search for their corresponding hypyerlinks in the source HTML.","metadata":{"_uuid":"c26812cb7d0ab1afc8490e0512aea897f5b3cb5d","_cell_guid":"4339d38c-3d7f-4766-aa8f-77666515425f"},"cell_type":"markdown"},{"outputs":[],"source":"def get_article(page):\n    res = re.search('.*_*.wiki', page)\n    return res.group()[:-8] # Extract article info. only\n\nenglish_articles = train[train['lang'] == 'en']['Page'].map(get_article).tolist()\nfrench_articles = train[train['lang'] == 'fr']['Page'].map(get_article).tolist()\njapanese_articles = train[train['lang'] == 'ja']['Page'].map(get_article).tolist()\nchinese_articles = train[train['lang'] == 'zh']['Page'].map(get_article).tolist()\nrussian_articles = train[train['lang'] == 'ru']['Page'].map(get_article).tolist()\ngerman_articles = train[train['lang'] == 'de']['Page'].map(get_article).tolist()\nspanish_articles = train[train['lang'] == 'es']['Page'].map(get_article).tolist()\nnon_language_articles = train[train['lang'] == 'na']['Page'].map(get_article).tolist()\n\nassert len(english_articles) + len(chinese_articles) + len(japanese_articles) \\\n       + len(german_articles) + len(french_articles) + len(russian_articles) \\\n       + len(spanish_articles) + len(non_language_articles) == train.shape[0]\n        \nprint('Sample English articles: {0}, {1}, {2}'.format(*english_articles[:3]), sep=',')\nprint('Sample Chinese articles: {0}, {1}, {2}'.format(*chinese_articles[:3]), sep=',')\nprint('Sample Russian articles: {0}, {1}, {2}'.format(*russian_articles[:3]), sep=',')","execution_count":null,"metadata":{"_uuid":"2d95aefb3d43a612edfca278cb44b3e245f5bec9","collapsed":true,"_cell_guid":"25b1dc64-edcd-4e73-b4cf-5d712292630e"},"cell_type":"code"},{"source":"Finally, let's make a function that will allow us to scrape all 366 date pages in all 7 languages, searching for articles that are both present in the dataset and on the date pages in the form of hyperlinks.","metadata":{"_uuid":"e4a719941bef367120758ac79453290fe759f9c9","_cell_guid":"d0a9dbfd-29a6-466b-b8ec-515d6be5a8d4"},"cell_type":"markdown"},{"outputs":[],"source":"### The function below is responsible for the actual requesting and requires\n### an Internet connection to call. We can define it in Kaggle without calling though.\n\ndef _request_parse_html(language, date):\n    html = requests.get('https://{0}.wikipedia.org/wiki/{1}'.format(language,date))\n    soup = BeautifulSoup(html.text, \"html5lib\")\n    lists = soup.find_all('ul')\n    all_articles = []\n    for ul in lists:\n        links = ul.find_all('a')\n        for link in links:\n            if language == 'ru':\n                try:\n                 # Russian hyperlinks have some strange encoding that I haven't figured out.\n                 # We can't extract the link directly, so we'll take the title instead.\n                 # In rare cases, the article titles and links differ, so matches will fail.\n                 # Not as reliable as the regex below, but best we can do.\n                    article = '_'.join(link['title'].split())\n                    all_articles.append(article)\n                except (AttributeError, KeyError, IndexError) as e:\n                    continue\n            elif language in {'ja', 'zh'}:\n                try:\n                # Asian hyperlinks have an encoding issue as well.\n                # Don't need to split and rejoin though because these languages don't use spaces\n                    article = link['title'] \n                    all_articles.append(article)\n                except (AttributeError, KeyError, IndexError) as e:\n                    continue\n            else: \n                try:\n                # Can extract link directly for English, French, German, Spanish\n                # This extracts the article link directly and is guaranteed to work for matching links\n                    article = re.search(r'(?<=/wiki/).*', link['href']).group()\n                    all_articles.append(article)\n                except (AttributeError, KeyError, IndexError) as e:\n                    continue\n    return all_articles\n\ndef find_key_articles(language, date, article_list):\n    key_articles = set()\n    date_articles = _request_parse_html(language, date)\n    for article in article_list:\n        if article in date_articles:\n            key_articles.add(article)\n    return key_articles","execution_count":null,"metadata":{"_uuid":"1de17b0266a911119ee640f40238d635bc8d6a33","collapsed":true,"_cell_guid":"272ff386-b1da-408f-835f-099c510bba23"},"cell_type":"code"},{"source":"This is as far as this kernel can run, on Kaggle at least. The last step is to uncomment and run the following 7 blocks on your machine to get the final output. Since 2,562 (366 days * 7 languages) requests will be made, this is could take a long time, so go ahead and work on fine-tuning other parts of your model while waiting. I like to run in individual blocks by language in case have a connection issue halfway through, but you can also combine all them into one block and run it overnight or something if needed.\n\nThe final output for each block will consist of a dictionary with 366 keys - one for each day of a leap year. The keys can be accessed using English dates of the form \"%B\\_%d\", e.g. \"October_7\". Each value is a set containing articles in the time series dataset that are present on the Wikipedia page for its respective date. When it's all said and done, there should be the following number of articles of interest for each language:\n\nSome redudant articles can be found in each set such as `Special:MyContributions`, `Special:RecentChanges`, `Wikipedia:Contact_us`. Although these articles are present in the time series data, they are not relevant for our purposes so can be ignored and/or filtered out. ","metadata":{"_uuid":"1f3d81a4c141e7ad25ec093228edf6bc2e367543","_cell_guid":"c558a38e-36f9-49ce-a754-cbbaa70ca603"},"cell_type":"markdown"},{"outputs":[],"source":"### Requires Internet connection\n\n# key_english_articles = dict()\n\n# for i, date in enumerate(english_dates):\n#     key_articles = find_key_articles('en', date, english_articles)\n#     key_english_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"eb926d22d8eb152e863cb560b4fbd51ff5a6ca49","collapsed":true,"_cell_guid":"9961d7e2-3017-47e8-82b2-7bf2f9c38136"},"cell_type":"code"},{"outputs":[],"source":"### Require Internet connection\n\n# key_french_articles = dict()\n\n# for i, date in enumerate(french_dates):\n#     key_articles = find_key_articles('fr', date, french_articles)\n#     key_french_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"d1b9e2cdcb526ac113c38b8d6411d9dcf63bef6b","collapsed":true,"_cell_guid":"66af605c-3dae-4ade-b373-2461bf6ac3bb"},"cell_type":"code"},{"outputs":[],"source":"### Requires Internet connection\n\n# key_japanese_articles = dict()\n\n# for i, date in enumerate(japanese_dates):\n#     key_articles = find_key_articles('ja', date, japanese_articles)\n#     key_japanese_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"9e88f7a4aa66522c07dd12f812db1d4d52626389","collapsed":true,"_cell_guid":"fa84a8d0-af74-4bc4-b55a-0ca2cce7b635"},"cell_type":"code"},{"outputs":[],"source":"### Require Internet connection\n\n# key_chinese_articles = dict()\n\n# for i, date in enumerate(chinese_dates):\n#     key_articles = find_key_articles('zh', date, chinese_articles)\n#     key_chinese_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"428ff0d74d4e013ea92946ed3f02bef43fa1dae9","collapsed":true,"_cell_guid":"b51bc494-6047-4e9a-8ea9-b60a174cb943"},"cell_type":"code"},{"outputs":[],"source":"### Requires Internet connection\n\n# key_russian_articles = dict()\n\n# for i, date in enumerate(russian_dates):\n#     key_articles = find_key_articles('ru', date, russian_articles)\n#     key_russian_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"1db7d0979aa587a1d393461dfe05c7c5e46d09e8","collapsed":true,"_cell_guid":"037760f9-4480-49f5-9c17-ab5f76bdbd91"},"cell_type":"code"},{"outputs":[],"source":"### Requires Internet connection\n\n# key_german_articles = dict()\n\n# for i, date in enumerate(german_dates):\n#     key_articles = find_key_articles('de', date, german_articles)\n#     key_german_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"93383764bc1112c7e40b6a29875df1b830bdae92","collapsed":true,"_cell_guid":"f606150a-c4d5-4a16-bc9b-a77baf4a1f84"},"cell_type":"code"},{"outputs":[],"source":"### Requires Internet connection\n\n# key_spanish_articles = dict()\n\n# for i, date in enumerate(spanish_dates):\n#     key_articles = find_key_articles('es', date, spanish_articles)\n#     key_spanish_articles[english_dates[i]] = key_articles","execution_count":null,"metadata":{"_uuid":"56284ab324f72613d010a7a48ede92abfe7dea92","collapsed":true,"_cell_guid":"77e81a8b-0480-406d-bc1d-4b98f758d357"},"cell_type":"code"},{"source":"### **Optional**\n\nIf you'd like to serialize and store locally the key article dictionaries the objects you've just spent all this time gathering, uncomment and run the following:","metadata":{"_uuid":"ac771019fbf4277a74aa49a607277b1393a35083","_cell_guid":"544c2d4b-5cb8-4b50-b705-3a2986189dd1"},"cell_type":"markdown"},{"outputs":[],"source":"# dest = 'pkl_objects'\n# if not os.path.isdir('./pkl_objects'):\n#     os.mkdir(dest)\n# pickle.dump(key_english_articles, open(os.path.join(dest, 'english_collection.pkl'), 'wb'), protocol=4)\n# pickle.dump(key_french_articles, open(os.path.join(dest, 'french_collection.pkl'), 'wb'), protocol=4)\n# pickle.dump(key_japanese_articles, open(os.path.join(dest, 'japanese_collection.pkl'), 'wb'), protocol=4)\n# pickle.dump(key_chinese_articles, open(os.path.join(dest, 'chinese_collection.pkl'), 'wb'), protocol=4)\n# pickle.dump(key_russian_articles, open(os.path.join(dest, 'russian_collection.pkl'), 'wb'), protocol=4)\n# pickle.dump(key_german_articles, open(os.path.join(dest, 'german_collection.pkl'), 'wb'), protocol=4)\n# pickle.dump(key_spanish_articles, open(os.path.join(dest, 'spanish_collection.pkl'), 'wb'), protocol=4)","execution_count":null,"metadata":{"_uuid":"d796287e045bf4161425de5270e1e2b4c7cdecf9","collapsed":true,"_cell_guid":"ca6c7ab0-566a-431b-a65f-b727844774cc"},"cell_type":"code"},{"source":"### **Where to go from here**:\n\n\n\n\n\n- Although we've identified relevant articles from the dataset that could attract annual interest for each day of the year, this won't translate directly to better predictive capacity for our models. What is needed at this point is further filtering to pinpoint the articles that display recurring spike patterns on the same days for 2015 and 2016. This could be accomplished with a mean-aware approach such as Z-Scores that considers a score greater than 3.0 or 4.0 for example to be a \"spike.\" Alternatively a mean-naive approach could be used that qualifies spikes using percentiles such as 98%, 99%, etc. \n\n\n\n\n- The articles that this cross-referencing step discovers can be examined and one can use such information to generate spikes in predicted traffic, using a method of choice such as averaging spikes between 2015 and 2016 . There's a bit of manual work involved upfront, but with a few helper functions, the whole process could be easily streamlined for all key articles.\n\n\n\n\n\n- In this module, we've only found articles that are matched between the dataset and the date pages. A further path of investigation could involve looking into how traffic \"spills\" over from one article to others that are related. One could start by examining traffic for the first five or ten hyperlinks on each article of interest. This could generate further insight and allow more predictable spikes to be discovered. In regards to `September_11_attacks`, here's a related spike:","metadata":{"_uuid":"aa44829e414f3ea3d06ce9a6d07dfce0a503a1b8","_cell_guid":"16bdc887-6c37-4b0b-ae49-11fd4866d061"},"cell_type":"markdown"},{"outputs":[],"source":"range_analysis('World_Trade_Center_(1973–2001)', '09-10','11-10')","execution_count":null,"metadata":{},"cell_type":"code"},{"outputs":[],"source":"","execution_count":null,"metadata":{"collapsed":true},"cell_type":"code"}],"nbformat":4,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","mimetype":"text/x-python","version":"3.6.1","file_extension":".py","nbconvert_exporter":"python","name":"python"}},"nbformat_minor":1}