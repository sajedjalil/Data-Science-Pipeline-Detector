{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTING THE REQUIRED LIBRARIES\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.layers import Dense, LSTM\nfrom keras.models import Sequential\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom numpy.fft import *\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_1= pd.read_csv(\"../input/web-traffic-time-series-forecasting/train_1.csv.zip\")\ntrain_2= pd.read_csv(\"../input/web-traffic-time-series-forecasting/train_2.csv.zip\")\nkey_1= pd.read_csv(\"../input/web-traffic-time-series-forecasting/key_1.csv.zip\")\nkey_2= pd.read_csv(\"../input/web-traffic-time-series-forecasting/key_2.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1= train_1.fillna(0)\ntrain_2= train_2.fillna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FUNCTION FOR CREATING ANOTHER COLUMN IN TRAIN DATASET FOR THE PAGE LANGAUGE\ndef find_lang(page):\n    res= re.search(\"[a-z][a-z].wikipedia.org\", page)\n    if res:\n        return res[0][0:2]\n    return(\"na\")\n\ntrain_1[\"lang\"]= train_1.Page.map(find_lang)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_1[\"lang\"])\nplt.title(\"Language Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_set= {}\nlang_set[\"en\"]= train_1[train_1.lang==\"en\"].iloc[:,0:-1]\nlang_set[\"ja\"]= train_1[train_1.lang==\"ja\"].iloc[:,0:-1]\nlang_set[\"de\"]= train_1[train_1.lang==\"de\"].iloc[:,0:-1]\nlang_set[\"na\"]= train_1[train_1.lang==\"na\"].iloc[:,0:-1]\nlang_set[\"fr\"]= train_1[train_1.lang==\"fr\"].iloc[:,0:-1]\nlang_set[\"zh\"]= train_1[train_1.lang==\"zh\"].iloc[:,0:-1]\nlang_set[\"ru\"]= train_1[train_1.lang==\"ru\"].iloc[:,0:-1]\nlang_set[\"es\"]= train_1[train_1.lang==\"es\"].iloc[:,0:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_set[\"en\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsums= {}\nfor key in lang_set:\n    sums[key]= lang_set[key].iloc[:,1:].sum(axis= 0) / lang_set[\"en\"].shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sums[\"en\"]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot and see individually traffic on particular language\ndays= [r for r in range(sums[\"en\"].shape[0])]\nfig= plt.figure(1, figsize= [10,10])\nplt.ylabel(\"Views per page\")\nplt.xlabel(\"Day\")\nplt.title(\"Page in Different Language\" )\nlabel= {\"en\":\"English\", \"ja\":\"Japanese\", \"de\":\"German\", \"na\": \"Media\",\n       \"fr\": \"French\", \"zh\": \"Chinese\", \"ru\": \"Russian\", \"es\": \"Spanish\"}\nfor key in sums:\n    plt.plot(days, sums[key], label= label[key])\n    \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)\n\na= np.array(train_1.iloc[0,1:-1],np.float32)\nsc= MinMaxScaler()\n\nplt.figure(figsize= (12,12))\nplt.plot(a)\nplt.show()\n\nplt.figure(figsize= (12,12))\nplt.plot(filter_signal(a))\nplt.show()\n\nregressor= Sequential()\nregressor.add(LSTM(units= 10, activation=\"relu\",return_sequences=True, input_shape=(None, 1)))\nregressor.add(Dense(units = 1))\nregressor.compile(optimizer= \"adam\", loss= \"mean_squared_error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a= np.array(train_1.iloc[0,1:-1],np.float32)\n\nplt.figure(figsize= (12, 6))\nplt.title(\"Before Fourier Transform\")\nplt.plot(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (12,6))\nplt.plot(filter_signal(a))\nplt.title(\"After Fourier Transform\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor= Sequential()\nregressor.add(LSTM(units= 10, activation=\"relu\",return_sequences=True, input_shape=(None, 1)))\nregressor.add(Dense(units = 1))\nregressor.compile(optimizer= \"adam\", loss= \"mean_squared_error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arr= filter_signal(a)\narr= arr.reshape(-1,1)\n\n# performing standardization of values using min max scaler\narr= sc.fit_transform(arr)\narr= np.reshape(arr, (-1,1,1))\n\n# Train data\narr_X= arr[:400]\narr_y= arr[1:401]\narr_X= np.reshape(arr_X, (-1,1,1))\n\n# Test data\narr_TX= arr[401:-1]\narr_ty= arr[402:]\narr_TX= np.reshape(arr_TX, (-1,1,1))\n\n# Training the model\nregressor.fit(arr_X, arr_y, batch_size= 5, epochs= 100, verbose= 0)\n\n# perdicting the value\nres= regressor.predict(arr_TX)\n\n# Reshaping the value for plotting purpose\nres= res.reshape(148,1)\narr_ty= arr_ty.reshape(148,1)\n\n#plotting the data\nplt.plot(res, color=\"r\")\nplt.plot(arr_ty, color=\"b\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_entry(key, idx):\n    data= lang_set[key].iloc[idx,1:]\n    fig= plt.figure(1, figsize= (10, 5))\n    plt.plot(days, data)\n    plt.xlabel(\"day\")\n    plt.ylabel(\"views\")\n    plt.title(train_1.iloc[lang_set[key].index[idx],0])\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx= [2, 7, 17, 57, 101, 257, 501, 757, 1117, 1517, 2777]\nfor i in idx:\n    plot_entry(\"en\",i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx= [2, 7, 17, 57, 101, 257, 501, 757, 1117, 1517, 2777]\nfor i in idx:\n    plot_entry(\"es\",i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Finding each page individually counts of no of most most in a Page"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For each language get highest few pages\nnpages= 5\ntop_pages= {}\nfor key in lang_set:\n    print(key)\n    sum_set= pd.DataFrame(lang_set[key][[\"Page\"]])\n    sum_set[\"Total\"]= lang_set[key].sum(axis= 1)\n    sum_set = sum_set.sort_values('Total',ascending=False)\n    top_pages[key] = sum_set.index[0]\n    print(sum_set.head(10))\n    print('\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key in top_pages:\n    fig = plt.figure(1,figsize=(10,5))\n    cols = train_1.columns\n    cols = cols[1:-1]\n    data = train_1.loc[top_pages[key],cols]\n    plt.plot(days,data)\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.title(train_1.loc[top_pages[key],'Page'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nimport warnings\n\ncols= train_1.columns[1:-1]\nfor key in top_pages:\n    data= np.array(train_1.loc[top_pages[key],cols],\"f\")\n    result= None\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\")\n        try:\n            arima= ARIMA(data, [2,1,4])\n            result= arima.fit(disp= False)\n        except:\n            try:\n                arima= ARIMA(data, [2,1,2])\n                result= arima.fit(disp= False)\n            except:\n                print(train_1.loc[top_pages[key], \"Page\"])\n                print(\"\\tARIMA FAILED\")\n    pred= result.predict(2,599, typ= \"levels\")\n    x= [i for i in range(600)]\n    i=0\n    plt.plot(x[2:len(data)],data[2:] ,label='Data')\n    plt.plot(x[2:],pred,label='ARIMA Model')\n    plt.title(train_1.loc[top_pages[key],'Page'])\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.legend()\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}