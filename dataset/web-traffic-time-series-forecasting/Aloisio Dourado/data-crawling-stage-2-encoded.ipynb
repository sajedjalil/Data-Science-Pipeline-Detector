{"metadata":{"language_info":{"file_extension":".py","name":"python","version":"3.6.1","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat_minor":1,"cells":[{"metadata":{"_cell_guid":"6e4d701a-b37c-44b1-b9c8-9d82acbc451a","_uuid":"05e11db0fa8e8a17f34b7a3d5d90290109c40d07","collapsed":true},"cell_type":"code","source":"# crawling data from 2017-09-01 to 2017-09-07\nimport urllib\nimport pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport json\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef get_views(web_info):\n    global date\n    purl = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/' \\\n          '{0}/{1}/{2}/{3}/daily/{4}/{5}' \\\n        .format(web_info[0], web_info[1], web_info[2], urllib.parse.quote(web_info[3]).replace(\"/\", \"%2F\"), date[0], date[-1])\n    #print(url)\n    #print(qurl)\n    res = np.array([np.nan for i in date])\n\n    ok = True\n\n\n    for tries in range(5):\n\n        try:\n            url = urllib.request.urlopen(purl)\n            ret =url.read().decode()\n            api_res = json.loads(ret)['items']\n        except:\n            ok = False\n\n        if ok:\n            break\n    if not ok:\n        print(purl, 'erro')\n        return res\n\n    for i in api_res:\n        time = i['timestamp'][0:-2]\n        res[date.index(time)] = i['views']\n    return res\n\n\ndef get_views_main(input_page):\n    pool_size = 4 #multiprocessing.cpu_count()\n    pool = multiprocessing.Pool(processes=pool_size)\n    res = pool.map(get_views, input_page)\n    pool.close()\n    pool.join()\n    return res\n\n\ndate = [\n    '20170901',\n    '20170902',\n    '20170903',\n    '20170904',\n    '20170905',\n    '20170906',\n    '20170907',\n    '20170908',\n    '20170909'\n]\n\nimport time\n\nprint(\"Reading...\")\n\npages = pd.read_csv(\"/data02/data/WTF/train_2.csv\", usecols=['Page'])\npage_details = pd.DataFrame([i.split(\"_\")[-3:] for i in pages[\"Page\"]],columns=[\"project\", \"access\", \"agent\"])\npage_details['PageFull'] = pages\n\ndef name_split(row):\n    return row.PageFull.split('_'+row.project+'_')[0]\n\npage_details['Page'] = page_details.apply(name_split, axis=1)\ndel page_details['PageFull']\n\nprint(\"Crawling...\")\nstart = time.time()\npage_web_traffic = np.array(get_views_main(page_details.values))\n\nprint(\"Time:\", time.time()-start)\n\nprint(\"total:\", len(page_web_traffic))\n","outputs":[],"execution_count":null}],"nbformat":4}