{"nbformat":4,"cells":[{"cell_type":"markdown","source":"From this GREAT KERNEL: https://www.kaggle.com/cpmpml/smape-weirdness\n\n(please review my one if any errors in r2 arrays/scores)\n\nthere're others timeseries related metrics: http://www.neural-forecasting-competition.com/NN3/instructions.htm , https://www.otexts.org/fpp/2/5","metadata":{"_uuid":"534f22921bce5e0c3361b077e47b120b616df3c1","_cell_guid":"6a06b075-c2d3-4909-821f-ffb86c285abb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.metrics import median_absolute_error,explained_variance_score\nfrom sklearn.metrics import mean_absolute_error,mean_squared_log_error\n        \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","metadata":{"_execution_state":"idle","_uuid":"98146638f70d9547cfacf8e010eff3a4bf365637","collapsed":true,"_cell_guid":"cbb3065b-7b30-42c3-8360-1de332961c2a"}},{"cell_type":"markdown","source":"Let's understand how to minimize smape with constant values, like what is done in the best public kernels.  Is the median the right value?\nFirst let us define the smape function.  It is quite straightforward, the only caveat is to treat nan correctly.  Thanks to the official answers on the forum, we know we can use this code.  It handles the case where there are nan in the y_true array, but it assumes there are no nan in the y_pred array.\n","metadata":{"_execution_state":"idle","_uuid":"3b34bc19ec5405b673f987a82e2b73dcbaca97c0","_cell_guid":"4282ff65-da1f-45a1-9757-315d5af724d8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"def smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.nanmean(diff)\n    #from https://www.otexts.org/fpp/2/5\n    #Hyndman and Koehler (2006) recommend that the sMAPE not be used. \n    #It is included here only because it is widely used, although we will not \n    #use it in this book.\n\n\n#from http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\ndef mse(y_true,y_pred):\n    #A non-negative floating point value (the best value is 0.0), \n    #or an array of floating point values, one for each individual target.\n    return mean_squared_error(y_true, y_pred)\ndef r2score(y_true, y_pred):\n    r2=r2_score(y_true, y_pred)\n    r2=max(-.5,r2)*200 # clip from -100 to +200\n    return r2\ndef mdae(y_true, y_pred):\n    #A positive floating point value (the best value is 0.0).\n    m=median_absolute_error(y_true, y_pred)\n    m=min(200,m)\n    return m\ndef mape(y_true, y_pred):\n    #MAE output is non-negative floating point. The best value is 0.0.\n    m=np.nanmean(np.abs((y_true-y_pred)/y_true) )\n    return m*100\ndef mae(y_true, y_pred):\n    #MAE output is non-negative floating point. The best value is 0.0.\n    m=mean_absolute_error(y_true, y_pred)*20 #just to scale better\n    m=min(200,m)\n    return m\ndef evs(y_true, y_pred):\n    #Best possible score is 1.0, lower values are worse.\n    m=explained_variance_score(y_true, y_pred)*200\n    m=min(0,m)\n    return m\ndef msle(y_true, y_pred):\n    #A non-negative floating point value (the best value is 0.0), \n    # or an array of floating point values, one for each individual target.\n    m=mean_squared_log_error(y_true, y_pred)*200 \n    m=min(200,m)\n    return m","metadata":{"_execution_state":"idle","_uuid":"6897d4291024d22098fdfd8f6950e2013114bc15","collapsed":true,"_cell_guid":"522607cd-e56a-4dc3-a1fb-930030b8e971"}},{"cell_type":"markdown","source":"Let's start with a simple example where the y_true series to predict contains only one point, say 3.","metadata":{"_execution_state":"idle","_uuid":"04969ef7eb9cb41a8334c50c73c87b33eb8cfa07","_cell_guid":"fa388a77-3b20-4893-a4f4-be8fff5d1055"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"y_true = np.array(3)\ny_pred = np.ones(1)\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue',label='SMAPE')\n\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\n    res3.append(mse(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\n    res4.append(mae(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\n    res5.append(evs(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\n    res6.append(msle(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\n    res7.append(mdae(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\n    res8.append(mape(np.array([y_true]*100).reshape(100,), \n                        np.array([y_pred*i]*100).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()","metadata":{"_execution_state":"idle","_uuid":"476ec295c65bbfdd30560480894162ffeaffb8d7","_cell_guid":"fa4028a5-632c-4651-b1e0-ca5583cc5380"}},{"cell_type":"markdown","source":"We see that SMAPE is 0 when the predicted value is equal to the true value.  We also see that an under estimate is penalized more than an over estimate.  Last, we see that the function is not convex for values above the true value.  This may lead to many local minima.  Let's see a second example to check for this, with two values 1 and 9 in the series we try to predict.","metadata":{"_execution_state":"idle","_uuid":"df20df4fe5529017e7e4d3991093e50f34083a15","_cell_guid":"3a39e5b8-ee64-4a4c-9bd1-15a6f2e751e1"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"y_true = np.array([1,9])\ny_pred = np.ones(len(y_true))\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue', label='SMAPE')\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res3.append(mse(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res4.append(mae(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res5.append(evs(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res6.append(msle(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res7.append(mdae(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res8.append(mape(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()\nprint('SMAPE min:%0.2f' % np.min(res), ' at %0.2f' % x[np.argmin(res)])\nprint('SMAPE is :%0.2f' % smape(y_true, y_pred*np.nanmedian(y_true)), \n      ' at median %0.2f' % np.nanmedian(y_true))","metadata":{"_execution_state":"idle","_uuid":"f4737aff257dfa2fe7137c644a4f53cbe58202b2","_cell_guid":"7e092573-57d8-4606-8c08-72b75cf2a118"}},{"cell_type":"markdown","source":"In this case there are two global minima with SMAPE = 80 for the two cases where our constant prediction is equal to one of the value in the true series.  The function reaches a local maxima with SMAPE = 100 for y_pred = 3.  And the value of the median (y_pred = 5) is about 95.24, i.e. it is significantly higher than the global minima.","metadata":{"_execution_state":"idle","_uuid":"16c45dc33840e3bb91277971445373a4cc0caa30","_cell_guid":"50dbfbae-f070-417e-9e97-0a7a7c0e8dfc"}},{"cell_type":"markdown","source":"Does this mean that SMAPE is impossible to optimize?  Let's see if we have more points in our y_true series, for instance a uniformly sampled series:","metadata":{"_execution_state":"idle","_uuid":"009bf55b4971eab012b0efac97850e2f904ab843","_cell_guid":"1f2ca917-7263-4988-b126-7aa1b845ecd1"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(0)\ny_true = np.random.uniform(1, 9, 100)\ny_pred = np.ones(len(y_true))\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue',label='SMAPE')\nplt.legend()\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res3.append(mse(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res4.append(mae(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res5.append(evs(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res6.append(msle(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res7.append(mdae(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res8.append(mape(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()\nprint('SMAPE min:%0.2f' % np.min(res), ' at %0.2f' % x[np.argmin(res)])\nprint('SMAPE is :%0.2f' % smape(y_true, y_pred*np.nanmedian(y_true)), \n      ' at median %0.2f' % np.nanmedian(y_true))","metadata":{"_execution_state":"idle","_uuid":"7628e7ee93bb625d3f9312367ffd42cc645233b5","_cell_guid":"d5c688e2-ee05-46da-b241-7b940b70f67e"}},{"cell_type":"markdown","source":"We see that the minimum of the smape function is met near the median which is good.  It could explain why the public kernels do well.  Let's see with a skewed distribution.","metadata":{"_execution_state":"idle","_uuid":"abf4b79784f3f8f7b3318531144ba389684a696a","_cell_guid":"9090f696-bcc8-4c17-99cc-cbdf2350bfa8"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(0)\ny_true = np.random.lognormal(1, 1, 100)\ny_pred = np.ones(len(y_true))\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue',label='SMAPE')\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res3.append(mse(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res4.append(mae(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res5.append(evs(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res6.append(msle(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res7.append(mdae(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\n    res8.append(mape(np.array(y_true).reshape(100,),\n                         np.array(y_pred*i).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()\nprint('SMAPE min:%0.2f' % np.min(res), ' at %0.2f' % x[np.argmin(res)])\nprint('SMAPE is :%0.2f' % smape(y_true, y_pred*np.nanmedian(y_true)), \n      ' at median %0.2f' % np.nanmedian(y_true))","metadata":{"_execution_state":"idle","_uuid":"80d2188af08f01db2173576346415b2518509708","_cell_guid":"4b38407a-a706-4cef-83e0-b74a11d7623e"}},{"cell_type":"markdown","source":"Here again the median does well.","metadata":{"_execution_state":"idle","_uuid":"3d702f55e27cade57c9a7b8c0082eec4c935938f","_cell_guid":"1539f863-5186-4916-916f-7747563a122d"}},{"cell_type":"markdown","source":"Wait a minute.  What is one or more values in the series are 0?   Let's start with one zero only.","metadata":{"_execution_state":"idle","_uuid":"404168104156ca907ab688ddd93d4207b32d1d6e","_cell_guid":"53992c2f-a96b-4e97-8036-cd24e7290914"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"y_true = np.array([0])\ny_pred = np.ones(len(y_true))\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue',label='SMAPE')\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\n    res3.append(mse(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\n    res4.append(mae(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\n    res5.append(evs(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\n    res6.append(msle(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\n    res7.append(mdae(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\n    res8.append(mape(np.array([y_true]*100).reshape(100,),\n                         np.array([y_pred*i]*100).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()","metadata":{"_execution_state":"idle","_uuid":"e22256f7ed49e0d3417f90c4ce7d54d796ab4e09","_cell_guid":"8a51f1b2-8256-445e-a5c7-cf39a3594812"}},{"cell_type":"markdown","source":"The function is discontinue at 0.  It is equal to 200 everywhere except at 0 where it equals 0.  Let's now look at two values.","metadata":{"_execution_state":"idle","_uuid":"24372b7239e747e48bdd1199be262d6c31a27ae1","_cell_guid":"ded48513-1a06-42ae-a1f5-fc385e9c3cdb"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(0)\ny_true = np.array([0,9])\ny_pred = np.ones(len(y_true))\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue',label='SMAPE')\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res3.append(mse(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res4.append(mae(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res5.append(evs(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res6.append(msle(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res7.append(mdae(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\n    res8.append(mape(np.array([y_true]*50).reshape(100,),\n                         np.array([y_pred*i]*50).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()\nprint('SMAPE min:%0.2f' % np.min(res), ' at %0.2f' % x[np.argmin(res)])\nprint('SMAPE is :%0.2f' % smape(y_true, y_pred*np.nanmedian(y_true)), \n      ' at median %0.2f' % np.nanmedian(y_true))","metadata":{"_execution_state":"idle","_uuid":"c4da9f4d665e3e5560699c2309ffd3464680837e","_cell_guid":"7b496788-0cad-404c-9646-40d9ceb86f3d"}},{"cell_type":"markdown","source":"Here we have two local minima at 100 when y_pred equals one of the values in the y_true series.   There is a discontinuity at 0. The mathematical limit of the series SMAPE(0, x) when x tends to 0 is 200.  It means that there is no local maxima near 0 as the value 200 cannot be reached.  But we can get values as close as we want to 200.","metadata":{"_execution_state":"idle","_uuid":"70d032ec5e6a8c36cd98ca3a8c0308de899eb50b","_cell_guid":"976bd42c-faea-481c-9b70-d5b178a545b5"}},{"cell_type":"markdown","source":"What if we have more values in the series, but still a proportion of 0?  ","metadata":{"_execution_state":"idle","_uuid":"54ab8077bee3567e6729515d6f365f3bf215ea92","_cell_guid":"1d4e6a05-1121-4ec5-a010-36318eaae5d6"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"np.random.seed(0)\ny_true = np.random.lognormal(1, 1, 100)\ny_true[y_true < 3] = 0\nprint('There are %d zeros in the series' % np.sum(y_true == 0))\ny_pred = np.ones(len(y_true))\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res,color='blue',label='SMAPE')\nres2,res3,res4,res5,res6,res7,res8=[],[],[],[],[],[],[]\nfor i in x:\n    res2.append(r2score(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\n    res3.append(mse(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\n    res4.append(mae(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\n    res5.append(evs(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\n    res6.append(msle(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\n    res7.append(mdae(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\n    res8.append(mape(np.array([y_true]*1).reshape(100,),\n                        np.array([y_pred*i]*1).reshape(100,)))\nplt.plot(x, res2,color='red',label='R2')\nplt.plot(x, res3,color='green',label='MSE')\nplt.plot(x, res4,color='yellow',label='MAE')\nplt.plot(x, res5,color='magenta',label='EVS')\nplt.plot(x, res6,color='cyan',label='MSLE')\nplt.plot(x, res7,color='black',label='MdAE')\nplt.plot(x, res8,color='orange',label='MAPE')\nplt.legend()\nplt.show()\nprint('SMAPE min:%0.2f' % np.min(res), ' at %0.2f' % x[np.argmin(res)])\nprint('SMAPE is :%0.2f' % smape(y_true, y_pred*np.nanmedian(y_true)), \n      ' at median %0.2f' % np.nanmedian(y_true))","metadata":{"_execution_state":"idle","_uuid":"35e01e3ee5a9959d04ed77c3fd58c6d8546b4b9c","_cell_guid":"8cfc4146-3ebd-46b6-8758-16ff60680c1c"}},{"cell_type":"markdown","source":"We see that the median is really not a good choice here and that 0 would be way better.  Moreover, a gradient descent from anywhere except 0 will miss the global minima, by large.","metadata":{"_execution_state":"idle","_uuid":"1793316c6429e8edd7579a57112955e19b91c439","_cell_guid":"aa09e975-a078-4234-8237-319ee810f5db"}},{"cell_type":"markdown","source":"I hope this notebook shows that the discontinuity at 0 makes it tricky to optimize SMAPE with constant predictions.  If you like it then please upvote it (button at the top left).","metadata":{"_execution_state":"idle","_uuid":"e6ebf03be720c614f87d5e2944a3ae49196faf08","_cell_guid":"0bbd7647-9791-40ee-89bb-dc4cfb878ae2"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"","metadata":{"_execution_state":"idle","_uuid":"8ee7df17bb332d5c78b5cfc53be503fd28625092","collapsed":true,"_cell_guid":"5c9ad8d8-c5ab-4b49-8818-d981c85528eb"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"nbconvert_exporter":"python","name":"python","version":"3.6.1","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","file_extension":".py"}},"nbformat_minor":1}