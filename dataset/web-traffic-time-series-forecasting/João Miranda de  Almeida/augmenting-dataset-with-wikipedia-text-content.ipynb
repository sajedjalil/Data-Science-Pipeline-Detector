{"nbformat":4,"cells":[{"cell_type":"markdown","source":"### What is this kernel for?\nTo demo how to easily add text content from wikipedia to our dataset.\n\nThis might allow us to extract some interesting features to use in the competition. I might explore this idea in another Kernel.\n\n\n---------\n__Note__ Unfortunately Kaggle blocks Internet access so this kernel won't retrieve the data but it should run just fine outside Kaggle.\n\n","metadata":{"_execution_state":"idle","_uuid":"6f67e4180602a11191cacdb397a9b4b6a3860d34","_cell_guid":"3db0605c-7ca1-4972-b061-ea2beb29cb98"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm, tqdm_notebook\nimport time\nimport requests\n\n\nSLEEP_TIME_S = 0.1","metadata":{"_execution_state":"idle","_uuid":"8093be598878cd036d6c7ff7ee4b2416d57b7e14","collapsed":true,"_cell_guid":"1521d106-994c-414c-b126-c89a4cba60bf"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"def extract_URL_and_Name(page):\n    \"\"\" From the page name in the input file extract the Name and the URL \"\"\"\n    return (['_'.join(page.split('_')[:-3])]\n            + ['http://' + page.split(\"_\")[-3:-2][0] +\n               '/wiki/' + '_'.join(page.split('_')[:-3])])","metadata":{"_execution_state":"idle","_uuid":"c56a3cd9ecd2b83323584c02af2152bc42f08540","collapsed":true,"_cell_guid":"75fba246-4d7b-4df0-bb68-edba34642579"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Load the dataset\ntrain = pd.read_csv('../input/train_1.csv')\n\n# We will just take a sample of the data, \n# remove this line to run on all the data\ntrain = train.sample(2)\n\n# Extract the Page name and URL:\npage_data = pd.DataFrame(\n    list(train['Page'].apply(extract_URL_and_Name)),\n    columns=['Name', 'URL'])\n","metadata":{"_execution_state":"idle","_uuid":"1574984b479d1774795a58879bf069fb8bef73e1","collapsed":true,"_cell_guid":"9347faac-296b-4342-b7e5-ce0d976820c3"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"page_data.head()","metadata":{"_uuid":"6709faf2d86610adfb0aae435f72d3c420efe03c","collapsed":true,"_cell_guid":"8100dfd0-f134-43f1-8589-1d317296e17d"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# Since Kaggle kernels don't have internet access this method will always return \n# an empty string\ndef fetch_wikipedia_text_content(row):\n    \"\"\"Fetch the all text data of a given page\"\"\"\n    try:\n        r = requests.get(row['URL'])\n        # Sleep for 100 ms so that we don't use too many Wikipedia resources \n        time.sleep(SLEEP_TIME_S)\n        to_return = [x.get_text() for x in \n                     BeautifulSoup(\n                         r.content, \"html.parser\"\n                     ).find(id=\"mw-content-text\").find_all('p')]\n    except:\n        to_return = [\"\"]\n    return to_return","metadata":{"_execution_state":"idle","_uuid":"838e2a0c95bf1e41a8a9324a7226b7b06bad8c6e","collapsed":true,"_cell_guid":"d11acfc4-897e-43a4-b10c-5655b3c5d4e0"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":"# This will fail due to lack of Internet\ntqdm.pandas(tqdm_notebook)\npage_data['TextData'] = page_data.progress_apply(fetch_wikipedia_text_content, axis=1)\n\npage_data.head()","metadata":{"_execution_state":"idle","_uuid":"0d6e195e63cc988a1a4d8a5ee6a39c15b25820ae","collapsed":true,"_cell_guid":"89182bc3-d6ed-43c9-a8fe-bff0898c439f"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"name":"python","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":1}