{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Web Traffic Time Series Forecasting\n\n**Forecast future traffic to Wikipedia pages**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Collecting DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"base_url = '/kaggle/input/web-traffic-time-series-forecasting/'\n\nkey_1 = pd.read_csv(base_url+'key_1.csv')\ntrain_1 = pd.read_csv(base_url+'train_1.csv')\nsample_submission_1 = pd.read_csv(base_url+'sample_submission_1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_1.shape, key_1.shape, sample_submission_1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analisys (EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of wikipedia main sites \nsites = [\"wikipedia.org\", \"commons.wikimedia.org\", \"www.mediawiki.org\"]\n\n# Function to create a new column having the site part of the article page\ndef filter_by_site(page):\n    for site in sites:\n        if site in page:\n            return site\n\n# Creating a new column having the site part of the article page\ntrain_1['Site'] = train_1.Page.apply(filter_by_site)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1['Site'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nplt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Sites\", fontsize=\"18\")\ntrain_1['Site'].value_counts().plot.bar(rot=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking which country codes exist in the article pages\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-3:].value_counts().index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a list of country codes\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking which agents + access exist in the article pages and creating a list with them\ntrain_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the list of country codes and agents\ncountries = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,0].str[-2:].value_counts().index.to_list()[0:7]\nagents = train_1.Page.str.split(pat=\".wikipedia.org\", expand=True).iloc[:,1].str[1:].value_counts().index.to_list()\n\n# Function to create a new column having the country code part of the article page\ndef filter_by_country(page):\n    for country in countries:\n        if \"_\"+country+\".\" in page:\n            return country\n\n# Creating a new column having the country code part of the article page\ntrain_1['Country'] = train_1.Page.apply(filter_by_country)\n\n# Function to create a new column having the agent + access part of the article page\ndef filter_by_agent(page):\n    for agent in agents:\n        if agent in page:\n            return agent\n\n# Creating a new column having the agent part of the article page\ntrain_1['Agent'] = train_1.Page.apply(filter_by_agent)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Understanding what are the NaN values for the Country column\n# It seems that the URL page does not contain the country code for those cases\n\npd.DataFrame(train_1.Page[train_1['Country'].isna() == True]).sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Country\", fontsize=\"18\")\ntrain_1['Country'].value_counts(dropna=False).plot.bar(rot=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1['Agent'].value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of Wikipedia Articles by Agents/Access\", fontsize=\"18\")\ntrain_1['Agent'].value_counts().plot.bar(rot=0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a sample dataset from the Train dataset for analysis\ntrain_1_sample = train_1.drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT = train_1_sample.drop('Page', axis=1).T\ntrain_1_sampleT.columns = train_1_sample.Page.values\ntrain_1_sampleT.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_sampleT.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample dataset \nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample dataset at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT.columns:\n    plt.plot(train_1_sampleT[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the histograms for the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    sns.distplot(train_1_sampleT[v])\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking that the number of visits to the Wikipedia Articles have Gaussian Distribution (p-value=0)\nfrom scipy.stats import kstest, ks_2samp\n\npages = list(train_1_sampleT.columns)\n\nprint(\"Kolgomorov-Smirnov - Normality Test\")\nprint()\n\nfor p in pages:\n    print(p,':', kstest(train_1_sampleT[p], 'norm', alternative = 'less'))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** the results shows p-values equals to zero, hence visits to the wikipedia articles for the extracted sample have normal distributions."},{"metadata":{},"cell_type":"markdown","source":"### Exploring Groups of Time Series for Different Sites     "},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of the main Wikipedia Article sites\nsites","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating sample datasets from the train dataset and filtering them by sites\ntrain_1_sample_site0 = train_1[train_1['Site'] == sites[0]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site1 = train_1[train_1['Site'] == sites[1]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\ntrain_1_sample_site2 = train_1[train_1['Site'] == sites[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing them to have the Date Time as index\ntrain_1_sampleT_site0 = train_1_sample_site0.drop('Page', axis=1).T\ntrain_1_sampleT_site0.columns = train_1_sample_site0.Page.values\ntrain_1_sampleT_site1 = train_1_sample_site1.drop('Page', axis=1).T\ntrain_1_sampleT_site1.columns = train_1_sample_site1.Page.values\ntrain_1_sampleT_site2 = train_1_sample_site2.drop('Page', axis=1).T\ntrain_1_sampleT_site2.columns = train_1_sample_site2.Page.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Series of \"WIKIPEDIA.ORG\" sites only**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site0.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site0[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site0.columns:\n    plt.plot(train_1_sampleT_site0[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Series of \"COMMONS.WIKIMEDIA.ORG\" sites only**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site1.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site1[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site1.columns:\n    plt.plot(train_1_sampleT_site1[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Time Series of \"WWW.MEDIAWIKI.ORG\" sites only**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_site2.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_site2[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_site2.columns:\n    plt.plot(train_1_sampleT_site2[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_1_sampleT_site2.columns[4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notes:\n\nFor all the sites samples, some series presented missing data (NaNs).\n\nFor one of the WWW.MEDIAWIKI.ORG Series sample, noticed there was no data at all.  \nFor this series, the URL contains the IP address instead of DNS name and it starts with \"User:\""},{"metadata":{},"cell_type":"markdown","source":"### Exploring a Group of Time Series for a Specific Country - DE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of the Wikipedia Article country codes\ncountries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a sample dataset from the train dataset for countries having \"de\" code\ntrain_1_sample_de = train_1[train_1['Country'] == countries[2]].drop(['Site','Country','Agent'], axis=1).sample(6, random_state=42)\n\n# Transposing the sample dataset to have Date Time at the index\ntrain_1_sampleT_de = train_1_sample_de.drop('Page', axis=1).T\ntrain_1_sampleT_de.columns = train_1_sample_de.Page.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample dataset\nplt.figure(figsize=(16,8))\n\nfor k, v in enumerate(train_1_sampleT_de.columns):\n    plt.subplot(2, 3, k + 1)\n    plt.title( str(v.split(\".org\")[0])+\".org\"+\"\\n\"+str(v.split(\".org\")[1]) )\n    train_1_sampleT_de[v].plot()\n\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the Series from the sample datasets at the same graph\nplt.figure(figsize=(15,8))\n\nfor v in train_1_sampleT_de.columns:\n    plt.plot(train_1_sampleT_de[v])\n    plt.legend(loc='upper center');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Time Series - Lags"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Picked up one Time Series for the prophet modeling\ntrain_1_sampleT.columns[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dataframe for the Time Series from the train_1 samples dataset\ndata = pd.DataFrame(train_1_sampleT.iloc[:,1].copy())\ndata.columns = ['y']\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nplt.plot(data.y.values, label=\"actual\", linewidth=2.0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the lag of the target variable from 1 step back up to 7\nfor i in range(1, 8):\n    data[\"lag_{}\".format(i)] = data.y.shift(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling with Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import TimeSeriesSplit ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for time-series cross-validation set 3 folds\n# ~180 days by fold from total of 550 days\ntscv = TimeSeriesSplit(n_splits=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\n# reserve 33% of data for testing\n# so test size would be ~180 days\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"* First we need to define the evaluation metric functions.\n\n**MAPE - Mean Absolute Percentage Error:**\n\n$$MAPE = \\frac{100}{n}\\sum\\limits_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{y_i}$$\n\n**SMAPE - Symmetric Mean Absolute Percentage Error:**\n\n$$ SMAPE = \\frac{100\\%}{n} \\sum_{t=1}^{n} \\frac{\\left|F_t - A_t\\right|}{(\\left|A_t\\right|+\\left|F_t\\right|)/2} $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for the MAPE error\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n# Function for the SMAPE error\ndef smape(y_true, y_pred):\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return 200 * np.mean(diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below are the function to plot the Model results and its coeficients."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        Plots modelled vs fact values, prediction intervals and anomalies\n    \n    \"\"\"\n    \n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(15, 7))\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_absolute_error\")\n        mae = cv.mean() * (-1)\n        deviation = cv.std()\n        \n        scale = 1.96\n        lower = prediction - (mae + scale * deviation)\n        upper = prediction + (mae + scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    mape_error = mean_absolute_percentage_error(prediction, y_test)\n    smape_error = smape(prediction, y_test)\n    plt.title(\"MAPE: \"+str(mape_error)+\"\\n\"+\"SMAPE: \"+str(smape_error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);\n    \ndef plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the Linear Regression Model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"plotModelResults(lr, plot_intervals=True)\nplotCoefficients(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aditional Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.index = pd.to_datetime(data.index)\ndata[\"weekday\"] = data.index.weekday\ndata['is_weekend'] = data.weekday.isin([5,6])*1\ndata.tail(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 5))\nplt.title(\"Encoded features\")\n#data.weekday.plot()\ndata.is_weekend.plot()\nplt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling Data"},{"metadata":{},"cell_type":"markdown","source":"After adding \"weekday\" and \"is_weekend\" features, we have different scales in data values.  \n\nHence, we need to tranform data to the same scale to continue the analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.dropna().y\nX = data.dropna().drop(['y'], axis=1)\n\nX_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_scaled.shape, y_train.shape, X_test_scaled.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression Model plus Additional Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression using Scaled Data\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**  \n\nThe MAPE and SMAPE errors decreased a litte bit, from 18.18% and 18.81% to 17.30% and 17.90%.  \n\nAlso the \"**is_weekend**\" added feature showed up as useful resource, while the \"**weekday**\" added feature not contributed so much. "},{"metadata":{},"cell_type":"markdown","source":"## Regularization - Ridge and Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(X_train.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV, RidgeCV\n\nridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(ridge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = LassoCV(cv=tscv)\nlasso.fit(X_train_scaled, y_train)\n\nplotModelResults(lasso, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lasso)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\nRegularization did not change neither helped to improve the results in this case."},{"metadata":{},"cell_type":"markdown","source":"## Modeling with XGboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor \n\nxgb = XGBRegressor()\nxgb.fit(X_train_scaled, y_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotModelResults(xgb, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:**\n\nThe MAPE and SMAPE errors were a litte bit bigger, increasing from 17.30% and 17.90% to 18.03% and 18.68%."},{"metadata":{},"cell_type":"markdown","source":"### Prediction for the next 60 days"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dataframe for the Time Series from the train_1 samples dataset\ndata = pd.DataFrame(train_1_sampleT.iloc[:,1].copy())\ndata.columns = ['y']\ndata.index = pd.to_datetime(data.index)\nfuture = pd.DataFrame(index=pd.date_range(start='2017-01-01', end='2017-03-01'), columns=data.columns).fillna(0)\ndata_future = data.append(future)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(1, 61):\n    data_future[\"lag_{}\".format(i)] = data_future.y.shift(i)\n\ndata_future[\"weekday\"] = data_future.index.weekday\ndata_future['is_weekend'] = data_future.weekday.isin([5,6])*1\ndata_future.tail(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_future.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data_future.iloc[:550,:].dropna().drop(['y'], axis=1)\ny_train = data_future.iloc[:550,:].dropna().y\n\nX_test = data_future.iloc[550:,:].dropna().drop(['y'], axis=1)\ny_test = data_future.iloc[550:,:].dropna().y\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True)\nplotCoefficients(lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate Time Series models"},{"metadata":{},"cell_type":"markdown","source":"I could be using Multivariate Time Series (MTS) instead of the univariate models against all Time Series.  \nFollowing this approach, below are some ideas I could try in the future:\n\n- Vector Auto Regression (VAR)\n  - Johansenâ€™s test for checking the stationarity of any multivariate time series data  \n    (statsmodels.tsa.vector_ar.vecm import coint_johansen)\n  - Fit the model using VAR model from statsmodel library  \n    (from statsmodels.tsa.vector_ar.var_model import VAR)  \n- Random Forest  \n- Recurrent Neural Networs (RNN)  \n\nSources:  \n\n<a href=\"https://link.medium.com/miaEiLC0c1\">A Multivariate Time Series Guide to Forecasting and Modeling (with Python codes)</a>)  \n<a href=\"https://towardsdatascience.com/multivariate-time-series-forecasting-using-random-forest-2372f3ecbad1\">Multivariate Time Series Forecasting Using Random Forest</a>)  \n<a href=\"https://link.medium.com/XFbTA4O0c1\">Interpreting recurrent neural networks on multivariate time series</a>"},{"metadata":{},"cell_type":"markdown","source":"## Multiple Time Series in parallel  \n\nAnother idea could be the use of Python multiprocessing package to forecast multiple Time Series in parallel.  \n\nSource:  \n\n<a href=\"https://medium.com/spikelab/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245\">Forecasting multiple time-series using Prophet in parallel</a>"},{"metadata":{},"cell_type":"markdown","source":"## Submitting to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\"\n# train_1_sampleT.columns[1]+\"_\"+\"2017-01-01\" in list(key_1.Page.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"toc":{"base_numbering":1,"nav_menu":{"height":"538px","width":"563px"},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}