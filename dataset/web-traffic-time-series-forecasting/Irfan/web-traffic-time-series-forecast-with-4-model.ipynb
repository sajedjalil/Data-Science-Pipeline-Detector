{"nbformat":4,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"b9e54ae2e59bbe01fae9ae8538f9051a7cc5523f","collapsed":true,"_cell_guid":"a2b0c508-7688-4ed9-9acf-b95b4a9f2dd3"},"source":"# Web Traffic Time Series Forecasting (Experimenting with different method)\n\nBy Lai Yiu Ming, Tom\n\n1. Introduction\n    1. Competition details\n    2. Load libraries and data files, file structure and content\n    3. Missing values\n    4. Data visualization\n    5. Extreme data\n2. Data transformation and helper functions\n    1. Article names and metadata\n    2. Split into train and validation dataset\n3. Forecast methods\n    1. SMAPE, the measurement\n    2. Simple median model\n    3. Median model - weekday, weekend and holiday\n    4. ARIMA model\n    5. Facebook prophet model\n    6. Sample series analysis (For script reconciliation)\n4. Selected model performance (validation score) over train dataset\n    1. Simple median model\n    2. Median model - weekday, weekend, holiday\n    3. ARIMA model\n    4. Facebook model\n    5. mixed model"},{"cell_type":"markdown","metadata":{"_uuid":"466341ff90ee4d39548c2f90174d6d5933c2c62d","_cell_guid":"80d89790-42b4-4117-82c0-4452210f13f7"},"source":"# 1. Introduction \n\n## A. Competition details\n\n### First stage\n* Training data from 2015-07-01 to 2016-12-29\n* Testing data from 2017-01-01 to 2017-03-01\n* Length of training vs length of testing = 547 days vs 59 days\n* Predict interval is ~10.7% of the training interval\n\n### Second stage\n* Training data from 2015-07-01 to 2017-09-01\n* Testing data from 2017-09-10 to 2017-11-10\n* Length of training vs length of testing = 793 days vs 61 days\n* Predict interval is ~7.7% of the training interval"},{"cell_type":"markdown","metadata":{"_uuid":"10592e734dd2568886858d69960a1e2c8f3f5ed9","_cell_guid":"323c4a34-6421-4288-ac99-2916900dd11e"},"source":"## B. Load libraries and data files, file structure and content"},{"cell_type":"code","metadata":{"_uuid":"28610d78b73a0eb79868e208dd46f91045c7bb75","collapsed":true,"_cell_guid":"102debc2-87d8-41c4-bfac-82508f003abb"},"execution_count":null,"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport math as math\n\n%matplotlib inline","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"9d9e2a33b5f7e70d1e3f080c01ef8b4f28a14bc6","collapsed":true,"_cell_guid":"a7bd9791-f58e-4b4b-ad28-dab934014da8"},"execution_count":null,"source":"# Load the data\ntrain = pd.read_csv(\"../input/web-traffic-time-series-forecasting/train_1.csv\")\nkeys = pd.read_csv(\"../input/web-traffic-time-series-forecasting/key_1.csv\")\nss = pd.read_csv(\"../input/web-traffic-time-series-forecasting/sample_submission_1.csv\")","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"f340420a80c636df3d8c6e45f07a418bd0d57583","collapsed":true,"_cell_guid":"7cf073a9-c545-4199-865e-8ffd911cb53b"},"execution_count":null,"source":"train.head()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"9ffedd5f603ceafcd6e6b192f813f64b22fdc9e9","_cell_guid":"9c72cdb3-e2ce-4ad5-8fbd-4cf501441145"},"source":"## C. Missing values"},{"cell_type":"code","metadata":{"_uuid":"c1be90c632f06a95e31a19a6305d4b204fe0c3dd","collapsed":true,"_cell_guid":"b897047e-daa0-49e7-8f60-2bb46487b9a6"},"execution_count":null,"source":"# Check the data\nprint(\"Check the number of records\")\nprint(\"Number of records: \", train.shape[0], \"\\n\")\n\nprint(\"Null analysis\")\nempty_sample = train[train.isnull().any(axis=1)]\nprint(\"Number of records contain 1+ null: \", empty_sample.shape[0], \"\\n\")","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"7ff43c83dacae8bbdea49114e98c73808628dba5","collapsed":true,"scrolled":false,"_cell_guid":"e47c83b5-c349-45c0-be65-a333871f14fc"},"execution_count":null,"source":"empty_sample.iloc[np.r_[0:10, len(empty_sample)-10:len(empty_sample)]]","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"5375f461500b9994ec73ee54bbb50f9dfa566708","_cell_guid":"d6370b56-3177-46af-a1f3-58c1d2c8eb6a"},"source":"## D. Data visualization"},{"cell_type":"code","metadata":{"_uuid":"73b03e85ebe22e512ed840f44f9c87fbed17a2b7","collapsed":true,"_cell_guid":"d5a55e88-28ab-4d0d-82a4-d912295cf3a3"},"execution_count":null,"source":"# plot 3 the time series\ndef plot_time_series(df, row_num, start_col =1, ax=None):\n    if ax is None:\n            fig = plt.figure(facecolor='w', figsize=(10, 6))\n            ax = fig.add_subplot(111)\n    else:\n        fig = ax.get_figure()\n        \n    series_title = df.iloc[row_num, 0]\n    sample_series = df.iloc[row_num, start_col:]\n    sample_series.plot(style=\".\", ax=ax)\n    ax.set_title(\"Series: %s\" % series_title)\n\nfig, axs  = plt.subplots(4,1,figsize=(12,12))\nplot_time_series(empty_sample, 1, ax=axs[0])\nplot_time_series(empty_sample, 10, ax=axs[1])\nplot_time_series(empty_sample, 100, ax=axs[2])\nplot_time_series(empty_sample, 1005, ax=axs[3])\n\nplt.tight_layout()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"b05fff7f45fcd175a98d9fbfc51c1129f92ab752","_cell_guid":"731719c5-3a82-46cb-8e95-a72f342e97cb"},"source":"## E. Extreme data"},{"cell_type":"code","metadata":{"_uuid":"53dc5fd82f094abb5775b8eab7ac320a502b5a8d","collapsed":true,"_cell_guid":"9c2b896d-d160-4097-9ff3-0202896cd15f"},"execution_count":null,"source":"# series with all NaN\nempty_sample.iloc[1000:1010]","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e8471f75073027f67fdf8bd9d2427d19451af98b","_cell_guid":"6feda8d9-9b89-4c3a-ac5a-4c15ea2b1f8a"},"source":"# 2. Data transformation and helper functions\n## A. Article names and metadata"},{"cell_type":"code","metadata":{"_uuid":"89747bd76d2f750f36e7335f2012262f5926edd0","collapsed":true,"_cell_guid":"8682bf5e-f60f-45d3-8df3-88a0996f4031"},"execution_count":null,"source":"import re\n\ndef breakdown_topic(str):\n    m = re.search('(.*)\\_(.*).wikipedia.org\\_(.*)\\_(.*)', str)\n    if m is not None:\n        return m.group(1), m.group(2), m.group(3), m.group(4)\n    else:\n        return \"\", \"\", \"\", \"\"\n\nprint(breakdown_topic(\"Рудова,_Наталья_Александровна_ru.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"台灣災難列表_zh.wikipedia.org_all-access_spider\"))\nprint(breakdown_topic(\"File:Memphis_Blues_Tour_2010.jpg_commons.wikimedia.org_mobile-web_all-agents\"))","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8558f96774bc5dcb78a31e936862c3933d254336","collapsed":true,"_cell_guid":"da27ba91-e49b-4a0f-bdb5-3cb649b69f80"},"execution_count":null,"source":"page_details = train.Page.str.extract(r'(?P<topic>.*)\\_(?P<lang>.*).wikipedia.org\\_(?P<access>.*)\\_(?P<type>.*)')\n\npage_details[0:10]","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"540494ecf1658f7649e0a9043b64c6ba274643e5","collapsed":true,"_cell_guid":"949a7a7c-010c-4792-af07-66016a4a6f86"},"execution_count":null,"source":"unique_topic = page_details[\"topic\"].unique()\nprint(unique_topic)\nprint(\"Number of distinct topics: \", unique_topic.shape[0])","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"04fb2f9495a2b53e554af13c9b5cc30dd5d731e0","collapsed":true,"_cell_guid":"93e6ec07-6165-410f-a753-a83780c8a5c5"},"execution_count":null,"source":"fig, axs  = plt.subplots(3,1,figsize=(12,12))\n\npage_details[\"lang\"].value_counts().sort_index().plot.bar(ax=axs[0])\naxs[0].set_title('Language - distribution')\n\npage_details[\"access\"].value_counts().sort_index().plot.bar(ax=axs[1])\naxs[1].set_title('Access - distribution')\n\npage_details[\"type\"].value_counts().sort_index().plot.bar(ax=axs[2])\naxs[2].set_title('Type - distribution')\n\nplt.tight_layout()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"35d2c27cfe8db4b0881b403d57773156eccc69c2","_cell_guid":"06e8a5ac-b792-4abc-978d-fef1c9c6d8f4"},"source":"## B. Split into train and validation dataset"},{"cell_type":"code","metadata":{"_uuid":"aceb98a79ee58d03db63929da691a08fe8ccfe5e","collapsed":true,"_cell_guid":"8b2a5205-893c-4a57-87d9-f7117915d46c"},"execution_count":null,"source":"# Generate train and validate dataset\ntrain_df = pd.concat([page_details, train], axis=1)\n\ndef get_train_validate_set(train_df, test_percent):\n    train_end = math.floor((train_df.shape[1]-5) * (1-test_percent))\n    train_ds = train_df.iloc[:, np.r_[0,1,2,3,4,5:train_end]]\n    test_ds = train_df.iloc[:, np.r_[0,1,2,3,4,train_end:train_df.shape[1]]]\n    \n    return train_ds, test_ds\n\nX_train, y_train = get_train_validate_set(train_df, 0.1)\n\nprint(\"The training set sample:\")\nprint(X_train[0:10])\nprint(\"The validation set sample:\")\nprint(y_train[0:10])","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f11bf9c1d853839b7ee1f1d1403e70a57904ad65","_cell_guid":"d1756260-6638-41c6-b6b8-09d7574d948a"},"source":"# 3 Forecast methods\n\nIn this section, I will show some popular methods in predicting time series. (Will work on XGBoost if I finish my study)."},{"cell_type":"code","metadata":{"_uuid":"005f5e7d99ebfc8fe6ebc11921d031974b4f03b7","collapsed":true,"_cell_guid":"1dd91743-c36a-454e-9d48-520653ade0eb"},"execution_count":null,"source":"def extract_series(df, row_num, start_idx):\n    y = df.iloc[row_num, start_idx:]\n    df = pd.DataFrame({ 'ds': y.index, 'y': y.values})\n    return df","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"e02a22b21b2ed36585d4c5c84ffde3b21e7e3fcd","_cell_guid":"4287b753-9bfd-4a16-aa70-272e4bf2050e"},"source":"## A. SMAPE, the measurement\n\nSMAPE is harsh when the series is near zero.\nA notebook https://www.kaggle.com/cpmpml/smape-weirdness give a very good visualization of the SMAPE function.\n\nAfter you find that there is no way to further improve quality of the result, you may consider doing a little bit hacking on SMAPE to give you better score."},{"cell_type":"code","metadata":{"_uuid":"1b9849d8bdc4e7d2877acfbc3e84d512a6f573ee","collapsed":true,"_cell_guid":"39c51bf7-2449-4bbc-a787-5404a144dbe2"},"execution_count":null,"source":"def smape(predict, actual, debug=False):\n    '''\n    predict and actual is a panda series.\n    In this implementation I will skip all the datapoint with actual is null\n    '''\n    actual = actual.fillna(0)\n    data = pd.concat([predict, actual], axis=1, keys=['predict', 'actual'])\n    data = data[data.actual.notnull()]\n    if debug:\n        print('debug', data)\n    \n    evals = abs(data.predict - data.actual) * 1.0 / (abs(data.predict) + abs(data.actual)) * 2\n    evals[evals.isnull()] = 0\n    #print(np.sum(evals), len(data), np.sum(evals) * 1.0 / len(data))\n    \n    result = np.sum(evals) / len(data)\n    \n    return result\n\n# create testing series\ntesting_series_1 = X_train.iloc[0, 5:494]\ntesting_series_2 = X_train.iloc[0, 5:494].shift(-1)\ntesting_series_3 = X_train.iloc[1, 5:494]\ntesting_series_4 = pd.Series([0,0,0,0])","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"26889456ad3d2f4f2566a9b5318565546974db77","collapsed":true,"scrolled":true,"_cell_guid":"fc669ec6-3568-4c10-b17f-16c159663852"},"execution_count":null,"source":"random_series_1 = pd.Series(np.repeat(3, 500))\nrandom_series_2 = pd.Series(np.random.normal(3, 1, 500))\nrandom_series_3 = pd.Series(np.random.normal(500, 20, 500))\nrandom_series_4 = pd.Series(np.repeat(500, 500))\n\n# testing 1 same series\nprint(\"\\nSMAPE score to predict a constant array of 3\")\nprint(\"Score (same series): %.3f\" % smape(random_series_1, random_series_1))\nprint(\"Score (same series - 1) %.3f\" % smape(random_series_1, random_series_1-1))\nprint(\"Score (same series + 1) %.3f\" % smape(random_series_1, random_series_1+1))\n\n# testing 2 same series shift by one\nprint(\"\\nSMAPE score to predict a array of normal distribution around 3\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_2, random_series_1))\nprint(\"Score (random vs mean-1) %.3f\" % smape(random_series_2, random_series_2-1))\nprint(\"Score (random vs mean+1) %.3f\" % smape(random_series_2, random_series_2+1))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_2, random_series_2*0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_2, random_series_2*1.1))\n\n# testing 3 totally different series\nprint(\"\\nSMAPE score to predict a array of normal distribution around 500\")\nprint(\"Score (random vs mean) %.3f\" % smape(random_series_3, random_series_4))\nprint(\"Score (random vs mean-20) %.3f\" % smape(random_series_3, random_series_3-20))\nprint(\"Score (random vs mean+20) %.3f\" % smape(random_series_3, random_series_3+20))\nprint(\"Score (random vs mean*0.9) %.3f\" % smape(random_series_3, random_series_3*0.9))\nprint(\"Score (random vs mean*1.1) %.3f\" % smape(random_series_3, random_series_3*1.1))\n","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"07b01ca5365bd2537a9cc83827e5b1b974c5fefb","collapsed":true,"_cell_guid":"74171c24-cc33-4229-bbe7-9af4fa486a1e"},"execution_count":null,"source":"y_true_1 = pd.Series(np.random.normal(1, 1, 500))\ny_true_2 = pd.Series(np.random.normal(2, 1, 500))\ny_true_3 = pd.Series(np.random.normal(3, 1, 500))\ny_pred = pd.Series(np.ones(500))\nx = np.linspace(0,10,1000)\nres_1 = list([smape(y_true_1, i * y_pred) for i in x])\nres_2 = list([smape(y_true_2, i * y_pred) for i in x])\nres_3 = list([smape(y_true_3, i * y_pred) for i in x])\nplt.plot(x, res_1, color='b')\nplt.plot(x, res_2, color='r')\nplt.plot(x, res_3, color='g')\nplt.axvline(x=1, color='k')\nplt.axvline(x=2, color='k')\nplt.axvline(x=3, color='k')","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"2e92732c900a79072586caeb9a275319f879dc77","_cell_guid":"d803e04a-a1f1-486c-a437-7ef813fa95c4"},"source":"## B. Simple median model"},{"cell_type":"code","metadata":{"_uuid":"f9df34726d19532211e00897c5a342902a5013e1","collapsed":true,"_cell_guid":"c345d9f2-5bb1-40ca-b3f8-d97c57ad1ba2"},"execution_count":null,"source":"def plot_prediction_and_actual_2(train, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax  = plt.subplots(1,1,figsize=figSize)\n    ax.plot(pd.to_datetime(train.index), train.values, 'k.')\n    ax.plot(pd.to_datetime(actual.index), actual.values, 'r.')\n    ax.plot(pd.to_datetime(forecast.index), forecast.values, 'b-')\n    ax.set_title(title)\n    plt.show()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8b8f71ccfaa3830f44e3f99d0c1e526ad4b65e3c","collapsed":true,"_cell_guid":"f4d0da8a-7064-4401-86aa-bdaa20160eb1"},"execution_count":null,"source":"def median_model(df_train, df_actual, p, review=False, figSize=(12, 4)):\n    \n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train['y'] = df_train['y'].convert_objects(convert_numeric=True)\n    df_actual['y'] = df_actual['y'].convert_objects(convert_numeric=True)\n    visits = nanmedian_zero(df_train['y'].values[-p:])\n    train_series = df_train['y']\n    train_series.index = df_train.ds\n    \n    idx = np.arange( p) + np.arange(len(df_train)- p+1)[:,None]\n    b = [row[row>=0] for row in df_train.y.values[idx]]\n    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n    pre_forecast.index = df_train.ds\n    \n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual.ds\n    \n    forecast_series = pre_forecast.append(forecast_series)\n    \n    actual_series = df_actual.y\n    actual_series.index = df_actual.ds\n    \n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model')\n    \n    return smape(forecast_series, actual_series)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"38904abe75b4ccce7f9a481841bc7ef67703aea3","collapsed":true,"_cell_guid":"e0b6ed0f-c108-48b6-9a91-c1dcec6e5ec5"},"execution_count":null,"source":"# This is to demo the median model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_model(df_train.copy(), df_actual.copy(), 15, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"7894159ebfcaced527c078a8e209530625f4df40","_cell_guid":"0f97bda6-75b5-41c2-a21e-acd7eaa7abe7"},"source":"## C. Median model - weekday, weekend and holiday"},{"cell_type":"code","metadata":{"_uuid":"7b9cb56a8fee5548340ce23db7f7446f3e83c3ed","collapsed":true,"_cell_guid":"d8710782-db8d-46bc-9442-796cec9550d7"},"execution_count":null,"source":"# holiday variable\n#holiday_en = ['2015-01-01', '2015-01-19', '2015-04-03', '2015-05-04', '2015-05-25', '2015-07-01', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-01-18', '2016-03-25', '2016-05-02', '2016-05-30', '2016-07-01', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-16', '2017-04-14', '2017-05-01', '2017-05-29', '2017-07-01', '2017-07-03', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25', '2017-12-26']\n\nholiday_en_us = ['2015-01-01', '2015-01-19', '2015-05-25', '2015-07-03', '2015-09-07', '2015-11-26', '2015-11-27', '2015-12-25', '2016-01-01', '2016-01-18', '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-11', '2016-11-24', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-16', '2017-05-29', '2017-07-04', '2017-09-04', '2017-11-10', '2017-11-23', '2017-12-25']\nholiday_en_uk = ['2015-01-01', '2015-04-03', '2015-05-04', '2015-05-25', '2015-12-25', '2015-12-26', '2015-12-28', '2016-01-01', '2016-03-25', '2016-05-02', '2016-05-30', '2016-12-26', '2016-12-27', '2017-01-01', '2017-04-14', '2017-05-01', '2017-05-29', '2017-12-25', '2017-12-26']\nholiday_en_canada = ['2015-01-01', '2015-07-01', '2015-09-07', '2015-12-25', '2016-01-01', '2016-07-01', '2016-09-05', '2016-12-25', '2017-01-01', '2017-07-01', '2017-07-03', '2017-09-04', '2017-12-25']\n\nholiday_ru_russia = ['2015-01-01', '2015-01-02', '2015-01-05', '2015-01-06', '2015-01-07', '2015-01-08', '2015-01-09', '2015-02-23', '2015-03-09', '2015-05-01', '2015-05-04', '2015-05-09', '2015-05-11', '2015-06-12', '2015-11-04', '2016-01-01', '2016-01-04', '2016-01-05', '2016-01-06', '2016-01-07', '2016-02-22', '2016-02-23', '2016-03-08', '2016-05-01', '2016-05-09', '2016-06-12', '2016-06-13', '2016-11-04', '2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-02-23', '2017-02-24', '2017-03-08', '2017-05-01', '2017-05-08', '2017-05-09', '2017-06-12', '2017-11-04', '2017-11-06']\n#holiday_es = ['2015-01-01', '2015-01-06', '2015-01-12', '2015-02-02', '2015-03-16', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-09-16', '2015-10-12', '2015-11-01', '2015-11-02', '2015-11-16', '2015-12-06', '2015-12-08', '2015-12-12', '2015-12-25', '2016-01-01', '2016-01-06', '2016-01-11', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-09-16', '2016-10-12', '2016-10-17', '2016-11-01', '2016-11-02', '2016-11-07', '2016-11-14', '2016-11-21', '2016-12-06', '2016-12-08', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-01-06', '2017-01-09', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-09-16', '2017-10-12', '2017-10-16', '2017-11-01', '2017-11-02', '2017-11-06', '2017-11-13', '2017-11-20', '2017-12-06', '2017-12-08', '2017-12-12', '2017-12-25']\n\nholiday_es_mexico = ['2015-01-01', '2015-02-02', '2015-03-16', '2015-04-02', '2015-04-03', '2015-05-01', '2015-09-16', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-12', '2015-12-25', '2016-01-01', '2016-02-01', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-09-16', '2016-10-12', '2016-11-02', '2016-11-21', '2016-12-12', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-02', '2017-02-06', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-09-16', '2017-10-12', '2017-11-02', '2017-11-20', '2017-12-12', '2017-12-25']\nholiday_es_spain = ['2017-01-01', '2017-01-06', '2017-04-14', '2017-05-01', '2017-08-15', '2017-10-12', '2017-11-01', '2017-12-06', '2017-12-08', '2017-12-25', '2016-01-01', '2016-01-06', '2016-03-25', '2016-05-01', '2016-08-15', '2016-10-12', '2016-11-01', '2016-12-06', '2016-12-08', '2016-12-25', '2015-01-01', '2015-01-06', '2015-04-03', '2015-05-01', '2015-10-12', '2015-11-01', '2015-12-06', '2015-12-08', '2015-12-25']\nholiday_es_colombia = ['2015-01-01', '2015-01-12', '2015-03-23', '2015-04-02', '2015-04-03', '2015-05-01', '2015-05-18', '2015-06-08', '2015-06-15', '2015-06-29', '2015-07-20', '2015-08-07', '2015-08-17', '2015-10-12', '2015-11-02', '2015-11-16', '2015-12-08', '2015-12-25', '2016-01-01', '2016-01-11', '2016-03-21', '2016-03-24', '2016-03-25', '2016-05-01', '2016-05-09', '2016-05-30', '2016-06-06', '2016-07-04', '2016-07-20', '2016-08-07', '2016-08-15', '2016-10-17', '2016-11-07', '2016-11-14', '2016-12-08', '2016-12-25', '2017-01-01', '2017-01-09', '2017-03-20', '2017-04-13', '2017-04-14', '2017-05-01', '2017-05-29', '2017-06-19', '2017-06-26', '2017-07-03', '2017-07-20', '2017-08-07', '2017-08-15', '2017-10-16', '2017-11-06', '2017-11-13', '2017-12-08', '2017-12-25']\n\nholiday_fr_france = ['2015-01-01', '2015-04-06', '2015-05-01', '2015-05-08', '2015-05-14', '2015-05-25', '2015-07-14', '2015-08-15', '2015-11-01', '2015-11-11', '2015-12-25', '2016-01-01', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-08', '2016-05-16', '2016-07-14', '2016-08-15', '2016-11-01', '2016-11-11', '2016-12-25', '2017-01-01', '2017-04-17', '2017-05-01', '2017-05-08', '2017-05-25', '2017-06-05', '2017-07-14', '2017-08-15', '2017-11-01', '2017-11-11', '2017-12-25']\nholiday_jp_japan = ['2015-01-01', '2015-01-12', '2015-02-11', '2015-03-21', '2015-04-29', '2015-05-03', '2015-05-04', '2015-05-05', '2015-05-06', '2015-07-20', '2015-09-21', '2015-09-22', '2015-09-23', '2015-10-12', '2015-11-03', '2015-11-23', '2015-12-23', '2016-01-01', '2016-01-11', '2016-02-11', '2016-03-21', '2016-04-29', '2016-05-03', '2016-05-04', '2016-05-05', '2016-07-18', '2016-08-11', '2016-09-19', '2016-09-22', '2016-10-10', '2016-11-03', '2016-11-23', '2016-12-23', '2017-01-01', '2017-01-09', '2017-02-11', '2017-03-20', '2017-04-29', '2017-05-03', '2017-05-04', '2017-05-05', '2017-07-17', '2017-08-11', '2017-09-18', '2017-09-22', '2017-10-09', '2017-11-03', '2017-11-23', '2017-12-23']\n\n#holiday_de = ['2015-01-01', '2015-01-06', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-01', '2015-08-15', '2015-10-03', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-01', '2016-08-15', '2016-10-03', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-01', '2017-08-15', '2017-10-03', '2017-10-26', '2017-10-31', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\n\nholiday_de_germany = ['2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-14', '2015-05-25', '2015-10-03', '2015-12-25', '2015-12-26', '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-10-03', '2016-12-25', '2016-12-26', '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-10-03', '2017-10-31', '2017-12-25', '2017-12-26']\nholiday_de_austria = ['2015-01-01', '2015-01-06', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25', '2015-06-04', '2015-08-15', '2015-10-26', '2015-11-01', '2015-12-08', '2015-12-25', '2015-12-26', '2016-01-01', '2016-01-06', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16', '2016-05-26', '2016-08-15', '2016-10-26', '2016-11-01', '2016-12-08', '2016-12-25', '2016-12-26', '2017-01-01', '2017-01-06', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05', '2017-06-15', '2017-08-15', '2017-10-26', '2017-11-01', '2017-12-08', '2017-12-25', '2017-12-26']\nholiday_de_switzerland = ['2015-01-01', '2015-04-03', '2015-05-14', '2015-08-01', '2015-12-25', '2016-01-01', '2016-03-25', '2016-05-05', '2016-08-01', '2016-12-25', '2017-01-01', '2017-04-14', '2017-05-25', '2017-08-01', '2017-12-25']\n\n#holiday_zh = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-19', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-09', '2015-10-10', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-04-05', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-06-10', '2016-07-01', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-29', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-04', '2017-10-05', '2017-10-09', '2017-10-10', '2017-10-28', '2017-12-25', '2017-12-26']\n\nholiday_zh_hongkong = ['2015-01-01', '2015-02-19', '2015-02-20', '2015-04-03', '2015-04-04', '2015-04-05', '2015-04-06', '2015-04-07', '2015-05-01', '2015-05-25', '2015-06-20', '2015-07-01', '2015-09-03', '2015-09-28', '2015-10-01', '2015-10-21', '2015-12-25', '2015-12-26', '2016-01-01', '2016-02-08', '2016-02-09', '2016-02-10', '2016-03-25', '2016-03-26', '2016-03-28', '2016-04-04', '2016-05-01', '2016-05-02', '2016-05-14', '2016-06-09', '2016-07-01', '2016-09-16', '2016-10-01', '2016-10-10', '2016-12-25', '2016-12-26', '2016-12-27', '2017-01-01', '2017-01-02', '2017-01-28', '2017-01-30', '2017-01-31', '2017-04-04', '2017-04-14', '2017-04-15', '2017-04-17', '2017-05-01', '2017-05-03', '2017-05-30', '2017-07-01', '2017-10-01', '2017-10-02', '2017-10-05', '2017-10-28', '2017-12-25', '2017-12-26']\nholiday_zh_taiwan = ['2015-01-01', '2015-02-18', '2015-02-19', '2015-02-20', '2015-02-21', '2015-02-22', '2015-02-23', '2015-02-23', '2015-02-27', '2015-04-03', '2015-04-05', '2015-04-06', '2015-06-19', '2015-06-20', '2015-09-28', '2015-10-09', '2015-10-10', '2016-01-01', '2016-02-07', '2016-02-08', '2016-02-09', '2016-02-10', '2016-02-11', '2016-02-12', '2016-02-29', '2016-04-04', '2016-04-05', '2016-06-09', '2016-06-10', '2016-09-15', '2016-09-16', '2016-09-28', '2016-10-10', '2017-01-01', '2017-01-02', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30', '2017-01-31', '2017-02-01', '2017-02-27', '2017-02-28', '2017-04-03', '2017-04-04', '2017-05-01', '2017-05-29', '2017-05-30', '2017-10-04', '2017-10-09', '2017-10-10']\n\nholidays_en_us = pd.DataFrame({\n  'holiday': 'US public holiday',\n  'ds': pd.to_datetime(holiday_en_us),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_uk = pd.DataFrame({\n  'holiday': 'UK public holiday',\n  'ds': pd.to_datetime(holiday_en_uk),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en_canada = pd.DataFrame({\n  'holiday': 'Canada public holiday',\n  'ds': pd.to_datetime(holiday_en_canada),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_en = pd.concat((holidays_en_us, holidays_en_uk, holidays_en_canada))\n\nholidays_ru_russia = pd.DataFrame({\n  'holiday': 'Russia public holiday',\n  'ds': pd.to_datetime(holiday_ru_russia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_ru = holidays_ru_russia\n\nholidays_es_mexico = pd.DataFrame({\n  'holiday': 'Mexico public holiday',\n  'ds': pd.to_datetime(holiday_es_mexico),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_spain = pd.DataFrame({\n  'holiday': 'Spain public holiday',\n  'ds': pd.to_datetime(holiday_es_spain),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es_colombia = pd.DataFrame({\n  'holiday': 'Colombia public holiday',\n  'ds': pd.to_datetime(holiday_es_colombia),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_es = pd.concat((holidays_es_mexico, holidays_es_spain, holidays_es_colombia))\n\nholidays_fr_france = pd.DataFrame({\n  'holiday': 'France public holiday',\n  'ds': pd.to_datetime(holiday_fr_france),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_fr = holidays_fr_france\n\nholidays_jp_japan = pd.DataFrame({\n  'holiday': 'Japan public holiday',\n  'ds': pd.to_datetime(holiday_jp_japan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_jp = holidays_jp_japan\n\nholidays_de_germany = pd.DataFrame({\n  'holiday': 'Germany public holiday',\n  'ds': pd.to_datetime(holiday_de_germany),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_austria = pd.DataFrame({\n  'holiday': 'Austria public holiday',\n  'ds': pd.to_datetime(holiday_de_austria),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de_switzerland = pd.DataFrame({\n  'holiday': 'Switzerland public holiday',\n  'ds': pd.to_datetime(holiday_de_switzerland),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_de = pd.concat((holidays_de_germany, holidays_de_austria, holidays_de_switzerland))\n\nholidays_zh_hongkong = pd.DataFrame({\n  'holiday': 'HK public holiday',\n  'ds': pd.to_datetime(holiday_zh_hongkong),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh_taiwan = pd.DataFrame({\n  'holiday': 'Taiwan public holiday',\n  'ds': pd.to_datetime(holiday_zh_taiwan),\n  'lower_window': 0,\n  'upper_window': 0,\n})\n\nholidays_zh = pd.concat((holidays_zh_hongkong, holidays_zh_taiwan))\n\nholidays_dict = {\"en\": holidays_en, \n                 \"ru\": holidays_ru, \n                 \"es\": holidays_es, \n                 \"fr\": holidays_fr, \n                 \"ja\": holidays_jp,\n                 \"de\": holidays_de,\n                 \"zh\": holidays_zh}","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"ccb89f2b0cc92eedcd3b6d714c97ae4212bf5393","collapsed":true,"_cell_guid":"08c4b047-d6ae-4a08-98a6-ce55e85c7f39"},"execution_count":null,"source":"def median_holiday_model(df_train, df_actual, p, lang, review=False, figSize=(12, 4)):\n    # Split the train and actual set\n    \n        \n    df_train['ds'] = pd.to_datetime(df_train['ds'])\n    df_actual['ds'] = pd.to_datetime(df_actual['ds'])\n    train_series = df_train['y']\n    train_series.index = df_train.ds\n    \n    if(isinstance(lang, float) and math.isnan(lang)):\n        df_train['holiday'] = df_train.ds.dt.dayofweek >=5\n        df_actual['holiday'] = df_actual.ds.dt.dayofweek >=5\n    else:\n        df_train['holiday'] = (df_train.ds.dt.dayofweek >=5) | df_train.ds.isin(holidays_dict[lang].ds)\n        df_actual['holiday'] = (df_actual.ds.dt.dayofweek >=5) | df_actual.ds.isin(holidays_dict[lang].ds)\n     \n    # Combine the train and actual set\n    predict_holiday = median_holiday_helper(df_train, df_actual[df_actual.holiday], p, True)\n    predict_non_holiday = median_holiday_helper(df_train, df_actual[~df_actual.holiday], p, False)\n\n    forecast_series = predict_non_holiday.combine_first(predict_holiday)\n    \n    actual_series = df_actual.y\n    actual_series.index = df_actual.ds\n    \n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='Median model with holiday')\n    \n    return smape(forecast_series, actual_series)\n\n\ndef median_holiday_helper(df_train, df_actual, p, holiday):\n    def nanmedian_zero(a):\n        return np.nan_to_num(np.nanmedian(a))\n    \n    df_train['y'] = pd.to_numeric(df_train['y'])\n    df_actual['y'] = pd.to_numeric(df_actual['y'])\n    \n    sample = df_train[-p:]\n    if(holiday):\n        sample = sample[sample['holiday']]\n    else:\n        sample = sample[~sample['holiday']]\n\n    visits = nanmedian_zero(sample['y'])\n    \n    idx = np.arange( p) + np.arange(len(df_train)- p+1)[:,None]\n    b = [row[row>=0] for row in df_train.y.values[idx]]\n    pre_forecast = pd.Series(np.append(([float('nan')] * (p-1)), list(map(nanmedian_zero,b))))\n    pre_forecast.index = df_train.ds\n    \n    forecast_series = pd.Series(np.repeat(visits, len(df_actual)))\n    forecast_series.index = df_actual.ds\n    \n    forecast_series = pre_forecast.append(forecast_series)\n    \n    return forecast_series","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"79170ffdb313705aca01d50f5ab4b150809aeecf","collapsed":true,"_cell_guid":"51348547-d50e-4bb9-9b80-d437280a324a"},"execution_count":null,"source":"# This is to demo the median model - weekday, weekend and \nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = median_holiday_model(df_train.copy(), df_actual.copy(), 15, lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fffe6fb76a7606a0bdc4c7a6f38917dab2bb9cc4","_cell_guid":"8d3efd7d-ef3d-4e7e-9b93-e4da2cbc1c78"},"source":"## D. ARIMA model\n\nThe below use the ARIMA from a Python library statsmodels. Please refer to http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html for details.\n\nThe model is slow and may throw exception if the model cannot find a solution. (Make the model difficult to build for all series).\n\nWe will further investigate it performance in the later section."},{"cell_type":"code","metadata":{"_uuid":"f03c747e4520763f8e4d7eaf3cfca801ba3b1afc","collapsed":true,"_cell_guid":"ca14c193-4782-45bc-b438-d9d515cef043"},"execution_count":null,"source":"from statsmodels.tsa.arima_model import ARIMA   \nimport warnings\n\ndef arima_model(df_train, df_actual, p, d, q, figSize=(12, 4), review=False):\n    df_train = df_train.fillna(0)\n    train_series = df_train.y\n    train_series.index = df_train.ds\n\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        try:\n            arima = ARIMA(train_series ,[p, d, q])\n            result = arima.fit(disp=False)\n        except Exception as e:\n            print('\\tARIMA failed', e)\n                \n    #print(result.params)\n    start_idx = df_train.ds[d]\n    end_idx = df_actual.ds.max()\n    forecast_series = result.predict(start_idx, end_idx,typ='levels')\n    \n    actual_series = df_actual.y\n    actual_series.index = pd.to_datetime(df_actual.ds)\n\n    if(review):\n        plot_prediction_and_actual_2(train_series, forecast_series, actual_series, figSize=figSize, title='ARIMA model')\n    \n    return smape(forecast_series, actual_series)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"e16d0523074cfa0c3b87447f24ec9e238226b9b8","collapsed":true,"_cell_guid":"a42595cf-46d3-477e-9bb7-f0c44492dd21"},"execution_count":null,"source":"# This is to demo the ARIMA model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3905533849e82db4352abbef4d247d98df3ae258","_cell_guid":"eb67304c-ea85-49f6-9845-c5173ecd9766"},"source":"## E. Facebook prophet library\n\nFacebook prophet library is created by facebook and aims to create a human-friendly time series forecasting libary. For details, please refer to https://facebookincubator.github.io/prophet/\n\nThere are several favor, but I will focus on holiday, yearly and log model"},{"cell_type":"code","metadata":{"_uuid":"9f43ea61b480446da7cc51d73bf8afafacb54938","collapsed":true,"_cell_guid":"f16f77ec-5cd6-46aa-934d-31295451bf9f"},"execution_count":null,"source":"def plot_prediction_and_actual(model, forecast, actual, xlim=None, ylim=None, figSize=None, title=None):\n    fig, ax  = plt.subplots(1,1,figsize=figSize)\n    ax.set_ylim(ylim)\n    ax.plot(pd.to_datetime(actual.ds), actual.y, 'r.')\n    model.plot(forecast, ax=ax);\n    ax.set_title(title)\n    plt.show()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"5a5590c9f5bfb769a5609ccd2c4149af8f617e08","collapsed":true,"_cell_guid":"36860209-bf20-46fd-9464-0839b80d2cb9"},"execution_count":null,"source":"# simple linear model\ndef normal_model(df_train, df_actual, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float')\n    \n    df_actual['y'] = df_actual['y'].astype('float')\n    \n    m = Prophet()\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n        #\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Normal model')\n    \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef holiday_model(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float')\n    \n    df_actual['y'] = df_actual['y'].astype('float')\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Holiday model')\n    \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef yearly_model(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float')\n    \n    df_actual['y'] = df_actual['y'].astype('float')\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n\n    m = Prophet(holidays=holidays, yearly_seasonality=True)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n        \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -100\n        ymax = max(df_actual.y.max(), df_train.y.max()) +100\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Yealry model')\n    \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = forecast[mask].yhat\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"c9124dba2e2e0a5f31ba0beda2e8a95c73ecedc1","collapsed":true,"_cell_guid":"ac907ab4-6951-47b2-a881-9ad0bb641c90"},"execution_count":null,"source":"# log model\ndef normal_model_log(df_train, df_actual, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float')\n    df_train.y = np.log1p(df_train.y)\n    \n    df_actual['y'] = df_actual['y'].astype('float')\n    df_actual.y = np.log1p(df_actual.y)\n    \n    m = Prophet()\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n    \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Normal model in log')\n        \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n\n    return smape(forecast_series, actual_series)\n\ndef holiday_model_log(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float')\n    df_train.y = np.log1p(df_train.y)\n    \n    df_actual['y'] = df_actual['y'].astype('float')\n    df_actual.y = np.log1p(df_actual.y)\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n    m = Prophet(holidays=holidays)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n    \n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Holiday model in log')\n        \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n    \n    return smape(forecast_series, actual_series)\n\ndef yearly_model_log(df_train, df_actual, lang, review=False):\n    start_date = df_actual.ds.min()\n    end_date = df_actual.ds.max()\n    \n    actual_series = df_actual.y.copy()\n    actual_series.index = df_actual.ds\n\n    df_train['y'] = df_train['y'].astype('float')\n    df_train.y = np.log1p(df_train.y)\n    \n    df_actual['y'] = df_actual['y'].astype('float')\n    df_actual.y = np.log1p(df_actual.y)\n\n    if(isinstance(lang, float) and math.isnan(lang)):\n        holidays = None\n    else:\n        holidays = holidays_dict[lang]\n        \n    m = Prophet(holidays=holidays, yearly_seasonality=True)\n    m.fit(df_train)\n    future = m.make_future_dataframe(periods=60)\n    forecast = m.predict(future)\n\n    if(review):\n        ymin = min(df_actual.y.min(), df_train.y.min()) -2\n        ymax = max(df_actual.y.max(), df_train.y.max()) +2\n        plot_prediction_and_actual(m, forecast, df_actual, ylim=[ymin, ymax], figSize=(12,4), title='Yearly model in log')\n        \n    mask = (forecast['ds'] >= start_date) & (forecast['ds'] <= end_date)\n    forecast_series = np.expm1(forecast[mask].yhat)\n    forecast_series.index = forecast[mask].ds\n    forecast_series[forecast_series < 0] = 0\n    \n    return smape(forecast_series, actual_series)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"2be1e7acedf5b938ac62af9bd0cc3445deaedecd","collapsed":true,"_cell_guid":"5ea8c040-f6a1-4aec-92c9-4c659908961a"},"execution_count":null,"source":"# This is to demo the facebook prophet model\nprint(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4f42b04b568d3faab544815b2960881f491bb839","_cell_guid":"1039edbc-fc66-4b3a-9d5c-8f58b88066e4"},"source":"## F. Sample series analysis (For script reconciliation)"},{"cell_type":"markdown","metadata":{"_uuid":"26fd25bd241293c5a70078dc8a49101ceeddf177","_cell_guid":"d24966ad-e24b-4f15-8289-6c31acf7303f"},"source":"### Case 1: SMAPE evaluation near zero and SMAPE score is too big"},{"cell_type":"code","metadata":{"_uuid":"24d413fecd75c366a1afb56362fbd4f61fd27743","collapsed":true,"_cell_guid":"93edcdf8-f679-44fd-a7ca-f30a4e06e6af"},"execution_count":null,"source":"import warnings\nwarnings.filterwarnings('ignore')","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"8950b58c273e8d1f304d3874c19c8d970eae24e1","collapsed":true,"_cell_guid":"266500b3-cee0-4c6d-add1-d856e3d65cc2"},"execution_count":null,"source":"print(train.iloc[[2]])\n\ndf_train = extract_series(X_train, 2, 5)\ndf_actual = extract_series(y_train, 2, 5)\nlang = X_train.iloc[2, 1]\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"d5c1ef4155f38326acd3f3c91110c39d7d623c8a","_cell_guid":"9a9f2476-5884-469a-8edf-8587f258b61b"},"source":"### Case 2: Yearly model is the best model"},{"cell_type":"code","metadata":{"_uuid":"20d94726c954632bdcc15cfa16a5334085174112","collapsed":true,"scrolled":false,"_cell_guid":"bace57c0-6635-4d3a-8c08-37905ded472c"},"execution_count":null,"source":"print(train.iloc[[4464]])\n\ndf_train = extract_series(X_train, 4464, 5)\ndf_actual = extract_series(y_train, 4464, 5)\nlang = X_train.iloc[4464, 1]\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"3c53ee45f04e8353b58773cc9daad8568d9e72e9","_cell_guid":"7bcd1ddf-179f-4f02-a5dc-e2508f2f0931"},"source":"### Case 3: Non-yearly model is better"},{"cell_type":"code","metadata":{"_uuid":"ed258f644bc4369b67134298857ed37eee7c3ff5","collapsed":true,"_cell_guid":"96cfa1a0-82dd-4540-959e-1bf8cee97399"},"execution_count":null,"source":"train.iloc[[6245]]\n\ndf_train = extract_series(X_train, 6245, 5)\ndf_actual = extract_series(y_train, 6245, 5)\nlang = X_train.iloc[6245, 1]\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"eed2138dc51c80efc7d0ccc4484ad5c4d6016177","_cell_guid":"9014a21d-bf34-45df-ab60-b702dff43d1d"},"source":"### Case 4: SMAPE score is too high for all proposed models"},{"cell_type":"code","metadata":{"_uuid":"992e8486db836f77989723cf3204181a0703253e","collapsed":true,"_cell_guid":"6bf9e927-a141-408d-8ab7-927d74fe9d19"},"execution_count":null,"source":"train.iloc[[80002]]\n\ndf_train = extract_series(X_train, 80002, 5)\ndf_actual = extract_series(y_train, 80002, 5)\nlang = X_train.iloc[80002, 1]\ntitle = X_train.iloc[80002, 4]\nprint(title)\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\n# Please use this case to check your implementation of SMAPE\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"45f88848f6c62854cc7a249644f42ff5ab4eced9","_cell_guid":"bcedcdbc-dc97-4d4e-a46c-678aef805e77"},"source":"### Case 5: SMAPE score is too high for all proposed models"},{"cell_type":"code","metadata":{"_uuid":"b401c383500e117689177a81c7190e00fd5d4f70","collapsed":true,"_cell_guid":"740c9c66-db23-477e-a859-32b9f13895f5"},"execution_count":null,"source":"train.iloc[[80009]]\n\ndf_train = extract_series(X_train, 80009, 5)\ndf_actual = extract_series(y_train, 80009, 5)\nlang = X_train.iloc[80009, 1]\ntitle = X_train.iloc[80009, 4]\nprint(title)\n\nscore = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang=lang)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = arima_model(df_train.copy(), df_actual.copy(), 2, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"22f16608fb3bb04d2677f1c5fe10d22e8de2d0f1","_cell_guid":"b79fae38-900b-4568-a5c1-c4e09c8e656e"},"source":"### Case 6: SMAPE score is too high for all proposed models"},{"cell_type":"code","metadata":{"_uuid":"d2a2835f3739b3cd3f0b2d642b53d4f98503d558","collapsed":true,"scrolled":false,"_cell_guid":"e926c977-a13f-44a7-b947-02cbd0ce45a7"},"execution_count":null,"source":"train.iloc[[14211]]\n\ndf_train = extract_series(X_train, 14211, 5)\ndf_actual = extract_series(y_train, 14211, 5)\nlang = X_train.iloc[14211, 1]\ntitle = X_train.iloc[14211, 4]\nprint(title)\nscore = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\n# if there is too many zero, just use normal is OK.\nscore = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)\n\nscore = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\nprint(\"The SMAPE score is : %.5f\" % score)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"bae92d3c1d4e641f160181fa697fc0db611d7a86","_cell_guid":"6ef8d842-6b5c-4fc7-8a49-a6b6b72b58c0"},"source":"### Case ?: Adhoc study"},{"cell_type":"code","metadata":{"_uuid":"cb1a78f470e4135f3e6e2093cfd4dee3a3043edb","collapsed":true,"scrolled":false,"_cell_guid":"e7244a09-d28b-47bd-94f8-05c5091bb8d3"},"execution_count":null,"source":"series_num = 145033\nseries_num = 145057\n\nprint(train.iloc[[series_num]])\n\ndf_train = extract_series(X_train, series_num, 5)\ndf_actual = extract_series(y_train, series_num, 5)\n\nlang = X_train.iloc[series_num, 1]\ntitle = X_train.iloc[series_num, 4]\nprint(title)\n\ntry:\n    score = median_model(df_train.copy(), df_actual.copy(), 14, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating median model\", e)\n\ntry:\n    score = holiday_model(df_train.copy(), df_actual.copy(), review=True,lang = lang)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating holiday model\", e)\n    \ntry:\n    score = holiday_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating holiday model in log\", e)\n    \ntry:\n    score = yearly_model_log(df_train.copy(), df_actual.copy(), lang, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating yearly model in log\", e)\n\ntry:\n    score = arima_model(df_train.copy(), df_actual.copy(), 7, 1, 2, review=True)\n    print(\"The SMAPE score is : %.5f\" % score)\nexcept Exception as e:\n    print(\"Error in calculating arima model\", e)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"686deb71c24c0fe70ce8a5c85ea33fa0fc2fc8c3","collapsed":true,"_cell_guid":"1fb4f35b-706e-4065-b0b9-292195f1cc1f"},"execution_count":null,"source":"warnings.resetwarnings()","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6f5596a8ad3d79e261df1d80174b238f07c8dfc8","_cell_guid":"eddfe1ff-8074-4beb-940b-c7d048d515a6"},"source":"## 4. Selected model performance (validation score) over train dataset\n\nIn this session, we wil train the model and do prediction over 145000+ series in dataset. \nTo find out the validation score for comparison"},{"cell_type":"code","metadata":{"_uuid":"f1f20720611d8b754be84626695aa47840aaf5a1","collapsed":true,"_cell_guid":"0f8be8a7-6d92-4817-bc3e-85f6dfd21c8f"},"execution_count":null,"source":"import glob\n\ndef read_from_folder(path):\n    filenames = glob.glob(path + \"/*.csv\")\n\n    dfs = []\n    for filename in filenames:\n        dfs.append(pd.read_csv(filename, index_col=0))\n    \n    frame = pd.concat(dfs)\n    return frame.sort_index()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"b9171b0b831ddb461f8d50f9d3179180b3171a81","collapsed":true,"_cell_guid":"f7d00919-d6cf-403f-bbf9-c983c18b9743"},"execution_count":null,"source":"# TODO: overall validation score in one number.\ndef validation_score(score_series):\n    return score_series.mean()","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"6df92f7616e7e7936e95beb680f283b743b8479f","collapsed":true,"_cell_guid":"0805700f-ba13-46b8-993e-aad1cc23ad65"},"execution_count":null,"source":"valid_fn = r\"../input/wiktraffictimeseriesforecast/validation_score.csv\"\nvalid_score_data = pd.read_csv(valid_fn, index_col=0)\n\nprint(valid_score_data[0:10])","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"a9dff8bf2917d7cef03964e1a1c47563db002257","_cell_guid":"947ddf57-d10a-44f8-b9ab-e7470707166d"},"source":"### A. Simple median model\n\nWe will train up the median model using popular choice 7 to 49, with step 7, and compare the overall score."},{"cell_type":"code","metadata":{"_uuid":"27a1a57f047143412b2000a239ee9f56e1f8c8e4","collapsed":true,"_cell_guid":"5be881c9-e59e-42eb-89a4-82c0777c82b0"},"execution_count":null,"source":"# Check which model is the best\nprint(\"Validation score for median model (7 days) is: %.6f\" % validation_score(valid_score_data['median7']))\nprint(\"Validation score for median model (14 days) is: %.6f\" % validation_score(valid_score_data['median14']))\nprint(\"Validation score for median model (21 days) is: %.6f\" % validation_score(valid_score_data['median21']))\nprint(\"Validation score for median model (28 days) is: %.6f\" % validation_score(valid_score_data['median28']))\nprint(\"Validation score for median model (35 days) is: %.6f\" % validation_score(valid_score_data['median35']))\nprint(\"Validation score for median model (42 days) is: %.6f\" % validation_score(valid_score_data['median42']))\nprint(\"Validation score for median model (49 days) is: %.6f\" % validation_score(valid_score_data['median49']))\n\nfig, axs  = plt.subplots(4,2,figsize=(12,12))\nvalid_score_data['median7'].plot.hist(bins=40, ax=axs[0][0])\nvalid_score_data['median14'].plot.hist(bins=40, ax=axs[0][1])\nvalid_score_data['median21'].plot.hist(bins=40, ax=axs[1][0])\nvalid_score_data['median28'].plot.hist(bins=40, ax=axs[1][1])\nvalid_score_data['median35'].plot.hist(bins=40, ax=axs[2][0])\nvalid_score_data['median42'].plot.hist(bins=40, ax=axs[2][1])\nvalid_score_data['median49'].plot.hist(bins=40, ax=axs[3][0])","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"81a4eb1a4c237aacf2c06b8d5c19646ee2a773cc","_cell_guid":"a03e2e3f-8ddb-4eae-af05-fc6b011c1594"},"source":"### B. Median model - weekday, weekend, holiday"},{"cell_type":"code","metadata":{"_uuid":"c1c934998daa244f426f747f9a5362440b06d113","collapsed":true,"_cell_guid":"8d9ea6cb-332f-43dc-b9df-e0b5081ed0ed"},"execution_count":null,"source":"print(\"Validation score for median model w/holiday (7 days) is: %.6f\" % validation_score(valid_score_data['median7_h']))\nprint(\"Validation score for median model w/holiday (14 days) is: %.6f\" % validation_score(valid_score_data['median14_h']))\nprint(\"Validation score for median model w/holiday (21 days) is: %.6f\" % validation_score(valid_score_data['median21_h']))\nprint(\"Validation score for median model w/holiday (28 days) is: %.6f\" % validation_score(valid_score_data['median28_h']))\nprint(\"Validation score for median model w/holiday (35 days) is: %.6f\" % validation_score(valid_score_data['median35_h']))\nprint(\"Validation score for median model w/holiday (42 days) is: %.6f\" % validation_score(valid_score_data['median42_h']))\nprint(\"Validation score for median model w/holiday (49 days) is: %.6f\" % validation_score(valid_score_data['median49_h']))\n\nfig, axs  = plt.subplots(4,2,figsize=(12,12))\nvalid_score_data['median7_h'].plot.hist(bins=40, ax=axs[0][0])\nvalid_score_data['median14_h'].plot.hist(bins=40, ax=axs[0][1])\nvalid_score_data['median21_h'].plot.hist(bins=40, ax=axs[1][0])\nvalid_score_data['median28_h'].plot.hist(bins=40, ax=axs[1][1])\nvalid_score_data['median35_h'].plot.hist(bins=40, ax=axs[2][0])\nvalid_score_data['median42_h'].plot.hist(bins=40, ax=axs[2][1])\nvalid_score_data['median49_h'].plot.hist(bins=40, ax=axs[3][0])","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"bf844730db0123db1c1cbb09ad6b4bcb85eb404e","_cell_guid":"746a9c9c-6bdc-4577-91cf-61dfca3afa93"},"source":"### C. ARIMA model\n\nCurrently, it is no promising, and median seems a better base line, so I give up this section."},{"cell_type":"markdown","metadata":{"_uuid":"cf15ab6eb24b181889e283d907edf67bfd6e3174","_cell_guid":"87e6ef50-1392-43f5-989a-9e35b9ee675d"},"source":"### D. Facebook model\nWe will train up the model using model with yearly and non-yearly model to see the difference"},{"cell_type":"code","metadata":{"_uuid":"5ac78581fb57079909ab4f1ad35599216e52ddf7","collapsed":true,"_cell_guid":"b6ff168f-c927-4f6e-889e-16ea52d1654b"},"execution_count":null,"source":"print(\"Validation score for holiday model is: %.6f\" % validation_score(valid_score_data['holiday']))\nprint(\"Validation score for holiday model w/log is: %.6f\" % validation_score(valid_score_data['holiday_log']))\nprint(\"Validation score for yearly model w/log is: %.6f\" % validation_score(valid_score_data['yearly_log']))\n\nfig, axs  = plt.subplots(3,1,figsize=(12,12))\nvalid_score_data['holiday'].plot.hist(bins=40, ax=axs[0])\naxs[0].set_title(\"Holiday model\")\nvalid_score_data['holiday_log'].plot.hist(bins=40, ax=axs[1])\naxs[1].set_title(\"Holiday model w/log\")\nvalid_score_data['yearly_log'].plot.hist(bins=40, ax=axs[2])\naxs[2].set_title(\"Yearly model w/log\")","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"33b68b34bcf8f5bb3116f26deeab8c36ef6a1327","_cell_guid":"06234020-ef09-43d6-a158-2001674f664e"},"source":"### E. mixed model\n\nIn this section, we will try to mix the model together to give a better prediction model"},{"cell_type":"code","metadata":{"_uuid":"65cc25e7b304aca87b199c480330a08da2d1f69c","collapsed":true,"_cell_guid":"cfa216a4-e638-4b6f-b9d9-1a39c8b1e658"},"execution_count":null,"source":"def model_to_use( median, holiday_log, yearly_log):\n    result = median\n    if(median * 1 > yearly_log):\n        result = yearly_log\n    elif(median * 1 > holiday_log):\n        result = holiday_log\n        \n    return result\n\nmodel_score = valid_score_data.apply(lambda x: model_to_use( x['median14'], x['holiday_log'], x['yearly_log']), axis=1)\n\nprint(\"Validation score for a proposed model is: %.6f\" % validation_score(model_score))\nmodel_score.plot.hist(bins=40)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"df64f2422afc0b8dd04d8cd5fa9d6325b72ef009","collapsed":true,"_cell_guid":"aeb57480-2340-4288-b7fb-48c97c2c34f2"},"execution_count":null,"source":"model_score_2 = valid_score_data.min(axis=1)\nprint(\"Best possible Validation score for a mixed model is: %.6f\" % validation_score(model_score_2))\n\nmodel_score_2.plot.hist(bins=40)","outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"mimetype":"text/x-python","file_extension":".py","name":"python","pygments_lexer":"ipython3","version":"3.6.1","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python"}},"nbformat_minor":2}