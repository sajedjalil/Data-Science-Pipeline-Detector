{"nbformat_minor":0,"cells":[{"cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"efeb8887-052d-4840-9218-578a64c7153f","collapsed":false,"_uuid":"80b7920fa3155606f0dfc8cb1a6fab6577786a22"},"source":"Here, I am treating this problem as a Regression problem and Running individual model for each Pages with Date and some additional features."},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_cell_guid":"9826a682-73a8-4d28-b24d-ab4163adc361","_execution_state":"idle","trusted":false,"_uuid":"58f6e82cc3a813d8e8ff32580251577cb73be095"},"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"dc6070d8-62f0-4332-9174-46b90fa8345f","trusted":false,"collapsed":false,"_uuid":"fa42c8c363db7a886e81a6a3cf8769c3e7fcdbf0"},"source":"#Load the dataset\ntrain = pd.read_csv('../input/train_1.csv').fillna(0)\n#Save the dates for future use\npages = train['Page'].copy()\ndates = train.columns\n#Drop the page coloumn for now as we are training individual model\ntrain.drop(['Page'],inplace=True,axis=1)\n"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"04a9a070-0f41-4fca-b949-1773d343b356","trusted":false,"collapsed":false,"_uuid":"6e5076b8452d00edd89f3a1856ed9b59683a0e1b"},"source":"#Stack the coloumns as rows\ndf_train = train.stack().reset_index(level=0, drop=True).reset_index()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"f7036c12-0e60-46b0-a265-e23b23503a26","trusted":false,"collapsed":false,"_uuid":"cf7f3ad4a4cad17faaf019939ea70e5b6dc92d44"},"source":"#set the coloumn names to date and number of visits\ndf_train.columns = ['Date','number_of_visits']"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"ad1a4fa0-b5fc-461a-99ca-9b6d04cac593","trusted":false,"collapsed":false,"_uuid":"50c20e0e85641987e877d7385e73210c2903516c"},"source":"#Let's see how the data looks like\n\ndf_train.head()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"dec59e17-aa1b-47bc-a29d-8158a4351d73","trusted":false,"collapsed":false,"_uuid":"fe634bf57de01d5591638debab46a2812d0b5fb8"},"source":"#Create the pages data frmae\n\npages_repeat = pd.DataFrame(np.repeat(pages,550))\n"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"934f72e6-ff60-4b84-b7fb-68549d242cce","trusted":false,"collapsed":false,"_uuid":"56de517d27e8ab16adedc6beae6e5bad587b74a3"},"source":"#reset the index for not messing with it\ndf_train.reset_index(drop=True,inplace=True)\npages_repeat.reset_index(drop=True,inplace=True)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"d1cd3d65-0614-4ae2-801f-04b2fc67caea","trusted":false,"collapsed":false,"_uuid":"e13eb3445b0d436a1b9d9345bc8b7e849e869269"},"source":"#Add the page coloumn again to train data  as we require them to group it\ndf_train['Page'] = pages_repeat.Page.copy()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"d6a61c78-c4f5-42ea-bd94-7090ffe47b53","trusted":false,"collapsed":false,"_uuid":"13d18f3bb2b17d3147fe3e24188daa9ae370f22b"},"source":"#now group by Page to run individual model for each of them\n\ngrouped = df_train.groupby('Page')\n"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"0305e27d-0edb-4834-8205-76c90ac9a2f2","trusted":false,"collapsed":false,"_uuid":"f90ba1983506df4b9b7ac0548539d598388ce051"},"source":"#SMAPE calculate for each page\n#This one is from CPMP,Thanks ;-)\ndef smape(y_true, y_pred,page_name):\n    denominator = (np.abs(y_true) + np.abs(y_pred))\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    print (\"SMAPE score for \"+str(np.asarray(page_name))+\": \"+str(200 * np.mean(diff)))"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"51a5885e-0c3f-4c92-ac30-3aac4b8bb2e1","trusted":false,"collapsed":false,"_uuid":"c47aa920efa8055c858a322551fda1b13c3018f1"},"source":"#Prepare our test data from January 1,2017 to November 10,2017\n\n#generate dates between a range\ndate_test =[]\nfor dayes in pd.date_range('20170101','20171110'):\n    date_test.append(dayes.strftime('%Y-%m-%d'))\n    date_frame = pd.DataFrame(np.asarray(date_test))\n    #Save the date_frame for future use\n    date_append = date_frame\n    date_frame.columns = ['Date']\n    #Expand the date coloumn \n    date_frame = date_frame.Date.str.split('-',expand=True).astype(int)\n    date_frame.columns = ['Year','Month','Day']\n    #add the quarter to dataframe\n    date_frame['Quarter'] = (date_frame.Month-1)//3\n    #drop the year\n    date_frame.drop(['Year'],inplace=True,axis=1)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"3859b7a2-4153-4a4f-b2e8-16dce475d23f","trusted":false,"collapsed":false,"_uuid":"28989441efcb7f50423670d4702e3c225e3a1f2c"},"source":"def process_groups(test_template):\n    #Initialize a dataframe to combine all the page predictions\n    final_predictions = pd.DataFrame(columns=['Visits','Page','Date'])\n    counter = 0\n    #preprocess each group\n    for group in grouped.groups.keys():\n        #create a temp frame\n        group_predictions = pd.DataFrame(columns=['Visits','Page'])\n        data_train = grouped.get_group(group)\n        #Expand the Date coloumn\n        data_train_date= data_train.Date.str.split('-',expand=True).astype(int)\n        data_train_date.columns =['Year','Month','Day']\n        #concatenate to expanded date to data_train\n        data_train = pd.concat([data_train,data_train_date],axis=1)\n        targets = data_train.number_of_visits.copy()\n        #Save the page name for future\n        pages_frame = np.unique(data_train.Page.values)      \n        #drop the Year and number_of_visits,Page coloumn\n        data_train.drop(['Year','number_of_visits','Page','Date'],inplace=True,axis=1)\n        #Add the quarter date\n        data_train['Quarter'] = (data_train.Month-1)//3\n        #KFold cross validation\n        kfold = KFold(5)\n        predictions =[]\n        \n    \n        data_train = np.asarray(data_train)\n        targets = np.asarray(targets)\n        \n        for train_index,test_index in kfold.split(data_train,targets):\n            rf = RandomForestRegressor(n_estimators=100,max_depth=4)\n            X_train, X_test = data_train[train_index], data_train[test_index]\n            y_train, y_test = targets[train_index], targets[test_index]\n            rf.fit(X_train,y_train)\n            y_preds = rf.predict(X_test)\n            #Calculate the SMAPE\n            smape(y_test,y_preds,pages_frame)\n            #predict on test_data\n            predictions.append(rf.predict(test_template))\n        #Average the results from cross validation\n        pred_average = np.mean(np.asarray(predictions),0)\n        group_predictions['Page'] = np.repeat(pages_frame,len(test_template))\n        group_predictions['Visits'] = pred_average\n        group_predictions['Date'] = date_append\n        #Add the dates to the final predictions\n        final_predictions = final_predictions.append(group_predictions)\n        #Run only for three pages else it will run more than 3+ days :-(\n        #Comment the below line to run for all the pages\n        counter = counter+1\n        if counter > 2:\n            \n            return final_predictions\n    #Uncomment the line to run for all the pages\n    #Caution! : This will take long time\n    #return final_predictions\n        \n    "},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"59b9a107-7f13-4d15-aed4-f64089d02358","trusted":false,"collapsed":false,"_uuid":"5c3108acd6282e8d3fbb42d5453e99136d8945d7"},"source":"check_predictions=process_groups(date_frame)"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"c8cc0ae2-fabc-4b83-a07b-c5179c82c811","trusted":false,"collapsed":false,"_uuid":"2960ea5ce27377ac05598aad4d7feeb92ddaa3f6"},"source":"\n\ncheck_predictions['Visits'] = check_predictions.Visits.round()"},{"cell_type":"code","outputs":[],"execution_count":null,"metadata":{"_execution_state":"busy","_cell_guid":"388844ac-73e6-47ed-a9cd-7b7f100ec5de","trusted":false,"collapsed":false,"_uuid":"e6081b29b25132fdce6650e50c30017ad8a8deb4"},"source":"#Check the predictions \ncheck_predictions.head()"},{"cell_type":"markdown","outputs":[],"execution_count":null,"metadata":{"_execution_state":"idle","_cell_guid":"9a9bf71f-140f-42de-a292-a1ac864f34a8","collapsed":false,"_uuid":"92bbaa1e321d69d45f4f3a7c91a2b837d4233954"},"source":"Removing Seasonality,trend and adding week day etc will make the model better i guess.However,This script will run 145063 models(Each page one model) and finally come up with result.\n\n#This model assumes that there is no relationship between the pages,but i don't think that's the case here."}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","name":"python","nbconvert_exporter":"python","version":"3.6.1","mimetype":"text/x-python"}},"nbformat":4}