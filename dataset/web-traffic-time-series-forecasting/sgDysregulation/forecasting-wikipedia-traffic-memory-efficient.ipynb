{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"nbconvert_exporter":"python","file_extension":".py","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","version":"3.6.1","mimetype":"text/x-python"}},"cells":[{"metadata":{"_cell_guid":"aa222ad6-ae36-493c-80bd-d4d74fa0659d","collapsed":true,"_uuid":"a01cdd5122ed82ea3e09c3405fe3fc57a61bd075"},"outputs":[],"cell_type":"code","execution_count":null,"source":"import pandas as pd\nimport numpy as np\nimport scipy as scp\n\nfrom multiprocessing import Pool\nimport time\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\n\nimport warnings; warnings.simplefilter('ignore')"},{"metadata":{"_cell_guid":"5c9e3968-8283-4861-8575-e2ea0872464f","collapsed":true,"_uuid":"422080575fae469db870b69df0c4bed2136d6e91"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# path = '../input/web-traffic-time-series-forecasting/'\n# path=''\npath = '../input/'\nkfile = '{}key_1.csv'.format(path)\nsfile = '{}sample_submission_1.csv'.format(path)\ntfile = '{}train_1.csv'.format(path)"},{"metadata":{"_cell_guid":"4f7fd61b-a11e-4c33-8bb4-b1281f2bd575","collapsed":true,"_uuid":"79ef8a1d2b9a5abe5bcdc6bcfe1ce3c0d37ffefe"},"outputs":[],"cell_type":"code","execution_count":null,"source":"feature_keys = {'Access': {'all-access': 0, 'desktop': 1, 'mobile-web': 2},\n                'Agent': {'all-agents': 0, 'spider': 1},\n                'Domain': {'mediawiki.org': 0, 'wikimedia.org': 1, 'wikipedia.org': 2},\n                'Language': {'commons': 0, 'de': 1, 'en': 2,\n                             'es': 3, 'fr': 4,'ja': 5, 'ru': 6,\n                             'www': 7,'zh': 8}\n               }"},{"metadata":{"_cell_guid":"2e4b9405-e7cd-4ac4-9a9d-f98e99eede4a","collapsed":true,"_uuid":"69ed2e1707cd88d76815cad4343bd02ab5972e02"},"outputs":[],"cell_type":"code","execution_count":null,"source":"dateCols = ['quarter',\n            'is_month_start','is_month_end',\n            'is_quarter_start','is_quarter_end',\n            'is_year_start','is_year_end',\n            'dayofweek','month']\npageCols = ['Name','Language', 'Domain', 'Access', 'Agent']\n\nextractDate = lambda col: (np.uint8(col.quarter),\n                                      np.uint8(col.is_month_start),\n                                      np.uint8(col.is_month_end),\n                                      np.uint8(col.is_quarter_start),\n                                      np.uint8(col.is_quarter_end),\n                                      np.uint8(col.is_year_start),\n                                      np.uint8(col.is_year_end),\n                                      np.uint8(col.dayofweek),\n                                      np.uint8(col.month)\n                                      )"},{"metadata":{"_cell_guid":"dba1ba2f-9203-4d15-8aa9-e629d0866d2a","collapsed":true,"_uuid":"5cd4636f1bc287ba3b5eeb00a9ac903479323b14"},"outputs":[],"cell_type":"code","execution_count":null,"source":"def fun(x):\n    return page_dict[x]\ndef fun_d(x):\n    return date_dict[x]\n    \ndef parallel_map(ser,n=20,isDate=False):\n    \"\"\"\n    looks up the input pandas series values to in dictionary d\n    return res a series with the mapped values and the same index as ser\n    \"\"\"\n#     res = ser.copy()\n    t0 = time.time()\n    \n    try:\n        p = Pool(n)\n        if not isDate:\n            res = p.map(fun,ser)\n        else:\n            res = p.map(fun_d,ser)\n    except Exception as e:\n        print('failed',e)\n        p.close()\n        \n    p.close()\n    p=None\n    print('Time:',time.time()-t0)\n    return res"},{"metadata":{"_cell_guid":"5a88520a-2e70-4706-9812-0cf54a38971c","collapsed":true,"_uuid":"725d75c709407fbcb45372b29a8ae3aa5eb51fe0"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# parallel_map(pd.Series(pd.date_range('2015-01-01',periods=1000,freq='D').strftime('%Y-%m-%d').tolist()),n=20,isDate=True)"},{"metadata":{"_cell_guid":"3b4fca7d-8ea2-487e-aca9-cdcfb4bc94b7","_uuid":"908d11a3055899837f89f2cc8d08b56e548d3932"},"outputs":[],"cell_type":"code","execution_count":null,"source":"def create_dictionary(isDate=False):\n    \"\"\"\n    if isDate=True\n    return a dictionary with the unique dates as keys and their features as tuple value\n    if isDate=False\n    return dictionary with the unique pages as keys and their features as tuple value\n    \"\"\"\n    fnames = ['Name','Language','Domain','Access','Agent']\n    domain = '([A-Za-z0-9\\-]+\\.org)'\n    language = '([A-Za-z0-9\\-]+)'\n    access = '([A-Za-z0-9\\-]+)'\n    agent = '([A-Za-z0-9\\-]+)'\n    name = '(.+)'\n    pattern = '^{:}_{:}\\.{:}_{:}_{:}$'.format(name, language,\n                                              domain, access,\n                                              agent)\n    if not isDate:\n        keys = pd.read_csv(kfile,\n                           usecols=['Page'],\n                           converters={0:lambda p:p[:-11]},\n                           index_col='Page')\n        keys['Page'] = keys.index.tolist()\n        keys.drop_duplicates(inplace=True)\n        keys[fnames] = keys['Page'].str.extract(pattern)\n        keys[fnames[1:]] = keys[fnames[1:]].apply(\n            lambda col: col.map(feature_keys[col.name]).astype(np.uint8))\n        keys.drop('Page',axis=1,inplace=True)\n        keys = dict(zip(keys.index,map(tuple,keys.values)))\n        return keys\n    else:\n        keys = pd.read_csv(kfile,\n                           usecols=['Page'],\n                           converters={0:lambda p:p[-10:]},\n                           index_col='Page')\n        keys['Date'] = keys.index.tolist()\n        keys.drop_duplicates(inplace=True)\n        keys['Date'] = pd.to_datetime(keys['Date']).map(extractDate)\n        keys = dict(zip(keys.index,map(tuple,keys['Date'].values)))\n        return keys\npage_dict = create_dictionary()\ndate_dict = create_dictionary(True)\nprint(list(page_dict.items())[:5])\nprint(list(date_dict.items())[:5])"},{"metadata":{"_cell_guid":"eea6f6c9-db69-42e1-8e50-35b4ba351eac","_uuid":"10ac3fe02486c7fe77de8e04c42c1bd1ebdbb839"},"outputs":[],"cell_type":"code","execution_count":null,"source":"def load_validation_set(keyfile,samplefile):\n    \"\"\"\n    returns the validation set\n    typical use: load_validation_set(kfile,sfile)\n    ### Optimise for large files\n    #### read_csv Parameters\n    ###### na_filter : boolean, default True\n        Detect missing value markers (empty strings and the value of na_values). In\n        data without any NAs, passing na_filter=False can improve the performance\n        of reading a large file)\n    ###### memory_map : boolean, default False\n        If a filepath is provided for `filepath_or_buffer`, map the file object\n        directly onto memory and access the data directly from there. Using this\n        option can improve performance because there is no longer any I/O overhead.\n     ###### engine : {'c', 'python'}, optional\n        Parser engine to use. The C engine is faster while the python engine is\n        currently more feature-complete.\n    \"\"\"\n    keys = pd.read_csv(keyfile,\n                   index_col = 'Id',\n                   converters = {\n                                'Page':lambda p: \n                                 {'Page': p[:-11],'Date':p[-10:],}},     \n                   engine = 'c',\n                   na_filter = False,\n                   memory_map = True)\n    keys['Date'] = parallel_map(keys['Page'].apply(lambda d: d['Date']),isDate=True,n=50)\n    keys['Page'] = keys['Page'].apply(lambda d: d['Page'])\n    \n    sample = pd.read_csv(samplefile,\n                         index_col='Id',\n                         usecols=['Id'],\n                         engine = 'c',\n                         na_filter = False,\n                         memory_map = True)\n    df = pd.concat([keys,sample],join_axes=[sample.index],axis=1).to_sparse()\n    df['Page'] = parallel_map(df['Page'],n=50)\n    return df\nvData = load_validation_set(kfile,sfile)\nvData.head()   \n"},{"metadata":{"_cell_guid":"e0c4515a-b78d-41b1-a56c-6dbe84a46eff","_uuid":"79ad620a435216023b8e9a62b1f74e9cec0ad9fb"},"outputs":[],"cell_type":"code","execution_count":null,"source":"vData.info()"},{"metadata":{"_cell_guid":"ad8d0d62-b5cc-41b6-b09e-7fd754858489","collapsed":true,"_uuid":"2a1da6aa4f36f80fdd87d8a4628374c988e0aa23"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# vData.to_pickle('vData.csv')"},{"metadata":{"_cell_guid":"9fb85965-02a0-4581-aee8-5211ebcd0c92","_uuid":"29f04cd5de9b245f520da3406b67d32bbc75aee3"},"outputs":[],"cell_type":"code","execution_count":null,"source":"def load_train_set(train_file):\n    df = pd.read_csv(train_file, \n                    index_col=0,\n                   engine = 'c',\n                   memory_map = True).rename(\n        columns=pd.to_datetime).groupby(\n        extractDate,axis=1).mean().unstack().dropna().astype(int).to_frame().reset_index()\n    df['Page'] = parallel_map(df['Page'],n=50)\n    return df.rename(columns={0:'Visits','level_0':'Date'}).to_sparse()\n\ntData = load_train_set(tfile)\ntData.head()"},{"metadata":{"_cell_guid":"a4701cc9-3754-4b4a-9365-90326b7291ef","_uuid":"fd42f8276e3d33d41c61ff9ed4f3764d6a28eecf"},"outputs":[],"cell_type":"code","execution_count":null,"source":"tData.info()"},{"metadata":{"_cell_guid":"f69b173e-3483-46f4-9908-41953bc11dc7","collapsed":true,"_uuid":"e79aaed82042c175a2ccf35357c088c1b810ac59"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# tData.to_pickle('tData.csv')"},{"metadata":{"_cell_guid":"5f2bed2f-1f4e-420d-b7df-d84b7b30ce59","collapsed":true,"_uuid":"e3d0e41ecb8ba903daefd48d4455f17b49d280bd"},"outputs":[],"cell_type":"code","execution_count":null,"source":"def getXsparse(df,addText=False,test=False):\n    \"\"\"\n    \"\"\"\n    cv = TfidfVectorizer()\n    if addText:\n        X = sp.sparse.hstack([scp.sparse.csr_matrix(df[cols].values),\n                      cv.fit_transform(df['Name'].astype('str'))],'csr')\n    else:\n        X =scp.sparse.csr_matrix(pd.get_dummies(df[cols],columns=cols).values)\n    return X"},{"metadata":{"_cell_guid":"ddea4d1c-0f05-44ed-b77c-ebf40294f415","collapsed":true,"_uuid":"7320ae78a5c48eb323253dfd71af9f1f00b4ef05"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# X_sp = getXsparse(tData)"},{"metadata":{"_cell_guid":"b1e4d4ad-4a95-47b9-84c0-f445767f5958","collapsed":true,"_uuid":"c2e90d0f699bc838b5ef0a77d27e7cb59a94ac11"},"outputs":[],"cell_type":"code","execution_count":null,"source":""},{"metadata":{"_cell_guid":"6f81ed3f-7303-4f7e-887e-693511f4f06c","collapsed":true,"_uuid":"65287d17f12fae667869cf43a5e0bd5f0cfb761a"},"outputs":[],"cell_type":"code","execution_count":null,"source":"# X_sp = getXsparse(X)\n# X_Validate_sp = getXsparse(X_Validate)\n# X=None\n# X_Validate = None\n# X_sp.shape,X_Validate_sp.shape"}],"nbformat_minor":1,"nbformat":4}