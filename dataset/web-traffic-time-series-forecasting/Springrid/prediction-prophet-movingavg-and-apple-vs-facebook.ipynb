{"metadata":{"anaconda-cloud":{},"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.1","file_extension":".py","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"outputs":[],"cell_type":"markdown","source":"# Notebook of Kaggle Web Traffic Forecasting\n### by Springrid, Sweden","metadata":{"_uuid":"781542f4a21640f15072fc0e8988b495f28550a3","_cell_guid":"99d25b42-6238-4d34-8483-4b8cf5c79e82"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"Table of Contents: \n* 1 Introduction\n    * 1.1 Load Data and Packages\n* 2 Analysis\n    * 2.1 Language Trends\n        * 2.2.1 Fourier Transform Analysis\n    * 2.2 Apple Inc vs Other Companies\n* 3 Predictive Modelling - Apple vs. Others\n    * 3.1 Moving Average\n    * 3.2 fbprophet\n    * 3.3 ARIMA\n* 4 Submission","metadata":{"_uuid":"a266a4506b2a1cdd404e59afbd47aad0da25f8da","_cell_guid":"bf515140-e3bc-438f-8601-5bfd59cb6d8f"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"## 1 Introduction\nThis notebook is an exploration of the data from the Kaggle Web Traffic Forecasting competition. \n\nDataset is found here: https://www.kaggle.com/c/web-traffic-time-series-forecasting/data. The data set contains timeseries data of 145k Wikipedia pages with number of page visits per day during the time period of July, 1st, 2015 up until December 31st, 2016.\n\n### 1.1 Load Data and Packages\nI start by loading the necessary packages and data. I replace missing values with zeros for simplicity.","metadata":{"_uuid":"1c1fd633bd218a9532a68e942746a489358e03ef","_cell_guid":"2a2464a8-34b7-479d-8608-658f555c22b1"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport multiprocessing as mp\nfrom datetime import datetime\nfrom collections import Counter\nfrom scipy.fftpack import fft\n\nfrom fbprophet import Prophet\nfrom statsmodels.tsa.arima_model import ARIMA\nimport warnings\n\nkaggle_on = True\n\nif kaggle_on:\n    path = '../input/'\nelse:\n    path = 'data/'\n\ndf_train = pd.read_csv(path + 'train_1.csv', nrows=150000).fillna(0)\nprint('Len of data: ', len(df_train.index))","metadata":{"collapsed":false,"_uuid":"a7f641abe017d9f13a66b2bb124d8be12ac76aec","_cell_guid":"d015d820-e347-44ef-aa63-3ac7d947206e","trusted":false},"execution_count":1},{"outputs":[],"cell_type":"markdown","source":"Convert all page view counts to integers to save memory.","metadata":{"_uuid":"f51ed297c31a8907d57e3984d75b0edcb8361070","_cell_guid":"8bb1f9e1-3c64-4455-91a1-3083435c5a7a"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Convert page views to integers\nfor col in df_train.columns[1:]:\n    df_train[col] = pd.to_numeric(df_train[col], downcast='integer')","metadata":{"collapsed":true,"_uuid":"5f3509fb5cdb9ef4e238893f07dff9661d646585","_cell_guid":"b1e360b0-df0e-4dad-9ece-58cfb7f52f03","trusted":false},"execution_count":2},{"outputs":[],"cell_type":"markdown","source":"## 2 Analysis\nWe will focus on analysis of the languages of the pages first.\n\n### 2.1 Language Trends\nBy first extracting the languages of each article and the get the total count of page views per language each day we can analyze trends per country.\n\nSection is inspired by: https://www.kaggle.com/muonneutrino/wikipedia-traffic-data-exploration","metadata":{"_uuid":"39919157eb6bd49ac2dcaf4e35bffd5f2a7f88c6","_cell_guid":"619cfc83-3b10-49cd-ad12-560ebde60e7b"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Get the language of an article\ndef get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org', page)\n    if res:\n        return res.group(0)[0:2]\n    return 'na'\n\ndf_train['lang'] = df_train.Page.map(get_language)\nlanguages = df_train.lang.unique()\nprint(Counter(df_train.lang))","metadata":{"collapsed":false,"_uuid":"e42de09ba2ef8cd70f0dd72c4eaa2bb88a748142","_cell_guid":"4d84d112-3d8d-4f99-b018-de1a46e05203","trusted":false},"execution_count":3},{"outputs":[],"cell_type":"markdown","source":"Timeseries plot of the total count of page views per language. We see a clear periodic trends on all languages.","metadata":{"_uuid":"01ef0d45ba791cc77c640706a622a1823fef3e28","_cell_guid":"0e675b4d-7acc-49fc-9245-e8bffab88556"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# Analyze language feature\nlang_sets = {}\nfor language in languages:\n    lang_sets[language] = df_train[df_train.lang == language].iloc[:, 0:-1]\n\nsums = {}\nfor key in lang_sets:\n    sums[key] = lang_sets[key].iloc[:, 1:].sum(axis=0) / lang_sets[key].shape[0]\n    \ndays = [r for r in range(sums['fr'].shape[0])]\nfig = plt.figure(1, figsize=[10, 10])\nplt.ylabel('Views per Page')\nplt.xlabel('Day')\nplt.title('Pages in Different Languages')\nlabels = {'en': 'English', 'ja': 'Japanese', 'de': 'German',\n          'na': 'Media', 'fr': 'French', 'zh': 'Chinese',\n          'ru': 'Russian', 'es': 'Spanish'}\nfor key in sums:\n    plt.plot(days, sums[key], label=labels[key])\nplt.legend()\nplt.show()","metadata":{"collapsed":false,"_uuid":"739a6f429048a23786a5957acc6c29999fa60238","_cell_guid":"e4f3e23f-2ba1-4d7b-90b2-073b905c005f","trusted":false},"execution_count":4},{"outputs":[],"cell_type":"markdown","source":"### 2.1.1 Fourier Transform Analysis of Languages\nI apply the fourier transform on the data per language to investigate if we can see periodic trends in number of page visits.","metadata":{"_uuid":"0f8511b3192f9476d6b26f4bb02358996be6d898","_cell_guid":"0b5592a5-8774-4a9d-9148-0b236cf58e24"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"def plot_with_fft(key):\n    fig = plt.figure(1, figsize=[15, 5])\n    plt.ylabel('Views per Page')\n    plt.xlabel('Day')\n    plt.title(labels[key])\n    plt.plot(days, sums[key], label=labels[key])\n\n    fig = plt.figure(2, figsize=[15,5])\n    fft_complex = fft(sums[key])\n    fft_mag = [np.sqrt(np.real(x)*np.real(x)+np.imag(x)*np.imag(x)) for x in fft_complex]\n    fft_xvals = [day / days[-1] for day in days]\n    npts = len(fft_xvals) // 2 + 1\n    fft_mag = fft_mag[:npts]\n    fft_xvals = fft_xvals[:npts]\n\n    plt.ylabel('FFT Magnitude')\n    plt.xlabel(r\"Frequency [days]$^{-1}$\")\n    plt.title('Fourier Transform')\n    plt.plot(fft_xvals[1:], fft_mag[1:], label=labels[key])\n    # Draw lines at 1, 1/2, and 1/3 week periods\n    plt.axvline(x=1./7, color='red', alpha=0.3)\n    plt.axvline(x=2./7, color='red', alpha=0.3)\n    plt.axvline(x=3./7, color='red', alpha=0.3)\n\n    plt.show()\n\nfor key in sums:\n    plot_with_fft(key)\n","metadata":{"collapsed":false,"_uuid":"2c2ffaf725aaa17391170d288f35c92405c77b1d","_cell_guid":"0aed2a15-4d1b-4dcd-bb11-d0b8ee32b3a3","trusted":false},"execution_count":5},{"outputs":[],"cell_type":"code","source":"# For each language get highest few pages\nnpages = 5\ntop_pages = {}\nfor key in lang_sets:\n    # print(key)\n    sum_set = pd.DataFrame(lang_sets[key][['Page']])\n    sum_set['total'] = lang_sets[key].sum(axis=1)\n    sum_set = sum_set.sort_values('total',ascending=False)\n    # print(sum_set.head(10))\n    top_pages[key] = sum_set.index[0]\n    # print('\\n\\n')","metadata":{"collapsed":false,"_uuid":"d7f7ce6012628dcb814622d2b15808002af8c5bd","_cell_guid":"bc293cb3-9d78-4505-af09-5b69a3fa1bcf","trusted":false},"execution_count":6},{"outputs":[],"cell_type":"markdown","source":"### 2.2 Apple Inc vs. Other Companies\nWe'll here investigate if we can get some insights in trends for Apple, Microsoft, Facebook and Google. In section 3 Predictive Modelling, we'll try to make some simple predicting models for this data.","metadata":{"_uuid":"87f11dcd4899b384620264db3962d1a90d7240cf","_cell_guid":"1e493705-9520-4874-a09f-a9fc4d1e0737"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"cols = df_train.columns[1:-1]\n\ndef filter_df(df, word):\n    df_new = df[df['Page'].str.contains(word)]\n    apple_pages = df_new.Page.values\n    df_new = df_new[cols].transpose()\n    df_new[word] = df_new.values.sum(axis=1)\n    return df_new[[word]]\n\nword_to_filter_by = ['Apple_Inc', 'Microsoft', 'Facebook', 'Google']\n\ndf_companies = pd.DataFrame()\nfor word in word_to_filter_by:\n    df_tmp = filter_df(df_train, word)\n    df_companies = pd.concat([df_companies, df_tmp], axis=1)\n\nprint(df_companies.idxmax(axis=0))\ndf_companies.plot()\n\n# mark Apple releases and other important dates during time period\nif False:\n    holidays = ['2015-11-26', '2015-12-25']\n    stock_dates = ['2016-08-10']\n    apple_dates = ['2015-07-15', '2015-09-09', '2015-09-25', '2015-10-13', '2015-10-26', '2015-11-11',\n                  '2016-03-31', '2016-04-19', '2016-09-16', '2016-10-27', '2016-12-19']\n    for date in holidays + stock_dates:\n        plt.axvline(df_companies.index.get_loc(date), color='black', linestyle='solid')","metadata":{"collapsed":false,"_uuid":"0c54a2c50a91597517cd2667b39c6b0cbc95fe52","_cell_guid":"be2254c8-e3ca-406f-ae5b-f03c39b36187","trusted":false},"execution_count":7},{"outputs":[],"cell_type":"code","source":"## 3 Predictive Modelling - Apple vs. Others\nWe will try a few different simple models to see how they perform.\n\n### 3.1 Moving Average\nSimple moving average approach over one week.","metadata":{"collapsed":false,"_uuid":"1238b74250f45edf0215d6b653b464818d4817d8","_cell_guid":"70f01d72-58f6-4bb9-8117-b2078ca21269","trusted":false},"execution_count":8},{"outputs":[],"cell_type":"code","source":"def moving_average_approach(df):\n    moving_avg = df.rolling(window=7).mean()\n    # + df.rolling(window=28).mean()/2 # + df.rolling(window=56).mean()/4\n    # moving_std = df.rolling(window=3).std()\n    return moving_avg\n\nmoving_average_approach(df_companies).plot()\ndf_companies.plot()","metadata":{"collapsed":false,"_uuid":"f4e7f88dc4cf0e78a64005ce9baa06a7af12429e","_cell_guid":"95f320c3-4e5c-48f2-ae9d-a42c2fe409b3","trusted":false},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"### 3.2 Prophet\nWe will now try Facebook Prophet to forecast page views. We'll use the two page pages for English and Spanish:\n\n1. Main_Page_en.wikipedia.org_all-access_all-agents\n2. Wikipedia:Portada_es.wikipedia.org_all-access_all-agents\n\nClass suppress_stdout_stderr is to stop all stdouts from fbprophet when fitting data.","metadata":{"_uuid":"79904378cfadd41071f25dadb9c48be0ee97a0c5","_cell_guid":"79932a28-344b-4f7a-b781-fbcb6def04e1"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# https://github.com/facebookincubator/prophet/issues/223\n# from https://stackoverflow.com/questions/11130156/suppress-stdout-stderr-print-from-python-functions\nclass suppress_stdout_stderr(object):\n    '''\n    A context manager for doing a \"deep suppression\" of stdout and stderr in\n    Python, i.e. will suppress all print, even if the print originates in a\n    compiled C/Fortran sub-function.\n       This will not suppress raised exceptions, since exceptions are printed\n    to stderr just before a script exits, and after the context manager has\n    exited (at least, I think that is why it lets exceptions through).\n\n    '''\n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = (os.dup(1), os.dup(2))\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        os.close(self.null_fds[0])\n        os.close(self.null_fds[1])","metadata":{"collapsed":true,"_uuid":"4bd2ecac4c48dbafcde28ab748381fb271be86d3","_cell_guid":"26e9a446-2a94-4ebe-ba46-5e43f9c583e0","trusted":false},"execution_count":null},{"outputs":[],"cell_type":"code","source":"from fbprophet import Prophet\nsns.set(font_scale=1) \n\ndef make_forecast_with_prophet(df, cols):\n    values = df.values\n    df_prophet = pd.DataFrame(columns=['ds', 'y'])\n    df_prophet['ds'] = cols\n    df_prophet = df_prophet.set_index('ds')\n    df_prophet['y'] = values\n    df_prophet.reset_index(drop=False,inplace=True)\n    \n\n    m = Prophet(yearly_seasonality=True).fit(df_prophet)\n    future = m.make_future_dataframe(periods=days_to_forecast,freq='D', include_history=True)\n    forecast = m.predict(future)\n    return forecast, m\n\nif False:\n    days_to_forecast = 31+28\n    plot_on = False\n\n    cols = df_train.columns[1:-1]\n    for key in top_pages[0]:\n        df_tmp = df_train.loc[top_pages[key], cols].copy()\n        forecast, m = make_forecast_with_prophet(df_tmp)\n\n        if plot_on:\n            plt.figure(figsize=(10, 10))\n            fig = m.plot(forecast)\n            fig = m.plot_components(forecast)","metadata":{"collapsed":false,"_uuid":"0900cbcaee62bda53a2051df38109827ac5b6de2","_cell_guid":"26312fd9-3a3d-4df2-9148-d613e0f057c1","trusted":false},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"From Facebook's prophet we get some really insightful plots over the weekly, monthly and yearly trends for the data of Apple, Microsoft, Google and Facebook.\n\n#### Apple\n* global minima (in data range) near December 2015, since then positive trend.\n* large monthly flucuations\n\n#### Microsoft\n* strong declining trend over the entire period\n* large monthly flucuations\n\n#### Facebook\n* declining trend\n* relatively small monthly flucuations\n\n#### Google\n* although strong first 9 months of 2016, prediction shows strong decline in the beginning of 2017. Most likely because of week ending of 2016.\n* as\n\n#### All\n* all four companies showed more page visits during the week than during the weekend\n\n","metadata":{"_uuid":"4a3b6672f9a17c6445009a166b90af10cea8cf33","_cell_guid":"d0ecc6cd-a44b-42f0-afd3-16675cf9c037"},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"### 3.3 ARIMA Model\nNot quite there yet... Needs more work.","metadata":{"_uuid":"89a7e07f7d445ae707ff3c1b93c50c9fe7745b29","_cell_guid":"548f503a-8458-4a71-9581-832c00954d53"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nimport warnings\n\nn_cols = len(df_train.columns) - 1\nn_cols_train = round(n_cols / 10*7)\ncols_train = df_train.columns[1:n_cols_train]\ncols_predict = df_train.columns[n_cols_train:-1]\n\nfor key in top_pages:\n    data = np.array(df_train.loc[top_pages[key], df_train.columns[1:-1]],'f')\n    data_train = np.array(df_train.loc[top_pages[key], cols_train],'f')\n    data_predict = np.array(df_train.loc[top_pages[key], cols_predict],'f')\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        try:\n            arima = ARIMA(data_train, [2,1,4])\n            result = arima.fit(disp=False)\n        except:\n            try:\n                arima = ARIMA(data_train, [2,1,2])\n                result = arima.fit(disp=False)\n            except:\n                print(df_train.loc[top_pages[key],'Page'])\n                print('\\tARIMA failed')\n    stop = 599\n    pred = result.forecast(steps=100)[0]\n    x = [i for i in range(len(data))]\n    x_train = [i for i in range(n_cols_train-1)]\n    x_pred = [i for i in range(n_cols_train, n_cols_train+100)]\n    i=0\n    \n    plt.plot(x, data, label='Data')\n    plt.plot(x_train, data_train, label='Train Data')\n    plt.plot(x_pred, pred,label='ARIMA Model')\n    plt.title(df_train.loc[top_pages[key], 'Page'])\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.legend()\n    plt.show()","metadata":{"collapsed":false,"_uuid":"c950ec2eb86af41386df2a3e896aee9dedcafefa","_cell_guid":"863aacfb-55d7-4eda-9a35-f122ff1048dd","trusted":false},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"## 4. Submission\nAttempt at sumission of results using prophet. To slow computer/kernel. I have done a setup with Google Cloud and will try to get a complete submission through there. :)","metadata":{"_uuid":"bf28826301a3bf31aef48363d32428b1d6fb9aee","_cell_guid":"72a65ffb-f9ff-4886-b425-64f84db76a7e"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"def batch_process_with_prophet(df):\n    submission = pd.DataFrame(columns=['Id', 'Visits'])\n    results_forecasts = pd.DataFrame(columns=['ds', 'yhat'])\n\n    cols = df.columns[1:-1]\n    # i = 0\n\n    # Comment out this to be able to run the rest :)\n    for index, row in df.iterrows():\n        df_tmp = df.loc[index, cols].copy()\n\n        # Workaround for handle pages where all visists are zero.\n        if df_tmp.sum() == 0:\n            dates_index = pd.date_range(start=\"2017-01-01\", end=\"2017-02-28\", freq=\"D\")\n            forecast = pd.DataFrame(columns=['ds', 'yhat'])\n            forecast['ds'] = dates_index\n            forecast = forecast.set_index(forecast.ds)\n            forecast['yhat'] = 0\n        else:\n            forecast, m = make_forecast_with_prophet(df_tmp, cols)\n            forecast = forecast[['ds', 'yhat']]\n            \n        forecast['ds'] = row.Page + '_' + forecast.ds.apply(lambda x: x.strftime('%Y-%m-%d'))\n        results_forecasts = results_forecasts.append(forecast.tail(days_to_forecast))\n        \n        # if i % 10 == 0:\n        #     print(i)\n        # i += 1\n        \n    return results_forecasts","metadata":{"collapsed":true,"_uuid":"d3b9d85b2147db770009cdfea22f6a0ba98393c8","_cell_guid":"10b3892c-54df-4571-8ddc-1cf89eeb2d92","trusted":false},"execution_count":null},{"outputs":[],"cell_type":"code","source":"CHUNKSIZE = 100\n\ndf_key = pd.read_csv(path + 'key_1.csv')\nreader = pd.read_csv(path + 'train_1.csv\", chunksize=CHUNKSIZE)\n                     \npool = mp.Pool(4) # use 4 processes\n\nfunclist = []\ni = 0\n\nfor df in reader:\n    # process each data frame\n    f = pool.apply_async(batch_process_with_prophet,[df])\n    funclist.append(f)\n    i += 1\n    if i > 10:\n        break\n\nresult = []\nwith suppress_stdout_stderr():\n    for f in funclist:\n        result.append(f.get(timeout=60*60)) # timeout in 300 seconds = 60 mins\n\n# combine chunks with transformed data into a single training set\ntraining = pd.concat(result)\n\ntraining.to_csv('sub.csv')\n\nif False:\n    results_forecasts = training.sort_values('ds')\n    df_key = df_key.sort_values('Page')\n    submission['Id'] = df_key.Id.values\n    submission['Visits'] = results_forecasts.yhat.values\n    submission.to_csv('submissions/' + datetime.now().strftime(\"%Y%m%d-%I%M%p\") + '.csv')\n\nprint(training.head())\nprint(training.describe())","metadata":{"collapsed":false,"_uuid":"08be0b92b3fe12c7fa10a141726482f05d1dfcff","_cell_guid":"98f1d5ea-c561-455f-bf2d-ccc325c516b2","trusted":false},"execution_count":null},{"outputs":[],"cell_type":"markdown","source":"## 5. Evaluation","metadata":{"_uuid":"074effbcddaf3440c2fdb8c5ecb94057b6f4b6f8","_cell_guid":"bcf157c1-94cc-4f1f-8c91-4a539eafbccf"},"execution_count":null},{"outputs":[],"cell_type":"code","source":"# from: https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/37232\nfrom numba import jit\nimport math\n\n@jit\ndef smape_fast(y_true, y_pred):\n    out = 0\n    for i in range(y_true.shape[0]):\n        a = y_true[i]\n        b = y_pred[i]\n        c = a+b\n        if c == 0:\n            continue\n        out += math.fabs(a - b) / c\n    out *= (200.0 / y_true.shape[0])\n    return out","metadata":{"collapsed":true,"_uuid":"ac52d46cefaab9a731c2137e4d654b7ee5de28eb","_cell_guid":"ae4d3afc-2eb5-472d-9517-aad07a88a205","trusted":false},"execution_count":9},{"outputs":[],"cell_type":"code","source":"","metadata":{"collapsed":true,"_uuid":"ec8767ace94a4e6ede4937794ca47692071b6223","_cell_guid":"01cb7925-2d30-4389-954e-c28195d38b14","trusted":false},"execution_count":null}],"nbformat_minor":1,"nbformat":4}