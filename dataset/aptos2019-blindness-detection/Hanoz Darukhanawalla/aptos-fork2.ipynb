{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Listing all the imports\n! pip install imutils\n! mkdir /kaggle/working/trained_images/\n! rm -rf /kaggle/working/trained_images/\n! mkdir /kaggle/working/trained_images/\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport imutils\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls /kaggle/working\n! ls /kaggle/working/trained_images\n!ls -l /kaggle/working/trained_images/ | egrep -c '^-' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image height and image width ----> GLOBAL\nimg_ht = 256\nimg_wd = 256\n\ndef displayImage(display_name, image):\n    cv2.namedWindow(display_name,cv2.WINDOW_AUTOSIZE)\n    cv2.imshow(display_name, image)\n\ndef findContourEye(thresh_image):\n    cnts = cv2.findContours(thresh_image.copy(), cv2.RETR_EXTERNAL,\n\tcv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = max(cnts, key=cv2.contourArea)\n    return cnts\n\ndef findContourEyeExtreme(cnts):\n    # Locating extreme points on all 4 sides\n    leftmost = tuple(cnts[cnts[:,:,0].argmin()][0])\n    rightmost = tuple(cnts[cnts[:,:,0].argmax()][0])\n    topmost = tuple(cnts[cnts[:,:,1].argmin()][0])\n    bottommost = tuple(cnts[cnts[:,:,1].argmax()][0])\n    # Locating the top left and bottom right corner\n    x1 = leftmost[0]\n    y1 = topmost[1]\n    x2 = rightmost[0]\n    y2 = bottommost[1]\n    return x1,y1,x2,y2 \n\ndef findRadiusAndCentreOfContourEye(cnts):\n    M = cv2.moments(cnts)\n    if( M[\"m00\"]==0):\n        cX, cY = 0, 0\n    else:\n        cX = int(M[\"m10\"] / M[\"m00\"])\n        cY = int(M[\"m01\"] / M[\"m00\"])\n    if(cX < cY):\n        r = cX\n    else:\n        r = cY\n    return cX,cY,r\n\ndef drawCentreOnContourEye(image,cnts,cX,cY):\n    cv2.drawContours(image, [cnts], -1, (0, 255, 0), 2)\n    cv2.circle(image, (cX, cY), 7, (255, 255, 255), -1)\n    cv2.putText(image, \"center\", (cX - 20, cY - 20),\n    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2)\n    return image\n    \ndef Radius_Reduction(img,cX,cY,r):\n    h,w,c=img.shape\n    Frame=np.zeros((h,w,c),dtype=np.uint8)\n    cv2.circle(Frame,(int(cX),int(cY)),int(r), (255,255,255), -1)\n    Frame1=cv2.cvtColor(Frame, cv2.COLOR_BGR2GRAY)\n    img1 =cv2.bitwise_and(img,img,mask=Frame1)\n    return img1\n\ndef imageResize(image, ht, wd):\n    # resized_image = imutils.resize(image, height = ht, width = wd)\n    resized_image = cv2.resize(image,(wd,ht))\n    return resized_image\n\ndef crop_black(image):\n    org = image.copy()\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    thresh = cv2.threshold(blurred, 10, 255, cv2.THRESH_BINARY)[1]\n    # displayImage('thresh',thresh)\n    cnts = findContourEye(thresh)\n    x1,y1,x2,y2 = findContourEyeExtreme(cnts)\n    # print(x1,y1,x2,y2)\n    crop = org[y1:y2, x1:x2]\n    crop = imageResize(crop, img_ht, img_wd)\n    # displayImage(\"cr1\",crop)\n    return crop\n\ndef imageAugmentation(image):\n    x_flip = cv2.flip( image, 0 )\n    y_flip = cv2.flip( image, 1 )\n    xy_flip = cv2.flip(x_flip,1)\n    return x_flip, y_flip, xy_flip\n\ndef imageHistEqualization(image):\n    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl,a,b))\n    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n    return final\n\ndef subtract_median_bg_image(im):\n    k = np.max(im.shape)//20*2+1\n    bg = cv2.medianBlur(im, k)\n    sub_med = cv2.addWeighted (im, 4, bg, -4, 100)\n    return sub_med\n\ndef colorEnhancement(image1,image2):\n    image_final = cv2.bitwise_and(image1,image2)\n    return image_final\n\ndef imageAugSave(path,img1,img2,img3,img4,img_ht,img_wd):\n    count = len(os.listdir(path))\n\n    img1 = imageResize(img1, img_ht, img_wd)\n    img2 = imageResize(img2, img_ht, img_wd)\n    img3 = imageResize(img3, img_ht, img_wd)\n    img4 = imageResize(img4, img_ht, img_wd)\n\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+1)), img1)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+2)), img2)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+3)), img3)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+4)), img4)\n    return count+1,count+2,count+3,count+4\n\ndef processed_test_save(path,img,img_ht,img_wd):\n    count = len(os.listdir(path))\n    img = imageResize(img,img_ht,img_wd)\n    cv2.imwrite(os.path.join(path , '%d.png'%(count+1)), img)\n    return count+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_ht = 256\nimg_wd = 256\npath_toCollect =  '/kaggle/input/aptos2019-blindness-detection/train_images'\npath_toSave = '/kaggle/working/trained_images'\ntrain_data = pd.read_csv('/kaggle/input/aptos2019-blindness-detection/train.csv')\nnewDataframe_cols = ['id_code','diagnosis'] \ntrained_data = pd.DataFrame(columns=newDataframe_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feedToPipeline(image_name,diagnosis_type):\n    global path_toCollect\n    global path_toCollect\n    global img_ht,img_wd\n    global trained_data, train_data\n\n    try:\n        image_name = str(image_name) + '.png'\n        image = cv2.imread(os.path.join(path_toCollect,image_name))\n        image = imageResize(image, img_ht, img_wd)\n        org_copy = image.copy()\n        image_crop = crop_black(image)\n        image_clahe = imageHistEqualization(image_crop)\n        sub_med = subtract_median_bg_image(image_clahe)\n        image_final = colorEnhancement(sub_med, image_clahe)\n        aug1, aug2, aug3 = imageAugmentation(image_final)\n        count1,count2,count3,count4 = imageAugSave(path_toSave,image_final, aug1, aug2, aug3,img_ht,img_wd)\n        count1 = str(count1) + '.png'\n        count2 = str(count2) + '.png'\n        count3 = str(count3) + '.png'\n        count4 = str(count4) + '.png'\n        len_trained_data = len(trained_data)\n        trained_data.loc[len_trained_data]   = [count1,diagnosis_type] \n        trained_data.loc[len_trained_data+1] = [count2,diagnosis_type] \n        trained_data.loc[len_trained_data+2] = [count3,diagnosis_type] \n        trained_data.loc[len_trained_data+3] = [count4,diagnosis_type]\n#         print(\"Processed\")\n    except:\n        print(\"+========================+\")\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\n# # Vectorize approach took 846 seconds and the for loop took 905 seconds to process more than 3 thousand images\n# # \n# # np.vectorize(feedToPipeline)(train_data['id_code'],train_data['diagnosis'])\n# # \nfrom tqdm.notebook import tqdm\nfor i in tqdm(range(len(train_data))): \n# for i in tqdm(range(1)): \n#     print(i)\n    feedToPipeline(train_data['id_code'][i],train_data['diagnosis'][i])\n# # \ntrained_data.to_csv('/kaggle/working/final_trained.csv',index = False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls /kaggle/working/\n# !ls /kaggle/working/trained_images\n!ls -l /kaggle/working/trained_images/ | egrep -c '^-' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}