{"cells":[{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"1f87e893-12d3-41b4-a415-e7a5d1d93704","_uuid":"6bd4a3d11fdfe20a4a3901d9d66054a758f2b726","collapsed":true},"source":"# importing basic dependencies\nimport matplotlib.pyplot as plt # for seeing the images\n%matplotlib inline\nimport cv2 # for image processing\nimport glob # for file handling\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output # to get the files in currect folder\nfrom keras.utils import to_categorical # to convert to one-hot encodings\nimport tqdm # progress bar\nfrom collections import Counter # for getting breed data\n\n# Importing ML Dependencies\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\n'''# Importing ML Dependencies --> Using InceptionV3 as base model\nfrom keras.applications.inception_v3 import InceptionV3 # using this model\nfrom keras.preprocessing import image # preprocessing the images\nfrom keras.models import Model # custom model\nfrom keras.layers import Dense, GlobalAveragePooling2D # layers\nfrom keras import backend as K # backend\nfrom keras.optimizers import SGD # during second compilation, for smoother learning'''\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfiles = check_output([\"ls\", \"../input\"]).decode(\"utf8\")\nprint(files)\n# Any results you write to the current directory are saved as output.","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"0e91888e-a007-45c9-bbbe-72dca380c633","_uuid":"6d043f6a328713f06b1f6718237802129673a7cb","collapsed":true},"source":"# loading images path --> train\nimages_train_path = '../input/train/*.jpg'\nimages_train_paths = glob.glob(images_train_path)\nprint(images_train_paths[0])\n\n# laoding images path --> test\nimages_test_path = '../input/test/*.jpg'\nimages_test_paths = glob.glob(images_test_path)\nprint(images_test_paths[0])","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a6c82e2f-4953-48d8-9ef1-4cb4455092e5","_uuid":"8c400bd77adacc13975cc8a0a697b4c87672b47c","collapsed":true},"source":"# taking the labels for the images\nlabels = pd.read_csv('../input/labels.csv')\nprint(labels.head())","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a2874fd6-9a5c-427c-9f22-f19b0266002a","scrolled":true,"_uuid":"b5680e536cc8b425c5e80a302908b9c037944d6f","collapsed":true},"source":"# taking the labels and converting to one hot\nbreeds = sorted(list(set(labels['breed'].values)))\n# making a dictionary of breeds which will be used for one-hot encoding\nb2id = dict((b,i) for i,b in enumerate(breeds))\n# converting labeled breeds to numbers\nbreed_vector = [b2id[i] for i in labels['breed'].values]\n# converting to one-hot encoding\ndata_y = to_categorical(breed_vector)","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c2fdfcf8-fd66-4595-8175-8d107e25d76f","_uuid":"2a52f0f43579b4bb14bcadec7e8351f750969b87","collapsed":true},"source":"print('[*]Total images:', len(images_test_paths) + len(images_train_paths))\nprint('[*]Total training images:', len(images_train_paths))\nprint('[*]Total test images:', len(images_test_paths))\nprint('[*]Total breeds:',len(breeds))\nprint('[*]data_y.shape:', data_y.shape)","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"edb4f409-5a65-401b-8b5f-1252f8203d59","_uuid":"a5359212e1b109c3478261d7625e47a15c126020","collapsed":true},"source":"print(data_y[0])","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"2677e854-3843-48a1-9b2e-122cd4af3416","_uuid":"836b1e8d4ab72728c07e02322b77a2761b68b44a","collapsed":true},"source":"# understanding the distribution of breeds\nbreed_dict = Counter(labels['breed'].values)\n# getting top 5 breeds\nbreed_numbers = [i for i in breed_dict.values()]\nbreed_names = [b for b in breed_dict.keys()]","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"f2c8eb6c-c6b5-43ff-9ef2-312ece613340","_uuid":"4c5ae2496921a5ae60221f9d9dd65dbd6981d9d0"},"source":"## Looking at a Sample image"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"5881433c-b807-4faa-9e77-a3ecb01e8ae9","_uuid":"de41ee0aa52d661b53e12d96a1855d81832daf51","collapsed":true},"source":"# taking a sample image\nimg1 = cv2.imread(images_train_paths[120])\nplt.imshow(img1)","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"e3a7ab80-9006-465e-b202-a17697b3bb15","_uuid":"bb8d6373709f4194ce77c998ec5c1eb8ec444bc6","collapsed":true},"source":"img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\nplt.imshow(img1)","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"d862d8ac-b6c0-40c0-b29f-08941965611c","_uuid":"1d1b455f310d2fe768a739314397fec16164fd52","collapsed":true},"source":"# Resizing an image to a sqaure\nimg1 = cv2.resize(img1, (224, 224))\nplt.imshow(img1)","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"810a172b-f549-4b52-96ea-224edaf6af46","_uuid":"8ca3c58a116edc5cde6058b7c9731cbdf82860b0","collapsed":true},"source":"# converting to the image to array which will be understood by the model\nprint(img1.shape)","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"e8aaa67c-d413-40c2-a0a0-d9714cc59de3","_uuid":"ca8fe5bf03b22a7348de56d3f933ca2aaccfcc41"},"source":"## Loading the images"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"a48ba518-a03a-4b1c-909c-8470fda477b8","_uuid":"736a2865cd281caf0542a2220151de5ced4b1d98","collapsed":true},"source":"print('[!]Getting training images:')\ntotal_images_train = np.zeros((len(images_train_paths), 224, 224, 3))\nfor i in tqdm.tqdm(range(len(images_train_paths))):\n    image = cv2.imread(images_train_paths[i]) # reading the image\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to proper colour channel\n    image = cv2.resize(image, (224,224)) # resizing to feed into model\n    total_images_train[i] = image # adding to the total data","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"f2de7b8b-9616-4010-8b77-f0ccdfce9915","_uuid":"a9f0d404b00d9c81db93514728b79c28214c828d","collapsed":true},"source":"print('[!]Getting testing images:')\ntotal_images_test = np.zeros((len(images_test_paths), 224, 224, 3))\nfor i in tqdm.tqdm(range(len(images_test_paths))):\n    image = cv2.imread(images_test_paths[i]) # reading the image\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converting to proper colour channel\n    image = cv2.resize(image, (224,224)) # resizing to feed into model\n    total_images_test[i] = image # adding to the total data","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"f7d0367b-ea02-4efc-ac0c-b759655245f0","_uuid":"96e4b0989604b9df1d767495c33f623fbf59e8bb","collapsed":true},"source":"total_images_train = np.array(total_images_train)\n# total_images_test = np.array(total_images_test)","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"1a8d6dfb-b0f0-4b6c-b895-1f75a56208f5","_uuid":"9ca279f9cc19c56639cd78c2d1761b0101527ff7","collapsed":true},"source":"print('[*]Traning set shape:', total_images_train.shape)\n# print('[*]Testing set shape:', total_images_test.shape)","execution_count":null},{"cell_type":"markdown","metadata":{"_cell_guid":"0b242360-fb8e-48a3-9a85-d82c147cb0ea","_uuid":"5a8e24ae8d8a09587c7df99d08d329d3a7a07272"},"source":"## Making Custom Classifier"},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"c2a76bcc-b9d2-4994-945c-8173bde1a7aa","_uuid":"db01e396de5d08493c3f72691488532e38496c65","collapsed":true},"source":"model = Sequential()\nmodel.add(Conv2D(filters = 16, kernel_size = 2, padding = 'same',\n                 activation = 'relu', input_shape = (224, 224, 3)))\nmodel.add(MaxPooling2D(pool_size= 2))\nmodel.add(Conv2D(filters = 32, kernel_size = 2, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size= 2))\nmodel.add(Conv2D(filters = 64, kernel_size = 2, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size= 2))\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(len(breeds), activation = 'softmax'))\n\nprint(model.summary())","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"8fcaed8b-b65e-478f-8b1c-61065f4e15b0","_uuid":"56245ccc95181d44a749ec87b9eaf654a11984f9","collapsed":true},"source":"# Compiling the model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null},{"outputs":[],"cell_type":"code","metadata":{"_cell_guid":"9214dd65-6b2e-4297-bf55-60b2512d445f","_uuid":"4fa36059a38686b13c82c42b12728ee0bbdd1939","collapsed":true},"source":"model.fit(total_images_train, data_y,  epochs = 10, batch_size = 64)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","nbconvert_exporter":"python","file_extension":".py","pygments_lexer":"ipython3","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}