{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# End to End Dog Breed Multi Class Classification\nThis notebook is written in tensorflow 2.0 and tensorhub\n## 1.Problem\nThere are 120 breeds of dog in the data. We have to identify according to their breeds.\n## 2.Data\nThere are training set and a test set of images of dogs. Each image has a filename that is its unique id. The dataset comprises 120 breeds of dogs.\n* train.zip - the training set, you are provided the breed for these dogs\n* test.zip - the test set, you must predict the probability of each breed for each image\n* sample_submission.csv - a sample submission file in the correct format\n* labels.csv - the breeds for the images in the train set\n\n## Evaluation\nPredicting the probability of each breed in test data.\n## Features\n* we are dealing with the unstructure data set.\n* There are about 10000+ images in the training data.\n* There are about 10000+ images in the test data.\n\nNow Since we have unstructured data we have to work with the library like tensorflow. so lets import the necessary libraries and directly jump into the project","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# standard imports\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plte\n%matplotlib inline\nimport tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting our data ready\ncommunication with our data and turning them into the tensors.All data must be in a numerical form.\n\nLets start by accessing our data and checking the labels.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_csv = pd.read_csv('/kaggle/input/dog-breed-identification/labels.csv')\nlabels_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_csv.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_csv['breed'].value_counts().plot.bar(figsize=(20,12));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#median number of image in each class.\nlabels_csv['breed'].value_counts().median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#viewing any image from the train data.\nfrom IPython.display import Image\nImage('/kaggle/input/dog-breed-identification/train/0a0c223352985ec154fd604d7ddceabd.jpg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Getting Images and their labels.\nCreate path names for image ID's.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filenames = ['/kaggle/input/dog-breed-identification/train/' + fname + '.jpg' for fname in labels_csv['id']]\nfilenames[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check wheather the the number of files matches number of actual images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nif len(os.listdir('/kaggle/input/dog-breed-identification/train/')) == len(filenames):\n    print('Number of file matches number of actual images!')\nelse:\n    print('Number of file doesnot matches number of actual images!!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualizing images according to their index.\nImage(filenames[900])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the name of the above displayed dog.\nlabels_csv['breed'][900]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Turning our data into numbers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = labels_csv['breed']\nlabels = np.array(labels)\nlabels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"chech wheather the number of labels matches the number of filenames.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"if len(labels) == len(filenames):\n    print('Number of labels matches the number of filenames.')\nelse:\n    print('Number of labels doesnot matches the number of filenames')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding the unique labels values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_breed = np.unique(labels) \nunique_breed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Turn single label into an array of boolean.\nprint(labels[0])\nlabels[0] == unique_breed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Turning every label into an array of boolean\nboolean_labels = [labels == unique_breed for labels in labels]\nboolean_labels[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turining boolean arrays into integers.\nprint(labels[0])   #orginal index\nprint(np.where(unique_breed==labels[0]))    #index where labels occurs.\nprint(boolean_labels[0].argmax())     #index where label occurs in boolean array\nprint(boolean_labels[0].astype(int))   #there will be a 1 where sample label occurs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating our own validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup x and y variables.\nX = filenames\ny = boolean_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First starting with ~1000 images because we have lots of data to train for the very first attempt","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#set number of images to set for the experiment.\nNUM_IMAGES = 1000 #@param {type:\"slider\",min:1000,max:10000}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's split our data into train and validation.\nfrom sklearn.model_selection import train_test_split\n\n#spliting into training and validation of total size NUM_IMAGES.\n\nX_train,X_val,y_train,y_val = train_test_split(X[:NUM_IMAGES],\n                                                y[:NUM_IMAGES],\n                                                test_size=0.2,\n                                                random_state=42)\nlen(X_train),len(X_val),len(y_train),len(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[:5],y_train[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing Images\nTurning images into tensors\n\nLet's write a function to preprocess the image. The function will do the following tasks.\n\n* The function will take an image filepath as input.\n* Use the tensorflow to read the file and save it to the variable.\n* Turn our variable (.jpg) into tensors.\n* Normalize our image(convert color channel from 0-255 to 0-1).|\n* Resize the image.\n* Return the modified variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting images to numpy array\n\nfrom matplotlib.pyplot import imread\nimage = imread(filenames[42])\nimage.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets conver them into tensor\ntf.constant(image)[:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making a function to preprocess the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define image size\nIMG_SIZE = 224\n\ndef process_image(image_path):\n  \"\"\"\n  Takes an image file path and turns it into a Tensor.\n  \"\"\"\n  # Read in image file\n  image = tf.io.read_file(image_path)\n  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n  image = tf.image.decode_jpeg(image, channels=3)\n  # Convert the colour channel values from 0-225 values to 0-1 values\n  image = tf.image.convert_image_dtype(image, tf.float32)\n  # Resize the image to our desired size (224, 244)\n  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n  return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Turning our data into batches\n\nWhy turn our data into batches?\n\n We are trying to fit the 10000+ data images. They all might not fit into memory.\n\nSo,that's why we use 32(this is batch size) images at a time. we can change the batch size whenever we need.\n\nIn order to use the tensorflow effective we need to convert the images into tuple tensor which looks like   `(image,labels)`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a simple function to return a tuple (image, label)\ndef get_image_label(image_path, label):\n  \"\"\"\n  Takes an image file path name and the associated label,\n  processes the image and returns a tuple of (image, label).\n  \"\"\"\n  image = process_image(image_path)\n  return image, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's make a function to turn all our data (x,y) into batches","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Define the batch size, 32 is a good default\nBATCH_SIZE = 32\n\n# Create a function to turn data into batches\ndef create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n  \"\"\"\n  Creates batches of data out of image (x) and label (y) pairs.\n  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.\n  Also accepts test data as input (no labels).\n  \"\"\"\n  # If the data is a test dataset, we probably don't have labels\n  if test_data:\n    print(\"Creating test data batches...\")\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x))) # only filepaths\n    data_batch = data.map(process_image).batch(BATCH_SIZE)\n    return data_batch\n  \n  # If the data if a valid dataset, we don't need to shuffle it\n  elif valid_data:\n    print(\"Creating validation data batches...\")\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n                                               tf.constant(y))) # labels\n    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n    return data_batch\n\n  else:\n    # If the data is a training dataset, we shuffle it\n    print(\"Creating training data batches...\")\n    # Turn filepaths and labels into Tensors\n    data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n                                              tf.constant(y))) # labels\n    \n    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images\n    data = data.shuffle(buffer_size=len(x))\n\n    # Create (image, label) tuples (this also turns the image path into a preprocessed image)\n    data = data.map(get_image_label)\n\n    # Turn the data into batches\n    data_batch = data.batch(BATCH_SIZE)\n  return data_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create training and validation data batches\ntrain_data = create_data_batches(X_train, y_train)\nval_data = create_data_batches(X_val, y_val, valid_data=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check out the different attributes of our data batches\ntrain_data.element_spec, val_data.element_spec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at that! We've got our data in batches, more specifically, they're in Tensor pairs of (images, labels) ready for use on a GPU.\n\nBut having our data in batches can be a bit of a hard concept to understand. Let's build a function which helps us visualize what's going on under the hood.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Visualizing data batches","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Create a function for viewing images in a data batch\ndef show_25_images(images, labels):\n  \"\"\"\n  Displays 25 images from a data batch.\n  \"\"\"\n  # Setup the figure\n  plt.figure(figsize=(10, 10))\n  # Loop through 25 (for displaying 25 images)\n  for i in range(25):\n    # Create subplots (5 rows, 5 columns)\n    ax = plt.subplot(5, 5, i+1)\n    # Display an image\n    plt.imshow(images[i])\n    # Add the image label as the title\n    plt.title(unique_breed[labels[i].argmax()])\n    # Turn gird lines off\n    plt.axis(\"off\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To make computation efficient, a batch is a tighly wound collection of Tensors.\n\nSo to view data in a batch, we've got to unwind it.\n\nWe can do so by calling the as_numpy_iterator() method on a data batch.\n\nThis will turn our a data batch into something which can be iterated over.\n\nPassing an iterable to next() will return the next item in the iterator.\n\nIn our case, next will return a batch of 32 images and label pairs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize training images from the training data batch\ntrain_images, train_labels = next(train_data.as_numpy_iterator())\nshow_25_images(train_images, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize validation images from the validation data batch\nval_images, val_labels = next(val_data.as_numpy_iterator())\nshow_25_images(val_images, val_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating and training a model.\nNow our data is ready now lets model our data.\n\nBefore we build a model, there are a few things we need to define:\n\n* The input shape (images, in the form of Tensors) to our model.\n* The output shape (image labels, in the form of Tensors) of our model.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting up input shape to our model.\n# Setting up input shape to the model\nINPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels\n\n# Setting up output shape of the model\nOUTPUT_SHAPE = len(unique_breed) # number of unique labels\n\n# Setting up model URL from TensorFlow Hub\nMODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've got the inputs, outputs and model we're using ready to go. We can start to put them together\n\nThere are many ways of building a model in TensorFlow but one of the best ways to get started is to use the Keras API.\n\nKnowing this, let's create a function which:\n\n* Takes the input shape, output shape and the model we've chosen's URL as parameters.\n* Defines the layers in a Keras model in a sequential fashion (do this first, then this, then that).\n* Compiles the model (says how it should be evaluated and improved).\n* Builds the model (tells it what kind of input shape it'll be getting).\n* Returns the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function which builds a Keras model\ndef create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):\n  print(\"Building model with:\", MODEL_URL)\n\n  # Setup the model layers\n  model = tf.keras.Sequential([\n    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)\n    tf.keras.layers.Dense(units=OUTPUT_SHAPE, \n                          activation=\"softmax\") # Layer 2 (output layer)\n  ])\n\n  # Compile the model\n  model.compile(\n      loss=tf.keras.losses.CategoricalCrossentropy(), # Our model wants to reduce this (how wrong its guesses are)\n      optimizer=tf.keras.optimizers.Adam(), # A friend telling our model how to improve its guesses\n      metrics=[\"accuracy\"] # We'd like this to go up\n  )\n\n  # Build the model\n  model.build(INPUT_SHAPE) # Let the model know what kind of inputs it'll be getting\n  \n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a model and check its details\nmodel = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating callbacks\nWe've got a model ready to go but before we train it we'll make some callbacks.\n\nCallbacks are helper functions a model can use during training to do things such as save a models progress, check a models progress or stop training early if a model stops improving.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%load_ext tensorboard","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\n# Create a function to build a TensorBoard callback\ndef create_tensorboard_callback():\n  # Create a log directory for storing TensorBoard logs\n  logdir = os.path.join(\"drive/My Drive/Data/logs\",\n                        # Make it so the logs get tracked whenever we run an experiment\n                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n  return tf.keras.callbacks.TensorBoard(logdir)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Early Stopping Callback\nEarly stopping helps prevent overfitting by stopping a model when a certain evaluation metric stops improving. If a model trains for too long, it can do so well at finding patterns in a certain dataset that it's not able to use those patterns on another dataset it hasn't seen before (doesn't generalize).\n\nIt's basically like saying to our model, \"keep finding patterns until the quality of those patterns starts to go down.\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create early stopping (once our model stops improving, stop training)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\n                                                  patience=3) # stops after 3 rounds of no improvements","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training a model (On a subset of data)\nOur first model is only going to be trained on 1000 images. Or trained on 800 images and then validated on 200 images, meaning 1000 images total or about 10% of the total data.\n\nWe do this to make sure everything is working. And if it is, we can step it up later and train on the entire training dataset.\n\nThe final parameter we'll define before training is NUM_EPOCHS (also known as number of epochs).\n\nNUM_EPOCHS defines how many passes of the data we'd like our model to do. A pass is equivalent to our model trying to find patterns in each dog image and see which patterns relate to each label.\n\nIf NUM_EPOCHS=1, the model will only look at the data once and will probably score badly because it hasn't a chance to correct itself\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_EPOCHS = 100\n# Build a function to train and return a trained model\ndef train_model():\n  \"\"\"\n  Trains a given model and returns the trained version.\n  \"\"\"\n  # Create a model\n  model = create_model()\n\n  # Create new TensorBoard session everytime we train a model\n  tensorboard = create_tensorboard_callback()\n\n  # Fit the model to the data passing it the callbacks we created\n  model.fit(x=train_data,\n            epochs=NUM_EPOCHS,\n            validation_data=val_data,\n            validation_freq=1, # check validation metrics every epoch\n            callbacks=[tensorboard, early_stopping])\n  \n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model to the data\nmodel = train_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It look like our model is overfitting.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Making and evaluating predictions using a trained model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the validation data (not used to train on)\npredictions = model.predict(val_data, verbose=1) # verbose shows us how long there is to go\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the shape of predictions\npredictions.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions with our model returns an array with a different value for each label.\n\nIn this case, making predictions on the validation data (200 images) returns an array (predictions) of arrays, each containing 120 different values (one for each unique dog breed).\n\nThese different values are the probabilities or the likelihood the model has predicted a certain image being a certain breed of dog. The higher the value, the more likely the model thinks a given image is a specific breed of dog.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First prediction\nprint(predictions[0])\nprint(f\"Max value (probability of prediction): {np.max(predictions[0])}\") # the max probability value predicted by the model\nprint(f\"Sum: {np.sum(predictions[0])}\") # because we used softmax activation in our model, this will be close to 1\nprint(f\"Max index: {np.argmax(predictions[0])}\") # the index of where the max value in predictions[0] occurs\nprint(f\"Predicted label: {unique_breed[np.argmax(predictions[0])]}\") # the predicted label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn prediction probabilities into their respective label (easier to understand)\ndef get_pred_label(prediction_probabilities):\n  \"\"\"\n  Turns an array of prediction probabilities into a label.\n  \"\"\"\n  return unique_breed[np.argmax(prediction_probabilities)]\n\n# Get a predicted label based on an array of prediction probabilities\npred_label = get_pred_label(predictions[0])\npred_label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've got a list of all different predictions our model has made, we'll do the same for the validation images and validation labels.\n\nThe model hasn't trained on the validation data, during the fit() function, it only used the validation data to evaluate itself. So we can use the validation images to visually compare our models predictions with the validation labels.\n\nSince our validation data (val_data) is in batch form, to get a list of validation images and labels, we'll have to unbatch it (using unbatch()) and then turn it into an iterator using as_numpy_iterator().\n\nLet's make a small function to do so.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a function to unbatch a batched dataset\ndef unbatchify(data):\n  \"\"\"\n  Takes a batched dataset of (image, label) Tensors and returns separate arrays\n  of images and labels.\n  \"\"\"\n  images = []\n  labels = []\n  # Loop through unbatched data\n  for image, label in data.unbatch().as_numpy_iterator():\n    images.append(image)\n    labels.append(unique_breed[np.argmax(label)])\n  return images, labels\n\n# Unbatchify the validation data\nval_images, val_labels = unbatchify(val_data)\nval_images[0], val_labels[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we've got ways to get:\n\n* Prediction labels\n* Validation labels (truth labels)\n* Validation images\nLet's make some functions to make these all a bit more visualize.\n\nMore specifically, we want to be able to view an image, its predicted label and its actual label (true label).\n\nThe first function we'll create will:\n\n* Take an array of prediction probabilities, an array of truth labels, an array of images and an integer.\n* Convert the prediction probabilities to a predicted label.\n* Plot the predicted label, its predicted probability, the truth label and target image on a single plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pred(prediction_probabilities, labels, images, n=1):\n  \"\"\"\n  View the prediction, ground truth label and image for sample n.\n  \"\"\"\n  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\n  \n  # Get the pred label\n  pred_label = get_pred_label(pred_prob)\n  \n  # Plot image & remove ticks\n  plt.imshow(image)\n  plt.xticks([])\n  plt.yticks([])\n\n  # Change the color of the title depending on if the prediction is right or wrong\n  if pred_label == true_label:\n    color = \"green\"\n  else:\n    color = \"red\"\n\n  plt.title(\"{} {:2.0f}% ({})\".format(pred_label,\n                                      np.max(pred_prob)*100,\n                                      true_label),\n                                      color=color)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View an example prediction, original image and truth label\nplot_pred(prediction_probabilities=predictions,\n          labels=val_labels,\n          images=val_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we're working with a multi-class problem (120 different dog breeds), it would also be good to see what other guesses our model is making. More specifically, if our model predicts a certain label with 24% probability, what else did it predict?\n\nLet's build a function to demonstrate. The function will:\n\n* Take an input of a prediction probabilities array, a ground truth labels array and an integer.\n* Find the predicted label using get_pred_label().\n* Find the top 10:\n    * Prediction probabilities indexes\n    * Prediction probabilities values\n    * Prediction labels\n* Plot the top 10 prediction probability values and labels, coloring the true label green.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pred_conf(prediction_probabilities, labels, n=1):\n  \"\"\"\n  Plots the top 10 highest prediction confidences along with\n  the truth label for sample n.\n  \"\"\"\n  pred_prob, true_label = prediction_probabilities[n], labels[n]\n\n  # Get the predicted label\n  pred_label = get_pred_label(pred_prob)\n\n  # Find the top 10 prediction confidence indexes\n  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]\n  # Find the top 10 prediction confidence values\n  top_10_pred_values = pred_prob[top_10_pred_indexes]\n  # Find the top 10 prediction labels\n  top_10_pred_labels = unique_breed[top_10_pred_indexes]\n\n  # Setup plot\n  top_plot = plt.bar(np.arange(len(top_10_pred_labels)), \n                     top_10_pred_values, \n                     color=\"grey\")\n  plt.xticks(np.arange(len(top_10_pred_labels)),\n             labels=top_10_pred_labels,\n             rotation=\"vertical\")\n\n  # Change color of true label\n  if np.isin(true_label, top_10_pred_labels):\n    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color(\"green\")\n  else:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_pred_conf(prediction_probabilities=predictions,\n               labels=val_labels,\n               n=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check a few predictions and their different values\ni_multiplier = 0\nnum_rows = 3\nnum_cols = 2\nnum_images = num_rows*num_cols\nplt.figure(figsize=(5*2*num_cols, 5*num_rows))\nfor i in range(num_images):\n  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n  plot_pred(prediction_probabilities=predictions,\n            labels=val_labels,\n            images=val_images,\n            n=i+i_multiplier)\n  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n  plot_pred_conf(prediction_probabilities=predictions,\n                labels=val_labels,\n                n=i+i_multiplier)\nplt.tight_layout(h_pad=1.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving and reloading a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_model(model, suffix=None):\n  \"\"\"\n  Saves a given model in a models directory and appends a suffix (str)\n  for clarity and reuse.\n  \"\"\"\n  # Create model directory with current time\n  modeldir = os.path.join(\"/kaggle/working/drive/My Drive/Data/\",\n                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n  print(f\"Saving model to: {model_path}...\")\n  model.save(model_path)\n  return model_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_model(model_path):\n  \"\"\"\n  Loads a saved model from a specified path.\n  \"\"\"\n  print(f\"Loading saved model from: {model_path}\")\n  model = tf.keras.models.load_model(model_path,\n                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save our model trained on 1000 images\nsave_model(model, suffix=\"1000-images-Adam\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training a model in full dataset\n\nNow we know our model works on a subset of the data, we can start to move forward with training one on the full data.\n\nAbove, we saved all of the training filepaths to X and all of the training labels to y. Let's check them out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X),len(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" We've got over 10,000 images and labels in our training set.\n\nBefore we can train a model on these, we'll have to turn them into a data batch.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn full training data in a data batch\nfull_data = create_data_batches(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is in a data batch, all we need now is a model.\n\nwe've got a function for that too! Let's use create_model() to instantiate another model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate a new model for training on the full dataset\nfull_model = create_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we've made a new model instance, full_model, we'll need some callbacks too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create full model callbacks\n\n# TensorBoard callback\nfull_model_tensorboard = create_tensorboard_callback()\n\n# Early stopping callback\n# Note: No validation set when training on all the data, therefore can't monitor validation accruacy\nfull_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",\n                                                             patience=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting the full model to the full training data with about 10000+ images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the full model to the full training data\nfull_model.fit(x=full_data,\n               epochs=5,\n               callbacks=[full_model_tensorboard, \n                          full_model_early_stopping])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving the full trained model to the file followed by the suffix all-images-Adam","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model to file\nsave_model(full_model, suffix=\"all-images-Adam\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading the full saved model using load_model()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load in the full model\nloaded_full_model = load_model('/kaggle/working/drive/My Drive/Data/20200628-03271593314820-all-images-Adam.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load test image filenames (since we're using os.listdir(), these already have .jpg)\ntest_path = \"/kaggle/input/dog-breed-identification/test/\"\ntest_filenames = [test_path + fname for fname in os.listdir(test_path)]\n\ntest_filenames[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating test data batches.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test data batch\ntest_data = create_data_batches(test_filenames, test_data=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions on test data batch using the loaded full model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on test data batch using the loaded full model\ntest_predictions = loaded_full_model.predict(test_data,\n                                             verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Displaying the outcome in the pandas dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create pandas DataFrame with empty columns\npreds_df = pd.DataFrame(columns=[\"id\"] + list(unique_breed))\npreds_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Append test image ID's to predictions DataFrame\ntest_path = \"/kaggle/input/dog-breed-identification/test/\"\npreds_df[\"id\"] = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\npreds_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add the prediction probabilities to each dog breed column\npreds_df[list(unique_breed)] = test_predictions\npreds_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the csv file to submit for the competition.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_df.to_csv(\"/kaggle/working/drive/My Drive/Data/MySubmission.csv\",\n                 index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"So, this is the predicted outcome of the dog breed identification.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}