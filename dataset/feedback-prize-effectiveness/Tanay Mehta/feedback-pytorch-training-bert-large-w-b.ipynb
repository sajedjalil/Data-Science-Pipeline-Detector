{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize Predicting Effective Arguments - BERT Large Model training with KFolds and WandB üéØ\n\nThis notebooks shows how you can train big transformer models in this exciting competition data. Here I am training BERT-Large model on a max-length of 185 characters.\n\nI hope to add more optimizations to increase the max-length without affecting training or throwing OOM errors.\n\n<center>\n    <img src=\"https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle\">\n</center>","metadata":{}},{"cell_type":"code","source":"%%sh\npip install -q --upgrade transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T14:22:49.374771Z","iopub.execute_input":"2022-05-26T14:22:49.375263Z","iopub.status.idle":"2022-05-26T14:23:12.27102Z","shell.execute_reply.started":"2022-05-26T14:22:49.375157Z","shell.execute_reply":"2022-05-26T14:23:12.269815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport wandb\nimport platform\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:12.27431Z","iopub.execute_input":"2022-05-26T14:23:12.274675Z","iopub.status.idle":"2022-05-26T14:23:20.859557Z","shell.execute_reply.started":"2022-05-26T14:23:12.274629Z","shell.execute_reply":"2022-05-26T14:23:20.858537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Config = {\n    'NB_EPOCHS': 3,\n    'N_SPLITS': 5,\n    'NUM_WORKERS': 8,\n    'NUM_LABELS': 3,\n    'LR': 2e-5,\n    'T_0': 20,\n    'Œ∑_min': 1e-4,\n    'MAX_LEN': 200,\n    'N_SPLITS': 5,\n    'TRAIN_BS': 16,\n    'VALID_BS': 32,\n    'DEVICE': 'cuda',\n    'MODEL_NAME': 'bert-large-uncased',\n    'CSV_PATH': '../input/feedback-prize-effectiveness/train.csv',\n    'TXT_PATHS': '../input/feedback-prize-effectiveness/train/',\n    'TOKENIZER': transformers.BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True),\n    'scaler': GradScaler(),\n    'infra': \"Kaggle\",\n    'competition': 'feedback_prize',\n    '_wandb_kernel': 'tanaym',\n}","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:20.863085Z","iopub.execute_input":"2022-05-26T14:23:20.863734Z","iopub.status.idle":"2022-05-26T14:23:25.138189Z","shell.execute_reply.started":"2022-05-26T14:23:20.863701Z","shell.execute_reply":"2022-05-26T14:23:25.13722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## About W&B:\n<center><img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\"/></center><br>\n<p style=\"text-align:center\">WandB is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br></p>","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:01:46.754672Z","iopub.execute_input":"2022-05-25T18:01:46.755025Z","iopub.status.idle":"2022-05-25T18:01:46.786654Z","shell.execute_reply.started":"2022-05-25T18:01:46.754935Z","shell.execute_reply":"2022-05-25T18:01:46.785174Z"}}},{"cell_type":"markdown","source":"To login to W&B, you can use below snippet.\n\n```python\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wb_key)\n```\nMake sure you have your W&B key stored as `WANDB_API_KEY` under Add-ons -> Secrets\n\nYou can view [this](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases) notebook to learn more about W&B tracking.\n\nIf you don't want to login to W&B, the kernel will still work and log everything to W&B in anonymous mode.","metadata":{}},{"cell_type":"code","source":"# Start W&B logging\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wb_key)\n\nrun = wandb.init(\n    project='pytorch',\n    config=Config,\n    group='nlp',\n    job_type='train',\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:25.141499Z","iopub.execute_input":"2022-05-26T14:23:25.142073Z","iopub.status.idle":"2022-05-26T14:23:32.882644Z","shell.execute_reply.started":"2022-05-26T14:23:25.142024Z","shell.execute_reply":"2022-05-26T14:23:32.880586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetchEssay(essay_id: str):\n    \"\"\"\n    Read the text file of the specific essay_id\n    \"\"\"\n    essay_path = os.path.join(Config['TXT_PATHS'], essay_id + '.txt')\n    essay_text = open(essay_path, 'r').read()\n    return essay_text\n\ndef wandb_log(**kwargs):\n    \"\"\"\n    Logs a key-value pair to W&B\n    \"\"\"\n    for k, v in kwargs.items():\n        wandb.log({k: v})","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:32.885006Z","iopub.execute_input":"2022-05-26T14:23:32.885572Z","iopub.status.idle":"2022-05-26T14:23:32.908294Z","shell.execute_reply.started":"2022-05-26T14:23:32.885524Z","shell.execute_reply":"2022-05-26T14:23:32.906869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, data, config=Config, is_test=False):\n        self.data = data\n        self.config = config\n        self.is_test = is_test\n    \n    def __getitem__(self, idx):\n        text = self.data['text'].values[idx]\n        if not self.is_test:\n            target_value = self.data['discourse_effectiveness'].values[idx]\n                \n        inputs = self.config['TOKENIZER'].encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=Config['MAX_LEN'],\n            pad_to_max_length=True\n        )\n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        \n        if self.is_test:\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n            }\n        else:\n            targets = torch.tensor(target_value, dtype=torch.long)\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': targets\n            }\n        \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:32.914007Z","iopub.execute_input":"2022-05-26T14:23:32.921815Z","iopub.status.idle":"2022-05-26T14:23:32.976349Z","shell.execute_reply.started":"2022-05-26T14:23:32.921758Z","shell.execute_reply":"2022-05-26T14:23:32.97516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTModel(nn.Module):\n    def __init__(self, backbone_arch):\n        super(BERTModel, self).__init__()\n        hidden_size = self.get_model_size(backbone_arch)\n        \n        self.backbone = transformers.BertModel.from_pretrained(backbone_arch)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_size, Config['NUM_LABELS'])\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.backbone(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n        \n    def get_model_size(self, backbone_name: str):\n        \"\"\"\n        Gets the size of the BERT model variant and decides other parameter based off it.\n        \"\"\"\n        if \"base\" in backbone_name:\n            hidden_size = 768\n        elif \"large\" in backbone_name:\n            hidden_size = 1024\n        else:\n            raise NotImplementedError(\"Unknown BERT variant\")\n        \n        return hidden_size","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:32.987949Z","iopub.execute_input":"2022-05-26T14:23:32.991378Z","iopub.status.idle":"2022-05-26T14:23:33.034625Z","shell.execute_reply.started":"2022-05-26T14:23:32.991323Z","shell.execute_reply":"2022-05-26T14:23:33.033376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, config, dataloaders, optimizer, model, loss_fns, scheduler, device=\"cuda:0\", apex=False):\n        self.train_loader, self.valid_loader = dataloaders\n        self.train_loss_fn, self.valid_loss_fn = loss_fns\n        self.scheduler = scheduler\n        self.optimizer = optimizer\n        self.model = model\n        self.device = torch.device(device)\n        self.apex = apex\n        self.config = config\n\n    def train_one_epoch(self):\n        \"\"\"\n        Trains the model for 1 epoch\n        \"\"\"\n        if self.apex:\n            scaler = GradScaler()\n\n        self.model.train()\n        train_pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n        train_preds, train_labels = [], []\n        running_loss = 0\n        for bnum, data_cache in train_pbar:\n            ids = self._convert_if_not_tensor(data_cache['ids'], dtype=torch.long)\n            mask = self._convert_if_not_tensor(data_cache['mask'], dtype=torch.long)\n            ttis = self._convert_if_not_tensor(data_cache['token_type_ids'], dtype=torch.long)\n            targets = self._convert_if_not_tensor(data_cache['targets'], dtype=torch.long)\n            \n            # Support of Apex üõ†Ô∏è\n            if self.apex:\n                outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis)\n                loss = self.train_loss_fn(outputs, targets)\n                \n                Config['scaler'].scale(loss).backward()\n                Config['scaler'].step(self.optimizer)\n                Config['scaler'].update()\n                self.optimizer.zero_grad()\n                self.scheduler.step()\n            \n            # No Apex\n            else:\n                outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis)\n                loss = self.train_loss_fn(outputs, targets)\n                \n                loss.backward()\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                self.scheduler.step()\n            \n            loss_value = loss.item()\n            running_loss += loss_value\n\n            train_pbar.set_description(desc=f\"loss: {loss_value:.4f}\")\n\n            # Chug the targets and labels in a list\n            train_preds += [outputs.detach().cpu().numpy()]\n            train_labels += [targets.detach().cpu().numpy()]\n        \n        all_train_preds = np.concatenate(train_preds)\n        all_train_labels = np.concatenate(train_labels)\n        \n        # Tidy\n        del outputs, targets, train_preds, train_labels, loss_value, loss, ids, mask, ttis\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        return running_loss / len(self.train_loader)\n\n    @torch.no_grad()\n    def valid_one_epoch(self):\n        \"\"\"\n        Validates the model for 1 epoch\n        \"\"\"\n        self.model.eval()\n        valid_pbar = tqdm(enumerate(self.valid_loader), total=len(self.valid_loader))\n        valid_preds, valid_targets = [], []\n        running_val_loss = 0\n\n        for idx, cache in valid_pbar:\n            ids = self._convert_if_not_tensor(cache['ids'], dtype=torch.long)\n            mask = self._convert_if_not_tensor(cache['mask'], dtype=torch.long)\n            ttis = self._convert_if_not_tensor(cache['token_type_ids'], dtype=torch.long)\n            targets = self._convert_if_not_tensor(cache['targets'], dtype=torch.long)\n\n            outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis)\n            valid_loss = self.valid_loss_fn(outputs, targets)\n            running_val_loss += valid_loss.item()\n            \n            valid_pbar.set_description(desc=f\"val_loss: {valid_loss.item():.4f}\")\n\n            valid_preds += [outputs.cpu().numpy()]\n            valid_targets += [targets.cpu().numpy()]\n\n        all_valid_preds = np.concatenate(valid_preds)\n        all_valid_targets = np.concatenate(valid_targets)\n\n        # Tidy\n        del ids, mask, ttis, targets, outputs, valid_loss, valid_preds, valid_targets\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        return all_valid_preds, all_valid_targets, running_val_loss / len(self.valid_loader)\n\n    def fit(self, fold: str, epochs: int = 10, output_dir: str = \"/kaggle/working/\", custom_name: str = 'model.pth'):\n        \"\"\"\n        Low-effort alternative for doing the complete training and validation process\n        \"\"\"\n        best_loss = int(1e+7)\n        for epx in range(epochs):\n            print(f\"{'='*20} Epoch: {epx+1} / {epochs} {'='*20}\")\n\n            train_running_loss = self.train_one_epoch()\n            print(f\"Training loss: {train_running_loss:.4f}\")\n\n            valid_preds, valid_targets, valid_loss = self.valid_one_epoch()\n            # valid_loss = log_loss(valid_targets, valid_preds)\n            print(f\"Validation loss: {valid_loss:.4f}\")\n\n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                self.save_model(output_dir, custom_name)\n                print(f\"Saved model with val_loss: {best_loss:.4f}\")\n            \n            # Log\n            wandb_log(\n                train_loss=train_running_loss,\n                val_loss=valid_loss\n            )\n            \n    def save_model(self, path, name, verbose=False):\n        \"\"\"\n        Saves the model at the provided destination\n        \"\"\"\n        try:\n            if not os.path.exists(path):\n                os.makedirs(path)\n        except:\n            print(\"Errors encountered while making the output directory\")\n\n        torch.save(self.model.state_dict(), os.path.join(path, name))\n        if verbose:\n            print(f\"Model Saved at: {os.path.join(path, name)}\")\n\n    def _convert_if_not_tensor(self, x, dtype):\n        if self._tensor_check(x):\n            return x.to(self.device, dtype=dtype)\n        else:\n            return torch.tensor(x, dtype=dtype, device=self.device)\n\n    def _tensor_check(self, x):\n        return isinstance(x, torch.Tensor)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:33.03702Z","iopub.execute_input":"2022-05-26T14:23:33.037763Z","iopub.status.idle":"2022-05-26T14:23:33.105417Z","shell.execute_reply.started":"2022-05-26T14:23:33.037716Z","shell.execute_reply":"2022-05-26T14:23:33.104403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    kf = StratifiedKFold(n_splits=Config['N_SPLITS'])\n    train_file = pd.read_csv(Config['CSV_PATH'])\n    train_file['discourse_effectiveness'] = train_file['discourse_effectiveness'].map(\n        {'Ineffective': 0, 'Adequate': 1, 'Effective': 2}\n    )\n    train_file['essay'] = train_file['essay_id'].apply(fetchEssay)\n    train_file['text'] = train_file['discourse_text'] + ' [SEP] ' + train_file['essay']\n    train_file = train_file.drop(['discourse_text', 'essay'], axis=1)\n    \n    for fold_, (train_idx, valid_idx) in enumerate(kf.split(X=train_file, y=train_file['essay_id'])):\n        print(f\"{'='*40} Fold: {fold_+1} / {Config['N_SPLITS']} {'='*40}\")\n        \n        train_ = train_file.loc[train_idx]\n        valid_ = train_file.loc[valid_idx]\n        \n        train_set = BERTDataset(\n            data = train_,\n            config = Config,\n        )\n        valid_set = BERTDataset(\n            data = valid_,\n            config = Config,\n        )\n        \n        train_loader = DataLoader(\n            train_set,\n            batch_size = Config['TRAIN_BS'],\n            shuffle = True,\n            num_workers = Config['NUM_WORKERS'],\n            pin_memory = True\n        )\n        \n        valid_loader = DataLoader(\n            valid_set,\n            batch_size = Config['VALID_BS'],\n            shuffle = False,\n            num_workers = Config['NUM_WORKERS'],\n        )\n        \n        model = BERTModel(backbone_arch=Config['MODEL_NAME'])\n        model = model.to(torch.device(Config['DEVICE']))\n        \n        # Log model to WandB\n        wandb.watch(model)\n            \n        optimizer = torch.optim.AdamW(model.parameters(), lr=Config['LR'])\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer, \n            T_0=Config['T_0'], \n            eta_min=Config['Œ∑_min']\n        )\n        train_lfn, valid_lfn = nn.CrossEntropyLoss(), nn.CrossEntropyLoss()\n        \n        trainer = Trainer(\n            config = Config,\n            dataloaders=(train_loader, valid_loader),\n            loss_fns=(train_lfn, valid_lfn),\n            optimizer=optimizer,\n            model = model,\n            scheduler=scheduler,\n            apex=True\n        )\n        \n        trainer.fit(\n            fold = fold_,\n            epochs = Config['NB_EPOCHS'],\n            custom_name = f\"{Config['MODEL_NAME']}_fold_{fold_}.bin\"\n        )","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:23:33.107737Z","iopub.execute_input":"2022-05-26T14:23:33.108387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTEBOOK IS STILL UNDER PROGRESS**","metadata":{}}]}