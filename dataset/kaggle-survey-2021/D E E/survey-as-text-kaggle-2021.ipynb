{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Survey as Text: An Open Approach to Kaggle's Annual Machine Learning and Data Science Survey Competition","metadata":{}},{"cell_type":"markdown","source":"## I've built some tools for [Kaggle's annual Machine Learning and Data Science Survey competition](https://www.kaggle.com/c/kaggle-survey-2021).\n\n#### Below I use them to analyze the [Kaggle 2021 response dataset](https://www.kaggle.com/c/kaggle-survey-2021/data).\n\n**First**, identifying the Kaggle data's topic-structure at a high level. **Next** selecting and describing one Kaggle user-group. **Then** looking at associations within and across the Kaggle data and other datasets, identifying and analyzing coding language usage rates across countries, cities and attempting to suggest life-details of the selected user group. **Finally**, arguing that a story has emerged.      ","metadata":{}},{"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/2/2a/Funerary_text%2C_Egypt%2C_hieratic_script%2C_Third_Intermediate_Period%2C_1069-664_BC%2C_linen_with_ink_-_Albany_Institute_of_History_and_Art_-_DSC08195.JPG)\n\n*Above: Funerary text, Egypt, hieratic script, Third Intermediate Period, 1069-664 BC, linen with ink - Albany Institute of History and Art [Wikipedia Commons](https://upload.wikimedia.org/wikipedia/commons/2/2a/Funerary_text%2C_Egypt%2C_hieratic_script%2C_Third_Intermediate_Period%2C_1069-664_BC%2C_linen_with_ink_-_Albany_Institute_of_History_and_Art_-_DSC08195.JPG)*","metadata":{}},{"cell_type":"code","source":"!pip install ruptures\n!pip install country_converter\n!pip install resize-and-crop\n!pip install image_tools\nfrom IPython.display import Markdown as md\n!jupyter nbextension enable --py widgetsnbextension\nimport ipywidgets as widgets\n\nfrom IPython.display import display\nimport ipywidgets as widgets\nimport warnings\nimport sys\nimport spacy\nimport sklearn\nimport ruptures as rpt\nimport re\nimport random as rn\nimport random\nimport pandas as pd\nimport os\nimport numpy as np\nimport nltk; nltk.download('stopwords')\nimport nltk\nimport matplotlib.pyplot as plt\nimport math\nimport logging\nimport image_tools\nimport glob\nimport gensim.corpora as corpora\nimport gensim\nimport gc\nfrom sklearn.cluster import KMeans\nfrom resize_and_crop import resize_and_crop\nfrom pprint import pprint\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nfrom matplotlib.pyplot import figure\nfrom IPython.display import Image\nfrom image_tools.sizes import resize_and_crop\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom collections import OrderedDict\npd.set_option('display.max_colwidth', None)\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\n%matplotlib inline\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-30T21:11:26.012709Z","iopub.execute_input":"2021-11-30T21:11:26.0136Z","iopub.status.idle":"2021-11-30T21:12:10.58124Z","shell.execute_reply.started":"2021-11-30T21:11:26.01344Z","shell.execute_reply":"2021-11-30T21:12:10.580357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.  The land","metadata":{}},{"cell_type":"markdown","source":"Our initial [**Exploratory data analysis (EDA)**](https://en.wikipedia.org/wiki/Exploratory_data_analysis) of the Kaggle 2021 data will not be traditional. Instead we will make a **topic map** of the most important words and phrases occurring in the raw text of the survey responses and in the document headers.\n\nThis is done mainly with a quick implementation of [**Selva Prabhakaran**](https://github.com/selva86)'s 2018 tutorial [**Topic Modelling with Gensim**](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/), a powerful [**Natural Language Processing (NLP)**](https://www.ibm.com/cloud/learn/natural-language-processing) tool that provides a bird's-eye view into the survey data by extracting **keywords** which seem to comprise **high-level topics** of the survey.\n\nStart by opening the Kaggle 2021 survey data, the 'responses.csv' document, in its raw text format.","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\ndf = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\nc=int(len(df.columns))\nn=int(len(df))\nx=2500\n\ndf=df.fillna(0)\ndf=df.sample(n=x)\n\ncolumns = list(df. columns)\ndf=df.astype(str)\ndf=df+\"   \"\ndf['period'] = df[columns].astype(str).sum(axis=1)\ndf = df[df.columns[df.columns.isin(['index','period'])]] \ndfz=df['period'][:1]\ndf.shape\ndf=df['period'].to_list()\ndf=[s.strip(' None ') for s in df]\ndf=[s.strip(' Never ') for s in df]\ndf=[s.strip(' Other ') for s in df]\ndfz=[s.strip(' None ') for s in dfz]\ndfz=[s.strip(' Never ') for s in dfz]\ndfz=[s.strip(' Other ') for s in dfz]\ndf=[s.strip('$') for s in df]\ndf=[s.strip('*') for s in df]\ndfz=[s.strip('$') for s in dfz]\ndfz=[s.strip('*') for s in dfz]\n\n\n\n\nfrom IPython.display import Markdown as md\nmd(str('<sub><sup>'+str(dfz)+'<sub><sup>'))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:12:10.583293Z","iopub.execute_input":"2021-11-30T21:12:10.583695Z","iopub.status.idle":"2021-11-30T21:12:13.761553Z","shell.execute_reply.started":"2021-11-30T21:12:10.58365Z","shell.execute_reply":"2021-11-30T21:12:13.759555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output above is one row of the spreadsheet, the complete response-set for one respondent. The way we are working with this spreadsheet, each row is just a long string of raw text like this.","metadata":{}},{"cell_type":"code","source":"\nmd(f\"The *'responses.csv'* spreadsheet contains **{n}** observations across **{c}** columns, but we have joined all the columns into one, so we have **{n}** observations across only **one** column.\")\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:12:13.76301Z","iopub.execute_input":"2021-11-30T21:12:13.763487Z","iopub.status.idle":"2021-11-30T21:12:13.771125Z","shell.execute_reply.started":"2021-11-30T21:12:13.763444Z","shell.execute_reply":"2021-11-30T21:12:13.769742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use a random sample of **2500** observations for this first analysis task: making the topic map.","metadata":{}},{"cell_type":"markdown","source":"## Topic Map\n\nTo build the topic map we break up our text strings into words and word-groupings, assigning numeric values to each based on frequency of occurrence. \n\nWith our words turned into numbers and a few [distribution assumptions](https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd), we assign topics to each word and *rearrange* (1) *the topics within each string*, and (2) *the words under each topic* “to obtain a good composition of topic-keywords distribution” (Prabhakaran, 2018) of our text collection (which, remember is comprised of the response-sets for each respondent in the 'responses.csv' document).\n\n<span style=\"background-color: #F9F5AC\">See below for the readout of keywords in our topic and their \"weights\", or relevance to the topic.</span>  ","metadata":{}},{"cell_type":"code","source":"\ndata = df\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\ndata = [re.sub(\"None\", \"\", sent) for sent in data]\ndata = [re.sub(\"none\", \"\", sent) for sent in data]\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\ndata_words = list(sent_to_words(data))\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=90000)\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=90000)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\ndef lemmatization(texts, allowed_postags=['PROPN', 'NOUN', 'PART' 'X', 'CCONJ', 'ADV', 'VERB']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\ndata_words_nostops = remove_stopwords(data_words)\ndata_words_bigrams = make_bigrams(data_words_nostops)\nnlp = spacy.load(\"en_core_web_sm\")\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['PROPN', 'NOUN', 'PART','X', 'CCONJ', 'ADV', 'VERB'])\nid2word = corpora.Dictionary(data_lemmatized)\ntexts = data_lemmatized\ncorpus = [id2word.doc2bow(text) for text in texts]\nid2word = corpora.Dictionary(data_lemmatized)\ntexts = data_lemmatized\ncorpus = [id2word.doc2bow(text) for text in texts]\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=2, \n                                           random_state=1,\n                                           update_every=1,\n                                           chunksize=1,\n                                           passes=1,\n                                           alpha='auto',\n                                           per_word_topics=False)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\nswt=str(lda_model.print_topics())\nmd(str((swt)))","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-30T21:12:13.77364Z","iopub.execute_input":"2021-11-30T21:12:13.774545Z","iopub.status.idle":"2021-11-30T21:13:07.904327Z","shell.execute_reply.started":"2021-11-30T21:12:13.774496Z","shell.execute_reply":"2021-11-30T21:13:07.903386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Storytelling Tools in the Toolbar** \n\nEDA data visualization tools are included in all spreadsheet applications, Who can live without quick bar charts pies, lines, and scatterplots? <span style=\"background-color: #F9F5AC\"> Think of this next step as putting a new button up in your toolbar called <span style=\"color:blue\">TOPIC MAP</span>.","metadata":{}},{"cell_type":"markdown","source":"The first part of our \"Topic Map\" pipeline was the preceding analysis. <span style=\"background-color: #F9F5AC\">Next we visualize the weights and map them: we chart topic-keywords in a labelled scatterplot where y axis values are the topic weights, and x axis values are the taken differences of each y observation from the following y observation.</span> \n\n**Colors and K Means** \n\nThe finishing touch of the topic map is clustering the keyword labels by performing a K-Means cluster analysis on the ranked values of x and y, assigning each value to one of five clusters. This cluster assignment just determines the color of the marker on the scatter plot and *is only meant to provide an intuitive sense of groupings in the chart data.*\n\n<span style=\"background-color: #E4DDF3\">We perform clustering analyses and assign the variables to k-mean clusters in order to color in the markers on our scatterplots throughout this notebook without much explanation.</span> <span style=\"background-color: #F9F5AC\">We are effectively treating this option as another independent, new option in the EDA toolbar for scatterplots. It might be called <span style=\"color:red\">CLUSTER COLOR</span>. \n\nNote: more-detailed usage and explanation of K-Means clustering is given in a later section.","metadata":{}},{"cell_type":"code","source":"%%capture\nflip=re.findall(r'\\b\\d+\\b',swt)\nsp=pd.DataFrame(flip)\nsp=sp.astype('int')\nsp\nsp= sp[sp[0] !=0]\nspx=sp\nspx\nqwpe=pd.DataFrame(spx[0])\n#qwpe=qwpe[-10:]\n#qwpe=qwpe.reset_index()\nqwpe\n#sp=sp[-10:]\n#str(quoted.findall(swt))[1]\n\nimport re\nquoted = re.compile('\"[^\"]*\"')\nfor i in range(10):\n  sp.iloc[i]=str(quoted.findall(swt)[i])\n  #if i is 1:\nsp    \n\nsp=sp[:10]\nsp=sp.reset_index()\nspx=spx.reset_index()\nqwpe=qwpe.reset_index()\nsp['ti']=qwpe[0]\n\n\nsp['kfx']=qwpe[0]\n\nimport random as rn\nedc=int(rn.randrange(1,9))\n\n\nsp[0]\nsp['kfi']=sp[0]\n\nsp = sp[sp.columns[sp.columns.isin([0,'kfx'])]] \nsp[0]=pd.DataFrame(sp[0]).applymap(lambda x: x.replace('\"', ''))\nsp=sp.replace('machine', 'machine learning')\nsp\n\n\n#sp[0]\nsp['kfx']=sp['kfx'].astype('int')*.001\n\nfloat(sp['kfx'].loc[i]+(float(rn.randrange(1,9)))*.9)\nsp['kx']=sp['kfx']\nfor i in range (int(len(sp['kfx']))):\n  #sp['kx']=sp['kfx']\n  sp['kx'].loc[i]=float(sp['kfx'].loc[i]+(float(rn.randrange(1,9)))*.001)\nsp['cha']=sp['kx'].pct_change() * 1\n\n\nsp=sp.bfill()\nsp[:1]['cha']=float(sp[:1]['cha'])*.1+sp[:1]['cha']\n\nsp['x'] = sp['cha'].rank()\nsp['y'] = sp['kx'].rank()\nval=sp[0]\n\n# Convert to numpy\nxs = sp[ 'x'].to_numpy()#np.random.randint( 0, 10, size=10)\nys = sp['y'].to_numpy()#np.random.randint(-5, 5,  size=10)\nval=sp[0].to_numpy()\nsp['topic']=sp[0]\nsp['weight']=sp['kfx']\nspii=sp[sp.columns[sp.columns.isin(['topic','weight'])]] \nspii.style.bar().hide_index()\nboob = sp[sp.columns[sp.columns.isin(['x','y'])]] \nboob\n\nmat = boob.values\n# Using sklearn\nkm = sklearn.cluster.KMeans(n_clusters=5)\nkm.fit(mat)\nlabels = km.labels_\n# Format results as a DataFrame\nresults = pd.DataFrame([boob.index,labels]).T\n#erb=results[1].to_numpy()\nsp['cluser']=results[1]\n\ns=pd.DataFrame([i**2*2+2 for i in list(sp['cha'])])\ns=s*-10\ns=s.rank()\ns=s*10\n\nfrom matplotlib.font_manager import FontProperties\ndef maro():\n  import matplotlib.pyplot as plt\n  plt.rcParams['font.family'] = 'serif'\n  plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n  fig, ax = plt.subplots()\n  ax.axis(\"off\")\n  cola=[0, 1, 2]\n  groups = sp.groupby('cluser')\n  colors={0:'SaddleBrown', 1:'Indigo', 2:'DarkOliveGreen', 3:'DarkGoldenrod', 4: 'FireBrick'}\n  #plt.style.use('dark_background')\n\n    \n  plt.scatter(x=xs,y=ys, s=40,c=sp['cluser'].map(colors))#c='#E0DBFF')\n  \n\n  for x,y,z in zip(xs,ys,val):\n    label = z\n    \n#for name, group in groups:\n#    plt.plot(group[\"X Value\"], group[\"Y Value\"], marker=\"o\", linestyle=\"\", label=name)\n    # font from OS\n    #hfont = {'fontname':'Wingdings'}\n\n    #plt.title(\"Topics\", \n   #           loc='Center', fontsize=26, **hfont)\n\n    \n    plt.title('Topic Keywords: 2021 Kaggle Survey Responses', y=1.12, fontsize=18)\n    #plt.style.use('dark_background')\n    plt.annotate(label, # this is the, text\n                 (x,y), # these are the coordinates to position the label\n                 textcoords=\"offset points\", # how to position the text\n                 size=12,\n                 #doll={0:'red', 1:'blue', 2:'green', 3:'black', 4: 'yellow'},\n                 color='grey',#sp['kfi'].map({0:'r', 1:'b', 2:'g', 3:'k', 4: 'o'}),\n                 xytext=(-1,10), # distance from text to points (x,y)\n                 ha='center') # horizontal alignment can be left, right or center\n\n\n\n    #plt.savefig('output.png', dpi=300)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-30T21:13:07.905908Z","iopub.execute_input":"2021-11-30T21:13:07.906408Z","iopub.status.idle":"2021-11-30T21:13:08.045168Z","shell.execute_reply.started":"2021-11-30T21:13:07.906373Z","shell.execute_reply":"2021-11-30T21:13:08.044286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Okay, we can now generate our topic map. ","metadata":{}},{"cell_type":"code","source":"maro()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:13:08.046617Z","iopub.execute_input":"2021-11-30T21:13:08.046955Z","iopub.status.idle":"2021-11-30T21:13:08.252197Z","shell.execute_reply.started":"2021-11-30T21:13:08.046914Z","shell.execute_reply":"2021-11-30T21:13:08.251328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Look what we have here: If someone out of the blue handed you, the reader a copy of the Kaggle survey response dataset and asked \"what's the survey all about?\", you could probably infer something about the lay of the land","metadata":{}},{"cell_type":"markdown","source":"Now of course we see, or rather don't see that  **there is nothing personal showing up in these initial results**. We can't tell a human story with just this, and of course it was meant to be a high level look. Now we go in deep.\n\n## Look at another raw text-string:","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\ndf = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\nc=int(len(df.columns))\nn=int(len(df))\nx=2500\n\ndf=df.fillna(0)\ndf=df.sample(n=x)\n\ncolumns = list(df. columns)\ndf=df.astype(str)\ndf=df+\"   \"\ndf['period'] = df[columns].astype(str).sum(axis=1)\ndf = df[df.columns[df.columns.isin(['index','period'])]] \ndfz=df['period'][:1]\ndf.shape\ndf=df['period'].to_list()\ndf=[s.strip(' None ') for s in df]\ndf=[s.strip(' Never ') for s in df]\ndf=[s.strip(' Other ') for s in df]\ndfz=[s.strip(' None ') for s in dfz]\ndfz=[s.strip(' Never ') for s in dfz]\ndfz=[s.strip(' Other ') for s in dfz]\ndf=[s.strip('$') for s in df]\ndf=[s.strip('*') for s in df]\ndfz=[s.strip('$') for s in dfz]\ndfz=[s.strip('*') for s in dfz]\n\n\nfrom IPython.display import Markdown as md\nmd(str(str(dfz)))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:13:08.253728Z","iopub.execute_input":"2021-11-30T21:13:08.254072Z","iopub.status.idle":"2021-11-30T21:13:10.970047Z","shell.execute_reply.started":"2021-11-30T21:13:08.254042Z","shell.execute_reply":"2021-11-30T21:13:10.969021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"background-color: #F9F5AC\"> There are some personal labels there, obviously, but the majority of the data-points we're working with are technical terms and names of corporate entities, products, and platforms (not to mention a ton of zeros). \n\n## <span style=\"background-color: #F9F5AC\"> To <span style=\"color:blue\">break into this data and find the humanity</span>, we will \"explode\" the dataset with a special encoding technique, thereby making human traits more readily seen by our machine. ","metadata":{}},{"cell_type":"markdown","source":"# 2. All data can explode","metadata":{}},{"cell_type":"markdown","source":"### One-hot Encoding\n\nOne-hot encoding is a data pre-processing technique that can help machines see things in the data they might not otherwise. With one-hot encoding, every column's value... \n\n> ... is converted into a new column and assigned a 1 or 0 (notation for true/false) value to the column. - https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n\nThis creates a far larger and more complex dataset.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\ndfw=df.fillna(0)\nnew_header = dfw.iloc[0] \ndfw = dfw[1:] \ndfw.columns = new_header\ndfcow=dfw['Duration (in seconds)']\ndfw=dfw.drop(['Duration (in seconds)'], axis=1)\ndfw=dfw.astype('str')\ndfw=pd.get_dummies(dfw)\ndfw.astype('float')\nmd(f\"The one-hot-encoded dataframe is big indeed. **{str(int(len(dfw.columns)-int(len(df.columns))))}** columns were added for total of **{str(int(len(dfw.columns)))}** columns across the **{int(len(dfw))}** observations. \")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:13:10.971395Z","iopub.execute_input":"2021-11-30T21:13:10.971618Z","iopub.status.idle":"2021-11-30T21:13:16.125916Z","shell.execute_reply.started":"2021-11-30T21:13:10.97159Z","shell.execute_reply":"2021-11-30T21:13:16.124896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the encoding, I visualize a map of 1's of 0's. Below are 500 rows of dataset as 500 x 968 grid of 1's and 0's. Dataset is **25973** rows, **968** columns after one-hot encoding.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\ndfw=df.fillna(0)\nnew_header = dfw.iloc[0] \ndfw = dfw[1:] \ndfw.columns = new_header\ndfcow=dfw['Duration (in seconds)']\ndfw=dfw.drop(['Duration (in seconds)'], axis=1)\ndfw=dfw.astype('str')\ndfw=pd.get_dummies(dfw)\ndfw.astype('float')\n\n\n\nzsl=dfw[:500]\nzs=zsl.values#dfw.values\n\n# IMAGE\n\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,7)\n#plt.subplot(211)\nplt.imshow(zs)\n#plt.subplot(212)\nplt.imshow(zs, cmap='bone',  interpolation='nearest')\nplt.axis('off')\nplt.rcParams[\"figure.figsize\"] = (20,20)\n#plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:13:16.12774Z","iopub.execute_input":"2021-11-30T21:13:16.128099Z","iopub.status.idle":"2021-11-30T21:13:25.821559Z","shell.execute_reply.started":"2021-11-30T21:13:16.128057Z","shell.execute_reply":"2021-11-30T21:13:25.820521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I find visualizations of one-hot encoded data beautiful, they remind me of DNA staining.","metadata":{}},{"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/7/76/RAPD-profiles-of-genomic-DNA-isolated-from-the-leaves-of-Mentha-arvensis-seedlings-after-12-days-of-Hg-treatment-along-w.jpg)","metadata":{}},{"cell_type":"markdown","source":"## k-means again\n\nWith the one-hot encoded data in hand, we assign each of the **968** columns to one of five clusters with another k-means analysis. This is a bigger operation than the <span style=\"color:red\">CLUSTER COLOR</span> button we built earlier using k-means and warrants more discussion on clustering analysis generally and k-means specifically: \n\n**Generally**...\n\n> ...**clustering** aims at forming subsets or groups within a dataset consisting of data points which are really similar to each other and the groups or subsets or clusters formed can be significantly differentiated from each other.\n\n**Specifically**...\n\n> **K-Means** Clustering is an Unsupervised Learning algorithm, used to group the unlabeled dataset into different clusters...\n\n\nThe K-Means clustering algorithm detect similarity and difference in the structure of the dataset and groups the variables (our **968** columns) according to the structure. The diagram below from Wikipedia explains the general idea of clustering better than words.    \n\nNow imagine the simple diagram below has **968** dots getting split inside five clusters instead of three. That's roughly what we're about to do now. ","metadata":{}},{"cell_type":"markdown","source":"\n<a href=\"url\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/FactorAnalysis_ConceptualModel_DotsRings.png\" align=\"center\" height=\"280\" width=\"280\" ></a>","metadata":{}},{"cell_type":"markdown","source":"Performing even large k-means analysis with Python is simple in practice and done in a few lines of code. The fundamental operation used next is downright laconic:\n\n$$ kmeans = KMeans(nclusters=5).fit(df) $$\n\nLet us execute. ","metadata":{}},{"cell_type":"markdown","source":"Now see below: a dataframe is generated from this analysis with five rows, one for each cluster in the model, for all **968** columns.\n\n* The rows contain the \"loadings\" of each column for the cluster, how strongly the column is \"in\" each cluster. \n\n* Each column in the response data is \"loaded\" on each of the clusters at some amount over zero. \n\n#### **Columns** are **assigned** to clusters according to their **maximum values**, shown shaded in grey. ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\ndf=df.fillna(0)\nnew_header = df.iloc[0] \ndf = df[1:] \ndf.columns = new_header\ndfco=df['Duration (in seconds)']\ndf=df.drop(['Duration (in seconds)'], axis=1)\ndf=df.astype('str')\ndf=pd.get_dummies(df)\ndf.astype('str')\ndat1=df\ndfx=pd.DataFrame(dat1.columns.to_list())\ndfx=dfx.T\nkmeans = KMeans(n_clusters=5).fit(df)\ncentroids = kmeans.cluster_centers_\npd.set_option('precision',10)\ndfg=pd.DataFrame(centroids)\ndfg=dfg.T\n#dfg=dfg.rank()\ndfg=dfg.T\ndfg\nyesp=dfg.T\nyesp['-']=dfx.T\nyesp.index=yesp['-']\nyesp=yesp.drop(['-'], axis=1).T\nyesp\nyesper=yesp\nindex=['cluster 1', 'cluster 2', 'cluster 3', 'cluster 4', 'cluster 5']\nyesper.index=index\nsdea=yesper.iloc[: , -5:]\nsdea.style.highlight_max(color = 'lightgrey', axis = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:13:25.825617Z","iopub.execute_input":"2021-11-30T21:13:25.826256Z","iopub.status.idle":"2021-11-30T21:14:10.032348Z","shell.execute_reply.started":"2021-11-30T21:13:25.826199Z","shell.execute_reply":"2021-11-30T21:14:10.03084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### The five-row sample below transposes the rows and columns of the table above. \n\nIt is meant to show the structure of the data: a row of cluster assignments, a row of weights, and labels (original columns names) on the left.  ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\ndf=df.fillna(0)\nnew_header = df.iloc[0] \ndf = df[1:] \ndf.columns = new_header\ndfco=df['Duration (in seconds)']\ndf=df.drop(['Duration (in seconds)'], axis=1)\ndf=df.astype('str')\ndf=pd.get_dummies(df)\ndf.astype('str')\ndat1=df\ndfx=pd.DataFrame(dat1.columns.to_list())\ndfx=dfx.T\nkmeans = KMeans(n_clusters=5).fit(df)\ncentroids = kmeans.cluster_centers_\npd.set_option('precision',10)\ndfg=pd.DataFrame(centroids)\ndfg=dfg.T\n#dfg=dfg.rank()\ndfg=dfg.T\ndfg\nyesp=dfg.T\nyesp['-']=dfx.T\nyesp.index=yesp['-']\nyesp=yesp.drop(['-'], axis=1).T\nyesp\nyesper=yesp\nindex=['cluster i', 'cluster ii', 'cluster iii', 'cluster iv', 'cluster v']\nyesper.index=index\nyesper.style.highlight_max(color = 'lightgrey', axis = 0)\n\ndfg1=pd.DataFrame(dfg.idxmax()).T\ndfg1.head()\n\ndfg2=pd.DataFrame(dfg.max()).T\ndfg2\n\ndfg3=dfg1.append(dfg2)\ndfg3\n\ndfg3=dfg3.append(dfx)\ndfg3\n\n\ndfg3=dfg3.T\ndfg3\n\n#dfg3\ndfg3.columns=['Cluster Assigned','Model Weights', 'Survey Q & A']\ndfg3['Cluster Assigned']=dfg3['Cluster Assigned'].astype(int)\n\n\n\n\ndfg4=dfg3#.sample(10)\n#dfg4.style.hide_index()\ncols = list(dfg4.columns)\ncols = [cols[-1]] + cols[:-1]\ndfg4 = dfg4[cols]\ndfgyu=dfg4[-5:]\n\ndfgyu.style.hide_index()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-30T21:14:10.034704Z","iopub.execute_input":"2021-11-30T21:14:10.035209Z","iopub.status.idle":"2021-11-30T21:14:58.421203Z","shell.execute_reply.started":"2021-11-30T21:14:10.035162Z","shell.execute_reply":"2021-11-30T21:14:58.420159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Every label in the 'Survey Q & A' column is associated with the cluster where it loaded strongest, where it showed the highest model weight. \n\n### Our columns have been **clustered**, and we've flipped the table axes so column names are now the row labels. \n\nNow to charting the Model Weights for each cluster:   ","metadata":{}},{"cell_type":"code","source":"cvrb=4#rn.randint(1,5)-1\nis_2002 =  dfg4['Cluster Assigned']==cvrb\n#print('CLUSTER __  ',  cvrb)\ngapminder_2002 = dfg4[is_2002]\n#gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n#gapminder_2002.head(30)\n\n\n\n\n\ngapminder_2002=gapminder_2002.reset_index()\n#gapminder_2002\n#gapminder_2002['Model Weights'] = gapminder_2002['Model Weights'].apply(lambda x: round(x, decimals))\n\n\ny = np.array(gapminder_2002['Model Weights'].to_list())\n#y\nn_breaks=5\nmodel = rpt.Dynp(model=\"l2\")\nmodel.fit(y)\nbreaks = model.predict(n_bkps=n_breaks-1)\nbreaks_rpt = []\nfor i in breaks:\n    breaks_rpt.append(gapminder_2002.index[i-1])\n    \n#gapminder_2002['Model Weights'].plot()\n\nbreaks_rpt\n\n# -\nplt.rcParams[\"figure.figsize\"] = (8,6)\nfig, ax = plt.subplots()\n#ax.plot(range(10))\n\nfig.patch.set_visible(True)\nax.axis('off')\n\nax.spines['bottom'].set_visible(True)\nplt.plot(y, color='k',label='sorted weights')\nplt.title('Unsorted Cluster Weights | Cluster ' + str(cvrb+1), y=1.12, fontsize=18)\n\nprint_legend=True\n\n\n        \n#plt.grid()\n\n#plt.legend()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:14:58.424126Z","iopub.execute_input":"2021-11-30T21:14:58.425102Z","iopub.status.idle":"2021-11-30T21:14:58.589763Z","shell.execute_reply.started":"2021-11-30T21:14:58.425051Z","shell.execute_reply":"2021-11-30T21:14:58.588276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def exy(): \n  is_2002 =  dfg4['Cluster Assigned']==0\n  gapminder_2002 = dfg4[is_2002]\n  #gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps1.csv')\n  y1 = np.array(gapminder_2002['Model Weights'].to_list())\n  x1=np.arange(0, int(len(y1)), 1)\n  np.savetxt('y1.txt', y1)\n  np.savetxt('x1.txt', x1)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n\n  is_2002 =  dfg4['Cluster Assigned']==1\n  gapminder_2002 = dfg4[is_2002]\n  ##gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps2.csv')\n  y2 = np.array(gapminder_2002['Model Weights'].to_list())\n  x2=np.arange(0, int(len(y2)), 1)\n  np.savetxt('y2.txt', y2)\n  np.savetxt('x2.txt', x2)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n\n  is_2002 =  dfg4['Cluster Assigned']==2\n  gapminder_2002 = dfg4[is_2002]\n  #gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps3.csv')\n  y3 = np.array(gapminder_2002['Model Weights'].to_list())\n  x3=np.arange(0, int(len(y3)), 1)\n  np.savetxt('y3.txt', y3)\n  np.savetxt('x3.txt', x3)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n  is_2002 =  dfg4['Cluster Assigned']==3\n  gapminder_2002 = dfg4[is_2002]\n  #gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps4.csv')\n  y4 = np.array(gapminder_2002['Model Weights'].to_list())\n  x4=np.arange(0, int(len(y4)), 1)\n  np.savetxt('y4.txt', y4)\n  np.savetxt('x4.txt', x4)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n\n  is_2002 =  dfg4['Cluster Assigned']==4\n  gapminder_2002 = dfg4[is_2002]\n  #gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps5.csv')\n\n  y5 = np.array(gapminder_2002['Model Weights'].to_list())\n  x5=np.arange(0, int(len(y5)), 1)\n  np.savetxt('y5.txt', y5)\n  np.savetxt('x5.txt', x5)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\nexy()\n\nx0 = np.loadtxt('x1.txt', dtype=float)\nx1 = np.loadtxt('x2.txt', dtype=float)\nx2 = np.loadtxt('x3.txt', dtype=float)\nx3 = np.loadtxt('x4.txt', dtype=float)\nx4 = np.loadtxt('x5.txt', dtype=float)\ny0= np.loadtxt('y1.txt', dtype=float)\ny1 = np.loadtxt('y2.txt', dtype=float)\ny2 = np.loadtxt('y3.txt', dtype=float)\ny3 = np.loadtxt('y4.txt', dtype=float)\ny4 = np.loadtxt('y5.txt', dtype=float)\nprint_legend=True\ny = np.array(gapminder_2002['Model Weights'].to_list())\n#y\nn_breaks=5\nmodel = rpt.Dynp(model=\"l2\")\n\n    \ndef ben():\n  f, (ax1, ax2,ax3, ax4, ax5) = plt.subplots(1,5,figsize=(17,2.4))\n\n\n  model.fit(y0)\n  breaks0 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt0 = []\n  for i in breaks0:\n    breaks_rpt0.append(x0[i-1])\n  blak=pd.DataFrame(breaks_rpt0)\n  awr=pd.read_csv('gaps1.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks0.csv')\n  ax1.plot(x0, y0,c='k',linestyle='dashed')\n  for i in breaks_rpt0:\n      if print_legend:\n          ax1.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax1.axvline(i, color='white')\n\n\n\n  model.fit(y1)\n  breaks1 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt1 = []\n  for i in breaks1:\n    breaks_rpt1.append(x1[i-1])\n  blak=pd.DataFrame(breaks_rpt1)\n  awr=pd.read_csv('gaps2.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks1.csv')\n  ax2.plot(x1, y1,c='k',linestyle='dashed')\n  for i in breaks_rpt1:\n      if print_legend:\n          ax2.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax2.axvline(i, color='white')\n\n\n  model.fit(y2)\n  breaks2 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt2 = []\n  for i in breaks2:\n    breaks_rpt2.append(x2[i-1])\n  blak=pd.DataFrame(breaks_rpt2)\n  awr=pd.read_csv('gaps3.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    \n    color='white'\n  \n  blak.to_csv('blecks2.csv')\n  ax3.plot(x2, y2,c='k',linestyle='dashed')\n  for i in breaks_rpt2:\n      if print_legend:\n          ax3.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax3.axvline(i, color='white')\n\n  model.fit(y3)\n  breaks3 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt3 = []\n  for i in breaks3:\n    breaks_rpt3.append(x3[i-1])\n  blak=pd.DataFrame(breaks_rpt3)\n  awr=pd.read_csv('gaps4.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks3.csv')\n  ax4.plot(x3, y3,c='k',linestyle='dashed')\n  for i in breaks_rpt3:\n      if print_legend:\n          ax4.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax4.axvline(i, color='white')\n\n\n  model.fit(y4)\n  breaks4 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt4 = []\n  for i in breaks4:\n    breaks_rpt4.append(x4[i-1])\n  blak=pd.DataFrame(breaks_rpt4)\n  awr=pd.read_csv('gaps5.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks4.csv')\n  ax5.plot(x4, y4,c='k',linestyle='dashed')\n  for i in breaks_rpt4:\n      if print_legend:\n          ax5.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax5.axvline(i, color='white')\n\n  ax1.axis('off')\n  ax2.axis('off')\n  ax3.axis('off')\n  ax4.axis('off')\n  ax5.axis('off')\n\n  ax1.set_title('Cluster ' + 'One', y=1.1,  fontsize=14)\n  ax2.set_title('Cluster ' + 'Two', y=1.1,  fontsize=14)\n  ax3.set_title('Cluster ' + 'Three', y=1.1,  fontsize=14)\n  ax4.set_title('Cluster ' + 'Four', y=1.1,  fontsize=14)\n  ax5.set_title('Cluster ' + 'Five', y=1.1,  fontsize=14)\nben()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:14:58.592586Z","iopub.execute_input":"2021-11-30T21:14:58.593099Z","iopub.status.idle":"2021-11-30T21:14:59.710572Z","shell.execute_reply.started":"2021-11-30T21:14:58.593044Z","shell.execute_reply":"2021-11-30T21:14:59.709853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now sort each of them. ","metadata":{}},{"cell_type":"code","source":"def exy(): \n  is_2002 =  dfg4['Cluster Assigned']==0\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps1.csv')\n  y1 = np.array(gapminder_2002['Model Weights'].to_list())\n  x1=np.arange(0, int(len(y1)), 1)\n  np.savetxt('y1.txt', y1)\n  np.savetxt('x1.txt', x1)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n\n  is_2002 =  dfg4['Cluster Assigned']==1\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps2.csv')\n  y2 = np.array(gapminder_2002['Model Weights'].to_list())\n  x2=np.arange(0, int(len(y2)), 1)\n  np.savetxt('y2.txt', y2)\n  np.savetxt('x2.txt', x2)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n\n  is_2002 =  dfg4['Cluster Assigned']==2\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps3.csv')\n  y3 = np.array(gapminder_2002['Model Weights'].to_list())\n  x3=np.arange(0, int(len(y3)), 1)\n  np.savetxt('y3.txt', y3)\n  np.savetxt('x3.txt', x3)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n  is_2002 =  dfg4['Cluster Assigned']==3\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps4.csv')\n  y4 = np.array(gapminder_2002['Model Weights'].to_list())\n  x4=np.arange(0, int(len(y4)), 1)\n  np.savetxt('y4.txt', y4)\n  np.savetxt('x4.txt', x4)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\n\n  is_2002 =  dfg4['Cluster Assigned']==4\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps5.csv')\n\n  y5 = np.array(gapminder_2002['Model Weights'].to_list())\n  x5=np.arange(0, int(len(y5)), 1)\n  np.savetxt('y5.txt', y5)\n  np.savetxt('x5.txt', x5)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    \n    ydycn=9\nexy()\n\nx0 = np.loadtxt('x1.txt', dtype=float)\nx1 = np.loadtxt('x2.txt', dtype=float)\nx2 = np.loadtxt('x3.txt', dtype=float)\nx3 = np.loadtxt('x4.txt', dtype=float)\nx4 = np.loadtxt('x5.txt', dtype=float)\ny0= np.loadtxt('y1.txt', dtype=float)\ny1 = np.loadtxt('y2.txt', dtype=float)\ny2 = np.loadtxt('y3.txt', dtype=float)\ny3 = np.loadtxt('y4.txt', dtype=float)\ny4 = np.loadtxt('y5.txt', dtype=float)\nprint_legend=True\n\ndef ben():\n  f, (ax1, ax2,ax3, ax4, ax5) = plt.subplots(1,5,figsize=(17,2.4))\n\n\n  model.fit(y0)\n  breaks0 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt0 = []\n  for i in breaks0:\n    breaks_rpt0.append(x0[i-1])\n  blak=pd.DataFrame(breaks_rpt0)\n  awr=pd.read_csv('gaps1.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks0.csv')\n  ax1.plot(x0, y0,c='k',linestyle='dashed')\n  for i in breaks_rpt0:\n      if print_legend:\n          ax1.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax1.axvline(i, color='white')\n\n\n\n  model.fit(y1)\n  breaks1 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt1 = []\n  for i in breaks1:\n    breaks_rpt1.append(x1[i-1])\n  blak=pd.DataFrame(breaks_rpt1)\n  awr=pd.read_csv('gaps2.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks1.csv')\n  ax2.plot(x1, y1,c='k',linestyle='dashed')\n  for i in breaks_rpt1:\n      if print_legend:\n          ax2.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax2.axvline(i, color='white')\n\n\n  model.fit(y2)\n  breaks2 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt2 = []\n  for i in breaks2:\n    breaks_rpt2.append(x2[i-1])\n  blak=pd.DataFrame(breaks_rpt2)\n  awr=pd.read_csv('gaps3.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    \n    color='white'\n  \n  blak.to_csv('blecks2.csv')\n  ax3.plot(x2, y2,c='k',linestyle='dashed')\n  for i in breaks_rpt2:\n      if print_legend:\n          ax3.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax3.axvline(i, color='white')\n\n  model.fit(y3)\n  breaks3 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt3 = []\n  for i in breaks3:\n    breaks_rpt3.append(x3[i-1])\n  blak=pd.DataFrame(breaks_rpt3)\n  awr=pd.read_csv('gaps4.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks3.csv')\n  ax4.plot(x3, y3,c='k',linestyle='dashed')\n  for i in breaks_rpt3:\n      if print_legend:\n          ax4.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax4.axvline(i, color='white')\n\n\n  model.fit(y4)\n  breaks4 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt4 = []\n  for i in breaks4:\n    breaks_rpt4.append(x4[i-1])\n  blak=pd.DataFrame(breaks_rpt4)\n  awr=pd.read_csv('gaps5.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='white'\n    #display(int(len(awq)))\n  else:\n    color='white'\n  blak.to_csv('blecks4.csv')\n  ax5.plot(x4, y4,c='k',linestyle='dashed')\n  for i in breaks_rpt4:\n      if print_legend:\n          ax5.axvline(i, color='white')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax5.axvline(i, color='white')\n\n  ax1.axis('off')\n  ax2.axis('off')\n  ax3.axis('off')\n  ax4.axis('off')\n  ax5.axis('off')\n\n  ax1.set_title('Cluster ' + 'One', y=1.1,  fontsize=14)\n  ax2.set_title('Cluster ' + 'Two', y=1.1,  fontsize=14)\n  ax3.set_title('Cluster ' + 'Three', y=1.1,  fontsize=14)\n  ax4.set_title('Cluster ' + 'Four', y=1.1,  fontsize=14)\n  ax5.set_title('Cluster ' + 'Five', y=1.1,  fontsize=14)\nben()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:14:59.711763Z","iopub.execute_input":"2021-11-30T21:14:59.713159Z","iopub.status.idle":"2021-11-30T21:15:00.43961Z","shell.execute_reply.started":"2021-11-30T21:14:59.713118Z","shell.execute_reply":"2021-11-30T21:15:00.438719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Slice up** the list of sorted weights using [**breakpoint detection**](https://towardsdatascience.com/getting-started-with-breakpoints-analysis-in-python-124471708d38), a technique sometimes used in time-series analyses. ","metadata":{}},{"cell_type":"code","source":"def exy(): \n  is_2002 =  dfg4['Cluster Assigned']==0\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps1.csv')\n  y1 = np.array(gapminder_2002['Model Weights'].to_list())\n  x1=np.arange(0, int(len(y1)), 1)\n  np.savetxt('y1.txt', y1)\n  np.savetxt('x1.txt', x1)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    yyysdv=8#')\n\n  is_2002 =  dfg4['Cluster Assigned']==1\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps2.csv')\n  y2 = np.array(gapminder_2002['Model Weights'].to_list())\n  x2=np.arange(0, int(len(y2)), 1)\n  np.savetxt('y2.txt', y2)\n  np.savetxt('x2.txt', x2)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    yyysdv=8#')\n\n\n  is_2002 =  dfg4['Cluster Assigned']==2\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps3.csv')\n  y3 = np.array(gapminder_2002['Model Weights'].to_list())\n  x3=np.arange(0, int(len(y3)), 1)\n  np.savetxt('y3.txt', y3)\n  np.savetxt('x3.txt', x3)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    yyysdv=8#')\n\n  is_2002 =  dfg4['Cluster Assigned']==3\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps4.csv')\n  y4 = np.array(gapminder_2002['Model Weights'].to_list())\n  x4=np.arange(0, int(len(y4)), 1)\n  np.savetxt('y4.txt', y4)\n  np.savetxt('x4.txt', x4)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    yyysdv=8#')\n\n  is_2002 =  dfg4['Cluster Assigned']==4\n  gapminder_2002 = dfg4[is_2002]\n  gapminder_2002=gapminder_2002.sort_values(by=['Model Weights'])\n  gapminder_2002=gapminder_2002.reset_index()\n  gapminder_2002.to_csv('gaps5.csv')\n\n  y5 = np.array(gapminder_2002['Model Weights'].to_list())\n  x5=np.arange(0, int(len(y5)), 1)\n  np.savetxt('y5.txt', y5)\n  np.savetxt('x5.txt', x5)\n  try:\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    vw=int(vw[0])\n    np.savetxt('fee.txt', vw)\n\n  except:\n    yyysdv=8#')\nexy()\n\nx0 = np.loadtxt('x1.txt', dtype=float)\nx1 = np.loadtxt('x2.txt', dtype=float)\nx2 = np.loadtxt('x3.txt', dtype=float)\nx3 = np.loadtxt('x4.txt', dtype=float)\nx4 = np.loadtxt('x5.txt', dtype=float)\ny0= np.loadtxt('y1.txt', dtype=float)\ny1 = np.loadtxt('y2.txt', dtype=float)\ny2 = np.loadtxt('y3.txt', dtype=float)\ny3 = np.loadtxt('y4.txt', dtype=float)\ny4 = np.loadtxt('y5.txt', dtype=float)\nprint_legend=True\n\ndef ben():\n  f, (ax1, ax2,ax3, ax4, ax5) = plt.subplots(1,5,figsize=(17,2.4))\n\n\n  model.fit(y0)\n  breaks0 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt0 = []\n  for i in breaks0:\n    breaks_rpt0.append(x0[i-1])\n  blak=pd.DataFrame(breaks_rpt0)\n  awr=pd.read_csv('gaps1.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='grey'\n    #display(int(len(awq)))\n  else:\n    color='grey'\n  blak.to_csv('blecks0.csv')\n  ax1.plot(x0, y0,c='k',linestyle='dashed')\n  for i in breaks_rpt0:\n      if print_legend:\n          ax1.axvline(i, color='grey')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax1.axvline(i, color='grey')\n\n\n\n  model.fit(y1)\n  breaks1 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt1 = []\n  for i in breaks1:\n    breaks_rpt1.append(x1[i-1])\n  blak=pd.DataFrame(breaks_rpt1)\n  awr=pd.read_csv('gaps2.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='grey'\n    #display(int(len(awq)))\n  else:\n    color='grey'\n  blak.to_csv('blecks1.csv')\n  ax2.plot(x1, y1,c='k',linestyle='dashed')\n  for i in breaks_rpt1:\n      if print_legend:\n          ax2.axvline(i, color='grey')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax2.axvline(i, color='grey')\n\n\n  model.fit(y2)\n  breaks2 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt2 = []\n  for i in breaks2:\n    breaks_rpt2.append(x2[i-1])\n  blak=pd.DataFrame(breaks_rpt2)\n  awr=pd.read_csv('gaps3.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='grey'\n    #display(int(len(awq)))\n  else:\n    \n    color='grey'\n  \n  blak.to_csv('blecks2.csv')\n  ax3.plot(x2, y2,c='k',linestyle='dashed')\n  for i in breaks_rpt2:\n      if print_legend:\n          ax3.axvline(i, color='grey')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax3.axvline(i, color='grey')\n\n  model.fit(y3)\n  breaks3 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt3 = []\n  for i in breaks3:\n    breaks_rpt3.append(x3[i-1])\n  blak=pd.DataFrame(breaks_rpt3)\n  awr=pd.read_csv('gaps4.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='grey'\n    #display(int(len(awq)))\n  else:\n    color='grey'\n  blak.to_csv('blecks3.csv')\n  ax4.plot(x3, y3,c='k',linestyle='dashed')\n  for i in breaks_rpt3:\n      if print_legend:\n          ax4.axvline(i, color='grey')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax4.axvline(i, color='grey')\n\n\n  model.fit(y4)\n  breaks4 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt4 = []\n  for i in breaks4:\n    breaks_rpt4.append(x4[i-1])\n  blak=pd.DataFrame(breaks_rpt4)\n  awr=pd.read_csv('gaps5.csv')\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    #print('goteem')\n    color='grey'\n    #display(int(len(awq)))\n  else:\n    color='grey'\n  blak.to_csv('blecks4.csv')\n  ax5.plot(x4, y4,c='k',linestyle='dashed')\n  for i in breaks_rpt4:\n      if print_legend:\n          ax5.axvline(i, color='grey')\n          #print_legend = False\n      else:\n          #print_legend = True \n          ax5.axvline(i, color='grey')\n\n  ax1.axis('off')\n  ax2.axis('off')\n  ax3.axis('off')\n  ax4.axis('off')\n  ax5.axis('off')\n\n  ax1.set_title('Cluster ' + 'One', y=1.1,  fontsize=14)\n  ax2.set_title('Cluster ' + 'Two', y=1.1,  fontsize=14)\n  ax3.set_title('Cluster ' + 'Three', y=1.1,  fontsize=14)\n  ax4.set_title('Cluster ' + 'Four', y=1.1,  fontsize=14)\n  ax5.set_title('Cluster ' + 'Five', y=1.1,  fontsize=14)\nben()","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-30T21:15:00.441372Z","iopub.execute_input":"2021-11-30T21:15:00.44161Z","iopub.status.idle":"2021-11-30T21:15:01.145206Z","shell.execute_reply.started":"2021-11-30T21:15:00.441581Z","shell.execute_reply":"2021-11-30T21:15:01.144356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ABOVE: We have five clusters, each sliced into five segments. \n\n### That gives us **25** lists of survey items like 'what is your age', 'what is your gender', 'where do you live.' There are **968** total items, and each of our  25 lists is made up of a different number of them. \n    \n### <span style=\"background-color: #E1E2FF\"> We are betting the items in each list will be related to each other, since they are in the same cluster and the same cluster-segment. \n    \n### <span style=\"background-color: #E1E2FF\"> We are also betting these lists of related items will point toward the human element: interesting Kaggle user-groups we can study and talk about\n\n# 3. Gender cloud    \n\nWe could use any number of strategies now to select clusters for our search and study. We'll narrow our present scope and look at only slices that contain a 'gender' response label; every label occurs only once in our data so maximum four segments out of the 25 segments will contain a gender label. \n\n#### Let's briefly see some **labels** found within the **cluster segments** where **gender labels** appear. But first please note:\n* <span style=\"background-color: #F9F5AC\"> The gender of the group we decide to study in the end is **Women**. \n* **Men** were excluded from next analyses using keyword charts. Under current specifications of the study the label 'Man' is associated only with the labels 'Python', 'Matplotlib', and 'Scikit-learn'. Future work must revisit.","metadata":{}},{"cell_type":"markdown","source":"#### First we examine the segment in which Nonbinary gendered responses loaded strongest\n\nSee below: the sorted and segmented cluster is line-plotted and annotated to indicate the location of the **Nonbinary** gender label's position in the data.","metadata":{}},{"cell_type":"markdown","source":"\n\n\n","metadata":{}},{"cell_type":"code","source":"def non():\n\n  for i in range(5):\n    is_2002 =  dfg4['Cluster Assigned']==i\n    gapminder_2002 = dfg4[is_2002]\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Nonbinary'].tolist()\n    try:\n      vw=int(vw[0]+1)\n      if vw >0:\n        #dummyhere=1111#(vw)\n        with open('assigned_cluster_indexer.txt', 'w') as f:\n          f.write('%d' % vw)\n        with open('assigned_cluster.txt', 'w') as f:\n          f.write(str(int(gapminder_2002['Cluster Assigned'][-1:])))\n      else:\n        #dummyhere=1111#('here')\n        skdjf=11\n    except:\n      dummyhere=1111#('Not here')\n        #np.savetxt('fee.txt',vw)#, vw)\n\n\n\n      #rwey=awr['Model Weights']\n      #rwey=rwey.iloc[vw]\n  #ader=pd.read_csv('/content/assigned_cluster.txt')\n  f = open('assigned_cluster.txt')\n  kawa=f.read()\n  kawa4gapper=int(kawa)+1\n  yer=str('y')+str(kawa)\n  xer=str('x')+str(kawa)\n  #yer=str('y')+str(kawa)\n  breaks_rpter=str('breaks_rpt')+str(kawa)\n  gapper=str('gaps')+str(kawa4gapper)+'.csv'\n  gapper\n  #yer=eval(yer)\n  xer=eval(xer)\n  yer=eval(yer)\n  #breaks_rpter=eval(breaks_rpter)\n\n  model.fit(yer)\n  breaks3 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt3 = []\n  for i in breaks3:\n    breaks_rpt3.append(xer[i-1])\n  blak=pd.DataFrame(breaks_rpt3)\n  awr=pd.read_csv(gapper)\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Nonbinary'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    color='grey'\n    #display(' ')#(int(len(awq)))\n  else:\n    color='grey'\n  blak.to_csv('blecks4.csv')\n  fig, ax5 = plt.subplots(figsize=(10,2.4))\n  \n  ax5.plot(xer, yer,c='grey',linestyle='dashed')\n  ax5.axis(\"off\")\n\n\n\n  vw=awr.index[awr['Survey Q & A'] == 'What is your gender? - Selected Choice_Nonbinary'].tolist()\n  vw=int(vw[0])\n  rwey=awr['Model Weights']\n  rwey=rwey.iloc[vw]\n\n  #ax5.axis('off')\n  rweye=round(rwey,6)\n  ax5.annotate('NonBinary,  '+str(rweye),\n            xy=(vw, rwey),\n            xytext=(4,.8),\n            #textcoords='offset points',\n            arrowprops=dict(arrowstyle='->', color='lightgrey'),\n            va='center',\n            ha='left',\n            fontsize=13)\n\n  #ax5.plot(x1, y1,c='k',linestyle='dashed')\n\n\n  for i in breaks_rpt3:\n      if print_legend:\n          ax5.axvline(i, color='lightgrey',ymin=.01,ymax=.5)\n      else:\n          ax5.axvline(i, color='lightgrey')\n\n\n\n\n    # the bleks spreadsheets are the breaks\n\n  blecks=pd.read_csv('blecks4.csv')\n\n  # so list out \n  breaks_rpt=list(blecks['0'])\n  breaks_rpt\n  #make a dataframe containing in two rows. \n  breaks_rpt=pd.DataFrame(breaks_rpt)\n  breaks_rpt.index=breaks_rpt[0]\n  breaks_rpt['goose']=breaks_rpt[0]\n  breaks_rpt\n  # the gaps spreadsheets are the items\n  gapminder_2002=pd.read_csv(gapper)\n\n  #transpose the dataframe and rename the columns\n  ad=breaks_rpt.T\n  ad\n  # Create SECTION's labels in 'T' column\n  ad.columns=['1ST', 'SECOND','THIRD', 'FOURTH', 'FIFTH']\n  ad=ad.T\n  ad['T']=ad.index\n  ad.index=ad['goose']\n  #display(' ')#(ad)\n\n  # add the labels to each observation of the 'gaps' sheet\n  df_merged = gapminder_2002.merge(ad, how='outer', left_index=True, right_index=True)\n\n  df_merged=df_merged.fillna(method='backfill')\n\n  df_merged.head(3)\n\n\n# isolate a cluster title as 'x'\n  i=0\n  if i is 0:\n    x='1ST'\n  else:\n    if i is 1:\n      x='SECOND'\n    else:\n      if i is 2:\n        x='THIRD'\n      else:\n        if i is 3:\n          x='FOURTH'\n        else:\n          if i is 4:\n            x='FIFTH'\n  #display(' ')#(x)\n\n  # Filter down to only that section\n  is_2002 =  df_merged['T']==x\n\n\n  dummyhere=1111#('Section ', x)\n\n  df_merged = df_merged[is_2002]\n  df_merged.head(3)\n\n\n\n  df_merged['NAM']=df_merged['Survey Q & A']\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"_0\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"_None\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\" None\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"_other\")]\n  #df_merged = df_merged[~df_merged['NAM'].str.contains(\"prefer to self-describe\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"Other\")]\n  df_merged['NAM']=df_merged['NAM'].str.replace('_0', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('_', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('Selected Choice', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace(' - ', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('(Select all that apply)', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('?', '|')\n  df_merged['NAM']=df_merged['NAM'].str.replace(':', '|')\n  df_merged['NAM']=df_merged['NAM'].str.replace('(', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace(')', '')\n\n\n\n  qor=pd.DataFrame([x.split('|')[:12] for x in df_merged['NAM']])\n  qor[-3:]\n\n\n  try:\n    qor['Desired'] = (qor[1].str.split()\n                                  .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                                  .str.join(' '))\n  except: \n    qor['Desired']=qor[1]\n\n\n\n  try:\n    df_merged=df_merged.reset_index() \n  except:\n    pass\n  df_merged['lean']=qor['Desired']\n\n  sp=df_merged\n  sp[-3:]\n\n\n\n\n  sp['kfx']=sp['Model Weights']\n  sp['cha']=sp['kfx'].pct_change() * 1\n  sp=sp.bfill()\n  sp[:1]['cha']=float(sp[:1]['cha'])*.1+sp[:1]['cha']\n  sp=sp.bfill()\n  sp[-3:]\n\n\n\n  # x and y variables the chart,  a list of the labels for the chart\n  sp['x'] = sp['cha'].rank()\n  sp['y'] = sp['kfx'].rank()\n  val=sp['lean']\n\n\n\n  #check if a string is present in what you are analyzing\n  if str(val.isin(['Woman'])) is 'True':\n    yyy=4\n  else:\n    yyy=5\n\n  # convert variables to numpy arrays\n\n  xs = sp['x'].to_numpy()#np.random.randint( 0, 10, size=10)\n  ys = sp['y'].to_numpy()#np.random.randint(-5, 5,  size=10)\n  val=sp['lean'].to_numpy() \n\n  reu=sp['lean'].isin(['Woman'])\n  reu=str(reu)\n  fullstring = reu\n  substring = \"True\"\n\n  if substring in fullstring:\n      dummyhere=1111#(\"Found!\")\n      sp['lean'].to_csv('reu.csv')\n  else:\n      dummyhere=1111#(\"Not found!\")\n\n\n\n#rename everything for presentatiom \n\n  sp['topic']=sp['lean']\n  sp['weight']=sp['kfx']\n\n\n  # spii is used uniquely in the maro function \n  spii=sp[sp.columns[sp.columns.isin(['topic','weight'])]] \n  spii=round(2/(int(len(spii)))*160)\n  rt=spii\n  #rt=6\n  if rt <6:  \n    rt=6\n    dummyhere=1111#('rt')\n  else:\n    dummyhere=1111#('already there')\n\n  if rt >12:\n    rt=12\n    dummyhere=1111#('12')\n  else:\n    dummyhere=1111#('ok')\n  spii=rt\n  spii\n  #######\n\n\n  boob = sp[sp.columns[sp.columns.isin(['x','y'])]] \n  boob\n\n\n\n  #take the boob values and fit them onto clusters (not sure if this is necessary or added later in order to color the chart but forgot to folllow up)\n  mat = boob.values\n  # Using sklearn\n  km = sklearn.cluster.KMeans(n_clusters=5)\n  km.fit(mat)\n  labels = km.labels_\n  # Format results as a DataFrame\n  results = pd.DataFrame([boob.index,labels]).T\n  #erb=results[1].to_numpy()\n  sp['cluser']=results[1]\n  sp\n  ax.axis(\"off\")\n\n\n  # s = ranekd weights ... THIS IS THE WORD SIZE\n  s=pd.DataFrame([i**2*2+2 for i in list(sp['cha'])])\n  s=s*-10\n  s=s.rank()\n  s=s*10\n\n\n  tyui=i#('CLUSTER __  ',  cvrb)\n  tyur=x#('SECTION __', x)\n\n\n  dummyhere=1111#(' Certainly... ... ... ')\n  dummyhere=1111#(' Certainly... ... ... ')\n  dummyhere=1111#(' Certainly... ... ... ')\n\n\n\n\n\n\n\n  def maro():\n    import matplotlib.pyplot as plt\n    plt.rcParams['font.family'] = 'serif'\n    plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n\n    fig, ax = plt.subplots(figsize=(10.4,6))\n    ax.axis(\"off\")\n    cola=[0, 1, 2]\n    groups = sp.groupby('cluser')\n    colors={0:'SaddleBrown', 1:'Indigo', 2:'DarkOliveGreen', 3:'DarkGoldenrod', 4: 'FireBrick'}\n    \n    plt.scatter(x=xs,y=ys, s=40,c=sp['cluser'].map(colors))#sp['cluser'].map(colors))#c='#E0DBFF')\n    ax.axis(\"off\")\n\n\n    \n    for x,y,z in zip(xs,ys,val):\n      label = z\n      \n  #for name, group in groups:\n  #    plt.plot(group[\"X Value\"], group[\"Y Value\"], marker=\"o\", linestyle=\"\", label=name)\n      # font from OS\n    \n\n      #plt.title(\"Topics\", \n    #           loc='Center', fontsize=26, **5hfont)\n    \n      plt.suptitle('Above: Cluster segment contains label Nonbinary', y=1.1,fontsize=19)\n      plt.title('Below: Keyword cloud for cluster segment containing label Nonbinary', y=1.05,fontsize=19)\n\n \n\n\n\n      tyur\n      plt.annotate(label, # this is the, text\n                  (x,y), # these are the coordinates to position the label5\n                  textcoords=\"offset points\", # how to position the text\n                  size=spii,\n                  xytext=(-1,10), # distance from text to points (x,y)\n                  ha='center') # horizontal alignment can be left, right or center\n                  \n\n  maro()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:01.147264Z","iopub.execute_input":"2021-11-30T21:15:01.147601Z","iopub.status.idle":"2021-11-30T21:15:01.189553Z","shell.execute_reply.started":"2021-11-30T21:15:01.147557Z","shell.execute_reply":"2021-11-30T21:15:01.188527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the model weights as y-axis values, we'll use the same charting techniques we did with the <span style=\"color:blue\">TOPIC MAP</span> earlier, including a brand new k-means analysis to color the chart marks using the <span style=\"color:red\">CLUSTER COLOR</span> technique. ","metadata":{}},{"cell_type":"code","source":"non()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:01.190846Z","iopub.execute_input":"2021-11-30T21:15:01.191422Z","iopub.status.idle":"2021-11-30T21:15:01.954678Z","shell.execute_reply.started":"2021-11-30T21:15:01.191382Z","shell.execute_reply":"2021-11-30T21:15:01.95373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### <span style=\"background-color: #E1E2FF\">Some rather interesting stories in this keyword cloud!\n\n* UPPER LEFT: Kagglers who are Statisticians, Project Managers and Public Service workers holding High School diplomas in Russia and the Middle East. \n\n* LOWER LEFT: Developer Relations and Advocacy work in Thailand and Ireland sounds like an interesting subject for Journalism and Social Science research.\n\n* Military/Security/Defense in... \n\n**...all very interesting.**","metadata":{}},{"cell_type":"code","source":"# -  Woman\n\ndef women():\n\n  for i in range(5):\n    is_2002 =  dfg4['Cluster Assigned']==i\n    gapminder_2002 = dfg4[is_2002]\n    vw=gapminder_2002.index[gapminder_2002['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n    try:\n      vw=int(vw[0]+1)\n      if vw >0:\n        dummyhere=1111#(vw)\n        with open('assigned_cluster_indexer.txt', 'w') as f:\n          f.write('%d' % vw)\n        with open('assigned_cluster.txt', 'w') as f:\n          f.write(str(int(gapminder_2002['Cluster Assigned'][-1:])))\n      else:\n        dummyhere=1111#('here')\n    except:\n      dummyhere=1111#('Not here')\n        #np.savetxt('fee.txt',vw)#, vw)\n\n\n\n      #rwey=awr['Model Weights']\n      #rwey=rwey.iloc[vw]\n  #ader=pd.read_csv('/content/assigned_cluster.txt')\n  f = open('assigned_cluster.txt')\n  kawa=f.read()\n  kawa4gapper=int(kawa)+1\n  yer=str('y')+str(kawa)\n  xer=str('x')+str(kawa)\n  #yer=str('y')+str(kawa)\n  breaks_rpter=str('breaks_rpt')+str(kawa)\n  gapper=str('gaps')+str(kawa4gapper)+'.csv'\n  gapper\n  #yer=eval(yer)\n  xer=eval(xer)\n  yer=eval(yer)\n  #breaks_rpter=eval(breaks_rpter)\n\n  model.fit(yer)\n  breaks3 = model.predict(n_bkps=n_breaks-1)\n  breaks_rpt3 = []\n  for i in breaks3:\n    breaks_rpt3.append(xer[i-1])\n  blak=pd.DataFrame(breaks_rpt3)\n  awr=pd.read_csv(gapper)\n  awq=awr['Survey Q & A'].isin(['What is your gender? - Selected Choice_Woman', 'What is your gender? - Selected Choice_Woman'])\n  awq=awq.index[awq == True].tolist()\n  if int(len(awq)) >0:\n    color='grey'\n    #display(' ')#(int(len(awq)))\n  else:\n    color='grey'\n  blak.to_csv('blecks4.csv')\n  fig, ax5 = plt.subplots(figsize=(10,2.4))\n  \n  ax5.plot(xer, yer,c='grey',linestyle='dashed')\n  ax5.axis(\"off\")\n\n\n\n  vw=awr.index[awr['Survey Q & A'] == 'What is your gender? - Selected Choice_Woman'].tolist()\n  vw=int(vw[0])\n  rwey=awr['Model Weights']\n  rwey=rwey.iloc[vw]\n\n  #ax5.axis('off')\n  rweye=round(rwey,2)\n  ax5.annotate('♀,  '+str(rweye),\n            xy=(vw, rwey),\n            xytext=(4,.8),\n            #textcoords='offset points',\n            arrowprops=dict(arrowstyle='->', color='lightgrey'),\n            va='center',\n            ha='left',\n            fontsize=13)\n\n  #ax5.plot(x1, y1,c='k',linestyle='dashed')\n\n\n  for i in breaks_rpt3:\n      if print_legend:\n          ax5.axvline(i, color='lightgrey',ymin=.01,ymax=.5)\n      else:\n          ax5.axvline(i, color='lightgrey')\n\n\n\n\n    # the bleks spreadsheets are the breaks\n\n  blecks=pd.read_csv('blecks4.csv')\n\n  # so list out \n  breaks_rpt=list(blecks['0'])\n  breaks_rpt\n  #make a dataframe containing in two rows. \n  breaks_rpt=pd.DataFrame(breaks_rpt)\n  breaks_rpt.index=breaks_rpt[0]\n  breaks_rpt['goose']=breaks_rpt[0]\n  breaks_rpt\n  # the gaps spreadsheets are the items\n  gapminder_2002=pd.read_csv(gapper)\n\n  #transpose the dataframe and rename the columns\n  ad=breaks_rpt.T\n  ad\n  # Create SECTION's labels in 'T' column\n  ad.columns=['1ST', 'SECOND','THIRD', 'FOURTH', 'FIFTH']\n  ad=ad.T\n  ad['T']=ad.index\n  ad.index=ad['goose']\n  #display(' ')#(ad)\n\n  # add the labels to each observation of the 'gaps' sheet\n  df_merged = gapminder_2002.merge(ad, how='outer', left_index=True, right_index=True)\n\n  df_merged=df_merged.fillna(method='backfill')\n\n  df_merged.head(3)\n\n\n# isolate a cluster title as 'x'\n  i=0\n  if i is 0:\n    x='1ST'\n  else:\n    if i is 1:\n      x='SECOND'\n    else:\n      if i is 2:\n        x='THIRD'\n      else:\n        if i is 3:\n          x='FOURTH'\n        else:\n          if i is 4:\n            x='FIFTH'\n  #display(' ')#(x)\n\n  # Filter down to only that section\n  is_2002 =  df_merged['T']==x\n\n\n  dummyhere=1111#('Section ', x)\n\n  df_merged = df_merged[is_2002]\n  df_merged.head(3)\n\n\n\n  df_merged['NAM']=df_merged['Survey Q & A']\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"_0\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"_None\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\" None\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"_other\")]\n  #df_merged = df_merged[~df_merged['NAM'].str.contains(\"prefer to self-describe\")]\n  df_merged = df_merged[~df_merged['NAM'].str.contains(\"Other\")]\n  df_merged['NAM']=df_merged['NAM'].str.replace('_0', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('_', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('Selected Choice', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace(' - ', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('(Select all that apply)', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace('?', '|')\n  df_merged['NAM']=df_merged['NAM'].str.replace(':', '|')\n  df_merged['NAM']=df_merged['NAM'].str.replace('(', '')\n  df_merged['NAM']=df_merged['NAM'].str.replace(')', '')\n\n\n\n  qor=pd.DataFrame([x.split('|')[:12] for x in df_merged['NAM']])\n  qor[-3:]\n\n\n  try:\n    qor['Desired'] = (qor[1].str.split()\n                                  .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                                  .str.join(' '))\n  except: \n    qor['Desired']=qor[1]\n\n\n\n  try:\n    df_merged=df_merged.reset_index() \n  except:\n    pass\n  df_merged['lean']=qor['Desired']\n\n  sp=df_merged\n  sp[-3:]\n\n\n\n\n  sp['kfx']=sp['Model Weights']\n  sp['cha']=sp['kfx'].pct_change() * 1\n  sp=sp.bfill()\n  sp[:1]['cha']=float(sp[:1]['cha'])*.1+sp[:1]['cha']\n  sp=sp.bfill()\n  sp[-3:]\n\n\n\n  # x and y variables the chart,  a list of the labels for the chart\n  sp['x'] = sp['cha'].rank()\n  sp['y'] = sp['kfx'].rank()\n  val=sp['lean']\n\n\n\n  #check if a string is present in what you are analyzing\n  if str(val.isin(['Woman'])) is 'True':\n    yyy=4\n  else:\n    yyy=5\n\n  # convert variables to numpy arrays\n\n  xs = sp['x'].to_numpy()#np.random.randint( 0, 10, size=10)\n  ys = sp['y'].to_numpy()#np.random.randint(-5, 5,  size=10)\n  val=sp['lean'].to_numpy() \n\n  reu=sp['lean'].isin(['Woman'])\n  reu=str(reu)\n  fullstring = reu\n  substring = \"True\"\n\n  if substring in fullstring:\n      #dummyhere=1111#(\"Found!\")\n      sp['lean'].to_csv('reu.csv')\n  else:\n      dummyhere=1111#(\"Not found!\")\n\n\n\n#rename everything for presentatiom \n\n  sp['topic']=sp['lean']\n  sp['weight']=sp['kfx']\n\n\n  # spii is used uniquely in the maro function \n  spii=sp[sp.columns[sp.columns.isin(['topic','weight'])]] \n  spii=round(2/(int(len(spii)))*160)\n  rt=spii\n  #rt=6\n  if rt <6:  \n    rt=6\n    #dummyhere=1111#('rt')\n  else:\n    dummyhere=1111#('already there')\n\n  if rt >12:\n    rt=12\n    #dummyhere=1111#('12')\n  else:\n    dummyhere=1111#('ok')\n  spii=rt\n  spii\n  #######\n\n\n  boob = sp[sp.columns[sp.columns.isin(['x','y'])]] \n  boob\n\n\n\n  #take the boob values and fit them onto clusters (not sure if this is necessary or added later in order to color the chart but forgot to folllow up)\n  mat = boob.values\n  # Using sklearn\n  km = sklearn.cluster.KMeans(n_clusters=5)\n  km.fit(mat)\n  labels = km.labels_\n  # Format results as a DataFrame\n  results = pd.DataFrame([boob.index,labels]).T\n  #erb=results[1].to_numpy()\n  sp['cluser']=results[1]\n  sp\n  ax.axis(\"off\")\n\n\n  # s = ranekd weights ... THIS IS THE WORD SIZE\n  s=pd.DataFrame([i**2*2+2 for i in list(sp['cha'])])\n  s=s*-10\n  s=s.rank()\n  s=s*10\n\n\n  #tyui=('CLUSTER __  ',  cvrb)\n  tyur=('SECTION __', x)\n\n\n\n\n\n\n\n  def maro():\n    import matplotlib.pyplot as plt\n    plt.rcParams['font.family'] = 'serif'\n    plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n\n    fig, ax = plt.subplots(figsize=(10.4,6))\n    ax.axis(\"off\")\n    cola=[0, 1, 2]\n    groups = sp.groupby('cluser')\n    colors={0:'SaddleBrown', 1:'Indigo', 2:'DarkOliveGreen', 3:'DarkGoldenrod', 4: 'FireBrick'}\n    \n    plt.scatter(x=xs,y=ys, s=40,c=sp['cluser'].map(colors))#sp['cluser'].map(colors))#c='#E0DBFF')\n    ax.axis(\"off\")\n\n\n    \n    for x,y,z in zip(xs,ys,val):\n      label = z\n      \n  #for name, group in groups:\n  #    plt.plot(group[\"X Value\"], group[\"Y Value\"], marker=\"o\", linestyle=\"\", label=name)\n      # font from OS\n    \n\n      #plt.title(\"Topics\", \n    #           loc='Center', fontsize=26, **5hfont)\n    \n    \n      plt.suptitle('Above: Cluster segment contains label Woman', y=1.1,fontsize=19)\n      plt.title('Below: Keyword cloud for cluster segment containing label Woman', y=1.05,fontsize=19)\n\n \n\n\n\n      tyur\n      plt.annotate(label, # this is the, text\n                  (x,y), # these are the coordinates to position the label5\n                  textcoords=\"offset points\", # how to position the text\n                  size=spii,\n                  xytext=(-1,10), # distance from text to points (x,y)\n                  ha='center') # horizontal alignment can be left, right or center\n                  \n\n  maro()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:01.956292Z","iopub.execute_input":"2021-11-30T21:15:01.957111Z","iopub.status.idle":"2021-11-30T21:15:01.999852Z","shell.execute_reply.started":"2021-11-30T21:15:01.957068Z","shell.execute_reply":"2021-11-30T21:15:01.999005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Now in the segment containing the Women label, same as above, we sort and segment the cluster weights and then line-plot them, indicating on the chart the location of the Woman gender label in the data. \n\nAgain too with the Model Weights as y-axis values, we'll use the charting techniques we used on the <span style=\"color:blue\">TOPIC MAP</span>, including a brand new k-means analysis to color the chart marks using the <span style=\"color:red\">CLUSTER COLOR</span> technique","metadata":{}},{"cell_type":"code","source":"women()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:02.000996Z","iopub.execute_input":"2021-11-30T21:15:02.001434Z","iopub.status.idle":"2021-11-30T21:15:02.589262Z","shell.execute_reply.started":"2021-11-30T21:15:02.001399Z","shell.execute_reply":"2021-11-30T21:15:02.588253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The word cloud above shows young women, lower education, more use of four specific programming languages and living within a **specific area**. \n\n#### The map below shows just how specific a geography we are in now.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport geopandas as gpd\n# from descartes import PolygonPatch\n\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n# or plot Africa continent\nax2 = world.plot(figsize=(8,8), edgecolor='lightgrey', color='white')\n\nworld[world.name == \"Iran\"].plot(edgecolor=u'gray', color='k', ax=ax2)\nworld[world.name == \"Algeria\"].plot(edgecolor=u'gray', color='k', ax=ax2)\nworld[world.name == \"Tunisia\"].plot(edgecolor=u'gray', color='k', ax=ax2)\nworld[world.name == \"Morocco\"].plot(edgecolor=u'gray', color='k', ax=ax2)\nworld[world.name == \"China\"].plot(edgecolor=u'grey', color='k', ax=ax2)\n\n# the place to plot additional vector data (points, lines)\n\n#plt.ylabel('Latitude')\n#plt.xlabel('Longitude')\n\nax2.axis('off')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:02.591006Z","iopub.execute_input":"2021-11-30T21:15:02.591618Z","iopub.status.idle":"2021-11-30T21:15:03.722687Z","shell.execute_reply.started":"2021-11-30T21:15:02.591567Z","shell.execute_reply":"2021-11-30T21:15:03.721774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## <span style=\"background-color: #F9F5AC\">Now this feels more close-up to people. It is the most personal result we've seen so far and it seems appropriate to take this as a starting point for our user group.  ","metadata":{}},{"cell_type":"markdown","source":"## Fine. Young females, current and former university students, specific geography, three or four programming languages. <span style=\"background-color: #F9F5AC\">Is that a story? ","metadata":{}},{"cell_type":"markdown","source":"### No but maybe we are closer...\n\nAlas, looking for more details in the Kaggle data, filtering the data on gender, education, and country of residences to match our cluster - we almost run out of data to analyze.  ","metadata":{}},{"cell_type":"code","source":"original_emport=pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\")\ndf=original_emport.fillna(0)\nnew_header = df.iloc[0] \ndf = df[1:] \ndf.columns = new_header\ndfco=df['Duration (in seconds)']\ndf=df.astype('str')\ndf.astype('str')\ndat1=df\ndfx=pd.DataFrame(dat1.columns.to_list())\ndfx=dfx.T\ndf.shape\ndf['What is your age (# years)?'] = df['What is your age (# years)?'].map(lambda x: x.lstrip('+').rstrip('+'))\ndf['What is your age (# years)?']=df['What is your age (# years)?'].str.replace('70', '70-80')\ndf[['A', 'B']] = df['What is your age (# years)?'].str.split('-', 1, expand=True)\ndf['A']=df['A'].astype('int')\ndf['B']=df['B'].astype('int')\ndf['age']=((df['A']+df['B'])/2)\n\ndf\n\nman = df['What is your gender? - Selected Choice']!='Man'\ndf=df[man]\ndf\n\n\n\nyears = ['Morocco', 'China', 'Algeria', 'Tunisia', 'Iran, Islamic Republic of...']\nyears=df['In which country do you currently reside?'].isin(years)\ndf=df[years]\nuslet=df\n#df\n\n#df['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'].unique().tolist()\n\n\n\n\nis_2002 =  df['What is the highest level of formal education that you have attained or plan to attain within the next 2 years?'] == 'Some college/university study without earning a bachelor’s degree'\n\ndf=df[is_2002]\n#is_2002=df['What is your age (# years)?']=='18-21'\n#df=df[is_2002]\n#df\n\n\n#df.head()\n\ndf['yeei']=df.index\nglio=df['yeei']\n#glio","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:03.724057Z","iopub.execute_input":"2021-11-30T21:15:03.724291Z","iopub.status.idle":"2021-11-30T21:15:08.427108Z","shell.execute_reply.started":"2021-11-30T21:15:03.724264Z","shell.execute_reply":"2021-11-30T21:15:08.42596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Using the same charting method from earlier of all 1's and 0's we make the chart below. The unshaded portion of the chart is our sample at this stage, and we are not even done with the filtering down on labels. And again that chart y axis is only 500 observations out of the over 25000 we could plot.","metadata":{}},{"cell_type":"code","source":"\n\n# list of iloc\n\nglio=list(glio)\n\n\n#dfw=pd.read_csv('/content/drive/MyDrive/kaggle_survey_2021_responses.csv')\ndfw=original_emport.fillna(0)\nnew_header = dfw.iloc[0] \ndfw = dfw[1:] \ndfw.columns = new_header\ndfcow=dfw['Duration (in seconds)']\ndfw=dfw.drop(['Duration (in seconds)'], axis=1)\ndfw=dfw.astype('str')\ndfw=pd.get_dummies(dfw)\ndfw.astype('float')\n\n\n\n\n\nfor i in glio:\n  dfw.iloc[i]=dfw.iloc[i]+1\n  dfw.iloc[i]=dfw.iloc[i].replace(1,0)\n  dfw = pd.concat([dfw.iloc[[i],:], dfw.drop(i, axis=0)], axis=0)\n\n  #print(dfw.iloc[i])\n\n\n#dfw.head()\n\n\n\n\n\n\n\n\n\nzsl=dfw[:500]\nzs=zsl.values#dfw.values\n\n# IMAGE\n\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,20)\n#plt.subplot(211)\nplt.imshow(zs)\n#plt.subplot(212)\nplt.imshow(zs, cmap='bone',  interpolation='nearest')\nplt.axis('off')\nplt.rcParams[\"figure.figsize\"] = (20,20)\n#plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:08.428843Z","iopub.execute_input":"2021-11-30T21:15:08.429217Z","iopub.status.idle":"2021-11-30T21:15:14.877446Z","shell.execute_reply.started":"2021-11-30T21:15:08.42917Z","shell.execute_reply":"2021-11-30T21:15:14.876486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## That tiny sample just won't do. Are we loosing momentum?\n\n### Maybe not. Let's follow the thread out of here for a moment and into external datasets for clues and inspiration.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# 4. The four languages","metadata":{}},{"cell_type":"markdown","source":"## Why C, C++, Java, and Javascript? \n\n### <span style=\"background-color: #F9F5AC\"> It may be notable that C, C++, Java, and Javascript are the only languages loading in same segment as 'Women'.  \n\nSee charts below, where we find geographic associations between **rates of use for (only) these four programming languages** (Kaggle sample) and two **global indicators** from the [World Bank](https://www.kaggle.com/mutindafestus/world-statistics-dataset-from-world-bank): **GDP** and\n **Percent Female Population working in agriculture by country**. <span style=\"background-color: #F9F5AC\"> The **use rates** of nearly all 6 other **coding languages** show either inverted or flat associations. ","metadata":{}},{"cell_type":"markdown","source":"## Coding Language use rates  X  % of female employment in agriculture","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n\n\n\n\ndef goah(thver):\n    import pandas as pd\n    df=pd.read_csv(\"../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv\").fillna(0)\n    new_header = df.iloc[0] \n    df = df[1:] \n    df.columns = new_header\n    dfco=df['Duration (in seconds)']\n    df=df.astype('str')\n    df.astype('str')\n    dat1=df\n    dfx=pd.DataFrame(dat1.columns.to_list())\n    dfx=dfx.T\n    df['What is your age (# years)?'] = df['What is your age (# years)?'].map(lambda x: x.lstrip('+').rstrip('+'))\n    df['What is your age (# years)?']=df['What is your age (# years)?'].str.replace('70', '70-80')\n    df[['A', 'B']] = df['What is your age (# years)?'].str.split('-', 1, expand=True)\n    df['A']=df['A'].astype('int')\n    df['B']=df['B'].astype('int')\n    df['age']=((df['A']+df['B'])/2)\n    df['country']=df['In which country do you currently reside?']\n    boop=list(df['country'])\n    import country_converter as coco\n    some_names =boop\n    standard_names = coco.convert(names=some_names, to='name_short')\n    #print(standard_names)\n    df['trucou']=standard_names\n    #df.index=df['trucou']\n    connectthisone=df\n    #df.head(60)\n    df=1\n    worlddataimport=pd.read_csv('../input/world-statistics-dataset-from-world-bank/data.csv')\n    df=worlddataimport\n    df['year']=df['year'].astype('str')\n    years = ['2019']\n    years=df['year'].isin(years)\n    #years\n    df=df[years]\n    df=df.dropna(axis=1, how='all')\n    #df\n    #Country Name\n    boop=list(df['Country Name'])\n    some_names =boop\n    standard_names = coco.convert(names=some_names, to='name_short')\n    #print(standard_names)\n    df['crucoun']=standard_names\n    #df.index=df['trucou']\n    #connectthisone=df\n    #df.head(10)\n    #df\n    connectthistwo=df\n    df1=connectthisone\n    df2=connectthistwo\n    langs=['Python', 'R','SQL' ,'C','C++','Java','Javascript' ,'Julia','Swift','Bash','MATLAB']\n    #langs\n    df3 = pd.merge(left=df1, right=df2, left_on='trucou', right_on='crucoun', how='outer')\n    df3['trucou'] = df3.apply(lambda row: row['trucou'] if not pd.isnull(row['trucou']) else row['crucoun'], axis=1)\n    df3=df3[:int(len(df1))]\n    appear=langs\n    # merge, using 'outer' to avoid losing records from either left or right\n    df3 = pd.merge(left=df1, right=df2, left_on='trucou', right_on='crucoun', how='outer')\n    # combining the columns used to match\n    df3['trucou'] = df3.apply(lambda row: row['trucou'] if not pd.isnull(row['trucou']) else row['crucoun'], axis=1)\n    # dropping the now spare column\n    #df3 = df3.drop('crucoun', axis=1)\n    #df3=df3[10:300]\n    df3=df3[:int(len(df1))]\n    #df3['op']=df3['What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C']\n\n    for i in range(int(len(langs))):\n\n      apper=langs[i]\n\n      df3[apper]=df3['What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - '+ apper]\n      df3[apper]=df3[apper].replace([apper], [1])\n      df3[apper]=df3[apper].astype('int')\n\n    df3['nmer']= df3['Employment in agriculture (% of total employment) (modeled ILO estimate)']\n    #df3\n    dst=df3.groupby(df3['trucou']).mean()\n    import matplotlib\n    from matplotlib.pyplot import figure as fig\n    from pylab import rcParams\n    #rcParams['figure.figsize'] = 9, 9\n    #plt.style.use('dark_background')\n    #dst\n    #https://towardsdatascience.com/bring-your-jupyter-notebook-to-life-with-interactive-widgets-bc12e03f0916\n    # List of options\n    state_options = ['','Agricultural raw materials exports (% of merchandise exports)', 'Agricultural raw materials imports (% of merchandise imports)', 'Agriculture, forestry, and fishing, value added (% of GDP)', 'Agriculture, forestry, and fishing, value added (current US$)', 'Employment in agriculture (% of total employment) (modeled ILO estimate)', 'Employment in agriculture, female (% of female employment) (modeled ILO estimate)', 'Employment in agriculture, male (% of male employment) (modeled ILO estimate)', 'GDP per capita (current US$)', 'Literacy rate, adult total (% of people ages 15 and above)', 'Mortality rate, infant (per 1,000 live births)']\n    #thver= 'Employment in agriculture, female (% of female employment) (modeled ILO estimate)'\n    #thver\n    varstr=thver\n    y=dst[varstr]\n    x1=dst['Python']\n    x2=dst['R']\n    x3=dst['SQL']\n    x4=dst['C']\n    x5=dst['C++']\n    x6=dst['Java']\n    x7=dst['Javascript']\n    x8=dst['Julia']\n    x9=dst['Swift']\n    x10=dst['Bash']\n    #x11=dst['MATLAB']\n\n    plt.tight_layout\n    clear_output(wait=True)\n\n\n    def ben():\n      \n      plt.rcParams['font.family'] = 'serif'\n      plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n      f, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5,figsize=(15,3.0), constrained_layout=True, sharey=True)\n      f.tight_layout(pad=3.0)\n      f.suptitle('x-Axis = % language users | y-Axis = Variable: '+thver , fontsize=15,y=1.3)\n    \n      print('1')\n      ax1.scatter(x1, y, c='w', s=6, edgecolor='grey')\n      ax1.set_title('Python', size=19)\n      ax2.scatter(x2, y, c='w', s=6, edgecolor='grey')\n      ax2.set_title('R', size=19)\n      ax3.scatter(x3, y, c='w', s=6,  edgecolor='grey')\n      ax3.set_title('SQL', size=19)\n      ax4.scatter(x4, y, c='w',  s=6, edgecolor='grey')\n      ax4.set_title('C', size=19)\n      ax5.scatter(x5, y, c='w',  s=6, edgecolor='grey')\n      ax5.set_title('C++', size=19)\n      print('2')\n\n      N = len(x1)\n      x1_mean = x1.mean()\n      y_mean = y.mean()\n      B1_num = ((x1 - x1_mean) * (y - y_mean)).sum()\n      B1_den = ((x1 - x1_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x1_mean)\n      ax1.plot(x1, B0 + B1*x1,  c='k', linewidth=.5, alpha=.9, solid_capstyle='round')\n\n\n      N = len(x2)\n      x2_mean = x2.mean()\n      y_mean = y.mean()\n      B1_num = ((x2 - x2_mean) * (y - y_mean)).sum()\n      B1_den = ((x2 - x2_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x2_mean)\n      ax2.plot(x2, B0 + B1*x2,  c='k', linewidth=.5, alpha=.9, solid_capstyle='round')\n\n\n      N = len(x3)\n      x3_mean = x3.mean()\n      y_mean = y.mean()\n      B1_num = ((x3 - x3_mean) * (y - y_mean)).sum()\n      B1_den = ((x3 - x3_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x3_mean)\n      ax3.plot(x3, B0 + B1*x3,  c='k', linewidth=.5, alpha=.9, solid_capstyle='round')\n\n      N = len(x4)\n      x4_mean = x4.mean()\n      y_mean = y.mean()\n      B1_num = ((x4 - x4_mean) * (y - y_mean)).sum()\n      B1_den = ((x4 - x4_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x4_mean)\n      ax4.plot(x4, B0 + B1*x4, linewidth=.5, alpha=.9, solid_capstyle='round')\n\n      N = len(x5)\n      x5_mean = x5.mean()\n      y_mean = y.mean()\n      B1_num = ((x5 - x5_mean) * (y - y_mean)).sum()\n      B1_den = ((x5 - x5_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x5_mean)\n      ax5.plot(x5, B0 + B1*x5, linewidth=.5, alpha=.9, solid_capstyle='round')\n\n      print('3')\n      ax1.spines['right'].set_visible(False)\n      ax2.spines['right'].set_visible(False)\n      ax3.spines['right'].set_visible(False)\n      ax4.spines['right'].set_visible(False)\n      ax5.spines['right'].set_visible(False)\n      ax1.spines['left'].set_visible(False)\n      ax2.spines['left'].set_visible(False)\n      ax3.spines['left'].set_visible(False)\n      ax4.spines['left'].set_visible(False)\n      ax5.spines['left'].set_visible(False)\n      ax1.spines['bottom'].set_visible(False)\n      ax2.spines['bottom'].set_visible(False)\n      ax3.spines['bottom'].set_visible(False)\n      ax4.spines['bottom'].set_visible(False)\n      ax5.spines['bottom'].set_visible(False)\n\n\n      print('4')\n      clear_output(wait=True)\n    def duniy():\n\n      plt.rcParams['font.family'] = 'serif'\n      plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n\n      f, (ax6,ax7,ax8,ax9,ax10) = plt.subplots(1,5,figsize=(15,3), constrained_layout=True, sharey=True)\n      f.tight_layout(pad=3.0)\n\n      ax6.scatter(x6, y, c='w',  s=11, edgecolor='grey')\n      ax6.set_title('Java', size=19)\n      #ax6.axes.yaxis.set_visible(False)\n      ax7.scatter(x7, y, c='w',  s=11, edgecolor='grey')\n      ax7.set_title('Javascript', size=19)\n      ax8.scatter(x8, y, c='w',  s=11, edgecolor='grey')\n      ax8.set_title('Julia', size=19)\n      ax9.scatter(x9, y, c='w',  s=11, edgecolor='grey')\n      ax9.set_title('Swift', size=19)\n      ax10.scatter(x10, y, c='w',  s=11, edgecolor='grey')\n      ax10.set_title('Bash', size=19)\n      #ax11.scatter(x11, y, c='w',  s=6, edgecolor='w')\n      #ax11.set_title('% MATLAB', size=12)\n\n\n      N = len(x6)\n      x6_mean = x6.mean()\n      y_mean = y.mean()\n      B1_num = ((x6 - x6_mean) * (y - y_mean)).sum()\n      B1_den = ((x6 - x6_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x6_mean)\n      ax6.plot(x6, B0 + B1*x6, linewidth=.5, alpha=.9, solid_capstyle='round')\n\n      N = len(x7)\n      x7_mean = x7.mean()\n      y_mean = y.mean()\n      B1_num = ((x7 - x7_mean) * (y - y_mean)).sum()\n      B1_den = ((x7 - x7_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x7_mean)\n      ax7.plot(x7, B0 + B1*x7, linewidth=.5, alpha=.9, solid_capstyle='round')\n\n\n      N = len(x8)\n      x8_mean = x8.mean()\n      y_mean = y.mean()\n      B1_num = ((x8 - x8_mean) * (y - y_mean)).sum()\n      B1_den = ((x8 - x8_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x8_mean)\n      ax8.plot(x8, B0 + B1*x8,  c='k', linewidth=.5, alpha=.9, solid_capstyle='round')\n\n\n      N = len(x9)\n      x9_mean = x9.mean()\n      y_mean = y.mean()\n      B1_num = ((x9 - x9_mean) * (y - y_mean)).sum()\n      B1_den = ((x9 - x9_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x9_mean)\n      ax9.plot(x9, B0 + B1*x9, c='k', linewidth=.5, alpha=.9, solid_capstyle='round')\n\n      N = len(x10)\n      x10_mean = x10.mean()\n      y_mean = y.mean()\n      B1_num = ((x10 - x10_mean) * (y - y_mean)).sum()\n      B1_den = ((x10 - x10_mean)**2).sum()\n      B1 = B1_num / B1_den\n      B0 = y_mean - (B1*x10_mean)\n      ax10.plot(x10, B0 + B1*x10, c='k',linewidth=.5, alpha=.9, solid_capstyle='round')\n\n      ax6.spines['right'].set_visible(False)\n      ax7.spines['right'].set_visible(False)\n      ax8.spines['right'].set_visible(False)\n      ax9.spines['right'].set_visible(False)\n      ax10.spines['right'].set_visible(False)\n      ax6.spines['left'].set_visible(False)\n      ax7.spines['left'].set_visible(False)\n      ax8.spines['left'].set_visible(False)\n      ax9.spines['left'].set_visible(False)\n      ax10.spines['left'].set_visible(False)\n      ax6.spines['bottom'].set_visible(False)\n      ax7.spines['bottom'].set_visible(False)\n        \n      ax8.spines['bottom'].set_visible(False)\n      ax9.spines['bottom'].set_visible(False)\n      ax10.spines['bottom'].set_visible(False)\n    \n      \n    clear_output(wait=True)\n    display(ben())\n    clear_output(wait=True)\n    \n    duniy()\ngoah('Employment in agriculture, female (% of female employment) (modeled ILO estimate)')","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-11-30T21:15:14.879297Z","iopub.execute_input":"2021-11-30T21:15:14.880154Z","iopub.status.idle":"2021-11-30T21:15:51.973862Z","shell.execute_reply.started":"2021-11-30T21:15:14.880103Z","shell.execute_reply":"2021-11-30T21:15:51.972931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Coding Language use rates  X  GDP by country ","metadata":{}},{"cell_type":"code","source":"goah('GDP per capita (current US$)')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:15:51.975393Z","iopub.execute_input":"2021-11-30T21:15:51.975948Z","iopub.status.idle":"2021-11-30T21:16:27.175709Z","shell.execute_reply.started":"2021-11-30T21:15:51.9759Z","shell.execute_reply":"2021-11-30T21:16:27.174738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"background-color: #F9F5AC\"> Among Kaggle users, higher rates of use of  these four languages is associated with residence in lower GDP-countries, and in countries with higher rates of female employment in agriculture. \n\n### <span style=\"background-color: #F9F5AC\"> Among all languages in the dataset only these four languages seem to track these discrepancies.  ","metadata":{}},{"cell_type":"markdown","source":"The evidence is directional, certainly. Still, how's this for a developing story: \n\n#### <span style=\"background-color: #E1E2FF\"> We might have here a blurry image of a small number of Woman scattered throughout a vast but known geography, similar though they have likely never met. They use the four programming languages discussed above the most. Lower GDP means higher likelihood of poverty. Personal connections to agricultural workers more likely for them. \n\nAre these the Kaggle users of today? Yes this is a tiny sliver of them. \n\n#### <span style=\"background-color: #E1E2FF\"> And the Kaggle users of Tomorrow? Perhaps this anaylsis is directing us toward where they might come from, and the programming language preferences they might arrive with. Check back.","metadata":{}},{"cell_type":"markdown","source":"# 5. A conclusion inside cities","metadata":{}},{"cell_type":"markdown","source":"Let's now compare [Google search](https://trends.google.com/trends/?geo=US) rates for **coding languages** in 2020-2021 with [average rents and disposable incomes](https://www.kaggle.com/blitzr/movehub-city-rankings) in 37 cities worldwide. \n\nI have consolidated the data and added latitude and longitude columns, [access this dataset here](https://www.kaggle.com/daanderson/programming-language-search-trends-by-city-2021). Plotting:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\nimport folium\nfrom folium import Marker, GeoJson\nfrom folium.plugins import MarkerCluster, HeatMap\n\nwc = gpd.read_file(\"../input/programming-language-search-trends-by-city-2021/xworddata.csv\")\nimport pandas as pd\nfrom shapely.geometry import Point\nimport geopandas\nfrom geopandas import GeoDataFrame\nimport geoplot\n\ndf = wc\ndf['Long']=df['Long'].astype('float')\ndf['Lat']=df['Lat'].astype('float')\n\n\ngeometry = [Point(xy) for xy in zip(df['Long'], df['Lat'])]\ngdf = GeoDataFrame(df, geometry=geometry)   \n\n#this is a simple map that goes with geopandas\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\ngdf.plot(ax=world.plot(color='white', edgecolor='lightgrey',figsize=(10, 6)), marker='o', color='k', markersize=15);\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:16:27.177919Z","iopub.execute_input":"2021-11-30T21:16:27.178283Z","iopub.status.idle":"2021-11-30T21:16:29.365915Z","shell.execute_reply.started":"2021-11-30T21:16:27.178237Z","shell.execute_reply":"2021-11-30T21:16:29.364945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Now** We run another of our \"simple\" k-means analyses on the coding language use-rates in the cities in the dataset. Then we plot average rents in these cities against average spending money by city, and color the dots corresponding to the cluster the city was assigned in the k-means analysis.              ","metadata":{}},{"cell_type":"code","source":"\n\ndf = pd.read_csv(\"../input/programming-language-search-trends-by-city-2021/xworddata.csv\", low_memory=False)\nbelor=df['City']\n#df=df.drop(['City', ], axis=1)\ndf=df.drop(['Pop','City', 'Long', 'Lat', 'Movehub Rating', 'Purchase Power', 'Python: (11/18/20 - 11/18/21).1', 'Price of a Coffee', 'Price of a Movie', 'Wine Price', 'Gas Price', 'Health Care','Pollution','Quality of Life','Crime Rating','Avg Rent','Avg Disposable Income'], axis=1)\ndf=df.astype('int')\ndeeper=df\ndf=df.T\ndf=df.rank()\ndf=df.T\ndf=df.rank()\n\n#df['City']=belor\n#df.T\ndf\nblist=list(df.columns)\nfrom sklearn.preprocessing import StandardScaler\nfeatures = blist\n# Separating out the features\nx = df.loc[:, features].values\n# Separating out the target\ndf['City']=belor\ny = df.loc[:,['City']].values\n#y = belor.values\n# Standardizing the features\nx = StandardScaler().fit_transform(x)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\n\n\n\nkmeans = KMeans(\n    init=\"random\",\n    n_clusters=5,\n    #n_init=10,\n    n_init=30000,\n    #max_iter=30,\n    max_iter=5000000,\n    random_state=42\n    )\n\nzx=kmeans.fit(x)\ndf = pd.read_csv(\"../input/programming-language-search-trends-by-city-2021/xworddata.csv\", low_memory=False)\n\nbx=zx.fit_predict(x)\n\nbreex=pd.DataFrame(bx,columns=['a'])\n\nbreex['Avg Disposable Income']=df['Avg Disposable Income'].rank()+1\nbreex['Avg Rent']=df['Avg Rent'].rank()+1\nbreex['Quality of Life']=df['Quality of Life'].rank()+1\nbreex.index=df['City']\n\nbreex=breex.sort_values(by=['a'], ascending=True)\n\ncolors={0:'lightgreen', 1:'lightblue', 2:'yellow', 3:'black', 4: 'pink'}\n\n    \nbreex.style.bar(subset=['Avg Disposable Income', 'Quality of Life'])#, color=breex['a'].map(colors))\nx=breex['Avg Disposable Income'].to_numpy()\ny=breex['Avg Rent'].to_numpy()\npoliyu=list(breex.index)\nplt.rcParams[\"figure.figsize\"] = (12,8)\nplt.axis('off')\nplt.scatter(x=x,y=y, s=100,color=breex['a'].map(colors))\n#plt.text(11,12,'sfd', )\nax.legend(colors)\nfor i in range(len(x)):\n    plt.annotate(poliyu[i], (x[i], y[i] + 0.2))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-30T21:16:29.367256Z","iopub.execute_input":"2021-11-30T21:16:29.367476Z","iopub.status.idle":"2021-11-30T21:17:50.084261Z","shell.execute_reply.started":"2021-11-30T21:16:29.36745Z","shell.execute_reply":"2021-11-30T21:17:50.083396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Notice above**: The k-means analysis and the scatter are of course completely independent analyses, yet code-language clusters indicated by colors seem aligned with the wealth inequality gradation across these cities (x axis=incomes, y axis=rents). <span style=\"background-color: #F9F5AC\"> An association between wealth broadly speaking and Google searches for specific coding languages is observed here across cities, globally. ","metadata":{}},{"cell_type":"markdown","source":"Our (still blurry) picture is: some small number of Women across vastly bounded geography, similar but they have not met. They tend to have more connections to agricultural workers and they use one or more of the four programming languages discussed here. For those living in cities, their city of residence's relative wealth on a global scale may predict which coding languages they regularly use. Generally lower GDP for countries of residence, so more poverty in sight.      ","metadata":{}},{"cell_type":"markdown","source":"*It now befalls the reader to judge where we've ended up. I have tried to tell a Kaggle user-group's story with data in a new and interesting way. Thank you for your time and attention.*\n\n$$D e e, Portland. 11/28/2021 $$","metadata":{}},{"cell_type":"markdown","source":"# References and resources\n\nExploratory data analysis\n\n* https://en.wikipedia.org/wiki/Exploratory_data_analysis\n* https://en.wikipedia.org/wiki/Scatter_plot\n\nNatural Language Processing\n\n* https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n* author: https://github.com/selva86\n* https://towardsdatascience.com/lda-topic-modeling-an-explanation-e184c90aadcd\n\nOne-hot encoding\n\n* https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n\nBreakpoint detection\n\n* https://towardsdatascience.com/getting-started-with-breakpoints-analysis-in-python-124471708d38\n\nK-Means\n\n* https://www.analyticsvidhya.com/blog/2020/12/a-detailed-introduction-to-k-means-clustering-in-python/\n\n\nDatasets:\n\n* Kaggle\n* https://www.kaggle.com/c/kaggle-survey-2021/data\n\n* Worldbank\n* https://www.kaggle.com/mutindafestus/world-statistics-dataset-from-world-bank\n\n* https://data.worldbank.org/\n\n* Google Search + Movehub\n* https://www.kaggle.com/daanderson/programming-language-search-trends-by-city-2021\n* https://www.kaggle.com/blitzr/movehub-city-rankings      \n* https://trends.google.com/trends/?geo=US\n\n\n\n\n","metadata":{}}]}