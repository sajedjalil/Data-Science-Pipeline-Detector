{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>2021 Kaggle competition</h1>\n    <p><strong>Note:</strong> For all charts in this module, I only selected working Professionals.</p>\n<p>Non-professionals were defined as those who answered Job Title as either:</p>\n<ul>\n  <li>Student</li>\n  <li>Currently not employed</li>\n  <li>Who didn't answer the question (NaN)</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <h1>Data Science, The Next Battle Ground</h1>\n</div>\n    ","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <h2>Table of contents</h2>\n    <ol type=\"I\">\n        <li>Data Science is key for the future of cloud industries</li>\n            <ul>\n                <li>Highlights from the Big Tech</li>\n                <li>Future application of DS</li>\n            </ul>\n        <li>Google, a mile ahead</li>\n            <ul>\n                <li>Softpower advantage: Colab, Kaggle, Tensorflow</li>\n                <li>Hardware advantage: TPU</li>\n            </ul>\n        <li>Current status of data science</li>\n        <ul>\n            <li>Less researchers, More engineers</li>\n            <li>Computer vision and NLP took off a baby step</li>\n        </ul>\n</ol>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <h2>Summary</h2>\n    <ul>\n        <li>At the end of the day, Cloud industries converge to data science.</li>\n        <li>Google already integrated its vertical IaaS and PaaS pipeline.</li>\n        <li>Path to the mainstream is now a matter of time. The industry is asking for a more practical labour force.</li>\n    </ul>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<h1><strong>Introduction</strong></h1>\n   \n<p>It has been more than a year and a half since the first lockdown announcement in North America. Pandemic has had a big impact not only on our lives but also on the cloud industry. On April 30th 2020, Microsoft CEO Satya Nadella said \"We saw 2 years of digital transformation in 2 months\". As we are seeing the light at the end of the covid tunnel, the rate of digitalization will likely deaccelerate. The beautiful part is that the cloud penetration will continue in post covid world. According to new CEO of Amazon Andy Jassy, a mere 4% of IT spending currently goes into the cloud. That being said there's still tremendous upside potential left.</p>\n    <p>Till now on, the Cloud industries' focus was mostly on how to efficiently store and provide hosting services. The result was an oligopoly of a few giant players. Amazon, the number one IaaS/PaaS solution provider, Azure as the Second and Google, the third. The chance that these players to lose their shares from this framework is low because it is hard to differentiate themselves from each other at this stage.</p>\n        <p>So, the vendors were seeking new opportunities outside of the existing scope. The result was the addition of analytical features and introduction of second generation of cloud computing service such as Serverless computing or Function as a Service. However, these changes are not big enough to shake the existing landscape.</p>\n    <p>Along with the adoption of multi-clouds, the importance of data management is rising exponentially. As such, it is natural that the vendors' interest to shift from gathering data to processing data.</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/Evolution%20of%20cloud%20industry.png?raw=true\" alt=\"Evolution of Cloud\">\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n                <div class=\"alert alert-info\">\n                  <h2><Strong>Data Science is key for the future of cloud industries</Strong></h2>\n                </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>1. Highlights from the big three's key events</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n    <li>Each year the big three host a keynote event where it addresses the future strategic plan</li>\n    <li>Although each vendor has a different priority and different target audiences, there is no doubt that <u>machine learning is common practice</u></li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/cloud%20keynotes.jpg?raw=true\" alt=\"Highligts from the big tech\">\n\n<div style=\"font-size:8px; line-height:1.7; color:black;\">\n\nSource: Amazon Web Services. \"AWS re:Invent 2020 - Keynote with Andy Jassy.\" Youtube, uploaded by Amazon Web Services, 11 Dec. 2020, https://www.youtube.com/watch?v=xZ3k7Fd6_eU&t=675s.<Br>\n    \nSource: Google. \"Google Keynote (Google I/O ‘21) - American Sign Language.\" Youtube, uploaded by Google. 18 May. 2021, https://www.youtube.com/watch?v=Mlk888FiI8A&t=2s.<Br>\n    \nSource: Microsoft \"Microsoft Ignite 2021.\" Youtube, uploaded by Microsoft Ignite. 02 Nov. 2021, https://www.youtube.com/watch?v=PraEcNDGSqY&t=2s.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>2. Application of data science in the future</Strong></h2>\n<hr>\n</div>\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <ul>\n        <li>Machines can outperform humans in repetitive tasks. Drivers, factory manufacturing, dispatchers, and cashiers are no longer an exception.</li>\n        <li>Most companies are already using robots in their factories.</li>\n        <li>Tesla is already trying to replace the human driver, Amazon's cashierless GO is under practice, and chatbots are replacing the call centers.</li>\n        <li><Strong>The fourth industrial revolution has already started.</Strong> The data is <u>the single most important commodity</u> in the next revolution.</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/cloud%20application.jpg?raw=true\" alt=\"Evolution of Cloud\">\n\n<div style=\"font-size:8px; line-height:1.7; color:black;\">\nSource: Kumar, Bharani. \"AI Applications Assisting Specialists to Increase Their Efficiency.\" 23 Apr. 2020, https://360digitmg.com/application-of-artificial-intelligence.\n    <Br>    \nSource: Maguire, James. \"12 Examples of Artificial Intelligence: AI Powers Business.\" 13 Sep. 2019, https://www.datamation.com/artificial-intelligence/12-examples-of-artificial-intelligence-ai-powers-business/.\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n                <div class=\"alert alert-info\">\n                  <h2><Strong>Google, a mile ahead</Strong></h2>\n                </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>3. In the world of machine learning, Google is clealy <u>ahead of the curve</u></Strong></h2>\n<hr>\n<ul>\n    <li>Google already holds a significant <Strong>soft power</Strong> advantage over the competitors.</li>\n    <li><Strong>Kaggle and Colab notebooks</Strong> are the top pick for hosted notebook product in data science community.</li>\n    <li><Strong>Kaggle</Strong> is the 2nd preferred platform to share or deploy ML applications followed by <Strong>Github</Strong> among data science professionals.</li>\n</ul>\n    \n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/soft%20powers%20frame.png?raw=true\" alt=\"Kaggle_Github\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>4. Tensorflow, the preferred ML Framework</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n    <li>Created by the Google Brain team, TensorFlow is an open source library for numerical computation and large-scale machine learning.</li>\n    <li>Keras runs on top of the Tensorflow.</li>\n    <li><Strong>While TensorFlow remains dominant ML framework</Strong>, another ML framework (Pytorch) led by Facebook AI is catching up quickly.</li>\n    <li>Potential risk is that Google's TensorFlow end up being similar to Google's front-end framework Angular.</li>\n</ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/keras_tensor_pytorch2.png?raw=true\" alt=\"Tensor_Pytorch\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>5. Managed Machine Learning, The battle ground</Strong></h2>\n<hr>\n</div>\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <ul>\n        <li>No dominant player in 2019. Google outpaced the rest in 2020 but pushed back behind Databricks.</li>\n        <li>A relatively small player like <Strong>Databricks is competing against the big three.</Strong></li>\n        <li>While Google <u>went back to the 1st place</u> in the future preference survey, Azure ML studio sits just behind Google Cloud vertex AI by a small margin.</li>\n        <li>One of the reasons behind Google's underperformance in 2021 might be Google's <Strong>rebranding action</Strong>. Google combined their ML products into Google Cloud Vertex AI.</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/managed%20ML.png?raw=true\" alt=\"ManagedML\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>6. AutoML, Google already dominant, but big opportunity for the rest</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n<li>Google leads since 2019</li>\n<li>Besides Google, Amazon and Microsoft do not show particular strength.</li>\n<li>Opportunities are here for relatively small players like <u>Databricks, H2O, and DataRobot.</u></li>\n</ul>\n\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/autoML.png?raw=true\" alt=\"AutoML\">\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>7. NVDIA, the arms dealer</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n    <li>One of the hottest stocks in the market in the second half of this year was Nvidia.</li>\n    <li>It became the world's largest semiconductor company reaching a peak market capitalization of $824 billion of dollars.</li>\n    <li>While the crypto demand decreased along with the pandemic effect, the market focused on the fact that <Strong>Nvidia was the biggest Beneficiary of accelerated AI service competition in the Cloud industry.</Strong></li>\n    <li>Furthermore, Facebook's name change to Meta reduced uncertainty about end-market uses of AI, giving Nvidia's shares a booster shot.</li>\n    <li>Nvidia's rise is important for Google because <Strong>TPU has the potential to replace GPU.</Strong> Nvidia is an indicator of <u>how far the TPU business can grow.</u></li>\n    <li>In this quarter's conference call, <Strong>Nvidia's CEO Hwang said that the use of AI in the cloud is still less than 10%.</Strong></li>\n    <li>Taking this into account, both GPUs and TPUs have room to grow, but TPUs will take away GPU share and thus have the potential to grow faster.</li>\n</ul>\n\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/NVDIA_market_cap.png?raw=true\" alt=\"NVIDIA's market cap\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>8. Then How much ?</Strong></h2>\n<hr>\n</div>\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <ul>\n        <li>NVIDIA generated <Strong>6.7 bln of revenues from the data center</Strong> segment in 2020.</li>\n        <li>Which means, the big three spent <Strong>at least 1.6 bln dollars on GPU infrastruture</Strong> in 2020.</li>\n        <li>This trend will likely accelerate further along with the data science penetration.</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/nvidia_bigtech.png?raw=true\" alt=\"NVIDIA's Earning\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>9. Why TPU over GPU?</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n    <li>It seems natural for data professionals to use TPU because it is cheaper and faster than GPU</li>\n    <li>The numbers of ALUs in TPUs are substantially higher than in GPUs making them a lot faster. <u>GPUs have between 2500 to 5000 ALUs per core, while TPUs have 32768 ALUs per core</u></li>\n    <li>This chart compares the performance per WATT by processor released by Google. <Strong>At full operation, the economic feasibility of TPU is overwhelming.</Strong></li>\n    </ul>\n    \n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/tpu_images.png?raw=true\" alt=\"TPU performance\">\n  \n</div>\n\n<div style=\"font-size:8px; line-height:1.7; color:black;\">\nSource: Jouppi, N. P. et al. In-datacenter performance analysis of a tensor processing unit. Proc. 44th Int. Symp. Comp. Architecture (ISCA) https://doi.org/10.1145/3079856.3080246 (2017).<Br>\n    \nSource: Sato, Kaz. \"What makes TPUs fine-tuned for deep learning?\". Google Cloud, 31 Aug. 2018, https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning.\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>10. The bottleneck</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n        <li>The only concern for TPU is that if there is not enough data to fully operate the TPU, the relative performance drops sharply.  </li>\n        <li>For example, most factories have to operate at full capacity to maximize profitability. The same logic applies to TPUs. <Strong>The smaller the batch size of the training, the weaker the relative performance of TPU.</Strong> As the AI market is still in its early growth phase, <Strong>there are not enough users to use all the computational capacity of the TPUs.</Strong></li>\n        <li>For this reason, whenever the benchmark is released every year, Nvidia's strategy of taking a realistic approach to AI development and Google's strategy of pursuing the eventual ideals collide.</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/tpu_performance.png?raw=true\" alt=\"TPU+GPU usage\">\n  \n<div style=\"font-size:8px; line-height:1.7; color:black;\">\n    \nSource: Plathottam, S. J. DataDrivenInvestor. Medium, 29 Nov. 2018, https://medium.datadriveninvestor.com/comparing-gpu-and-tpu-training-performance-on-google-colaboratory-c1e54e26993f. Acessed 27 Nov 2021.<Br>\n    \nSource: Salvator, Dave. \"Extending NVIDIA Performance Leadership with MLPerf Inference 1.0 Results.\" Deloper Blog, Nvidia, 22 Apr. 2021, https://developer.nvidia.com/blog/extending-nvidia-performance-leadership-with-mlperf-inference-1-0-results/.<Br>\n    \nSource: Tao Wang, Aarush Selvan. \"Google demonstrates leading performance in latest MLPerf Benchmarks.\" Blog, Google, 1 Jul. 2021, https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>11. Any other NPU?</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <ul>\n        <li>Why not sell TPUs like a Bitcoin miner? <Strong>It doesn't have to</Strong></li>\n        <li>Since it is a completely custom chip, TPUs only works for Google's custom computers.</li>\n        <li>In addition, TPU is a type of NPU, and NPU has been developed by several companies and has been used for a long time. Yet, no one has commercialized.</li>\n        <li>The reason behind this would be either the market is too small or too specific. If there were demand today, then only a few large-scale enterprises could afford TPU. </li>\n        <li>Because of such a limit, <Strong>Google chose to distribute TPU in form of a cloud computing service.</Strong> </li>\n        <li>Google managed to bring the data science community into its cloud notebook product <u>the Colab</u>. In conjunction with its leading ML framework the <u>Tensorflow</u>, <Strong>as such TPU differentiates itself from other NPU. </Strong></li>\n        <li>Hierarchy is Colab - > Tensorflow - > TPU package</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/npu%20status.png?raw=true\" alt=\"custom npu\">\n  \n<div style=\"font-size:8px; line-height:1.7; color:black;\">\nSource: Wikichip. \"\"Neural Processor.\" Wikichip, 4 Nov. 2021, https://en.wikichip.org/wiki/neural_processor. Acessed 27 Nov 2021.\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>12. TPU Penetratrion Rate is Surprisingly Fast</Strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n        <li>Google prepared early on its version of ML chips to reduce potential <Strong>GPU infrastructure cost burden.</Strong></li>\n        <li>As you have seen in the previous charts, the appetite for Google's feature is amazingly high among data science professionals. These efforts worked out. <Strong>Google has successfully managed to penetrate TPU usage.</Strong></li>\n    <li>There was a major TPU usage <u>jump in 2021</u></li>\n        <li>Amazon did introduce its chip in 2021. However, Amazon does not possess as much soft power as Google. Thus, penetration will likely take some time.</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/special%20hardware%20usage.png?raw=true\" alt=\"TPU+GPU usage\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n<h2><Strong>13. Not just fast, but Broadly</Strong></h2>\n<hr>\n</div>\n    \n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n<li>Google's TPUs ared used widely <Strong>regardless of the size of the company</Strong></li>\n<li>A reason behind this would be <u>TPU started to support the Pytorch framework as of 2020.</u></li>\n</ul>\n</div>  \n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/moneyspenton.jpg?raw=true\" alt=\"TPU+GPU usage\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    \n<h2><Strong>14. TPU VS GPU, Battle is reminiscent of Bitcoin mining</Strong></h2>\n\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n        <li>There was a time when Bitcoin was mined through GPUs. The increasing difficulty in mining made GPUs unprofitable compared to ASICs which provided a much higher hash rate per dollar.</li>\n        <li>Similarly, TPU will become more attractive with an ever-increasing amount of data processing in the future.</li>  \n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/bitcoin%20mining.png?raw=true\" alt=\"TPU+GPU usage\">\n\n<div style=\"font-size:8px; line-height:1.7; color:black;\">\nSource: \"Bitcoin Mining Profitability historical chart.\" Bitinfocharts. 28 Nov. 2021, https://bitinfocharts.com/comparison/bitcoin-mining_profitability.html#alltime.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>15. TPU, Time is on your side</Strong></h2>\n\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n        <li>Data usage is expected to increase exponentially both from the consumer side and the business side.</li>\n        <li>The key difference in the upcoming era of digitalization is that there will be <Strong>more data created by machines than the data created by humans.</Strong></li>\n        <li>With the increased adoption of 5G, IoT, AR/VR, as well as metaverse, data processing is expected to explode.</li>\n    </ul>\n</div>\n     \n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/human_machine_data.png?raw=true\" alt=\"TPU+GPU usage\">\n    \n<div style=\"font-size:8px; line-height:1.7; color:black;\">\nSource: \"The Exponential Growth of Data.\" Insidebigdata. 16 Feb. 2017, https://insidebigdata.com/2017/02/16/the-exponential-growth-of-data/.\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n                <div class=\"alert alert-info\">\n                  <h2><Strong>Current status of data science</Strong></h2>\n                </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n    <h2><Strong>16. Digital transformation / Data science everywhere</Strong><h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n        <li>The key difference in data science professionals distribution in 2018 and 2021 is <Strong>Computers/Technology industry proportion experienced <u>a large decline.</u></Strong></li>\n        <li>Academics/Education gained significant shares followed by Manufacturing/Fabrication and Medical/Pharmaceutical industries. A sign that data science is spreading all over the industry.</li>\n        <li>Data science professionals are distributed regardless of the size of the company which further confirm data science is on the path of mainstream business.</li>\n    </ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/data_pros_distr.png?raw=true\n\" alt=\"DS distribution vs. industry\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n    <hr>\n<h2><strong>17. Industry is asking less scientists, but more engineers </strong></h2>\n    <hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n  <li>The number of software engineers and data scientists in the Computer/Technology field declined in 2021.</li>\n  <li>While Research scientists working in the Academic/Education field declined, the number of data scientists and software engineers increased in 2021.</li>\n  <li><strong>The machine learning engineer role is newly added in 2021.</strong></li>\n  <li>MLE's tasks in the workplace are the hybrid of traditional data scientists and software engineers. <strong>Focusing on less analytics, but more on managing product lines.</strong></li>\n  <li>Much like other industries, the proportion of academia and scientists will gradually go down and be replaced by engineers.</li>\n    <li><Strong>Job market shows a sign of great rotation from research to real world practice.</Strong></li>\n</ul>\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/more%20engi.png?raw=true\" alt=\"enginners in demand\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:25px; line-height:1.7; color:black;\">\n<hr>\n<h2><strong>18. Current Status of Computer Vision and Natural language processing</strong></h2>\n<hr>\n</div>\n\n<div style=\"font-size:18px; line-height:1.7; color:black;\">\n<ul>\n  <li>The public acknowledges these two technologies to be the true adoption of data science in conjunction with robotics.</li>\n  <li>25% of respondents in the Computer/Technology industry said they are exposed to NLP technology. The rests sit only around 15%.</li>\n  <li>NLP is already adopted in the form of chatbot or speech recognition or translation. <strong>Any tasks that require inter-person communication or paperwork will eventually apply NLP technology.</strong></li>\n    <Br>\n  <li>Only 35% of respondents in the Computer/Technology industry said they are exposed to computer vision technology. 29% of respondents were from the Academics/Education industry. The rest are hovering around 20%.</li>\n  <li>Tesla officially launched its full self-driving subscription package on July 22, 2021, Hyundai completed its Boston dynamics acquisition on June 21, 2021, Deepmind acquired physics simulator MuJoCo and has made it open-source. Facebook changed its company name to Meta. <strong>Given that computer vision is a must-have from robotics to the metaverse, the number of respondents who will say \"YES\" is likely to explode in near the future.</strong></li>\n</ul>\n\n</div>\n\n<img src=\"https://github.com/treksis/2021_Kaggle_Survey_Competition/blob/main/nlp_vision.png?raw=true\" alt=\"NLP and CV\">\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <h2>Conclusion. Behind the scenes</h2>\n    <ul>\n        <li>Now, I have a better understand why cloud industries are obsessed with data science. IaaS and PaaS potential is amazingly high.</li>\n        <li>I am  surprised how well Google is prepared for the next battleground.</li>\n        <li>I am shocked about the performance of Google's NPU and how well it is penetrated inside the data science community.</li>\n        <li>I am surpised the adoption of computer vision and natural language processing was that small within data science community.</li>\n        <li>Thanks to the Kaggle community and moderator to share the valuable data set.</li>\n    </ul>\n    </div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:18px; line-height:1.7; color:black;\">\n    <h2>Reference</h2>\n    </div>\n\n[1] Amazon Web Services. \"AWS re:Invent 2020 - Keynote with Andy Jassy.\" Youtube, uploaded by Amazon Web Services, 11 Dec. 2020, https://www.youtube.com/watch?v=xZ3k7Fd6_eU&t=675s.\n\n[2] Google. \"Google Keynote (Google I/O ‘21) - American Sign Language.\" Youtube, uploaded by Google. 18 May. 2021, https://www.youtube.com/watch?v=Mlk888FiI8A&t=2s.\n\n[3] Microsoft \"Microsoft Ignite 2021.\" Youtube, uploaded by Microsoft Ignite. 02 Nov. 2021, https://www.youtube.com/watch?v=PraEcNDGSqY&t=2s.\n\n[4] Kumar, Bharani. \"AI Applications Assisting Specialists to Increase Their Efficiency.\" 23 Apr. 2020, https://360digitmg.com/application-of-artificial-intelligence.\n\n[5] Maguire, James. \"12 Examples of Artificial Intelligence: AI Powers Business.\" 13 Sep. 2019, https://www.datamation.com/artificial-intelligence/12-examples-of-artificial-intelligence-ai-powers-business/.\n\n[6] Jouppi, N. P. et al. In-datacenter performance analysis of a tensor processing unit. Proc. 44th Int. Symp. Comp. Architecture (ISCA) https://doi.org/10.1145/3079856.3080246 (2017).\n\n[7] Sato, Kaz. \"What makes TPUs fine-tuned for deep learning?\". Google Cloud, 31 Aug. 2018, https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning.\n\n[8] Plathottam, S. J. DataDrivenInvestor. Medium, 29 Nov. 2018, https://medium.datadriveninvestor.com/comparing-gpu-and-tpu-training-performance-on-google-colaboratory-c1e54e26993f. Acessed 27 Nov 2021.\n\n[9] Salvator, Dave. \"Extending NVIDIA Performance Leadership with MLPerf Inference 1.0 Results.\" Deloper Blog, Nvidia, 22 Apr. 2021, https://developer.nvidia.com/blog/extending-nvidia-performance-leadership-with-mlperf-inference-1-0-results/.\n\n[10] Tao Wang, Aarush Selvan. \"Google demonstrates leading performance in latest MLPerf Benchmarks.\" Blog, Google, 1 Jul. 2021, https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4.\n\n[11] Wikichip. \"\"Neural Processor.\" Wikichip, 4 Nov. 2021, https://en.wikichip.org/wiki/neural_processor. Acessed 27 Nov 2021.\n\n[12] \"Bitcoin Mining Profitability historical chart.\" Bitinfocharts. 28 Nov. 2021, https://bitinfocharts.com/comparison/bitcoin-mining_profitability.html#alltime.\n\n[13] \"The Exponential Growth of Data.\" Insidebigdata. 16 Feb. 2017, https://insidebigdata.com/2017/02/16/the-exponential-growth-of-data/.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-size:20px; line-height:1.7; color:black;\">\n    <Strong>Thank you</Strong>\n    </div>","metadata":{}},{"cell_type":"markdown","source":" <div style=\"font-family:Helvetica Neue; font-size:16px; line-height:1.7; color:black;\">\n                <div class=\"alert alert-warning\">\n                  <strong>Code Section</strong>\n                </div>","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as patches\nfrom matplotlib.gridspec import GridSpec\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.options.display.max_colwidth\npd.options.mode.chained_assignment = None  # default='warn'\n\ndata_2021 = pd.read_csv('../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv', low_memory = False, encoding='UTF-8')\nquestions_2021 = data_2021.iloc[0, :].T\ndata_2021 = data_2021.iloc[1:, :]\ndata_2020 = pd.read_csv('../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv', low_memory = False, encoding='UTF-8')\nquestions_2020 = data_2020.iloc[0, :].T\ndata_2020 = data_2020.iloc[1:, :]\ndata_2019 = pd.read_csv('../input/kaggle-survey-2019/multiple_choice_responses.csv', low_memory = False, encoding='UTF-8')\nquestions_2019 = data_2019.iloc[0, :].T\ndata_2019 = data_2019.iloc[1:, :]\ndata_2018 = pd.read_csv('../input/kaggle-survey-2018/multipleChoiceResponses.csv', low_memory = False, encoding='UTF-8')\nquestions_2018 = data_2018.iloc[0, :].T\ndata_2018 = data_2018.iloc[1:, :]\ndata_2017 = pd.read_csv('../input/kaggle-survey-2017/multipleChoiceResponses.csv', low_memory = False, encoding='ISO-8859-1')\nquestions_2017 = data_2017.iloc[0, :].T\ndata_2017 = data_2017.iloc[1:, :]\n\n!pip install openpyxl\n\nnvda_earnings = pd.read_excel('../input/stonks-data/nvda_earnings.xlsx')\ncloud_earnings = pd.read_excel('../input/stonks-data/cloud.xlsx')\n# 2021 Yes. 2020 Yes. 2019 Yes. About TPUs\n\ndef get_professionals(data, column):\n    data = data.loc[data[column] != 'Student']\n    data = data.loc[data[column] != 'Currently not employed']\n    data = data.loc[data[column] != 'Not employed']\n    data = data.loc[data[column].notna()]\n    return data\n\npros_2021 = get_professionals(data_2021, 'Q5')\npros_2020 = get_professionals(data_2020, 'Q5')\n\npros_2019 = data_2019[data_2019['Q5'].notna()]\npros_2019 = pros_2019[pros_2019['Q5'] != 'Student']\npros_2019 = pros_2019[pros_2019['Q5'] != 'Not employed']","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:13.17389Z","iopub.execute_input":"2021-11-28T19:40:13.174983Z","iopub.status.idle":"2021-11-28T19:40:31.25466Z","shell.execute_reply.started":"2021-11-28T19:40:13.174835Z","shell.execute_reply":"2021-11-28T19:40:31.253548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def open_source(x):\n    if x == 'MySQL':\n        return 1\n    elif x == 'PostgreSQL':\n        return 1\n    elif x == 'MongoDB':\n        return 1\n    elif x == 'SQLite':\n        return 1\n    elif x == 'PostgresSQL':\n        return 1\n    else:\n        return 0\n    \ndef aws(x):\n    if 'Amazon' in x:\n        return 1\n    elif 'AWS' in x:\n        return 1\n    else:\n        return 0\n    \ndef gcp(x):\n    if 'Google' in x:\n        return 1\n    else:\n        return 0\n    \ndef azure(x):\n    if 'Microsoft' in x:\n        return 1\n    elif 'Azure' in x:\n        return 1\n    else: \n        return 0\n    \ndef annotate_axes(fig):\n    for i, ax in enumerate(fig.axes):\n        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n        ax.tick_params(labelbottom=False, labelleft=False)","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:31.256348Z","iopub.execute_input":"2021-11-28T19:40:31.256607Z","iopub.status.idle":"2021-11-28T19:40:31.266592Z","shell.execute_reply.started":"2021-11-28T19:40:31.256578Z","shell.execute_reply":"2021-11-28T19:40:31.265677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 1.1: Big 3 in world of cloud != Big 3 in the world of data science</strong><br>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- 2021 Q.27 Cloud Computing Platform ---\n\ncloud_2021 = ['Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8', 'Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']\ndf_2021 = pros_2021[cloud_2021]\n\ncount_2021 = pd.Series(df_2021[cloud_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_2021 = pd.DataFrame(count_2021)\ndf_count_2021 = df_count_2021.reset_index()\ndf_count_2021.columns = ['Cloud', 'Counts']\n\n# --- 2020 Q.26 Cloud Computing Platform ---\n\ncloud_2020 = ['Q26_A_Part_1','Q26_A_Part_2','Q26_A_Part_3','Q26_A_Part_4','Q26_A_Part_5','Q26_A_Part_6', 'Q26_A_Part_7','Q26_A_Part_8','Q26_A_Part_9','Q26_A_Part_10','Q26_A_Part_11','Q26_A_OTHER']\ndf_2020 = pros_2020[cloud_2020]\n\ncount_2020 = pd.Series(df_2020[cloud_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_2020 = pd.DataFrame(count_2020)\ndf_count_2020 = df_count_2020.reset_index()\ndf_count_2020.columns = ['Cloud', 'Counts']\n\n# --- 2019 Q.29 Cloud Computing Platform ---\n\ncloud_2019 = ['Q29_Part_1','Q29_Part_2','Q29_Part_3','Q29_Part_4','Q29_Part_5','Q29_Part_6','Q29_Part_7', 'Q29_Part_8','Q29_Part_9','Q29_Part_10','Q29_Part_11','Q29_Part_12']\ndf_2019 = pros_2019[cloud_2019]\n\ncount_2019 = pd.Series(df_2019[cloud_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_2019 = pd.DataFrame(count_2019)\ndf_count_2019 = df_count_2019.reset_index()\ndf_count_2019.columns = ['Cloud', 'Counts']\ndf_count_2019 = df_count_2019.append({'Cloud':'IBM Cloud / Red Hat', 'Counts': 451}, ignore_index=True).append({'Cloud':'Tencent Cloud', 'Counts': None}, ignore_index=True)\ndf_count_2019 = df_count_2019.drop([4,11]).reset_index(drop = True)\n\n# --- Merge All Three ---\n\ncloud_df = df_count_2021.merge(df_count_2020, on = 'Cloud').merge(df_count_2019, how = 'left')\ncloud_df['Counts'][4] = 451\ncloud_df = cloud_df.rename(columns = {'Cloud': 'Cloud', 'Counts_x':'2021', 'Counts_y': '2020', 'Counts': '2019'})\ncloud_df_bar = cloud_df.set_index('Cloud').T[::-1]\n\ncloud_df_bar.columns = cloud_df_bar.columns.str.strip()\ncloud_df_bar = cloud_df_bar.rename(columns = {'Amazon Web Services (AWS)': 'AWS',\n                                             'Google Cloud Platform (GCP)': 'GCP',\n                                             'Microsoft Azure': 'Azure',\n                                             'IBM Cloud / Red Hat': 'IBM',\n                                             'Oracle Cloud': 'Oracle',\n                                              'VMware Cloud': 'VMware',\n                                              'SAP Cloud': 'SAP',\n                                              'Salesforce Cloud': 'Salesforce',\n                                              'Alibaba Cloud': 'Alibaba',\n                                              'Tencent Cloud': 'Tencent'\n                                             })\n\n# --- Plot ---\n\nplt.style.use('classic')\nplt.rcParams['font.size'] = 20\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ncloud_df_bar.plot(kind = 'bar', \n                  ax = ax,\n                  color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f','#7f7f7f'),\n                  width = 1)\n\nax.set_title(\"Cloud computing platform usage\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.savefig('1.1 Cloud Computing Platform Usage.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:31.267819Z","iopub.execute_input":"2021-11-28T19:40:31.268037Z","iopub.status.idle":"2021-11-28T19:40:32.057767Z","shell.execute_reply.started":"2021-11-28T19:40:31.26801Z","shell.execute_reply":"2021-11-28T19:40:32.056903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Cloud Computing Platform Market Share ---\n\nfor_perc = cloud_df_bar\nfor_perc = for_perc.divide(for_perc.sum(axis=1), axis = 0)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\nfor_perc.plot(kind='area',\n              stacked=True,\n              ax = ax,\n              color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f','#7f7f7f')\n             )\n\nax.annotate('Share of None is decreasing',\n             xy=(1.6, 0.53),\n             xytext=(0.3, 0.58),\n             arrowprops = dict(facecolor='black', shrink=0.05),\n             fontsize=20,\n            )\n\nax.set_title(\"Cloud computing platform usage by share\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Percentage', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.savefig('1.2 Cloud Computing Platform Usage by share.png', bbox_inches='tight')\nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:32.059926Z","iopub.execute_input":"2021-11-28T19:40:32.06067Z","iopub.status.idle":"2021-11-28T19:40:32.748212Z","shell.execute_reply.started":"2021-11-28T19:40:32.060628Z","shell.execute_reply":"2021-11-28T19:40:32.747375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 1.2: The future preference on Google continues</strong><br>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- 2021 Q27. The best developer's experience with ---\n\ncloud_2_year_list = ['Q27_A_Part_1',\n          'Q27_A_Part_2',\n          'Q27_A_Part_3',\n          'Q27_A_Part_4',\n          'Q27_A_Part_5',\n          'Q27_A_Part_6',\n          'Q27_A_Part_7',\n          'Q27_A_Part_8',\n          'Q27_A_Part_9',\n          'Q27_A_Part_10',\n          'Q27_A_Part_11',\n          'Q27_A_OTHER']\n\ncloud_2_year_later = pros_2021[cloud_2_year_list]\n\ncount_cloud_2_year_later = pd.Series(cloud_2_year_later[cloud_2_year_list].squeeze().values.ravel()).value_counts()\ndf_count_cloud_2_year_later = pd.DataFrame(count_cloud_2_year_later).reset_index()\ndf_count_cloud_2_year_later.columns = ['Cloud', '2021']\ndf_count_cloud_2_year_later = df_count_cloud_2_year_later.set_index('Cloud').T\n\n\ncloud_best_exp = pd.Series(pros_2021['Q28'].squeeze().values.ravel()).value_counts()\ndf_cloud_best_exp = pd.DataFrame(cloud_best_exp).reset_index()\ndf_cloud_best_exp.columns = ['Cloud', '2021']\ndf_cloud_best_exp = df_cloud_best_exp.set_index('Cloud').T\ndf_cloud_best_exp.columns = df_cloud_best_exp.columns.str.strip()\ndf_cloud_best_exp = df_cloud_best_exp.rename(columns = {'Amazon Web Services (AWS)': 'AWS',\n                                             'Google Cloud Platform (GCP)': 'GCP',\n                                            'They all had a similarly enjoyable developer experience':'All similar',\n                                             'Microsoft Azure': 'Azure',\n                                                        'None were satisfactory':'All bad',\n                                             'IBM Cloud / Red Hat': 'IBM',\n                                             'Oracle Cloud': 'Oracle',\n                                              'VMware Cloud': 'VMware',\n                                              'SAP Cloud': 'SAP',\n                                              'Salesforce Cloud': 'Salesforce',\n                                              'Alibaba Cloud': 'Alibaba',\n                                              'Tencent Cloud': 'Tencent'\n                                             })\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndf_cloud_best_exp.plot(kind = 'bar',\n                       ax = ax,\n                       color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f','#7f7f7f', '#7f7f7f'))\n                        # AWS = Blue, GCP = Red, Azure = Orange\n\nax.set_title(\"Cloud platform with the best developer experience\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Percentage', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.savefig('1.3 Cloud Platform with the best developer experience.png', bbox_inches='tight')\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:32.749458Z","iopub.execute_input":"2021-11-28T19:40:32.749687Z","iopub.status.idle":"2021-11-28T19:40:33.339465Z","shell.execute_reply.started":"2021-11-28T19:40:32.749664Z","shell.execute_reply":"2021-11-28T19:40:33.338637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 2021 Q27_B Future Preference ---\n\ncloud_2_year_list = ['Q27_B_Part_1',\n          'Q27_B_Part_2',\n          'Q27_B_Part_3',\n          'Q27_B_Part_4',\n          'Q27_B_Part_5',\n          'Q27_B_Part_6',\n          'Q27_B_Part_7',\n          'Q27_B_Part_8',\n          'Q27_B_Part_9',\n          'Q27_B_Part_10',\n          'Q27_B_Part_11',\n          'Q27_B_OTHER']\n\ncloud_2_year_later = pros_2021[cloud_2_year_list]\n\ncount_cloud_2_year_later = pd.Series(cloud_2_year_later[cloud_2_year_list].squeeze().values.ravel()).value_counts()\ndf_count_cloud_2_year_later = pd.DataFrame(count_cloud_2_year_later).reset_index()\ndf_count_cloud_2_year_later.columns = ['Cloud', '2021']\ndf_count_cloud_2_year_later = df_count_cloud_2_year_later.set_index('Cloud').T\n\ndf_count_cloud_2_year_later.columns = df_count_cloud_2_year_later.columns.str.strip()\ndf_count_cloud_2_year_later = df_count_cloud_2_year_later.rename(columns = {'Amazon Web Services (AWS)': 'AWS',\n                                             'Google Cloud Platform (GCP)': 'GCP',\n                                             'Microsoft Azure': 'Azure',\n                                             'IBM Cloud / Red Hat': 'IBM',\n                                             'Oracle Cloud': 'Oracle',\n                                              'VMware Cloud': 'VMware',\n                                              'SAP Cloud': 'SAP',\n                                              'Salesforce Cloud': 'Salesforce',\n                                              'Alibaba Cloud': 'Alibaba',\n                                              'Tencent Cloud': 'Tencent'\n                                             })\n\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndf_count_cloud_2_year_later.plot(kind='bar',\n                                 ax=ax,\n                                color = ('#1f77b4', '#ff7f0e', '#d62728', '#2ca02c', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f','#7f7f7f', '#7f7f7f'))\n                                # Azure = Red = #d62728 // None = Green = #2ca02c\nax.set_title(\"Cloud platform that respondant is willing to become more familiar in 2 year\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.savefig('1.4 Cloud platform that respondant is willing to become more familiar in 2 year.png', bbox_inches='tight')\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:33.341219Z","iopub.execute_input":"2021-11-28T19:40:33.341501Z","iopub.status.idle":"2021-11-28T19:40:33.92067Z","shell.execute_reply.started":"2021-11-28T19:40:33.341465Z","shell.execute_reply":"2021-11-28T19:40:33.920105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 1.3: Preference on Google goes beyond Amazon</strong><br>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- Cloud Computing PRODUCT ---\n\n# --- 2019 Q30 ---\n\ncloud_computing_2019 = ['Q30_Part_1','Q30_Part_2','Q30_Part_3','Q30_Part_4','Q30_Part_5','Q30_Part_6','Q30_Part_7','Q30_Part_8','Q30_Part_9','Q30_Part_10','Q30_Part_11','Q30_OTHER_TEXT']\ndf_computing_2019 = pros_2019[cloud_computing_2019]\ncount_compute_2019 = pd.Series(df_computing_2019[cloud_computing_2019].squeeze().values.ravel()).value_counts()\n\n# ---\ndf_count_compute_2019 = pd.DataFrame(count_compute_2019)\ndf_count_compute_2019 = df_count_compute_2019.reset_index()\ndf_count_compute_2019.columns = ['Cloud Compute', 'Counts']\n\n# ---\ndf_count_compute_2019['Cloud Compute'] = df_count_compute_2019['Cloud Compute'].str.strip()\ndf_count_compute_2019 = df_count_compute_2019[(df_count_compute_2019['Cloud Compute'] == 'AWS Elastic Compute Cloud (EC2)') |\n                      (df_count_compute_2019['Cloud Compute'] == 'Google Compute Engine (GCE)') |\n                     (df_count_compute_2019['Cloud Compute'] == 'Azure Virtual Machines') |\n                     (df_count_compute_2019['Cloud Compute'] == 'None')\n                                             ]\n# ---\ndf_count_compute_2019['Cloud Compute'] = df_count_compute_2019['Cloud Compute'].replace({\n                                                'AWS Elastic Compute Cloud (EC2)': 'Amazon Elastic Compute Cloud (EC2)',\n                                               'Google Compute Engine (GCE)': 'Google Cloud Compute Engine',\n                                                'Azure Virtual Machines': 'Microsoft Azure Virtual Machines',\n                                                'None': 'No / None'\n                                               })\n\n# --- 2020 Q27 ---\ncloud_computing_2020 = ['Q27_A_Part_1','Q27_A_Part_2','Q27_A_Part_3','Q27_A_Part_4','Q27_A_Part_5','Q27_A_Part_6','Q27_A_Part_7','Q27_A_Part_8','Q27_A_Part_9','Q27_A_Part_10','Q27_A_Part_11','Q27_A_OTHER']\ndf_computing_2020 = pros_2020[cloud_computing_2020]\ncount_compute_2020 = pd.Series(df_computing_2020[cloud_computing_2020].squeeze().values.ravel()).value_counts()\n# ---\ndf_count_compute_2020 = pd.DataFrame(count_compute_2020)\ndf_count_compute_2020 = df_count_compute_2020.reset_index()\ndf_count_compute_2020.columns = ['Cloud Compute', 'Counts']\n# ---\ndf_count_compute_2020['Cloud Compute'] = df_count_compute_2020['Cloud Compute'].str.strip()\ndf_count_compute_2020 = df_count_compute_2020[(df_count_compute_2020['Cloud Compute'] == 'Amazon EC2') |\n                      (df_count_compute_2020['Cloud Compute'] == 'Google Cloud Compute Engine') |\n                     (df_count_compute_2020['Cloud Compute'] == 'Azure Cloud Services') |\n                     (df_count_compute_2020['Cloud Compute'] == 'No / None')\n                     ]\n# ---\ndf_count_compute_2020['Cloud Compute'] = df_count_compute_2020['Cloud Compute'].replace({\n                                                'Amazon EC2': 'Amazon Elastic Compute Cloud (EC2)',\n                                               'Google Cloud Compute Engine': 'Google Cloud Compute Engine',\n                                                'Azure Cloud Services': 'Microsoft Azure Virtual Machines'\n                                               })\n\n\n\n# --- 2021 Q29 ---\ncloud_computing_2021 = ['Q29_A_Part_1','Q29_A_Part_2','Q29_A_Part_3','Q29_A_Part_4','Q29_A_OTHER']\ndf_computing_2021 = pros_2021[cloud_computing_2021]\ncount_compute_2021 = pd.Series(df_computing_2021[cloud_computing_2021].squeeze().values.ravel()).value_counts()\n# ---\ndf_count_compute_2021 = pd.DataFrame(count_compute_2021)\ndf_count_compute_2021 = df_count_compute_2021.reset_index()\ndf_count_compute_2021.columns = ['Cloud Compute', 'Counts']\n# ---\ndf_count_compute_2021['Cloud Compute'] = df_count_compute_2021['Cloud Compute'].str.strip()\ndf_count_compute_2021 = df_count_compute_2021[(df_count_compute_2021['Cloud Compute'] != 'Other')]\n\ncloud_compute_df = df_count_2021.merge(df_count_2020, on = 'Cloud').merge(df_count_2019, how = 'left')\ncloud_compute_df = df_count_compute_2021.merge(df_count_compute_2020, on = 'Cloud Compute').merge(df_count_compute_2019, how = 'left')\ncloud_compute_df = cloud_compute_df.rename(columns = {'Cloud': 'Cloud', 'Counts_x':'2021', 'Counts_y': '2020', 'Counts': '2019'})\n\ncloud_compute_df = cloud_compute_df.T\ncloud_compute_df.columns = cloud_compute_df.iloc[0]\ncloud_compute_df = cloud_compute_df.drop(cloud_compute_df.index[0])\n#cloud_df.index.name = 'Cloud'\ncloud_compute_df = cloud_compute_df.iloc[::-1] # reverse ro\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n                      \ncloud_compute_df.plot(kind = 'bar', \n                      ax= ax,\n                      color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'))\n\n\nax.set_title(\"Cloud platform product usage in regular basis in kaggle survey\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.9, 0.5), fontsize=20)\n\nplt.savefig('1.6 Cloud platform product usage in regular basis in kaggle survey.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:33.921952Z","iopub.execute_input":"2021-11-28T19:40:33.922292Z","iopub.status.idle":"2021-11-28T19:40:34.515918Z","shell.execute_reply.started":"2021-11-28T19:40:33.922265Z","shell.execute_reply":"2021-11-28T19:40:34.515362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Cloud computing product Market share ---\n\nfor_perc = cloud_compute_df\nfor_perc = for_perc.divide(for_perc.sum(axis=1), axis = 0)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n                      \nfor_perc.plot(kind='area', \n              stacked=True,\n              ax = ax,\n             color = ('#1f77b4', '#ff7f0e', '#2ca02c', '#d62728')\n             )\n\nax.set_title(\"Cloud platform product usage by share\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Percentage', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.9, 0.5), fontsize=20)\n\nplt.savefig('1.7 Cloud platform product usage by share.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:34.516863Z","iopub.execute_input":"2021-11-28T19:40:34.517535Z","iopub.status.idle":"2021-11-28T19:40:34.996937Z","shell.execute_reply.started":"2021-11-28T19:40:34.517502Z","shell.execute_reply":"2021-11-28T19:40:34.996315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- 2021 Q29_B Future preference ---\n\ncloud_computing_in_2 = ['Q29_B_Part_1','Q29_B_Part_2','Q29_B_Part_3','Q29_B_Part_4','Q29_B_OTHER']\n\ncount_cloud_computing_in_2 = pd.Series(pros_2021[cloud_computing_in_2].squeeze().values.ravel()).value_counts()\n\ndf_count_cloud_computing_in_2 = pd.DataFrame(count_cloud_computing_in_2)\ndf_count_cloud_computing_in_2 = df_count_cloud_computing_in_2.reset_index()\ndf_count_cloud_computing_in_2.columns = ['Cloud Compute', '2021']\ndf_count_cloud_computing_in_2 = df_count_cloud_computing_in_2.set_index('Cloud Compute').T\n\n# -- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\ndf_count_cloud_computing_in_2.plot(kind = 'bar', \n                      ax= ax,\n                      color = ('#ff7f0e', '#d62728', '#1f77b4', '#2ca02c','#7f7f7f'))\n                    # Red = Google / Blue = Amazon / None = Green / Azure = Red\n\nax.set_title(\"Cloud computing platform that willing to become more familiar in 2 year\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.9, 0.5), fontsize=20)\nax.text(0.01, 2500, 'AWS is pushed back to the third place',\n        verticalalignment='bottom',\n        fontsize=12)\n\nplt.savefig('1.8 Cloud computing platform that willing to become more familiar in 2 year.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:34.997978Z","iopub.execute_input":"2021-11-28T19:40:34.998746Z","iopub.status.idle":"2021-11-28T19:40:35.532442Z","shell.execute_reply.started":"2021-11-28T19:40:34.998711Z","shell.execute_reply":"2021-11-28T19:40:35.531645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 2.1: The big data and rise of Snowflakes</strong><br>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- 2021 Q32 Big data ---\n\nbig_data_2021 = ['Q32_A_Part_1','Q32_A_Part_2',\n                 'Q32_A_Part_3','Q32_A_Part_4',\n                 'Q32_A_Part_5','Q32_A_Part_6',\n                 'Q32_A_Part_7','Q32_A_Part_8',\n                 'Q32_A_Part_9','Q32_A_Part_10',\n                 'Q32_A_Part_11','Q32_A_Part_12',\n                 'Q32_A_Part_13','Q32_A_Part_14',\n                 'Q32_A_Part_15','Q32_A_Part_16',\n                 'Q32_A_Part_17','Q32_A_Part_18',\n                 'Q32_A_Part_19','Q32_A_Part_20','Q32_A_OTHER']\n\ndf_bigdata_2021 = pros_2021[big_data_2021]\ncount_bigdata_2021 = pd.Series(df_bigdata_2021[big_data_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_bigdata_2021 = pd.DataFrame(count_bigdata_2021)\ndf_count_bigdata_2021 = df_count_bigdata_2021.reset_index()\ndf_count_bigdata_2021.columns = ['big data', 'Counts']\n\n# --- 2020 Q29 Big data ---\n\nbig_data_2020 = ['Q29_A_Part_1','Q29_A_Part_2',\n                 'Q29_A_Part_3','Q29_A_Part_4',\n                 'Q29_A_Part_5','Q29_A_Part_6',\n                 'Q29_A_Part_7','Q29_A_Part_8',\n                 'Q29_A_Part_9','Q29_A_Part_10',\n                 'Q29_A_Part_11','Q29_A_Part_12',\n                 'Q29_A_Part_13','Q29_A_Part_14',\n                 'Q29_A_Part_15','Q29_A_Part_16',\n                 'Q29_A_Part_17','Q29_A_OTHER']\n\ndf_bigdata_2020 = pros_2020[big_data_2020]\ncount_bigdata_2020 = pd.Series(df_bigdata_2020[big_data_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_bigdata_2020 = pd.DataFrame(count_bigdata_2020)\ndf_count_bigdata_2020 = df_count_bigdata_2020.reset_index()\ndf_count_bigdata_2020.columns = ['big data', 'Counts']\n\n# --- 2019 Q34 Big data ---\n\nbig_data_2019 = ['Q34_Part_1','Q34_Part_2',\n                 'Q34_Part_3','Q34_Part_4',\n                 'Q34_Part_5','Q34_Part_6',\n                 'Q34_Part_7','Q34_Part_8',\n                 'Q34_Part_9','Q34_Part_10',\n                 'Q34_Part_11','Q34_Part_12',\n                 'Q34_OTHER_TEXT']\n\ndf_bigdata_2019 = pros_2019[big_data_2019]\ncount_bigdata_2019 = pd.Series(df_bigdata_2019[big_data_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_bigdata_2019 = pd.DataFrame(count_bigdata_2019)\ndf_count_bigdata_2019 = df_count_bigdata_2019.reset_index()\ndf_count_bigdata_2019.columns = ['big data', 'Counts']\ndf_count_bigdata_2019 = df_count_bigdata_2019[(df_count_bigdata_2019['big data'] == 'MySQL') |\n                                              (df_count_bigdata_2019['big data'] == 'PostgresSQL') |\n                                              (df_count_bigdata_2019['big data'] == 'Microsoft SQL Server') |\n                                              (df_count_bigdata_2019['big data'] == 'SQLite') |\n                                              (df_count_bigdata_2019['big data'] == 'Oracle Database') |\n                                              (df_count_bigdata_2019['big data'] == 'AWS Relational Database Service') |\n                                              (df_count_bigdata_2019['big data'] == 'Microsoft Access') |\n                                              (df_count_bigdata_2019['big data'] == 'Google Cloud SQL') |\n                                              (df_count_bigdata_2019['big data'] == 'Azure SQL Database') |\n                                              (df_count_bigdata_2019['big data'] == 'AWS DynamoDB')\n                                             ]\n\n# --- 2021 Clean up. Merge into AWS, GCP, and AZURE // Open source and Commercial ---\n\ndf_count_bigdata_2021['big data'] = df_count_bigdata_2021['big data'].str.strip()\ndf_count_bigdata_2021['open source'] = df_count_bigdata_2021['big data'].apply(open_source)\ndf_count_bigdata_2021['aws'] = df_count_bigdata_2021['big data'].apply(aws)\ndf_count_bigdata_2021['gcp'] = df_count_bigdata_2021['big data'].apply(gcp)\ndf_count_bigdata_2021['azure'] = df_count_bigdata_2021['big data'].apply(azure)\n\nopen_source_2021 = df_count_bigdata_2021[df_count_bigdata_2021['open source'] == 1]\nopen_source_2021 = open_source_2021[['big data', 'Counts']]\n\naws_2021 = df_count_bigdata_2021[df_count_bigdata_2021['aws'] == 1]\ngcp_2021 = df_count_bigdata_2021[df_count_bigdata_2021['gcp'] == 1]\nazure_2021 = df_count_bigdata_2021[df_count_bigdata_2021['azure'] == 1]\nothers_2021 = df_count_bigdata_2021[(df_count_bigdata_2021['aws'] != 1) & \n                                    (df_count_bigdata_2021['gcp'] != 1) & \n                                    (df_count_bigdata_2021['azure'] != 1) &\n                                    (df_count_bigdata_2021['open source'] != 1)]\n\naws_2021['big data'].iloc[0] = 'aws'\ngcp_2021['big data'].iloc[0] = 'gcp'\nazure_2021['big data'].iloc[0] = 'azure'\n\ncommercial_2021 = aws_2021.iloc[:1].append([gcp_2021.iloc[:1], azure_2021.iloc[:1], others_2021])\nto_drop = ['None', 'Other']\ncommercial_2021 = commercial_2021[~commercial_2021['big data'].isin(to_drop)]\ncommercial_2021 = commercial_2021[['big data', 'Counts']]\n\nopen_source_2021 = open_source_2021.merge(commercial_2021, on = 'big data', how = 'outer')\nopen_source_2021 = open_source_2021.rename(columns = {'Counts_x' : 'open source_2021', 'Counts_y' : 'commercial_2021'})\n\nopen_source_2021 = open_source_2021.set_index('big data').T\n\n# --- 2020 Clean up. Merge into AWS, GCP, and AZURE // Open source and Commercial ---\n\ndf_count_bigdata_2020['big data'] = df_count_bigdata_2020['big data'].str.strip()\ndf_count_bigdata_2020['open source'] = df_count_bigdata_2020['big data'].apply(open_source)\ndf_count_bigdata_2020['aws'] = df_count_bigdata_2020['big data'].apply(aws)\ndf_count_bigdata_2020['gcp'] = df_count_bigdata_2020['big data'].apply(gcp)\ndf_count_bigdata_2020['azure'] = df_count_bigdata_2020['big data'].apply(azure)\n\nopen_source_2020 = df_count_bigdata_2020[df_count_bigdata_2020['open source'] == 1]\nopen_source_2020 = open_source_2020[['big data', 'Counts']]\n\naws_2020 = df_count_bigdata_2020[df_count_bigdata_2020['aws'] == 1]\ngcp_2020 = df_count_bigdata_2020[df_count_bigdata_2020['gcp'] == 1]\nazure_2020 = df_count_bigdata_2020[df_count_bigdata_2020['azure'] == 1]\nothers_2020 = df_count_bigdata_2020[(df_count_bigdata_2020['aws'] != 1) & \n                                    (df_count_bigdata_2020['gcp'] != 1) & \n                                    (df_count_bigdata_2020['azure'] != 1) &\n                                    (df_count_bigdata_2020['open source'] != 1)]\n\naws_2020['big data'].iloc[0] = 'aws'\ngcp_2020['big data'].iloc[0] = 'gcp'\nazure_2020['big data'].iloc[0] = 'azure'\n\ncommercial_2020 = aws_2020.iloc[:1].append([gcp_2020.iloc[:1], azure_2020.iloc[:1], others_2020])\nto_drop = ['None', 'Other']\ncommercial_2020 = commercial_2020[~commercial_2020['big data'].isin(to_drop)]\ncommercial_2020 = commercial_2020[['big data', 'Counts']]\n\nopen_source_2020 = open_source_2020.merge(commercial_2020, on = 'big data', how = 'outer')\nopen_source_2020 = open_source_2020.rename(columns = {'Counts_x' : 'open source_2020', 'Counts_y' : 'commercial_2020'})\n\nopen_source_2020 = open_source_2020.set_index('big data').T\n\n# --- 2019 Clean up. Merge into AWS, GCP, and AZURE // Open source and Commercial ---\n\ndf_count_bigdata_2019['big data'] = df_count_bigdata_2019['big data'].str.strip()\ndf_count_bigdata_2019['open source'] = df_count_bigdata_2019['big data'].apply(open_source)\ndf_count_bigdata_2019['aws'] = df_count_bigdata_2019['big data'].apply(aws)\ndf_count_bigdata_2019['gcp'] = df_count_bigdata_2019['big data'].apply(gcp)\ndf_count_bigdata_2019['azure'] = df_count_bigdata_2019['big data'].apply(azure)\n\nopen_source_2019 = df_count_bigdata_2019[df_count_bigdata_2019['open source'] == 1]\nopen_source_2019 = open_source_2019[['big data', 'Counts']]\n\naws_2019 = df_count_bigdata_2019[df_count_bigdata_2019['aws'] == 1]\ngcp_2019 = df_count_bigdata_2019[df_count_bigdata_2019['gcp'] == 1]\nazure_2019 = df_count_bigdata_2019[df_count_bigdata_2019['azure'] == 1]\nothers_2019 = df_count_bigdata_2019[(df_count_bigdata_2019['aws'] != 1) & \n                                    (df_count_bigdata_2019['gcp'] != 1) & \n                                    (df_count_bigdata_2019['azure'] != 1) &\n                                    (df_count_bigdata_2019['open source'] != 1)]\n\naws_2019['big data'].iloc[0] = 'aws'\ngcp_2019['big data'].iloc[0] = 'gcp'\nazure_2019['big data'].iloc[0] = 'azure'\n\ncommercial_2019 = aws_2019.iloc[:1].append([gcp_2019.iloc[:1], azure_2019.iloc[:1], others_2019])\nto_drop = ['None', 'Other']\ncommercial_2019 = commercial_2019[~commercial_2019['big data'].isin(to_drop)]\ncommercial_2019 = commercial_2019[['big data', 'Counts']]\n\nopen_source_2019 = open_source_2019.merge(commercial_2019, on = 'big data', how = 'outer')\nopen_source_2019 = open_source_2019.rename(columns = {'Counts_x' : 'open source_2019', 'Counts_y' : 'commercial_2019'})\n\nopen_source_2019 = open_source_2019.set_index('big data').T\n\nbig_data_usage = pd.concat([open_source_2021, open_source_2020, open_source_2019])\nbig_data_usage['PostgresSQL'] = big_data_usage['PostgresSQL'].fillna(big_data_usage['PostgreSQL'])\nbig_data_usage = big_data_usage.drop(['PostgreSQL'], axis = 1)\n\n# --- Clean && Combine into Open / Commercial yearly ---\n\nbig_data_usage[\"total\"] = big_data_usage.sum(axis=1)\ndummy_big_data = big_data_usage['total']\ndummy_big_data_2021 = dummy_big_data[:2]\ndummy_big_data_2020 = dummy_big_data[2:4]\ndummy_big_data_2019 = dummy_big_data[4:6]\n\nopen_commercial_2021 = pd.DataFrame(dummy_big_data_2021)\nopen_commercial_2021 = open_commercial_2021.rename(index = ({'open source_2021': 'open source', 'commercial_2021': 'commercial'}), columns=({'total': '2021'}))\n\nopen_commercial_2020 = pd.DataFrame(dummy_big_data_2020)\nopen_commercial_2020 = open_commercial_2020.rename(index = ({'open source_2020': 'open source', 'commercial_2020': 'commercial'}), columns=({'total': '2020'}))\n\nopen_commercial_2019 = pd.DataFrame(dummy_big_data_2019)\nopen_commercial_2019 = open_commercial_2019.rename(index = ({'open source_2019': 'open source', 'commercial_2019': 'commercial'}), columns=({'total': '2019'}))\n\nopen_commercial_2021['2020'] = open_commercial_2020\nopen_commercial_2021['2019'] = open_commercial_2019\nopen_commercial_2021 = open_commercial_2021.iloc[:, ::-1]\nopen_commercial_2021 = open_commercial_2021.T\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\nopen_commercial_2021.plot(kind = 'bar',\n                          stacked = True,\n                          ax=ax,\n                         color = ['#1f77b4', '#ff7f0e'])\n\nax.set_title(\"Big data product usage\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.5), fontsize=20)\n\nplt.savefig('1.9 Big data product usage.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:35.536294Z","iopub.execute_input":"2021-11-28T19:40:35.536756Z","iopub.status.idle":"2021-11-28T19:40:36.107912Z","shell.execute_reply.started":"2021-11-28T19:40:35.536713Z","shell.execute_reply":"2021-11-28T19:40:36.107095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Big data Product market shares. Open vs. Commercial\n\nbig_data_usage.drop(['total'], axis = 1, inplace = True)\nbig_data_usage = big_data_usage.T\n\nbig_data_usage_open_source = big_data_usage[['open source_2019', 'open source_2020', 'open source_2021']]\nbig_data_usage_open_source.dropna(axis = 0, how = 'all', inplace = True)\n\nbig_data_usage_commercial = big_data_usage[['commercial_2019', 'commercial_2020', 'commercial_2021']]\nbig_data_usage_commercial = big_data_usage_commercial.rename({\n    'aws': 'AWS',\n    'gcp': 'GCP',\n    'azure': 'AZURE',\n    'Oracle Database': 'Oracle',\n    'Snowflake': 'Snowflake',\n    'IBM Db2': 'IBM'\n})\nbig_data_usage_commercial.dropna(axis = 0, how = 'all', inplace = True)\n\n# --- Plot 3 pie charts twice for Open and Commerical ---\n\nfig, ax = plt.subplots(figsize = (17,10))\n\nbig_data_usage_open_source.plot.pie(subplots = True, \n                                    ax=ax, \n                                    legend = False,\n                                    xlabel = \"\", \n                                    ylabel =\"\", \n                                    autopct='%0.2f%%',\n                                   colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\nplt.savefig('2.0 Open source Big data product usage share.png', bbox_inches='tight')\n\nfig, ax = plt.subplots(figsize = (17,10))\n\nbig_data_usage_commercial.plot.pie(subplots = True, \n                                   ax=ax, legend = False, \n                                   xlabel = \"\", \n                                   ylabel =\"\", \n                                   autopct='%0.2f%%',\n                                  colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\nplt.savefig('2.1 Commercial Big data product usage share.png', bbox_inches='tight')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:36.109394Z","iopub.execute_input":"2021-11-28T19:40:36.109651Z","iopub.status.idle":"2021-11-28T19:40:37.465251Z","shell.execute_reply.started":"2021-11-28T19:40:36.109623Z","shell.execute_reply":"2021-11-28T19:40:37.46444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n<Strong>Exhibit 3.1: Tableau, the king of Business Intelligence</Strong>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- Business Intelligence Tools 2021 Q34 ---\n\nviz_2021 = ['Q34_A_Part_1','Q34_A_Part_2',\n                    'Q34_A_Part_3','Q34_A_Part_4',\n                    'Q34_A_Part_5','Q34_A_Part_6',\n                    'Q34_A_Part_7','Q34_A_Part_8',\n                    'Q34_A_Part_9','Q34_A_Part_10',\n                    'Q34_A_Part_11','Q34_A_Part_12',\n                    'Q34_A_Part_13','Q34_A_Part_14',\n                    'Q34_A_Part_15','Q34_A_Part_16','Q34_A_OTHER']\ndf_viz_2021 = pd.Series(pros_2021[viz_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_viz_2021 = pd.DataFrame(df_viz_2021)\ndf_count_viz_2021 = df_count_viz_2021.reset_index()\ndf_count_viz_2021.columns = ['BI tools', '2021']\n\ndf_count_viz_2021 = df_count_viz_2021[1:] # Drop None\n\n# --- Business Intelligence Tools 2020 Q31 ---\n\nviz_2020 = ['Q31_A_Part_1','Q31_A_Part_2',\n              'Q31_A_Part_3','Q31_A_Part_4',\n              'Q31_A_Part_5','Q31_A_Part_6',\n              'Q31_A_Part_7','Q31_A_Part_8',\n              'Q31_A_Part_9','Q31_A_Part_10',\n              'Q31_A_Part_11','Q31_A_Part_12',\n              'Q31_A_Part_13','Q31_A_Part_14','Q31_A_OTHER']\ndf_viz_2020 = pd.Series(pros_2020[viz_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_viz_2020 = pd.DataFrame(df_viz_2020)\ndf_count_viz_2020 = df_count_viz_2020.reset_index()\ndf_count_viz_2020.columns = ['BI tools', '2020']\n\ndf_count_viz_2020 = df_count_viz_2020[1:] # Drop None\n\nmerged_viz = df_count_viz_2021.set_index('BI tools').combine_first(df_count_viz_2020.set_index('BI tools'))\nmerged_viz = merged_viz.sort_values(by=['2021'], ascending = False)\n\n# --- Merged ---\n\nmerged_viz['2021'].iloc[0] = merged_viz['2021'].iloc[0] + merged_viz['2021'].iloc[6] # Tableau + Tableau CRM\nmerged_viz['2020'].iloc[4] = merged_viz['2020'].iloc[4] + merged_viz['2020'].iloc[16] # Salesforce + Einstein Analytics\nmerged_viz['2021'].iloc[1] = merged_viz['2021'].iloc[1] + merged_viz['2021'].iloc[7]\n\nmerged_viz = merged_viz.drop(merged_viz.index[[6,7,16]])[:-1]\nmerged_viz = merged_viz.T\n\n# --- plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nmerged_viz.plot(kind = 'bar',\n                ax=ax,\n                color = ('#1f77b4', '#ff7f0e', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#17becf', '#7f7f7f','#7f7f7f', '#7f7f7f'))\n\nax.set_title(\"Commerical Business Intelligence tool usage by professionals in kaggle survey\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.savefig('3.1 Commerical Business Intelligence tool usage by professionals in kaggle survey.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:37.46636Z","iopub.execute_input":"2021-11-28T19:40:37.466562Z","iopub.status.idle":"2021-11-28T19:40:38.15311Z","shell.execute_reply.started":"2021-11-28T19:40:37.466537Z","shell.execute_reply":"2021-11-28T19:40:38.152325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Pct Change ---\nmerged_viz_perc = merged_viz.pct_change(periods = 1)[1:]\n\n# pct_change\ndata_bi_growth = pd.DataFrame()\ndata_bi_growth['total'] = merged_viz.sum(axis=1)\ndata_bi_growth = data_bi_growth.pct_change(periods = 1)\n\navg_growth_merged_2021 = round(data_bi_growth.iloc[1].mean(),2) # avg growth rate\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nmerged_viz_perc.plot(kind = 'bar',\n                ax=ax,\n                color = ('#1f77b4', '#ff7f0e', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#17becf', '#7f7f7f','#7f7f7f', '#7f7f7f'))\n\nax.set_title(\"Commerical Business Intelligence growth rate compare to previous year\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.axhline(y=avg_growth_merged_2021, xmin=0.2, xmax= 0.7, color='black', linestyle='dotted', linewidth=5)\n\nax.text(-0.495, 0.52, 'Average growth rate ' +str(avg_growth_merged_2021*100) + '%')\n\nplt.savefig('3.2 Commerical Business Intelligence growth rate compare to previous year.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:38.154443Z","iopub.execute_input":"2021-11-28T19:40:38.154768Z","iopub.status.idle":"2021-11-28T19:40:38.826465Z","shell.execute_reply.started":"2021-11-28T19:40:38.154736Z","shell.execute_reply":"2021-11-28T19:40:38.825878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n<Strong>Exhibit 3.2: Business Intelligence: Pay or Free?</Strong>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# --- Python vizualization library ---\n\n# --- Q14 2020 ---\n\nviz_bi_2020 =['Q14_Part_1','Q14_Part_2',\n              'Q14_Part_3','Q14_Part_4',\n              'Q14_Part_5','Q14_Part_6',\n              'Q14_Part_7','Q14_Part_8',\n              'Q14_Part_9','Q14_Part_10',\n              'Q14_Part_11','Q14_OTHER',\n              'Q31_A_Part_1','Q31_A_Part_2',\n              'Q31_A_Part_3','Q31_A_Part_4',\n              'Q31_A_Part_5','Q31_A_Part_6',\n              'Q31_A_Part_7','Q31_A_Part_8',\n              'Q31_A_Part_9','Q31_A_Part_10',\n              'Q31_A_Part_11','Q31_A_Part_12',\n              'Q31_A_Part_13','Q31_A_Part_14','Q31_A_OTHER']\n\nlib_BI_2020 = pros_2020[viz_bi_2020].rename(columns ={\n    'Q14_Part_1': 'Matplotlib',\n    'Q14_Part_2': 'Seaborn',\n    'Q14_Part_3': 'Plotly',\n    'Q14_Part_4': 'Ggplot',\n    'Q14_Part_5': 'Shiny',\n    'Q14_Part_6': 'D3 js',\n    'Q14_Part_7': 'Altair',\n    'Q14_Part_8': 'Bokeh',\n    'Q14_Part_9': 'Geoplotlib',\n    'Q14_Part_10': 'Leaflet',\n    'Q14_Part_11': 'None',\n    'Q14_OTHER': 'Other',\n    \n    'Q31_A_Part_1': 'Amazon QuickSight',\n    'Q31_A_Part_2': 'MS Power BI',\n    'Q31_A_Part_3': 'Google Data Studio',\n    'Q31_A_Part_4': 'Looker',\n    'Q31_A_Part_5': 'Tableau',\n    'Q31_A_Part_6': 'Salesforce',\n    'Q31_A_Part_7': 'Einstein Analytics',\n    'Q31_A_Part_8': 'Qlik',\n    'Q31_A_Part_9': 'Domo',\n    'Q31_A_Part_10': 'TIBCO',\n    'Q31_A_Part_11': 'Alteryx',\n    'Q31_A_Part_12': 'Sisense',\n    'Q31_A_Part_13': 'SAP',\n    'Q31_A_Part_14': 'None',\n    'Q31_A_OTHER': 'Other'\n})\n\nlib_2020 = ['Matplotlib',\n    'Seaborn',\n    'Plotly',\n    'Ggplot']\n\nBI_2020 = ['Amazon QuickSight',\n    'MS Power BI',\n    'Google Data Studio',\n    'Looker',\n    'Tableau',\n    'Salesforce',\n    'Einstein Analytics',\n    'Qlik',\n    'Domo',\n    'TIBCO',\n    'Alteryx',\n    'Sisense',\n    'SAP'\n    ]\n# ----- library growth -----\n\ndata_viz_li_2019 = ['Q20_Part_1',\n                    'Q20_Part_2',\n                    'Q20_Part_3',\n                    'Q20_Part_4',\n                    'Q20_Part_5',\n                    'Q20_Part_6',\n                    'Q20_Part_7',\n                    'Q20_Part_8',\n                    'Q20_Part_9',\n                    'Q20_Part_10',\n                    'Q20_Part_11',\n                    'Q20_Part_12']\n\ndf_data_viz_li_2019 = pros_2019[data_viz_li_2019]\n\n\ncount_data_viz_li_2019 = pd.Series(df_data_viz_li_2019[data_viz_li_2019].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_data_viz_li_2019 = pd.DataFrame(count_data_viz_li_2019)\ndf_count_data_viz_li_2019 = df_count_data_viz_li_2019.reset_index()\ndf_count_data_viz_li_2019.columns = ['lib', 'Counts']\ndf_count_data_viz_li_2019\n\n\ndata_viz_li_2020 =['Q14_Part_1','Q14_Part_2',\n              'Q14_Part_3','Q14_Part_4',\n              'Q14_Part_5','Q14_Part_6',\n              'Q14_Part_7','Q14_Part_8',\n              'Q14_Part_9','Q14_Part_10',\n              'Q14_Part_11','Q14_OTHER']\n\ndf_data_viz_li_2020 = pros_2020[data_viz_li_2020]\n\n\ncount_data_viz_li_2020 = pd.Series(df_data_viz_li_2020[data_viz_li_2020].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_data_viz_li_2020 = pd.DataFrame(count_data_viz_li_2020)\ndf_count_data_viz_li_2020 = df_count_data_viz_li_2020.reset_index()\ndf_count_data_viz_li_2020.columns = ['lib', 'Counts']\ndf_count_data_viz_li_2020\n\nviz_bi_2021 = ['Q14_Part_1','Q14_Part_2',\n                         'Q14_Part_3','Q14_Part_4',\n                         'Q14_Part_5','Q14_Part_6',\n                         'Q14_Part_7','Q14_Part_8',\n                         'Q14_Part_9','Q14_Part_10',\n                         'Q14_Part_11','Q14_OTHER']\n\ndf_data_viz_li_2021 = pros_2021[viz_bi_2021]\n\n\ncount_data_viz_li_2021 = pd.Series(df_data_viz_li_2021[viz_bi_2021].squeeze().values.ravel()).value_counts()\n#count_bigdata_2020\n\ndf_count_data_viz_li_2021 = pd.DataFrame(count_data_viz_li_2021)\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.reset_index()\ndf_count_data_viz_li_2021.columns = ['lib', 'Counts']\n\n# ---------------\n\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.merge(df_count_data_viz_li_2020, on = 'lib', how = 'outer')\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.merge(df_count_data_viz_li_2019, on = 'lib', how = 'outer')\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.rename(columns = {'Counts_x' : '2021 ', 'Counts_y' : '2020', 'Counts': '2019'})\ndf_count_data_viz_li_2021.at[8,'lib'] = 'D3.js'\ndf_count_data_viz_li_2021.at[8,'2019'] = df_count_data_viz_li_2021.at[12,'2019']\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021[:-1]\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.set_index('lib').T\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021[::-1]\ndf_count_data_viz_li_2021 = df_count_data_viz_li_2021.drop(columns = ['None'])\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize=(10,10))\n\ndf_count_data_viz_li_2021.plot(kind='bar', \n                               ax = ax,\n                              color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n                              )\n\nax.set_title(\"Visualization Library usage by professionals in kaggle survey\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.8), fontsize=20)\n\nplt.savefig('3.3 Visualization Library usage by professionals in kaggle survey.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:38.827938Z","iopub.execute_input":"2021-11-28T19:40:38.82833Z","iopub.status.idle":"2021-11-28T19:40:39.527528Z","shell.execute_reply.started":"2021-11-28T19:40:38.828282Z","shell.execute_reply":"2021-11-28T19:40:39.526652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Library growth ---\n\ndf_count_data_viz_li_2021_perc = df_count_data_viz_li_2021.pct_change(periods = 1)[1:]\n\n# pct_change\ndata_viz_growth = pd.DataFrame()\ndata_viz_growth['total'] = df_count_data_viz_li_2021.sum(axis=1)\ndata_viz_growth_perc = data_viz_growth.pct_change(periods = 1)\n\navg_growth_2020 = round(data_viz_growth_perc.iloc[1].mean(),2)\navg_growth_2021 = round(data_viz_growth_perc.iloc[2].mean(),2)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize=(10,10))\n\ndf_count_data_viz_li_2021_perc.plot(kind = 'bar', \n                                    ax=ax,\n                                   color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"Visualization library usage growth rate compare to previous year\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.6, 0.8), fontsize=20)\n\nplt.axhline(y=0, xmin=-1, xmax= 2, color='red', linestyle='dotted', linewidth=5)\nplt.axhline(y=avg_growth_2020, xmin=0.05, xmax= 0.45, color='black', linestyle='dotted', linewidth=5)\nplt.axhline(y=avg_growth_2021, xmin=0.55, xmax= 0.95, color='black', linestyle='dotted', linewidth=5)\n\nplt.rc('font', size=16)\n\nax.text(0.26, 0.08, 'Avg growth rate ' +str(avg_growth_2020*100) + '%')\nax.text(0.25, 0.35, 'Avg growth rate '+ str(avg_growth_2021*100) + '%')\n\nplt.savefig('3.4 Visualization library usage growth rate compare to previous year.png', bbox_inches='tight')\nplt.show()\nplt.rcParams['font.size'] = 20","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:39.528745Z","iopub.execute_input":"2021-11-28T19:40:39.528976Z","iopub.status.idle":"2021-11-28T19:40:40.266312Z","shell.execute_reply.started":"2021-11-28T19:40:39.528947Z","shell.execute_reply":"2021-11-28T19:40:40.265557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 4.1: Managed Machine Learning, The battle ground</strong>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# --- Machine learning tools ---\n# --- Google cloud Speect-to-Text, Google Cloud Natural Language, Google Cloud Vision, Google Cloud Translation are assumed under Google Vertex AI ---\n\n# --- 2019 Q32 ML tools ---\n\nml_product_2019 = ['Q32_Part_1','Q32_Part_2','Q32_Part_3','Q32_Part_4','Q32_Part_5','Q32_Part_6','Q32_Part_7','Q32_Part_8','Q32_Part_9','Q32_Part_10','Q32_Part_11','Q32_Part_12']\ndf_ml_product_2019 = pros_2019[ml_product_2019]\ncount_ml_product_2019 = pd.Series(df_ml_product_2019[ml_product_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_ml_2019 = pd.DataFrame(count_ml_product_2019)\ndf_count_ml_2019 = df_count_ml_2019.reset_index()\ndf_count_ml_2019.columns = ['ML engine', 'Counts']\n\n# --- 2020 Q28 ML tools ---\n\nml_product_2020 = ['Q28_A_Part_1','Q28_A_Part_2','Q28_A_Part_3','Q28_A_Part_4','Q28_A_Part_5','Q28_A_Part_6','Q28_A_Part_7','Q28_A_Part_8','Q28_A_Part_9','Q28_A_Part_10']\ndf_ml_product_2020 = pros_2020[ml_product_2020]\ncount_ml_product_2020 = pd.Series(df_ml_product_2020[ml_product_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_ml_2020 = pd.DataFrame(count_ml_product_2020)\ndf_count_ml_2020 = df_count_ml_2020.reset_index()\ndf_count_ml_2020.columns = ['ML engine', 'Counts']\n\n# --- 2021 Q31 ML tools ---\n\nml_product_2021 = ['Q31_A_Part_1','Q31_A_Part_2','Q31_A_Part_3','Q31_A_Part_4','Q31_A_Part_5','Q31_A_Part_6','Q31_A_Part_7','Q31_A_Part_8','Q31_A_Part_9','Q31_A_OTHER']\ndf_ml_product_2021 = pros_2021[ml_product_2021]\ncount_ml_product_2021 = pd.Series(df_ml_product_2021[ml_product_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_ml_2021 = pd.DataFrame(count_ml_product_2021)\ndf_count_ml_2021 = df_count_ml_2021.reset_index()\ndf_count_ml_2021.columns = ['ML engine', 'Counts']\n\n# --- Products are merged into one big category ---\n\n# ex1) google vision, google NLP - > Google Cloud Vertex AI\n# ex2) Amazon forecast, recognition - > Amazon SageMaker\n\ndf_count_ml_2019 = df_count_ml_2019.drop(df_count_ml_2019.index[[0,5,7,8,9,11]])\n\ndf_count_ml_2020 = df_count_ml_2020.drop(df_count_ml_2020.index[[0,4,5,6,7,8,9]])\n\ndf_count_ml_2021 = df_count_ml_2021.drop(df_count_ml_2021.index[[0, 7]])\n\nengine_df = df_count_ml_2021.merge(df_count_ml_2020, on = 'ML engine', how = 'outer').merge(df_count_ml_2019, how = 'outer')\nengine_df = engine_df.rename(columns = {'Counts_x' : '2021', 'Counts_y' : '2020', 'Counts' : '2019'})\nengine_df['ML engine'] = engine_df['ML engine'].str.strip()\nengine_df.at[8, 'ML engine'] = 'Google Cloud Vertex AI'\nengine_df.at[9, 'ML engine'] = 'Google Cloud Vertex AI'\nengine_df = engine_df.groupby(['ML engine']).sum()\nengine_df = engine_df.sort_values(by=['2021'], ascending = False).reset_index()\n\nengine_df = engine_df.set_index('ML engine').T[::-1]\ncolumns_clean = ['Amazon SageMaker','Azure Machine Learning Studio','Databricks','Google Cloud Vertex AI','DataRobot','Rapidminer','Alteryx','Dataiku']\nengine_df = engine_df[columns_clean]\n\ndummy_df = engine_df[['Amazon SageMaker','Azure Machine Learning Studio','Google Cloud Vertex AI']]\ndummy_df['multiplier'] = None\ndummy_df['multiplier'] = dummy_df.sum(axis=1).pct_change(periods = 1)\ngrowth_multiplier_2020 = dummy_df['multiplier'].iloc[1]\ngrowth_multiplier_2021 = dummy_df['multiplier'].iloc[2]\n\nfor col in ['Databricks', 'DataRobot', 'Rapidminer', 'Alteryx', 'Dataiku']:\n    engine_df[col].iloc[1] = engine_df[col].iloc[2]/(1+growth_multiplier_2021)\n    engine_df[col].iloc[0] = engine_df[col].iloc[1]/(1+growth_multiplier_2020)\n\nengine_df = engine_df.round(0)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nengine_df.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f'])\n\nax.set_title(\"Managed ML product usage on regular basis\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.8, 0.6), fontsize=20)\nax.text(2, 740, 'Google pushed back')\nax.text(1, 740, 'Google led')\n\nplt.annotate('* The number of Databricks, Datarobot, RapidMiner, Alteryx and Dataiku in 2019 and 2020 were derived from the yearly avg industry growth', \n             (0,0), \n             (-50,-95), \n             fontsize=8, \n             xycoords='axes fraction', \n             textcoords='offset points', va='top')\n\n\nplt.annotate(\"* Google vertex AI is launched in 2021. Google vertex AI in 2019 and 2020 is the sum of google's listed products. This applies to AWS and Azure too\", \n             (0,0), \n             (-50,-105), \n             fontsize=8, \n             xycoords='axes fraction', \n             textcoords='offset points', va='top')\n\nplt.savefig('4.1 Managed ML product usage on regular basis.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:40.267978Z","iopub.execute_input":"2021-11-28T19:40:40.268221Z","iopub.status.idle":"2021-11-28T19:40:41.032748Z","shell.execute_reply.started":"2021-11-28T19:40:40.268194Z","shell.execute_reply":"2021-11-28T19:40:41.032111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"managed_ml_2021_2_year = ['Q31_B_Part_1','Q31_B_Part_2',\n                          'Q31_B_Part_3','Q31_B_Part_4',\n                          'Q31_B_Part_5','Q31_B_Part_6',\n                          'Q31_B_Part_7','Q31_B_Part_8',\n                          'Q31_B_Part_9','Q31_B_OTHER']\n\ndf_managed_ml_2021_2_year = pros_2021[managed_ml_2021_2_year]\n\ncount_managed_ml_2021_2_year = pd.Series(df_managed_ml_2021_2_year[managed_ml_2021_2_year].squeeze().values.ravel()).value_counts()\n\ndf_count_managed_ml_2021_2_year = pd.DataFrame(count_managed_ml_2021_2_year)\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year.reset_index()\ndf_count_managed_ml_2021_2_year.columns = ['Managed ML', '2021']\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year.set_index('Managed ML').T\ndf_count_managed_ml_2021_2_year.columns = df_count_managed_ml_2021_2_year.columns.str.strip()\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year[['Amazon SageMaker','Azure Machine Learning Studio',\n                                 'Google Cloud Vertex AI','Databricks', 'DataRobot', 'Rapidminer', 'Alteryx', 'Dataiku']]\ndf_count_managed_ml_2021_2_year = df_count_managed_ml_2021_2_year.T.sort_values(by= '2021', ascending = False).T\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndf_count_managed_ml_2021_2_year.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f', '#7f7f7f'])\n\nax.set_title(\"Willingness to become more familiar in the next 2 years\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.8, 0.6), fontsize=20)\nax.text(-0.42, 2350, 'Google is back to 1st, Azure just behind')\n\nplt.savefig('4.2 Willingness to become more familiar in the next 2 years.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:41.033717Z","iopub.execute_input":"2021-11-28T19:40:41.034581Z","iopub.status.idle":"2021-11-28T19:40:41.57821Z","shell.execute_reply.started":"2021-11-28T19:40:41.034541Z","shell.execute_reply":"2021-11-28T19:40:41.577419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 4.2: AutoML: Google already dominant, but big opportunity for the rest</strong>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# --- AutoML ---\n\n# --- 2021 Q21 AutoML---\n\nautoML_2021 = ['Q37_A_Part_1','Q37_A_Part_2','Q37_A_Part_3','Q37_A_Part_4','Q37_A_Part_5','Q37_A_Part_6','Q37_A_Part_7','Q37_A_OTHER']\ndf_autoML_2021 = pros_2021[autoML_2021]\ncount_autoML_2021 = pd.Series(df_autoML_2021[autoML_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_autoML_2021 = pd.DataFrame(count_autoML_2021)\ndf_count_autoML_2021 = df_count_autoML_2021.reset_index()\ndf_count_autoML_2021.columns = ['AutoML', 'Counts']\n\n# --- 2020 Q34 AutoML---\n\nautoML_2020 = ['Q34_A_Part_1','Q34_A_Part_2','Q34_A_Part_3','Q34_A_Part_4','Q34_A_Part_5','Q34_A_Part_6','Q34_A_Part_7','Q34_A_Part_8','Q34_A_Part_9','Q34_A_Part_10','Q34_A_Part_11','Q34_A_OTHER']\ndf_autoML_2020 = pros_2020[autoML_2020]\ncount_autoML_2020 = pd.Series(df_autoML_2020[autoML_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_autoML_2020 = pd.DataFrame(count_autoML_2020)\ndf_count_autoML_2020 = df_count_autoML_2020.reset_index()\ndf_count_autoML_2020.columns = ['AutoML', 'Counts']\n\n# --- 2019 Q33 AutoML---\n\nautoML_2019 = ['Q33_Part_1','Q33_Part_2','Q33_Part_3','Q33_Part_4','Q33_Part_5','Q33_Part_6','Q33_Part_7','Q33_Part_8','Q33_Part_9','Q33_Part_10','Q33_Part_11','Q33_Part_12']\ndf_autoML_2019 = pros_2019[autoML_2019]\ncount_autoML_2019 = pd.Series(df_autoML_2019[autoML_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_autoML_2019 = pd.DataFrame(count_autoML_2019)\ndf_count_autoML_2019 = df_count_autoML_2019.reset_index()\ndf_count_autoML_2019.columns = ['AutoML', 'Counts']\n\n# --- AutoML merged ---\n\nautoML_df = df_count_autoML_2021.merge(df_count_autoML_2020, on = 'AutoML', how = 'outer').merge(df_count_autoML_2019, how = 'outer')\nautoML_df = autoML_df.rename(columns = {'Counts_x' : '2021', 'Counts_y' : '2020', 'Counts' : '2019'})\nautoML_df = autoML_df.drop(autoML_df.index[[0,7,8,9,10,12,13,14,15]])\nautoML_df.at[16, 'AutoML'] = 'Google Cloud AutoML'\nautoML_df.at[11, 'AutoML'] = 'H2O Driverless AI'\nautoML_df['AutoML'] = autoML_df['AutoML'].str.strip()\nautoML_df = autoML_df.groupby(['AutoML']).sum()\nautoML_df = autoML_df.sort_values(by=['2021'], ascending = False)\n\nautoML_df = autoML_df.T\nautoML_df = autoML_df[::-1]\n\ndummy_df = autoML_df[['DataRobot AutoML','Databricks AutoML','Google Cloud AutoML','H2O Driverless AI']]\ndummy_df['total'] = dummy_df.sum(axis=1)\ndummy_df['total'] = dummy_df['total'].pct_change(periods=1)\nautoML_growth_2020 = dummy_df['total'].iloc[1]\nautoML_growth_2021 = dummy_df['total'].iloc[2]\n\nsage_azure = ['Amazon Sagemaker Autopilot', 'Azure Automated Machine Learning']\n\nfor col in sage_azure:\n    autoML_df[col].iloc[1] = autoML_df[col].iloc[2]/(1+autoML_growth_2021)\n    autoML_df[col].iloc[0] = autoML_df[col].iloc[1]/(1+autoML_growth_2020)\n    \nautoML_df = autoML_df.round(0)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nautoML_df.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"AutoML usage from 2019 to 2021\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.8, 0.6), fontsize=20)\n\nplt.annotate('* Numbers in Amazon Sagemaker Autopilot and Azure Automated Machine Learning in 2019 and 2020 were derived from the yearly avg industry growth', \n             (0,0), \n             (-50,-95), \n             fontsize=8, \n             xycoords='axes fraction', \n             textcoords='offset points', va='top')\n\nplt.savefig('5.1 AutoML usage from 2019 to 2021.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:41.579481Z","iopub.execute_input":"2021-11-28T19:40:41.579685Z","iopub.status.idle":"2021-11-28T19:40:42.250894Z","shell.execute_reply.started":"2021-11-28T19:40:41.579661Z","shell.execute_reply":"2021-11-28T19:40:42.250123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- AutoML future Preference ---\n\n# --- 2021 Q31 AutoML ---\n\nmanaged_ml_2021_2_year = ['Q31_B_Part_1','Q31_B_Part_2',\n                          'Q31_B_Part_3','Q31_B_Part_4',\n                          'Q31_B_Part_5','Q31_B_Part_6',\n                          'Q31_B_Part_7','Q31_B_Part_8',\n                          'Q31_B_Part_9','Q31_B_OTHER']\n\nautoml_in_2 = ['Q37_B_Part_1',\n               'Q37_B_Part_2',\n               'Q37_B_Part_3',\n               'Q37_B_Part_4',\n               'Q37_B_Part_5',\n               'Q37_B_Part_6',\n               'Q37_B_Part_7',\n               'Q37_B_OTHER']\n\ndf_automl_in_2 = pros_2021[automl_in_2]\n\ncount_df_automl_in_2 = pd.Series(df_automl_in_2[automl_in_2].squeeze().values.ravel()).value_counts()\n\ndf_count_df_automl_in_2 = pd.DataFrame(count_df_automl_in_2)\ndf_count_df_automl_in_2 = df_count_df_automl_in_2.reset_index()\ndf_count_df_automl_in_2.columns = ['Auto ML', '2021']\ndf_count_df_automl_in_2 = df_count_df_automl_in_2.set_index('Auto ML').T\ndf_count_df_automl_in_2.columns = df_count_df_automl_in_2.columns.str.strip()\ndf_count_df_automl_in_2 = df_count_df_automl_in_2[['Google Cloud AutoML','Azure Automated Machine Learning', 'Amazon Sagemaker Autopilot', 'Databricks AutoML', 'DataRobot AutoML', 'H2O Driverless AI']]\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndf_count_df_automl_in_2.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"AutoML: Willingness to become more familiar in the next 2 years\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.8, 0.6), fontsize=20)\n\nplt.savefig('5.2 AutoML: Willingness to become more familiar in the next 2 years.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:42.252614Z","iopub.execute_input":"2021-11-28T19:40:42.252927Z","iopub.status.idle":"2021-11-28T19:40:42.759173Z","shell.execute_reply.started":"2021-11-28T19:40:42.252889Z","shell.execute_reply":"2021-11-28T19:40:42.758329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 5.1: Google's TPU, the Kryptonite of NVIDIA's GPU</strong>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# --- Special hardware usage ---\n\n# 2021 Q12 Special hardware usage \n\nhardware_2021 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_OTHER']\nhardware_2021_df = pros_2021[hardware_2021]\ncount_hardware_2021 = pd.Series(hardware_2021_df[hardware_2021].squeeze().values.ravel()).value_counts()\n\n# 2020 Q12 Special hardware usage \n\nhardware_2020 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_OTHER']\nhardware_2020_df = pros_2020[hardware_2020]\ncount_hardware_2020 = pd.Series(hardware_2020_df[hardware_2020].squeeze().values.ravel()).value_counts()\n\n# 2021 Q19 Special hardware usage \n\nhardware_2019 = ['Q21_Part_1','Q21_Part_2','Q21_Part_3','Q21_Part_4','Q21_Part_5','Q21_OTHER_TEXT']\nhardware_2019_df = pros_2019[hardware_2019]\ncount_hardware_2019 = pd.Series(hardware_2019_df[hardware_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_hardware_2021 = pd.DataFrame(count_hardware_2021)\ndf_count_hardware_2021 = df_count_hardware_2021.reset_index()\ndf_count_hardware_2021.columns = ['manage', '2021']\n\ndf_count_hardware_2020 = pd.DataFrame(count_hardware_2020)\ndf_count_hardware_2020 = df_count_hardware_2020.reset_index()\ndf_count_hardware_2020.columns = ['manage', '2020']\n\ndf_count_hardware_2019 = pd.DataFrame(count_hardware_2019)\ndf_count_hardware_2019 = df_count_hardware_2019.reset_index()\ndf_count_hardware_2019.columns = ['manage', '2019']\n\ndf_count_hardware_2019 = df_count_hardware_2019.iloc[1:6]\ndf_count_hardware_2019 = df_count_hardware_2019.set_index('manage').T[['GPUs', 'TPUs','Other']]\ndf_count_hardware_2020 = df_count_hardware_2020.set_index('manage').T[['GPUs', 'TPUs', 'Other']]\n\ndf_count_hardware_2021 = df_count_hardware_2021.set_index('manage').T\ndf_count_hardware_2021.columns = df_count_hardware_2021.columns.str.strip()\ndf_count_hardware_2021 = df_count_hardware_2021[['NVIDIA GPUs','Google Cloud TPUs','Other','AWS Inferentia Chips','AWS Trainium Chips']]\ndf_count_hardware_2021 = df_count_hardware_2021.rename(columns = {'NVIDIA GPUs': 'GPUs',\n                                                                    'Google Cloud TPUs': 'TPUs',\n                                                                    'Other': 'Other',\n                                                                    'AWS Inferentia Chips': 'AWS Inferentia',\n                                                                    'AWS Trainium Chips': 'AWS Trainium'})\n\n# Merge all three\n\ndf_hardware_merged = df_count_hardware_2019.append(df_count_hardware_2020).append(df_count_hardware_2021).fillna(0)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ndf_hardware_merged.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"Special hardware usage trend\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.5, 0.6), fontsize=20)\nax.annotate(\"AWS chips added in 2021\", xy=(2.1, 1000), xytext=(2.3, 1500),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\n\nplt.savefig('6.1 Special hardware usage trend.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:42.760397Z","iopub.execute_input":"2021-11-28T19:40:42.760641Z","iopub.status.idle":"2021-11-28T19:40:43.372859Z","shell.execute_reply.started":"2021-11-28T19:40:42.760612Z","shell.execute_reply":"2021-11-28T19:40:43.372106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Special hardware // Company Spending ---\n\n# --- Special hardware // Company Spending 2020 Q12 ---\n\nlarge_spender_2020 = pros_2020.loc[pros_2020['Q25'].isin(['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)'])]\nhardware_2020 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_OTHER']\n\nspender_hardware_2020 = pd.DataFrame()\n\nmoney_array = ['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)']\n\nfor idx, comp_size in enumerate(money_array):\n    spender_comp_size = large_spender_2020.loc[large_spender_2020['Q25'] == comp_size]\n    idx = pd.Series(spender_comp_size[hardware_2020].squeeze().values.ravel()).value_counts()\n    spender_hardware_2020[comp_size] = idx\n    \nspender_hardware_2020 = spender_hardware_2020.rename(columns = {'$100,000 or more ($USD)': '100,000 +'})\nspender_hardware_2020 = spender_hardware_2020.T[['GPUs','TPUs', 'Other']]\n\n# --- Special hardware // Company Spending 2021 Q26 ---\n\nlarge_spender_2021 = pros_2021.loc[pros_2021['Q26'].isin(['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)'])]\n\nhardware_2021 = ['Q12_Part_1','Q12_Part_2','Q12_Part_3','Q12_Part_4','Q12_Part_5','Q12_OTHER']\n\nspender_hardware_2021 = pd.DataFrame()\n\nmoney_lst = ['$1-$99', '$100-$999', '$1000-$9,999', '$10,000-$99,999', '$100,000 or more ($USD)']\n\nfor idx, comp_size in enumerate(money_lst):\n    spender_comp_size = large_spender_2021.loc[large_spender_2021['Q26'] == comp_size]\n    idx = pd.Series(spender_comp_size[hardware_2021].squeeze().values.ravel()).value_counts()\n    spender_hardware_2021[comp_size] = idx\n    \nspender_hardware_2021 = spender_hardware_2021.rename(columns = {'$100,000 or more ($USD)': '100,000 +'})\nspender_hardware_2021 = spender_hardware_2021[1:].T\n\n# --- Plot (1) ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nspender_hardware_2020.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"Special hardware usage by spending size in 2020\", fontsize ='large', pad=20)\nax.set_xlabel('Money spent', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.3, 0.6), fontsize=20)\n\nplt.savefig('6.2 Special hardware usage trend.png', bbox_inches='tight')\nplt.show()\n\n# --- Plot (2) ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nspender_hardware_2021.plot(kind = 'bar',\n                ax=ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"Special hardware usage by spending size in 2021\", fontsize ='large', pad=20)\nax.set_xlabel('Money spent', fontsize='medium', labelpad=20)\nax.set_ylabel('Number of respondents', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.6, 0.6), fontsize=20)\n\nplt.savefig('6.3 Special hardware usage trend.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:43.374186Z","iopub.execute_input":"2021-11-28T19:40:43.374483Z","iopub.status.idle":"2021-11-28T19:40:45.032408Z","shell.execute_reply.started":"2021-11-28T19:40:43.374446Z","shell.execute_reply":"2021-11-28T19:40:45.031416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 5.2: NVIDIA, Money Blackhole</strong>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- NVIDIA Earnings ---\n\nnvda_earnings = pd.read_excel('../input/stonks-data/nvda_earnings.xlsx')\nnvda_earnings = nvda_earnings.rename(columns = {'Unnamed: 0': 'Revenue'})\n\ndf = nvda_earnings.T\ndf.columns = df.iloc[0]\ndf = df.drop(df.index[0])\ndf.drop('Total', axis = 1, inplace = True)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10, 10))\n\ndf.plot(kind = 'bar',\n        stacked=True, \n        ax = ax,\n       color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"NVIDIA Earnings\", fontsize ='large', pad=20)\nax.set_xlabel('Adj Fiscal Year', fontsize='medium', labelpad=20)\nax.set_ylabel('in Million USD', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.65, 0.6), fontsize=20)\n\nplt.annotate('',\nha = 'center', va = 'bottom',\nxytext = (7.8, 1800),\nxy = (12.3, 3800),\narrowprops = { 'facecolor' : 'red', 'shrink' : 0.05 })\n\nplt.annotate('Data Center Revenue Explosion Begins',\n        fontsize = 14,\n        ha = 'center', va = 'bottom',\n        xytext = (8, 5000),\n        xy = (8, 3200),\n        arrowprops = { 'facecolor' : 'black', 'shrink' : 0.05 })\n\n\n#plt.annotate('Data Center revenue growth 100% YoY in 2020', xytext = (3, 6000), xy = (8, 6000), fontsize = 22, color = 'red', )\nplt.axhline(y=3200, linestyle='dashed', linewidth=2, color = 'black') \nplt.annotate('Source: Nvidia', (0,0), (-80,-95), fontsize=8, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.rc('font', size=11) \nfor c in (ax.containers):\n    ax.bar_label(c, label_type='center')\n    \nplt.rcParams['font.size'] = 20\n\nplt.savefig('7.1 NVIDIA Earnings.png', bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:45.033728Z","iopub.execute_input":"2021-11-28T19:40:45.034038Z","iopub.status.idle":"2021-11-28T19:40:47.022158Z","shell.execute_reply.started":"2021-11-28T19:40:45.034006Z","shell.execute_reply":"2021-11-28T19:40:47.021361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Estimated Public Cloud's Spending on GPU infrastructure ---\n\n# Current market share: AWS = 31%, Azure = 22%, and GCP = 8%\n# Jensen Huang said 51% of data center revenue was coming from public cloud\n\ndata_center_segment = nvda_earnings.iloc[[2]]\ndata_center_segment = data_center_segment.set_index(['Revenue'])\ndata_center_aws =  data_center_segment.copy() * 0.31\ndata_center_aws = data_center_aws.rename(index = {'data center': 'AWS'})\n\ndata_center_azure = data_center_segment.copy() * 0.22\ndata_center_azure = data_center_azure.rename(index = {'data center': 'Azure'})\n\ndata_center_gcp = data_center_segment.copy() * 0.08\ndata_center_gcp = data_center_gcp.rename(index = {'data center': 'GCP'})\n\ndata_center_others = data_center_segment.copy() * 0.39\ndata_center_others = data_center_others.rename(index = {'data center': 'Other Enterprises'})\n\ndf_data_center_others = pd.concat([data_center_aws, data_center_azure, data_center_gcp, data_center_others])\n\ndf_data_center_others.iloc[2, 0] = 0\ndf_data_center_others.iloc[2, 1] = 0\ndf_data_center_others.iloc[2, 2] = 0 \ndf_data_center_others = df_data_center_others.T\ndf_data_center_others = df_data_center_others[['AWS', 'Azure', 'GCP']]*0.51\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10, 10))\n\ndf_data_center_others.plot(kind = 'bar',\n        ax = ax,\n       color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"Estimated Nvidia's revenue from the big 3\", fontsize ='large', pad=20)\nax.set_xlabel('Adj Fiscal Year', fontsize='medium', labelpad=20)\nax.set_ylabel('in Million USD', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.3, 0.6), fontsize=20)\n\nplt.axvline(x=2.5, linestyle='dashed', linewidth=2, color = 'black') \nplt.annotate('Google disclosed GCP as of 2019', xytext = (2.6, 600), xy = (8, 600), fontsize = 14, color = 'black')\n\nplt.annotate('Source: NVIDIA, https://www.parkmycloud.com/', (0,0), (-80,-120), fontsize=8, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.savefig(\"7.2 Estimated Nvidia's revenue from the big 3.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:47.023288Z","iopub.execute_input":"2021-11-28T19:40:47.023508Z","iopub.status.idle":"2021-11-28T19:40:47.784684Z","shell.execute_reply.started":"2021-11-28T19:40:47.023483Z","shell.execute_reply":"2021-11-28T19:40:47.783941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nvda_stocks = pd.read_excel('../input/nvda-market-cap/nvda_marketcap.xlsx')\nnvda_stocks = nvda_stocks.head(1000)\nnvda_stocks = nvda_stocks.rename(columns = {'NVDA.O (Fundamental)': 'Market Cap'})\nnvda_stocks = nvda_stocks.iloc[1:, :]\nnvda_stocks = nvda_stocks.drop('NVDA.O', axis = 1)\nnvda_stocks = nvda_stocks.set_index('Date')\nnvda_stocks = nvda_stocks.div(1000000000)\n\nfig, ax = plt.subplots(figsize=(18,6))\nnvda_stocks.plot(ax=ax,\n                    color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"NVDA Market cap\", fontsize ='large', pad=20)\nax.set_xlabel('Date', fontsize='medium', labelpad=20)\nax.set_ylabel('Market cap (Billions)', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.2, 0.6), fontsize=20)\n\nplt.annotate('Gaming, Crypto and Cloud Beneficiary', (0,0), (500,150), fontsize=15, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.annotate('Accelerated AI competition', (0,0), (800,220), fontsize=15, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.annotate('META', (0,0), (950,280), fontsize=15, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.annotate('Source: Refinitiv', (0,0), (-80,-80), fontsize=8, \n             xycoords='axes fraction', textcoords='offset points', va='top')\n\nplt.savefig(\"NVDIA_market_cap.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:47.7858Z","iopub.execute_input":"2021-11-28T19:40:47.78628Z","iopub.status.idle":"2021-11-28T19:40:48.584439Z","shell.execute_reply.started":"2021-11-28T19:40:47.786244Z","shell.execute_reply":"2021-11-28T19:40:48.583588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 6.1: Digitization everywhere</strong>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- Data Science professional distribution ---\n\n# Get data from 2021\n\nindustry_2021 = data_2021[data_2021['Q20'].notna()]\nc = industry_2021['Q20'].value_counts(normalize=True).rename_axis('industry').reset_index(name='counts')\n#c =industry_2021['Q20'].value_counts().rename_axis('industry').reset_index(name='counts')\n\n# Get data from 2018\n\nindustry_2018 = data_2018[data_2018['Q7'] != 'I am a student']\nindustry_2018 = industry_2018[industry_2018['Q7'].notna()]\nd = industry_2018['Q7'].value_counts(normalize=True).rename_axis('industry').reset_index(name='counts')\n#d = industry_2018['Q7'].value_counts().rename_axis('industry').reset_index(name='counts')\n\n# compute the industry\n\nk = pd.merge(left = d, right = c, on = 'industry')\nk = k.rename(columns = {'counts_x': '2018', 'counts_y': '2021'})\nk = k.sort_values(by=['2021'], ascending=False)\n\n# compute the difference\ndiff_industry = k.copy()\ndiff_industry['dff'] = k['2021'] - k['2018']\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (20,10))\n\ngs = gridspec.GridSpec(1, 2, wspace=0.2,hspace=0.5, width_ratios=[3,1]) \n\nax1 = plt.subplot(gs[0])\nk.plot.barh(x = \"industry\", \n            ax= ax1,\n           color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\nax1.set(title = \"Data Science professionals distribution by Industry. 2018 vs 2021\",\n      xlabel = \"Percentage (Total sum = 1.0)\",\n      ylabel = \"Industry\")\nax1.invert_yaxis()\n\nax2 = plt.subplot(gs[1])\ndiff_industry['dff'].plot(kind='barh', x = 'industry', ax = ax2,\n                    color=(diff_industry['dff'] > 0).map({True: 'g',\n                                                    False: 'r'}))\nax2.set(title = \"Change\",\n      xlabel = \"Percentage\",\n      ylabel = \"Industry\")\nplt.xticks(rotation=45)\nax2.set_yticks([])\n\nplt.gca().invert_yaxis()\n\nplt.savefig(\"8.1 Data Science professionals distribution by Industry. 2018 vs 2021.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:48.586092Z","iopub.execute_input":"2021-11-28T19:40:48.586513Z","iopub.status.idle":"2021-11-28T19:40:50.036664Z","shell.execute_reply.started":"2021-11-28T19:40:48.586471Z","shell.execute_reply":"2021-11-28T19:40:50.035888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exhibit 1.1 Data Science distribution by company size 2021.\n\ntest = get_professionals(data_2021, 'Q5')\n#print(len(test))\ntest = test[test['Q21'].notna()]\n#print(len(test))\n\ncategory_test = test.groupby(['Q20', 'Q21']).size()\n#category_test.plot(kind='bar')\nnew_df = category_test.to_frame(name = 'size').reset_index()\nnew_df_2= pd.pivot(\n    data = new_df,\n    index = 'Q20',\n    columns = 'Q21',\n    values = 'size',\n)\nnew_df_2.index.names = ['Industry']\nnew_df_2.columns.names = ['Company Size']\n\ncolumns_order = ['0-49 employees', '50-249 employees', '250-999 employees', '1000-9,999 employees','10,000 or more employees']\n\nnew_df_2 = new_df_2.reindex(columns = columns_order)\nnew_df_2['total'] = new_df_2[columns_order].sum(axis = 1)\nnew_df_2 = new_df_2.sort_values(by = 'total', ascending = False)\nnew_df_2 = new_df_2.drop(columns='total')\n\n# -----\n\n\nnew_df_2['total'] = new_df_2[columns_order].sum(axis = 1)\nnew_df_2 = new_df_2.sort_values(by = 'total', ascending = False)\nnew_df_2 = new_df_2.drop(columns = 'total')\nres = new_df_2.div(new_df_2.sum(axis=1), axis = 0)\n\n# --------------------- plot ---------------------\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize = (20,10))\n\ngs = gridspec.GridSpec(1, 2, wspace=0.2,hspace=0.5, width_ratios=[3,1]) \n\nax1 = plt.subplot(gs[0])\nnew_df_2.plot(use_index = True,  \n              kind='barh', \n              stacked=True, \n              ax = ax1,\n              color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n              )\n\nax1.set(title = \"DS professional distibution by company size across different industry 2021\",\n      xlabel = \"the number of respondents\",\n      ylabel = \"Industry\")\nplt.gca().invert_yaxis()\n\nax2 = plt.subplot(gs[1])\nres.plot(use_index = True,  \n              kind='barh', \n              stacked=True, \n              ax = ax2,\n         color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n              )\nax2.set(title='Company Size portion',\n      xlabel = \"Percentage\",\n      ylabel = \" \")\n\n#plt.legend(title = \"Company Size\", bbox_to_anchor=(1.04,1), loc=\"upper left\")\nax2.get_legend().remove()\nax2.set_yticks([])\n\nplt.gca().invert_yaxis()\n\nplt.savefig(\"8.2 DS professional distibution by company size across different industry 2021.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:50.038688Z","iopub.execute_input":"2021-11-28T19:40:50.039277Z","iopub.status.idle":"2021-11-28T19:40:52.397924Z","shell.execute_reply.started":"2021-11-28T19:40:50.039237Z","shell.execute_reply":"2021-11-28T19:40:52.397113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 7.1: Industry is asking less scientists, but more engineers</strong>\n</div>","metadata":{}},{"cell_type":"code","source":"pd.options.mode.chained_assignment = None\n\n# Industry and professionals heatmap\n\n# ----------------\n# Job title filter\n# ----------------\n\njob_title = {'Other':'Other',\n     'Product Manager': 'Product/Project Manager',\n 'Program/Project Manager':'Product/Project Manager',\n 'Principal Investigator':'Product/Project Manager',\n 'Chief Officer':'Product/Project Manager',\n 'Manager':'Product/Project Manager',\n 'Software Developer/Software Engineer': 'Software Engineer',\n 'Operations Research Practitioner': 'Research Scientist',\n 'Computer Scientist': 'Research Scientist',\n 'Scientist/Researcher': 'Research Scientist',\n 'Researcher': 'Research Scientist',\n 'Data Scientist': 'Data Scientist',\n     'Business Analyst': 'Business Analyst',\n     'Engineer': 'Other',\n     'DBA/Database Engineer': 'DBA/Database Engineer',\n     'Data Analyst':'Data Analyst',\n     'Machine Learning Engineer': 'Machine Learning Engineer',\n     'Statistician':'Statistician',\n     'Predictive Modeler':'Research Scientist',\n     'Programmer': 'Software Engineer',\n     'Data Miner': 'Data Engineer',\n     'Consultant': 'Other',\n     'Research Assistant': 'Research Scientist',\n     'Chief Officer':'Product/Project Manager',\n     'Data Engineer':'Data Engineer',\n     'Developer Advocate': 'Developer Relations/Advocacy',\n     'Marketing Analyst': 'Business Analyst',\n     'Data Analyst': 'Data Analyst',\n     'Software Engineer': 'Software Engineer',\n     'Research Scientist': 'Research Scientist',\n     'Data Journalist': 'Data Analyst',\n     'Salesperson':'Developer Relations/Advocacy',\n     'Product/Project Manager': 'Product/Project Manager',\n     'Developer Relations/Advocacy': 'Developer Relations/Advocacy'\n}\n\n# -----------------------------------------\n# Heatmap of job title within industry 2021\n# -----------------------------------------\n\nworkforce_2021 = get_professionals(data_2021, 'Q5')\nprofessional_2021 = workforce_2021[workforce_2021['Q20'].notna()]\nprofessional_2021['Q5'] = professional_2021['Q5'].map(job_title)\nindustry_2021 = professional_2021['Q20'].unique()\ndf_2021 = professional_2021[['Q5','Q20']]\n\ntemp_d = {}\n\nfor industry in industry_2021:\n    temp_df = df_2021[df_2021['Q20'] == industry]\n    temp_dict = dict(temp_df['Q5'].value_counts())\n    temp_d[industry] = temp_dict\n\ndef sorted_simple_dict(d):\n    return {k: v for k, v in sorted(d.items())}\n\ndef sorted_once_nested_dict(d):\n    return {k: sorted_simple_dict(v) for k, v in sorted(d.items())}\n\ntemp_d = sorted_once_nested_dict(temp_d)\n\nd_2021 = {}\n\nh_lst = list(k['industry'])\n\nfor i in h_lst:\n    d_2021[i] = temp_d[i]\n\ndf_industry_2021 = pd.DataFrame.from_dict(d_2021, orient='index')\ndf_industry_2021.fillna(0, inplace = True)\ndf_industry_2021 = df_industry_2021.sort_values(by=df_industry_2021.index[0], ascending=False, axis=1)\ndf_industry_2021\n\n# -----------------------------------------\n# Heatmap of job title within industry 2021\n# -----------------------------------------\n\nstudent_2018 = data_2018.loc[data_2018['Q7'] == 'I am a student']\nworkforce_2018 = data_2018.loc[data_2018['Q7'] != 'I am a student']\n\nprofessional_2018 = workforce_2018[workforce_2018['Q7'].notna()]\nprofessional_2018['Q6'] = professional_2018['Q6'].map(job_title)\n\nindustry_2018 = professional_2018['Q7'].unique()\n\ndf_2018 = professional_2018[['Q6','Q7']]\n\ntemp_d = {}\n\nfor industry in industry_2018:\n    temp_df = df_2018[df_2018['Q7'] == industry]\n    temp_dict = dict(temp_df['Q6'].value_counts())\n    temp_d[industry] = temp_dict\n\ndef sorted_simple_dict(d):\n    return {k: v for k, v in sorted(d.items())}\n\ndef sorted_once_nested_dict(d):\n    return {k: sorted_simple_dict(v) for k, v in sorted(d.items())}\n\ntemp_d = sorted_once_nested_dict(temp_d)\n\nd_2018 = {}\n\nfor i in h_lst:\n    d_2018[i] = temp_d[i]\n\ndf_industry_2018 = pd.DataFrame.from_dict(d_2018, orient='index')\ndf_industry_2018.fillna(0, inplace = True)\ndf_industry_2018 = df_industry_2018.sort_values(by=df_industry_2018.index[0], ascending=False, axis=1)\n\n# --- Plot ---\n\nfig, (ax1, ax2) = plt.subplots(ncols = 2, figsize=(20,10))\n\nax1 = plt.subplot(121)\nplt.title('2018 industry workforce distribution heatmap')\nsns.heatmap(df_industry_2018, annot=True, annot_kws = {\"size\": 8}, linewidth = 0.5, fmt='g', square=True, cmap = 'BuGn')\n\nfig.subplots_adjust(wspace=0.2)\n\nax2 = plt.subplot(122)\nplt.title('2021 industry workforce distribution heatmap')\nsns.heatmap(df_industry_2021, annot=True, annot_kws = {\"size\": 8}, linewidth = 0.5, fmt='g', square=True, cmap = 'BuGn')\n\nplt.axvline(x = 2, ymin= -0.05, ymax= 1.1, color='red', linestyle='solid', linewidth=5)\nplt.axvline(x = 3, ymin= -0.05, ymax= 1.1, color='red', linestyle='solid', linewidth=5)\n\nframe1 = plt.gca()\nframe1.axes.yaxis.set_ticklabels([])\n\nplt.savefig(\"9.1 industry workforce heatmap.png\", bbox_inches='tight')\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:52.399304Z","iopub.execute_input":"2021-11-28T19:40:52.399964Z","iopub.status.idle":"2021-11-28T19:40:58.128923Z","shell.execute_reply.started":"2021-11-28T19:40:52.399928Z","shell.execute_reply":"2021-11-28T19:40:58.128365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Job title and Tasks during the work ---\n\nfrom math import pi\n# activities at work\n\n# Q24_Part_1\tQ24_Part_2\tQ24_Part_3\tQ24_Part_4\tQ24_Part_5\tQ24_Part_6\tQ24_Part_7\tQ24_OTHER\n\nactivities_2021 = ['Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER']\n\nrole_2021 = ['Q5']\n\nrole_activities_2021 = ['Q5','Q24_Part_1','Q24_Part_2','Q24_Part_3','Q24_Part_4','Q24_Part_5','Q24_Part_6','Q24_Part_7','Q24_OTHER']\n\njob_name = ['Data Scientist', 'Machine Learning Engineer',  'Software Engineer', 'Data Analyst', 'Research Scientist']\n\ndf_role_act_2021 = pros_2021[role_activities_2021]\n\ndf_role_act_2021 = df_role_act_2021[df_role_act_2021['Q5'].isin(job_name)]\n\ndf_role_act_2021[activities_2021] = df_role_act_2021[activities_2021].notnull().astype('int')\n\n# --------------\n\ndf_SE = df_role_act_2021[df_role_act_2021['Q5'] == 'Software Engineer']\ndf_SE = df_SE.groupby(by='Q5', dropna=False).sum()\ndf_SE = df_SE.rename(columns = {'Q24_Part_1': 'Analyze data', \n                     'Q24_Part_2': 'Build infrastructure', \n                     'Q24_Part_3': 'Build ML prototypes', \n                     'Q24_Part_4': 'Deploy & Improve',\n                     'Q24_Part_5': 'Improve existing ML',\n                     'Q24_Part_6': 'Do research',\n                     'Q24_Part_7': 'None',\n                     'Q24_OTHER': 'Other'\n                    })\n\ndf_SE = df_SE.T.reset_index()\n\n# --------------\n\ndf_DS = df_role_act_2021[df_role_act_2021['Q5'] == 'Data Scientist']\ndf_DS = df_DS.groupby(by='Q5', dropna=False).sum()\ndf_DS = df_DS.rename(columns = {'Q24_Part_1': 'Analyze data', \n                     'Q24_Part_2': 'Build infrastructure', \n                     'Q24_Part_3': 'Build ML prototypes', \n                     'Q24_Part_4': 'Deploy & Improve',\n                     'Q24_Part_5': 'Improve existing ML',\n                     'Q24_Part_6': 'Do research',\n                     'Q24_Part_7': 'None',\n                     'Q24_OTHER': 'Other'\n                    })\ndf_DS = df_DS.T.reset_index()\n\n# -------------\n\ndf_MLE = df_role_act_2021[df_role_act_2021['Q5'] == 'Machine Learning Engineer']\n\ndf_MLE = df_MLE.groupby(by='Q5', dropna=False).sum()\ndf_MLE = df_MLE.rename(columns = {'Q24_Part_1': 'Analyze data', \n                     'Q24_Part_2': 'Build infrastructure', \n                     'Q24_Part_3': 'Build ML prototypes', \n                     'Q24_Part_4': 'Deploy & Improve',\n                     'Q24_Part_5': 'Improve existing ML',\n                     'Q24_Part_6': 'Do research',\n                     'Q24_Part_7': 'None',\n                     'Q24_OTHER': 'Other'\n                    })\ndf_MLE = df_MLE.T.reset_index()\n\n#----------\n\ndf_MLE['percent'] = (df_MLE['Machine Learning Engineer'] / \n                  df_MLE['Machine Learning Engineer'].sum()) * 100\ndf_MLE = df_MLE.drop('Machine Learning Engineer', axis = 1)\n\ndf_DS['percent'] = (df_DS['Data Scientist'] / \n                  df_DS['Data Scientist'].sum()) * 100\ndf_DS = df_DS.drop('Data Scientist', axis = 1)\n\ndf_SE['percent'] = (df_SE['Software Engineer'] / \n                  df_SE['Software Engineer'].sum()) * 100\ndf_SE = df_SE.drop('Software Engineer', axis = 1)\n\n# -------\n\ntheta = np.arange(len(df_MLE) + 1) / float(len(df_MLE)) * 2 * np.pi\nvalues = df_MLE['percent'].values\nvalues = np.append(values, values[0])\n\nvalues1 = df_DS['percent'].values\nvalues1 = np.append(values1, values1[0])\n\nvalues2 = df_SE['percent'].values\nvalues2 = np.append(values2, values2[0])\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize=(10,10))\nplt.suptitle(\"Tasks in workplace. MLE vs. DS vs. SE\", y = 1)\n\nax = plt.subplot(projection=\"polar\")\n\n# draw the polygon and the mark the points for each angle/value combination\nl1, = ax.plot(theta, values, color=\"C2\", marker=\"o\", label=\"MLE\")\nl2, = ax.plot(theta, values1, color=\"C3\", marker=\"o\", label=\"DS\")\nl3, = ax.plot(theta, values2, color=\"C4\", marker=\"o\", label=\"SE\")\n\nplt.xticks(theta[:-1], df_MLE['index'], color='grey', size=20)\nax.tick_params(pad=40) # to increase the distance of the labels to the plot\n# fill the area of the polygon with green and some transparency\nax.fill(theta, values, 'green', alpha=0.1)\nax.fill(theta, values1, 'red', alpha=0.1)\nax.fill(theta, values2, 'purple', alpha=0.1)\nax.legend(loc = 'lower right',bbox_to_anchor = (1.3,0.1))\n\nplt.savefig(\"9.2 Tasks in workplace. MLE vs. DS vs. SE.png\", bbox_inches='tight')\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:58.132288Z","iopub.execute_input":"2021-11-28T19:40:58.132644Z","iopub.status.idle":"2021-11-28T19:40:59.377102Z","shell.execute_reply.started":"2021-11-28T19:40:58.13261Z","shell.execute_reply":"2021-11-28T19:40:59.376243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 8.1: Current Status of Computer Vision and Natural language processing</strong>\n</div>\n","metadata":{}},{"cell_type":"code","source":"test = get_professionals(data_2021, 'Q5')\n\n# ----------------------------------------------------------------------------------------\n# 2021 (#18 and #19), 2020 (#18 and #19), 2019 (#26 and #27) About Computer vision and NLP\n# ----------------------------------------------------------------------------------------\n\n# ---------------------------------\n# COMPUTER VISION YES OR NO in 2021\n# ---------------------------------\n\nvision = test[['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER']]\nnlp = test[['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER']]\nvision = vision.fillna(0)\nvision[vision != 0] = 1\nvision_df = vision[vision==True].count(axis=0).rename_axis('Algo').reset_index(name='counts')\nvision_df = vision_df.set_index('Algo').T\nstuff_2020 = vision_df\n\nvision['yes'] = vision[['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_OTHER']].sum(axis = 1)\nvision['yes'] = vision['yes'].apply(lambda x: x>=1)\n#vision['yes'].value_counts()\nvision['no'] = (vision['yes'].apply(lambda x: x == 0) | vision['Q18_Part_6'].apply(lambda x: x == 1))\n#vision['no'].value_counts()\n\nvision['Q20'] = test['Q20']\nvision = vision.drop(columns=['Q18_Part_1','Q18_Part_2','Q18_Part_3','Q18_Part_4','Q18_Part_5','Q18_Part_6','Q18_OTHER'])\nvision.replace({False: 0, True: 1}, inplace=True)\nvision_df = vision[vision['yes'] == 1].groupby('Q20').size()\n\ntotal_boss = vision['Q20'].value_counts()\n\nboss = pd.DataFrame(vision_df)\ntotal_boss = pd.DataFrame(total_boss)\nfinal_boss = total_boss.join(boss)\nfinal_boss.rename(columns = {final_boss.columns[0]: 'NO', final_boss.columns[1]: 'YES'}, inplace = True)\nfinal_boss = final_boss[::-1]\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfinal_boss.plot(kind='barh',\n                ax = ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\nax.set(title = \"Computer Vision Yes / No\",\n       xlabel = \"the number of respondents\",\n       ylabel = \"Industry\")\n\nplt.rc('font', size=12)\n\nfor i,j in (zip(ax.containers[0], ax.containers[1])):\n\n    perc = j.get_width() / i.get_width()\n    perc = (perc*100).round(1)\n    non_perc = (100 - perc).round(1)\n    \n    width = i.get_width()\n    height = i.get_height()\n    x, y = i.get_xy()\n    ax.annotate(f'{non_perc}%', (x + width, y + height*1.02), ha=\"left\", va=\"center\")\n    \n    width2 = j.get_width()\n    height2 = j.get_height()\n    x2, y2 = j.get_xy() \n    ax.annotate(f'{perc}%', (x + width2, y2 + height2*1.02), ha=\"left\", va=\"center\")\n    \nplt.rcParams['font.size'] = 20\n\nplt.savefig(\"10.1 Computer Vision.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:40:59.378629Z","iopub.execute_input":"2021-11-28T19:40:59.379014Z","iopub.status.idle":"2021-11-28T19:41:01.383015Z","shell.execute_reply.started":"2021-11-28T19:40:59.378965Z","shell.execute_reply":"2021-11-28T19:41:01.382299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ---------------------\n# NLP YES OR NO in 2021\n# ---------------------\n\nnlp = test[['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER']]\nnlp = nlp.fillna(0)\nnlp[nlp != 0] = 1\nnlp_df = nlp[nlp==True].count(axis=0).rename_axis('Algo').reset_index(name='counts')\nnlp_df = nlp_df.set_index('Algo').T\n\nnlp['yes'] = nlp[['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_OTHER']].sum(axis = 1)\nnlp['yes'] = nlp['yes'].apply(lambda x: x>=1)\n#vision['yes'].value_counts()\nnlp['no'] = (nlp['yes'].apply(lambda x: x == 0) | nlp['Q19_Part_5'].apply(lambda x: x == 1))\n#vision['no'].value_counts()\n\nnlp['Q20'] = test['Q20']\nnlp = nlp.drop(columns=['Q19_Part_1','Q19_Part_2','Q19_Part_3','Q19_Part_4','Q19_Part_5','Q19_OTHER'])\nnlp.replace({False: 0, True: 1}, inplace=True)\nnlp_df = nlp[nlp['yes'] == 1].groupby('Q20').size()\n\ntotal_boss2 = nlp['Q20'].value_counts()\n\nboss2 = pd.DataFrame(nlp_df)\ntotal_boss2 = pd.DataFrame(total_boss2)\nfinal_boss2 = total_boss.join(boss2)\nfinal_boss2.rename(columns = {final_boss2.columns[1]: 'hello'}, inplace = True)\n#final_boss2['perc'] = final_boss2['hello'] * 100 / final_boss2['Q20']\nfinal_boss2.rename(columns = {final_boss2.columns[0]: 'NO', final_boss2.columns[1]: 'YES'}, inplace = True)\nfinal_boss2 = final_boss2[::-1]\n\n# --------------- \n#  SUBPLOTS - 1x3\n# ---------------\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nfinal_boss2.plot(kind='barh',\n                 ax = ax,\n                color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\nax.set(title = \"NLP Yes / No\",\n       xlabel = \"the number of respondents\",\n       ylabel = \"Industry\")\n\nplt.rc('font', size=12)\n\nfor i,j in (zip(ax.containers[0], ax.containers[1])):\n\n    perc = j.get_width() / i.get_width()\n    perc = (perc*100).round(1)\n    non_perc = (100 - perc).round(2)\n    \n    width = i.get_width()\n    height = i.get_height()\n    x, y = i.get_xy()\n    ax.annotate(f'{non_perc}%', (x + width, y + height*1.02), ha=\"left\", va=\"center\")\n    \n    width2 = j.get_width()\n    height2 = j.get_height()\n    x2, y2 = j.get_xy() \n    ax.annotate(f'{perc}%', (x + width2, y2 + height2*1.02), ha=\"left\", va=\"center\")\n\nplt.rcParams['font.size'] = 20\n\nplt.savefig(\"10.2 NLP.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:01.384323Z","iopub.execute_input":"2021-11-28T19:41:01.38455Z","iopub.status.idle":"2021-11-28T19:41:02.915162Z","shell.execute_reply.started":"2021-11-28T19:41:01.384524Z","shell.execute_reply":"2021-11-28T19:41:02.914561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 9.1: Soft Powers</strong>\n</div>","metadata":{}},{"cell_type":"code","source":"ML_framework_2021 = ['Q16_Part_1',\n                     'Q16_Part_2',\n                     'Q16_Part_3',\n                     'Q16_Part_4',\n                     'Q16_Part_5',\n                     'Q16_Part_6',\n                     'Q16_Part_7',\n                     'Q16_Part_8',\n                     'Q16_Part_9',\n                     'Q16_Part_10',\n                     'Q16_Part_11',\n                     'Q16_Part_12',\n                     'Q16_Part_13',\n                     'Q16_Part_14',\n                     'Q16_Part_15',\n                     'Q16_Part_16',\n                     'Q16_Part_17'\n                     ,'Q16_OTHER']\n\ndf_ML_framework_2021 = pros_2021[ML_framework_2021]\ncount_ML_framework_2021 = pd.Series(df_ML_framework_2021[ML_framework_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_ML_framework_2021 = pd.DataFrame(count_ML_framework_2021)\ndf_count_ML_framework_2021 = df_count_ML_framework_2021.reset_index()\ndf_count_ML_framework_2021.columns = ['ML Frameworks', 'Counts']\n\ndf_count_ML_framework_2021 = df_count_ML_framework_2021.set_index('ML Frameworks').T\ndf_count_ML_framework_2021.columns = df_count_ML_framework_2021.columns.str.strip()\n\nML_framework_2020 = ['Q16_Part_1',\n                     'Q16_Part_2',\n                     'Q16_Part_3',\n                     'Q16_Part_4',\n                     'Q16_Part_5',\n                     'Q16_Part_6',\n                     'Q16_Part_7',\n                     'Q16_Part_8',\n                     'Q16_Part_9',\n                     'Q16_Part_10',\n                     'Q16_Part_11',\n                     'Q16_Part_12',\n                     'Q16_Part_13',\n                     'Q16_Part_14',\n                     'Q16_Part_15',\n                     'Q16_OTHER']\n\ndf_ML_framework_2020 = pros_2020[ML_framework_2020]\ncount_ML_framework_2020 = pd.Series(df_ML_framework_2020[ML_framework_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_ML_framework_2020 = pd.DataFrame(count_ML_framework_2020)\ndf_count_ML_framework_2020 = df_count_ML_framework_2020.reset_index()\ndf_count_ML_framework_2020.columns = ['ML Frameworks', 'Counts']\n\ndf_count_ML_framework_2020 = df_count_ML_framework_2020.set_index('ML Frameworks').T\ndf_count_ML_framework_2020.columns = df_count_ML_framework_2020.columns.str.strip()\n\nML_framework_2019 = ['Q28_Part_1',\n                     'Q28_Part_2',\n                     'Q28_Part_3',\n                     'Q28_Part_4',\n                     'Q28_Part_5',\n                     'Q28_Part_6',\n                     'Q28_Part_7',\n                     'Q28_Part_8',\n                     'Q28_Part_9',\n                     'Q28_Part_10',\n                     'Q28_Part_11',\n                     'Q28_Part_12',\n                     'Q28_OTHER_TEXT']\n\ndf_ML_framework_2019 = pros_2019[ML_framework_2019]\ncount_ML_framework_2019 = pd.Series(df_ML_framework_2019[ML_framework_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_ML_framework_2019 = pd.DataFrame(count_ML_framework_2019)\ndf_count_ML_framework_2019 = df_count_ML_framework_2019.reset_index()\ndf_count_ML_framework_2019.columns = ['ML Frameworks', 'Counts']\n\n# Some clean up\ndf_count_ML_framework_2019 = df_count_ML_framework_2019.set_index('ML Frameworks').T\ndf_count_ML_framework_2019.columns = df_count_ML_framework_2019.columns.str.strip()\ndf_count_ML_framework_2019 = df_count_ML_framework_2019[['Scikit-learn', 'Keras', 'TensorFlow', 'RandomForest', 'Xgboost', 'PyTorch', 'LightGBM', 'None', 'Caret', 'Spark MLib',\n                           'Fast.ai', 'Other']]\n\nmerged_ML_framcework = pd.concat([df_count_ML_framework_2019, df_count_ML_framework_2020, df_count_ML_framework_2021])\nmerged_ML_framcework.index = ['2019','2020','2021']\nmerged_ML_framcework = merged_ML_framcework.fillna(0)\nmerged_ML_framcework['total'] = merged_ML_framcework.sum(axis=1)\nmerged_ML_framcework[['TensorFlow', 'PyTorch', 'Keras', 'total']]\n\nmerged_ML_framcework['TensorFlow'] = merged_ML_framcework['TensorFlow']/merged_ML_framcework['total']\nmerged_ML_framcework['PyTorch'] = merged_ML_framcework['PyTorch']/merged_ML_framcework['total']\nmerged_ML_framcework['Keras'] = merged_ML_framcework['Keras']/merged_ML_framcework['total']\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nmerged_ML_framcework[['TensorFlow', 'PyTorch','Keras']].plot(kind='bar',\n                                                     ax=ax,\n                                                     stacked = True,\n                                                     color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\nax.set_title(\"ML frameworks usage proportion\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Percentage', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.4, 0.6), fontsize=20)\n\nplt.savefig(\"11.1 Softpower ML frameworks.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:02.916279Z","iopub.execute_input":"2021-11-28T19:41:02.916602Z","iopub.status.idle":"2021-11-28T19:41:03.425888Z","shell.execute_reply.started":"2021-11-28T19:41:02.916576Z","shell.execute_reply":"2021-11-28T19:41:03.425104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hosting_notebook_2021 = ['Q10_Part_1',\n                    'Q10_Part_2',\n                    'Q10_Part_3',\n                    'Q10_Part_4',\n                    'Q10_Part_5',\n                    'Q10_Part_6',\n                    'Q10_Part_7',\n                    'Q10_Part_8',\n                    'Q10_Part_9',\n                    'Q10_Part_10',\n                    'Q10_Part_11',\n                    'Q10_Part_12',\n                    'Q10_Part_13',\n                    'Q10_Part_14',\n                    'Q10_Part_15',\n                    'Q10_Part_16',\n                    'Q10_OTHER'\n                   ]\n\ndf_hosting_notebook_2021 = pros_2021[hosting_notebook_2021]\ncount_hosting_notebook_2021 = pd.Series(df_hosting_notebook_2021[hosting_notebook_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_hosting_notebook_2021 = pd.DataFrame(count_hosting_notebook_2021)\ndf_count_hosting_notebook_2021 = df_count_hosting_notebook_2021.reset_index()\ndf_count_hosting_notebook_2021.columns = ['ML Frameworks', 'Counts']\n\ndf_count_hosting_notebook_2021 = df_count_hosting_notebook_2021.set_index('ML Frameworks').T\ndf_count_hosting_notebook_2021.columns = df_count_hosting_notebook_2021.columns.str.strip()\n\nhosting_notebook_2020 = ['Q10_Part_1',\n                         'Q10_Part_2',\n                         'Q10_Part_3',\n                         'Q10_Part_4',\n                         'Q10_Part_5',\n                         'Q10_Part_6',\n                         'Q10_Part_7',\n                         'Q10_Part_8',\n                         'Q10_Part_9',\n                         'Q10_Part_10',\n                         'Q10_Part_11',\n                         'Q10_Part_12',\n                         'Q10_Part_13',\n                         'Q10_OTHER']\n\ndf_hosting_notebook_2020 = pros_2020[hosting_notebook_2020]\ncount_hosting_notebook_2020 = pd.Series(df_hosting_notebook_2020[hosting_notebook_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_hosting_notebook_2020 = pd.DataFrame(count_hosting_notebook_2020)\ndf_count_hosting_notebook_2020 = df_count_hosting_notebook_2020.reset_index()\ndf_count_hosting_notebook_2020.columns = ['ML Frameworks', 'Counts']\n\ndf_count_hosting_notebook_2020 = df_count_hosting_notebook_2020.set_index('ML Frameworks').T\ndf_count_hosting_notebook_2020.columns = df_count_hosting_notebook_2020.columns.str.strip()\n\nhosting_notebook_2019 =['Q17_Part_1',\n                        'Q17_Part_2',\n                        'Q17_Part_3',\n                        'Q17_Part_4',\n                        'Q17_Part_5',\n                        'Q17_Part_6',\n                        'Q17_Part_7',\n                        'Q17_Part_8',\n                        'Q17_Part_9',\n                        'Q17_Part_10',\n                        'Q17_Part_11',\n                        'Q17_Part_12',\n                        'Q17_OTHER_TEXT']\n\ndf_hosting_notebook_2019 = pros_2019[hosting_notebook_2019]\ncount_hosting_notebook_2019 = pd.Series(df_hosting_notebook_2019[hosting_notebook_2019].squeeze().values.ravel()).value_counts()\n\ndf_count_hosting_notebook_2019 = pd.DataFrame(count_hosting_notebook_2019)\ndf_count_hosting_notebook_2019 = df_count_hosting_notebook_2019.reset_index()\ndf_count_hosting_notebook_2019.columns = ['ML Frameworks', 'Counts']\n\ndf_count_hosting_notebook_2019 = df_count_hosting_notebook_2019.set_index('ML Frameworks').T\ndf_count_hosting_notebook_2019.columns = df_count_hosting_notebook_2019.columns.str.strip()\n\n# --- Clean up for merge ---\n\ndf_count_hosting_notebook_2019 = df_count_hosting_notebook_2019[['Kaggle Notebooks (Kernels)', 'Google Colab', 'Binder / JupyterHub', 'Microsoft Azure Notebooks',\n                               'AWS Notebook Products (EMR Notebooks, Sagemaker Notebooks, etc)',\n                               'IBM Watson Studio',\n                               'Paperspace / Gradient',\n                               'FloydHub',\n                              'Code Ocean']]\n\ndf_count_hosting_notebook_2019['total'] = df_count_hosting_notebook_2019.sum(axis=1)\ndf_count_hosting_notebook_2019[['Kaggle Notebooks (Kernels)', \n                                'Google Colab', \n                                'Binder / JupyterHub', \n                                'AWS Notebook Products (EMR Notebooks, Sagemaker Notebooks, etc)',\n                                'total']]\n\ndf_count_hosting_notebook_2019 = df_count_hosting_notebook_2019.rename(columns = \n                                     {'Kaggle Notebooks (Kernels)': 'Kaggle Notebooks', \n                                'Google Colab': 'Colab Notebooks',\n                                      'Microsoft Azure Notebooks': 'Azure Notebooks',\n                                'Binder / JupyterHub': 'Binder / JupyterHub', \n                                'AWS Notebook Products (EMR Notebooks, Sagemaker Notebooks, etc)': 'Amazon Sagemaker Studio',\n                                'total': 'total'\n                                     })\n\ndf_count_hosting_notebook_2020 = df_count_hosting_notebook_2020.drop('None', axis = 1)\ndf_count_hosting_notebook_2020['total'] = df_count_hosting_notebook_2020.sum(axis = 1)\ndf_count_hosting_notebook_2021 = df_count_hosting_notebook_2021.drop('None', axis = 1)\ndf_count_hosting_notebook_2021['total'] = df_count_hosting_notebook_2021.sum(axis = 1)\n\ndf_count_hosting_notebook_2021 = df_count_hosting_notebook_2021.rename(columns = {\"Amazon Sagemaker Studio Notebooks\": \"Amazon Sagemaker Studio\"})\n\n\n\n#  --- Now merge ---\n\nhosting_notebook_merge = pd.concat([df_count_hosting_notebook_2019, df_count_hosting_notebook_2020, df_count_hosting_notebook_2021])\nhosting_notebook_merge.index = ['2019', '2020', '2021']\nhosting_notebook_merge = hosting_notebook_merge.fillna(0)\ntop_notebooks_used = hosting_notebook_merge[['Kaggle Notebooks', \n                        'Colab Notebooks', \n                        'Binder / JupyterHub', \n                        'Azure Notebooks', \n                        'IBM Watson Studio', \n                        'Amazon Sagemaker Studio', \n                        'total']]\n\ntop_notebooks_used_perc = top_notebooks_used[['Kaggle Notebooks', \n                        'Colab Notebooks', \n                        'Binder / JupyterHub', \n                        'Azure Notebooks', \n                        'IBM Watson Studio', \n                        'Amazon Sagemaker Studio', \n                        'total']].div(top_notebooks_used['total'], axis = 0)\n\n# --- Plot ---\n\nfig, ax = plt.subplots(figsize = (10,10))\n\ntop_notebooks_used_perc[['Kaggle Notebooks', \n                        'Colab Notebooks', \n                        'Binder / JupyterHub', \n                        'Azure Notebooks', \n                        'IBM Watson Studio', \n                        'Amazon Sagemaker Studio']].plot(kind='bar', \n                                                         stacked = True,\n                                                        ax=ax,\n                                                     color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\n\nax.set_title(\"Proportion of hosted notebook products usage\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Percentage', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.7, 0.6), fontsize=20)\n\nplt.savefig(\"11.2 Softpower Notebook.png\", bbox_inches='tight')\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:03.42729Z","iopub.execute_input":"2021-11-28T19:41:03.427555Z","iopub.status.idle":"2021-11-28T19:41:04.013862Z","shell.execute_reply.started":"2021-11-28T19:41:03.427526Z","shell.execute_reply":"2021-11-28T19:41:04.013294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"share_deploy_2021 = [\n    'Q39_Part_1',\n    'Q39_Part_2',\n    'Q39_Part_3',\n    'Q39_Part_4',\n    'Q39_Part_5',\n    'Q39_Part_6',\n    'Q39_Part_7',\n    'Q39_Part_8',\n    'Q39_Part_9',\n    'Q39_OTHER'\n]\n\ndf_share_deploy_2021 = pros_2021[share_deploy_2021]\ncount_share_deploy_2021 = pd.Series(df_share_deploy_2021[share_deploy_2021].squeeze().values.ravel()).value_counts()\n\ndf_count_share_deploy_2021 = pd.DataFrame(count_share_deploy_2021)\ndf_count_share_deploy_2021 = df_count_share_deploy_2021.reset_index()\ndf_count_share_deploy_2021.columns = ['Platforms', 'Counts']\n\ndf_count_share_deploy_2021 = df_count_share_deploy_2021.set_index('Platforms').T\ndf_count_share_deploy_2021.columns = df_count_share_deploy_2021.columns.str.strip()\n\nshare_deploy_2020 =[\n    'Q36_Part_1',\n    'Q36_Part_2',\n    'Q36_Part_3',\n    'Q36_Part_4',\n    'Q36_Part_5',\n    'Q36_Part_6',\n    'Q36_Part_7',\n    'Q36_Part_8',\n    'Q36_Part_9',\n    'Q36_OTHER']\n\ndf_share_deploy_2020 = pros_2020[share_deploy_2020]\ncount_share_deploy_2020 = pd.Series(df_share_deploy_2020[share_deploy_2020].squeeze().values.ravel()).value_counts()\n\ndf_count_share_deploy_2020 = pd.DataFrame(count_share_deploy_2020)\ndf_count_share_deploy_2020 = df_count_share_deploy_2020.reset_index()\ndf_count_share_deploy_2020.columns = ['Platforms', 'Counts']\n\ndf_count_share_deploy_2020 = df_count_share_deploy_2020.set_index('Platforms').T\ndf_count_share_deploy_2020.columns = df_count_share_deploy_2020.columns.str.strip()\n\n# --- Merge ---\nmerged_share_deploy = pd.concat([df_count_share_deploy_2020, df_count_share_deploy_2021])\nmerged_share_deploy.index = ['2020', '2021']\n\nmerged_share_deploy = merged_share_deploy.drop('I do not share my work publicly', axis = 1)\nmerged_share_deploy['total'] = merged_share_deploy.sum(axis = 1)\nmerged_share_deploy = merged_share_deploy.div(merged_share_deploy['total'], axis = 0)\nmerged_share_deploy = merged_share_deploy.drop('total', axis = 1)\n\nfig, ax = plt.subplots(figsize = (10,10))\n\nmerged_share_deploy[['GitHub', 'Kaggle', 'Personal blog', 'Shiny']].plot(kind='bar', \n                                                         stacked = True,\n                                                        ax=ax,\n                                                     color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n\n\nax.set_title(\"Platform to share or deploy ML applications\", fontsize ='large', pad=20)\nax.set_xlabel('Year', fontsize='medium', labelpad=20)\nax.set_ylabel('Percentage', fontsize='medium', labelpad=20)\nax.legend(bbox_to_anchor = (1.45, 0.6), fontsize=20)\n\nplt.savefig(\"11.3 Softpower ML repository.png\", bbox_inches='tight')\nplt.show()\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:04.014938Z","iopub.execute_input":"2021-11-28T19:41:04.015486Z","iopub.status.idle":"2021-11-28T19:41:04.483639Z","shell.execute_reply.started":"2021-11-28T19:41:04.015455Z","shell.execute_reply":"2021-11-28T19:41:04.482845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"font-family:Helvetica Neue; font-size:25px; line-height:1.7; color:black;\">\n  <strong>Exhibit 10.1: Take aways from the big tech</strong>\n</div>","metadata":{}},{"cell_type":"code","source":"import matplotlib as mpl\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n#mpl.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\nmpl.rcParams['font.size']=12                #10 \nmpl.rcParams['savefig.dpi']=100             #72 \nmpl.rcParams['figure.subplot.bottom']=.1 \n\n\nstopwords = set(STOPWORDS)\nf = open(\"../input/big-tech-keynotes/2020_keynote_amzn.txt\", \"r\", encoding='utf-8')\nfile_content = f.read()\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(file_content)\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"amzn_keynote_clouds.png\", dpi=900)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:04.484824Z","iopub.execute_input":"2021-11-28T19:41:04.485144Z","iopub.status.idle":"2021-11-28T19:41:11.907122Z","shell.execute_reply.started":"2021-11-28T19:41:04.485114Z","shell.execute_reply":"2021-11-28T19:41:11.906456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = set(STOPWORDS)\nf = open(\"../input/big-tech-keynotes/2021_google_io.txt\", \"r\", encoding='utf-8')\nfile_content = f.read()\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(file_content)\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"google_io_2021.png\", dpi=900)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:11.908047Z","iopub.execute_input":"2021-11-28T19:41:11.908569Z","iopub.status.idle":"2021-11-28T19:41:18.94779Z","shell.execute_reply.started":"2021-11-28T19:41:11.908535Z","shell.execute_reply":"2021-11-28T19:41:18.947074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = set(STOPWORDS)\nf = open(\"../input/big-tech-keynotes/2021_ignite_2021.txt\", \"r\", encoding='utf-8')\nfile_content = f.read()\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(file_content)\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"msft_ignite_2021.png\", dpi=900)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-28T19:41:18.948923Z","iopub.execute_input":"2021-11-28T19:41:18.949153Z","iopub.status.idle":"2021-11-28T19:41:25.819826Z","shell.execute_reply.started":"2021-11-28T19:41:18.949129Z","shell.execute_reply":"2021-11-28T19:41:25.819211Z"},"trusted":true},"execution_count":null,"outputs":[]}]}