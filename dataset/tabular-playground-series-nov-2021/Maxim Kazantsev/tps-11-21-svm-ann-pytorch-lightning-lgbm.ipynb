{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\nimport gc\nimport torchmetrics\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-29T18:52:38.403405Z","iopub.execute_input":"2021-11-29T18:52:38.40397Z","iopub.status.idle":"2021-11-29T18:52:44.943851Z","shell.execute_reply.started":"2021-11-29T18:52:38.403869Z","shell.execute_reply":"2021-11-29T18:52:44.94215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def seed_torch(seed):\n#     random.seed(seed)\n#     os.environ['PYTHONHASHSEED'] = str(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n\n# seed_torch(42)\n\npl.utilities.seed.seed_everything(seed=42, workers=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:52:44.945671Z","iopub.execute_input":"2021-11-29T18:52:44.945913Z","iopub.status.idle":"2021-11-29T18:52:44.957868Z","shell.execute_reply.started":"2021-11-29T18:52:44.945883Z","shell.execute_reply":"2021-11-29T18:52:44.956903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data import**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\", low_memory=False)#, nrows=10000)\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\", low_memory=False)#, nrows=10000)\ntrain.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:52:44.959193Z","iopub.execute_input":"2021-11-29T18:52:44.959698Z","iopub.status.idle":"2021-11-29T18:53:29.062219Z","shell.execute_reply.started":"2021-11-29T18:52:44.959652Z","shell.execute_reply":"2021-11-29T18:53:29.061187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info(memory_usage=\"deep\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:29.064874Z","iopub.execute_input":"2021-11-29T18:53:29.065251Z","iopub.status.idle":"2021-11-29T18:53:29.082034Z","shell.execute_reply.started":"2021-11-29T18:53:29.065202Z","shell.execute_reply":"2021-11-29T18:53:29.081046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:29.083192Z","iopub.execute_input":"2021-11-29T18:53:29.083545Z","iopub.status.idle":"2021-11-29T18:53:29.087638Z","shell.execute_reply.started":"2021-11-29T18:53:29.083515Z","shell.execute_reply":"2021-11-29T18:53:29.086977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:29.088581Z","iopub.execute_input":"2021-11-29T18:53:29.089195Z","iopub.status.idle":"2021-11-29T18:53:29.155277Z","shell.execute_reply.started":"2021-11-29T18:53:29.089159Z","shell.execute_reply":"2021-11-29T18:53:29.154299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5, 6))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:29.156801Z","iopub.execute_input":"2021-11-29T18:53:29.157572Z","iopub.status.idle":"2021-11-29T18:53:29.307865Z","shell.execute_reply.started":"2021-11-29T18:53:29.157515Z","shell.execute_reply":"2021-11-29T18:53:29.306908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:29.309202Z","iopub.execute_input":"2021-11-29T18:53:29.31001Z","iopub.status.idle":"2021-11-29T18:53:31.988679Z","shell.execute_reply.started":"2021-11-29T18:53:29.309966Z","shell.execute_reply":"2021-11-29T18:53:31.988004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [x for x in train.columns if x[0]==\"f\"]\n\ndf = pd.concat([train[features], test[features]], axis=0)\ndf.reset_index(inplace=True, drop=True)\n\nunique_values = df[features].nunique() < 30\ncat_features = unique_values[unique_values==True].index\nunique_values = df[features].nunique() >= 30\nnum_features = unique_values[unique_values==True].index\n\nprint(f\"There are {len(cat_features)} categorical features: {cat_features}\")\nprint(f\"There are {len(num_features)} continuous features: {num_features}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:31.989744Z","iopub.execute_input":"2021-11-29T18:53:31.990337Z","iopub.status.idle":"2021-11-29T18:53:48.22033Z","shell.execute_reply.started":"2021-11-29T18:53:31.990304Z","shell.execute_reply":"2021-11-29T18:53:48.219308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6, 6))\n\nbars = ax.bar(train[\"target\"].value_counts().index,\n              train[\"target\"].value_counts().values,\n              color=colors,\n              edgecolor=\"black\",\n              width=0.4)\nax.set_title(\"Target values distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.set_xticks(train[\"target\"].value_counts().index)\nax.tick_params(axis=\"both\", labelsize=14)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"target\"].value_counts().values/(len(train)/100)],\n                 padding=5, fontsize=15)\nax.bar_label(bars, [f\"{x:2d}\" for x in train[\"target\"].value_counts().values],\n                 padding=-30, fontsize=15)\nax.margins(0.2, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:48.224344Z","iopub.execute_input":"2021-11-29T18:53:48.224687Z","iopub.status.idle":"2021-11-29T18:53:48.468581Z","shell.execute_reply.started":"2021-11-29T18:53:48.224651Z","shell.execute_reply":"2021-11-29T18:53:48.467671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target value classes are balanced which is good.","metadata":{}},{"cell_type":"code","source":"train.isna().sum().sum(), test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:48.469668Z","iopub.execute_input":"2021-11-29T18:53:48.46988Z","iopub.status.idle":"2021-11-29T18:53:48.693284Z","shell.execute_reply.started":"2021-11-29T18:53:48.469854Z","shell.execute_reply":"2021-11-29T18:53:48.692425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no missing values in the both datasets.\n\nLet's check feature values distribution in the both datasets.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\ncols = 5\nrows = len(columns) // cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,65), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            if i == 0:\n                axs[r, c].legend(fontsize=10)\n                                  \n        i+=1\n#plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:53:48.694706Z","iopub.execute_input":"2021-11-29T18:53:48.694985Z","iopub.status.idle":"2021-11-29T18:54:18.897562Z","shell.execute_reply.started":"2021-11-29T18:53:48.694957Z","shell.execute_reply":"2021-11-29T18:54:18.896836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the datasets are well balanced. So target distribution should probably be the same for test predictions.","metadata":{}},{"cell_type":"code","source":"print(\"Numerical features with the least amount of unique values:\")\ntrain[num_features].nunique().sort_values().head(5)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:54:18.89867Z","iopub.execute_input":"2021-11-29T18:54:18.89906Z","iopub.status.idle":"2021-11-29T18:54:22.609005Z","shell.execute_reply.started":"2021-11-29T18:54:18.899016Z","shell.execute_reply":"2021-11-29T18:54:22.607975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at feature correlation.","metadata":{}},{"cell_type":"code","source":"# Plot dataframe\ndf = train[features].corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:54:22.610164Z","iopub.execute_input":"2021-11-29T18:54:22.61083Z","iopub.status.idle":"2021-11-29T18:54:40.556255Z","shell.execute_reply.started":"2021-11-29T18:54:22.610795Z","shell.execute_reply":"2021-11-29T18:54:40.555199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is very weak linear correlation between the features.","metadata":{}},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"# Scaling all values\ns_scaler = StandardScaler()\nfor col in features:\n    train[col] = s_scaler.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = s_scaler.transform(np.array(test[col]).reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:54:40.558884Z","iopub.execute_input":"2021-11-29T18:54:40.559231Z","iopub.status.idle":"2021-11-29T18:54:42.219473Z","shell.execute_reply.started":"2021-11-29T18:54:40.559185Z","shell.execute_reply":"2021-11-29T18:54:42.218312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[features].copy()\nX_test = test[features].copy()\ny = train[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:54:42.220982Z","iopub.execute_input":"2021-11-29T18:54:42.221234Z","iopub.status.idle":"2021-11-29T18:54:42.855566Z","shell.execute_reply.started":"2021-11-29T18:54:42.221205Z","shell.execute_reply":"2021-11-29T18:54:42.854542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"markdown","source":"## LinearSVC","metadata":{}},{"cell_type":"code","source":"# Fold splitting parameters\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Two zero-filled arrays for out-of-fold and test predictions\nlinear_oof_preds = np.zeros((X.shape[0],))\nlinear_test_preds = np.zeros((X_test.shape[0],))\ntotal_mean_auc = 0\n\n# Generating folds and making training and prediction for each of them\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n\n    linear_model = LinearSVC(tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=42)\n    linear_model.fit(X_train, y_train)\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.    \n    linear_oof_preds[valid_idx] = linear_model.decision_function(X_valid)\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    linear_test_preds += linear_model.decision_function(X_test) / splits\n    \n    # Getting score for a fold model\n    fold_auc = roc_auc_score(y_valid, linear_oof_preds[valid_idx])\n    print(f\"Fold {num} ROC AUC: {fold_auc}\")\n    \n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_auc += fold_auc / splits\n\nprint(f\"\\nOverall ROC AUC: {total_mean_auc}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:54:42.857099Z","iopub.execute_input":"2021-11-29T18:54:42.857365Z","iopub.status.idle":"2021-11-29T18:55:49.147393Z","shell.execute_reply.started":"2021-11-29T18:54:42.857334Z","shell.execute_reply":"2021-11-29T18:55:49.146229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural network","metadata":{}},{"cell_type":"code","source":"X_nn = X.copy()\nX_test_nn = X_test.copy()\n# X_nn[\"linear_preds\"] = linear_oof_preds\n# X_test_nn[\"linear_preds\"] = linear_test_preds\n\n# Scaling all values\nmm_scaler = MinMaxScaler()\nfor col in X_nn.columns:\n    X_nn[col] = mm_scaler.fit_transform(np.array(X_nn[col]).reshape(-1,1))\n    X_test_nn[col] = mm_scaler.transform(np.array(X_test_nn[col]).reshape(-1,1))\n    \n# Transforming test data into tensors\nX_test_nn = torch.tensor(X_test_nn.to_numpy()).float()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:55:49.153784Z","iopub.execute_input":"2021-11-29T18:55:49.157984Z","iopub.status.idle":"2021-11-29T18:55:50.816178Z","shell.execute_reply.started":"2021-11-29T18:55:49.157906Z","shell.execute_reply":"2021-11-29T18:55:50.815263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_datasets(X_nn, X_valid_nn, y_nn, y_valid_nn):\n    # Transforming data into tensors\n    X_nn = torch.tensor(X_nn.to_numpy()).float()\n    y_nn = torch.tensor(y_nn.to_numpy()).int()\n    X_valid_nn = torch.tensor(X_valid_nn.to_numpy()).float()\n    y_valid_nn = torch.tensor(y_valid_nn.to_numpy()).int()\n    \n    print(\"Using these datasets:\")\n    # Transforming tensors into tensor datasets\n    train_ds = TensorDataset(X_nn, y_nn)\n    valid_ds = TensorDataset(X_valid_nn, y_valid_nn)\n#     test_ds = TensorDataset(X_test_nn)\n    print(f\"Train_ds elements: {len(train_ds)}\")\n    print(f\"Valid_ds elements: {len(valid_ds)}\")\n#     print(f\"Test_ds elements: {len(test_ds)}\")\n\n    # Transforming into dataloader objects using batches\n    BATCH_SIZE = 2048\n    train_ds = DataLoader(train_ds, batch_size=BATCH_SIZE)#, drop_last=True)\n    valid_ds = DataLoader(valid_ds, batch_size=BATCH_SIZE)#, drop_last=True)\n#     test_ds = DataLoader(test_ds, batch_size=BATCH_SIZE)\n\n    \n    for data, label in train_ds:\n        print(f\"Train_ds batch: {data.shape}, {label.shape}\")\n        break\n    for data, label in valid_ds:\n        print(f\"Valid_ds batch: {data.shape}, {label.shape}\")\n        break\n#     for data in test_ds:\n#         print(f\"Test_ds batch: {data[0].shape}\")\n#         break\n    return train_ds, valid_ds","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:55:50.817726Z","iopub.execute_input":"2021-11-29T18:55:50.818058Z","iopub.status.idle":"2021-11-29T18:55:50.82919Z","shell.execute_reply.started":"2021-11-29T18:55:50.818003Z","shell.execute_reply":"2021-11-29T18:55:50.827253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Splitting data into train and valid\n# X_nn, X_valid_nn, y_nn, y_valid_nn = train_test_split(X_nn, y, test_size=0.2, random_state=42, stratify=y)\n# X_nn.shape, X_valid_nn.shape, y_nn.shape, y_valid_nn.shape, X_test_nn.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:55:50.830577Z","iopub.execute_input":"2021-11-29T18:55:50.831333Z","iopub.status.idle":"2021-11-29T18:55:50.845913Z","shell.execute_reply.started":"2021-11-29T18:55:50.831289Z","shell.execute_reply":"2021-11-29T18:55:50.844979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computing class weights to be used in training\nclass_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\nclass_weights_dict={}\nfor label in np.sort(y.unique()):\n    class_weights_dict[label] = class_weights[label]\nclass_weights_dict","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:55:50.847142Z","iopub.execute_input":"2021-11-29T18:55:50.847389Z","iopub.status.idle":"2021-11-29T18:55:50.983774Z","shell.execute_reply.started":"2021-11-29T18:55:50.847362Z","shell.execute_reply":"2021-11-29T18:55:50.983051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function to initialize weights using Glorot normal initialization\ndef initialize_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_normal_(m.weight.data)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:43.692385Z","iopub.execute_input":"2021-11-29T19:01:43.693144Z","iopub.status.idle":"2021-11-29T19:01:43.698833Z","shell.execute_reply.started":"2021-11-29T19:01:43.69309Z","shell.execute_reply":"2021-11-29T19:01:43.697846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining model parameters\nclass Model(pl.LightningModule):\n    def __init__(self, input_shape):\n        super().__init__()\n        \n        # Input layer\n        self.input = nn.Linear(input_shape,128)\n        # Hidden layers\n        self.hidden1 = nn.Linear(128,64)\n        self.hidden2 = nn.Linear(64,32)\n#         self.hidden3 = nn.Linear(128,128)\n        # Output layer\n        self.output = nn.Linear(32,1)\n        \n        # Dropout rate\n        self.dr = 0.25\n        # Activation functions\n        self.swish = F.hardswish\n        self.sigmoid = torch.sigmoid\n        # Metrics\n        self.train_roc_auc_metric = torchmetrics.AUROC(pos_label=1)\n        self.val_roc_auc_metric = torchmetrics.AUROC(pos_label=1)\n    \n    def forward(self, x):\n        x = self.swish(self.input(x))\n        x = F.dropout(x,p=self.dr,training=self.training)\n        x = self.swish(self.hidden1(x))\n        x = F.dropout(x,p=self.dr,training=self.training)\n        x = self.swish(self.hidden2(x))\n        x = F.dropout(x,p=self.dr,training=self.training)\n#         x = self.swish(self.hidden3(x))\n#         x = F.dropout(x,p=self.dr,training=self.training)\n        x = self.sigmoid(self.output(x))\n        return x\n    \n    # Training loop with loss and metric computing for each train data batch\n    def training_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self(X).squeeze(1)\n        loss = F.binary_cross_entropy(y_hat, y.float())\n        self.train_roc_auc_metric(y_hat, y)\n        self.log('loss', loss, prog_bar=True, on_epoch=True, logger=True)\n        self.log('rocauc', self.train_roc_auc_metric, prog_bar=True, on_epoch=False, logger=True)\n        return {'loss': loss,}\n\n    # Uses batch loss and metric values to compute and print\n    # overall train data loss and metric \n    def training_epoch_end(self, outputs):\n        train_roc_auc = self.train_roc_auc_metric.compute()\n        self.log('train_roc_auc', train_roc_auc, prog_bar=True, on_epoch=True, on_step=False, logger=True)\n#         print(f\"Epoch {self.current_epoch} train_roc_auc: {train_roc_auc:.4f}\")\n        self.train_roc_auc_metric.reset()\n\n\n    # Computes loss and metric score for each valid data batch\n    def validation_step(self, batch, batch_idx):\n        X, y = batch\n        y_hat = self(X).squeeze(1)\n        self.val_roc_auc_metric(y_hat, y)\n        val_loss = F.binary_cross_entropy(y_hat, y.float())\n        self.log('val_loss',val_loss, prog_bar=True, on_epoch=True, logger=True)\n        return {'val_loss': val_loss}\n\n    # Uses batch loss and metric values to compute and print\n    # overall valid data loss and metric\n    def validation_epoch_end(self, outputs):\n        val_roc_auc = self.val_roc_auc_metric.compute()\n        self.log('val_roc_auc', val_roc_auc, prog_bar=True, on_epoch=True, on_step=False, logger=True)\n#         print(f\"Epoch {self.current_epoch} valid_roc_auc: {val_roc_auc:.4f}\")\n        self.val_roc_auc_metric.reset()\n        return {'val_roc_auc': val_roc_auc}\n\n\n#     def validation_epoch_end(self, outputs):\n#         val_roc_auc = self.roc_auc_metric.aggregate()\n#         self.roc_auc_metric.reset()\n#         self.log('val_roc_auc', val_roc_auc, logger=True, prog_bar=True)\n#         return {'val_roc_auc': val_roc_auc}\n        \n    def predict_step(self, X, batch_idx, dataloader_idx = None):\n        return self(X[0])    \n    \n    # Setting optimizer and learning rate scheduler parameters if any\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3, eps=1e-8, weight_decay=1e-2, amsgrad=False)\n#         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9, last_epoch=-1, verbose=False)\n        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3,\n                                                                  patience=12, min_lr=1e-05, eps=1e-08,\n                                                                  verbose=False)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler, \"monitor\": \"val_loss\"}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:43.945158Z","iopub.execute_input":"2021-11-29T19:01:43.945621Z","iopub.status.idle":"2021-11-29T19:01:43.969483Z","shell.execute_reply.started":"2021-11-29T19:01:43.945569Z","shell.execute_reply":"2021-11-29T19:01:43.968182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A custom callback to log required parameters like metrics and learning rate\n# since TensorBoard is blocked on Kaggle.\nclass ParamsTracker(pl.callbacks.Callback):\n\n    def __init__(self, verbose=True):\n        self.verbose = verbose\n        # Defining empty lists for metric values\n        self.train_loss = []\n        self.train_roc_auc = []\n        self.val_loss = []\n        self.val_roc_auc = []\n        self.lr_epoch_start = []\n    \n    # Getting learning rate from an optimizer params at epoch start\n    def on_train_epoch_start(self, trainer, module):\n        current_learning_rate = trainer.optimizers[0].state_dict()[\"param_groups\"][0][\"lr\"]\n        self.lr_epoch_start.append(current_learning_rate)\n#         print(f\"Epoch start lr {current_learning_rate}\")\n        \n    def on_validation_epoch_end(self, trainer, module):\n        metrics_logs = trainer.logged_metrics\n        self.val_loss.append(metrics_logs[\"val_loss\"].item())\n        self.val_roc_auc.append(metrics_logs[\"val_roc_auc\"].item())\n#         print(f\"Valid loss {metrics_logs['val_loss'].item():.4f}, roc_auc {metrics_logs['val_roc_auc'].item():.4f}\")  \n    \n    # Getting last saved metrics from a trainer object and appending\n    # the required values to the corresponding lists\n    def on_train_epoch_end(self, trainer, module):\n        metrics_logs = trainer.logged_metrics\n        self.train_loss.append(metrics_logs[\"loss_epoch\"].item())\n        self.train_roc_auc.append(metrics_logs[\"train_roc_auc\"].item())\n#         print(f\"Train loss {metrics_logs['loss_epoch'].item():.4f}, roc_auc {metrics_logs['train_roc_auc'].item():.4f}\")\n        \n        # Print all metrics at the end of current epoch if verbose is set to True.\n        # It is done here because on_train_epoch_end event happens after on_validation_epoch_end event.\n        if self.verbose == True:\n            print(f\"Epoch {module.current_epoch} start learning rate: {self.lr_epoch_start[-1]:.6f}, \"\n                  f\"train_loss: {self.train_loss[-1]:.4f}, \"\n                  f\"train_roc_auc: {self.train_roc_auc[-1]:.4f}, \"\n                  f\"val_loss: {self.val_loss[-1]:.4f}, \"\n                  f\"val_roc_auc: {self.val_roc_auc[-1]:.4f}\")  \n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:44.347965Z","iopub.execute_input":"2021-11-29T19:01:44.348452Z","iopub.status.idle":"2021-11-29T19:01:44.360476Z","shell.execute_reply.started":"2021-11-29T19:01:44.348406Z","shell.execute_reply":"2021-11-29T19:01:44.359532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trains the model using given train and valid datasets\n# and returns filepath of the best saved model\ndef train_ann(train_ds, valid_ds, Model=Model, input_shape=X_nn.shape[1]):\n    \n    model = Model(input_shape)\n    # Weights initialization using custom function\n    model.apply(initialize_weights)\n\n    # A callback to save the best model\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        dirpath=\"models\",\n        filename=f'model_' + '{val_loss:.4}',\n        monitor='val_loss',\n        mode='min',\n        save_weights_only=True)\n\n    # A callback to stop training when there is no improvement\n    early_stop_callback = EarlyStopping(\n        monitor='val_loss',\n        min_delta=0.00,\n        patience=50,\n        verbose=False,\n        mode='min'\n    )\n\n    # A callback to check learning rate if it is not constant\n    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n    \n    params_tracker_callback = ParamsTracker(verbose=True)\n\n    # print(ModelSummary(model))\n\n    # Setting training parameters\n    trainer = pl.Trainer(\n        fast_dev_run=False,\n        max_epochs=200,\n    #         gpus=1,\n        precision=32,\n        limit_train_batches=1.0,\n        limit_val_batches=1.0, \n        num_sanity_val_steps=0,\n        check_val_every_n_epoch=1,\n        val_check_interval=1.0, \n        callbacks=[checkpoint_callback, early_stop_callback, params_tracker_callback],\n     )\n\n    # Training \n    trainer.fit(model, train_ds, valid_ds)\n\n    # Switching model to evaluation mode\n    model.eval()\n\n    # Getting path of the best saved model\n    best_model_path = checkpoint_callback.best_model_path\n    \n    return best_model_path, params_tracker_callback","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:44.78632Z","iopub.execute_input":"2021-11-29T19:01:44.786832Z","iopub.status.idle":"2021-11-29T19:01:44.798145Z","shell.execute_reply.started":"2021-11-29T19:01:44.78678Z","shell.execute_reply":"2021-11-29T19:01:44.797177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Fold splitting parameters\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Two zero-filled arrays for out-of-fold and test predictions\nnn_oof_preds = np.zeros((X_nn.shape[0],))\nnn_test_preds = np.zeros((X_test_nn.shape[0],))\ntotal_mean_auc = 0\n\n# Generating folds and making training and prediction for each of them\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X_nn, y)):\n#     if num > 0:\n#         break\n    print(f\"\\n\\n===Training with fold {num}\")\n    X_train, X_valid = X_nn.loc[train_idx], X_nn.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    # Preparing datasets\n    train_ds, valid_ds = prepare_datasets(X_train, X_valid, y_train, y_valid)\n    \n    # Training model\n    best_model_path, tracked_values = train_ann(train_ds, valid_ds, Model, X_nn.shape[1])\n    \n    # Loading weights of the best model\n    model = Model(X_nn.shape[1])\n    model.load_state_dict(torch.load(best_model_path)['state_dict'])\n    model.eval()\n    \n    # Making valid data preds and plotting their histogram\n    preds = model(torch.tensor(X_valid.to_numpy()).float()).detach().numpy().flatten()\n#     display(pd.DataFrame(preds).hist(bins=50))\n    \n    # Calculating and printing this fold's model ROC AUC score\n    fold_score = roc_auc_score(y_valid, preds)\n    print(f\"\\n===Fold {num} valid data ROC AUC score is {fold_score}\")\n    \n    # Making test data preds and plotting their histogram\n    test_preds = model(X_test_nn).detach().numpy().flatten()\n#     display(pd.DataFrame(test_preds).hist(bins=50))\n    \n    # Saving preds in corresponding arrays\n    nn_oof_preds[valid_idx] = preds\n    nn_test_preds = test_preds / splits\n    \n    total_mean_auc += fold_score / splits\n    \nprint(f\"Average ROC AUC score of all models is {total_mean_auc}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:45.915761Z","iopub.execute_input":"2021-11-29T19:01:45.916191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model, train_ds, valid_ds, X_nn, X_test_nn\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:56:08.908922Z","iopub.execute_input":"2021-11-29T18:56:08.909142Z","iopub.status.idle":"2021-11-29T18:56:08.91251Z","shell.execute_reply.started":"2021-11-29T18:56:08.909117Z","shell.execute_reply":"2021-11-29T18:56:08.91183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"markdown","source":"### LightGBM hyperparameters optimization","metadata":{}},{"cell_type":"markdown","source":"Hyperparameters used in this notebook were optimized using Optuna. The code used or that is shown below. They are commented in order to save runtime as optimization has been already done.","metadata":{}},{"cell_type":"code","source":"# def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n#     \"\"\"\n#     A function to train a model using different hyperparamerters combinations provided by Optuna. \n#     Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n#     \"\"\"\n    \n        \n#     #A set of hyperparameters to optimize by optuna\n#     lgbm_params = {\n#                     \"objective\": trial.suggest_categorical(\"objective\", ['binary']),\n#                     \"boosting_type\": trial.suggest_categorical(\"boosting_type\", ['gbdt']),\n#                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n#                     \"max_depth\": trial.suggest_int(\"max_depth\", 1, 6),\n# #                     \"max_depth\": trial.suggest_categorical(\"max_depth\", [8]),\n#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.15, 1, step=0.01),\n#                     \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [40000]),        \n#                     \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 100.0, step=0.1),\n#                     \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.1, 100.0, step=0.1),\n#                     \"random_state\": trial.suggest_categorical(\"random_state\", [42]),\n#                     \"bagging_seed\": trial.suggest_categorical(\"bagging_seed\", [42]),\n#                     \"feature_fraction_seed\": trial.suggest_categorical(\"feature_fraction_seed\", [42]), \n#                     \"n_jobs\": trial.suggest_categorical(\"n_jobs\", [4]), \n#                     \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1, step=0.01),\n#                     \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 7),\n#                     \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1, step=0.01),\n# #                     \"colsample_bytree\": trial.suggest_categorical(\"colsample_bytree\", [1]),\n# #                     \"device_type\": trial.suggest_categorical(\"device_type\", [\"GPU\"]),\n#                     'min_child_samples': trial.suggest_int('min_child_samples', 5, 300),\n#                     'min_child_weight': trial.suggest_int('min_child_weight', 256, 512),\n        \n#                     }\n\n\n\n#     # Model loading and training\n#     model = LGBMClassifier(**lgbm_params)\n#     model.fit(X_train, y_train,\n#               eval_set=[(X_valid, y_valid)],\n#               eval_metric=\"auc\",\n#               early_stopping_rounds=100,\n#               verbose=False)\n    \n#     print(f\"Number of boosting rounds: {model.best_iteration_}\")\n#     oof = model.predict_proba(X_valid)[:, 1]\n    \n#     return roc_auc_score(y_valid, oof)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:56:08.916463Z","iopub.execute_input":"2021-11-29T18:56:08.916948Z","iopub.status.idle":"2021-11-29T18:56:08.927208Z","shell.execute_reply.started":"2021-11-29T18:56:08.916904Z","shell.execute_reply":"2021-11-29T18:56:08.926095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# time_limit = 3600 * 4\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n# #                n_trials = 2\n#                timeout=time_limit\n#               )\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:56:08.928353Z","iopub.execute_input":"2021-11-29T18:56:08.92924Z","iopub.status.idle":"2021-11-29T18:56:08.943647Z","shell.execute_reply.started":"2021-11-29T18:56:08.92919Z","shell.execute_reply":"2021-11-29T18:56:08.942921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LinearSVC and NN predictions will be used as features for LightGBM\nX[\"linear_preds\"] = linear_oof_preds\nX_test[\"linear_preds\"] = linear_test_preds\nX[\"nn_preds\"] = nn_oof_preds\nX_test[\"nn_preds\"] = nn_test_preds","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:56:08.945403Z","iopub.execute_input":"2021-11-29T18:56:08.945902Z","iopub.status.idle":"2021-11-29T18:56:08.965913Z","shell.execute_reply.started":"2021-11-29T18:56:08.945869Z","shell.execute_reply":"2021-11-29T18:56:08.965028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model hyperparameters\nlgbm_params = {'objective': 'binary',\n               'boosting_type': 'gbdt',\n               'num_leaves': 41,\n               'max_depth': 1,\n               'learning_rate': 0.15,\n               'n_estimators': 40000,\n               'reg_alpha': 17.6,\n               'reg_lambda': 74.7,\n               'random_state': 42,\n               'bagging_seed': 42,\n               'feature_fraction_seed': 42,\n               'n_jobs': 4,\n               'subsample': 0.86,\n               'subsample_freq': 4, \n               'colsample_bytree': 0.16, \n               'min_child_samples': 151, \n               'min_child_weight': 361}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:56:08.967275Z","iopub.execute_input":"2021-11-29T18:56:08.968334Z","iopub.status.idle":"2021-11-29T18:56:08.974165Z","shell.execute_reply.started":"2021-11-29T18:56:08.96815Z","shell.execute_reply":"2021-11-29T18:56:08.973518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Setting up fold parameters\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_auc = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = LGBMClassifier(**lgbm_params)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"auc\",\n              early_stopping_rounds=300,\n              )\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    preds += model.predict_proba(X_test)[:, 1] / splits\n    \n    # Getting mean feature importances (i.e. devided by number of splits)\n    model_fi += model.feature_importances_ / splits\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.\n    oof_preds[valid_idx] = model.predict_proba(X_valid)[:, 1]\n    \n    # Getting score for a fold model\n    fold_auc = roc_auc_score(y_valid, oof_preds[valid_idx])\n    print(f\"Fold {num} ROC AUC: {fold_auc}\")\n\n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_auc += fold_auc / splits\n    \nprint(f\"\\nOverall ROC AUC: {total_mean_auc}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:56:08.975322Z","iopub.execute_input":"2021-11-29T18:56:08.975616Z","iopub.status.idle":"2021-11-29T19:01:32.894674Z","shell.execute_reply.started":"2021-11-29T18:56:08.975559Z","shell.execute_reply":"2021-11-29T19:01:32.893635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LightGBM feature importances**","metadata":{}},{"cell_type":"code","source":"# Creating a dataframe to be used for plotting\ndf = pd.DataFrame()\ndf[\"Feature\"] = X.columns\n# Extracting feature importances from the trained model\ndf[\"Importance\"] = model_fi / model_fi.sum()\n# Sorting the dataframe by feature importance\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:32.895978Z","iopub.execute_input":"2021-11-29T19:01:32.896196Z","iopub.status.idle":"2021-11-29T19:01:32.90613Z","shell.execute_reply.started":"2021-11-29T19:01:32.896169Z","shell.execute_reply":"2021-11-29T19:01:32.905202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 35))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n               color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=13)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=13)\nax2.tick_params(axis=\"x\", labelsize=15)\nax.margins(0.05, 0.01)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:32.907376Z","iopub.execute_input":"2021-11-29T19:01:32.907866Z","iopub.status.idle":"2021-11-29T19:01:35.038988Z","shell.execute_reply.started":"2021-11-29T19:01:32.907832Z","shell.execute_reply":"2021-11-29T19:01:35.038049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predictions submission**","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"target\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T19:01:35.040373Z","iopub.execute_input":"2021-11-29T19:01:35.04064Z","iopub.status.idle":"2021-11-29T19:01:36.485416Z","shell.execute_reply.started":"2021-11-29T19:01:35.0406Z","shell.execute_reply":"2021-11-29T19:01:36.484198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}