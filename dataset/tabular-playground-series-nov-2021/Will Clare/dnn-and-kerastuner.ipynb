{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Imports\nHere we import the required packages.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, QuantileTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.backend import sigmoid\nfrom numpy.random import seed","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Preparation\nImport the data and prepare it for use in the models.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train.columns.drop(['id','target'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train[features]\ny = train['target']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = make_column_transformer((StandardScaler(), features))\n#preprocessor = make_column_transformer((MinMaxScaler((0,1)), features))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = preprocessor.fit_transform(X)\nX_test = preprocessor.transform(test[features])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Keras Tuning\nSet up the KerasTuner for a random search. Some notes:\n* We will use EarlyStopping and ReduceLROnPlateau to reduce overfitting, and we will use these same callbacks later when we submit the final model.\n* We will use the Swish activation function in all models. We define it below with the tf.function decorator to make it a tensorflow function.\n* I've included a crude skip to the end parameter calls \"skips\". The makes the labelled layer also connect to the final layer. This hasn't provided much value but could be used further.\n* I've used a constant dropout rate for each layer, differing dropout rates would be better (but take longer to search).\n* I've tuned on only 20% of the total data. This is an attempt to avoid effectively training the parameters to be a good fit for the validation data, and poor on test data. It also reduces the time spent on tuning.","metadata":{}},{"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True,mode='min')\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0,mode='min')\n\n@tf.function\ndef swish(x):\n    return x*sigmoid(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def base_model(hp):\n\n    num_input = keras.Input(shape=(X_test.shape[1],), name='num_data')#input layer\n    \n    l1_size = hp.Int(\"layer_1_size\", min_value=32, max_value=512, step=32)\n    l2_size = hp.Int(\"layer_2_size\", min_value=32, max_value=512, step=32)\n    l3_size = hp.Int(\"layer_3_size\", min_value=32, max_value=512, step=32)\n    l4_size = hp.Int(\"layer_4_size\", min_value=32, max_value=512, step=32)\n    #l5_size = hp.Int(\"layer_5_size\", min_value=32, max_value=512, step=32)\n    do_size = hp.Float(\"dropout_size\",min_value=0.1,max_value=0.5,step=0.01)\n    \n    shape=[]\n    shape.append(l1_size)\n    shape.append(l2_size)\n    shape.append(l3_size)\n    shape.append(l4_size)\n    #shape.append(l5_size)\n    \n    layer_list = []\n    \n    first_layer_size = shape.pop(0)\n    l = layers.Dense(units=first_layer_size, activation=swish)(num_input)\n    l = layers.BatchNormalization()(l)\n    l = layers.Dropout(do_size)(l)\n    layer_list.append(l)\n    \n    # Add one or more hidden layers\n    for s in shape:\n        l = layers.Dense(units=s, activation=swish)(l)\n        l1 = layers.BatchNormalization()(l)\n        l1 = layers.Dropout(do_size)(l)\n        layer_list.append(l)\n    \n    skips = hp.Choice(\"Skips\",values=['','1','2','3','4','12','13','14','23','24','34'])\n    skips = [int(skip) for skip in skips]\n    \n    for skip in skips:\n        l = layers.Concatenate()([l,layer_list[skip]])\n        \n    # A single output: our predicted target value probability\n    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(l)\n    \n    model = keras.Model(\n    inputs = [num_input],\n    outputs = out,\n    )\n    \n    model.compile(\n        #optimizer='adam',\n        optimizer = keras.optimizers.Adam(learning_rate=hp.Float(\"learning_rate\", min_value=0.0001, max_value=0.001, step=0.0001)),\n        loss='binary_crossentropy',\n        metrics=[keras.metrics.AUC()]\n    )\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tuning\nimport keras_tuner\nfrom keras_tuner import RandomSearch, BayesianOptimization\nX_tune,_ , y_tune,_  = train_test_split(X,y,train_size=0.2)\nX_tune_train, X_tune_test, y_tune_train, y_tune_test = train_test_split(X_tune,y_tune)\n\ntuner = RandomSearch(base_model,keras_tuner.Objective(\"val_auc\",direction=\"max\"),max_trials=100, overwrite=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Uncomment the below two cells to carry out the tuning.","metadata":{}},{"cell_type":"code","source":"#tuner.search(X_tune_train,y_tune_train, epochs=100, batch_size=2048, validation_data = (X_tune_test,y_tune_test),callbacks=[early_stopping,plateau])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tuner.results_summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Final model preparation and training\nFrom the above, I found that a model with layers of size 32,328,384,288 with learning rate = 0.0008 and dropout rate 0.38 skipping the first and second layers worked well. Note the unconventional shape, but that's what the tuner found.\n\nWe'll train using a 5-fold cross validation method.","metadata":{}},{"cell_type":"code","source":"seed(42)\ntf.random.set_seed(42)\n\ndef new_model(shape,dropout,skips=[]):\n    '''Shape is the shape of the hidden layers. Dropout is the dropout percentage per layer. Skips is the list of layers which skip to the end'''\n    layerlist = []\n    \n    inputs = keras.Input(shape=(X.shape[1],), name='Inputs')\n    #Add first layer\n    first_layer_size = shape.pop(0)\n    l = layers.Dense(first_layer_size,activation=swish,input_shape=[X.shape[1]])(inputs)\n    l = layers.BatchNormalization()(l)\n    l = layers.Dropout(dropout)(l)\n    layerlist.append(l)\n    #Add other hidden layers\n    for s in shape:\n        l = layers.Dense(s,activation=swish)(l)\n        l = layers.BatchNormalization()(l)\n        l = layers.Dropout(dropout)(l)\n        layerlist.append(l)\n    #Add output layer\n    for skip in skips:\n        l = layers.Concatenate()([l,layerlist[skip]])\n    out = layers.Dense(1,activation='sigmoid')(l)\n    model = keras.Model(inputs,out)\n    \n    return model\n\nearly_stopping = callbacks.EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True,mode='min')\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0,mode='min')\n\ndef new_model_train(shape,X_train,y_train,X_valid,y_valid,batch_size=2000,epochs=100,learning_rate=0.01,dropout=0.2,skips=[],verbose=1):\n    string = \"\"\n    for s in shape:\n        string = string + \",\" + str(s)\n    string = string + \";\" + str(batch_size)\n        \n    model = new_model(shape,dropout,skips)\n    #model = base_model()\n    \n    model.compile(\n        #optimizer='adam',\n        optimizer = keras.optimizers.Adam(learning_rate=learning_rate),\n        loss='binary_crossentropy',\n        metrics=[keras.metrics.AUC()]\n    )\n\n    history = model.fit(\n        X_train,y_train,validation_data=(X_valid,y_valid),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=[early_stopping,plateau],\n        verbose=verbose\n    )\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc_scores = []\n\ndef kfold_train(n_splits,shape,batch_size=2000,epochs=1000,learning_rate=0.01,dropout=0.2,skips=[],verbose=1):\n    global test_pred\n    test_pred = np.zeros(len(X_test))\n    global train_oof\n    train_oof = np.zeros(len(X))\n    kf = StratifiedKFold(n_splits=n_splits,random_state=2020,shuffle=True)\n    shape1 = shape.copy()\n    for i, (train_index, test_index) in enumerate(kf.split(X,y)):\n        print(\"Fitting Fold %2i/%2i\" %(i+1,n_splits))\n        shape = shape1.copy()\n        X_train, X_valid = X[train_index], X[test_index]\n        y_train, y_valid = y[train_index], y[test_index]\n        model = new_model_train(shape,X_train,y_train,X_valid,y_valid,batch_size=batch_size,epochs=epochs,learning_rate=learning_rate,dropout=dropout,skips=[],verbose=verbose)\n        valid_pred = model.predict(X_valid)\n        score = roc_auc_score(y_valid,valid_pred)\n        print(\"Fold %2i score: %1.6f\" %((i+1),score))\n        \n        train_oof[test_index] = valid_pred[:,0]\n        test_pred += model.predict(X_test)[:,0]/n_splits\n    \n    \n    score = roc_auc_score(y,train_oof)\n    auc_scores.append(score)\n    print(\"----------------------------\")\n    print(\"Final score is: %1.6f\" %score)\n    print(\"----------------------------\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold_train(5,[32,320,384,288],2048,verbose=1,learning_rate=0.0008,dropout=0.38,skips=[0,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = pd.DataFrame(test['id'])\nsubm['target'] = test_pred\nsubm.to_csv('submission.csv',index=False)\nsubm","metadata":{},"execution_count":null,"outputs":[]}]}