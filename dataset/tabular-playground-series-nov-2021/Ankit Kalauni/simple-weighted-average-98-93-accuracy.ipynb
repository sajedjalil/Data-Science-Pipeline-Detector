{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color:#189AB4;font-size:50px;\"><strong>Simple Weighted Average Ensemble<strong style=\"color:black\"> TPS November 2021</strong></strong></h1>\n\n<a><img src=\"https://i.ibb.co/PWvpT9F/header.png\" alt=\"header\" border=\"0\" width=800 height=400></a>","metadata":{}},{"cell_type":"markdown","source":"From [Medium](https://medium.com/analytics-vidhya/simple-weighted-average-ensemble-machine-learning-777824852426)\n\n<p style=\"font-size:120%\"><strong>The simple math behind it</strong></p>\n\nSuppose you have a set of five classifiers. You find out each of them may produce an error of 0.2, assuming the classifiers are all independent. And the situation where the ensemble classifier goes wrong on an instance is at least 3 out of your 5 classifiers made mistakes together (majority voting). Therefore, the probability of a wrong prediction is calculated as the following:\n\n> The combination of 3 out of 5 goes wrong is 10;\n\n> The combination of 4 out of 5 goes wrong is 5;\n\n> And the combination of all 5 goes wrong is 1.\n\n![for mula](https://miro.medium.com/max/622/1*wV2ohNiZsn-o0dCULvWXag.png)\n\nAs you can see, the total error is decreased from 0.2 to 0.058; however, as you add more classifiers into the combination, you should expect a bottleneck, and the trend of reduction of errors will plateau.\n\n![memes](https://miro.medium.com/max/630/1*E4_pTJctmAofSRpZCZbv-g.jpeg)","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport sys\nimport os\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-21T13:42:21.426782Z","iopub.execute_input":"2021-11-21T13:42:21.427322Z","iopub.status.idle":"2021-11-21T13:42:22.415898Z","shell.execute_reply.started":"2021-11-21T13:42:21.427273Z","shell.execute_reply":"2021-11-21T13:42:22.414994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:#189AB4;font-size:20px;\"><strong>Read in <strong style=\"color:black\"> submission files</strong></strong></h1>","metadata":{}},{"cell_type":"markdown","source":"<center>\n\n<h1 style=\"color:#189AB4;font-size:20px;\"><strong><strong style=\"color:black\">Credits:</strong></strong></h1>\n\n</center>    \n    \n<a href=\"https://www.kaggle.com/adityasharma01/simple-nn-tps-nov-21\"><p style=\"text-align:center\">ADITYA SHARMA</p></a>\n\n<a href=\"https://www.kaggle.com/jmcslk/tps-nov-21-dnn-cnn-model-extras\"><p style=\"text-align:center\">MARCIN PIETRZYCKI</p></a>\n\n<a href=\"https://www.kaggle.com/javiervallejos/simple-nn-with-good-results-tps-nov-21\"><p style=\"text-align:center\">JAVIER VALLEJOS</p></a>\n\n<a href=\"https://www.kaggle.com/chaudharypriyanshu/understanding-neural-net\"><p style=\"text-align:center\">PRIYANSHU CHAUDHARY</p></a>\n\n<a href=\"https://www.kaggle.com/dlaststark/tps-1121-dnn-v4\"><p style=\"text-align:center\">DLASTSTARK</p></a>\n\n<a href=\"https://www.kaggle.com/edrickkesuma/power-averaging-is-your-friend\"><p style=\"text-align:center\">Edrick Kesuma</p></a>\n\n<a href=\"https://www.kaggle.com/ambrosm/tpsnov21-007-postprocessing\"><p style=\"text-align:center\">AmbrosM</p></a>\n","metadata":{}},{"cell_type":"code","source":"%%time\n\ndir = '../input/tabular-playground-series-nov-2021/'\nz = '.csv'\n\ntrain = pd.read_csv('../input/november21/train.csv')\ntest = pd.read_csv(dir+'test'+z)\n\ndef seed_everything(seed=42):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n    \nTARGET = 'target'\nFOLD = 5\nSEED = 42\nN_ESTIMATORS=15000\nDEVICE = 'CPU'\n\nLOSS = 'BinaryCrossEntropy'\nEVAL_METRIC = \"AUC\"\n\nSTUDY_TIME = 60*60*8\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:42:22.417746Z","iopub.execute_input":"2021-11-21T13:42:22.417983Z","iopub.status.idle":"2021-11-21T13:42:51.899384Z","shell.execute_reply.started":"2021-11-21T13:42:22.417952Z","shell.execute_reply":"2021-11-21T13:42:51.898493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def postprocess_separate(sub, test_df=test, pure_df=train):\n    \"\"\"Update sub so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    sub : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https://www.kaggle.com/ambrosm/tpsnov21-007-postprocessing\n    \"\"\"\n    if pure_df is None: pure_df = pd.read_csv('../input/november21/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('../input/tabular-playground-series-nov-2021/test.csv')\n    if test_df.shape[0] != sub.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = sub[pure_test_pred == 0].target.max(), sub[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    sub.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    sub.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(sub[pure_test_pred == 0].target.min(), sub[pure_test_pred == 0].target.max(),\n          sub[pure_test_pred == 1].target.min(), sub[pure_test_pred == 1].target.max())\n    \n    del model1, model2\n    gc.collect()\n    return sub\n\n\n#reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-21T13:42:51.900779Z","iopub.execute_input":"2021-11-21T13:42:51.901001Z","iopub.status.idle":"2021-11-21T13:42:51.929874Z","shell.execute_reply.started":"2021-11-21T13:42:51.900972Z","shell.execute_reply":"2021-11-21T13:42:51.928812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reduce memory by changing its datatype datatype\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-21T13:42:51.930965Z","iopub.execute_input":"2021-11-21T13:42:51.93119Z","iopub.status.idle":"2021-11-21T13:43:09.269636Z","shell.execute_reply.started":"2021-11-21T13:42:51.931162Z","shell.execute_reply":"2021-11-21T13:43:09.268485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_ = '../input/tps-nov-2021/wmean/'\n\nsub1 = pd.read_csv(dir_ + 'sub1 0.74988.csv')\nsub2 = pd.read_csv(dir_ + 'sub2 0.74966.csv')\nsub3 = pd.read_csv(dir_ + 'sub3 0.74951.csv')\nsub4 = pd.read_csv(dir_ + 'sub4 0.74940.csv')\nsub5 = pd.read_csv(dir_ + 'sub5 0.74935.csv')\nsub6 = pd.read_csv(dir_ + 'sub6 0.74918.csv')\n\nsub1 = postprocess_separate(sub1)\nsub2 = postprocess_separate(sub2)\nsub3 = postprocess_separate(sub3)\nsub4 = postprocess_separate(sub4)\nsub5 = postprocess_separate(sub5)\nsub6 = postprocess_separate(sub6)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:43:09.273778Z","iopub.execute_input":"2021-11-21T13:43:09.274316Z","iopub.status.idle":"2021-11-21T13:45:39.837003Z","shell.execute_reply.started":"2021-11-21T13:43:09.274256Z","shell.execute_reply":"2021-11-21T13:45:39.836297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:#189AB4;font-size:20px;\"><strong>Check for <strong style=\"color:black\">correlations</strong></strong></h1>","metadata":{}},{"cell_type":"code","source":"import matplotlib as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nhist_data = [sub1.target, sub2.target, sub3.target,sub4.target, sub5.target, sub6.target]\ngroup_labels = ['sub1','sub2','sub3','sub4','sub5','sub6']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-21T13:45:39.838044Z","iopub.execute_input":"2021-11-21T13:45:39.838363Z","iopub.status.idle":"2021-11-21T13:46:18.631316Z","shell.execute_reply.started":"2021-11-21T13:45:39.838336Z","shell.execute_reply":"2021-11-21T13:46:18.630455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# High correlation between all models ~0.994+\ndata = np.corrcoef([sub1.target, sub2.target, sub3.target,sub4.target, sub5.target, sub6.target])\nfig=px.imshow(data,x=group_labels, y=group_labels)\n\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-21T13:46:18.632519Z","iopub.execute_input":"2021-11-21T13:46:18.633235Z","iopub.status.idle":"2021-11-21T13:46:19.579992Z","shell.execute_reply.started":"2021-11-21T13:46:18.633198Z","shell.execute_reply":"2021-11-21T13:46:19.579028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"color:#189AB4;font-size:20px;\"><strong>How to implement one <strong style=\"color:black\">intuitively</strong></strong></h1>\n\nUsually, <mark>there are two ways to do it.</mark>\n\nFirst, you train the same classifier (e.g., Decision Tree) over multiple different subsets of training data, which leads to multiple different models (DT1, DT2, DT3,â€¦). Then, you predict the test data with those models and average the results.\n\n![b](https://miro.medium.com/max/700/1*oJKi_Xdle5qXjv_XgIKExA.png)\n\nSecond, you can train multiple different (the more diverse, the better) classifiers with the whole training set, and average the results\n\n![n](https://miro.medium.com/max/700/1*UrmG9r6dc2I_ouwtadrWnQ.png)","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:#189AB4;font-size:20px;\"><strong>Submission file<strong style=\"color:black\"></strong></strong></h1>\n","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')\n\nsubmission0 = submission_df.copy()\nsubmission0.loc[:,'target'] = sub1.target\n\nsubmission1 = submission_df.copy() #33 hai abhi\nsubmission1.loc[:,'target'] = (sub1.target*170 + sub2.target*15 + sub3.target*13 + sub4.target*12 + sub5.target*6 + sub6.target*4) /220\n\nsubmission2 = submission_df.copy()\nsubmission2.loc[:,'target'] = (sub1.target*170 + sub2.target*20 + sub3.target*18 + sub4.target*16 + sub5.target*14 + sub6.target*12) /250\n\nsubmission3 = submission_df.copy()\nsubmission3.loc[:,'target'] = (sub1.target*190  + sub2.target*23 + sub3.target*14 + sub4.target*13) /240\n\nsubmission4 = submission_df.copy()\nsubmission4.loc[:,'target'] = (sub1.target*250  + sub2.target*33 + sub3.target*17) /300\n\nsubmission5 = submission_df.copy()\nsubmission5.loc[:,'target'] = (sub1.target*93  + sub2.target*7) /100","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:46:19.588936Z","iopub.execute_input":"2021-11-21T13:46:19.589409Z","iopub.status.idle":"2021-11-21T13:46:19.845238Z","shell.execute_reply.started":"2021-11-21T13:46:19.589367Z","shell.execute_reply":"2021-11-21T13:46:19.844401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.postimg.cc/L5L1LCHN/241426370-4989674537714337-7340156749542474389-n.jpg\" \n     width=\"400\" \n     height=\"500\" />","metadata":{}},{"cell_type":"code","source":"submission0.to_csv('Wmean0.csv', index=False)\nsubmission1.to_csv('Wmean1.csv', index=False)\nsubmission2.to_csv('Wmean2.csv', index=False)\nsubmission3.to_csv('Wmean3.csv', index=False)\nsubmission4.to_csv('Wmean4.csv', index=False)\nsubmission5.to_csv('Wmean5.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:46:19.846308Z","iopub.execute_input":"2021-11-21T13:46:19.84652Z","iopub.status.idle":"2021-11-21T13:46:31.324635Z","shell.execute_reply.started":"2021-11-21T13:46:19.846495Z","shell.execute_reply":"2021-11-21T13:46:31.323725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nhist_data = [sub1.target, sub2.target, sub3.target,sub4.target, sub5.target, sub6.target, submission1.target]\ngroup_labels = ['sub1','sub2','sub3','sub4','sub5','sub6','submission1']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-21T13:46:31.327549Z","iopub.execute_input":"2021-11-21T13:46:31.328562Z","iopub.status.idle":"2021-11-21T13:47:13.925224Z","shell.execute_reply.started":"2021-11-21T13:46:31.328509Z","shell.execute_reply":"2021-11-21T13:47:13.924251Z"},"trusted":true},"execution_count":null,"outputs":[]}]}