{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1 Introduction\n\nThis EDA explores the data available for the Tabular Playground Series - November 2021 competition. Simple data exploration is performed, as well as preliminary modeling.\n\n## 1.1 Evaluation Criteria\n\nThe goal for this competition is to maximize ROC AUC score. This means generating classifiers or regressions that predict the probability of the class target variable based on the features included.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\n\ntrain = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\": Train shape {}\".format(train.shape))\nprint(\": Test shape {}\".format(test.shape))\nprint(\"\")","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Training and Testing Files\n\nOur input data consists of:\n\n* `train.csv` - 521 MB in size, containing 102 columns and 600,000 rows\n* `test.csv` - 468 MB in size, containing 101 columns and 540,000 rows\n\nThe main observation is that while 1.0 GB fits in memory, model training may exert pressure on the Kaggle 16 GB CPU memory and GPU memory limitations. We should definitely explore what column formats are at play, and whether running functions to [reduce memory usage](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage) on Pandas dataframes can ease pressure on memory.","metadata":{}},{"cell_type":"markdown","source":"# 2 Features\n\n## 2.1 `id` Column\n\nThe `id` column is a `int64` integer column that contains unique record indicators ranging from 0 to 599,999. Like most Tabular Series, this is simply an identifier for the record and is likely not going to be of use for modelling purposes.\n\n## 2.2 `target` Column\n\nThe `target` column contains the class targets we are attempting to predict. We should look first to see what class breakdown we have.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nsns_params = {\"palette\": \"bwr_r\"}\n\ncounts = pd.DataFrame(train[\"target\"].value_counts())\nax = sns.barplot(x=counts.index, y=counts.target, **sns_params)\nfor p in ax.patches:\n    ax.text(x=p.get_x()+(p.get_width()/2), y=p.get_height(), s=\"{:,d}\".format(round(p.get_height())), ha=\"center\")\n_ = ax.set_title(\"Class Balance\", fontsize=15)\n_ = ax.set_ylabel(\"Number of Records\", fontsize=15)\n_ = ax.set_xlabel(\"Class\", fontsize=15)\n\ndel(counts)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predicted class is well balanced, with little to no skew. This is interesting as it gives us a lot of training data per class to look at.","metadata":{}},{"cell_type":"markdown","source":"## 2.3 `Fx` Columns\n\nFeature columns are `f0` through `f99`. All are continuous. The display below is credited to [@subinium](https://www.kaggle.com/subinium) (see their [Simple EDA](https://www.kaggle.com/subinium/tps-oct-simple-eda) for the October TPS competition).","metadata":{}},{"cell_type":"code","source":"features = [\"f{}\".format(x) for x in range(100)]\n\ntrain[features].describe().T.style.bar(subset=['mean'], color='#7BCC70')\\\n    .background_gradient(subset=['std'], cmap='Reds')\\\n    .background_gradient(subset=['50%'], cmap='coolwarm')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Right away there are some interesting observations. Features `f2` and `f35` have values that buck the trend of having a mean around 0 or 2. We should dive deeper into those two features. Another thing to note is that we aren't seeing any categorical features masquerading as continuous (i.e. features that have a min of 0 and max of 1 with no values between. ","metadata":{}},{"cell_type":"markdown","source":"## 2.4 Null Values\n\nWe should also check to see if we are missing any values in the columns.","metadata":{}},{"cell_type":"code","source":"# Count the number of null values that occur in each row\ntrain[\"null_count\"] = train.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = train.groupby(\"null_count\")[\"target\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[20, 10])\ncolors = sns.color_palette(\"bwr_r\")[0:5]\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5, colors=colors)\n_ = plt.title(\"Percentage of Null Values Per Row (Train Data)\", fontsize=14)\n\ndel(counts)\ndel(null_data)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the number of null values that occur in each row\ntest[\"null_count\"] = test.isnull().sum(axis=1)\n\n# Group the null counts\ncounts = test.groupby(\"null_count\")[\"null_count\"].count().to_dict()\nnull_data = {\"{} Null Value(s)\".format(k) : v for k, v in counts.items() if k < 6}\n\n# Plot the null count results\npie, ax = plt.subplots(figsize=[20, 10])\nplt.pie(x=null_data.values(), autopct=\"%.2f%%\", explode=[0.05]*len(null_data.keys()), labels=null_data.keys(), pctdistance=0.5, colors=colors)\n_ = plt.title(\"Percentage of Null Values Per Row (Test Data)\", fontsize=14)\n\ndel(counts)\ndel(null_data)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this competition, we're not seeing any missing values. This means we don't have to worry about imputing or creating new features based on null values.","metadata":{}},{"cell_type":"markdown","source":"## 2.5 P-Value Testing\n\nWhile looking at features visually will tell us some interesting information, we can also use p-value testing to see if a feature has a net impact on a simple regression model. This method is controversial in that it likely doesn't provide a correct look at what features are informative. Our null hypothesis is that the feature impacts the target variable of `target`. In this case, anything with a p-value greater than 0.05 means we reject that hypothesis, and can potentially flag it for removal.","metadata":{}},{"cell_type":"code","source":"from statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools.tools import add_constant\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nx = add_constant(train[features])\nmodel = OLS(train[\"target\"], x).fit()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pvalues = pd.DataFrame(model.pvalues)\npvalues.reset_index(inplace=True)\npvalues.rename(columns={0: \"pvalue\", \"index\": \"feature\"}, inplace=True)\npvalues.style.background_gradient(cmap='YlOrRd')","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(model)\ndel(x)\n_ = gc.collect()\n\nfeatures_to_drop = []\nfor index, row in pvalues.iterrows():\n    if row[\"pvalue\"] > 0.05:\n        features_to_drop.append(row[\"feature\"])\nfeatures_to_drop","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We may potentially be able to drop features `f0`, `f38`, `f52`, `f72`, and `f92`. We'll probably want to use more advanced feature selection and information techniques such as [SHAP](https://github.com/slundberg/shap) to guide feature selection more reliably.","metadata":{}},{"cell_type":"markdown","source":"## 2.6 Spearman Correlation\n\nWe should also check to see what variables are correlated to one another. We'll check the Spearman correlation first, since it does not make assumptions about distribution types or linearity. With Spearman correlation, we have values that range from -1 to +1. Values around either extreme end mean a neagative or positive correlation, while those around 0 mean no correlation exists.","metadata":{}},{"cell_type":"code","source":"columns_to_check = features.copy()\ncolumns_to_check.append(\"target\")\ncorrelation_matrix = train[columns_to_check].corr(method=\"spearman\")\n\nfrom matplotlib.colors import SymLogNorm\n\nf, ax = plt.subplots(figsize=(20, 20))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n_ = sns.heatmap(\n    correlation_matrix, \n    mask=np.triu(np.ones_like(correlation_matrix, dtype=bool)), \n    cmap=sns.diverging_palette(230, 20, as_cmap=True), \n    center=0,\n    square=True, \n    linewidths=.1, \n    cbar_kws={\"shrink\": .2},\n    norm=SymLogNorm(linthresh=0.03, linscale=0.03, vmin=-1.0, vmax=1.0, base=10),\n)","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Target Correlations\n\nThe following features are correlated positively to the target:\n\n* `f8`, `f27`, `f34`, `f41`, `f43`, `f50`, and `f57`\n\nThe following features are correlated negatively to the target:\n\n* `f55`, `f71`, `f80`, and `f91`\n\n#### Feature Correlations\n\nThe following pairs appear to be correlated:\n\n* `f4` and `f75` - these share a positive correlation.\n* `f9` and `f21` - these share a positive correlation.\n* `f18` and `f89` - these share a positive correlation.\n* `f20` and `f21` - these share a positive correlation.\n* `f23` and `f33` - these share a positive correlation.\n* `f27` and `f31` - these share a negative correlation. Since `f27` is strongly correlated to the target however, we may not want to drop either one.\n* `f46` and `f52` - these share a positive correlation.","metadata":{}},{"cell_type":"markdown","source":"# 3 Simple Models\n\nGiven we know a little about the distribution of data, we should establish a set of baseline models to understand what kind of performance we can get from models.","metadata":{}},{"cell_type":"markdown","source":"## 3.1 LightGBM\n\nWe'll start with a simple LightGBM model and see how our features work out from there.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\n\ntarget = train[\"target\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[features], target)):\n    x_train = train[features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"auc\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    train_preds[test_index] = train_oof_preds\n    train_probas[test_index] = train_oof_probas\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- ROC AUC: {}\".format(roc_auc_score(target, train_probas)))\n\ntrain[\"unmodified_preds\"] = train_preds\ntrain[\"unmodified_probas\"] = train_probas\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"target\"], train[\"unmodified_preds\"])\nax = sns.heatmap(confusion, annot=True, fmt=\",d\")\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Unmodified Dataset)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(train_probas)\ndel(confusion)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking across folds, we are seeing stability, which is good. Our overall precision and recall metrics are fairly high between the positive and negative class","metadata":{}},{"cell_type":"markdown","source":"## 3.2 LightGBM Dropping Uninformative Features\n\nLet's use the results of our P-value test and drop uninformative features.","metadata":{}},{"cell_type":"code","source":"target = train[\"target\"]\ncv_rounds = 3\n\nk_fold = StratifiedKFold(\n    n_splits=cv_rounds,\n    random_state=2021,\n    shuffle=True,\n)\n\nnew_features = features.copy()\nnew_features.remove('f89')\n    \ntrain_preds = np.zeros(len(train.index), )\ntrain_probas = np.zeros(len(train.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train[new_features], target)):\n    x_train = train[new_features].iloc[train_index]\n    y_train = target.iloc[train_index]\n\n    x_valid = train[new_features].iloc[test_index]\n    y_valid = target.iloc[test_index]\n\n    model = LGBMClassifier(\n        random_state=2021,\n        n_estimators=2000,\n        verbose=-1,\n        metric=\"auc\",\n    )\n    model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=50,\n        verbose=0,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    train_preds[test_index] = train_oof_preds\n    train_probas[test_index] = train_oof_probas\n    \n    print(\"-- Fold {}:\".format(fold+1))\n    print(\"{}\".format(classification_report(y_valid, train_oof_preds)))\n\nprint(\"-- Overall:\")\nprint(\"{}\".format(classification_report(target, train_preds)))\nprint(\"-- ROC AUC: {}\".format(roc_auc_score(target, train_probas)))\n\ntrain[\"dropped_preds\"] = train_preds\ntrain[\"dropped_probas\"] = train_probas\n\n# Show the confusion matrix\nconfusion = confusion_matrix(train[\"target\"], train[\"dropped_preds\"])\nax = sns.heatmap(confusion, annot=True, fmt=\",d\")\n_ = ax.set_title(\"Confusion Matrix for LGB Classifier (Dropped Features)\", fontsize=15)\n_ = ax.set_ylabel(\"Actual Class\")\n_ = ax.set_xlabel(\"Predicted Class\")\n\ndel(train_preds)\ndel(train_probas)\ndel(confusion)\n_ = gc.collect()","metadata":{"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More to come...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}