{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"reference LR model : https://www.kaggle.com/mohammadkashifunique/tps-nov-logistic-regression-using-pytorch","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport matplotlib.transforms as mtransforms\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nfrom sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport torch\nimport gc\nimport torch.nn as nn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-14T12:59:05.104725Z","iopub.execute_input":"2021-11-14T12:59:05.105263Z","iopub.status.idle":"2021-11-14T12:59:08.156075Z","shell.execute_reply.started":"2021-11-14T12:59:05.10516Z","shell.execute_reply":"2021-11-14T12:59:08.155352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-nov-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-nov-2021/test.csv\")\nss    = pd.read_csv('../input/tabular-playground-series-nov-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:08.157543Z","iopub.execute_input":"2021-11-14T12:59:08.157891Z","iopub.status.idle":"2021-11-14T12:59:37.46043Z","shell.execute_reply.started":"2021-11-14T12:59:08.157862Z","shell.execute_reply":"2021-11-14T12:59:37.459686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"train_df = train.drop(['id'], axis = 1 )\ntest_df = test.drop(['id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:37.461493Z","iopub.execute_input":"2021-11-14T12:59:37.461723Z","iopub.status.idle":"2021-11-14T12:59:37.813242Z","shell.execute_reply.started":"2021-11-14T12:59:37.461696Z","shell.execute_reply":"2021-11-14T12:59:37.812529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for NULL values","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()[train.isnull().sum() != 0]","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:37.815016Z","iopub.execute_input":"2021-11-14T12:59:37.815223Z","iopub.status.idle":"2021-11-14T12:59:38.081436Z","shell.execute_reply.started":"2021-11-14T12:59:37.815198Z","shell.execute_reply":"2021-11-14T12:59:38.080668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Null is nothing","metadata":{}},{"cell_type":"markdown","source":"**HEATMAP**","metadata":{}},{"cell_type":"markdown","source":"Visualize and confirm the correlation between features. Although seemingly trivial, statistical analysis is a very important task.\n\nIf you have time, draw a scatter plot between features as well.\n\nYou can get something out of a visualized graph.","metadata":{}},{"cell_type":"markdown","source":"Breaking 10 pieces to figure out the correlation","metadata":{}},{"cell_type":"markdown","source":"Rows 1 to 10","metadata":{}},{"cell_type":"code","source":"colormap = plt.cm.PuBu \nplt.figure(figsize=(15, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(train_df.iloc[:, 0:10].astype(float).corr(), linewidths = 0.1, vmax = 1.0,square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 10})","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:38.082605Z","iopub.execute_input":"2021-11-14T12:59:38.082805Z","iopub.status.idle":"2021-11-14T12:59:39.377359Z","shell.execute_reply.started":"2021-11-14T12:59:38.08278Z","shell.execute_reply":"2021-11-14T12:59:39.376413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rows 11 to 20","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(train_df.iloc[:, 11:20].astype(float).corr(), linewidths = 0.1, vmax = 1.0,square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 10})","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:39.378881Z","iopub.execute_input":"2021-11-14T12:59:39.379214Z","iopub.status.idle":"2021-11-14T12:59:40.34514Z","shell.execute_reply.started":"2021-11-14T12:59:39.379172Z","shell.execute_reply":"2021-11-14T12:59:40.344197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rows 21 to 30","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8)) \nplt.title(\"Diabetes Correlation of Features\", y = 1.05, size = 15) \nsns.heatmap(train_df.iloc[:, 21:30].astype(float).corr(), linewidths = 0.1, vmax = 1.0,square = True, cmap = colormap, linecolor = \"white\", annot = True, annot_kws = {\"size\" : 10})","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:40.346754Z","iopub.execute_input":"2021-11-14T12:59:40.347052Z","iopub.status.idle":"2021-11-14T12:59:41.322232Z","shell.execute_reply.started":"2021-11-14T12:59:40.347018Z","shell.execute_reply":"2021-11-14T12:59:41.321396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The correlation between the explanatory variables is very low. I think it would be suitable for use in the model.**ðŸ‘","metadata":{}},{"cell_type":"markdown","source":"# EDA -> Features Engineering","metadata":{}},{"cell_type":"markdown","source":"**Multicollinearity**","metadata":{}},{"cell_type":"markdown","source":"Multicollinearity, which is the most problematic when performing LR, is checked.\n\nWhen multicollinearity exists, the explanatory power of the model decreases, and the model breaks when other variables are added.\n\nAfter checking multicollinearity, features with VIF value of 10 or higher are removed.\n\n\nThere are several methods to remove multicollinearity. The main methods are PCA and VIF. I will use VIF\n\nNote that you have to remove them one by one using 'loop'. Remove one and check the VIF value again.\n\nps. It can be used for linear models such as SVM, but do not apply to ensemble models such as RF and Decision Tree.","metadata":{}},{"cell_type":"code","source":"train_df_X = train_df.drop('target', axis =1)\ntrain_df_y = train_df['target']","metadata":{"execution":{"iopub.status.busy":"2021-11-14T12:59:41.323395Z","iopub.execute_input":"2021-11-14T12:59:41.323707Z","iopub.status.idle":"2021-11-14T12:59:41.506867Z","shell.execute_reply.started":"2021-11-14T12:59:41.323676Z","shell.execute_reply":"2021-11-14T12:59:41.506099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndef show_vif(df):\n    vif = []\n    for idx in range(len(df.columns)):\n        vif.append(variance_inflation_factor(df.values, idx))\n\n    vif_dataframe = pd.DataFrame()\n    vif_dataframe['columns'] = df.columns\n    vif_dataframe['VIF'] = vif\n    return vif_dataframe","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.383986Z","iopub.status.idle":"2021-11-12T10:40:30.384256Z","shell.execute_reply.started":"2021-11-12T10:40:30.384115Z","shell.execute_reply":"2021-11-12T10:40:30.384129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_vif(train_df_X)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.385043Z","iopub.status.idle":"2021-11-12T10:40:30.385312Z","shell.execute_reply.started":"2021-11-12T10:40:30.385172Z","shell.execute_reply":"2021-11-12T10:40:30.385187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Eliminate multicollinearity(Remove vif > 10 higher)\ndef remove_multicollinearity(df):\n    while True:\n        vif_dataframe = show_vif(df)\n        \n        print(len(vif_dataframe[vif_dataframe['VIF'] >= 10]))\n        if len(vif_dataframe[vif_dataframe['VIF'] >= 10]) == 0:\n            break\n        \n        remove_column = vif_dataframe[vif_dataframe['VIF'] >= 10].sort_values(by='VIF', ascending=False)['columns'].reset_index(drop=True)[0]\n        print(f\"remove_column: {remove_column}\")\n        df = df.drop(remove_column, axis=1)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.386074Z","iopub.status.idle":"2021-11-12T10:40:30.386481Z","shell.execute_reply.started":"2021-11-12T10:40:30.386205Z","shell.execute_reply":"2021-11-12T10:40:30.38622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_removeVIF_df = remove_multicollinearity(train_df_X)","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.387354Z","iopub.status.idle":"2021-11-12T10:40:30.387621Z","shell.execute_reply.started":"2021-11-12T10:40:30.387482Z","shell.execute_reply":"2021-11-12T10:40:30.387496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There were no features with multicollinearity greater than 10.**","metadata":{}},{"cell_type":"markdown","source":"# EDA -> Select feature","metadata":{}},{"cell_type":"markdown","source":"**backward elimination**","metadata":{}},{"cell_type":"markdown","source":"* One of the stepwise regression analysis methods. \n\n* A method of simplifying the model by repeating the process of removing unnecessary independent variables one by one from the model including all variables\n\n* A method of removing explanatory variables with the smallest explanatory power (correlation) one by one from a full model including all explanatory variables","metadata":{}},{"cell_type":"code","source":"variables = list(train_df_X) ## feature list\n \ny = train['target'] ## label\nselected_variables = variables ## Initially, all variables are selected.\nsl_remove = 0.05\n \nsv_per_step = [] ## Variables selected for each step\nadjusted_r_squared = [] ## Modified r_squared for each step\nsteps = []\nstep = 0\nwhile len(selected_variables) > 0:\n    X = sm.add_constant(train[selected_variables])\n    p_vals = sm.OLS(y,X).fit().pvalues[1:] \n    max_pval = p_vals.max() ## Max p-value\n    if max_pval >= sl_remove: ## Exclude if max p-value is greater than or equal to the reference value\n        remove_variable = p_vals.idxmax()\n        selected_variables.remove(remove_variable)\n \n        step += 1\n        steps.append(step)\n        adj_r_squared = sm.OLS(y,sm.add_constant(train[selected_variables])).fit().rsquared_adj\n        adjusted_r_squared.append(adj_r_squared)\n        sv_per_step.append(selected_variables.copy())\n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.388554Z","iopub.status.idle":"2021-11-12T10:40:30.388868Z","shell.execute_reply.started":"2021-11-12T10:40:30.388695Z","shell.execute_reply":"2021-11-12T10:40:30.388709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nfig.set_facecolor('white')\n \nfont_size = 15\nplt.xticks(steps,[f'step {s}\\n'+'\\n'.join(sv_per_step[i]) for i,s in enumerate(steps)], fontsize=12)\nplt.plot(steps,adjusted_r_squared, marker='o')\n    \nplt.ylabel('Adjusted R Squared',fontsize=font_size)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.389699Z","iopub.status.idle":"2021-11-12T10:40:30.390018Z","shell.execute_reply.started":"2021-11-12T10:40:30.389866Z","shell.execute_reply":"2021-11-12T10:40:30.389881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data that does not affect the target\nremove = 'f0', 'f52', 'f72', 'f38'","metadata":{}},{"cell_type":"code","source":"train_refinded_data = train_df.drop(['f0', 'f52', 'f72', 'f38'], axis = 1)\ntest_refinded_data = test_df.drop(['f0', 'f52', 'f72', 'f38'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:00:31.864644Z","iopub.execute_input":"2021-11-14T13:00:31.866272Z","iopub.status.idle":"2021-11-14T13:00:32.38035Z","shell.execute_reply.started":"2021-11-14T13:00:31.866168Z","shell.execute_reply":"2021-11-14T13:00:32.379386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_refinded_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:01:31.191736Z","iopub.execute_input":"2021-11-14T13:01:31.192032Z","iopub.status.idle":"2021-11-14T13:01:31.221861Z","shell.execute_reply.started":"2021-11-14T13:01:31.192002Z","shell.execute_reply":"2021-11-14T13:01:31.220711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_refinded_data.drop(['target'], axis=1)\ny = train_refinded_data['target']\nX_test = test_refinded_data.copy()\n\ndel train_refinded_data\ngc.collect()\ndel test_refinded_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-14T13:01:59.787037Z","iopub.execute_input":"2021-11-14T13:01:59.788133Z","iopub.status.idle":"2021-11-14T13:02:00.528316Z","shell.execute_reply.started":"2021-11-14T13:01:59.788063Z","shell.execute_reply":"2021-11-14T13:02:00.526742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Scaling**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression Modeling","metadata":{}},{"cell_type":"code","source":"%%time\nEPOCHS = 100\nKFold = StratifiedKFold(n_splits=5, random_state=786, shuffle=True)\n\nfor fold, (train_idx, valid_idx) in enumerate(KFold.split(X, y)):\n    X_train, X_valid = X.iloc[train_idx].values, X.iloc[valid_idx].values\n    y_train, y_valid = y.iloc[train_idx].values, y.iloc[valid_idx].values\n    \n    X_train = torch.from_numpy(X_train.astype(np.float32))\n    X_valid = torch.from_numpy(X_valid.astype(np.float32))\n    y_train = torch.from_numpy(y_train.astype(np.float32).reshape(-1,1))\n    y_valid = torch.from_numpy(y_valid.astype(np.float32).reshape(-1,1))\n    \n    model = nn.Sequential(\n        nn.Linear(96,1),\n        nn.Sigmoid()\n    )\n    \n    criterion = nn.BCELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n    \n    train_losses = np.zeros(EPOCHS)\n    valid_losses = np.zeros(EPOCHS)\n    \n    scores = np.zeros(EPOCHS)\n    \n    for ep in range(EPOCHS):\n        \n        optimizer.zero_grad()\n        \n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        \n        loss.backward()\n        optimizer.step()\n        \n        outputs_valid = model(X_valid)\n        loss_valid = criterion(outputs_valid, y_valid)\n        \n        scores += roc_auc_score(y_valid.detach().numpy(), outputs_valid.detach().numpy())\n        \n        train_losses[ep] = loss.item()\n        valid_losses[ep] = loss_valid.item()\n        \n    print(f\"Fold: {fold + 1} Loss: {np.mean(valid_losses)} AUC: {np.mean(scores)}\")\n    plt.plot(train_losses, label='train loss')\n    plt.plot(valid_losses, label='test loss')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test.values\nX_test = torch.from_numpy(X_test.astype(np.float32))\npredictions = model(X_test)\npredictions = predictions.detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:42:32.006946Z","iopub.execute_input":"2021-11-13T01:42:32.007634Z","iopub.status.idle":"2021-11-13T01:42:32.024256Z","shell.execute_reply.started":"2021-11-13T01:42:32.007579Z","shell.execute_reply":"2021-11-13T01:42:32.023341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss['target'] = predictions\nss.to_csv('./submission.csv', index=False)\nss.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-12T10:40:30.399221Z","iopub.status.idle":"2021-11-12T10:40:30.399492Z","shell.execute_reply.started":"2021-11-12T10:40:30.399353Z","shell.execute_reply":"2021-11-12T10:40:30.399366Z"},"trusted":true},"execution_count":null,"outputs":[]}]}