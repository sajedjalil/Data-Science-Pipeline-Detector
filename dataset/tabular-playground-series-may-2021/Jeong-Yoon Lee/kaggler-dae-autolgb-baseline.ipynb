{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggler DAE + AutoLGB Baseline\n\n## **UPDATE on 5/2/2021**\n\n* Feature engineering using target encoding and label encoding from `Kaggler`.\n* Treating all features categorical based on the findings from [Simple yet interesting things about features](https://www.kaggle.com/jeongyoonlee/simple-yet-interesting-things-about-features): i.e. instead of creating two kinds of DAE features (one for categorical, the other for numerical features), creating just DAE features for categorical features.\n\n## **UPDATE on 5/1/2021**\n\nToday, [`Kaggler`](https://github.com/jeongyoonlee/Kaggler) v0.9.4 is released with additional features for DAE as follows:\n* In addition to the swap noise (`swap_prob`), the Gaussian noise (`noise_std`) and zero masking (`mask_prob`) have been added to DAE to overcome overfitting.\n* Stacked DAE is available through the `n_layer` input argument (see Figure 3. in [Vincent et al. (2010), \"Stacked Denoising Autoencoders\"](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf) for reference).\n\nFor example, to build a stacking DAE with 3 pairs of encoder/decoder and all three types of noises, you can do:\n```python\nfrom kaggler.preprocessing import DAE\n\ndae = DAE(cat_cols=cat_cols, num_cols=num_cols, n_layer=3, noise_std=.05, swap_prob=.2, masking_prob=.1)\nX = dae.fit_transform(pd.concat([trn, tst], axis=0))\n```\n\nIf you're using previous versions, please upgrade `Kaggler` using `pip install -U kaggler`.\n\n---\n\nIn this notebook, I will show how to create DAE features from both training and test data, then train a LightGBM model with feature selection and hyperparameter optimization using [Kaggler](https://github.com/jeongyoonlee/Kaggler), a Python package for Kaggle competition.\n\nThe contents of the notebook are as follows:\n1. Simple EDA and Target Transformation\n2. DAE Feature Generation\n3. AutoLGB Model Training\n4. Submission","metadata":{}},{"cell_type":"markdown","source":"## Part 1. Simple EDA and Target Transformation","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport lightgbm as lgb\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport seaborn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom warnings import simplefilter","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U kaggler","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\nsimplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import kaggler\nfrom kaggler.model import AutoLGB\nfrom kaggler.preprocessing import DAE, TargetEncoder, LabelEncoder\nprint(kaggler.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_name = 'dae_te_le'\nalgo_name = 'lgb'\nversion = 3\nmodel_name = f'{algo_name}_{feature_name}_v{version}'\n\ndata_dir = Path('../input/tabular-playground-series-may-2021')\ntrain_file = data_dir / 'train.csv'\ntest_file = data_dir / 'test.csv'\nsample_file = data_dir / 'sample_submission.csv'\n\nfeature_file = f'{feature_name}.h5'\npredict_val_file = f'{model_name}.val.txt'\npredict_tst_file = f'{model_name}.tst.txt'\nsubmission_file = f'{model_name}.sub.csv'\n\nid_col = 'id'\ntarget_col = 'target'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding_dim = 128\nseed = 42\nn_fold = 5\nn_class = 4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = trn[target_col].str.split('_').str[1].astype(int) - 1\nn_trn = trn.shape[0]\ndf = pd.concat([trn.drop(target_col, axis=1), tst], axis=0)\nfeature_cols = df.columns.tolist()\nprint(y.shape, df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 2. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### DAE","metadata":{}},{"cell_type":"markdown","source":"First, generating DAE features by treating all features as categorical features. Internally, an embedding layer will be added to each feature to convert the categories into an embedding vector.","metadata":{}},{"cell_type":"code","source":"dae = DAE(cat_cols=df.columns.to_list(), num_cols=[], encoding_dim=encoding_dim, random_state=seed, \n          swap_prob=.3, n_layer=3)\nX = dae.fit_transform(df)\ndf_dae = pd.DataFrame(X, columns=[f'dae1_{x}' for x in range(X.shape[1])])\nprint(df_dae.shape)\ndf_dae.head()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Encoding","metadata":{}},{"cell_type":"markdown","source":"Target encoding is a popular feature engineering method for categorical features. However, it is subject to overfitting. `Kaggler` uses cross-validation and smoothing to avoid it.","metadata":{}},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nte = TargetEncoder(cv=cv)\nte.fit(trn[feature_cols], y)\ndf_te = te.transform(df[feature_cols])\ndf_te.columns = [f'te_{x}' for x in df.columns]\ndf_te.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding with Grouping","metadata":{}},{"cell_type":"markdown","source":"Although features are already label-encoded, let's group rare categories with `Kaggler`'s label encoder.","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder(min_obs=50)\ndf_le = le.fit_transform(df[feature_cols])\ndf_le.columns = [f'le_{x}' for x in df.columns]\ndf_le.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 3. AutoLGB Model Training","metadata":{}},{"cell_type":"code","source":"params = {'num_class': n_class}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_feature = pd.concat([df_le, df_te, df_dae], axis=1)\ndf_feature.to_hdf(feature_file, key='data')\n\nX = df_feature.iloc[:n_trn]\nX_tst = df_feature.iloc[n_trn:]\n\nclf = AutoLGB(objective='multiclass', metric='multi_logloss', params=params, sample_size=X.shape[0], \n              feature_selection=False, random_state=seed)\nclf.tune(X, y)\n\nfeatures = clf.features\nparams = clf.params\nn_best = clf.n_best\nprint(f'{n_best}')\nprint(f'{params}')\nprint(f'{features}')\n\np = np.zeros((X.shape[0], n_class), dtype=float)\np_tst = np.zeros((X_tst.shape[0], n_class), dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    p_tst += clf.predict(X_tst) / n_fold\n    print(f'CV #{i + 1} Loss: {log_loss(y[i_val], p[i_val]):.6f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'CV Log Loss: {log_loss(y, p):.6f}')\nnp.savetxt(predict_val_file, p, fmt='%.6f')\nnp.savetxt(predict_tst_file, p_tst, fmt='%.6f')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Part 4. Submission","metadata":{}},{"cell_type":"code","source":"sub[sub.columns] = p_tst\nsub.to_csv(submission_file)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you find this notebook helpful, please upvote it and share your feedback in comments. I really appreciate it.\n\nYou can find my other notebooks in both the current and previous TPS competitions below:\n* [Adversarial Validation with LightGBM](https://www.kaggle.com/jeongyoonlee/adversarial-validation-with-lightgbm): shows how close/different the feature distributions between the training and test data. It's a good exercise to perform it at the begining of the competition to understand the risk of overfitting to the training data.\n* [DAE with 2 Lines of Code with Kaggler](https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler): shows how to extract DAE features and train the AutoLGB model with TPS4 data.\n* [AutoEncoder + Pseudo Label + AutoLGB](https://www.kaggle.com/jeongyoonlee/autoencoder-pseudo-label-autolgb): shows how to build a basic AutoEncoder using Keras, and perform automated feature selection and hyperparameter optimization using Kaggler's AutoLGB.\n* [Supervised Emphasized Denoising AutoEncoder](https://www.kaggle.com/jeongyoonlee/supervised-emphasized-denoising-autoencoder): shows how to build a more sophiscated version of * AutoEncoder, called supervised emphasized Denoising AutoEncoder (DAE), which trains DAE and a classifier simultaneously.\n* [Stacking Ensemble](https://www.kaggle.com/jeongyoonlee/stacking-ensemble): shows how to perform stacking ensemble.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}