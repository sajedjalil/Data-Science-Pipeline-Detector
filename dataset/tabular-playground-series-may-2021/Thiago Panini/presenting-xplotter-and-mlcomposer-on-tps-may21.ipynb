{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook aims to allocate the development related to exploratory analysis of insights related to [Kaggle Tabular Playground of May 2021](https://www.kaggle.com/c/tabular-playground-series-may-2021/data). Also, this notebook uses the tools presented on [xplotter](https://github.com/ThiagoPanini/xplotter) and [mlcomposer](https://github.com/ThiagoPanini/mlcomposer) python packages made by myself and published on PyPI repository. This is a real good effort for coding useful functions for making the Exploratory Data Analysis and applying Machine Learning process a lot more easier for Data Scientists and Data Analysis through deliverying charts customization and matplotlib/seaborn plots with a little few lines of code. I really hope you all enjoy it!\n\n<div align=\"center\">\n    <img src=\"https://i.imgur.com/5XFP1Ha.png\" height=300 width=200 alt=\"xplotter Logo\">\n    <img src=\"https://i.imgur.com/MIcPH8g.png\" width=450 height=450 alt=\"mlcomposer logo\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content</h3>\n\n* [1. Libraries and Project Variables](#1)\n* [2. Reading the Data](#2)\n* [3. EDA: Exploring Insights with xplotter](#3)\n    - [3.1 Target Class Balance](#3.1)\n    - [3.2 Correlation Matrix](#3.2)\n    - [3.3 Distribution Analysis](#3.3)\n    - [3.4 Categorical Countplots](#3.4)\n* [4. ML: Training Models with mlcomposer](#4)\n    - [4.1 Transformers Module](#4.1)\n        - [4.1.1 Selecting Features](#4.1.1)\n        - [4.1.2 Target Transformation](#4.1.2)\n        - [4.1.3 Split the Data](#4.1.3)\n        - [4.1.4 Prep Pipelines](#4.1.4)\n    - [4.2 Trainer Module](#4.2)\n        - [4.2.1 Initial Setup](#4.2.1)\n        - [4.2.2 Training Models](#4.2.2)\n        - [4.2.3 Evaluating Performance](#4.2.3)\n* [5. Hyperparameter Tunning](#5)\n    - [5.1 Feature Selection](#5.1)\n* [6. Submitting Results](#6)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Libraries and Project Variables</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"So let's do the work by importing libraries and defining project variables. This will start the implementation and help us to organize the code.","metadata":{}},{"cell_type":"code","source":"!pip install xplotter --upgrade\n!pip install mlcomposer --upgrade","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard python libraries\nimport pandas as pd\nimport os\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# Showing up xplotter\nfrom xplotter.insights import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Path variables\nPROJECT_PATH = '../input/tabular-playground-series-may-2021'\nTRAIN_FILEPATH = os.path.join(PROJECT_PATH, 'train.csv')\nTEST_FILEPATH = os.path.join(PROJECT_PATH, 'test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>2. Reading the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After importing libraries and defining user variables, let's read the data and make the first contact with the content available.","metadata":{}},{"cell_type":"code","source":"# Reading the data\ndf_train = pd.read_csv(TRAIN_FILEPATH)\nprint(f'Data shape: {df_train.shape}')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, we can see that the `training` data has 100,000 rows and 52 columns divided into:\n* 1 id column\n* 1 target column\n* 49 data features\n\n>**Note from competitions page:** The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\nNow that we have already read the data and put the eyes on it for the first time, we can start the job by looking a little deeper into the features for extracting useful information before training Machine Learning models. And thats the point we put `xplotter` package on the game: with `xplotter` we can execute already built functions for visualizing the content of a dataset in a faster and more beautiful way.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>3. EDA: Exploring Insights with xplotter</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"The `xplotter` package construction was motivated exactly to facilitate the work of data scientists in the pillars of insights and exploratory data analysis. The next steps will be based on the tools provided from xplotter library to make beautiful charts in order to get a deep understand of our data. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Target Class Balance</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"plot_donut_chart(df=df_train, col='target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With one line of code, the `plot_donut_chart()` function extracted from `xplotter` was able to deliver a complete donut chart with information about our target class balance. The chart above shows us how the data is distributed along the 4 different target classes and with that we can point out the following statements:\n\n* The *Class_1* category is the one with less data elements (8,480 rows)\n* The *Class_2* category is the one with more data elements (57,497 rows)\n\nThere is much more to explore but, by now, we can think of how this difference of balance can probably impact a further Machine Learning classification model. Let's keep going.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.2 Correlation Matrix</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"# Creating a numerical target from original one\ndf_train_corr = pd.get_dummies(df_train)\n\n# Plotting positive correlation matrix\ntarget_list = ['target_Class_' + str(i) for i in range(1, 5)]\nfor target_class in target_list:\n    # Creating a to_drop list for not considering other classes on correlation\n    to_drop = ['target_Class_' + str(i) for i in range(1, 5)]\n    to_drop.remove(target_class)\n    \n    # Applying xplotter function\n    plot_corr_matrix(df=df_train_corr.drop(to_drop, axis=1), corr_col=target_class, n_vars=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `xplotter` package have one of its most powerful functions called `plot_corr_matrix()`! With this function, we can easily plot a beautiful correlation matrix with custom parameters allowed. We just need to pass the DataFrame and the correlation column (`corr_col` parameter). Additionaly, for making the process computationally less expensive, we can pass the `n_vars` parameter for filtering just the top N features to be used on the matrix.\n\nThe function call above shows the top 10 features with most correlation between each of target class categories. The for loop coded iterates of each different target class after the `pd.get_dummies()` process and uses the `corr_col` paramter from `plot_corr_matrix()` to pass different correlation column as the target for analysis. With this we can see how each feature impacts on each class output.\n\nBut maybe the features have little \"real world meaning\" and the cells on the matrix shows us that the correlation values are always too low for every class. By the other hand, the `plot_corr_matrix()` allows us to see the correlation through the **negative** perspective by handling the `corr` parameter. Let's see the top 10 features with most negative correlation with each target class:","metadata":{}},{"cell_type":"code","source":"# Plotting negative correlation matrix\ntarget_list = ['target_Class_' + str(i) for i in range(1, 5)]\nfor target_class in target_list:\n    # Creating a to_drop list for not considering other classes on correlation\n    to_drop = ['target_Class_' + str(i) for i in range(1, 5)]\n    to_drop.remove(target_class)\n    \n    # Applying xplotter function\n    plot_corr_matrix(df=df_train_corr.drop(to_drop, axis=1), corr='negative',\n                     corr_col=target_class, n_vars=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the same way, the sequence of plots above shows a correlation analysis for each target class by a negative perspective. This is really important for guiding decisions to be made along the project development.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.3\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.3 Distribution Analysis</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"A good way to see the distributions of numerical features is trough plotting histogram, boxplot or another distribution chart. In the `xplotter` package, it's possible to plot custom individual distribution charts or multiple distribution charts with just one function.\n\nLet's see the distribution of 9 features (*feature_1* to *feature_9*).","metadata":{}},{"cell_type":"code","source":"# Multiple distribution plots\ncol_list = ['feature_' + str(i) for i in range(1, 10)]\nplot_multiple_distplots(df=df_train, col_list=col_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By setting up some parameters it's possible to see distribution by another look: the `kind` argument (defined on kwargs) can be used to plot not only histograms, but kdeplots, boxplots, boxenplots or striplots. Let's see another set of features (*feature_10* to *feature_18*).","metadata":{}},{"cell_type":"code","source":"# Plotting multiple boxenplots for features\ncol_list = ['feature_' + str(i) for i in range(10, 19)]\nplot_multiple_distplots(df=df_train, col_list=col_list, kind='boxen')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the features distribution, we can see that some of them has a kind of categorical behavior. If we take a look at the histograms, it can be seens some of discrete elevations on specific x-axis points. Because of that, it's could be useful to generate a countplot instead of distplot for the features. Fortunately, the `xplotter` package can also plot multiple countplots at once and that's what we will see on the next session","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.4\"></a>\n<font color=\"dimgrey\" size=+2.0><b>3.4 Categorical Countplots</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"code","source":"# Plotting countplots for some features\ncol_list = ['feature_2', 'feature_5', 'feature_9', 'feature_18', 'feature_20', 'feature_22',\n            'feature_23']\nplot_multiple_countplots(df=df_train, col_list=col_list, n_cols=2, palette='viridis')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After a short journey on `xplotter` for understanding the data and visualizing some useful patterns like multiple distribution charts, correlation matrix and countplots, there's enough information from data for starting another journey on data modelling by training and evaluating Machine Learning models.\n\nLet's do it using `mlcomposer`: another homemade built for encapsulating the hard work from data scientists for training and evaluating models. This happens by useful classes and methods that has all you need for applying Machine Learning on diverse contexts with few lines of code.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>4. ML: Training Models with mlcomposer</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"As I promissed you, this section will dive deep into an excellent package that you help you apply ML like you never did before. Meet `mlcomposer` as a new way for telling computers to learn from data. On this Tabular Playground Series, we will apply the methods and functions of this package and you will be able to see its power.\n\n<div align=\"center\">\n    <img src=\"https://i.imgur.com/MIcPH8g.png\" width=500 height=500 alt=\"mlcomposer logo\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.1 Transformers Module</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, after studying a little bit the dataset provided, it can be said that there is no much data transformation to be applied in order to train machine learning models. The data has already a pre processing made by another flow and so we have the features already built and a target classed defined.\n\nEven though, let's take the opportunity to show some of the tools presented in `mlcomposer` package, like its useful module called `transformers`. With this module, we can use python classes for applying data transformations in various aspects in order to construct efficient data pipelines for processing and preparing data for training models. ","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.1.1 Selecting Features</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"As long there's no much transformations to be done in this application, let's start by importing a class for selecting only the initial features to be used on training steps. ","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import ColumnSelection\n\n# Instancing object and transforming\nINITIAL_FEATURES = df_train.drop('id', axis=1).columns\nselector = ColumnSelection(features=INITIAL_FEATURES)\ndf_selected = selector.fit_transform(df_train)\n\n# Results\nprint(f'Columns of original dataset: {df_train.shape[1]}')\nprint(f'Columns of selected dataset: {df_selected.shape[1]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.1.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.1.2 Target Transformation</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After initializing the preparation section by selecting the features from a initial training dataset to be received in a previous workflow execution, we need to handle the transformation of your target column (original composed by string).\n\nMost machine learning models don't recognize string entries and so they require a encoding process for transforming strings onto numbers. The mlcomposer package doesn't have something for this because sklearn's already have `OneHotEncoder()` and `LabelEncoder()` classes. But for this point of view, let's keep it simple and create a custom transformation class for extracting the class number from the class string. This will be a good coding construction so we can see how a transformation class can be built for using it in further preparation pipelines.","metadata":{}},{"cell_type":"code","source":"# Importing classes\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Creating custom class\nclass TargetExtractor(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, old_target_name='target', new_target_name='target'):\n        self.old_target_name = old_target_name\n        self.new_target_name = new_target_name\n    \n    def fit(self, df, y=None):\n        return self\n    \n    def transform(self, df, y=None):\n        # Creating new target column\n        df_copy = df.copy()\n        df_copy[self.new_target_name] = df_copy[self.old_target_name].apply(lambda x: int(x[-1]))\n        \n        # Verifying if names are equal for dropping the old column\n        if self.old_target_name == self.new_target_name:\n            return df_copy\n        else:\n            return df_copy.drop(self.old_target_name, axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Executing custom class for transforming the target\ntarget_prep = TargetExtractor()\ndf_target_prep = target_prep.fit_transform(df_selected)\n\n# Results\nprint(f'Samples of old target column: \\n{df_selected[\"target\"].values[:5]}')\nprint(f'\\nSamples of new target column: \\n{df_target_prep[\"target\"].values[:5]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well done!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.1.3\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.1.3 Split the Data</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"For training machine learning models, it's important to validate the results in a set not seen by the models. Thinkng of this need, `mlcomposer` package brings the `SplitData()` class that enables splitting the data into a training pipeline. Let's see how it works.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import DataSplitter\n\n# Initializing object and applying transformation\nsplitter = DataSplitter(target='target')\nX_train, X_val, y_train, y_val = splitter.fit_transform(df_target_prep)\n\n# Results\nprint(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of X_val: {X_val.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.1.4\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.1.4 Prep Pipelines</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"As said before, there few transformations to be made on this dataset. Just for keep the structure and to be prepared for further ideas that can be taken in consideration along the project, let's create a pipeline for data preparation. In the future we can add to this structure:\n\n* PCA pipelines\n* New custom features implementation\n* Feature selection pipelines","metadata":{}},{"cell_type":"code","source":"# Creating prep pipelines for training and testing\nfrom sklearn.pipeline import Pipeline\n\n# Defining variables\nTARGET = 'target'\nINITIAL_FEATURES = ['feature_' + str(i) for i in range(50)] + [TARGET]\nTEST_FEATURES = INITIAL_FEATURES[:-1]\n\n# Train and test pipelines\ntrain_prep_pipe = Pipeline([\n    ('selector', ColumnSelection(features=INITIAL_FEATURES)),\n    ('target_encoder', TargetExtractor()),\n    ('splitter', DataSplitter(target=TARGET))\n])\n\ntest_prep_pipe = Pipeline([\n    ('selector', ColumnSelection(features=TEST_FEATURES))\n])\n\n# Reading the data and applying pipelines\ndf_train = pd.read_csv(TRAIN_FILEPATH)\ndf_test = pd.read_csv(TEST_FILEPATH)\n\nX_train, X_val, y_train, y_val = train_prep_pipe.fit_transform(df_train)\nX_test = test_prep_pipe.fit_transform(df_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Very good! Now we're ready for using the `mlcomposer.trainer` module for trying it out multiclass classification models for reaching out our goal! Keep watching!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.2\"></a>\n<font color=\"dimgrey\" size=+2.0><b>4.2 Trainer Module</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After preparing our data with `transformers` module from mlcomposer, we will use this section for applying the tools available for the `trainer` module. The one thing we must do before easily start training anything is to prepare a dictionary of models and its hyperparameters search space (if applicable) following the structure:\n\n    set_classifiers = {\n        'model_name': {\n            'estimator': ModelClass(),\n            'params': dictionary_params\n        }\n    }\n    \nAfter doing that, we can initialize an object and execute its methods for training and evaluating different models at once. Let's do it!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.2.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.2.1 Initial Setup</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":">Note: unhide the cell below to see hyperparameters definition","metadata":{}},{"cell_type":"code","source":"# Setting up hyperparameters for DecisionTrees\ndtree_tunning_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5, 6, 7, 8, 9, 10],\n    'class_weight': [None, 'balanced'],\n    'random_state': [42]\n}\n\n# Setting up hyperparameters for RandomForest\nforest_tunning_grid = {\n    #'bootstrap': [True, False],\n    'class_weight': [None, 'balanced'],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [5, 6, 7, 8, 9, 10],\n    #'max_features': [None, 'auto', 'sqrt', 'log2'],\n    #'max_leaf_nodes': np.arange(3, 50, 2),\n    #'min_impuriti_decrease': np.linspace(0, 1, 50),\n    #'min_samples_leaf': np.arange(1, 100, 1),\n    #'min_samples_split': np.arange(2, 100, 1),\n    #'min_weight_fraction_leaf': np.linspace(0, 1, 50),\n    'n_estimators': [500],\n    #'oob_score': [True, False],\n    'random_state': [42]\n}\n\n# # Setting up hyperparameters for LightGBM\nlgbm_tunning_grid = {\n    'boosting_type': ['gbdt'],\n    'class_weight': [None, 'balanced'],\n    #'colsample_bytree': np.linspace(.5, 1, 50),\n    #'importance_type': ['split', 'gain'],\n    'learning_rate': [0.003, 0.01, 0.03, 0.1, 0.3, 1, 3],\n    'max_depth': [5, 6, 7, 8, 9, 10],\n    #'min_child_samples': np.arange(10, 50, 1),\n    #'min_child_weight': np.linspace(1e-4, 1, 100),\n    'n_estimators': [500],\n    'num_leaves': [5, 10, 15, 20],\n    'objective': ['binary'],\n    'random_state': [42],\n    #'reg_alpha': np.linspace(.0, 1.0, 50)\n}","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\n\n# Defining variables\nN_CLASSES = len(np.unique(y_train))\nTARGET_NAMES = ['Class_' + str(i) for i in range(1, N_CLASSES + 1)]\n\n# Initializing objects\ndtree = DecisionTreeClassifier()\nforest = RandomForestClassifier()\nlgbm = LGBMClassifier(objective='multiclass', num_class=N_CLASSES)\n\n# Creating set classifiers\nmodel_obj = [dtree, forest, lgbm]\nmodel_names = [type(model).__name__ for model in model_obj]\nmodel_params = [dtree_tunning_grid, forest_tunning_grid, lgbm_tunning_grid]\nset_classifiers = {name: {'model': obj, 'params': param} for (name, obj, param) in zip(model_names, model_obj, model_params)}\n\nprint(f'Classifiers to be trained: \\n\\n{model_names}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4.2.2\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.2.2 Training Models</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Once defined the `set_classifiers` dictionary, you will now see how easy is to train and evaluate all selected models through a `mlcomposer.trainer` class already built for handling the hard work and the code for doing those steps.\n\nFor training the model of TPS May 21 we will use the class `ClassificadorMulticlasse` and apply its `fit()` method setting up some parameters.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.trainer import MulticlassClassifier\n\n# Initializing object and training models\ntrainer = MulticlassClassifier()\ntrainer.fit(set_classifiers, X_train, y_train, random_search=False, cv=5, n_jobs=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Easy like that! Now we have both models trained inside the `trainer` object. Let's dive into another section for evaluating models using the same object in an easy and interpretable way.\n\n>Note: Applying random search here it's expensive. We can try it in future versions.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4.2.3\"></a>\n<font color=\"dimgrey\" size=+1.0><b>4.2.3 Evaluating Performance</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"By now we saw that mlcomposer package has a classe caled ClassificadorMulticlasse with a `fit()` method that allows training multiple models at once as long as they are specified on `set_classifiers()` dictionary corretly. After training them, what can we do? It's time to show two excellent functions for extracting a handful and complete report for metrics in multiclass classification problem and plotting metrics in a beautiful chart.\n\nWe're talking of `evaluate_performance()` and `plot_metrics()` methods from the trainer object. Let's see what we can get from them.","metadata":{}},{"cell_type":"code","source":"# Extracting a complete performance report in a DataFrame format\nmetrics = trainer.evaluate_performance(X_train, y_train, X_val, y_val, target_names=TARGET_NAMES)\nmetrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's seems our models didn't perform well for this multiclassification task. Let's see it in a matplotlib chart.","metadata":{}},{"cell_type":"code","source":"# Plotting metrics\ntrainer.plot_metrics(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, it's clearly that for classic classification metrics like accuracy, precision, recall and f1-score our candidate models really perform poorly. Even though, let's explore other functionalities from mlcomposer by plotting a customized confusion matrix.","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Confusion Matrix_**\n___","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# Plotting a confusion matrix for training and validation data\ntrainer.plot_confusion_matrix(classes=TARGET_NAMES)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By the way, the matrix from the left shows that the models kindly perform well only for the *Class_2* category and this is something we can investigate in the future.\n\nFor the sake of the art, let's take a look at the feature importances for each model by using another useful mlcomposer method called `plot_feature_importance()`.","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Feature Importances_**\n___","metadata":{}},{"cell_type":"code","source":"# Plotando import√¢ncia das features\ntrainer.plot_feature_importance(features=TEST_FEATURES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's seems like *feature_38* is at the top for all models.","metadata":{}},{"cell_type":"markdown","source":"___\n* **_Extracting Log Loss_**\n___","metadata":{}},{"cell_type":"markdown","source":"By now, for completing the submission for this task, we can create a code for extracting the log loss metrics for each trained model. After that, we can compare them and select the best one for submitting on the test sample.","metadata":{}},{"cell_type":"code","source":"# Importing modules\nfrom sklearn.metrics import log_loss\n\n# Defining variables\ny_val_encoded = pd.get_dummies(y_val)\n\n# Iterating for each trained model on trainer class\nfor name in model_names:\n    model = trainer.get_estimator(model_name=name)\n    \n    # Predicting score for validation set\n    val_probas = model.predict_proba(X_val)\n    val_loss = log_loss(y_val_encoded, val_probas)\n    print(f'Model: {name} - Log Loss: {round(val_loss, 5)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>5. Hyperparameter Tunning</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"Well, after training some baseline models and looking at its performances, it would be a good idea to explore some tools for tunning hyperparameters for a good candidate model in order to extract the best combination for highest performance on data for this task. So, we're going to use the `LightGBM` trained model for exploring some of its hyperparameters through a `RandomizedSearchCV` application. We will also set the optimization metrics to be equal to `neg_log_loss` for finding the best model for this task.","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setting up hyperparameters for LightGBM\nlgbm_tunning_grid = {\n    'boosting_type': ['gbdt'],\n    'class_weight': [None, 'balanced'],\n    #'colsample_bytree': np.linspace(.5, 1, 50),\n    'importance_type': ['split', 'gain'],\n    'learning_rate': [0.003, 0.01, 0.03, 0.1, 0.3, 1, 3],\n    'max_depth': [5, 6, 7, 8, 9, 10],\n    #'min_child_samples': np.arange(10, 50, 1),\n    #'min_child_weight': np.linspace(1e-4, 1, 100),\n    'n_estimators': [500, 650, 700, 800],\n    'num_leaves': [5, 10, 15, 20],\n    'objective': ['binary'],\n    'random_state': [42],\n    'reg_alpha': np.linspace(.0, 1.0, 50)\n}\n\n# Initializing a new model and applying random search\nlgbm = LGBMClassifier()\nrnd_search = RandomizedSearchCV(lgbm, lgbm_tunning_grid, scoring='neg_log_loss', cv=5, verbose=10,\n                                random_state=42, n_jobs=5)\nrnd_search.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting the best estimator\nbest_model = rnd_search.best_estimator_\nprint(f'Best hyperparameters: \\n{rnd_search.best_params_}')\n\n# Computing log loss for the best model\ny_probas = best_model.predict_proba(X_val)\nloss = log_loss(y_val_encoded, y_probas)\nprint(f'\\nLoss after hyperparameter tunning: {round(loss, 5)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5.1\"></a>\n<font color=\"dimgrey\" size=+1.0><b>5.1 Feature Selection</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"On another trial for improving performance, let's use the useful class from mlcomposer called `FeatureSelection`. The `transform()` method of this class used the `feature_importances` list result for selecting the dataset columns based on a `k` parameter passed as an argument. When we put this on a pipeline and applies RandomizedSearchCV, it's possible to tune this `k` parameter in order to find the best combination for a optimization rule.","metadata":{}},{"cell_type":"markdown","source":"For this last approach, let's use the full data for training and building a final model. For result comparison, we will only use the `neg_log_loss` metric obtained by cross validation on this full set.","metadata":{}},{"cell_type":"code","source":"# Importing class\nfrom mlcomposer.transformers import FeatureSelection\n\n# Concatenating data\nX = X_train.append(X_val)\ny = np.concatenate([y_train, y_val])\n\n# Extracting feature importances for model trained just before\nfeature_importance = best_model.feature_importances_\n\n# Creating a tunning pipeline if FeatureSelection class\ntunning_pipeline = Pipeline([\n    ('selector', FeatureSelection(feature_importance, k=len(TEST_FEATURES))),\n    ('model', best_model)\n])\n\n# Deifning a hyparparemeter search for the tunning pipeline (k hyperparmeter only)\ntunning_param_grid = {\n    'selector__k': np.arange(5, len(TEST_FEATURES) + 1)\n}\n\n# Defining random search and training it\ntunning_search = RandomizedSearchCV(tunning_pipeline, tunning_param_grid, scoring='neg_log_loss', cv=5,\n                                    n_jobs=5, verbose=10, random_state=42)\ntunning_search.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How about the best hyperparameters for the feature selection pipeline?","metadata":{}},{"cell_type":"code","source":"# Best params\ntunning_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's seems like one feature was discarted from the final model.","metadata":{}},{"cell_type":"code","source":"# Results\nfinal_model = tunning_search.best_estimator_\nfinal_log_loss = -tunning_search.best_score_\nprint(f'Log loss using cross validation: {round(final_log_loss, 5)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font color=\"darkslateblue\" size=+2.5><b>6. Submitting Results</b></font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC</a>","metadata":{}},{"cell_type":"markdown","source":"After training, evaluating and selecting the best model for this task, we can use the test data for computing the scores for each element.","metadata":{}},{"cell_type":"code","source":"# Extracting scores for test data\nX_test = test_prep_pipe.fit_transform(df_test)\ntest_proba = final_model.predict_proba(X_test)\n\n# Generating submition dataset\nfor i in range(N_CLASSES):\n    df_test['Class_' + str(i + 1)] = test_proba[:, i]\n    \ndf_sub = df_test.loc[:, ['id'] + TARGET_NAMES]\ndf_sub.to_csv('initial_sub.csv', index=False)\ndf_sub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please tell me what do you think about `xplotter` and `mlcomposer` packages and leave here a comment or a upvote. Your opinion is really important and I'm really excited to show you new implementations on those packages.\n\n* **xplotter on Github:** https://github.com/ThiagoPanini/xplotter\n* **xplotter on PyPI:** https://pypi.org/project/xplotter/\n\n\n* **mlcomposer on Github:** https://github.com/ThiagoPanini/mlcomposer\n* **mlcomposer on PyPI:** https://pypi.org/project/mlcomposer/\n___\n\n<font size=\"+1\" color=\"black\"><b>You can also visit my other kernels by clicking on the buttons</b></font><br>\n\n<a href=\"https://www.kaggle.com/thiagopanini/pycomp-predicting-survival-on-titanic-disaster\" class=\"btn btn-primary\" style=\"color:white;\">Titanic EDA</a>\n<a href=\"https://www.kaggle.com/thiagopanini/pycomp-exploring-and-modeling-housing-prices\" class=\"btn btn-primary\" style=\"color:white;\">Housing Prices</a>\n<a href=\"https://www.kaggle.com/thiagopanini/predicting-restaurant-s-rate-in-bengaluru\" class=\"btn btn-primary\" style=\"color:white;\">Bengaluru's Restaurants</a>\n<a href=\"https://www.kaggle.com/thiagopanini/sentimental-analysis-on-e-commerce-reviews\" class=\"btn btn-primary\" style=\"color:white;\">Sentimental Analysis E-Commerce</a>","metadata":{}}]}