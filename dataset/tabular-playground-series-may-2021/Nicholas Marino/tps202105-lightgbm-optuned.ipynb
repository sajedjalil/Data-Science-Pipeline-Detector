{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading packages and modules","metadata":{}},{"cell_type":"code","source":"# core packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\npd.options.display.max_columns = None\n\n# feature engineering\nfrom sklearn.cluster import KMeans\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import SparsePCA\nimport umap\nfrom sklearn.pipeline import Pipeline\n\n# modelling\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\n\n# model evaluation\nfrom sklearn.metrics import classification_report, fbeta_score, roc_auc_score\n\n# hyperparameter optimization\nimport optuna\n\n# understanding\nfrom lightgbm import plot_importance, plot_metric\nimport shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining utility functions","metadata":{}},{"cell_type":"code","source":"# function to instantiate the model\ndef instantiate_model(model_hyperparameters):\n    # creating the model instance with the respective hyperparameters\n    model = LGBMClassifier(**model_hyperparameters)\n    # returning the model\n    return model\n\n\n# function to instantiate a pipeline if the embedding is selected\ndef create_pipeline(try_embedding, embedding_comps, scaler_embedding):\n    # creating a StandardScaler instance if this was the selected option, else use the MinMaxScaler\n    if scaler_embedding:\n        scaler = StandardScaler()\n    else:\n        scaler = MinMaxScaler()\n    \n    # creating the selected embedding instance \n    if try_embedding == 'UMAP':\n        embedding_instance = umap.UMAP(n_components = embedding_comps, random_state = 42)\n    elif try_embedding == 'PCA':\n        embedding_instance = PCA(n_components = embedding_comps, random_state = 42)\n    else:\n        embedding_instance = SparsePCA(n_components = embedding_comps, random_state = 42)\n    \n    # creating a pipeline instance\n    pipeline_instance = Pipeline(steps = [('scaler', scaler), ('embedding', embedding_instance)])\n    \n    # returning the pipeline instance\n    return pipeline_instance\n\n\n# function to generate predictions\ndef generate_predictions(model, X_values):\n    # predicting probabilities\n    predicted_probabilities = model.predict_proba(X_values)\n    # predicting class values\n    predicted_classes = model.predict(X_values)\n    # returning predictions\n    return predicted_probabilities, predicted_classes\n\n\n# function to calculate the desired metrics from the predictions\ndef extract_metrics(y_values, predicted_probabilities, predicted_classes, report = False):\n    # printing the classification report if necessary\n    if report:\n        print('Classification report:')\n        print(classification_report(y_true = y_values, y_pred = predicted_classes, zero_division = 0))\n    \n    # calculating the macro F1 score\n    f_macro = fbeta_score(y_true = y_values, y_pred = predicted_classes, average = 'macro', beta = 1)\n    \n    # calculating the micro F1 score\n    f_micro = fbeta_score(y_true = y_values, y_pred = predicted_classes, average = 'micro', beta = 1)\n    \n    # calculating the ROC AUC score for OVR\n    auc_ovr = roc_auc_score(y_true = y_values, y_score = predicted_probabilities, multi_class = 'ovr', average = 'macro')\n    \n    # calculating the ROC AUC score for OVO\n    auc_ovo = roc_auc_score(y_true = y_values, y_score = predicted_probabilities, multi_class = 'ovo', average = 'macro')\n    \n    # returning metrics\n    return {'f1_macro': f_macro, 'f1_micro': f_micro, 'auc_ovr': auc_ovr, 'auc_ovo': auc_ovo}\n\n\n\n# function to evaluate a stratified k-fold to the data given a set of hyperparameters\ndef evaluate_stratified_kfold(X, y, n_folds, hyperparameters, pruning, weighting, try_embedding, embedding_comps, scaler_embedding, use_kmeans_features, final_score = False, val_report = False):\n    \n    # creating a list to store the loss for each fold\n    fold_losses = []\n    \n    # creating a list to store the auc one-vs-one for each fold\n    ovo_losses = []\n    \n    # creating objects needed for the final fit\n    if final_score:\n        ## dictionary to store the models from each of the folds\n        models = dict()\n        ## dictionary to store each of the pipelines\n        pipelines = dict()\n        ## creating a list to add the predicted probabilities from each of the models\n        summed_probabilities = np.zeros(shape = (X_test.shape[0], 4))\n    \n    # defining the weighting scheme\n    ## if the selected weighting scheme is 0, turn it of\n    if weighting == 0:\n        weight_dictionary = {'class_weight': None}\n    ## if the selected scheme is one, than this is close to the balanced choice\n    elif weighting == 1:\n        weight_dictionary = {'class_weight': 'balanced'}\n    ## for all other values, downweight the weights by the selected ratio\n    else:\n        weight_dictionary = {'class_weight': {label: np.ceil(weight / weighting) for label, weight in default_weights.items()}}\n    \n    # updating the hyperparameter dictionary with the selected class_weights argument\n    hyperparameters.update(weight_dictionary)\n    \n    # instantiating the K fold\n    skf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = 42)\n    \n    # printing the hyperparameters\n    print(f'\\nThese are the hyperparameters that will be used across folds for this trial:\\n{hyperparameters}')\n    print(f'\\nThese are the preprocessing options that will be used across folds:\\ntry_embedding: {try_embedding}; embedding_comps: {embedding_comps}; scale_embedding: {scaler_embedding}; use_kmeans_features: {use_kmeans_features}')\n    \n    # running through each of the folds\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        # starting the loop\n        print(f'\\n--------------------------------------- Starting fold {fold + 1} ---------------------------------------')\n        \n        # creating a copy of the X and X_test dataframes\n        X_trial = X.copy()\n        X_test_trial = X_test.copy()\n    \n        # adding the kmeans features to the train and test dataframe if this option was selected\n        if use_kmeans_features:\n            X_trial['feature_kmeans'] = feature_kmeans_train\n            X_test_trial['feature_kmeans'] = feature_kmeans_test\n        \n        # getting the train and validation data for this fold\n        X_train_fold, X_val_fold, y_train_fold, y_val_fold = X_trial.iloc[train_idx], X_trial.iloc[val_idx], y[train_idx], y[val_idx]\n        \n        # training the embedding and transforming the data if this was selected\n        if try_embedding is not None:\n            # creating the pipeline for the preprocessing\n            print('Creating the pipeline for applying the embedding.')\n            pipeline = create_pipeline(try_embedding = try_embedding, embedding_comps = embedding_comps, scaler_embedding = scaler_embedding)\n            # training the pipeline\n            print('Training the pipeline with the embedding.')\n            pipeline.fit(X_train_fold)\n            # applying the embedding to the data\n            print('Applying the embedding.')\n            X_train_fold = pipeline.transform(X_train_fold)\n            X_val_fold = pipeline.transform(X_val_fold)\n        \n        # creating a model instance\n        clf = instantiate_model(model_hyperparameters = hyperparameters)\n            \n        # fitting the model\n        clf.fit(X = X_train_fold, \n                y = y_train_fold, \n                eval_set = [(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)], \n                eval_metric = 'multi_logloss', \n                early_stopping_rounds = 10, \n                verbose = False,\n                #callbacks = [pruning] # only works if there is a single metric\n               )\n        \n        # generating the predictions for the train and validation sets\n        train_probas, train_classes = generate_predictions(model = clf, X_values = X_train_fold)\n        val_probas, val_classes = generate_predictions(model = clf, X_values = X_val_fold)\n        \n        # extracting the metrics for the training and validation sets\n        train_metrics = extract_metrics(y_values = y_train_fold, predicted_probabilities = train_probas, predicted_classes = train_classes, report = False)\n        val_metrics = extract_metrics(y_values = y_val_fold, predicted_probabilities = val_probas, predicted_classes = val_classes, report = val_report)\n        \n        # extracting the fold loss\n        fold_losses.append(clf.best_score_['valid_1']['multi_logloss'])\n        \n        # extracting the auc one-vs-one values\n        ovo_losses.append(val_metrics[\"auc_ovo\"])\n        \n        # extracting all necessary information if the choice is the final fit\n        if final_score:\n            ## saving the model in the dictionary\n            models[f'model_{fold + 1}'] = clf\n            if try_embedding is not None:\n                ## applying the pipeline to the test data\n                X_test_trial = pipeline.transform(X_test_trial)\n                ## saving the pipeline\n                pipelines[f'pipeline_{fold + 1}'] = pipeline\n            ## generating predictions on the test set\n            test_probas, _ = generate_predictions(model = clf, X_values = X_test_trial)\n            # summing the test dataset probabilities\n            summed_probabilities = summed_probabilities + test_probas\n            \n        # printing the fold results\n        print(f'Loss (Training|Validation)       : {np.round(clf.best_score_[\"training\"][\"multi_logloss\"], 5)} | {np.round(clf.best_score_[\"valid_1\"][\"multi_logloss\"], 5)}')\n        print(f'F1 Macro (Training|Validation)   : {np.round(train_metrics[\"f1_macro\"], 5)} | {np.round(val_metrics[\"f1_macro\"], 5)}')\n        print(f'AUC OvO (Training|Validation)    : {np.round(train_metrics[\"auc_ovo\"], 5)} | {np.round(val_metrics[\"auc_ovo\"], 5)}')\n    \n    # printing the results obtained for this function evaluation\n    print(f'\\nEnd of Training!\\nAverage loss: {np.mean(fold_losses)} | Average AUC OvO: {np.mean(ovo_losses)}')\n    print('-----------------------------------------------------------------------------------------------\\n')\n    # returning the results\n    ## scores and model if this is for the final fit\n    if final_score:\n        if try_embedding is not None:\n            return (np.mean(fold_losses), np.mean(ovo_losses), models, pipelines, summed_probabilities)\n        else:\n            return (np.mean(fold_losses), np.mean(ovo_losses), models, summed_probabilities)\n    ## else, only the scores\n    else:\n        return (np.mean(fold_losses), np.mean(ovo_losses))\n\n    \n# function to perform the hyperparameter optimization\ndef objective(trial):\n    \n    ############################################## HYPERPARAMETER SELECTION ##############################################\n    \n    # setting up the weighting scheme for each instance\n    weighting_scheme = trial.suggest_int('weighting_scheme', 0, 6)\n    \n    # defining the search space of model hyperparameters\n    trial_hyperparameters = {\n        'boosting_type'            : trial.suggest_categorical('boosting_type', ['gbdt', 'goss']),\n        'num_leaves'               : trial.suggest_int('num_leaves', 3, 16),\n        'max_depth'                : trial.suggest_int('max_depth', 3, 16),\n        'learning_rate'            : trial.suggest_float('learning_rate', 0.001, 0.2),\n        'n_estimators'             : trial.suggest_int('n_estimators', 50, 1500),\n        'min_child_samples'        : trial.suggest_int('min_child_samples', 20, 1000),\n        'min_child_weight'         : trial.suggest_float('min_child_weight', 1e-4, 1e-2),\n        'colsample_bytree'         : trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'colsample_bynode'         : trial.suggest_float('colsample_bynode', 0.4, 1.0),\n        'extra_trees'              : trial.suggest_categorical('extra_trees', [False, True]),\n        'reg_alpha'                : trial.suggest_float('reg_alpha', 0.0, 10.0),\n        'reg_lambda'               : trial.suggest_float('reg_lambda', 0.0, 10.0)\n    }\n    \n    \n    # adding further hyperparameters to the search space if the boosting type is goss\n    if trial_hyperparameters['boosting_type'] == 'goss':\n        # defining the hyperparameters specific to the gooss boosting\n        goss_hyperparameters = {'top_rate'      : trial.suggest_float('top_rate', 0.1, 0.4),\n                                'other_rate'    : trial.suggest_float('other_rate', 0.05, 0.4)}\n        # updating the trial hyperparameters dictionary with those coming from goss\n        trial_hyperparameters.update(goss_hyperparameters)\n    \n    \n    # adding bagging if boosting is not goos\n    if trial_hyperparameters['boosting_type'] != 'goss':\n        # defining the hyperparameters specific to the gooss boosting\n        additional_hyperparameters = {'subsample'       : trial.suggest_float('subsample', 0.4, 1.0),\n                                      'subsample_freq'  : trial.suggest_int('subsample_freq', 0, 50)}\n        # updating the trial hyperparameters dictionary with those coming from goss\n        trial_hyperparameters.update(additional_hyperparameters)\n    \n\n    # defining the standard hyperparameter dictionary\n    standard_hyperparameters = {'objective'   : 'multiclass',\n                                'metric'      : 'multi_logloss',\n                                'num_class'   : 4, \n                                'random_state': 42,\n                                'silent'      : True,\n                                'verbosity'   : -1\n                               }\n    \n    # updating the trial hyperparameter dictionary\n    trial_hyperparameters.update(standard_hyperparameters)\n    \n    ############################################### PREPROCESSING SELECTION ##############################################\n    \n    # defining whether we are using an embedding layer\n    use_embedding = trial.suggest_categorical('use_embedding', [None, 'UMAP', 'PCA', 'SparsePCA'])\n    \n    # defining the number of components in the embedding layer if it is chosen as well as the scaler\n    if use_embedding is not None:\n        embedding_size = trial.suggest_int('embedding_size', 2, 15)\n        use_std_scaler = trial.suggest_categorical('use_std_scaler', [False, True])\n        \n    # defining whether to use the feature from the kmeans clustering if the embedding is not used\n    if use_embedding is None:\n        use_kmeans_features = trial.suggest_categorical('use_kmeans_features', [False, True])\n    \n    ################################################# CALLBACK DEFINITION ################################################\n    \n    # instantiating a prunning call back - won't have an effect here as we are optimizing more than one metric\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, metric = 'multi_logloss', valid_name = 'valid_1')\n    \n    ##################################################### MODEL FIT ######################################################\n    \n    # fitting the model through stratified k-fold\n    trial_loss, trial_auc = evaluate_stratified_kfold(X, y, n_folds = K, hyperparameters = trial_hyperparameters,\n                                                      weighting = weighting_scheme,\n                                                      pruning = pruning_callback, \n                                                      final_score = False,\n                                                      val_report = False,\n                                                      try_embedding = use_embedding,\n                                                      embedding_comps = None if use_embedding is None else embedding_size,\n                                                      scaler_embedding = None if use_embedding is None else use_std_scaler,\n                                                      use_kmeans_features = use_kmeans_features if use_embedding is None else False)\n    \n    # returning the final trial scores\n    return trial_auc, trial_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"# training data\ntrain = pd.read_csv(filepath_or_buffer = '../input/tabular-playground-series-may-2021/train.csv')\n\n# testing data\ntest = pd.read_csv(filepath_or_buffer = '../input/tabular-playground-series-may-2021/test.csv')\n\n# sample submission data\nsubmission = pd.read_csv(filepath_or_buffer = '../input/tabular-playground-series-may-2021/sample_submission.csv')\n\n# printing the shape of the data\nprint(f'Train data shape: {train.shape}')\nprint(f'Test data shape: {test.shape}')\nprint(f'Sample submission data shape: {submission.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the data","metadata":{}},{"cell_type":"markdown","source":"Features with values below 0.","metadata":{}},{"cell_type":"code","source":"# feature column names\nfeature_columns = [column for column in train.columns if 'feature' in column]\n\n# features where the minimum is below 0\ntrain_below = [feature for feature in feature_columns if train[feature].min() < 0]\ntest_below = [feature for feature in feature_columns if test[feature].min() < 0]\n\n# number of instances below zero for each column in the training set\n## creating a dictionary to store results\ndict_train_below = dict()\n## loop over the columns of the train set\nfor column in train_below:\n    # counting instances from each column\n    counts = dict(train[column].value_counts())\n    dict_train_below[column] = {k: v for k, v in counts.items() if k < 0}\n    \n# number of instances below zero for each column in the test set\n## creating a dictionary to store results\ndict_test_below = dict()\n## loop over the columns of the train set\nfor column in test_below:\n    # counting instances from each column\n    counts = dict(test[column].value_counts())\n    dict_test_below[column] = {k: v for k, v in counts.items() if k < 0}\n    \n# printing the number of instances below 0 for each dataset\nprint(f'Number of instances below zero for each of the columns in the train set:\\n{dict_train_below}.')\nprint(f'\\nNumber of instances below zero for each of the columns in the test set:\\n{dict_test_below}.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data","metadata":{}},{"cell_type":"markdown","source":"Separating the target from the features.","metadata":{}},{"cell_type":"code","source":"# getting the feature columns\n## copying the original dataframe\nX = train.copy()\n## extracting the columns\nX = X[feature_columns]\n\n# getting the target\ntarget = train.target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding the target.","metadata":{}},{"cell_type":"code","source":"# instantiating the label encoder\nle = LabelEncoder()\n# fitting the label encoder\ny = le.fit_transform(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting up the default weighting scheme.","metadata":{}},{"cell_type":"code","source":"# unpacking the counts for each of the labels\nlabels, counts = np.unique(y, return_counts = True)\n# setting a weighting scheme as the ratio between the maximum count and each count\ninverse_weights = [np.ceil(np.max(counts) / count) for count in counts]\n# creating a default weight dictionary for each label\ndefault_weights = dict(zip(labels, inverse_weights))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the test data.","metadata":{}},{"cell_type":"code","source":"# copying the dataframe\nX_test = test.copy()\n\n# selecting the columns\nX_test = X_test[feature_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"Creating a feature based on KMeans.","metadata":{}},{"cell_type":"code","source":"# creating a scaler to fit to the data\nscaler_cluster = StandardScaler()\n\n# fitting the scaler to the data\nX_scaled_cluster = scaler_cluster.fit_transform(X)\n\n# creating a list to store the inertia\nsse = []\n\n# setting the clusters values to be tried\ncluster_values = [20, 30] + list(range(33, 80, 3))\n\n# lopping over the number of possible clusters\nfor n_cluster in cluster_values:\n    # creating a kmeans instance\n    kmeans = KMeans(n_clusters = n_cluster)\n    # fitting the kmeans to the data\n    kmeans.fit(X_scaled_cluster)\n    # extracting the inertial\n    sse.append(kmeans.inertia_)\n    \n# plotting the results of the kmeans\nplt.plot(cluster_values, sse)\nplt.xticks(cluster_values)\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a pipeline to fit kmeans to the data\nkmeans_pipeline = Pipeline(steps = [('scaler', StandardScaler()), ('kmeans', KMeans(n_clusters = 57, random_state = 42))])\n\n# training the kmeans clustering\nkmeans_pipeline.fit(X)\n\n# creating the feature based on the kmeans\n## for the training data\nfeature_kmeans_train = kmeans_pipeline.predict(X)\n## for the test data\nfeature_kmeans_test = kmeans_pipeline.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter Tunning","metadata":{}},{"cell_type":"markdown","source":"Subsetting best features (from version 5 of this notebook).","metadata":{}},{"cell_type":"code","source":"# features with most votes\nfeature_subset = ['feature_38', 'feature_6', 'feature_14', 'feature_15', 'feature_28', 'feature_31', 'feature_34', 'feature_24', 'feature_9', 'feature_11', \n                  'feature_12', 'feature_18', 'feature_16', 'feature_23', 'feature_25', 'feature_37', 'feature_35']\n## features that were also selected: feature_2, feature_1, feature_19, feature_22, feature_42, feature_48, feature_33, feature_7, feature_10, feature_17, feature_20, feature_43, feature_46\n\n# subsetting both dataframe\nX = X[feature_subset]\nX_test = X_test[feature_subset]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the number of splits\nK = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a study\nstudy = optuna.create_study(directions = ['maximize', 'minimize'], pruner = optuna.pruners.MedianPruner())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# running the study\nstudy.optimize(func = objective, n_trials = 100, timeout = 60 * 60 * 7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best trial for each combination of metrics\noptuna.visualization.plot_pareto_front(study, target_names = ['AUC (One vs. One)', 'Multi Logloss'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best hyperparameters for multi_logloss\noptuna.visualization.plot_param_importances(study, target = lambda x: x.values[1], target_name = 'Multi Logloss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best hyperparameters for multi_logloss\noptuna.visualization.plot_param_importances(study, target = lambda x: x.values[0], target_name = 'AUC (One vs One)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trajectories for each hyperparameter combination for multi logloss\noptuna.visualization.plot_parallel_coordinate(study, target = lambda x: x.values[1], target_name = 'Multi Logloss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trajectories for each hyperparameter combination for AUC (one vs one)\noptuna.visualization.plot_parallel_coordinate(study, target = lambda x: x.values[0], target_name = 'AUC (One vs One)')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the tuned model for submission ","metadata":{}},{"cell_type":"code","source":"# extracting the best hyperparameter combination\nbest_hyperparameters = study.best_trials[0].params\nprint(f'The selected hyperparameter combination was:\\n{best_hyperparameters}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unpacking best hyperparameters and removing the keys that are not needed in this dictionary\n## extracting the weighting scheme\nweighting_scheme = best_hyperparameters['weighting_scheme']\n## deleting this key\nbest_hyperparameters.pop('weighting_scheme')\n\n## extracting the embedding key\nuse_embedding = best_hyperparameters['use_embedding']\n## deleting this key\nbest_hyperparameters.pop('use_embedding')\n\n## extracting the keys associated to the embedding\nif use_embedding is not None:\n    ## getting the embedding dimensions\n    embedding_size = best_hyperparameters['embedding_size']\n    best_hyperparameters.pop('embedding_size')\n    ## getting the scaler type\n    use_std_scaler = best_hyperparameters['use_std_scaler']\n    best_hyperparameters.pop('use_std_scaler')\n    \n## extracting the keys associated to the kmeans features\nif use_embedding is None:\n    ## getting the embedding dimensions\n    use_kmeans_features = best_hyperparameters['use_kmeans_features']\n    best_hyperparameters.pop('use_kmeans_features')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## fitting the model depending on the embedding\nif use_embedding is None:\n    # fitting the model with the selected hyperparameters and configurations\n    loss, auc_ovo, models, predictions = evaluate_stratified_kfold(X = X, y = y, n_folds = K, hyperparameters = best_hyperparameters,\n                                                                   pruning = None, final_score = True, val_report = True,\n                                                                   weighting = weighting_scheme,\n                                                                   try_embedding = use_embedding,\n                                                                   embedding_comps = None if use_embedding is None else embedding_size,\n                                                                   scaler_embedding = None if use_embedding is None else use_std_scaler,\n                                                                   use_kmeans_features = use_kmeans_features if use_embedding is None else False)\nelse:\n    # fitting the model with the selected hyperparameters and configurations\n    loss, auc_ovo, models, pipelines, predictions = evaluate_stratified_kfold(X = X, y = y, n_folds = K, hyperparameters = best_hyperparameters,\n                                                                              pruning = None, final_score = True, val_report = True,\n                                                                              weighting = weighting_scheme,\n                                                                              try_embedding = use_embedding,\n                                                                              embedding_comps = None if use_embedding is None else embedding_size,\n                                                                              scaler_embedding = None if use_embedding is None else use_std_scaler,\n                                                                              use_kmeans_features = use_kmeans_features if use_embedding is None else False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature importance for each of the models\n## for when the embedding is selected\nif use_embedding is not None:\n    for pipeline_id, model_id in zip(pipelines.keys(), models.keys()):\n        # instantiating the explainer for the model\n        explainer = shap.TreeExplainer(model = models[model_id])\n        # transforming the data\n        X_shap = pipelines[pipeline_id].transform(X)\n        # extracting the shap values\n        shap_values = explainer.shap_values(X_shap)\n        # plotting the model\n        shap.summary_plot(shap_values, X_shap)\n## when the embedding shouldn't be used\nelse:  \n    # feature importance for each of the models\n    for model_id in models.keys():\n        # instantiating the explainer for the model\n        explainer = shap.TreeExplainer(model = models[model_id])\n        # creating a copy of the dataframe\n        X_shap = X.copy()\n        # adding the kmeans feature if it is needed\n        if use_kmeans_features:\n            X_shap['feature_kmeans'] = feature_kmeans_train\n        # extracting the shap values\n        shap_values = explainer.shap_values(X_shap)\n        # plotting the model\n        shap.summary_plot(shap_values, X_shap)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lightgbm built-in feature importance\nfor model_id in models.keys():\n    plot_importance(models[model_id], figsize = (5, 6), max_num_features = 15, title = f'Feature Importance for model: {model_id}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lightgbm training history\nfor model_id in models.keys():\n    plot_metric(models[model_id], title = f'Training history for model: {model_id}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inputing the probabilities to the submission file","metadata":{}},{"cell_type":"code","source":"# putting the predictions into their respective columns\nsubmission[['Class_1', 'Class_2', 'Class_3', 'Class_4']] = predictions / K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the submission file","metadata":{}},{"cell_type":"code","source":"submission.to_csv(path_or_buf = 'submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}