{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline \nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport seaborn as sns\nprint(\"seaborn version: {}\". format(sns.__version__))\n\nimport sklearn # machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# About\nThis notebook is my EDA for the Tabular Playground Series May 2021. No modelling yet.\n\nLet's look at training and testing data.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ndf_test = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')\nsample_submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv') #the sample submission predicts everybody survived\ndf_all = df_train.append(df_test, ignore_index = True) # created because sometimes it is convenient to work on train and test set together","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe().transpose()\\\n        .drop(\"id\")\\\n        .style.bar(subset=['mean','std'])\\\n        .background_gradient(subset=['max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe().transpose()\\\n        .drop(\"id\")\\\n        .style.bar(subset=['mean','std'])\\\n        .background_gradient(subset=['max'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of rows with any values below zero\ndisplay(df_train[(df_train.drop([\"target\"],axis=1) < 0).any(1)].shape)\ndf_test[(df_test < 0).any(1)].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the target variable\ndf_train.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What do we have here? **Training data** with 50 anonymized numerical features and 100000 training examples, identified with an id. There are no missing values. Data type is integer. \n\nThe **target** is categorical with four values. Data type object -> str. Class_2 is dominant with more than half of the examples. Class_1 has the fewest examples (less than 10%). We need to take care about this when modelling. \n\nMost features have a minimum values of 0. Features 19, 30, 31, 32, 35, 38, 39 and 42 have negative values. There are in total 227 training examples with one or more negative values, which is 0.23%.  The maximum value for any feature is 66.\n\nThe features differ in terms of their mean and standard deviation. So depending on my choice of model later on, mean normalization will be necessary. \n\nThe **test data** consistis of 50000 examples. There are in total 109 training examples with negative values, which is 0,22%. Features 14, 19, 30, 31, 32, 38 and 39 have negative values. This is different from the training set!\n\nNote to myself: the percentage of rows with negative values is very low. Check model performance with and without the negative rows.\n\nLet's look at the **distributions**.","metadata":{}},{"cell_type":"code","source":"109*2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code for mean normalization. all features are centered around 0 and have variance in the same order\n#scaler = StandardScaler() \n#df_temp = pd.DataFrame(data=scaler.fit_transform(df_train.drop([\"target\"],axis=1)),columns=df_train.drop([\"target\"],axis=1).columns)\n#df_temp.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fsize = (18,18)\ndf_train.drop([\"id\",\"target\"],axis=1).hist(figsize=fsize)\nplt.suptitle(\"Distributions in training data (df_train)\", fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fsize = (18,18)\ndf_test.drop([\"id\"],axis=1).hist(figsize=fsize)\nplt.suptitle(\"Distributions in test data (df_test)\", fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see here that all distributions are right skewed. For feature_38 there is a visible difference in distribution in train and test data. ","metadata":{}},{"cell_type":"code","source":"# choose feature for a closer look\ncurrent_feature = \"feature_8\"\ncurrent_df = df_train\nprint(current_feature)\n\nfig = plt.figure(figsize=fsize) # create figure\nfsize = (16,12)\nax0 = fig.add_subplot(2, 1, 1) # add subplot 1 (2 rows, 1columns, first plot)\nax1 = fig.add_subplot(2, 1, 2)\n#current_df[current_feature].hist(figsize=fsize, ax=ax0)\nsns.histplot(x=current_feature, data=current_df, ax=ax0) # just an alternative with sns instead of plt\nsns.boxplot(x=current_feature, data=current_df, ax=ax1)\nplt.show()\n\nprint(current_df[current_feature].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's check if there are as many unique feature values as the range of values\ndf_features = df_train.drop([\"id\",\"target\"],axis=1) # use df_all, df_test here depending on what you want to see\n\nfeature_range = df_features.max() - df_features.min()\nno_unique_values = df_features.nunique()\n\npd.DataFrame(data={\"feature_range\": feature_range, \"no_unique_values\": no_unique_values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n\n- There are no binary features\n\n- There are features with few unique values (low cardinality), and with many unique values (high cardinality). Is it safe to guess that features with \"few\" unique values are categorical features and those with many are continous features???\n\n- When looking at the training and testing data to check if the feature values are \"continous\", I found this to be the case for most features. However there are features where the values go like ...27, 28, 29, 31 (feature_1) or 31, 32, 33, 34, 36, 39 (feature_8). This can be best seen using the sns boxplot.\n\n In some cases the unused feature values are present in the test set (like for feature_1), in other cases only partly (like for feature_8). \n\n I'm not sure what to make of this observation. I wonder if a feature like feature_8 is a real continous feature. Because, if it was a categorical, there should not be any missing labels...?\n\n- About the features with negative values: Either they are categorical features that have been label encoded \"by hand\" to produce the negative values or they are continous features. However I still could not come up with a feature of an eCommerce product that has multiple negative values. I considered, that a negative value just means \"missing data\". But again, this does not explain multiple negative values for a feature. ","metadata":{}},{"cell_type":"markdown","source":"Thanks to OmarVivas who informed us about **duplicates** in this data set. Let's check.\n\nThere are really 4 duplicates in the training set with different target. I will remove this rows before training.\n\nThere are further 6 rows from training set, that are also present in the test set. Remember to check those rows in the prediction. ","metadata":{}},{"cell_type":"code","source":"# look only in training data\ndisplay(df_train[df_train.drop(columns=[\"id\",\"target\"]).duplicated(keep=\"first\")])\ndisplay(df_train[df_train.drop(columns=[\"id\",\"target\"]).duplicated(keep=\"last\")])\n# 44423 - 13230, Class_4 vs Class_2\n# 73244 - 25648, Class_2 vs Class_4\n# 80571 - 44248, Class_4 vs Class_2\n# 89009 - 87104, Class_1 vs Class_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look in combined data, keep=False shows both duplicated rows in the result\nduplicates = df_all[df_all.drop(columns=[\"id\",\"target\"]).duplicated(keep=False)].drop([13230,25648,44248,87104,44423,73244,80571,89009],axis=0)\n#display(df_all[df_all.drop(columns=[\"id\",\"target\"]).duplicated(keep=\"first\")])\n#display(df_all[df_all.drop(columns=[\"id\",\"target\"]).duplicated(keep=\"last\")])\nduplicates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_3 36458 -> 143136\n# class_2 31717 -> 132016\n# class_4 23272 -> 143298\n# Class_2 62190 -> 114770\n# Class_4 81438 -> 120754\n# Class_4 63143 -> 101173","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Baseline submission:","metadata":{}},{"cell_type":"code","source":"# predict everything Class_2\nsample_submission.loc[36458][\"Class_1\"]=0\nsample_submission.loc[36458][\"Class_2\"]=0\nsample_submission.loc[36458][\"Class_3\"]=1\nsample_submission.loc[36458][\"Class_4\"]=0\nsample_submission.to_csv('submission_1.csv', index=False)\n#-> public leaderboard score: 14.62209","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.target.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict like train set probabilites\nsample_submission.Class_1=0.08\nsample_submission.Class_2=0.57\nsample_submission.Class_3=0.21\nsample_submission.Class_4=0.12\nsample_submission.to_csv('submission_2.csv', index=False)\n#-> public leaderboard score: 1.11369","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}