{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Why this Competition?\nThis competition provides an unique oppertunity for Data Science beginners to participate in a Hackathon style challenge. It also provides the unique oppertunities for beginners to get their hands dirty and indulge is practical application of ML and do one of the basic tasks machine learning algorithms are capable of doing:- Classification.\n\nThis competition has the right mix to Catergorical and Numerical features we might expect in a practical problem and this helps us know how to leverage both of thhem in conjugation for a Classification task.\n\n# Problem Statement\nThe goal of this competition is to provide a fun, and approachable for anyone, tabular dataset. These competition will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN.\n\nSo we are sort of dealing with a variation of actual real-world data and here as Data Scientists are expected to predict the Multi-Class Classification based on these features.\n\n## Expected Outcome:-\n* Build a model to predict the category on an eCommerce product given various attributes about the listing.\n* Grading Metric: Multi-Class Log Loss\n\n## Problem Category:-\nFrom the data and objective its is evident that this is a Multi-Class Classification Problem in the Tabular Data format.\n\nSo without further ado, let's now start with some basic imports to take us through this journey:-","metadata":{"id":"congressional-jungle"}},{"cell_type":"code","source":"! pip install --quiet pytorch-tabnet","metadata":{"id":"4qoeWnmlF7GK","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\n\n# Visialisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_validate\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\n\n#Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, log_loss\n\n# Models\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier","metadata":{"id":"heard-gross","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/tabular-playground-series-may-2021'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","metadata":{"id":"speaking-sailing","outputId":"c1bd280f-45d8-444d-e553-c7a3b2bc2cdd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 42","metadata":{"id":"cooked-solution","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"id":"incorrect-anatomy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything()","metadata":{"id":"sustainable-egyptian","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","metadata":{"id":"radio-elite","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Encoding","metadata":{"id":"BuFI85teIFcS"}},{"cell_type":"code","source":"mapping_dict = {'Class_1':1, 'Class_2':2, 'Class_3':3, 'Class_4':4}\ntrain_df['target'] = train_df['target'].map(mapping_dict)","metadata":{"id":"vbieePkTIErZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{"id":"constitutional-found"}},{"cell_type":"code","source":"not_features = ['id', 'target']\nfeatures = []\nfor feat in train_df.columns:\n    if feat not in not_features:\n        features.append(feat)\nprint(features)","metadata":{"id":"exceptional-fitness","outputId":"05819615-fef9-457f-926c-ef3c945564a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = preprocessing.StandardScaler()\nscaler.fit(train_df[features])\ntrain_df[features] = scaler.transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","metadata":{"id":"numerical-purple","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KFold Splits\nBefore we move on to feature engineering, it is always a good idea to perform cross validation splits. In that way, we will not risk any data leakage and would be more certain of the validation set being aptly represenative of the real world unknown data.","metadata":{"id":"normal-warehouse"}},{"cell_type":"code","source":"NUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.target.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","metadata":{"id":"capital-spyware","outputId":"74a4dcfc-8bcd-4731-fb95-20472fedcff3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Features Selection\nWe need to select only the important features for better performance of the model. As unnecessary in best case scenario will not add to any productive calculation of the algorithm or in worst case scenario 'confuse' the model.\n\nTo do the same let's create a wrapper class that has all the built in statistical tests required to perform feature selection and takes some basic inputs from user and spits out the required features.","metadata":{"id":"hcSdq9RsG4WD"}},{"cell_type":"code","source":"# From https://github.com/abhishekkrthakur/approachingalmost\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features","metadata":{"id":"IaqUO5lDGuUz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ufs = UnivariateFeatureSelction(\n    n_features=1.0,\n    problem_type=\"classification\",\n    scoring=\"f_classif\"\n)\n\nufs.fit(train_df[features], train_df['target'].values.ravel())\nselected_features = ufs.return_cols(train_df[features])","metadata":{"id":"XZi6ReeVGuXS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have already shared a baseline notebook [here](https://www.kaggle.com/manabendrarout/soft-voting-ensemble-starter-tps-may21/comments#1290740) if you want to understand the data. For the sake of not repeating myself, let's get started directly with tabnet here...","metadata":{"id":"uniform-consumption"}},{"cell_type":"markdown","source":"# What is Tabnet?\n![Tabnet Google](https://warehouse-camo.ingress.cmh1.psfhosted.org/88a031698031ed731cf65dc79a2f0085dbce4670/68747470733a2f2f6769746875622e636f6d2f74697475313939342f74662d5461624e65742f626c6f622f6d61737465722f696d616765732f5461624e65742e706e673f7261773d74727565 \"Tabnet\")\n\nTabnet is a novel high-performance and interpretable canonical deep tabular data learning architecture. TabNet uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. TabNet outperforms other variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into its global behavior.\n\nTabNet uses a sequential attention mechanism to choose a subset of semantically meaningful features to process at each decision step. Instance-wise feature selection enables efficient learning as the model capacity is fully used for the most salient features, and also yields more interpretable decision making via visualization of selection masks.\n\nFor deatils please refer [this](https://arxiv.org/pdf/1908.07442.pdf) awesome paper by Google.\n\nTabNet has two useful implemnetations in [Pytorch](https://github.com/dreamquark-ai/tabnet) and, by the original authors, in [Tensorflow](https://github.com/google-research/google-research/tree/master/tabnet). In this notebook we will look at the Pytorch implementation which has more community support and benchmarking, and compare our results under default parameters to a Feed-forward Dense Neural Network provided in Scikit-learn, and CatBoost. We will compare the global feature importances described by TabNet and our Catboost and let readers get a sense of the usability and promise of these tools.","metadata":{"id":"monthly-gossip"}},{"cell_type":"code","source":"def get_tabnet_model():\n    classifier = TabNetClassifier(\n      n_d=64, n_a=64, n_steps=5,\n      # gamma=1.5, n_independent=2, n_shared=2,\n      # lambda_sparse=1e-4, momentum=0.1,\n      optimizer_fn=torch.optim.Adam,\n      optimizer_params=dict(lr=2e-2),\n      # scheduler_fn=torch.optim.lr_scheduler.StepLR,\n      # scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n      verbose=1, seed=42)\n\n    return classifier","metadata":{"id":"XbZ9OGBX22bQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 100","metadata":{"id":"FtYCGh7n3Sez","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_clf = None\nbest_score = 10000\nall_clfs = []\n\nfor k in range(NUM_SPLITS):\n    print(f'******************* Staring fold: {k+1} *******************')\n    current_fold = k\n    train_fold = train_df[train_df['kfold'] != current_fold]\n    valid_fold = train_df[train_df['kfold'] == current_fold]\n\n    X_train = train_fold[selected_features].to_numpy()\n    y_train = train_fold['target'].to_numpy().squeeze()\n\n    X_valid = valid_fold[selected_features].to_numpy()\n    y_valid = valid_fold['target'].to_numpy().squeeze()\n\n    classifier = get_tabnet_model()\n\n    classifier.fit(X_train, y_train,\n                 eval_set=[(X_valid, y_valid)],\n                 eval_name=['valid'],\n                 patience=20,\n                 max_epochs=EPOCHS,\n                #  batch_size=1024,\n                #  virtual_batch_size=256,\n                 eval_metric=['logloss'])\n    print(' ')\n    preds = classifier.predict(X_valid)\n    pred_prob = classifier.predict_proba(X_valid)\n    print(f'Accuracy: {round(accuracy_score(y_valid, preds), 5)}')\n    print(f'Log Loss: {round(log_loss(y_valid, pred_prob), 5)}')\n    if round(log_loss(y_valid, pred_prob), 5) < best_score:\n        best_score = round(log_loss(y_valid, pred_prob), 5)\n        best_clf = classifier\n    all_clfs.append(classifier)\n    print('***********************************************************')\n    print(' ')","metadata":{"id":"Xp05_XUZTAU9","outputId":"bd3c55a2-c833-4153-b63f-6feffc942f9d","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Best Classifier had score of {best_score}')","metadata":{"id":"pxVVLPEQ-xe8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nplt.plot(best_clf.history['valid_logloss']);\nplt.xlim([0, 100])\nplt.show();","metadata":{"id":"d52PtVgB7uyK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Masks Visualization","metadata":{"id":"SllR0fAEjC-s"}},{"cell_type":"code","source":"X_test = test_df[selected_features].to_numpy()\nexplain_matrix, masks = classifier.explain(X_test)","metadata":{"id":"u8bXmTK3j_WR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 5, figsize=(20,20))\n\nfor i in range(5):\n    axs[i].imshow(masks[i][:50])\n    axs[i].set_title(f\"mask {i}\")","metadata":{"id":"U81V2waqkfJj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission Prediction","metadata":{"id":"F2IvQ6HKpKEn"}},{"cell_type":"code","source":"X_test = test_df[selected_features].to_numpy()\npred_prob = None\nfor k in range(NUM_SPLITS):\n    clf = all_clfs[k]\n    if pred_prob is None:\n        pred_prob = clf.predict_proba(X_test)\n    else:\n        pred_prob += clf.predict_proba(X_test)\n\npred_prob /= NUM_SPLITS\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['Class_1'] = pred_prob[:, 0]\nsubmission['Class_2'] = pred_prob[:, 1]\nsubmission['Class_3'] = pred_prob[:, 2]\nsubmission['Class_4'] = pred_prob[:, 3]","metadata":{"id":"jUJC02x-knrM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"id":"CYF28UHcps6s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"tabnet_sub.csv\",index=False)","metadata":{"id":"cHRSS5jhpuJ6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** ðŸ˜Š","metadata":{}}]}