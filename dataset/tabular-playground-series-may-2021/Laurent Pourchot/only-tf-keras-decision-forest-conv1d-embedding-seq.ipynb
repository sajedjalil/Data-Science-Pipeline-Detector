{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom keras.models import Sequential\nimport keras \nfrom keras.layers import Dense, Flatten, Conv1D,MaxPooling1D, Dropout,BatchNormalization,Embedding,Concatenate, Activation,Input,Bidirectional,LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import model_from_json\nfrom keras import backend as K\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-may-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-may-2021/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Basic data cooking","metadata":{}},{"cell_type":"markdown","source":"<h3> Data for OneHot Models","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/Lpourchot/dfencoding.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dfencoding import utilities","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dum = train.copy()\ntest_dum = test.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dum = train_dum.iloc[:,1:].astype('str')\ntest_dum = test_dum.iloc[:,1:].astype('str')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfe = utilities.dfencoding(train_dum,'target',test_dum, missing_value = 'Y', cat_limit = 150, dummies_limit = 150)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfe.get_dummies()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_Onehot = dfe.data.iloc[:len(train_dum),1:]\ntest_Onehot = dfe.data.iloc[len(train_dum):,1:]\nprint(X_Onehot.shape)\nprint(test_Onehot.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Data for others Models","metadata":{}},{"cell_type":"code","source":"# Without labelencoding\ntarget = pd.get_dummies(train['target'])\ny = train['target']\nX = train.iloc[:,1:-1]\ntest = test.iloc[:,1:]\n\n# To avoid negative values (for embedding), we just add 8 to all categories :\nX = X + 8\ntest = test + 8\nX.shape, test.shape, y.shape, target.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = pd.get_dummies(train['target']).astype('float')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Parameters for the training","metadata":{}},{"cell_type":"code","source":"es = callbacks.EarlyStopping(\n                monitor = 'val_categorical_crossentropy', \n                min_delta = 0.0000001, \n                patience = 4,\n                mode = 'min',\n                baseline = None, \n                restore_best_weights = True,\n                verbose = 1)\n\nplateau  = callbacks.ReduceLROnPlateau(\n                monitor = 'val_categorical_crossentropy',\n                factor = 0.5, \n                patience = 2, \n                mode = 'min', \n                min_delt = 0.0000001,\n                cooldown = 0, \n                min_lr = 1e-7,\n                verbose = 1) \n\nmetrics = [tf.keras.metrics.CategoricalCrossentropy()]\nloss = tf.keras.losses.CategoricalCrossentropy(\n                from_logits=False,\n                label_smoothing=0,\n                reduction=\"auto\",\n                name=\"categorical_crossentropy\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2> Base models : Decision Forest + Embedding + Conv1D + Sequential Onehot","metadata":{}},{"cell_type":"markdown","source":"<h3> Decision Forest Model","metadata":{}},{"cell_type":"markdown","source":"<h4> Class Decision tree (they are Base components of the forest)","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAdapted from : \nhttps://keras.io/examples/structured_data/deep_neural_decision_forests/\nIf you try the source from Keras, I found an issue in the source : \nchange vocabulary_size by vocab_size in the data pipeline (encode function)\n\"\"\"\nclass Decision_Tree(keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(Decision_Tree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2 ** depth\n        self.num_classes = num_classes\n        num_used_features = int(num_features * used_features_rate)    \n        one_hot = np.eye(num_features)                                \n        sampled_feature_indicies = np.random.choice(\n            np.arange(num_features), num_used_features, replace=False\n        )                                                            \n        self.used_features_mask = one_hot[sampled_feature_indicies]   \n        self.pi = tf.Variable(\n            initial_value = tf.random_normal_initializer()(\n            shape = [self.num_leaves, self.num_classes]\n            ),\n            dtype=\"float32\",\n            trainable=True,\n        )\n        \n        self.decision_fn = layers.Dense(\n            units=self.num_leaves, \n            activation=\"sigmoid\", \n            name=\"decision\"\n            )\n\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n        features = tf.matmul(\n            features, \n            self.used_features_mask, \n            transpose_b=True\n            )  \n        decisions = tf.expand_dims(\n            self.decision_fn(features),\n            axis=2\n            )  \n        decisions = layers.concatenate(\n            [decisions, 1 - decisions],\n            axis=2\n            ) \n\n        mu = tf.ones([batch_size, 1, 1]) \n        begin_idx = 1\n        end_idx = 2\n\n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])  \n            mu = tf.tile(mu, (1, 1, 2))  \n            level_decisions = decisions[:, begin_idx:end_idx, :]  \n            mu = mu * level_decisions  \n            begin_idx = end_idx\n            end_idx = begin_idx + 2 ** (level + 1)\n\n        mu = tf.reshape(mu, [batch_size, self.num_leaves])  \n        probabilities = keras.activations.softmax(self.pi)  \n        outputs = tf.matmul(mu, probabilities) \n        \n        return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Class Decision Forest","metadata":{}},{"cell_type":"code","source":"class Decision_Forest(keras.Model):\n    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n        super(Decision_Forest, self).__init__()\n        self.ensemble = []\n        self.num_classes = num_classes\n        \n        for _ in range(num_trees):\n            self.ensemble.append(\n                            Decision_Tree(depth, \n                            num_features,\n                            used_features_rate,\n                            self.num_classes)\n                            )\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        outputs = tf.zeros([batch_size, \n                            num_classes])\n        \n        for tree in self.ensemble:\n            outputs += tree(inputs)\n            \n        outputs /= len(self.ensemble)\n        \n        return outputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4> Decision Forest Model ( Decision_Forest (Decision_Tree) )","metadata":{}},{"cell_type":"code","source":"def api_func_forest():\n    num_trees = 20\n    depth = 5\n    used_features_rate = 0.5\n    num_classes = 4\n    num_features = 50\n    API_input_forest = Input(shape = (50,),\n                            name = 'API_input_forest')\n    num_features = 50\n    forest_model = Decision_Forest(\n                            num_trees,\n                            depth, \n                            num_features,\n                            used_features_rate,\n                            num_classes\n                            )\n\n    API_output_forest = forest_model(API_input_forest)\n\n    return API_output_forest, API_input_forest\n\nnum_classes = 4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> OneHot Model","metadata":{}},{"cell_type":"code","source":"def api_func_onhot():\n    inputs_API_Onehot = Input(shape = (1285,), name = 'API_input_Onehot')\n    w = Dense(1274, activation=\"relu\")(inputs_API_Onehot)\n    w = Dropout(0.3)(w)\n    w = Dense(20, activation = \"relu\")(w)\n    w = Dropout(0.3)(w)\n    outputs_API_Onehot = Dense(4, activation = \"relu\")(w)\n    \n    return  outputs_API_Onehot,inputs_API_Onehot","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Embedding Model","metadata":{}},{"cell_type":"code","source":"def api_func_embedding():\n    inputs_API_Embedding = Input(shape = (50,), name = 'API_input_Embedding')\n    x = Embedding(80, 10, input_length = 50)(inputs_API_Embedding)\n    x = Flatten()(x)\n    x = Dense(80, activation = \"relu\")(x)\n    x = Dense(20, activation = 'relu')(x)\n    outputs_API_Embedding = Dense(4, activation='relu')(x)\n    \n    return outputs_API_Embedding,inputs_API_Embedding","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3> Conv1D Model","metadata":{}},{"cell_type":"code","source":"def api_func_conv1D():\n    inputs_API_Conv1D = Input(shape=(50,1), name = 'API_input_Conv1D') \n    v = Conv1D(\n                filters = 256, \n                kernel_size = 4,\n                padding = 'same', \n                activation = 'relu',\n                )(inputs_API_Conv1D)\n\n    v = MaxPooling1D(pool_size = 3)(v)\n    v = Flatten()(v)\n    v = Dense(64, activation ='relu')(v)\n    outputs_API_Conv1D = Dense(4, activation = 'relu')(v)\n    \n    return outputs_API_Conv1D,inputs_API_Conv1D","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Decision Forest + Conv1D + Embedding + Onehot","metadata":{}},{"cell_type":"code","source":"N_FOLDS = 10\nSEED = 2021\noof = np.zeros((X.shape[0],4))\npred = np.zeros((test.shape[0],4))\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(X, y)):\n    print(f\"===== FOLD {fold} =====\")\n       \n    x_tr = X.iloc[tr_idx] \n    x_Onehot_tr = X_Onehot.iloc[tr_idx] \n    y_tr = target.iloc[tr_idx] \n    x_ts = X.iloc[ts_idx] \n    x_Onehot_ts = X_Onehot.iloc[ts_idx] \n    y_ts = target.iloc[ts_idx] \n      \n    #---------- Base models collection ---------------------\n    outputs_API_Onehot, inputs_API_Onehot = api_func_onhot()\n    outputs_API_Embedding, inputs_API_Embedding = api_func_embedding()    \n    outputs_API_Conv1D, inputs_API_Conv1D = api_func_conv1D()\n    API_output_forest, API_input_forest = api_func_forest()\n    \n   #---------- Final Model Layers --------------------------  \n    z = Concatenate(axis=1)(\n            [outputs_API_Conv1D,\n            outputs_API_Embedding,\n            outputs_API_Onehot,\n            API_output_forest])\n    z = Dense(16, activation = 'sigmoid')(z)\n    out = Dense(4, activation = 'softmax', name = 'out')(z)\n    \n    #----------Model instantiation--------------------------- \n    model_merged = Model(\n            inputs=[inputs_API_Conv1D,\n            inputs_API_Embedding,\n            inputs_API_Onehot,\n            API_input_forest], \n            outputs=out, \n            name=\"model_merged\")\n        \n    #----------Model compile--------------------------- \n    model_merged.compile(\n            tf.keras.optimizers.Adam(learning_rate=0.001),\n            loss = loss ,\n            metrics = metrics)\n    \n    #----------Model fit--------------------------- \n    model_merged.fit(\n            {'API_input_Conv1D': x_tr,\n            'API_input_Embedding': x_tr,\n            'API_input_Onehot': x_Onehot_tr,\n            'API_input_forest': x_tr},\n            {'out': y_tr},\n            validation_data = ([x_ts,x_ts,x_Onehot_ts,x_ts], y_ts),\n            batch_size = 256,\n            epochs = 50,\n            verbose = 1,\n            callbacks = [es,plateau])\n    \n    oof[ts_idx] = model_merged.predict(\n            [x_ts,x_ts, \n             x_Onehot_ts, \n             x_ts])\n    \n    score = log_loss(y_ts, oof[ts_idx])\n    print(f\"FOLD {fold} Score {score}\\n\")\n    \n    pred += model_merged.predict([test, test, test_Onehot, test]) / N_FOLDS\n\nscore = log_loss(y, oof)\nprint(f\"Score total {score}\\n\")   ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-may-2021/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(pred)\nsubmission_df.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nsubmission_df['id'] = submission['id']\nsubmission_df = submission_df[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\nsubmission_df.to_csv(\"submission_Keras_10.csv\", index=False)\ndisplay(submission_df.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}