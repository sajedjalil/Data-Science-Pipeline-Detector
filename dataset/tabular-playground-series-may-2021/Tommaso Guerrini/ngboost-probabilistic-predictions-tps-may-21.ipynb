{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Tabular Playground Series May 2021\n\n<img src=\"https://i.imgur.com/uHVJtv0.png\">\n<img src=\"https://opengraph.githubassets.com/275715e91fe6af32b0c8907606985ff18606c16c36c6d74935f5e9a10b4608c1/stanfordmlgroup/ngboost\">\n\n<br><br>\n\n### Notebook Contents:\n\nGiven the probabilistic nature of the output I've decided to try [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) a wonderful Gradient Boosting library developed by the great [Stanford ML group](https://stanfordmlgroup.github.io/). \n\nA first try gave results comparable to those I obtained using LightGBM (+ 5hrs of Optuna hyperparameters search). \n\n**Under construction**\n\n##### Props\n\nProps to [corochann](https://www.kaggle.com/corochann/optuna-tutorial-for-hyperparameter-optimization), I believe this notebook is the best you can find about Optuna.\n\n<h5> Versioning </h5>\n\nV3 was the submitted Run with 1.08820 Public Leaderboard score.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch\ndevice = 'gpu' if torch.cuda.is_available() else 'cpu'\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, PolynomialFeatures, LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest\nimport warnings\nwarnings.filterwarnings('ignore')\nimport optuna\nimport hyperopt\nimport tqdm\nimport gc\nimport os\nroot_path = '/kaggle/input/tabular-playground-series-may-2021'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(root_path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(root_path, 'sample_submission.csv'))\n\n#label mapping\nunique_targets = train['target'].unique().tolist()\nlabel_mapping = dict(zip(unique_targets, [int(i[-1]) - 1 for i in unique_targets]))\n\nlabel_mapping\n\n#preprocessing\n\ntrain['target'] = train['target'].map(label_mapping)\ndataset = pd.concat([train, test], axis = 0, ignore_index = True)\ntrain_len = len(train)\n\nfeatures = dataset.drop(['id', 'target'], axis=1).columns.tolist()\ncategorical_feature_columns = (dataset[features].apply(lambda x: x.nunique(), axis = 0)\n                               .rename('n_unique').to_frame()\n                               .query('n_unique < 10').index.tolist())\n\nlabel = LabelEncoder()\n\nfor column in categorical_feature_columns:\n    label.fit(dataset[column])\n    dataset[column] = label.transform(dataset[column])\n        \ncategorical_features = list(range(len(categorical_feature_columns)))\n\ntrain_preprocessed = dataset[:train_len]\ntest_preprocessed = dataset[train_len:]\n\nassert train_preprocessed.shape[1] == test_preprocessed.shape[1]\n\ndel train, test\ngc.collect()\ncat_indices = [features.index(i) for i in categorical_feature_columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ngboost==0.3.10","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ngboost import NGBClassifier\nfrom ngboost.distns import k_categorical\n\nOPTUNA_OPTIMIZATION = True\nN_SPLITS = 5 #Number of folds for validation\nN_TRIALS = 3 #Number of trials to find best hyperparameters\nTIME = 3600*2 #Time to run optimization (alternative to N_TRIALS)\nFOLD_RANDOM_SEED = 42\nREPEAT = True \n\nFIXED_PARAMS = {\"random_state\": 42,\n                \"Dist\": k_categorical(4), \n                \"verbose\": True,\n                \"verbose_eval\": 100,\n                \"n_estimators\": 500}\n\nEARLY_STOP = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndtr_friedman_3 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=3)\ndtr_friedman_5 = DecisionTreeRegressor(criterion='friedman_mse', max_depth=5)\ndtr_mse_3 = DecisionTreeRegressor(criterion='mse', max_depth=3)\ndtr_mse_5 = DecisionTreeRegressor(criterion='mse', max_depth=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skfold = StratifiedKFold(N_SPLITS, shuffle = True, random_state = FOLD_RANDOM_SEED)\nif REPEAT:\n    skfold = RepeatedStratifiedKFold(N_SPLITS, n_repeats=2, random_state=FOLD_RANDOM_SEED)\n\ndef objective(trial, cv=skfold):\n    \n    param_to_search_ngb = {\n        \"Base\": trial.suggest_categorical(\"Base\", [dtr_friedman_3,dtr_friedman_5,\n                                                   dtr_mse_3,dtr_mse_5]),\n        \"natural_gradient\": trial.suggest_categorical(\"natural_gradient\", [True, False]),\n        \"col_sample\": trial.suggest_float('col_sample', 1E-16, 1.0),\n        \"minibatch_frac\": trial.suggest_float('minibatch_frac', 1E-16, 1.0),\n        \"learning_rate\": trial.suggest_categorical('learning_rate', [0.001, 0.005, 0.01, 0.05, 0.1]),\n    }\n    \n    param_ngb = param_to_search_ngb.copy()\n    param_ngb.update(FIXED_PARAMS)\n    \n    val_losses = []\n    losses = []\n    \n    for kfold, (train_idx, val_idx) in tqdm.tqdm(enumerate(cv.split(train_preprocessed[features].values, \n                                                                    train_preprocessed['target'].values))):\n        \n        X_train = train_preprocessed.loc[train_idx, features].values\n        y_train = train_preprocessed.loc[train_idx, 'target'].astype(int)\n        \n        X_valid = train_preprocessed.loc[val_idx, features].values\n        y_valid = train_preprocessed.loc[val_idx, 'target'].astype(int)\n        \n        model = NGBClassifier(**param_ngb)  \n        model.fit(X_train, y_train, X_val = X_valid, Y_val = y_valid,\n                  early_stopping_rounds = EARLY_STOP)\n        scores = model.predict_proba(X_valid)\n        loss = log_loss(y_valid, scores)\n        losses.append(loss)\n    \n    return np.average(losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if OPTUNA_OPTIMIZATION:\n    study = optuna.create_study(study_name = 'ngb_parameter_opt', direction=\"minimize\")\n    \n    #study.optimize(objective, n_trials=1, show_progress_bar = True)\n    study.optimize(objective, timeout=TIME, show_progress_bar = True) \n    \n    trial = study.best_trial\n    \n    print(\"  Value: {}\".format(trial.value))\n    \n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    best_params = FIXED_PARAMS.copy()\n    best_params.update(trial.params)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if OPTUNA_OPTIMIZATION:\n    final_model = NGBClassifier(**best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = []\nloglosses = []\nfor kfold, (train_idx, val_idx) in enumerate(skfold.split(train_preprocessed[features].values, \n                                                       train_preprocessed['target'].values)):\n        \n        final_model = NGBClassifier(**best_params)\n        \n        X_train = train_preprocessed.loc[train_idx, features].values\n        y_train = train_preprocessed.loc[train_idx, 'target'].values.astype(int)\n        X_valid = train_preprocessed.loc[val_idx, features].values\n        y_valid = train_preprocessed.loc[val_idx, 'target'].astype(int)\n        \n        final_model.fit(X_train, y_train, X_val = X_valid, Y_val = y_valid, \n                        early_stopping_rounds=EARLY_STOP)\n        \n        probs = final_model.predict_proba(X_valid)\n        \n        logloss = log_loss(y_valid, probs)\n        loglosses.append(logloss)\n        print('Fold: {}\\t Validation logloss: {}\\n'.format(kfold, logloss))\n        \n        test_preds.append(final_model.predict_proba(test_preprocessed[features].values))\n        \nprint(\"Best Parameters mean logloss: {}\".format(np.mean(loglosses)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = np.mean(test_preds, axis = 0)\nassert len(test_predictions) == len(test_preprocessed)\npredictions_df = pd.DataFrame(test_predictions, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sample_submission['id']\npredictions_df.to_csv(\"submission.csv\", index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}