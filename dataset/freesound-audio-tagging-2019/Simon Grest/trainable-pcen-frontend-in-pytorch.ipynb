{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import librosa\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\nimport math\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"I read this discussion thread about PCEN as an alternative to mel-scale filters - [Anyone tried PCEN in front end?](https://www.kaggle.com/c/freesound-audio-tagging-2019/discussion/91859) and thought I'd give it a go.\n"},{"metadata":{},"cell_type":"markdown","source":"### Load an example wav file and compute the melspectrogram features"},{"metadata":{"trusted":true},"cell_type":"code","source":"slice_len   = 2\nslice_count = 1\nsr          = 44100\nn_mels      = 256\nfmin        = 20\nhop_length  = int(sr/(n_mels/slice_len)) # ensures square mel-spectrogram slice\nfmax        = sr//2\n\ny = librosa.effects.trim(librosa.load('../input/train_noisy/42f7abb4.wav' , sr)[0])[0]\n\ns = librosa.feature.melspectrogram(y, \n                                   sr         = sr,\n                                   n_mels     = n_mels,\n                                   hop_length = hop_length,\n                                   n_fft      = n_mels*20,\n                                   fmin       = fmin,\n                                   fmax       = fmax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compute PCEN from mels"},{"metadata":{},"cell_type":"markdown","source":"\nAs suggested in the thread I tried out a few different sets of parameters using the [librosa implementation](https://librosa.github.io/librosa/generated/librosa.core.pcen.html) mentioned in the thread. I found some parameters that seemed to give me a bit of a boost over the '''power_to_db''' function that I had been using. Those parameters were:\n```\ngain          = 0.6\nbias          = 0.1 \npower         = 0.2 \ntime_constant = 0.4 \neps           = 1e-9\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"gain          = 0.6\nbias          = 0.1 \npower         = 0.2 \ntime_constant = 0.4 \neps           = 1e-9\n\ntime_constant = 0.4\n\npower_to_db = librosa.power_to_db(s)\n\npcen_librosa = librosa.core.pcen(s, \n                                 sr            = sr,\n                                 hop_length    = hop_length,\n                                 gain          = gain,\n                                 bias          = bias,\n                                 power         = power,\n                                 time_constant = time_constant,\n                                 eps           = eps)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compare plots of the Power to Db and the PCEN mel-spectrogram "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,3))\nfig.suptitle(\"Power to Db\")\nplt.imshow(power_to_db)\n\n\nfig = plt.figure(figsize=(20,3))\nfig.suptitle(\"PCEN\")\nplt.imshow(pcen_librosa)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"### Learn the PCEN parameters\n\nThe idea of incorporating the tuning of these parameters was appealing so I adapted and simplified code for a PyTorch PCEN layer that I found [here](https://github.com/daemon/pytorch-pcen).\n\nThe code there uses different names for the PCEN parameters and it took me a little while to figure exactly how to change the parameter values to be equivalent to the librosa implementation.\n\nA change I found necessary to make was to ensure that the parameters remained positive during training. Initially using the code from github, my gradients would become undefined because the PCEN calculation went wrong if the paramters became negative. \n\nI followed the approach suggested in the [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/8a75d472dc7286653a5245a80a7603a1db308af0.pdf) that proposed PCEN, and learnt the logarithms of the parameters and then exponentiated them in the PCEN calculation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def pcen(x, eps=1E-6, s=0.025, alpha=0.98, delta=2, r=0.5, training=False):\n    frames = x.split(1, -2)\n    m_frames = []\n    last_state = None\n    for frame in frames:\n        if last_state is None:\n            last_state = s * frame\n            m_frames.append(last_state)\n            continue\n        if training:\n            m_frame = ((1 - s) * last_state).add_(s * frame)\n        else:\n            m_frame = (1 - s) * last_state + s * frame\n        last_state = m_frame\n        m_frames.append(m_frame)\n    M = torch.cat(m_frames, 1)\n    if training:\n        pcen_ = (x / (M + eps).pow(alpha) + delta).pow(r) - delta ** r\n    else:\n        pcen_ = x.div_(M.add_(eps).pow_(alpha)).add_(delta).pow_(r).sub_(delta ** r)\n    return pcen_\n\n\nclass PCENTransform(nn.Module):\n\n    def __init__(self, eps=1E-6, s=0.025, alpha=0.98, delta=2, r=0.5, trainable=True):\n        super().__init__()\n        if trainable:\n            self.log_s = nn.Parameter(torch.log(torch.Tensor([s])))\n            self.log_alpha = nn.Parameter(torch.log(torch.Tensor([alpha])))\n            self.log_delta = nn.Parameter(torch.log(torch.Tensor([delta])))\n            self.log_r = nn.Parameter(torch.log(torch.Tensor([r])))\n        else:\n            self.s = s\n            self.alpha = alpha\n            self.delta = delta\n            self.r = r\n        self.eps = eps\n        self.trainable = trainable\n\n    def forward(self, x):\n        x = x.permute((0,1,3,2)).squeeze(dim=1)\n        if self.trainable:\n            x = pcen(x, self.eps, torch.exp(self.log_s), torch.exp(self.log_alpha), torch.exp(self.log_delta), torch.exp(self.log_r), self.training and self.trainable)\n        else:\n            x = pcen(x, self.eps, self.s, self.alpha, self.delta, self.r, self.training and self.trainable)\n        x = x.unsqueeze(dim=1).permute((0,1,3,2))\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used the ```PCENTransform``` module above as the first layer in my CNN. \n\nSomething like:\n\n\n```\nclass MyNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pcen = PCENTransform(eps=1E-6, s=0.025, alpha=0.6, delta=0.1, r=0.2, trainable=True)\n   \n    ...\n    \n    def forward(self, x):\n        p = self.pcen(x)\n\n    ...\n\n```"},{"metadata":{},"cell_type":"markdown","source":"### Test PyTorch PCEN definition\n\nThe librosa pcen documentation states that:\n\n> If b is not provided, it is calculated as:\n> \n> b = (sqrt(1 + 4* T**2) - 1) / (2 * T**2)\n> where T = time_constant * sr / hop_length."},{"metadata":{"trusted":true},"cell_type":"code","source":"t = torch.tensor(s)\n\nT = time_constant * sr / hop_length\n\nb = (math.sqrt(1 + 4* T**2) - 1) / (2 * T**2)    # as per librosa documentation\n\npcen_torch = pcen(t[None,...].permute((0,2,1)),  # change the shape of the mels appropriately for the PyTorch pcen function\n                  eps      = eps, \n                  s        = b, \n                  alpha    = gain,\n                  delta    = bias, \n                  r        = power, \n                  training = True)\n\npcen_torch = pcen_torch.permute((0,2,1)).squeeze().numpy()     # change the shape back and convert to numpy array from tensor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check that the PyTorch and Librosa PCEN are the same"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.allclose(pcen_librosa, pcen_torch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Problem: Slice then PCEN â‰  PCEN then Slice\n\nFor this competition to ensure constant input size for my networks I was using the approach of randomly slicing a section of the spectrogram and zero-padding if necessary.\n\nI realised that the order of slicing and PCEN matters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"width  = 256\nstart  = np.random.randint(0,s.shape[1]-width) \nend    = start + width\n\ncrop_then_slice = librosa.core.pcen(s[:,start:end],\n                                    sr            = sr,\n                                    hop_length    = hop_length,\n                                    gain          = gain,\n                                    bias          = bias,\n                                    power         = power,\n                                    time_constant = time_constant,\n                                    eps           = eps)\n\nslice_then_crop = pcen_librosa[:,start:end]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.allclose(crop_then_slice, slice_then_crop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,3,figsize=[20,9])\nax[0].set_title(\"Crop then slice\")\nax[0].imshow(crop_then_slice)\n\nax[1].set_title(\"Slice then crop\")\nax[1].imshow(slice_then_crop)\n\nax[2].set_title(\"Difference\")\nax[2].imshow(crop_then_slice-slice_then_crop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When the PCEN is computed on the entire spectrogram information is carried through from the start of the spectrogram, whereas when the slicing is done first this information is not available."},{"metadata":{},"cell_type":"markdown","source":"### Trainable PCEN CNN performance"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Using a PCEN layer in my network with the above slicing approach posed a bit of a challenge, the PCEN will be computed on the sliced spectrogram not on the whole spectrogram and this seemed to make a difference to my training results.\n\nI found that I could not get the trainable PCEN to perform as well as the static parameters I chose with librosa.\n\nI tried setting my PCEN layer to not be trainable and gave it the same static parameters, but the performance was still worse.\n\nThe only thing I can think to attribute this to is the problem I highlighted above."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}