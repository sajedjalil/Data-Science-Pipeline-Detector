{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nimport time\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\nimport h5py\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit\nfrom sklearn.metrics import label_ranking_average_precision_score as lrap\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.keras.utils.data_utils import Sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, MaxPool2D, Conv2D, AvgPool2D, ReLU, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Lambda, LeakyReLU\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_train_data_df(mode='curated'):\n    # Load training data filenames and labels (raw -> multilabels are represented as a string with comma separated values)\n    data_folder = '/kaggle/input/freesound-audio-tagging-2019' \n    csv_path = f'{data_folder}/train_{mode}.csv'\n    raw_df = pd.read_csv(csv_path, index_col='fname')\n        \n    # Extract list of expected labels\n    sub = pd.read_csv('/kaggle/input/freesound-audio-tagging-2019/sample_submission.csv', index_col='fname')\n    labels_list = sub.columns.values \n\n    # Encode multi-labels in a binary vector\n    splitted_labels = [ labels.split(',') for labels in raw_df['labels'].values ]\n    encoder = MultiLabelBinarizer()\n    encoded_labels = encoder.fit_transform(splitted_labels)\n\n    # Create a new pandas Dataframe to represent training labels as binary vectors\n    labels_df = pd.DataFrame(data=encoded_labels, index=list(raw_df.index), columns=labels_list)\n    \n    return labels_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data filenames and labels\ntrain_curated_labels = load_train_data_df(mode='curated')\ntrain_noisy_labels = load_train_data_df(mode='noisy')\ntest_labels = pd.read_csv('/kaggle/input/freesound-audio-tagging-2019/sample_submission.csv', index_col='fname')\n\n# Main info about the training/testing sets\nprint(f'{train_curated_labels.shape[1]} possible classes.')\nprint(f'{train_curated_labels.shape[0]} curated training samples.')\nprint(f'{train_noisy_labels.shape[0]} noisy training samples.')\nprint(f'{test_labels.shape[0]} test samples.')\n\ntrain_curated_labels.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OPTIONAL - Reduced train dataset (10 random classes ~750 curated samples and ~3000 noisy labels)\nuse_reduced_data = True\nn_classes = 80\n\nif use_reduced_data:\n    n_classes = 10\n    labels_list = np.random.choice(train_curated_labels.columns, 10, replace=False)\n    samples_to_keep = train_curated_labels.loc[:, labels_list].sum(axis=1) > 0\n    train_curated_labels = train_curated_labels.loc[samples_to_keep, labels_list]\n    print(f'Using reduced curated dataset {train_curated_labels.shape}.')\n    print(train_curated_labels.sum())\n    \n    samples_to_keep = train_noisy_labels.loc[:, labels_list].sum(axis=1) > 0\n    train_noisy_labels = train_noisy_labels.loc[samples_to_keep, labels_list]\n    print(f'Using reduced noisy dataset {train_noisy_labels.shape}.')\n    print(train_noisy_labels.sum())\n    \ntrain_curated_h5 = '/kaggle/input/fat2019wav/train_curated_wav.h5'\ntrain_curated_ids = np.array([ 't' + file.split('.')[0] for file in train_curated_labels.index.values ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data exploration (fat2019wav dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_h5(sample_ids, h5_filename):\n    if not isinstance(sample_ids, np.ndarray):\n        sample_ids = np.array([sample_ids])\n    with h5py.File(h5_filename, mode='r') as dataset:\n        samples = [ convert_to_float(dataset[f][()]) for f in sample_ids ]\n    return samples\n\ndef load_random_wav(sample_ids, h5_filename):\n    random_id = np.random.choice(train_curated_ids, 1)[0]\n    sample = load_h5(random_id, h5_filename)[0]\n    return random_id, sample\n\ndef get_label(sample_id, labels_df, decode_label=False):\n    label = labels_df.loc[sample_id[1:] + '.wav']\n    if decode_label:\n        label = list(label[label>0].index)\n    return label \n\ndef listen_sample(sample, sr=44100):\n    return IPython.display.display(ipd.Audio(data=sample, rate=sr))\n\ndef convert_to_float(sample):\n    return np.array(sample) / 255.\n\ndef show_signal(sample, mode='temporal', ax=None, sr=44100):\n    if ax is None:\n        fig, ax = plt.subplots()\n        \n    if mode == 'temporal':\n        librosa.display.waveplot(sample, sr=sr, ax=ax)\n        ax.set_title('Temporal')\n        print(f'Temporal: {sample.shape}')\n    elif mode == 'stft':\n        stft = librosa.stft(sample)\n        stft = librosa.amplitude_to_db(abs(stft), ref=np.max)\n        librosa.display.specshow(stft, x_axis='time', y_axis='log', sr=sr, ax=ax)\n        ax.set_title('STFT')\n        print(f'STFT: {stft.shape}')\n    elif mode == 'mfcc':\n        mfccs = librosa.feature.mfcc(sample, sr=sr, n_mfcc=20)\n        librosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=ax)\n        ax.set_title('MFCC')\n        print(f'MFCC: {mfccs.shape}')\n    elif mode == 'logmel':\n        mel = librosa.feature.melspectrogram(y=sample, sr=sr, n_fft=1024, hop_length=512, n_mels=128, fmin=20, fmax=sr//2)\n        mel = librosa.core.power_to_db(mel, ref=np.max)\n        librosa.display.specshow(mel, sr=sr, x_axis='time', y_axis='mel', hop_length=512, fmin=20, fmax=sr//2, ax=ax)\n        ax.set_title('Log-Mel')\n        print(f'Log-Mel: {mel.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load random .wav sample\nsr = 44100\nrandom_id, sample = load_random_wav(train_curated_ids, train_curated_h5)\nlabels = get_label(random_id, train_curated_labels, decode_label=True)\nlisten_sample(sample)\n\nprint(f'Length: {len(sample)/sr:.2f}s')\nprint(f'Labels: {labels}')\n\nfig, ax = plt.subplots(4, 1, figsize=(16,10))\nshow_signal(sample, mode='temporal', ax=ax[0])\nshow_signal(sample, mode='stft', ax=ax[1])\nshow_signal(sample, mode='mfcc', ax=ax[2])\nshow_signal(sample, mode='logmel', ax=ax[3])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**STFT (Short Term Fourier Transform):** Computes the amplitude of the frequencies for different bands  over time. <br>\n**MFCC (Mel Frequency Cepstral Coefficients):** Describe the instantaneous spectral envelope shape of the speech signal. [[source](https://wiki.aalto.fi/display/ITSP/Deltas+and+Delta-deltas)] <br>\n**Mel spectrogram:** Similar to STFT but represented in the Mel scale (non-linear transformation of the frequency scale), that is closer to how humans perceive sounds."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of labels distribution\nfig, ax = plt.subplots(figsize=(10,5))\ntrain_curated_labels.sum(axis=0).sort_values(ascending=True).plot(kind='barh', x='Count', y=train_curated_labels.columns, ax=ax)\nax.set_xlabel('Count')\nax.set_title('Distribution of training labels');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:** Most labels are fairly equally distributed, but we might want to oversample the last 14 labels in order to have equally represented labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract duration of each sample in the curated training set\ncurated_len_df = pd.DataFrame(columns=['fname', 'length'])\ncurated_len_df.set_index(['fname'], inplace=True)\n\nfor file in train_curated_ids:\n    sample = load_h5(file, train_curated_h5)[0]\n    \n    fname = file.split('/')[-1]\n    curated_len_df.loc[fname] = len(sample) / sr\n\ncurated_len_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Histogram of signal length distribution\nfig, ax = plt.subplots(figsize=(10,4))\ncurated_len_df['length'].sort_values(ascending=False).plot(kind='hist', bins=100, ax=ax)\nax.set_xlim([0,31])\nax.set_ylabel('Count')\nax.set_xlabel('Duration (s)')\nax.set_title('Distribution of signal durations');\n\nprint(f'Smallest duration: {curated_len_df[\"length\"].min():.2f}s')\nprint(f'Largest duration: {curated_len_df[\"length\"].max():.2f}s')\nprint(f'Mean duration: {curated_len_df[\"length\"].mean():.2f}s')\nprint(f'Median duration: {curated_len_df[\"length\"].median():.2f}s')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remark:** >50% of all samples are less than 5s long. We should think about a cropping strategy in order to have fixed-length samples."},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature extraction\n\nWe can extract log melspectrogram features (n_mels=128) using Librosa and the following parameters:\n- sr = 44100\n- nmels = 128 \n- nfft = 1024\n- hop_length = 512\n- fmin = 20\n- fmax = sr//2\n\nSee [Kaggle notebook](https://www.kaggle.com/obione26/freesound-audio-tagging-feature-extraction) ([Colab version](https://colab.research.google.com/drive/1uTEdXfazhonVHxP4vmuYAgG2BexzHfcu?usp=sharing))\n"},{"metadata":{},"cell_type":"markdown","source":"# 4. Data modeling\n\n\nModel architecture inspired from [this notebook](https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data) and this [github](https://github.com/sainathadapa/kaggle-freesound-audio-tagging/blob/master/approaches_all.md). <br>\nAugmentation methods inspired from [this notebook](https://www.kaggle.com/osciiart/resnet34-multi1024).\n\n**Metrics:**\n- **LWLRAP (Label-Weighted Label-Ranking Average Precision):** Evaluates the ability of a model (given the predicted ranked list of labels probabilities) to give higher probabilities to the relevant labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper functions of this section\ndef extract_mel(sample, n_mels=128, sr=44100, hop=512, log=False):\n    \"\"\" Return melspectrogram of shape (n_mels, len(sample)/hop + 1). \"\"\"\n    mel = librosa.feature.melspectrogram(sample, sr=sr, n_fft=1024, hop_length=hop, n_mels=n_mels, fmin=20, fmax=sr//2)\n    if log:\n        mel = librosa.core.power_to_db(mel, ref=1.0, amin=1e-10, top_db=None)\n    return mel.astype(np.float32)\n\ndef random_chunk(x, chunk_size=44100, hop_len=512):\n    \"\"\" Extract fixed-size chunk from signal. If signal shorter than chunk size -> repeat pattern. Else -> random cropping.\n    Parameters:\n    - chunk_size (int, >0): Number of frames in a chunk (corresponds to chunk_size*hop_len/sr seconds).\n    - hop_len (int, >0): Number of temporal frames represented in one mel frame.\n    \"\"\"\n    n_frames = x.shape[-1]\n    \n    if n_frames <= chunk_size:\n        x = np.tile(x, chunk_size//n_frames + 1)\n        x = x[..., :chunk_size]\n    else:\n        roi_start = np.random.randint(0, n_frames - chunk_size)\n        x = x[..., roi_start:roi_start + chunk_size]\n    return x\n\ndef random_ampli(x, min_gain=0.5, max_gain=2):\n    rate = (max_gain - min_gain) * np.random.random() + min_gain\n    return rate * x\n\ndef mixup(x, label, source_ids, source_h5, source_labels, chunk_size, hop_len, alpha=1.):\n    \"\"\" Return a weighted combination of audio samples and labels. \"\"\"\n    if x.shape[-1] != chunk_size:\n        x = random_chunk(x, chunk_size, hop_len)\n    \n    # Load second signal and associated label\n    random_id2, sample2 = load_random_wav(source_ids, source_h5)\n    sample2 = random_chunk(sample2, chunk_size, hop_len)\n    label2 = get_label(random_id2, source_labels)\n    \n    # Combine input signals and labels\n#     rate = np.random.beta(alpha, alpha)\n    rate = np.random.random()\n    x = rate * x + (1-rate) * sample2\n    label = rate * label + (1-rate) * label2\n    \n    return x, label\n\ndef normalize(X):\n    mean = X.mean(axis=0, keepdims=True)\n    std = X.std(axis=0, keepdims=True)\n    X = (X - mean) / (std + 1e-7)\n    return X\n\nclass DataLoader(Sequence):\n    def __init__(self, h5_file, sample_ids, labels_df, batch_size, chunk_size=44100, n_mels=128, hop=512, sr=44100, compute_deltas=False, shuffle=True):\n        self.h5_file = h5_file\n        self.sample_ids = np.array(sample_ids)\n        self.labels_df = labels_df\n        self.batch_size = batch_size\n        self.chunk_size = chunk_size\n        self.n_mels = n_mels\n        self.hop = hop\n        self.sr = sr\n        self.compute_deltas = compute_deltas\n        self.shuffle = shuffle\n        self.on_epoch_end()\n        \n        # FIXME: Create mel dataset\n        self.X = load_h5(self.sample_ids, self.h5_file)\n        self.X = [ extract_mel(x, n_mels, sr, hop, log=True) for x in self.X]\n        self.y = np.array([ get_label(id, self.labels_df) for id in self.sample_ids ])\n\n    def __len__(self):\n        \"\"\" Corresponds to the number of steps in one epoch. \"\"\"\n        return int(np.ceil(len(self.sample_ids) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        indexes = self.indexes[idx*self.batch_size: (idx+1)*self.batch_size]\n        batch_X, batch_y = self.generate_Xy(indexes)\n        \n        if self.compute_deltas:\n            pass\n                    \n        batch_X = normalize(batch_X)\n        batch_X = np.expand_dims(batch_X, axis=-1)\n        \n        return batch_X, batch_y\n\n    def generate_Xy(self, indexes):\n        # Fixed-length size \n        X = [ random_chunk(self.X[i], self.chunk_size, self.hop) for i in indexes ]\n        y = self.y[indexes]\n#             x = random_ampli(x)\n#             x, label = mixup(x, label, self.mel_paths, self.labels_df, self.chunk_size, self.hop, alpha=1.)\n        return np.array(X).astype(np.float32), np.array(y).astype(np.float32)\n    \n    def on_epoch_end(self):\n        \"\"\" Shuffle the data after each epoch to avoid oscillation patterns in the loss. \"\"\"\n        self.indexes = np.arange(0, len(self.sample_ids))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n            \nclass CNN9:\n    def __init__(self, n_feats=128, chunk_size=44100, hop=512, n_classes=80):\n        self.n_feats = n_feats\n        self.n_frames = int(chunk_size / hop + 1)\n        self.n_classes = n_classes\n        \n    def add_conv_block(self, n_filts, k_size):\n        self.model.add(Conv2D(n_filts, k_size, padding='same'))\n        self.model.add(LeakyReLU(alpha=0.01))\n        self.model.add(BatchNormalization(momentum=0.1))\n        self.model.add(ReLU())\n        \n        self.model.add(Conv2D(n_filts, k_size, padding='same'))\n        self.model.add(LeakyReLU(alpha=0.01))\n        self.model.add(BatchNormalization(momentum=0.1))\n        self.model.add(ReLU())\n        \n    def create_model(self):\n        self.model = Sequential()\n        self.model.add(Input(shape=(self.n_feats, self.n_frames, 1)))\n\n        self.add_conv_block(n_filts=64, k_size=(3,3))\n        self.model.add(AvgPool2D(pool_size=(2,2)))\n\n        self.add_conv_block(n_filts=128, k_size=(3,3))\n        self.model.add(AvgPool2D(pool_size=(2,2)))\n\n        self.add_conv_block(n_filts=256, k_size=(3,3))\n        self.model.add(AvgPool2D(pool_size=(2,2)))\n\n        self.add_conv_block(n_filts=512, k_size=(3,3))\n        self.model.add(AvgPool2D(pool_size=(1,1)))\n\n        self.model.add(Lambda(lambda x: K.max(K.mean(x, axis=3), axis=2)))\n        self.model.add(Dense(512, activation='relu'))\n        self.model.add(Dense(self.n_classes, activation='softmax'))\n\n        return self.model\n\ndef create_shallow_cnn(n_feats=128, chunk_size=512, n_classes=80):\n    model = Sequential()\n    model.add(Input(shape=(n_feats, chunk_size, 1)))\n\n    model.add(Conv2D(48, 11, strides=(2,3),kernel_initializer='he_uniform', activation='relu', padding='same'))\n    model.add(MaxPooling2D(3, strides=(1,2)))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(128, 5, strides=(2,3), kernel_initializer='he_uniform', activation='relu', padding='same'))\n    model.add(MaxPooling2D(3, strides=2))\n    model.add(BatchNormalization())\n\n    model.add(Conv2D(192, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(192, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, 3, strides=1,kernel_initializer='he_uniform', activation='relu', padding='same'))\n    model.add(MaxPooling2D(3, strides=(1,2)))\n    model.add(BatchNormalization())\n\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(n_classes, activation='linear'))\n    return model    \n    \ndef create_mobile_cnn(n_feats=128, chunk_size=512, n_classes=80):\n    model = Sequential()\n    model.add(Input(shape=(n_feats, chunk_size, 1)))\n              \n    model.add(BatchNormalization())\n    model.add(Conv2D(10, kernel_size=(1,1), padding='same', activation='relu'))\n    model.add(Conv2D(3, kernel_size=(1,1), padding='same', activation='relu'))\n    \n    mobilenet = MobileNetV2(include_top=False)\n    mobilenet.layers.pop(0)\n    model.add(mobilenet)\n    \n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(1024, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(512, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(n_classes, activation='softmax'))\n    return model\n              \ndef plot_metric(hist, metric='loss', ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n        \n    ax.plot(hist.history[metric])\n    ax.plot(hist.history[f'val_{metric}'])\n    ax.set_title(f'{metric.upper()} vs Epoch')\n    ax.set_ylabel(metric.upper())\n    ax.set_xlabel('Epochs')\n    ax.legend(['train', 'validation'], loc='upper right')\n#     plt.show()\n    \ndef lw_lrap(y_true, y_pred):\n    \"\"\" Compute label-weighted label-ranking average precision. \n    Source: https://github.com/qiuqiangkong/dcase2019_task2/blob/master/utils/lwlrap.py\"\"\"\n    \n    weights = K.sum(y_true, axis=1)\n    score = tf.py_function(lrap, [y_true, \n                                  y_pred, \n                                  weights],\n                           tf.float32)                               \n    return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try out mixup\nhop_len = 512\nchunk_len = 4\nchunk_size = sr * chunk_len\n\nid1, sample1 = load_random_wav(train_curated_ids, train_curated_h5)\nlabel1 = get_label(id1, train_curated_labels)\nprint(f'Initial label: {label1[label1>0]}')\nsample1 = random_chunk(sample1, chunk_size, hop_len)\n\nmixup_sample, mixup_label = mixup(sample1, label1, train_curated_ids, train_curated_h5, train_curated_labels, chunk_size, hop_len, alpha=1)\nprint(f'Mixup label: {mixup_label[mixup_label>0]}')\n\nlisten_sample(sample1)\nlisten_sample(mixup_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try out random gain\nid1, sample1 = load_random_wav(train_curated_ids, train_curated_h5)\nlisten_sample(sample1)\n\nampli_sample = random_ampli(sample1)\nlisten_sample(ampli_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Feature extraction hyperparameters\nsr = 44100\nchunk_size = sr\nn_feats = 128\nhop = 512\n\n# Training hyperparameters\nbatch_size = 32\n\n# Training/validation splits\nSEED = 42\nkf = list(ShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED).split(np.arange(len(train_curated_ids))))\n# kf = list(KFold(n_splits=5, shuffle=True, random_state=SEED).split(np.arange(len(train_mix_mel_paths))))\n\nmodels = {}\nhists = {}\nfor fold_idx, (train_ids, valid_ids) in enumerate(kf):\n    print(f'Fold {fold_idx + 1} - {len(train_ids)} train ids / {len(valid_ids)} valid ids')\n    \n    # Define data loader\n    train_loader = DataLoader(train_curated_h5, train_curated_ids[train_ids], train_curated_labels.iloc[train_ids], batch_size, chunk_size, n_feats, hop, sr)\n    valid_loader = DataLoader(train_curated_h5, train_curated_ids[valid_ids], train_curated_labels.iloc[valid_ids], batch_size, chunk_size, n_feats, hop, sr)\n\n    # Define architecture\n    model = CNN9(n_feats, chunk_size, hop_len, n_classes).create_model()\n\n    # Train model\n    es = EarlyStopping(monitor='val_acc', mode='max', patience=5)\n    mc = ModelCheckpoint(f'best_model_{fold_idx}.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) # lw_lrap\n\n    hist = model.fit(train_loader, steps_per_epoch=len(train_loader),\n                     validation_data=valid_loader,\n                     validation_steps=len(valid_loader),\n                     epochs=50, verbose=1, callbacks=[es, mc])\n    \n    # Save history and trained model\n    models[f'fold_{fold_idx}'] = model\n    hists[f'fold_{fold_idx}'] = hist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(len(kf) +1 , 2, figsize=(15 ,3*len(kf)))\n\nfor fold_idx in range(len(kf)):\n    plot_metric(hists[f'fold_{fold_idx}'], 'loss', ax=ax[fold_idx, 0])\n    plot_metric(hists[f'fold_{fold_idx}'], 'acc', ax=ax[fold_idx, 1])\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_random_test(paths_list):\n    path = np.random.choice(paths_list, 1)[0]\n    mel = np.load(path)\n    return mel\n\ndef prepare_mel(mel, chunk_size=256, sr=44100):\n    n_mels, n_frames = mel.shape\n    \n    if n_frames <= chunk_size:\n        x = np.tile(mel, chunk_size//n_frames + 1)\n        x = x[np.newaxis, :, :chunk_size]\n    else:\n        n_crops = n_frames // chunk_size + 1\n        x = np.pad(mel, ([0,0], [0, n_crops * chunk_size - n_frames]))\n        x = np.split(x, n_crops, axis=-1)\n        x = np.array(x)\n        \n    features = normalize(x)\n    features = np.expand_dims(features, axis=-1)\n    \n    return features\n\ndef show_pred(models, mel, labels_list, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10,4))\n    \n    if not isinstance(models, list):\n        models = [models]\n    \n    mel = prepare_mel(mel)\n    pred = average_models_pred(models, mel)\n    \n    top5_idx = np.argsort(pred)[::-1][:5]\n    top_labels = labels_list[top5_idx]\n    top_preds = pred[top5_idx]\n    \n    ax.bar(top_labels, top_preds)\n    \ndef average_models_pred(models, mel):\n    preds = [ np.mean(m.predict(mel), axis=0) for m in models ]\n    return np.mean(preds, axis=0)\n    \ndef create_submission(models, save_name='model_preds'):\n    submission_df = pd.read_csv('/kaggle/input/freesound-audio-tagging-2019/sample_submission.csv', index_col='fname')\n    test_mel_paths = glob.glob('/kaggle/input/logmel128/test_logmel/*.npy')\n    \n    for path in test_mel_paths:\n        mel = np.load(path)\n        mel = prepare_sample(mel)\n        pred = average_models_pred(models, mel)        \n        submission_df.loc[path.split('/')[-1]] = pred\n    \n    submission_df.to_csv(f'{save_name}.csv',index = False)\n    print(f'Successfully created {save_name}.csv !')\n    \n    return submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mel_test = load_random_test(test_mel_paths)\n\nfig, ax = plt.subplots(figsize=(15, 3))\n\ntrained_models = list(models.values())\n# trained_models = [ m.load_weights(f'best_model_{i}.h5') for i,m in enumerate(trained_models) ]\n\nshow_pred(trained_models, mel_test, train_curated_labels.columns, ax=ax)    \nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"audio = convert_spectra2sound(mel_test)\nlisten_sample(audio)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = create_submission(model1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TODO\n\n**Debugging:**\n- [x] Try out on a reduced dataset (~10 labels) -> add a flag to load reduced dataset.\n\n**Preprocessing:**\n- [ ] Trim silences\n- [ ] Augmentation pipeline (random shift, pitch, noise, ...) [Mixup paper](https://arxiv.org/pdf/1710.09412.pdf)\n- [x] Improving feature extraction (import as separate dataset)\n- [ ] Write signal transformers (wrap padding, centering, trimming)\n\n**Modelling:**\n- [ ] Reproduce baseline [github](https://github.com/qiuqiangkong/dcase2019_task2)\n- [ ] Implement approach from [this paper](http://dcase.community/documents/challenge2019/technical_reports/DCASE2019_Akiyama_94_t2.pdf).\n- [ ] Take a look at [this notebook](https://www.kaggle.com/romul0212/freesound-predict-dataset) and associated [github](https://github.com/lRomul/argus-freesound)\n- [ ] Try out the approach described in [this paper](https://arxiv.org/pdf/1904.03476.pdf).\n- [ ] Explore ensemble learning strategy [from this paper](https://arxiv.org/pdf/1810.12832.pdf) and associated [github](https://github.com/Cocoxili/DCASE2018Task2/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}